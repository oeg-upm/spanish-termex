{
    "id": "I-65",
    "original_text": "Graphical Models for Online Solutions to Interactive POMDPs Prashant Doshi Dept. of Computer Science University of Georgia Athens, GA 30602, USA pdoshi@cs.uga.edu Yifeng Zeng Dept. of Computer Science Aalborg University DK-9220 Aalborg, Denmark yfzeng@cs.aau.edu Qiongyu Chen Dept. of Computer Science National Univ. of Singapore 117543, Singapore chenqy@comp.nus.edu.sg ABSTRACT We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation. These graphical models called interactive dynamic influence diagrams (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables. I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs. I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness. Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Theory 1. INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in partially observable multiagent environments. They generalize POMDPs [13] to multiagent settings by including the other agents computable models in the state space along with the states of the physical environment. The models encompass all information influencing the agents behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11]. I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a decision-makers perspective in the interaction. In [15], Polich and Gmytrasiewicz introduced interactive dynamic influence diagrams (I-DIDs) as the computational representations of I-POMDPs. I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs. I-DIDs contribute to a growing line of work [19] that includes multi-agent influence diagrams (MAIDs) [14], and more recently, networks of influence diagrams (NIDs) [8]. These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables. MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agents actions and chance nodes capturing the agents private information. MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the independence structure. NIDs extend MAIDs to include agents uncertainty over the game being played and over models of the other agents. Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent. Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently. However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games. Matters are more complex when we consider interactions that are extended over time, where predictions about others future actions must be made using models that change as the agents act and observe. I-DIDs address this gap by allowing the representation of other agents models as the values of a special model node. Both, other agents models and the original agents beliefs over these models are updated over time using special-purpose implementations. In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID. Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently. Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I-DID by [15], and show that it could be replaced by traditional dependency links. In the previous representation of the I-DID, the update of the agents belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time. We explicate the semantics of this link by showing how it can be implemented using the traditional dependency links between the chance nodes that constitute the model nodes. The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs. We show how IDIDs may be used to model an agents uncertainty over others models, that may themselves be I-DIDs. Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others models. Analogous to DIDs, I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. 2. BACKGROUND: FINITELY NESTED IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents models as part of the state space [9]. Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents models and their beliefs about others. For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j. A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment. Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj . Here, j is Bayes rational and OCj is js optimality criterion. SMj is the set of subintentional models of j. Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent. We give a recursive bottom-up construction of the interactive state space below. ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . . ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent is preferences over its interactive states. Usually only the physical states will matter. Agent is policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i. Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai). 2.1 Belief Update Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes. However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones. First, since the state of the physical environment depends on the actions of both agents, is prediction of how the physical state changes has to be made based on its prediction of js actions. Second, changes in js models have to be included in is belief update. Specifically, if j is intentional then an update of js beliefs due to its action and observation has to be included. In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief. If js model is subintentional, then js probable observations are appended to the observation history contained in the model. Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update. For a version of the belief update when js model is subintentional, see [9]. If agent j is also modeled as an I-POMDP, then is belief update invokes js belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke is belief update and so on. This recursion in belief nesting bottoms out at the 0th level. At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9]. 2.2 Value Iteration Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)). Eq. 2 is a basis for value iteration in I-POMDPs. Agent is optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3. INTERACTIVEINFLUENCEDIAGRAMS A naive extension of influence diagrams (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes. However, this approach assumes that the agents actions are controlled using a probability distribution that does not change over time. Interactive influence diagrams (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node. We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon. We note that the probability distribution over the chance node, S, and the model node together represents agent is belief over its interactive states. In addition to the model 1 The 0th level model is a POMDP: Other agents actions are treated as exogenous events and folded into the T, O, and R functions. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j. The hexagon is the model node (Mj,l−1) whose structure we show in (b). Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ). Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the policy link in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model. In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs. The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2. Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional. Because the model node contains the alternative models of the other agent as its values, its representation is not trivial. In particular, some of the models within the node are I-IDs that when solved generate the agents optimal policy in their decision nodes. Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise. Borrowing insights from previous work [8], we observe that the model node and the dashed policy link that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b). The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability. The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj. Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent is beliefs. The conditional probability table of the chance node, Aj, is a multiplexer that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj]. The values of Mod[Mj] denote the different models of j. In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1. The distribution over the node, Mod[Mj], is the agent is belief over the models of j given a physical state. For more agents, we will have as many model nodes as there are agents. Notice that Fig. 1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional dependency links. In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them. In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes. This allows I-IDs to be represented and implemented using conventional application tools that target IDs. Note that we may view the level l I-ID as a NID. Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2). If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID. Note that within the I-IDs (or IDs) at each level, there is only a single decision node. Thus, our NID does not contain any MAIDs. Figure 2: A level l I-ID represented as a NID. The probabilities assigned to the blocks of the NID are is beliefs over js models conditioned on a physical state. 3.2 Solution The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively. We start by solving the level 0 models, which, if intentional, are traditional IDs. Their solutions provide probability distributions over the other agents actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID. The mapping from the level 0 models decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability. Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c). During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj]. As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent is belief over the models of j conditioned on the physical state. The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j. Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18]. This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief. Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agents current belief is known. 4. INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic influence diagrams (I-DIDs) extend I-IDs (and NIDs) to allow sequential decision-making over several time steps. Just as DIDs are structured graphical representations of POMDPs, I-DIDs are the graphical online analogs for finitely nested I-POMDPs. I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents. 4.1 Syntax We depict a general two time-slice I-DID in Fig. 3(a). In addition to the model nodes and the dashed policy link, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a). We explained the semantics of the model node and the policy link in the previous section; we describe the model updates next. The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1. Recall from Section 2 that an agents intentional model includes its belief. Because the agents act and receive observations, their models are updated to reflect their changed beliefs. Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models. Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models. Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model. These steps are a part of agent is belief update formalized using Eq. 1. In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID. If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ). These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation. The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously. Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed. The probability that js updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node. In order to obtain the probability of js possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ]. Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a multiplexer modulated by Mod[Mt j ]. Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1. Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1. Notice that the model update link may be replaced by the dependency links between the chance nodes that constitute the model nodes in the two time slices. In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them. Chance nodes and dependency links that not in bold are standard, usually found in DIDs. Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents js observation at time t + 1. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links. We note that the possible set of models of the other agent j grows exponentially with the number of time steps. For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node. 4.2 Solution Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively. For the purpose of illustration, let l=1 and T=2. The solution method uses the standard look-ahead technique, projecting the agents action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step. Because agent i has a belief over js models as well, the lookahead includes finding out the possible models that j could have in the future. Consequently, each of js subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions. These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b). We note the recursive nature of this solution: in solving agent is level 1 I-DID, js level 0 DIDs must be solved. If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner. We briefly outline the recursive algorithm for solving agent is Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase 1. For t from 1 to T − 1 do 2. If l ≥ 1 then Populate Mt+1 j,l−1 3. For each mt j in Range(Mt j,l−1) do 4. Recursively call algorithm with the l − 1 I-ID (or ID) that represents mt j and the horizon, T − t + 1 5. Map the decision node of the solved I-ID (or ID), OPT(mt j), to a chance node Aj 6. For each aj in OPT(mt j) do 7. For each oj in Oj (part of mt j) do 8. Update js belief, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← New I-ID (or ID) with bt+1 j as the initial belief 10. Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11. Add the model node, Mt+1 j,l−1, and the dependency links between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b)) 12. Add the chance, decision, and utility nodes for t + 1 time slice and the dependency links between them 13. Establish the CPTs for each chance node and utility node Look-Ahead Phase 14. Apply the standard look-ahead and backup method to solve the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5. We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the dependency links and the conditional probability tables for each node. We particularly focus on establishing and populating the model nodes (lines 3-11). Note that Range(·) returns the values (lower level models) of the random variable given as input (model node). In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs. Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents. As we mentioned previously, the 0-th level models are the traditional DIDs. Their solutions provide probability distributions over actions of the agent modeled at that level to I-DIDs at level 1. Given probability distributions over other agents actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models. Assume that the number of models considered at each level is bound by a number, M. Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs. 5. EXAMPLE APPLICATIONS To illustrate the usefulness of I-DIDs, we apply them to three problem domains. We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it. 5.1 Followership-Leadership in the Multiagent Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9]. The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L). In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agents opening one of the doors. When any door is opened, the tiger persists in its original location with a probability of 95%. Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%. Agent j, on the other hand, hears growls with a reliability of 95%. Thus, the setting is such that agent i hears agent j opening doors more reliably than the tigers growls. This suggests that i could use js actions as an indication of the location of the tiger, as we discuss below. Each agents preferences are as in the single agent game discussed in [13]. The transition, observation, and reward functions are shown in [16]. A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions. In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship. In particular, we analyze the situational and epistemological conditions sufficient for their emergence. The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the others actions in order to maximize its payoffs. Let us consider a particular setting of the tiger problem in which agent i believes that js preferences are aligned with its own - both of them just want to get the gold - and js hearing is more reliable in comparison to itself. As an example, suppose that j, on listening can discern the tigers location 95% of the times compared to is 65% accuracy. Additionally, agent i does not have any initial information about the tigers location. In other words, is single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger. In addition, i considers two models of j, which differ in js flat level 0 initial beliefs. This is represented in the level 1 I-ID shown in Fig. 6(a). According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)). Agent i is undecided on these two models of j. If we vary is hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior. If is probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow js actions: i opens the same door that j opened previously iff is own assessment of the tigers location confirms js pick. If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)). Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem. Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations. We observed that a single level of belief nesting - beliefs about the others models - was sufficient for followership to emerge in the tiger problem. However, the epistemological requirements for the emergence of leadership are more complex. For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i. As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow js actions over time. Agent j emerges as a leader if it believes that i will follow it, which implies that js belief must be nested two levels deep to enable it to recognize its leadership role. Realizing that i will follow presents j with an opportunity to influence is actions in the benefit of the collective good or its self-interest alone. For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original. If j alone selects the correct door, it gets the payoff of 10. On the other hand, if both agents pick the wrong door, their penalties are cut in half. In this setting, it is in both js best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader. However, consider a slightly different problem in which j gains from is loss and is penalized if i gains. Specifically, let is payoff be subtracted from js, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then is loss of 100 becomes js gain. Agent j believes that i incorrectly thinks that js preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is. Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following js actions. We show is normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a). The policy demonstrates that i will blindly follow js actions. Since the tiger persists in its original location with a probability of 0.95, i will select the same door again. If j begins the game with a 99% probability that the tiger is on the right, solving js I-DID nested two levels deep, results in the policy shown in Fig. 8(b). Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL. Agent js intention is to deceive i who, it believes, will follow js actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest. Figure 8: Emergence of deception between agents in the tiger problem. Behaviors of interest are in bold. * denotes as before. (a) Agent is policy demonstrating that it will blindly follow js actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i. 5.2 Altruism and Reciprocity in the Public Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves. Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot. However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes. Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others contributions. However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions. The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4]. Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting. These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others. For simplicity, we assume that the game is played between M = 2 agents, i and j. Let each agent be initially endowed with XT amount of resources. While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions. Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute. The latter action is deThe Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D). We assume that the actions are not observable to others. The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return. We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain. Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment. In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment. Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent. For simplicity, we assume that the cost of punishing is same for both the agents. The one-shot PG game with punishment is shown in Table. 1. Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action. If P < XT − ciXT , then defection is the dominating action for both. If P = XT − ciXT , then the game is not dominance-solvable. Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a). We formulate a sequential version of the PG problem with punishment from the perspective of agent i. Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents. Each agent may contribute a fixed amount, xc, or defect. An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot. Notice that the observations are also indirectly indicative of agent js actions because the state of the public pot is influenced by them. The amount of resources in agent is private pot, is perfectly observable to i. The payoffs are analogous to Table. 1. Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)). Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect. Let xc = 1 and the level 0 agent be punished half the times it defects. With one action remaining, both types of agents choose to contribute to avoid being punished. With two actions to go, the altruistic type chooses to contribute, while the other defects. This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids. Because cj for the non-altruistic type is less, it prefers not to contribute. With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects. For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection. We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level 0 models, mentioned previously, to j (see Fig. 9). If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps. This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type. However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1. These results demonstrate that the behavior of our altruistic type resembles that found experimentally. The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic. We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection. The reciprocal types marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other. We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full. For this prior belief, i chooses to defect. On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)). This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from js action to contribute. Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step. Agent i therefore chooses to contribute to reciprocate js action. An analogous reasoning leads i to defect when it observes a meager pot. With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations. Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic). 5.3 Strategies in Two-Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2]. Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck. While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands. To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit. Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot. During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to 8. Notice that, for example, if a lower numbered card is discarded, the probability of receiving a low card in exchange is now reduced. We show the level 1 I-ID for the simplified two-player Poker in Fig. 11. We considered two models (personality types) of agent j. The conservative type believes that it is likely that its opponent has a high numbered card in its hand. On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card. Thus, the two types differ in their beliefs over their opponents hand. In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution. With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand. This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange. The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one. Figure 11: (a) Level 1 I-ID of agent i. The observation reveals information about js hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in js hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. is own hand contains the card numbered 8. The agent starts by keeping its card. On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card. If i observes that j discarded its card into the L or H pile, i believes that j is aggressive. On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange. Because the probability of receiving a low card is high now, i chooses to keep its card. On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card. In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff. This is partly due to the fact that an observation of, say, L resets the agent is previous time step beliefs over js hand to the low numbered cards only. 6. DISCUSSION We showed how DIDs may be extended to I-DIDs that enable online sequential decision-making in uncertain multiagent settings. Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent is three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs. I-DIDs extend NIDs to allow sequential decision-making over multiple time steps in the presence of other interacting agents. I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs. We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality. Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work. The first author would like to acknowledge the support of a UGARF grant. 7. REFERENCES [1] R. J. Aumann. Interactive epistemology i: Knowledge. International Journal of Game Theory, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron. The challenge of poker. AIJ, 2001. [3] A. Brandenburger and E. Dekel. Hierarchies of beliefs and common knowledge. Journal of Economic Theory, 59:189-198, 1993. [4] C. Camerer. Behavioral Game Theory: Experiments in Strategic Interaction. Princeton University Press, 2003. [5] E. Fehr and S. Gachter. Cooperation and punishment in public goods experiments. American Economic Review, 90(4):980-994, 2000. [6] D. Fudenberg and D. K. Levine. The Theory of Learning in Games. MIT Press, 1998. [7] D. Fudenberg and J. Tirole. Game Theory. MIT Press, 1991. [8] Y. Gal and A. Pfeffer. A language for modeling agents decision-making processes in games. In AAMAS, 2003. [9] P. Gmytrasiewicz and P. Doshi. A framework for sequential planning in multiagent settings. JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz and E. Durfee. Rational coordination in multi-agent environments. JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi. Games with incomplete information played by bayesian players. Management Science, 14(3):159-182, 1967. [12] R. A. Howard and J. E. Matheson. Influence diagrams. In R. A. Howard and J. E. Matheson, editors, The Principles and Applications of Decision Analysis. Strategic Decisions Group, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman, and A. Cassandra. Planning and acting in partially observable stochastic domains. Artificial Intelligence Journal, 2, 1998. [14] D. Koller and B. Milch. Multi-agent influence diagrams for representing and solving games. In IJCAI, pages 1027-1034, 2001. [15] K. Polich and P. Gmytrasiewicz. Interactive dynamic influence diagrams. In GTDT Workshop, AAMAS, 2006. [16] B. Rathnas., P. Doshi, and P. J. Gmytrasiewicz. Exact solutions to interactive pomdps using behavioral equivalence. In Autonomous Agents and Multi-Agent Systems Conference (AAMAS), 2006. [17] S. Russell and P. Norvig. Artificial Intelligence: A Modern Approach (Second Edition). Prentice Hall, 2003. [18] R. D. Shachter. Evaluating influence diagrams. Operations Research, 34(6):871-882, 1986. [19] D. Suryadi and P. Gmytrasiewicz. Learning models of other agents using influence diagrams. In UM, 1999. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821",
    "original_translation": "Modelos gráficos para soluciones en línea para POMDPS interactivos Prashant Doshi Dept. de la Universidad de Informática de Georgia Atenas, GA 30602, EE. UU. Pdoshi@cs.uga.edu Yifeng Zeng Dept. de Computer Science Aalborg Universidad DK-9220 Aalborg, Dinamarca yfzeng@cs.aau.edu Qiongyu Chen Dept. de Informática Nacional Univ.de Singapur 117543, Singapur chenqy@comp.nus.edu.sg Resumen Desarrollamos una nueva representación gráfica para los procesos de decisión de Markov parcialmente observables interactivos (I POMDPS) que es significativamente más transparente y semánticamente clara que la representación anterior. Estos modelos gráficos llamados diagramas de influencia dinámica interactiva (I-DID) buscan modelar explícitamente la estructura que a menudo está presente en los problemas del mundo real al descomponer la situación en variables casuales y de decisión, y las dependencias entre las variables. Los I-DID generalizan los DID, que pueden verse como representaciones gráficas de POMDPS, a configuraciones multiagentes de la misma manera que los I-POMDP generalizan POMDPS. Los I-DID se pueden usar para calcular la política de un agente en línea como el agente actúa y observa en un entorno poblado por otros agentes interactivos. Usando varios ejemplos, mostramos cómo se pueden aplicar I-Dids y demostrar su utilidad. Categorías y descriptores de sujetos I.2.11 [Inteligencia artificial distribuida]: Sistemas Multiagentes Términos generales Teoría 1. Introducción Los procesos de decisión Markov parcialmente observables interactivos (IPOMDPS) [9] proporcionan un marco para la toma de decisiones secuenciales en entornos multiagentes parcialmente observables. Generalizan POMDPS [13] a entornos múltiples al incluir los modelos computables de otros agentes en el espacio estatal junto con los estados del entorno físico. Los modelos abarcan toda la información que influye en los comportamientos de los agentes, incluidas sus preferencias, capacidades y creencias, y por lo tanto son análogos a los tipos en los juegos bayesianos [11]. Los I POMDP adoptan un enfoque subjetivo para comprender el comportamiento estratégico, arraigado en un marco teórico de decisión que toma una perspectiva de toma de decisiones en la interacción. En [15], Polich y Gmytrasiewicz introdujeron diagramas de influencia dinámica interactiva (I-DID) como las representaciones computacionales de I-POMDPS. Los I-DID generalizan los DID [12], que pueden verse como contrapartes computacionales de POMDPS, a la configuración de múltiples giros de la misma manera que los I-POMDP generalizan POMDPS. Los I-Did contribuyen a una línea de trabajo creciente [19] que incluye diagramas de influencia de múltiples agentes (mucamas) [14], y más recientemente, redes de diagramas de influencia (NID) [8]. Estos formalismos buscan modelar explícitamente la estructura que a menudo está presente en los problemas del mundo real al descomponer la situación en variables de casualidad y decisión, y las dependencias entre las variables. Las criadas proporcionan una alternativa a las formas de juego normales y extensas utilizando un formalismo gráfico para representar juegos de información imperfecta con un nodo de decisión para las acciones de cada agente y los nodos casuales que capturan la información privada de los agentes. Las criadas analizan objetivamente el juego, calculando eficientemente el perfil de equilibrio Nash explotando la estructura de independencia. NIDS extiende a las criadas para incluir la incertidumbre de los agentes sobre el juego que se juega y sobre los modelos de los otros agentes. Cada modelo es una criada y la red de mucamas se colapsan, abajo, en una sola criada para calcular el equilibrio del juego teniendo en cuenta los diferentes modelos de cada agente. Formalismos gráficos como mucamas y NID abren un área prometedora de investigación que tiene como objetivo representar interacciones multiagentes de manera más transparente. Sin embargo, las mucamas proporcionan un análisis del juego desde un punto de vista externo y la aplicabilidad de ambos se limita a los juegos estáticos de juego único. Las cosas son más complejas cuando consideramos interacciones que se extienden con el tiempo, donde las predicciones sobre otras acciones futuras deben hacerse utilizando modelos que cambian a medida que los agentes actúan y observan. Los I-DID abordan esta brecha al permitir la representación de otros modelos de agentes como los valores de un nodo modelo especial. Tanto los modelos de otros agentes como las creencias de los agentes originales sobre estos modelos se actualizan con el tiempo utilizando implementaciones de uso especial. En este artículo, mejoramos la representación preliminar anterior del I-DID que se muestra en [15] utilizando la idea de que el I-ID estático es un tipo de NID. Por lo tanto, podemos utilizar construcciones de lenguaje específicas de NID como multiplexores para representar el nodo modelo y, posteriormente, el I-ID, de manera más transparente. Además, aclaramos la semántica del enlace de política de propósito especial introducido en la representación de I-Did por [15], y mostramos que podría ser reemplazado por enlaces de dependencia tradicionales. En la representación anterior de la I-DID, la actualización de la creencia de los agentes sobre los modelos de otros a medida que los agentes actúan y reciben observaciones se denotó utilizando un enlace especial llamado enlace de actualización del modelo que conectó los nodos modelo a lo largo del tiempo. Explicamos la semántica de este enlace mostrando cómo se puede implementar utilizando los enlaces de dependencia tradicionales entre los nodos casuales que constituyen los nodos modelo. El resultado neto es una representación de I-DID que es significativamente más transparente, semánticamente clara y capaz de implementarse utilizando los algoritmos estándar para resolver DIDS. Mostramos cómo se pueden usar IDID para modelar la incertidumbre de los agentes sobre otros modelos, que pueden ser I-Dids. La solución al I-Did es una política que prescribe lo que el agente debe hacer con el tiempo, dadas sus creencias sobre el estado físico y otros modelos. Análogo a los DIDS, los I-DID se pueden usar para calcular la política de un agente en línea como el agente actúa y observa en un entorno poblado por otros agentes interactivos.2. Antecedentes: los POMDP interactivos de IPOMDPS finitamente anidados generalizan POMDPS en entornos múltiples al incluir otros modelos de agentes como parte del espacio estatal [9]. Dado que otros agentes también pueden razonar sobre otros, el espacio estatal interactivo está estratégicamente anidado;Contiene creencias sobre otros modelos de agentes y sus creencias sobre los demás. Por simplicidad de presentación consideramos un agente, I, que está interactuando con otro agente, j. Un i-POMDP finitamente anidado del Agente I con un nivel de estrategia L se define como la tupla: I-POMDPI, L = ISI, L, A, Ti, ωi, Oi, Ri donde: • ISI, L denota un conjunto de interactivosestados definidos como, isi, l = s × mj, l - 1, donde mj, l - 1 = {θj, l - 1 ∪ smj}, para l ≥ 1, e isi, 0 = s, donde s es el conjuntode los estados del entorno físico. Θj, L - 1 es el conjunto de modelos intencionales computables del agente J: θj, l - 1 = bj, l - 1, ˆθj donde el marco, ˆθj = a, ωj, tj, oj, rj, ocj. Aquí, J es Bayes Rational y OCJ es JS Optimity Criterion. SMJ es el conjunto de modelos subintencionales de j. Ejemplos simples de modelos subintencionales incluyen un modelo de no información [10] y un modelo de juego ficticio [6], los cuales son independientes de la historia. Damos una construcción recursiva de abajo hacia arriba del espacio de estado interactivo a continuación. Isi, 0 = s, θj, 0 = {bj, 0, ˆθj |bj, 0 ∈ δ (isj, 0)} ISI, 1 = S × {θj, 0 ∪ Smj}, θj, 1 = {bj, 1, ˆθj |bj, 1 ∈ δ (isj, 1)}...... Isi, l = s × {θj, l - 1 ∪ smj}, θj, l = {bj, l, ˆθj |bj, l ∈ δ (isj, l)} formulaciones similares de espacios anidados han aparecido en [1, 3].• A = AI × AJ es el conjunto de acciones conjuntas de todos los agentes en el medio ambiente;• Ti: S × A × S → [0, 1], describe el efecto de las acciones conjuntas en los estados físicos del medio ambiente;• ωi es el conjunto de observaciones del agente I;• Oi: S × A × ωi → [0, 1] da la probabilidad de las observaciones dada el estado físico y la acción articular;• RI: ISI × A → R describe el agente es preferencias sobre sus estados interactivos. Por lo general, solo los estados físicos importarán. El agente es la política es el mapeo, Ω ∗ i → δ (ai), donde ω ∗ i es el conjunto de todas las historias de observación del agente i. Dado que la creencia sobre los estados interactivos forma una estadística suficiente [9], la política también puede representarse como un mapeo del conjunto de todas las creencias del Agente I a una distribución sobre sus acciones, δ (ISI) → δ (AI).2.1 Actualización de creencias análoga a POMDPS, un agente dentro del marco I-POMDP actualiza su creencia como actúa y observa. Sin embargo, hay dos diferencias que complican la actualización de creencias en entornos multiagentes en comparación con los de un solo agente. Primero, dado que el estado del entorno físico depende de las acciones de ambos agentes, es la predicción de cómo se debe hacer los cambios en el estado físico en función de su predicción de las acciones JS. En segundo lugar, los cambios en los modelos JS deben incluirse en la actualización de la creencia. Específicamente, si J es intencional, entonces se debe incluir una actualización de las creencias JS debido a su acción y observación. En otras palabras, tengo que actualizar su creencia en función de su predicción de lo que J observaría y cómo J actualizaría su creencia. Si el modelo JS es subintencional, entonces las observaciones probables de JS se agregan al historial de observación contenido en el modelo. Formalmente, tenemos: Pr (ist | AT - 1 I, BT - 1 I, L) = β IST - 1: MT - 1 J = θt J BT - 1 I, L (ist - 1) × AT - 1 JPR (AT - 1 J | θT - 1 J, L - 1) OI (ST, AT - 1 I, AT - 1 J, OT I) × TI (ST - 1, AT - 1 I, AT - 1 J,st) ot J Oj (ST, AT - 1 I, AT - 1 J, OT J) × τ (SEθt J (BT - 1 J, L - 1, AT - 1 J, OT J) - BT J, L−1) (1) donde β es la constante de normalización, τ es 1 si su argumento es 0 de lo contrario es 0, PR (AT - 1 J | θt - 1 J, L - 1) es la probabilidad de que AT - 1 J ISBayes racional para el agente descrito por el modelo θt - 1 J, L - 1 y SE (·) es una abreviatura para la actualización de creencias. Para una versión de la actualización de creencias cuando el modelo JS es subinstendente, ver [9]. Si el Agente J también se modela como un I POMDP, entonces la actualización de creencias invoca la actualización de creencias de JS (a través del término Seθt J (BT-1 J, L-1, At-1 J, OT J)), que a su vez podríaInvoke es la actualización de creencias, etc. Esta recursión en las creencias que anidan en el nivel 0. En este nivel, la actualización de creencias del agente se reduce a una actualización de creencias de POMDP.1 Para las ilustraciones de la actualización de creencias, detalles adicionales sobre i-POMDPS y cómo se comparan con otros marcos multiagentes, ver [9].2.2 Valor iteración Cada estado de creencia en un I-POMDP finitamente anidado tiene un valor asociado que refleja el pago máximo que el agente puede esperar en este estado de creencia: un (bi, l, θi) = max ai∈Ai is∈Isi, l eri ((is, ai) bi, l (is)+ γ oi∈ωi pr (oi | ai, bi, l) un - 1 (seθi (bi, l, ai, oi), θi) (2) donde, eri (is (is (is (is (is (is (is (is (is (iss, ai) = aj ri (is, ai, aj) pr (aj | mj, l - 1) (ya que es = (s, mj, l - 1)). Ec.2 es una base para la iteración de valor en i-POMDPS. El agente es una acción óptima, a ∗ i, para el caso del horizonte finito con descuento, es un elemento del conjunto de acciones óptimas para el estado de creencia, OPT (θi), definido como: Opt (bi, l, θi) = argmaxai∈Ai is∈Isi, l eri (is, ai) bi, l (is) +γ oi∈ωi pr (oi | ai, bi, l) un (seθi (bi, l, ai, oi), θi)(3) 3. InteractiveInfluencedAgrams Una extensión ingenua de los diagramas de influencia (ID) a la configuración poblada por múltiples agentes es posible tratando a otros agentes como autómatas, representados usando nodos casuales. Sin embargo, este enfoque supone que las acciones de los agentes se controlan utilizando una distribución de probabilidad que no cambia con el tiempo. Los diagramas de influencia interactiva (I-IDS) adoptan un enfoque más sofisticado al generalizar las ID para hacerlos aplicables a la configuración compartida con otros agentes que pueden actuar y observar, y actualizar sus creencias.3.1 Sintaxis Además de la oportunidad habitual, la decisión y los nodos de utilidad, los IID incluyen un nuevo tipo de nodo llamado nodo modelo. Mostramos un nivel L I-ID general en la Fig. 1 (a), donde el nodo modelo (MJ, L-1) se denota usando un hexágono. Observamos que la distribución de probabilidad sobre el nodo casual, S y el nodo modelo juntos representa el agente es la creencia sobre sus estados interactivos. Además del Modelo 1, el modelo de nivel 0 es un POMDP: otras acciones de agentes se tratan como eventos exógenos y se doblan en las funciones T, O y R. El sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 815 Figura 1: (a) Un nivel genérico L I-ID para el agente I situado con otro agente j. El hexágono es el nodo modelo (MJ, L - 1) cuya estructura mostramos en (b). Los miembros del nodo modelo son I-IDS (M1 J, L-1, M2 J, L-1; diagramas no se muestran aquí por simplicidad) cuyos nodos de decisión se asignan a los nodos de oportunidad correspondientes (A1 J, A2 J). Dependiendo del valor del nodo, Mod [MJ], la distribución de cada uno de los nodos de posibilidades se asigna al nodo AJ.(c) El I-ID transformado con el nodo modelo reemplazado por los nodos casuales y las relaciones entre ellos.Nodo, I-IDS difieren de las ID al tener un enlace discontinuo (llamado enlace de política en [15]) entre el nodo modelo y un nodo casual, AJ, que representa la distribución sobre las acciones de otros agentes dado su modelo. En ausencia de otros agentes, el nodo modelo y el nodo Chance, AJ, Vanish e I-ID colapsan en ID tradicionales. El nodo modelo contiene los modelos computacionales alternativos atribuidos por I al otro agente del conjunto, θj, L - 1 ∪ SMJ, donde θj, L - 1 y SMJ se definieron previamente en la Sección 2. Por lo tanto, un modelo en el nodo modelo puede ser un I-ID o ID, y la recursión termina cuando un modelo es una ID o subintencionales. Debido a que el nodo modelo contiene los modelos alternativos del otro agente como sus valores, su representación no es trivial. En particular, algunos de los modelos dentro del nodo son I-IDS que cuando se resuelven generan la política óptima de los agentes en sus nodos de decisión. Cada nodo de decisión se asigna al nodo de oportunidad correspondiente, digamos A1 J, de la siguiente manera: si OPT es el conjunto de acciones óptimas obtenidas resolviendo el I-ID (o ID), entonces Pr (AJ ∈ A1 J) = 1| OP T |Si aj ∈ Opt, 0 de lo contrario. Pediendo prestados ideas de trabajos anteriores [8], observamos que el nodo modelo y el enlace de política discontinua que lo conecta al nodo casual, AJ, podría representarse como se muestra en la Fig. 1 (b). El nodo de decisión de cada nivel L-1 I-ID se transforma en un nodo casual, como mencionamos anteriormente, de modo que las acciones con el mayor valor en el nodo de decisión se asignan probabilidades uniformes en el nodo casual, mientras que el resto se asigna ceroprobabilidad. Los diferentes nodos casuales (A1 J, A2 J), uno para cada modelo, y además, el nodo probable etiquetado Mod [MJ] forman los padres del nodo casual, AJ. Por lo tanto, hay tantos nodos de acción (A1 J, A2 J) en MJ, L - 1, ya que el número de modelos en el apoyo del agente son las creencias. La tabla de probabilidad condicional del nodo casual, AJ, es un multiplexor que asume la distribución de cada uno de los nodos de acción (A1 J, A2 J) dependiendo del valor de Mod [MJ]. Los valores de Mod [MJ] denotan los diferentes modelos de j. En otras palabras, cuando Mod [MJ] tiene el valor M1 J, L - 1, el nodo casual AJ asume la distribución del nodo A1 J, y AJ asume la distribución de A2 J cuando Mod [MJ] tiene el valor M2 J, L - 1. La distribución sobre el nodo, Mod [MJ], es que el agente es la creencia sobre los modelos de J dado un estado físico. Para más agentes, tendremos tantos nodos modelo como agentes. Observe que la Fig. 1 (b) aclara la semántica del enlace de la política y muestra cómo se puede representar utilizando los enlaces de dependencia tradicionales. En la Fig. 1 (c), mostramos el I-ID transformado cuando el nodo modelo se reemplaza por los nodos casuales y las relaciones entre ellos. A diferencia de la representación en [15], no hay enlaces de política de uso especial, sino que el I-ID está compuesto solo por aquellos tipos de nodos que se encuentran en las ID tradicionales y las relaciones de dependencia entre los nodos. Esto permite que los I-ID se representen e implementen utilizando herramientas de aplicación convencionales que se dirigen a IDS. Tenga en cuenta que podemos ver el nivel L I-ID como un NID. Específicamente, cada uno de los modelos de nivel L - 1 dentro del nodo modelo son bloques en el NID (ver Fig. 2). Si el nivel l = 1, cada bloque es una identificación tradicional, de lo contrario si l> 1, cada bloque dentro del NID puede ser un NID. Tenga en cuenta que dentro de los I-ID (o IDS) en cada nivel, solo hay un solo nodo de decisión. Por lo tanto, nuestro NID no contiene ninguna criada. Figura 2: A Nivel L I-ID representado como un NID. Las probabilidades asignadas a los bloques del NID son las creencias sobre los modelos JS condicionados en un estado físico.3.2 SOLUCIÓN La solución de un I-ID se realiza de manera ascendente, y se implementa de manera recursiva. Comenzamos resolviendo los modelos de nivel 0, que, si son intencionales, son ID tradicionales. Sus soluciones proporcionan distribuciones de probabilidad sobre las acciones de otros agentes, que se ingresan en los nodos de posibilidad correspondientes que se encuentran en el nodo modelo del ID I Nivel 1. La asignación de los nodos de decisión de los modelos de nivel 0 a los nodos de casualidad se lleva a cabo para que las acciones con el mayor valor en el nodo de decisión se les asigne probabilidades uniformes en el nodo casual, mientras que al resto se le asigna una probabilidad cero. Dadas las distribuciones sobre las acciones dentro de los diferentes nodos de probabilidad (uno para cada modelo del otro agente), el ID I-ID de nivel 1 se transforma como se muestra en la Fig. 1 (c). Durante la transformación, la tabla de probabilidad condicional (CPT) del nodo, AJ, se pobla de tal manera que el nodo asume la distribución de cada uno de los nodos de posibilidades dependiendo del valor del nodo, Mod [MJ]. Como mencionamos anteriormente, los valores del Nodo Mod [MJ] denotan los diferentes modelos del otro agente, y su distribución es que el agente es la creencia sobre los modelos de J condicionados en el estado físico. El ID ID de Nivel 1 transformado es una identificación tradicional que se puede resolver US816 el Sexto INTL. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) (a) (b) Figura 3: (a) Un nivel genérico de dos niveles de tiempo L i-Did para el agente I en un entorno con otro agente j. Observe el enlace de actualización del modelo punteado que denota la actualización de los modelos de J y la distribución sobre los modelos a lo largo del tiempo.(b) La semántica del enlace de actualización del modelo.en el método estándar de maximización de utilidad esperado [18]. Este procedimiento se lleva a cabo hasta el nivel L I-ID cuya solución proporciona el conjunto no vacío de acciones óptimas que el agente debe realizar dada su creencia. Tenga en cuenta que análogo a las ID, los I-ID son adecuados para la toma de decisiones en línea cuando se conoce la creencia actual de los agentes.4. Diagramas de influencia dinámica interactiva Los diagramas de influencia dinámica interactiva (I-DID) extienden I-IDS (y NIDS) para permitir la toma de decisiones secuenciales en varios pasos de tiempo. Así como las DID son representaciones gráficas estructuradas de POMDPS, los I-DID son los análogos gráficos en línea para i-POMDP finitamente anidados. Los I-Did se pueden usar para optimizar a través de una apariencia finita dadas las creencias iniciales mientras interactúan con otros agentes, posiblemente similares.4.1 Sintaxis representamos un I-Did de tono de tiempo general en la Fig. 3 (a). Además de los nodos modelo y el enlace de política discontinua, lo que diferencia un I-DID de A DO es el enlace de actualización del modelo que se muestra como una flecha punteada en la Fig. 3 (a). Explicamos la semántica del nodo modelo y el enlace de política en la sección anterior;Describimos las actualizaciones del modelo a continuación. La actualización del nodo del modelo con el tiempo implica dos pasos: primero, dados los modelos en el tiempo T, identificamos el conjunto actualizado de modelos que residen en el nodo del modelo en el tiempo t + 1. Recuerde de la Sección 2 que un modelo intencional de agentes incluye su creencia. Debido a que los agentes actúan y reciben observaciones, sus modelos se actualizan para reflejar sus creencias cambiadas. Dado que el conjunto de acciones óptimas para un modelo podría incluir todas las acciones, y el agente puede recibir cualquiera de | ωj |Posibles observaciones, el conjunto actualizado en el paso de tiempo t + 1 tendrá como máximo | mt j, l - 1 || aj || ωj |modelos. Aquí, | Mt J, L - 1 |es el número de modelos en el paso de tiempo T, | AJ |y | ωj |son los espacios más grandes de acciones y observaciones respectivamente, entre todos los modelos. En segundo lugar, calculamos la nueva distribución sobre los modelos actualizados dada la distribución original y la probabilidad de que el agente realice la acción y reciba la observación que condujo al modelo actualizado. Estos pasos son parte de la actualización de la creencia formalizada por la ecuación.1. En la Fig. 3 (b), mostramos cómo se implementa el enlace de actualización del modelo punteado en el I-DID. Si cada uno de los dos modelos de nivel L - 1 atribuidos a J en el paso T del tiempo T da como resultado una acción, y J podría hacer una de las dos observaciones posibles, entonces el nodo del modelo en el paso de tiempo T + 1 contiene cuatro modelos actualizados (MT + 1, 1 J, L - 1, MT+1,2 J, L - 1, MT+1,3 J, L - 1 y MT+1,4 J, L - 1). Estos modelos difieren en sus creencias iniciales, cada uno de los cuales es el resultado de J actualizando sus creencias debido a su acción y una posible observación. Los nodos de decisión en cada uno de los I-DID o DID que representan los modelos de nivel inferior se asignan a la Figura 4 correspondiente: I-DID transformado con los nodos modelo y el enlace de actualización del modelo reemplazado por los nodos de posibilidades y las relaciones (en negrita).Nodos del azar, como se mencionó anteriormente. A continuación, describimos cómo se calcula la distribución sobre el conjunto actualizado de modelos (se calcula la distribución sobre el mod de nodo Chance [MT+1 J] en MT+1 J, L - 1). La probabilidad de que el modelo actualizado JS sea, por ejemplo, MT+1,1 J, L - 1, depende de la probabilidad de realizar la acción y recibir la observación que condujo a este modelo, y la distribución previa sobre los modelos en el paso de tiempo T.Debido a que el nodo casual en J asume la distribución de cada uno de los nodos de acción en función del valor de Mod [MT J], la probabilidad de la acción viene dada por este nodo casual. Para obtener la probabilidad de JS Posible observación, introducimos el Nodo Nodo OJ, que dependiendo del valor de MOD [MT J] supone la distribución del nodo de observación en el modelo de nivel inferior denotado por MOD [MT J]. Debido a que la probabilidad de observaciones JS depende del estado físico y de las acciones conjuntas de ambos agentes, el nodo OJ está vinculado con ST+1, en J, y en i.2 Análogo a AT J, la tabla de probabilidad condicional de OJ también es un multiplexor modulado por MOD [MT J]. Finalmente, la distribución sobre los modelos anteriores en el tiempo t se obtiene del nodo casual, mod [mt j] en mt j, l - 1. En consecuencia, los nodos casuales, mod [mt j], en j, y OJ, forman los padres de mod [mt+1 j] en mt+1 j, l - 1. Observe que el enlace de actualización del modelo puede ser reemplazado por los enlaces de dependencia entre los nodos casuales que constituyen los nodos modelo en los dos cortes de tiempo. En la Fig. 4 mostramos los dos I-Did de corte de tiempo con los nodos modelo reemplazados por los nodos casuales y las relaciones entre ellos. Los nodos casuales y los enlaces de dependencia que no están en negrita son estándar, generalmente se encuentran en DIDS. La expansión del I-Did en más pasos de tiempo requiere la repetición de los dos pasos para actualizar el conjunto de modelos que forman la nota de 2 que OJ representa la observación JS en el tiempo t + 1. El sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 817 Valores del nodo modelo y agregando las relaciones entre los nodos casuales, tantas veces como hay enlaces de actualización del modelo. Observamos que el posible conjunto de modelos del otro agente J crece exponencialmente con la cantidad de pasos de tiempo. Por ejemplo, después de los pasos t, puede haber como máximo | mt = 1 j, l - 1 | (| aj || ωj |) t −1 modelos candidatos que residen en el nodo modelo.4.2 Solución análoga a I-IDS, la solución a un nivel L I-Did para el agente que expandí durante los pasos de tiempo T se puede llevar a cabo de manera recursiva. Para el propósito de la ilustración, deje l = 1 y t = 2. El método de solución utiliza la técnica estándar de apariencia, proyectando las secuencias de acción y observación de los agentes hacia adelante desde el estado de creencia actual [17], y encontrando las posibles creencias que podría tener en el próximo paso de tiempo. Debido a que el Agente I también cree sobre los modelos JS, el LookAhead incluye descubrir los posibles modelos que J podría tener en el futuro. En consecuencia, cada uno de los modelos JS Subintencions o Nivel 0 (representados usando un DIDA estándar) en el primer paso debe resolverse para obtener su conjunto óptimo de acciones. Estas acciones se combinan con el conjunto de posibles observaciones que J podría hacer en ese modelo, lo que resulta en un conjunto actualizado de modelos candidatos (que incluyen las creencias actualizadas) que podrían describir el comportamiento de J.Las creencias sobre este conjunto actualizado de modelos candidatos se calculan utilizando los métodos de inferencia estándar utilizando las relaciones de dependencia entre los nodos modelo como se muestra en la Fig. 3 (b). Observamos la naturaleza recursiva de esta solución: al resolver el agente de la resolución es el nivel 1 I-Did, JS Nivel 0 DIDS debe resolverse. Si la anidación de los modelos es más profunda, todos los modelos en todos los niveles a partir de 0 se resuelven de manera ascendente. Esbozamos brevemente el algoritmo recursivo para resolver el agente es el algoritmo para resolver la entrada I-DID: Nivel L ≥ 1 I-ID o ID de nivel 0, T Fase 1 de expansión. Para t de 1 a t - 1 do 2. Si L ≥ 1, entonces llenue MT+1 J, L - 1 3. Para cada MT J en el rango (Mt J, L - 1) do 4. Algoritmo de llamada recursivamente con el I-ID (o ID) recursivamente que representa MT J y el horizonte, t-t + 1 5. Mapee el nodo de decisión del ID (o ID) resuelto, OPT (MT J), a un nodo casual AJ 6. Para cada AJ en Opt (Mt J) do 7. Para cada OJ en OJ (parte de Mt J) do 8. ACTUALIZAR JS BREVIST, BT+1 J ← SE (BT J, AJ, OJ) 9. MT+1 J ← NUEVO I-ID (o ID) con BT+1 J como la creencia inicial 10. Rango (mt+1 j, l - 1) ∪ ← {mt+1 j} 11. Agregue el nodo modelo, MT+1 J, L - 1 y los enlaces de dependencia entre MT J, L - 1 y Mt+1 J, L - 1 (que se muestra en la Fig. 3 (b)) 12. Agregue la oportunidad, la decisión y los nodos de utilidad para T + 1 Time Slice y los enlaces de dependencia entre ellos 13. Establezca los CPT para cada nodo casual y de nodo de utilidad. Aplique el método estándar de apariencia y respaldo para resolver el I-DID expandido Figura 5: Algoritmo para resolver un nivel l ≥ 0 I-DID.Nivel L I-Did expandido durante los pasos de tiempo T con otro agente J en la figura 5. Adoptamos un enfoque de dos fases: dado un I-ID de nivel L (descrito anteriormente en la Sección 3) con todos los modelos de nivel inferior también representados como I-ID o IDS (IF Nivel 0), el primer paso es expandir el nivelL I-ID durante t Pasos de tiempo Agregar los enlaces de dependencia y las tablas de probabilidad condicionales para cada nodo. Nos centramos particularmente en establecer y poblar los nodos modelo (líneas 3-11). Tenga en cuenta que el rango (·) devuelve los valores (modelos de nivel inferior) de la variable aleatoria dada como entrada (nodo modelo). En la segunda fase, utilizamos una técnica de aspecto estándar que proyecta la acción y las secuencias de observación en los pasos de tiempo T en el futuro y respaldamos los valores de utilidad de las creencias accesibles. Similar a I-IS, los I-Dids se reducen a DIDS en ausencia de otros agentes. Como mencionamos anteriormente, los modelos de nivel 0-° son los DID tradicionales. Sus soluciones proporcionan distribuciones de probabilidad sobre las acciones del agente modeladas en ese nivel a I-Dids en el nivel 1. Dadas las distribuciones de probabilidad sobre las acciones de otros agentes, los IDID de nivel 1 se pueden resolver como dids y proporcionar distribuciones de probabilidad a modelos de nivel más alto. Suponga que el número de modelos considerados en cada nivel está sujeto a un número, M. Resolviendo un I-Did de nivel L en entonces equivalente a resolver O (ml) DIDS.5. Ejemplo de aplicaciones Para ilustrar la utilidad de los I-DID, las aplicamos a tres dominios de problemas. Describimos, en particular, la formulación del I-DID y las prescripciones óptimas obtenidas para resolverlo.5.1 Seguimiento de la vida útil En el problema de tigre multiagente Comenzamos nuestras ilustraciones de usar I-ID e I-DID con una versión ligeramente modificada del problema de tigre multiagente discutido en [9]. El problema tiene dos agentes, cada uno de los cuales puede abrir la puerta derecha (o), la puerta izquierda (OL) o escuchar (L). Además de escuchar gruñidos (desde la izquierda (GL) o desde la derecha (GR)) cuando escuchan, los agentes también escuchan crujidos (desde la izquierda (CL), desde la derecha (CR), o ninguna (s) crujir (s)), que indican ruidosamente los otros agentes que abren una de las puertas. Cuando se abre cualquier puerta, el tigre persiste en su ubicación original con una probabilidad del 95%. El agente I escucha gruñidos con una confiabilidad del 65% y cruje con una confiabilidad del 95%. El agente J, por otro lado, escucha gruñidos con una fiabilidad del 95%. Por lo tanto, la configuración es tal que el agente I escucha el agente j abriendo puertas de manera más confiable que los tigres gruñen. Esto sugiere que podría usar las acciones JS como una indicación de la ubicación del tigre, como discutimos a continuación. Las preferencias de cada agente son como en el juego de un solo agente discutido en [13]. Las funciones de transición, observación y recompensa se muestran en [16]. Un buen indicador de la utilidad de los métodos normativos para la toma de decisiones como I-Dids es el surgimiento de comportamientos sociales realistas en sus recetas. En entornos del persistente problema de tigre múltiple que reflejan situaciones del mundo real, demostramos seguidores entre los agentes y, como se muestra en [15], el engaño entre los agentes que creen que están en una relación con el líder de seguidores. En particular, analizamos las condiciones situacionales y epistemológicas suficientes para su aparición. El comportamiento de seguidores, por ejemplo, resulta del agente conociendo sus propias debilidades, evaluando las fortalezas, las preferencias y los posibles comportamientos del otro, y se da cuenta de que es mejor que siga las acciones de los demás para maximizar sus pagos. Consideremos una configuración particular del problema del tigre en el que el agente creo que las preferencias de JS están alineadas con la suya, ambos solo quieren obtener el oro, y la audición JS es más confiable en comparación con sí misma. Como ejemplo, suponga que J, en escuchar puede discernir la ubicación de los Tigres, el 95% de las veces en comparación con el 65% de precisión. Además, el Agente I no tiene información inicial sobre la ubicación de Tigers. En otras palabras, es la creencia anidada de un solo nivel, BI, 1, asigna 0.5 a cada una de las dos ubicaciones del tigre. Además, considero dos modelos de J, que difieren en JS Flat Level 0 Creencias iniciales. Esto se representa en el ID I-ID de Nivel 1 que se muestra en la Fig. 6 (a). Según un modelo, J asigna una probabilidad de 0.9 que el tigre está detrás de la puerta izquierda, mientras que el otro 818 el sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) Figura 6: (a) Nivel 1 I-ID del Agente I, (b) Dos ID de nivel 0 del Agente J cuyos nodos de decisión se asignan a los nodos casuales, A1 J,A2 J, en (a).El modelo asigna 0.1 a esa ubicación (ver Fig. 6 (b)). El agente I está indeciso en estos dos modelos de j. Si variamos es la capacidad auditiva y resolvemos el ID de nivel 1 correspondiente expandido en tres pasos de tiempo, obtenemos las políticas de comportamiento normativas que se muestran en la Fig. 7 que exhiben un comportamiento de seguidores. Si es la probabilidad de escuchar correctamente los gruñidos es 0.65, entonces como se muestra en la política en la Fig. 7 (a), comienzo a seguir condicionalmente las acciones de JS: abre la misma puerta que jeinó anteriormente IFF es una evaluación propia de la ubicación de los tigres.confirma JS Pick. Si pierde la capacidad de interpretar correctamente los gruñidos por completo, sigue ciegamente a J y abre la misma puerta que J abrió anteriormente (Fig. 7 (b)). Figura 7: Emergencia de (a) seguidores condicionales, y (b) seguidores ciegos en el problema del tigre. Los comportamientos de interés están en negrita.* es un comodín y denota cualquiera de las observaciones. Observamos que un solo nivel de anidación de creencias, creencias sobre los modelos de otros, era suficiente para que surgiera seguidores en el problema del tigre. Sin embargo, los requisitos epistemológicos para la aparición del liderazgo son más complejos. Para que un agente, digamos J, que emerja como líder, los seguidores primero deben surgir en el otro agente i. Como mencionamos anteriormente, si estoy seguro de que sus preferencias son idénticas a las de J, y cree que J tiene un mejor sentido de audición, seguiré las acciones de JS con el tiempo. El agente J emerge como un líder si cree que lo seguiré, lo que implica que la creencia JS debe estar anidada dos niveles de profundidad para permitirle reconocer su papel de liderazgo. Darme cuenta de que seguiré a los regalos con la oportunidad de influir en las acciones en beneficio del bien colectivo o su interés propio solo. Por ejemplo, en el problema del tigre, consideremos una configuración en la que tanto I y J abren la puerta correcta, cada uno obtiene una recompensa de 20 que es el doble del original. Si J solo selecciona la puerta correcta, obtiene la recompensa de 10. Por otro lado, si ambos agentes eligen la puerta equivocada, sus sanciones se cortan a la mitad. En este entorno, es tanto en el mejor interés de JS como en el mejoramiento colectivo para que J use su experiencia en la selección de la puerta correcta y, por lo tanto, sea un buen líder. Sin embargo, considere un problema ligeramente diferente en el que J gana de la pérdida de IS y se penaliza si gana. Específicamente, deje que se pague de JS, lo que indica que J es antagónico hacia I - si J elige la puerta correcta y yo la que es la pérdida de 100 se convierte en JS. El agente J cree que creo incorrectamente que las preferencias de JS son aquellas que promueven el bien colectivo y que comienza creyendo con un 99% de confianza donde está el tigre. Debido a que cree que sus preferencias son similares a las de J, y que J comienza creyendo casi seguramente que uno de los dos es la ubicación correcta (dos modelos de nivel 0 de J), comenzaré siguiendo las acciones de JS. Mostramos que es una política normativa para resolver su I-Did anidada individual en tres pasos de tiempo en la Fig. 8 (a). La política demuestra que seguiré ciegamente las acciones de JS. Dado que el tigre persiste en su ubicación original con una probabilidad de 0.95, seleccionaré nuevamente la misma puerta. Si J comienza el juego con una probabilidad del 99% de que el tigre esté a la derecha, resolviendo JS I-Did anidada dos niveles de profundidad, da como resultado la política que se muestra en la Fig. 8 (b). Aunque J está casi seguro de que OL es la acción correcta, comenzará seleccionando o, seguido de OL. La intención del agente JS es engañar a I Who Who, cree, seguirá las acciones de JS, a fin de ganar $ 110 en el segundo paso, que es más de lo que J ganaría si fuera honesto. Figura 8: Emergencia del engaño entre los agentes en el problema del tigre. Los comportamientos de interés están en negrita.* denota como antes.(a) El agente es una política que demuestra que seguirá ciegamente las acciones de JS.(b) Aunque J está casi seguro de que el tigre está a la derecha, comenzará seleccionando o, seguido de OL, para engañar I.5.2 Altruismo y reciprocidad En el problema público del bien público El problema del bien público (PG) [7], consiste en un grupo de agentes M, cada uno de los cuales debe contribuir con algunos recursos a una olla pública o mantenerlo por sí mismos. Dado que los recursos contribuidos a la olla pública se comparten entre todos los agentes, son menos valiosos para el agente cuando están en la olla pública. Sin embargo, si todos los agentes eligen contribuir con sus recursos, entonces la recompensa a cada agente es más que si nadie contribuye. Dado que un agente obtiene su parte de la olla pública, independientemente de si ha contribuido o no, la acción dominante es que cada agente no contribuya y, en cambio, sea un viaje gratuito en las contribuciones de otros. Sin embargo, los comportamientos de los jugadores humanos en las simulaciones empíricas del problema PG difieren de las predicciones normativas. Los experimentos revelan que muchos jugadores inicialmente contribuyen una gran cantidad a la olla pública y continúan contribuyendo cuando el problema de PG se juega repetidamente, aunque en cantidades decrecientes [4]. Muchos de estos experimentos [5] informan que un pequeño grupo central de jugadores contribuye persistentemente a la olla pública, incluso cuando todos los demás están deserviendo. Estos experimentos también revelan que los jugadores que contribuyen persistentemente tienen preferencias altruistas o recíprocas que coinciden con la cooperación esperada de otros. Para simplificar, suponemos que el juego se juega entre los agentes M = 2, I y J. Deje que cada agente esté inicialmente dotado de la cantidad de recursos XT. Si bien la formulación clásica del juego PG permite que cada agente contribuya con cualquier cantidad de recursos (≤ XT) al bote público, simplificamos el espacio de acción al permitir dos posibles acciones. Cada agente puede optar por contribuir (c) una cantidad fija de los recursos o no contribuir. La última acción es Dethe Sixth Intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 819 señalados como defecto (D). Suponemos que las acciones no son observables para otros. El valor de los recursos en el bote público es descontado por CI para cada agente I, donde CI es el retorno privado marginal. Suponemos que CI <1 para que el agente no se beneficie lo suficiente como para que contribuya a la olla pública para obtener ganancias privadas. Simultáneamente, CIM> 1, haciendo que la contribución colectiva Pareto sea óptima.I/J C D C 2Cixt, 2CJXT CIXT-CP, XT + CJXT-P D XT + CIXT-P, CJXT-CP XT, XX Tabla 1: El juego PG de un disparo con castigo. Para fomentar las contribuciones, los agentes contribuyentes castigan a los pasajeros libres pero incurren en un pequeño costo por administrar el castigo. Sea P el castigo impartido al agente de defectos y CP el costo no cero de castigar al agente contribuyente. Para simplificar, suponemos que el costo de castigar es el mismo para ambos agentes. El juego PG de un solo disparo con castigo se muestra en la mesa.1. Deje Ci = CJ, CP> 0, y si P> Xt - CIXT, entonces la deserción ya no es una acción dominante. Si p <xt - cixt, entonces la deserción es la acción dominante para ambos. Si P = Xt-CIXT, entonces el juego no es solucionable de dominio. Figura 9: (a) Nivel 1 I-ID del Agente I, (b) IDS de nivel 0 del Agente J con nodos de decisión asignados a los nodos casuales, A1 J y A2 J, en (A). Formulamos una versión secuencial del problema PG con el castigo desde la perspectiva del agente i. Aunque en el juego PG repetido, la cantidad en el bote público se revela a todos los agentes después de cada ronda de acciones, asumimos en nuestra formulación que está oculta a los agentes. Cada agente puede contribuir con una cantidad fija, XC o defecto. Un agente sobre la realización de una acción recibe una observación de abundancia (py) o escasa (MR) que simboliza el estado de la olla pública. Observe que las observaciones también son indirectamente indicativas de las acciones del agente JS porque el estado de la olla pública está influenciado por ellas. La cantidad de recursos en el agente es la olla privada, es perfectamente observable para i. Los pagos son análogos a la mesa.1. Tomando prestados de las investigaciones empíricas del problema PG [5], construimos ID de nivel 0 para J que modelan tipos altruistas y no altruistas (Fig. 9 (b)). Específicamente, nuestro agente altruista tiene un alto rendimiento privado marginal (CJ está cerca de 1) y no castiga a otros que defectan. Deje que XC = 1 y el agente de nivel 0 se castigan a la mitad de las veces que defecta. Con una acción restante, ambos tipos de agentes eligen contribuir a evitar ser castigados. Con dos acciones por recorrer, el tipo altruista elige contribuir, mientras que los otros defectos. Esto se debe a que CJ para el tipo altruista está cerca de 1, por lo tanto, el castigo esperado, 0.5p> (1 - CJ), que el tipo altruista evita. Debido a que CJ para el tipo no altruista es menor, prefiere no contribuir. A tres pasos por recorrer, el agente altruista contribuye a evitar el castigo (0.5p> 2 (1-CJ)) y los defectos de tipo no altruista. Para más de tres pasos, mientras que el agente altruista continúa contribuyendo a la olla pública dependiendo de qué tan cerca esté su rendimiento privado marginal a 1, el tipo no altruista prescribe la deserción. Analizamos las decisiones de un agente altruista que modelé utilizando un I-Did de nivel 1 expandido en 3 pasos de tiempo.Afirmé los dos modelos de nivel 0, mencionados anteriormente, a J (ver Fig. 9). Si cree con una probabilidad 1 que J es altruista, elige contribuir para cada uno de los tres pasos. Este comportamiento persiste cuando no tengo conocimiento de si J es altruista (Fig. 10 (a)), y cuando asigno una alta probabilidad de que J sea el tipo no altruista. Sin embargo, cuando cree con una probabilidad 1 de que J no es altruista y, por lo tanto, seguramente desertará, elige desertar para evitar ser castigado y porque su retorno privado marginal es inferior a 1. Estos resultados demuestran que el comportamiento de nuestro tipo altruista se asemeja a que se encuentran experimentalmente. El agente de nivel 1 no altruista elige desertar independientemente de la probabilidad de que el otro agente sea altruista. Analizamos el comportamiento de un tipo de agente recíproco que coincide con la cooperación o deserción esperadas. El rendimiento privado marginal de los tipos recíprocos es similar al del tipo no altruista, sin embargo, obtiene una mayor recompensa cuando su acción es similar a la del otro. Consideramos el caso cuando el agente recíproco I no está seguro de si J es altruista y cree que es probable que la olla pública esté medio llena. Para esta creencia previa, elijo desertar. Al recibir una observación de abundancia, decide contribuir, mientras que una observación de escasez lo hace defecto (Fig. 10 (b)). Esto se debe a que una observación de abundancia señala que es probable que el bote esté más de medio lleno, lo que resulta de la acción JS para contribuir. Por lo tanto, entre los dos modelos atribuidos a J, es probable que su tipo sea altruista, lo que es probable que J contribuya nuevamente en el próximo paso de tiempo. Por lo tanto, el agente I elige contribuir a la acción de reciprocar JS. Un razonamiento análogo lleva a I para desertar cuando observa una olla escasa. Con una acción por recorrer, creo que J contribuye, elegirá contribuir también para evitar el castigo independientemente de sus observaciones. Figura 10: (a) Un agente altruista de nivel 1 siempre contribuye.(b) Un agente recíproco I comienza por defecto seguido de la elección de contribuir o defectos en función de su observación de abundancia (lo que indica que J es probable que sea altruista) o escaso (J no es altruista).5.3 Estrategias en el póker de póker de dos jugadores es un popular juego de cartas de suma cero que ha recibido mucha atención entre la comunidad de investigación de inteligencia artificial como un testeble [2]. El póker se juega entre M ≥ 2 jugadores en los que cada jugador recibe una mano de cartas de un mazo. Si bien existen varios sabores de póker con una complejidad variable, consideramos una versión simple en la que cada jugador tiene tres capas durante las cuales el jugador puede intercambiar una carta (e), mantener la mano existente (k), pliegue (f) y retirarse deEl juego, o llamar (c), requerir que todos los jugadores muestren sus manos. Para mantener las cosas simples, deje que M = 2, y cada jugador reciba una mano que consiste en una sola carta extraída del mismo traje. Por lo tanto, durante un enfrentamiento, el jugador que tiene la carta numéricamente más grande (2 es la más baja, ACE es la más alta) gana la olla. Durante un intercambio de tarjetas, la tarjeta desechada se coloca en la pila L, lo que indica al otro agente que era una tarjeta de bajo número de 8 años, o en el 820 el sexto INTL. Conf.en agentes autónomos y sistemas de agentes múltiples (AAMAS 07) H Pila, lo que indica que la tarjeta tenía un rango mayor o igual a 8. Observe que, por ejemplo, si se descarta una tarjeta numerada más baja, ahora se reduce la probabilidad de recibir una tarjeta baja a cambio. Mostramos el ID de Nivel 1 para el póker simplificado de dos jugadores en la figura 11. Consideramos dos modelos (tipos de personalidad) del agente j. El tipo conservador cree que es probable que su oponente tenga una tarjeta numerada en su mano. Por otro lado, el agente agresivo J cree con una alta probabilidad de que su oponente tenga una carta de menor número. Por lo tanto, los dos tipos difieren en sus creencias sobre sus oponentes. En ambos modelos de nivel 0, se supone que el oponente realiza sus acciones después de una distribución fija y uniforme. Con tres acciones por recorrer, independientemente de su mano (a menos que sea un as), el agente agresivo elige intercambiar su tarjeta, con la intención de mejorar su mano actual. Esto se debe a que cree que el otro tiene una tarjeta baja, lo que mejora sus posibilidades de obtener una tarjeta alta durante el intercambio. El agente conservador elige mantener su tarjeta, sin importar su mano porque sus posibilidades de obtener una tarjeta alta son escasas, ya que cree que su oponente tiene una. Figura 11: (a) Nivel 1 I-ID del agente i. La observación revela información sobre JS Mano del paso de tiempo anterior, (b) ID de nivel 0 del Agente J cuyos nodos de decisión se asignan a los nodos casuales, A1 J, A2 J, en (A). La política de un agente de Nivel 1 que cree que cada carta, excepto la suya, tiene la misma probabilidad de estar en JS Hand (tipo de personalidad neutral) y J podría ser un tipo agresivo o conservador, se muestra en la Fig. 12. Es propio.La mano contiene la tarjeta numerada 8. El agente comienza manteniendo su tarjeta. Al ver que J no intercambió una tarjeta (N), cree con la probabilidad 1 de que J es conservador y, por lo tanto, mantendrá sus cartas.Responde manteniendo su tarjeta o intercambiándola porque J es igualmente probable que tenga una tarjeta más baja o superior. Si observo que J desechó su tarjeta en la pila L o H, creo que J es agresivo. Al observar L, me doy cuenta de que J tenía una tarjeta baja y es probable que tenga una tarjeta alta después de su intercambio. Debido a que la probabilidad de recibir una tarjeta baja es alta ahora, elige mantener su tarjeta. Al observar H, creyendo que la probabilidad de recibir una tarjeta numerada es alta, elige intercambiar su tarjeta. En el paso final, elige llamar independientemente de su historial de observación porque su creencia de que J tiene una tarjeta más alta no es lo suficientemente alta como para concluir que es mejor doblar y renunciar a la recompensa. Esto se debe en parte al hecho de que una observación de, por ejemplo, restablecer el agente son las creencias de pasos de tiempo anteriores sobre JS mano a las tarjetas con numeradas bajas solamente.6. Discusión Mostramos cómo los DIDS pueden extenderse a I-DID que permiten la toma de decisiones secuenciales en línea en entornos múltiples inciertos. Nuestra representación gráfica de I-DID mejora en la Figura 12 anterior: un agente de nivel 1 es una política de tres pasos en el problema de póker.Comienzo creyendo que J es igualmente probable que sea agresivo o conservador y podría tener cualquier tarjeta en su mano con igual probabilidad.Trabaje significativamente por ser más transparente, semánticamente claro y capaz de resolverse utilizando algoritmos estándar que el objetivo es DID. Los I-DID extienden NIDS para permitir la toma de decisiones secuenciales en múltiples pasos de tiempo en presencia de otros agentes interactivos. Los I-DID pueden verse como representaciones gráficas concisas para IPOMDP que proporcionan una forma de explotar la estructura del problema y llevar a cabo la toma de decisiones en línea como el agente actúa y observa dadas sus creencias previas. Actualmente estamos investigando formas de resolver I-Dids aproximadamente con límites comprobables en la calidad de la solución. Reconocimiento: Agradecemos a Piotr Gmytrasiewicz por algunas discusiones útiles relacionadas con este trabajo. El primer autor desea reconocer el apoyo de una subvención de Ugarf.7. Referencias [1] R. J. Aumann. Epistemología interactiva I: Conocimiento. International Journal of Game Theory, 28: 263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer y D. Szafron. El desafío del póker. AIJ, 2001. [3] A. Brandenburger y E. Dekel. Jerarquías de creencias y conocimiento común. Journal of Economic Theory, 59: 189-198, 1993. [4] C. Camerer. Teoría del juego conductual: experimentos en la interacción estratégica. Princeton University Press, 2003. [5] E. Fehr y S. Gachter. Cooperación y castigo en experimentos de bienes públicos. American Economic Review, 90 (4): 980-994, 2000. [6] D. Fudenberg y D. K. Levine. La teoría del aprendizaje en los juegos. MIT Press, 1998. [7] D. Fudenberg y J. Tirole. Teoría de juego. MIT Press, 1991. [8] Y. Gal y A. Pfeffer. Un idioma para modelar procesos de toma de decisiones de los agentes de modelado en los juegos. En Aamas, 2003. [9] P. Gmytrasiewicz y P. Doshi. Un marco para la planificación secuencial en entornos multiagentes. Jair, 24: 49-79, 2005. [10] P. Gmytrasiewicz y E. Durfee. Coordinación racional en entornos de múltiples agentes. Jaamas, 3 (4): 319-350, 2000. [11] J. C. Harsanyi. Juegos con información incompleta jugadas por jugadores bayesianos. Management Science, 14 (3): 159-182, 1967. [12] R. A. Howard y J. E. Matheson. Diagramas de influencia. En R. A. Howard y J. E. Matheson, editores, los principios y aplicaciones del análisis de decisiones. Grupo de decisiones estratégicas, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman y A. Cassandra. Planificación y actuación en dominios estocásticos parcialmente observables. Journal de inteligencia artificial, 2, 1998. [14] D. Koller y B. Milch. Diagramas de influencia de múltiples agentes para representar y resolver juegos. En IJCAI, páginas 1027-1034, 2001. [15] K. Polich y P. Gmytrasiewicz. Diagramas de influencia dinámica interactiva. En GTDT Workshop, Aamas, 2006. [16] B. Rathnas., P. Doshi y P. J. Gmytrasiewicz. Soluciones exactas para POMDP interactivos utilizando equivalencia de comportamiento. En Agentes Autónomos y Conferencia de Sistemas de Aguos Multi-Agentes (AAMAS), 2006. [17] S. Russell y P. Norvig. Inteligencia artificial: un enfoque moderno (segunda edición). Prentice Hall, 2003. [18] R. D. Shachter. Evaluar los diagramas de influencia. Operations Research, 34 (6): 871-882, 1986. [19] D. Suryadi y P. Gmytrasiewicz. Modelos de aprendizaje de otros agentes que utilizan diagramas de influencia. En Um, 1999. El sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 821",
    "original_sentences": [
        "Graphical Models for Online Solutions to Interactive POMDPs Prashant Doshi Dept.",
        "of Computer Science University of Georgia Athens, GA 30602, USA pdoshi@cs.uga.edu Yifeng Zeng Dept. of Computer Science Aalborg University DK-9220 Aalborg, Denmark yfzeng@cs.aau.edu Qiongyu Chen Dept. of Computer Science National Univ. of Singapore 117543, Singapore chenqy@comp.nus.edu.sg ABSTRACT We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation.",
        "These graphical models called interactive dynamic influence diagrams (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
        "I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs.",
        "I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents.",
        "Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness.",
        "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Theory 1.",
        "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in partially observable multiagent environments.",
        "They generalize POMDPs [13] to multiagent settings by including the other agents computable models in the state space along with the states of the physical environment.",
        "The models encompass all information influencing the agents behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11].",
        "I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a decision-makers perspective in the interaction.",
        "In [15], Polich and Gmytrasiewicz introduced interactive dynamic influence diagrams (I-DIDs) as the computational representations of I-POMDPs.",
        "I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs.",
        "I-DIDs contribute to a growing line of work [19] that includes multi-agent influence diagrams (MAIDs) [14], and more recently, networks of influence diagrams (NIDs) [8].",
        "These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
        "MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agents actions and chance nodes capturing the agents private information.",
        "MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the independence structure.",
        "NIDs extend MAIDs to include agents uncertainty over the game being played and over models of the other agents.",
        "Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent.",
        "Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently.",
        "However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games.",
        "Matters are more complex when we consider interactions that are extended over time, where predictions about others future actions must be made using models that change as the agents act and observe.",
        "I-DIDs address this gap by allowing the representation of other agents models as the values of a special model node.",
        "Both, other agents models and the original agents beliefs over these models are updated over time using special-purpose implementations.",
        "In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID.",
        "Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently.",
        "Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I-DID by [15], and show that it could be replaced by traditional dependency links.",
        "In the previous representation of the I-DID, the update of the agents belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time.",
        "We explicate the semantics of this link by showing how it can be implemented using the traditional dependency links between the chance nodes that constitute the model nodes.",
        "The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs.",
        "We show how IDIDs may be used to model an agents uncertainty over others models, that may themselves be I-DIDs.",
        "Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others models.",
        "Analogous to DIDs, I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. 2.",
        "BACKGROUND: FINITELY NESTED IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents models as part of the state space [9].",
        "Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents models and their beliefs about others.",
        "For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j.",
        "A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment.",
        "Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj .",
        "Here, j is Bayes rational and OCj is js optimality criterion.",
        "SMj is the set of subintentional models of j.",
        "Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent.",
        "We give a recursive bottom-up construction of the interactive state space below.",
        "ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . .",
        "ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent is preferences over its interactive states.",
        "Usually only the physical states will matter.",
        "Agent is policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i.",
        "Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai). 2.1 Belief Update Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes.",
        "However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones.",
        "First, since the state of the physical environment depends on the actions of both agents, is prediction of how the physical state changes has to be made based on its prediction of js actions.",
        "Second, changes in js models have to be included in is belief update.",
        "Specifically, if j is intentional then an update of js beliefs due to its action and observation has to be included.",
        "In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief.",
        "If js model is subintentional, then js probable observations are appended to the observation history contained in the model.",
        "Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update.",
        "For a version of the belief update when js model is subintentional, see [9].",
        "If agent j is also modeled as an I-POMDP, then is belief update invokes js belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke is belief update and so on.",
        "This recursion in belief nesting bottoms out at the 0th level.",
        "At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9]. 2.2 Value Iteration Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)).",
        "Eq. 2 is a basis for value iteration in I-POMDPs.",
        "Agent is optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3.",
        "INTERACTIVEINFLUENCEDIAGRAMS A naive extension of influence diagrams (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes.",
        "However, this approach assumes that the agents actions are controlled using a probability distribution that does not change over time.",
        "Interactive influence diagrams (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node.",
        "We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon.",
        "We note that the probability distribution over the chance node, S, and the model node together represents agent is belief over its interactive states.",
        "In addition to the model 1 The 0th level model is a POMDP: Other agents actions are treated as exogenous events and folded into the T, O, and R functions.",
        "The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j.",
        "The hexagon is the model node (Mj,l−1) whose structure we show in (b).",
        "Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ).",
        "Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the policy link in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model.",
        "In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs.",
        "The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2.",
        "Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional.",
        "Because the model node contains the alternative models of the other agent as its values, its representation is not trivial.",
        "In particular, some of the models within the node are I-IDs that when solved generate the agents optimal policy in their decision nodes.",
        "Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise.",
        "Borrowing insights from previous work [8], we observe that the model node and the dashed policy link that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b).",
        "The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
        "The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj.",
        "Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent is beliefs.",
        "The conditional probability table of the chance node, Aj, is a multiplexer that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj].",
        "The values of Mod[Mj] denote the different models of j.",
        "In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1.",
        "The distribution over the node, Mod[Mj], is the agent is belief over the models of j given a physical state.",
        "For more agents, we will have as many model nodes as there are agents.",
        "Notice that Fig. 1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional dependency links.",
        "In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them.",
        "In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes.",
        "This allows I-IDs to be represented and implemented using conventional application tools that target IDs.",
        "Note that we may view the level l I-ID as a NID.",
        "Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2).",
        "If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID.",
        "Note that within the I-IDs (or IDs) at each level, there is only a single decision node.",
        "Thus, our NID does not contain any MAIDs.",
        "Figure 2: A level l I-ID represented as a NID.",
        "The probabilities assigned to the blocks of the NID are is beliefs over js models conditioned on a physical state. 3.2 Solution The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively.",
        "We start by solving the level 0 models, which, if intentional, are traditional IDs.",
        "Their solutions provide probability distributions over the other agents actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID.",
        "The mapping from the level 0 models decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
        "Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c).",
        "During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj].",
        "As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent is belief over the models of j conditioned on the physical state.",
        "The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j.",
        "Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18].",
        "This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief.",
        "Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agents current belief is known. 4.",
        "INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic influence diagrams (I-DIDs) extend I-IDs (and NIDs) to allow sequential decision-making over several time steps.",
        "Just as DIDs are structured graphical representations of POMDPs, I-DIDs are the graphical online analogs for finitely nested I-POMDPs.",
        "I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents. 4.1 Syntax We depict a general two time-slice I-DID in Fig. 3(a).",
        "In addition to the model nodes and the dashed policy link, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a).",
        "We explained the semantics of the model node and the policy link in the previous section; we describe the model updates next.",
        "The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1.",
        "Recall from Section 2 that an agents intentional model includes its belief.",
        "Because the agents act and receive observations, their models are updated to reflect their changed beliefs.",
        "Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models.",
        "Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models.",
        "Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model.",
        "These steps are a part of agent is belief update formalized using Eq. 1.",
        "In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID.",
        "If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ).",
        "These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation.",
        "The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously.",
        "Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed.",
        "The probability that js updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node.",
        "In order to obtain the probability of js possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ].",
        "Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a multiplexer modulated by Mod[Mt j ].",
        "Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1.",
        "Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1.",
        "Notice that the model update link may be replaced by the dependency links between the chance nodes that constitute the model nodes in the two time slices.",
        "In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them.",
        "Chance nodes and dependency links that not in bold are standard, usually found in DIDs.",
        "Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents js observation at time t + 1.",
        "The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links.",
        "We note that the possible set of models of the other agent j grows exponentially with the number of time steps.",
        "For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node. 4.2 Solution Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively.",
        "For the purpose of illustration, let l=1 and T=2.",
        "The solution method uses the standard look-ahead technique, projecting the agents action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step.",
        "Because agent i has a belief over js models as well, the lookahead includes finding out the possible models that j could have in the future.",
        "Consequently, each of js subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions.",
        "These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b).",
        "We note the recursive nature of this solution: in solving agent is level 1 I-DID, js level 0 DIDs must be solved.",
        "If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner.",
        "We briefly outline the recursive algorithm for solving agent is Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase 1.",
        "For t from 1 to T − 1 do 2.",
        "If l ≥ 1 then Populate Mt+1 j,l−1 3.",
        "For each mt j in Range(Mt j,l−1) do 4.",
        "Recursively call algorithm with the l − 1 I-ID (or ID) that represents mt j and the horizon, T − t + 1 5.",
        "Map the decision node of the solved I-ID (or ID), OPT(mt j), to a chance node Aj 6.",
        "For each aj in OPT(mt j) do 7.",
        "For each oj in Oj (part of mt j) do 8.",
        "Update js belief, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← New I-ID (or ID) with bt+1 j as the initial belief 10.",
        "Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11.",
        "Add the model node, Mt+1 j,l−1, and the dependency links between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b)) 12.",
        "Add the chance, decision, and utility nodes for t + 1 time slice and the dependency links between them 13.",
        "Establish the CPTs for each chance node and utility node Look-Ahead Phase 14.",
        "Apply the standard look-ahead and backup method to solve the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5.",
        "We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the dependency links and the conditional probability tables for each node.",
        "We particularly focus on establishing and populating the model nodes (lines 3-11).",
        "Note that Range(·) returns the values (lower level models) of the random variable given as input (model node).",
        "In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs.",
        "Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents.",
        "As we mentioned previously, the 0-th level models are the traditional DIDs.",
        "Their solutions provide probability distributions over actions of the agent modeled at that level to I-DIDs at level 1.",
        "Given probability distributions over other agents actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models.",
        "Assume that the number of models considered at each level is bound by a number, M. Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs. 5.",
        "EXAMPLE APPLICATIONS To illustrate the usefulness of I-DIDs, we apply them to three problem domains.",
        "We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it. 5.1 Followership-Leadership in the Multiagent Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9].",
        "The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L).",
        "In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agents opening one of the doors.",
        "When any door is opened, the tiger persists in its original location with a probability of 95%.",
        "Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%.",
        "Agent j, on the other hand, hears growls with a reliability of 95%.",
        "Thus, the setting is such that agent i hears agent j opening doors more reliably than the tigers growls.",
        "This suggests that i could use js actions as an indication of the location of the tiger, as we discuss below.",
        "Each agents preferences are as in the single agent game discussed in [13].",
        "The transition, observation, and reward functions are shown in [16].",
        "A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions.",
        "In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship.",
        "In particular, we analyze the situational and epistemological conditions sufficient for their emergence.",
        "The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the others actions in order to maximize its payoffs.",
        "Let us consider a particular setting of the tiger problem in which agent i believes that js preferences are aligned with its own - both of them just want to get the gold - and js hearing is more reliable in comparison to itself.",
        "As an example, suppose that j, on listening can discern the tigers location 95% of the times compared to is 65% accuracy.",
        "Additionally, agent i does not have any initial information about the tigers location.",
        "In other words, is single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger.",
        "In addition, i considers two models of j, which differ in js flat level 0 initial beliefs.",
        "This is represented in the level 1 I-ID shown in Fig. 6(a).",
        "According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)).",
        "Agent i is undecided on these two models of j.",
        "If we vary is hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior.",
        "If is probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow js actions: i opens the same door that j opened previously iff is own assessment of the tigers location confirms js pick.",
        "If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)).",
        "Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem.",
        "Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations.",
        "We observed that a single level of belief nesting - beliefs about the others models - was sufficient for followership to emerge in the tiger problem.",
        "However, the epistemological requirements for the emergence of leadership are more complex.",
        "For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i.",
        "As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow js actions over time.",
        "Agent j emerges as a leader if it believes that i will follow it, which implies that js belief must be nested two levels deep to enable it to recognize its leadership role.",
        "Realizing that i will follow presents j with an opportunity to influence is actions in the benefit of the collective good or its self-interest alone.",
        "For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original.",
        "If j alone selects the correct door, it gets the payoff of 10.",
        "On the other hand, if both agents pick the wrong door, their penalties are cut in half.",
        "In this setting, it is in both js best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader.",
        "However, consider a slightly different problem in which j gains from is loss and is penalized if i gains.",
        "Specifically, let is payoff be subtracted from js, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then is loss of 100 becomes js gain.",
        "Agent j believes that i incorrectly thinks that js preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is.",
        "Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following js actions.",
        "We show is normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a).",
        "The policy demonstrates that i will blindly follow js actions.",
        "Since the tiger persists in its original location with a probability of 0.95, i will select the same door again.",
        "If j begins the game with a 99% probability that the tiger is on the right, solving js I-DID nested two levels deep, results in the policy shown in Fig. 8(b).",
        "Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL.",
        "Agent js intention is to deceive i who, it believes, will follow js actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest.",
        "Figure 8: Emergence of deception between agents in the tiger problem.",
        "Behaviors of interest are in bold. * denotes as before. (a) Agent is policy demonstrating that it will blindly follow js actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i. 5.2 Altruism and Reciprocity in the Public Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves.",
        "Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot.",
        "However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes.",
        "Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others contributions.",
        "However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions.",
        "The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4].",
        "Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting.",
        "These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others.",
        "For simplicity, we assume that the game is played between M = 2 agents, i and j.",
        "Let each agent be initially endowed with XT amount of resources.",
        "While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions.",
        "Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute.",
        "The latter action is deThe Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D).",
        "We assume that the actions are not observable to others.",
        "The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return.",
        "We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain.",
        "Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment.",
        "In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment.",
        "Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent.",
        "For simplicity, we assume that the cost of punishing is same for both the agents.",
        "The one-shot PG game with punishment is shown in Table. 1.",
        "Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action.",
        "If P < XT − ciXT , then defection is the dominating action for both.",
        "If P = XT − ciXT , then the game is not dominance-solvable.",
        "Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a).",
        "We formulate a sequential version of the PG problem with punishment from the perspective of agent i.",
        "Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents.",
        "Each agent may contribute a fixed amount, xc, or defect.",
        "An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot.",
        "Notice that the observations are also indirectly indicative of agent js actions because the state of the public pot is influenced by them.",
        "The amount of resources in agent is private pot, is perfectly observable to i.",
        "The payoffs are analogous to Table. 1.",
        "Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)).",
        "Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect.",
        "Let xc = 1 and the level 0 agent be punished half the times it defects.",
        "With one action remaining, both types of agents choose to contribute to avoid being punished.",
        "With two actions to go, the altruistic type chooses to contribute, while the other defects.",
        "This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids.",
        "Because cj for the non-altruistic type is less, it prefers not to contribute.",
        "With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects.",
        "For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection.",
        "We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level 0 models, mentioned previously, to j (see Fig. 9).",
        "If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps.",
        "This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type.",
        "However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1.",
        "These results demonstrate that the behavior of our altruistic type resembles that found experimentally.",
        "The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic.",
        "We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection.",
        "The reciprocal types marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other.",
        "We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full.",
        "For this prior belief, i chooses to defect.",
        "On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)).",
        "This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from js action to contribute.",
        "Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step.",
        "Agent i therefore chooses to contribute to reciprocate js action.",
        "An analogous reasoning leads i to defect when it observes a meager pot.",
        "With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations.",
        "Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic). 5.3 Strategies in Two-Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2].",
        "Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck.",
        "While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands.",
        "To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit.",
        "Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot.",
        "During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to 8.",
        "Notice that, for example, if a lower numbered card is discarded, the probability of receiving a low card in exchange is now reduced.",
        "We show the level 1 I-ID for the simplified two-player Poker in Fig. 11.",
        "We considered two models (personality types) of agent j.",
        "The conservative type believes that it is likely that its opponent has a high numbered card in its hand.",
        "On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card.",
        "Thus, the two types differ in their beliefs over their opponents hand.",
        "In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution.",
        "With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand.",
        "This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange.",
        "The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one.",
        "Figure 11: (a) Level 1 I-ID of agent i.",
        "The observation reveals information about js hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a).",
        "The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in js hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. is own hand contains the card numbered 8.",
        "The agent starts by keeping its card.",
        "On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card.",
        "If i observes that j discarded its card into the L or H pile, i believes that j is aggressive.",
        "On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange.",
        "Because the probability of receiving a low card is high now, i chooses to keep its card.",
        "On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card.",
        "In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff.",
        "This is partly due to the fact that an observation of, say, L resets the agent is previous time step beliefs over js hand to the low numbered cards only. 6.",
        "DISCUSSION We showed how DIDs may be extended to I-DIDs that enable online sequential decision-making in uncertain multiagent settings.",
        "Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent is three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs.",
        "I-DIDs extend NIDs to allow sequential decision-making over multiple time steps in the presence of other interacting agents.",
        "I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs.",
        "We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality.",
        "Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work.",
        "The first author would like to acknowledge the support of a UGARF grant. 7.",
        "REFERENCES [1] R. J. Aumann.",
        "Interactive epistemology i: Knowledge.",
        "International Journal of Game Theory, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron.",
        "The challenge of poker.",
        "AIJ, 2001. [3] A. Brandenburger and E. Dekel.",
        "Hierarchies of beliefs and common knowledge.",
        "Journal of Economic Theory, 59:189-198, 1993. [4] C. Camerer.",
        "Behavioral Game Theory: Experiments in Strategic Interaction.",
        "Princeton University Press, 2003. [5] E. Fehr and S. Gachter.",
        "Cooperation and punishment in public goods experiments.",
        "American Economic Review, 90(4):980-994, 2000. [6] D. Fudenberg and D. K. Levine.",
        "The Theory of Learning in Games.",
        "MIT Press, 1998. [7] D. Fudenberg and J. Tirole.",
        "Game Theory.",
        "MIT Press, 1991. [8] Y. Gal and A. Pfeffer.",
        "A language for modeling agents decision-making processes in games.",
        "In AAMAS, 2003. [9] P. Gmytrasiewicz and P. Doshi.",
        "A framework for sequential planning in multiagent settings.",
        "JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz and E. Durfee.",
        "Rational coordination in multi-agent environments.",
        "JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi.",
        "Games with incomplete information played by bayesian players.",
        "Management Science, 14(3):159-182, 1967. [12] R. A. Howard and J. E. Matheson.",
        "Influence diagrams.",
        "In R. A. Howard and J. E. Matheson, editors, The Principles and Applications of Decision Analysis.",
        "Strategic Decisions Group, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman, and A. Cassandra.",
        "Planning and acting in partially observable stochastic domains.",
        "Artificial Intelligence Journal, 2, 1998. [14] D. Koller and B. Milch.",
        "Multi-agent influence diagrams for representing and solving games.",
        "In IJCAI, pages 1027-1034, 2001. [15] K. Polich and P. Gmytrasiewicz.",
        "Interactive dynamic influence diagrams.",
        "In GTDT Workshop, AAMAS, 2006. [16] B.",
        "Rathnas., P. Doshi, and P. J. Gmytrasiewicz.",
        "Exact solutions to interactive pomdps using behavioral equivalence.",
        "In Autonomous Agents and Multi-Agent Systems Conference (AAMAS), 2006. [17] S. Russell and P. Norvig.",
        "Artificial Intelligence: A Modern Approach (Second Edition).",
        "Prentice Hall, 2003. [18] R. D. Shachter.",
        "Evaluating influence diagrams.",
        "Operations Research, 34(6):871-882, 1986. [19] D. Suryadi and P. Gmytrasiewicz.",
        "Learning models of other agents using influence diagrams.",
        "In UM, 1999.",
        "The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821"
    ],
    "error_count": 0,
    "keys": {
        "graphical model": {
            "translated_key": "modelo gráfico",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Graphical Models for Online Solutions to Interactive POMDPs Prashant Doshi Dept.",
                "of Computer Science University of Georgia Athens, GA 30602, USA pdoshi@cs.uga.edu Yifeng Zeng Dept. of Computer Science Aalborg University DK-9220 Aalborg, Denmark yfzeng@cs.aau.edu Qiongyu Chen Dept. of Computer Science National Univ. of Singapore 117543, Singapore chenqy@comp.nus.edu.sg ABSTRACT We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation.",
                "These <br>graphical model</br>s called interactive dynamic influence diagrams (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents.",
                "Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Theory 1.",
                "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in partially observable multiagent environments.",
                "They generalize POMDPs [13] to multiagent settings by including the other agents computable models in the state space along with the states of the physical environment.",
                "The models encompass all information influencing the agents behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11].",
                "I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a decision-makers perspective in the interaction.",
                "In [15], Polich and Gmytrasiewicz introduced interactive dynamic influence diagrams (I-DIDs) as the computational representations of I-POMDPs.",
                "I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs contribute to a growing line of work [19] that includes multi-agent influence diagrams (MAIDs) [14], and more recently, networks of influence diagrams (NIDs) [8].",
                "These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agents actions and chance nodes capturing the agents private information.",
                "MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the independence structure.",
                "NIDs extend MAIDs to include agents uncertainty over the game being played and over models of the other agents.",
                "Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent.",
                "Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently.",
                "However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games.",
                "Matters are more complex when we consider interactions that are extended over time, where predictions about others future actions must be made using models that change as the agents act and observe.",
                "I-DIDs address this gap by allowing the representation of other agents models as the values of a special model node.",
                "Both, other agents models and the original agents beliefs over these models are updated over time using special-purpose implementations.",
                "In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID.",
                "Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently.",
                "Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I-DID by [15], and show that it could be replaced by traditional dependency links.",
                "In the previous representation of the I-DID, the update of the agents belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time.",
                "We explicate the semantics of this link by showing how it can be implemented using the traditional dependency links between the chance nodes that constitute the model nodes.",
                "The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs.",
                "We show how IDIDs may be used to model an agents uncertainty over others models, that may themselves be I-DIDs.",
                "Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others models.",
                "Analogous to DIDs, I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. 2.",
                "BACKGROUND: FINITELY NESTED IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents models as part of the state space [9].",
                "Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents models and their beliefs about others.",
                "For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j.",
                "A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment.",
                "Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj .",
                "Here, j is Bayes rational and OCj is js optimality criterion.",
                "SMj is the set of subintentional models of j.",
                "Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent.",
                "We give a recursive bottom-up construction of the interactive state space below.",
                "ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . .",
                "ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent is preferences over its interactive states.",
                "Usually only the physical states will matter.",
                "Agent is policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i.",
                "Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai). 2.1 Belief Update Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes.",
                "However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones.",
                "First, since the state of the physical environment depends on the actions of both agents, is prediction of how the physical state changes has to be made based on its prediction of js actions.",
                "Second, changes in js models have to be included in is belief update.",
                "Specifically, if j is intentional then an update of js beliefs due to its action and observation has to be included.",
                "In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief.",
                "If js model is subintentional, then js probable observations are appended to the observation history contained in the model.",
                "Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update.",
                "For a version of the belief update when js model is subintentional, see [9].",
                "If agent j is also modeled as an I-POMDP, then is belief update invokes js belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke is belief update and so on.",
                "This recursion in belief nesting bottoms out at the 0th level.",
                "At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9]. 2.2 Value Iteration Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)).",
                "Eq. 2 is a basis for value iteration in I-POMDPs.",
                "Agent is optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3.",
                "INTERACTIVEINFLUENCEDIAGRAMS A naive extension of influence diagrams (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes.",
                "However, this approach assumes that the agents actions are controlled using a probability distribution that does not change over time.",
                "Interactive influence diagrams (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node.",
                "We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon.",
                "We note that the probability distribution over the chance node, S, and the model node together represents agent is belief over its interactive states.",
                "In addition to the model 1 The 0th level model is a POMDP: Other agents actions are treated as exogenous events and folded into the T, O, and R functions.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j.",
                "The hexagon is the model node (Mj,l−1) whose structure we show in (b).",
                "Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ).",
                "Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the policy link in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model.",
                "In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs.",
                "The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2.",
                "Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional.",
                "Because the model node contains the alternative models of the other agent as its values, its representation is not trivial.",
                "In particular, some of the models within the node are I-IDs that when solved generate the agents optimal policy in their decision nodes.",
                "Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise.",
                "Borrowing insights from previous work [8], we observe that the model node and the dashed policy link that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b).",
                "The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj.",
                "Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent is beliefs.",
                "The conditional probability table of the chance node, Aj, is a multiplexer that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj].",
                "The values of Mod[Mj] denote the different models of j.",
                "In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1.",
                "The distribution over the node, Mod[Mj], is the agent is belief over the models of j given a physical state.",
                "For more agents, we will have as many model nodes as there are agents.",
                "Notice that Fig. 1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional dependency links.",
                "In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them.",
                "In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes.",
                "This allows I-IDs to be represented and implemented using conventional application tools that target IDs.",
                "Note that we may view the level l I-ID as a NID.",
                "Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2).",
                "If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID.",
                "Note that within the I-IDs (or IDs) at each level, there is only a single decision node.",
                "Thus, our NID does not contain any MAIDs.",
                "Figure 2: A level l I-ID represented as a NID.",
                "The probabilities assigned to the blocks of the NID are is beliefs over js models conditioned on a physical state. 3.2 Solution The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively.",
                "We start by solving the level 0 models, which, if intentional, are traditional IDs.",
                "Their solutions provide probability distributions over the other agents actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID.",
                "The mapping from the level 0 models decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c).",
                "During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj].",
                "As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent is belief over the models of j conditioned on the physical state.",
                "The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j.",
                "Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18].",
                "This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief.",
                "Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agents current belief is known. 4.",
                "INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic influence diagrams (I-DIDs) extend I-IDs (and NIDs) to allow sequential decision-making over several time steps.",
                "Just as DIDs are structured graphical representations of POMDPs, I-DIDs are the graphical online analogs for finitely nested I-POMDPs.",
                "I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents. 4.1 Syntax We depict a general two time-slice I-DID in Fig. 3(a).",
                "In addition to the model nodes and the dashed policy link, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a).",
                "We explained the semantics of the model node and the policy link in the previous section; we describe the model updates next.",
                "The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1.",
                "Recall from Section 2 that an agents intentional model includes its belief.",
                "Because the agents act and receive observations, their models are updated to reflect their changed beliefs.",
                "Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models.",
                "Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models.",
                "Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model.",
                "These steps are a part of agent is belief update formalized using Eq. 1.",
                "In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID.",
                "If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ).",
                "These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation.",
                "The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously.",
                "Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed.",
                "The probability that js updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node.",
                "In order to obtain the probability of js possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ].",
                "Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a multiplexer modulated by Mod[Mt j ].",
                "Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1.",
                "Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1.",
                "Notice that the model update link may be replaced by the dependency links between the chance nodes that constitute the model nodes in the two time slices.",
                "In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them.",
                "Chance nodes and dependency links that not in bold are standard, usually found in DIDs.",
                "Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents js observation at time t + 1.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links.",
                "We note that the possible set of models of the other agent j grows exponentially with the number of time steps.",
                "For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node. 4.2 Solution Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively.",
                "For the purpose of illustration, let l=1 and T=2.",
                "The solution method uses the standard look-ahead technique, projecting the agents action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step.",
                "Because agent i has a belief over js models as well, the lookahead includes finding out the possible models that j could have in the future.",
                "Consequently, each of js subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions.",
                "These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b).",
                "We note the recursive nature of this solution: in solving agent is level 1 I-DID, js level 0 DIDs must be solved.",
                "If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner.",
                "We briefly outline the recursive algorithm for solving agent is Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase 1.",
                "For t from 1 to T − 1 do 2.",
                "If l ≥ 1 then Populate Mt+1 j,l−1 3.",
                "For each mt j in Range(Mt j,l−1) do 4.",
                "Recursively call algorithm with the l − 1 I-ID (or ID) that represents mt j and the horizon, T − t + 1 5.",
                "Map the decision node of the solved I-ID (or ID), OPT(mt j), to a chance node Aj 6.",
                "For each aj in OPT(mt j) do 7.",
                "For each oj in Oj (part of mt j) do 8.",
                "Update js belief, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← New I-ID (or ID) with bt+1 j as the initial belief 10.",
                "Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11.",
                "Add the model node, Mt+1 j,l−1, and the dependency links between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b)) 12.",
                "Add the chance, decision, and utility nodes for t + 1 time slice and the dependency links between them 13.",
                "Establish the CPTs for each chance node and utility node Look-Ahead Phase 14.",
                "Apply the standard look-ahead and backup method to solve the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5.",
                "We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the dependency links and the conditional probability tables for each node.",
                "We particularly focus on establishing and populating the model nodes (lines 3-11).",
                "Note that Range(·) returns the values (lower level models) of the random variable given as input (model node).",
                "In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs.",
                "Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents.",
                "As we mentioned previously, the 0-th level models are the traditional DIDs.",
                "Their solutions provide probability distributions over actions of the agent modeled at that level to I-DIDs at level 1.",
                "Given probability distributions over other agents actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models.",
                "Assume that the number of models considered at each level is bound by a number, M. Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs. 5.",
                "EXAMPLE APPLICATIONS To illustrate the usefulness of I-DIDs, we apply them to three problem domains.",
                "We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it. 5.1 Followership-Leadership in the Multiagent Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9].",
                "The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L).",
                "In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agents opening one of the doors.",
                "When any door is opened, the tiger persists in its original location with a probability of 95%.",
                "Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%.",
                "Agent j, on the other hand, hears growls with a reliability of 95%.",
                "Thus, the setting is such that agent i hears agent j opening doors more reliably than the tigers growls.",
                "This suggests that i could use js actions as an indication of the location of the tiger, as we discuss below.",
                "Each agents preferences are as in the single agent game discussed in [13].",
                "The transition, observation, and reward functions are shown in [16].",
                "A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions.",
                "In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship.",
                "In particular, we analyze the situational and epistemological conditions sufficient for their emergence.",
                "The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the others actions in order to maximize its payoffs.",
                "Let us consider a particular setting of the tiger problem in which agent i believes that js preferences are aligned with its own - both of them just want to get the gold - and js hearing is more reliable in comparison to itself.",
                "As an example, suppose that j, on listening can discern the tigers location 95% of the times compared to is 65% accuracy.",
                "Additionally, agent i does not have any initial information about the tigers location.",
                "In other words, is single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger.",
                "In addition, i considers two models of j, which differ in js flat level 0 initial beliefs.",
                "This is represented in the level 1 I-ID shown in Fig. 6(a).",
                "According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)).",
                "Agent i is undecided on these two models of j.",
                "If we vary is hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior.",
                "If is probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow js actions: i opens the same door that j opened previously iff is own assessment of the tigers location confirms js pick.",
                "If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)).",
                "Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem.",
                "Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations.",
                "We observed that a single level of belief nesting - beliefs about the others models - was sufficient for followership to emerge in the tiger problem.",
                "However, the epistemological requirements for the emergence of leadership are more complex.",
                "For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i.",
                "As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow js actions over time.",
                "Agent j emerges as a leader if it believes that i will follow it, which implies that js belief must be nested two levels deep to enable it to recognize its leadership role.",
                "Realizing that i will follow presents j with an opportunity to influence is actions in the benefit of the collective good or its self-interest alone.",
                "For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original.",
                "If j alone selects the correct door, it gets the payoff of 10.",
                "On the other hand, if both agents pick the wrong door, their penalties are cut in half.",
                "In this setting, it is in both js best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader.",
                "However, consider a slightly different problem in which j gains from is loss and is penalized if i gains.",
                "Specifically, let is payoff be subtracted from js, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then is loss of 100 becomes js gain.",
                "Agent j believes that i incorrectly thinks that js preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is.",
                "Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following js actions.",
                "We show is normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a).",
                "The policy demonstrates that i will blindly follow js actions.",
                "Since the tiger persists in its original location with a probability of 0.95, i will select the same door again.",
                "If j begins the game with a 99% probability that the tiger is on the right, solving js I-DID nested two levels deep, results in the policy shown in Fig. 8(b).",
                "Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL.",
                "Agent js intention is to deceive i who, it believes, will follow js actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest.",
                "Figure 8: Emergence of deception between agents in the tiger problem.",
                "Behaviors of interest are in bold. * denotes as before. (a) Agent is policy demonstrating that it will blindly follow js actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i. 5.2 Altruism and Reciprocity in the Public Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves.",
                "Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot.",
                "However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes.",
                "Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others contributions.",
                "However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions.",
                "The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4].",
                "Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting.",
                "These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others.",
                "For simplicity, we assume that the game is played between M = 2 agents, i and j.",
                "Let each agent be initially endowed with XT amount of resources.",
                "While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions.",
                "Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute.",
                "The latter action is deThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D).",
                "We assume that the actions are not observable to others.",
                "The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return.",
                "We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain.",
                "Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment.",
                "In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment.",
                "Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent.",
                "For simplicity, we assume that the cost of punishing is same for both the agents.",
                "The one-shot PG game with punishment is shown in Table. 1.",
                "Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action.",
                "If P < XT − ciXT , then defection is the dominating action for both.",
                "If P = XT − ciXT , then the game is not dominance-solvable.",
                "Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a).",
                "We formulate a sequential version of the PG problem with punishment from the perspective of agent i.",
                "Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents.",
                "Each agent may contribute a fixed amount, xc, or defect.",
                "An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot.",
                "Notice that the observations are also indirectly indicative of agent js actions because the state of the public pot is influenced by them.",
                "The amount of resources in agent is private pot, is perfectly observable to i.",
                "The payoffs are analogous to Table. 1.",
                "Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)).",
                "Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect.",
                "Let xc = 1 and the level 0 agent be punished half the times it defects.",
                "With one action remaining, both types of agents choose to contribute to avoid being punished.",
                "With two actions to go, the altruistic type chooses to contribute, while the other defects.",
                "This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids.",
                "Because cj for the non-altruistic type is less, it prefers not to contribute.",
                "With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects.",
                "For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection.",
                "We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level 0 models, mentioned previously, to j (see Fig. 9).",
                "If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps.",
                "This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type.",
                "However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1.",
                "These results demonstrate that the behavior of our altruistic type resembles that found experimentally.",
                "The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic.",
                "We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection.",
                "The reciprocal types marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other.",
                "We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full.",
                "For this prior belief, i chooses to defect.",
                "On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)).",
                "This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from js action to contribute.",
                "Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step.",
                "Agent i therefore chooses to contribute to reciprocate js action.",
                "An analogous reasoning leads i to defect when it observes a meager pot.",
                "With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations.",
                "Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic). 5.3 Strategies in Two-Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2].",
                "Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck.",
                "While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands.",
                "To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit.",
                "Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot.",
                "During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to 8.",
                "Notice that, for example, if a lower numbered card is discarded, the probability of receiving a low card in exchange is now reduced.",
                "We show the level 1 I-ID for the simplified two-player Poker in Fig. 11.",
                "We considered two models (personality types) of agent j.",
                "The conservative type believes that it is likely that its opponent has a high numbered card in its hand.",
                "On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card.",
                "Thus, the two types differ in their beliefs over their opponents hand.",
                "In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution.",
                "With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand.",
                "This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange.",
                "The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one.",
                "Figure 11: (a) Level 1 I-ID of agent i.",
                "The observation reveals information about js hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a).",
                "The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in js hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. is own hand contains the card numbered 8.",
                "The agent starts by keeping its card.",
                "On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card.",
                "If i observes that j discarded its card into the L or H pile, i believes that j is aggressive.",
                "On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange.",
                "Because the probability of receiving a low card is high now, i chooses to keep its card.",
                "On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card.",
                "In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff.",
                "This is partly due to the fact that an observation of, say, L resets the agent is previous time step beliefs over js hand to the low numbered cards only. 6.",
                "DISCUSSION We showed how DIDs may be extended to I-DIDs that enable online sequential decision-making in uncertain multiagent settings.",
                "Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent is three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs.",
                "I-DIDs extend NIDs to allow sequential decision-making over multiple time steps in the presence of other interacting agents.",
                "I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs.",
                "We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality.",
                "Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work.",
                "The first author would like to acknowledge the support of a UGARF grant. 7.",
                "REFERENCES [1] R. J. Aumann.",
                "Interactive epistemology i: Knowledge.",
                "International Journal of Game Theory, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron.",
                "The challenge of poker.",
                "AIJ, 2001. [3] A. Brandenburger and E. Dekel.",
                "Hierarchies of beliefs and common knowledge.",
                "Journal of Economic Theory, 59:189-198, 1993. [4] C. Camerer.",
                "Behavioral Game Theory: Experiments in Strategic Interaction.",
                "Princeton University Press, 2003. [5] E. Fehr and S. Gachter.",
                "Cooperation and punishment in public goods experiments.",
                "American Economic Review, 90(4):980-994, 2000. [6] D. Fudenberg and D. K. Levine.",
                "The Theory of Learning in Games.",
                "MIT Press, 1998. [7] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [8] Y. Gal and A. Pfeffer.",
                "A language for modeling agents decision-making processes in games.",
                "In AAMAS, 2003. [9] P. Gmytrasiewicz and P. Doshi.",
                "A framework for sequential planning in multiagent settings.",
                "JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz and E. Durfee.",
                "Rational coordination in multi-agent environments.",
                "JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi.",
                "Games with incomplete information played by bayesian players.",
                "Management Science, 14(3):159-182, 1967. [12] R. A. Howard and J. E. Matheson.",
                "Influence diagrams.",
                "In R. A. Howard and J. E. Matheson, editors, The Principles and Applications of Decision Analysis.",
                "Strategic Decisions Group, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman, and A. Cassandra.",
                "Planning and acting in partially observable stochastic domains.",
                "Artificial Intelligence Journal, 2, 1998. [14] D. Koller and B. Milch.",
                "Multi-agent influence diagrams for representing and solving games.",
                "In IJCAI, pages 1027-1034, 2001. [15] K. Polich and P. Gmytrasiewicz.",
                "Interactive dynamic influence diagrams.",
                "In GTDT Workshop, AAMAS, 2006. [16] B.",
                "Rathnas., P. Doshi, and P. J. Gmytrasiewicz.",
                "Exact solutions to interactive pomdps using behavioral equivalence.",
                "In Autonomous Agents and Multi-Agent Systems Conference (AAMAS), 2006. [17] S. Russell and P. Norvig.",
                "Artificial Intelligence: A Modern Approach (Second Edition).",
                "Prentice Hall, 2003. [18] R. D. Shachter.",
                "Evaluating influence diagrams.",
                "Operations Research, 34(6):871-882, 1986. [19] D. Suryadi and P. Gmytrasiewicz.",
                "Learning models of other agents using influence diagrams.",
                "In UM, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Estos \"modelo gráfico\" llamados diagramas de influencia dinámica interactiva (I-DID) buscan modelar explícitamente la estructura que a menudo está presente en los problemas del mundo real al descomponer la situación en las variables casuales y de decisión, y las dependencias entre las variables."
            ],
            "translated_text": "",
            "candidates": [
                "modelo gráfico",
                "modelo gráfico"
            ],
            "error": []
        },
        "agent online": {
            "translated_key": "Agente en línea",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Graphical Models for Online Solutions to Interactive POMDPs Prashant Doshi Dept.",
                "of Computer Science University of Georgia Athens, GA 30602, USA pdoshi@cs.uga.edu Yifeng Zeng Dept. of Computer Science Aalborg University DK-9220 Aalborg, Denmark yfzeng@cs.aau.edu Qiongyu Chen Dept. of Computer Science National Univ. of Singapore 117543, Singapore chenqy@comp.nus.edu.sg ABSTRACT We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation.",
                "These graphical models called interactive dynamic influence diagrams (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs may be used to compute the policy of an <br>agent online</br> as the agent acts and observes in a setting that is populated by other interacting agents.",
                "Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Theory 1.",
                "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in partially observable multiagent environments.",
                "They generalize POMDPs [13] to multiagent settings by including the other agents computable models in the state space along with the states of the physical environment.",
                "The models encompass all information influencing the agents behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11].",
                "I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a decision-makers perspective in the interaction.",
                "In [15], Polich and Gmytrasiewicz introduced interactive dynamic influence diagrams (I-DIDs) as the computational representations of I-POMDPs.",
                "I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs contribute to a growing line of work [19] that includes multi-agent influence diagrams (MAIDs) [14], and more recently, networks of influence diagrams (NIDs) [8].",
                "These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agents actions and chance nodes capturing the agents private information.",
                "MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the independence structure.",
                "NIDs extend MAIDs to include agents uncertainty over the game being played and over models of the other agents.",
                "Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent.",
                "Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently.",
                "However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games.",
                "Matters are more complex when we consider interactions that are extended over time, where predictions about others future actions must be made using models that change as the agents act and observe.",
                "I-DIDs address this gap by allowing the representation of other agents models as the values of a special model node.",
                "Both, other agents models and the original agents beliefs over these models are updated over time using special-purpose implementations.",
                "In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID.",
                "Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently.",
                "Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I-DID by [15], and show that it could be replaced by traditional dependency links.",
                "In the previous representation of the I-DID, the update of the agents belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time.",
                "We explicate the semantics of this link by showing how it can be implemented using the traditional dependency links between the chance nodes that constitute the model nodes.",
                "The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs.",
                "We show how IDIDs may be used to model an agents uncertainty over others models, that may themselves be I-DIDs.",
                "Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others models.",
                "Analogous to DIDs, I-DIDs may be used to compute the policy of an <br>agent online</br> as the agent acts and observes in a setting that is populated by other interacting agents. 2.",
                "BACKGROUND: FINITELY NESTED IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents models as part of the state space [9].",
                "Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents models and their beliefs about others.",
                "For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j.",
                "A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment.",
                "Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj .",
                "Here, j is Bayes rational and OCj is js optimality criterion.",
                "SMj is the set of subintentional models of j.",
                "Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent.",
                "We give a recursive bottom-up construction of the interactive state space below.",
                "ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . .",
                "ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent is preferences over its interactive states.",
                "Usually only the physical states will matter.",
                "Agent is policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i.",
                "Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai). 2.1 Belief Update Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes.",
                "However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones.",
                "First, since the state of the physical environment depends on the actions of both agents, is prediction of how the physical state changes has to be made based on its prediction of js actions.",
                "Second, changes in js models have to be included in is belief update.",
                "Specifically, if j is intentional then an update of js beliefs due to its action and observation has to be included.",
                "In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief.",
                "If js model is subintentional, then js probable observations are appended to the observation history contained in the model.",
                "Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update.",
                "For a version of the belief update when js model is subintentional, see [9].",
                "If agent j is also modeled as an I-POMDP, then is belief update invokes js belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke is belief update and so on.",
                "This recursion in belief nesting bottoms out at the 0th level.",
                "At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9]. 2.2 Value Iteration Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)).",
                "Eq. 2 is a basis for value iteration in I-POMDPs.",
                "Agent is optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3.",
                "INTERACTIVEINFLUENCEDIAGRAMS A naive extension of influence diagrams (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes.",
                "However, this approach assumes that the agents actions are controlled using a probability distribution that does not change over time.",
                "Interactive influence diagrams (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node.",
                "We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon.",
                "We note that the probability distribution over the chance node, S, and the model node together represents agent is belief over its interactive states.",
                "In addition to the model 1 The 0th level model is a POMDP: Other agents actions are treated as exogenous events and folded into the T, O, and R functions.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j.",
                "The hexagon is the model node (Mj,l−1) whose structure we show in (b).",
                "Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ).",
                "Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the policy link in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model.",
                "In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs.",
                "The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2.",
                "Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional.",
                "Because the model node contains the alternative models of the other agent as its values, its representation is not trivial.",
                "In particular, some of the models within the node are I-IDs that when solved generate the agents optimal policy in their decision nodes.",
                "Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise.",
                "Borrowing insights from previous work [8], we observe that the model node and the dashed policy link that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b).",
                "The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj.",
                "Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent is beliefs.",
                "The conditional probability table of the chance node, Aj, is a multiplexer that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj].",
                "The values of Mod[Mj] denote the different models of j.",
                "In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1.",
                "The distribution over the node, Mod[Mj], is the agent is belief over the models of j given a physical state.",
                "For more agents, we will have as many model nodes as there are agents.",
                "Notice that Fig. 1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional dependency links.",
                "In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them.",
                "In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes.",
                "This allows I-IDs to be represented and implemented using conventional application tools that target IDs.",
                "Note that we may view the level l I-ID as a NID.",
                "Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2).",
                "If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID.",
                "Note that within the I-IDs (or IDs) at each level, there is only a single decision node.",
                "Thus, our NID does not contain any MAIDs.",
                "Figure 2: A level l I-ID represented as a NID.",
                "The probabilities assigned to the blocks of the NID are is beliefs over js models conditioned on a physical state. 3.2 Solution The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively.",
                "We start by solving the level 0 models, which, if intentional, are traditional IDs.",
                "Their solutions provide probability distributions over the other agents actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID.",
                "The mapping from the level 0 models decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c).",
                "During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj].",
                "As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent is belief over the models of j conditioned on the physical state.",
                "The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j.",
                "Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18].",
                "This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief.",
                "Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agents current belief is known. 4.",
                "INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic influence diagrams (I-DIDs) extend I-IDs (and NIDs) to allow sequential decision-making over several time steps.",
                "Just as DIDs are structured graphical representations of POMDPs, I-DIDs are the graphical online analogs for finitely nested I-POMDPs.",
                "I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents. 4.1 Syntax We depict a general two time-slice I-DID in Fig. 3(a).",
                "In addition to the model nodes and the dashed policy link, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a).",
                "We explained the semantics of the model node and the policy link in the previous section; we describe the model updates next.",
                "The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1.",
                "Recall from Section 2 that an agents intentional model includes its belief.",
                "Because the agents act and receive observations, their models are updated to reflect their changed beliefs.",
                "Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models.",
                "Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models.",
                "Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model.",
                "These steps are a part of agent is belief update formalized using Eq. 1.",
                "In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID.",
                "If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ).",
                "These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation.",
                "The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously.",
                "Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed.",
                "The probability that js updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node.",
                "In order to obtain the probability of js possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ].",
                "Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a multiplexer modulated by Mod[Mt j ].",
                "Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1.",
                "Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1.",
                "Notice that the model update link may be replaced by the dependency links between the chance nodes that constitute the model nodes in the two time slices.",
                "In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them.",
                "Chance nodes and dependency links that not in bold are standard, usually found in DIDs.",
                "Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents js observation at time t + 1.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links.",
                "We note that the possible set of models of the other agent j grows exponentially with the number of time steps.",
                "For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node. 4.2 Solution Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively.",
                "For the purpose of illustration, let l=1 and T=2.",
                "The solution method uses the standard look-ahead technique, projecting the agents action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step.",
                "Because agent i has a belief over js models as well, the lookahead includes finding out the possible models that j could have in the future.",
                "Consequently, each of js subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions.",
                "These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b).",
                "We note the recursive nature of this solution: in solving agent is level 1 I-DID, js level 0 DIDs must be solved.",
                "If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner.",
                "We briefly outline the recursive algorithm for solving agent is Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase 1.",
                "For t from 1 to T − 1 do 2.",
                "If l ≥ 1 then Populate Mt+1 j,l−1 3.",
                "For each mt j in Range(Mt j,l−1) do 4.",
                "Recursively call algorithm with the l − 1 I-ID (or ID) that represents mt j and the horizon, T − t + 1 5.",
                "Map the decision node of the solved I-ID (or ID), OPT(mt j), to a chance node Aj 6.",
                "For each aj in OPT(mt j) do 7.",
                "For each oj in Oj (part of mt j) do 8.",
                "Update js belief, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← New I-ID (or ID) with bt+1 j as the initial belief 10.",
                "Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11.",
                "Add the model node, Mt+1 j,l−1, and the dependency links between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b)) 12.",
                "Add the chance, decision, and utility nodes for t + 1 time slice and the dependency links between them 13.",
                "Establish the CPTs for each chance node and utility node Look-Ahead Phase 14.",
                "Apply the standard look-ahead and backup method to solve the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5.",
                "We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the dependency links and the conditional probability tables for each node.",
                "We particularly focus on establishing and populating the model nodes (lines 3-11).",
                "Note that Range(·) returns the values (lower level models) of the random variable given as input (model node).",
                "In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs.",
                "Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents.",
                "As we mentioned previously, the 0-th level models are the traditional DIDs.",
                "Their solutions provide probability distributions over actions of the agent modeled at that level to I-DIDs at level 1.",
                "Given probability distributions over other agents actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models.",
                "Assume that the number of models considered at each level is bound by a number, M. Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs. 5.",
                "EXAMPLE APPLICATIONS To illustrate the usefulness of I-DIDs, we apply them to three problem domains.",
                "We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it. 5.1 Followership-Leadership in the Multiagent Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9].",
                "The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L).",
                "In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agents opening one of the doors.",
                "When any door is opened, the tiger persists in its original location with a probability of 95%.",
                "Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%.",
                "Agent j, on the other hand, hears growls with a reliability of 95%.",
                "Thus, the setting is such that agent i hears agent j opening doors more reliably than the tigers growls.",
                "This suggests that i could use js actions as an indication of the location of the tiger, as we discuss below.",
                "Each agents preferences are as in the single agent game discussed in [13].",
                "The transition, observation, and reward functions are shown in [16].",
                "A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions.",
                "In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship.",
                "In particular, we analyze the situational and epistemological conditions sufficient for their emergence.",
                "The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the others actions in order to maximize its payoffs.",
                "Let us consider a particular setting of the tiger problem in which agent i believes that js preferences are aligned with its own - both of them just want to get the gold - and js hearing is more reliable in comparison to itself.",
                "As an example, suppose that j, on listening can discern the tigers location 95% of the times compared to is 65% accuracy.",
                "Additionally, agent i does not have any initial information about the tigers location.",
                "In other words, is single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger.",
                "In addition, i considers two models of j, which differ in js flat level 0 initial beliefs.",
                "This is represented in the level 1 I-ID shown in Fig. 6(a).",
                "According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)).",
                "Agent i is undecided on these two models of j.",
                "If we vary is hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior.",
                "If is probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow js actions: i opens the same door that j opened previously iff is own assessment of the tigers location confirms js pick.",
                "If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)).",
                "Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem.",
                "Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations.",
                "We observed that a single level of belief nesting - beliefs about the others models - was sufficient for followership to emerge in the tiger problem.",
                "However, the epistemological requirements for the emergence of leadership are more complex.",
                "For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i.",
                "As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow js actions over time.",
                "Agent j emerges as a leader if it believes that i will follow it, which implies that js belief must be nested two levels deep to enable it to recognize its leadership role.",
                "Realizing that i will follow presents j with an opportunity to influence is actions in the benefit of the collective good or its self-interest alone.",
                "For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original.",
                "If j alone selects the correct door, it gets the payoff of 10.",
                "On the other hand, if both agents pick the wrong door, their penalties are cut in half.",
                "In this setting, it is in both js best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader.",
                "However, consider a slightly different problem in which j gains from is loss and is penalized if i gains.",
                "Specifically, let is payoff be subtracted from js, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then is loss of 100 becomes js gain.",
                "Agent j believes that i incorrectly thinks that js preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is.",
                "Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following js actions.",
                "We show is normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a).",
                "The policy demonstrates that i will blindly follow js actions.",
                "Since the tiger persists in its original location with a probability of 0.95, i will select the same door again.",
                "If j begins the game with a 99% probability that the tiger is on the right, solving js I-DID nested two levels deep, results in the policy shown in Fig. 8(b).",
                "Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL.",
                "Agent js intention is to deceive i who, it believes, will follow js actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest.",
                "Figure 8: Emergence of deception between agents in the tiger problem.",
                "Behaviors of interest are in bold. * denotes as before. (a) Agent is policy demonstrating that it will blindly follow js actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i. 5.2 Altruism and Reciprocity in the Public Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves.",
                "Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot.",
                "However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes.",
                "Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others contributions.",
                "However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions.",
                "The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4].",
                "Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting.",
                "These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others.",
                "For simplicity, we assume that the game is played between M = 2 agents, i and j.",
                "Let each agent be initially endowed with XT amount of resources.",
                "While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions.",
                "Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute.",
                "The latter action is deThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D).",
                "We assume that the actions are not observable to others.",
                "The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return.",
                "We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain.",
                "Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment.",
                "In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment.",
                "Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent.",
                "For simplicity, we assume that the cost of punishing is same for both the agents.",
                "The one-shot PG game with punishment is shown in Table. 1.",
                "Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action.",
                "If P < XT − ciXT , then defection is the dominating action for both.",
                "If P = XT − ciXT , then the game is not dominance-solvable.",
                "Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a).",
                "We formulate a sequential version of the PG problem with punishment from the perspective of agent i.",
                "Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents.",
                "Each agent may contribute a fixed amount, xc, or defect.",
                "An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot.",
                "Notice that the observations are also indirectly indicative of agent js actions because the state of the public pot is influenced by them.",
                "The amount of resources in agent is private pot, is perfectly observable to i.",
                "The payoffs are analogous to Table. 1.",
                "Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)).",
                "Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect.",
                "Let xc = 1 and the level 0 agent be punished half the times it defects.",
                "With one action remaining, both types of agents choose to contribute to avoid being punished.",
                "With two actions to go, the altruistic type chooses to contribute, while the other defects.",
                "This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids.",
                "Because cj for the non-altruistic type is less, it prefers not to contribute.",
                "With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects.",
                "For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection.",
                "We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level 0 models, mentioned previously, to j (see Fig. 9).",
                "If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps.",
                "This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type.",
                "However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1.",
                "These results demonstrate that the behavior of our altruistic type resembles that found experimentally.",
                "The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic.",
                "We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection.",
                "The reciprocal types marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other.",
                "We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full.",
                "For this prior belief, i chooses to defect.",
                "On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)).",
                "This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from js action to contribute.",
                "Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step.",
                "Agent i therefore chooses to contribute to reciprocate js action.",
                "An analogous reasoning leads i to defect when it observes a meager pot.",
                "With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations.",
                "Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic). 5.3 Strategies in Two-Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2].",
                "Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck.",
                "While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands.",
                "To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit.",
                "Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot.",
                "During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to 8.",
                "Notice that, for example, if a lower numbered card is discarded, the probability of receiving a low card in exchange is now reduced.",
                "We show the level 1 I-ID for the simplified two-player Poker in Fig. 11.",
                "We considered two models (personality types) of agent j.",
                "The conservative type believes that it is likely that its opponent has a high numbered card in its hand.",
                "On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card.",
                "Thus, the two types differ in their beliefs over their opponents hand.",
                "In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution.",
                "With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand.",
                "This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange.",
                "The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one.",
                "Figure 11: (a) Level 1 I-ID of agent i.",
                "The observation reveals information about js hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a).",
                "The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in js hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. is own hand contains the card numbered 8.",
                "The agent starts by keeping its card.",
                "On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card.",
                "If i observes that j discarded its card into the L or H pile, i believes that j is aggressive.",
                "On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange.",
                "Because the probability of receiving a low card is high now, i chooses to keep its card.",
                "On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card.",
                "In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff.",
                "This is partly due to the fact that an observation of, say, L resets the agent is previous time step beliefs over js hand to the low numbered cards only. 6.",
                "DISCUSSION We showed how DIDs may be extended to I-DIDs that enable online sequential decision-making in uncertain multiagent settings.",
                "Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent is three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs.",
                "I-DIDs extend NIDs to allow sequential decision-making over multiple time steps in the presence of other interacting agents.",
                "I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs.",
                "We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality.",
                "Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work.",
                "The first author would like to acknowledge the support of a UGARF grant. 7.",
                "REFERENCES [1] R. J. Aumann.",
                "Interactive epistemology i: Knowledge.",
                "International Journal of Game Theory, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron.",
                "The challenge of poker.",
                "AIJ, 2001. [3] A. Brandenburger and E. Dekel.",
                "Hierarchies of beliefs and common knowledge.",
                "Journal of Economic Theory, 59:189-198, 1993. [4] C. Camerer.",
                "Behavioral Game Theory: Experiments in Strategic Interaction.",
                "Princeton University Press, 2003. [5] E. Fehr and S. Gachter.",
                "Cooperation and punishment in public goods experiments.",
                "American Economic Review, 90(4):980-994, 2000. [6] D. Fudenberg and D. K. Levine.",
                "The Theory of Learning in Games.",
                "MIT Press, 1998. [7] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [8] Y. Gal and A. Pfeffer.",
                "A language for modeling agents decision-making processes in games.",
                "In AAMAS, 2003. [9] P. Gmytrasiewicz and P. Doshi.",
                "A framework for sequential planning in multiagent settings.",
                "JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz and E. Durfee.",
                "Rational coordination in multi-agent environments.",
                "JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi.",
                "Games with incomplete information played by bayesian players.",
                "Management Science, 14(3):159-182, 1967. [12] R. A. Howard and J. E. Matheson.",
                "Influence diagrams.",
                "In R. A. Howard and J. E. Matheson, editors, The Principles and Applications of Decision Analysis.",
                "Strategic Decisions Group, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman, and A. Cassandra.",
                "Planning and acting in partially observable stochastic domains.",
                "Artificial Intelligence Journal, 2, 1998. [14] D. Koller and B. Milch.",
                "Multi-agent influence diagrams for representing and solving games.",
                "In IJCAI, pages 1027-1034, 2001. [15] K. Polich and P. Gmytrasiewicz.",
                "Interactive dynamic influence diagrams.",
                "In GTDT Workshop, AAMAS, 2006. [16] B.",
                "Rathnas., P. Doshi, and P. J. Gmytrasiewicz.",
                "Exact solutions to interactive pomdps using behavioral equivalence.",
                "In Autonomous Agents and Multi-Agent Systems Conference (AAMAS), 2006. [17] S. Russell and P. Norvig.",
                "Artificial Intelligence: A Modern Approach (Second Edition).",
                "Prentice Hall, 2003. [18] R. D. Shachter.",
                "Evaluating influence diagrams.",
                "Operations Research, 34(6):871-882, 1986. [19] D. Suryadi and P. Gmytrasiewicz.",
                "Learning models of other agents using influence diagrams.",
                "In UM, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Los I-DID pueden usarse para calcular la política de un \"agente en línea\" como el agente actúa y observa en un entorno poblado por otros agentes interactivos.",
                "Análogo a los DIDS, los I-DID se pueden usar para calcular la política de un \"agente en línea\" como el agente actúa y observa en un entorno poblado por otros agentes interactivos.2."
            ],
            "translated_text": "",
            "candidates": [
                "Agente en línea",
                "agente en línea",
                "Agente en línea",
                "agente en línea"
            ],
            "error": []
        },
        "interactive partially observable markov decision process": {
            "translated_key": "Proceso de decisión de Markov parcialmente observable interactivo",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Graphical Models for Online Solutions to Interactive POMDPs Prashant Doshi Dept.",
                "of Computer Science University of Georgia Athens, GA 30602, USA pdoshi@cs.uga.edu Yifeng Zeng Dept. of Computer Science Aalborg University DK-9220 Aalborg, Denmark yfzeng@cs.aau.edu Qiongyu Chen Dept. of Computer Science National Univ. of Singapore 117543, Singapore chenqy@comp.nus.edu.sg ABSTRACT We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation.",
                "These graphical models called interactive dynamic influence diagrams (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents.",
                "Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Theory 1.",
                "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in partially observable multiagent environments.",
                "They generalize POMDPs [13] to multiagent settings by including the other agents computable models in the state space along with the states of the physical environment.",
                "The models encompass all information influencing the agents behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11].",
                "I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a decision-makers perspective in the interaction.",
                "In [15], Polich and Gmytrasiewicz introduced interactive dynamic influence diagrams (I-DIDs) as the computational representations of I-POMDPs.",
                "I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs contribute to a growing line of work [19] that includes multi-agent influence diagrams (MAIDs) [14], and more recently, networks of influence diagrams (NIDs) [8].",
                "These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agents actions and chance nodes capturing the agents private information.",
                "MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the independence structure.",
                "NIDs extend MAIDs to include agents uncertainty over the game being played and over models of the other agents.",
                "Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent.",
                "Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently.",
                "However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games.",
                "Matters are more complex when we consider interactions that are extended over time, where predictions about others future actions must be made using models that change as the agents act and observe.",
                "I-DIDs address this gap by allowing the representation of other agents models as the values of a special model node.",
                "Both, other agents models and the original agents beliefs over these models are updated over time using special-purpose implementations.",
                "In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID.",
                "Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently.",
                "Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I-DID by [15], and show that it could be replaced by traditional dependency links.",
                "In the previous representation of the I-DID, the update of the agents belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time.",
                "We explicate the semantics of this link by showing how it can be implemented using the traditional dependency links between the chance nodes that constitute the model nodes.",
                "The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs.",
                "We show how IDIDs may be used to model an agents uncertainty over others models, that may themselves be I-DIDs.",
                "Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others models.",
                "Analogous to DIDs, I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. 2.",
                "BACKGROUND: FINITELY NESTED IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents models as part of the state space [9].",
                "Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents models and their beliefs about others.",
                "For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j.",
                "A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment.",
                "Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj .",
                "Here, j is Bayes rational and OCj is js optimality criterion.",
                "SMj is the set of subintentional models of j.",
                "Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent.",
                "We give a recursive bottom-up construction of the interactive state space below.",
                "ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . .",
                "ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent is preferences over its interactive states.",
                "Usually only the physical states will matter.",
                "Agent is policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i.",
                "Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai). 2.1 Belief Update Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes.",
                "However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones.",
                "First, since the state of the physical environment depends on the actions of both agents, is prediction of how the physical state changes has to be made based on its prediction of js actions.",
                "Second, changes in js models have to be included in is belief update.",
                "Specifically, if j is intentional then an update of js beliefs due to its action and observation has to be included.",
                "In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief.",
                "If js model is subintentional, then js probable observations are appended to the observation history contained in the model.",
                "Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update.",
                "For a version of the belief update when js model is subintentional, see [9].",
                "If agent j is also modeled as an I-POMDP, then is belief update invokes js belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke is belief update and so on.",
                "This recursion in belief nesting bottoms out at the 0th level.",
                "At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9]. 2.2 Value Iteration Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)).",
                "Eq. 2 is a basis for value iteration in I-POMDPs.",
                "Agent is optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3.",
                "INTERACTIVEINFLUENCEDIAGRAMS A naive extension of influence diagrams (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes.",
                "However, this approach assumes that the agents actions are controlled using a probability distribution that does not change over time.",
                "Interactive influence diagrams (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node.",
                "We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon.",
                "We note that the probability distribution over the chance node, S, and the model node together represents agent is belief over its interactive states.",
                "In addition to the model 1 The 0th level model is a POMDP: Other agents actions are treated as exogenous events and folded into the T, O, and R functions.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j.",
                "The hexagon is the model node (Mj,l−1) whose structure we show in (b).",
                "Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ).",
                "Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the policy link in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model.",
                "In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs.",
                "The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2.",
                "Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional.",
                "Because the model node contains the alternative models of the other agent as its values, its representation is not trivial.",
                "In particular, some of the models within the node are I-IDs that when solved generate the agents optimal policy in their decision nodes.",
                "Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise.",
                "Borrowing insights from previous work [8], we observe that the model node and the dashed policy link that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b).",
                "The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj.",
                "Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent is beliefs.",
                "The conditional probability table of the chance node, Aj, is a multiplexer that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj].",
                "The values of Mod[Mj] denote the different models of j.",
                "In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1.",
                "The distribution over the node, Mod[Mj], is the agent is belief over the models of j given a physical state.",
                "For more agents, we will have as many model nodes as there are agents.",
                "Notice that Fig. 1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional dependency links.",
                "In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them.",
                "In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes.",
                "This allows I-IDs to be represented and implemented using conventional application tools that target IDs.",
                "Note that we may view the level l I-ID as a NID.",
                "Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2).",
                "If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID.",
                "Note that within the I-IDs (or IDs) at each level, there is only a single decision node.",
                "Thus, our NID does not contain any MAIDs.",
                "Figure 2: A level l I-ID represented as a NID.",
                "The probabilities assigned to the blocks of the NID are is beliefs over js models conditioned on a physical state. 3.2 Solution The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively.",
                "We start by solving the level 0 models, which, if intentional, are traditional IDs.",
                "Their solutions provide probability distributions over the other agents actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID.",
                "The mapping from the level 0 models decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c).",
                "During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj].",
                "As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent is belief over the models of j conditioned on the physical state.",
                "The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j.",
                "Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18].",
                "This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief.",
                "Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agents current belief is known. 4.",
                "INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic influence diagrams (I-DIDs) extend I-IDs (and NIDs) to allow sequential decision-making over several time steps.",
                "Just as DIDs are structured graphical representations of POMDPs, I-DIDs are the graphical online analogs for finitely nested I-POMDPs.",
                "I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents. 4.1 Syntax We depict a general two time-slice I-DID in Fig. 3(a).",
                "In addition to the model nodes and the dashed policy link, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a).",
                "We explained the semantics of the model node and the policy link in the previous section; we describe the model updates next.",
                "The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1.",
                "Recall from Section 2 that an agents intentional model includes its belief.",
                "Because the agents act and receive observations, their models are updated to reflect their changed beliefs.",
                "Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models.",
                "Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models.",
                "Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model.",
                "These steps are a part of agent is belief update formalized using Eq. 1.",
                "In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID.",
                "If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ).",
                "These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation.",
                "The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously.",
                "Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed.",
                "The probability that js updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node.",
                "In order to obtain the probability of js possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ].",
                "Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a multiplexer modulated by Mod[Mt j ].",
                "Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1.",
                "Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1.",
                "Notice that the model update link may be replaced by the dependency links between the chance nodes that constitute the model nodes in the two time slices.",
                "In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them.",
                "Chance nodes and dependency links that not in bold are standard, usually found in DIDs.",
                "Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents js observation at time t + 1.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links.",
                "We note that the possible set of models of the other agent j grows exponentially with the number of time steps.",
                "For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node. 4.2 Solution Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively.",
                "For the purpose of illustration, let l=1 and T=2.",
                "The solution method uses the standard look-ahead technique, projecting the agents action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step.",
                "Because agent i has a belief over js models as well, the lookahead includes finding out the possible models that j could have in the future.",
                "Consequently, each of js subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions.",
                "These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b).",
                "We note the recursive nature of this solution: in solving agent is level 1 I-DID, js level 0 DIDs must be solved.",
                "If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner.",
                "We briefly outline the recursive algorithm for solving agent is Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase 1.",
                "For t from 1 to T − 1 do 2.",
                "If l ≥ 1 then Populate Mt+1 j,l−1 3.",
                "For each mt j in Range(Mt j,l−1) do 4.",
                "Recursively call algorithm with the l − 1 I-ID (or ID) that represents mt j and the horizon, T − t + 1 5.",
                "Map the decision node of the solved I-ID (or ID), OPT(mt j), to a chance node Aj 6.",
                "For each aj in OPT(mt j) do 7.",
                "For each oj in Oj (part of mt j) do 8.",
                "Update js belief, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← New I-ID (or ID) with bt+1 j as the initial belief 10.",
                "Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11.",
                "Add the model node, Mt+1 j,l−1, and the dependency links between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b)) 12.",
                "Add the chance, decision, and utility nodes for t + 1 time slice and the dependency links between them 13.",
                "Establish the CPTs for each chance node and utility node Look-Ahead Phase 14.",
                "Apply the standard look-ahead and backup method to solve the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5.",
                "We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the dependency links and the conditional probability tables for each node.",
                "We particularly focus on establishing and populating the model nodes (lines 3-11).",
                "Note that Range(·) returns the values (lower level models) of the random variable given as input (model node).",
                "In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs.",
                "Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents.",
                "As we mentioned previously, the 0-th level models are the traditional DIDs.",
                "Their solutions provide probability distributions over actions of the agent modeled at that level to I-DIDs at level 1.",
                "Given probability distributions over other agents actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models.",
                "Assume that the number of models considered at each level is bound by a number, M. Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs. 5.",
                "EXAMPLE APPLICATIONS To illustrate the usefulness of I-DIDs, we apply them to three problem domains.",
                "We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it. 5.1 Followership-Leadership in the Multiagent Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9].",
                "The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L).",
                "In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agents opening one of the doors.",
                "When any door is opened, the tiger persists in its original location with a probability of 95%.",
                "Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%.",
                "Agent j, on the other hand, hears growls with a reliability of 95%.",
                "Thus, the setting is such that agent i hears agent j opening doors more reliably than the tigers growls.",
                "This suggests that i could use js actions as an indication of the location of the tiger, as we discuss below.",
                "Each agents preferences are as in the single agent game discussed in [13].",
                "The transition, observation, and reward functions are shown in [16].",
                "A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions.",
                "In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship.",
                "In particular, we analyze the situational and epistemological conditions sufficient for their emergence.",
                "The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the others actions in order to maximize its payoffs.",
                "Let us consider a particular setting of the tiger problem in which agent i believes that js preferences are aligned with its own - both of them just want to get the gold - and js hearing is more reliable in comparison to itself.",
                "As an example, suppose that j, on listening can discern the tigers location 95% of the times compared to is 65% accuracy.",
                "Additionally, agent i does not have any initial information about the tigers location.",
                "In other words, is single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger.",
                "In addition, i considers two models of j, which differ in js flat level 0 initial beliefs.",
                "This is represented in the level 1 I-ID shown in Fig. 6(a).",
                "According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)).",
                "Agent i is undecided on these two models of j.",
                "If we vary is hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior.",
                "If is probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow js actions: i opens the same door that j opened previously iff is own assessment of the tigers location confirms js pick.",
                "If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)).",
                "Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem.",
                "Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations.",
                "We observed that a single level of belief nesting - beliefs about the others models - was sufficient for followership to emerge in the tiger problem.",
                "However, the epistemological requirements for the emergence of leadership are more complex.",
                "For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i.",
                "As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow js actions over time.",
                "Agent j emerges as a leader if it believes that i will follow it, which implies that js belief must be nested two levels deep to enable it to recognize its leadership role.",
                "Realizing that i will follow presents j with an opportunity to influence is actions in the benefit of the collective good or its self-interest alone.",
                "For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original.",
                "If j alone selects the correct door, it gets the payoff of 10.",
                "On the other hand, if both agents pick the wrong door, their penalties are cut in half.",
                "In this setting, it is in both js best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader.",
                "However, consider a slightly different problem in which j gains from is loss and is penalized if i gains.",
                "Specifically, let is payoff be subtracted from js, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then is loss of 100 becomes js gain.",
                "Agent j believes that i incorrectly thinks that js preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is.",
                "Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following js actions.",
                "We show is normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a).",
                "The policy demonstrates that i will blindly follow js actions.",
                "Since the tiger persists in its original location with a probability of 0.95, i will select the same door again.",
                "If j begins the game with a 99% probability that the tiger is on the right, solving js I-DID nested two levels deep, results in the policy shown in Fig. 8(b).",
                "Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL.",
                "Agent js intention is to deceive i who, it believes, will follow js actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest.",
                "Figure 8: Emergence of deception between agents in the tiger problem.",
                "Behaviors of interest are in bold. * denotes as before. (a) Agent is policy demonstrating that it will blindly follow js actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i. 5.2 Altruism and Reciprocity in the Public Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves.",
                "Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot.",
                "However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes.",
                "Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others contributions.",
                "However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions.",
                "The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4].",
                "Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting.",
                "These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others.",
                "For simplicity, we assume that the game is played between M = 2 agents, i and j.",
                "Let each agent be initially endowed with XT amount of resources.",
                "While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions.",
                "Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute.",
                "The latter action is deThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D).",
                "We assume that the actions are not observable to others.",
                "The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return.",
                "We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain.",
                "Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment.",
                "In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment.",
                "Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent.",
                "For simplicity, we assume that the cost of punishing is same for both the agents.",
                "The one-shot PG game with punishment is shown in Table. 1.",
                "Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action.",
                "If P < XT − ciXT , then defection is the dominating action for both.",
                "If P = XT − ciXT , then the game is not dominance-solvable.",
                "Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a).",
                "We formulate a sequential version of the PG problem with punishment from the perspective of agent i.",
                "Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents.",
                "Each agent may contribute a fixed amount, xc, or defect.",
                "An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot.",
                "Notice that the observations are also indirectly indicative of agent js actions because the state of the public pot is influenced by them.",
                "The amount of resources in agent is private pot, is perfectly observable to i.",
                "The payoffs are analogous to Table. 1.",
                "Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)).",
                "Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect.",
                "Let xc = 1 and the level 0 agent be punished half the times it defects.",
                "With one action remaining, both types of agents choose to contribute to avoid being punished.",
                "With two actions to go, the altruistic type chooses to contribute, while the other defects.",
                "This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids.",
                "Because cj for the non-altruistic type is less, it prefers not to contribute.",
                "With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects.",
                "For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection.",
                "We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level 0 models, mentioned previously, to j (see Fig. 9).",
                "If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps.",
                "This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type.",
                "However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1.",
                "These results demonstrate that the behavior of our altruistic type resembles that found experimentally.",
                "The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic.",
                "We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection.",
                "The reciprocal types marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other.",
                "We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full.",
                "For this prior belief, i chooses to defect.",
                "On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)).",
                "This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from js action to contribute.",
                "Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step.",
                "Agent i therefore chooses to contribute to reciprocate js action.",
                "An analogous reasoning leads i to defect when it observes a meager pot.",
                "With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations.",
                "Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic). 5.3 Strategies in Two-Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2].",
                "Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck.",
                "While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands.",
                "To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit.",
                "Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot.",
                "During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to 8.",
                "Notice that, for example, if a lower numbered card is discarded, the probability of receiving a low card in exchange is now reduced.",
                "We show the level 1 I-ID for the simplified two-player Poker in Fig. 11.",
                "We considered two models (personality types) of agent j.",
                "The conservative type believes that it is likely that its opponent has a high numbered card in its hand.",
                "On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card.",
                "Thus, the two types differ in their beliefs over their opponents hand.",
                "In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution.",
                "With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand.",
                "This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange.",
                "The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one.",
                "Figure 11: (a) Level 1 I-ID of agent i.",
                "The observation reveals information about js hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a).",
                "The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in js hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. is own hand contains the card numbered 8.",
                "The agent starts by keeping its card.",
                "On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card.",
                "If i observes that j discarded its card into the L or H pile, i believes that j is aggressive.",
                "On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange.",
                "Because the probability of receiving a low card is high now, i chooses to keep its card.",
                "On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card.",
                "In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff.",
                "This is partly due to the fact that an observation of, say, L resets the agent is previous time step beliefs over js hand to the low numbered cards only. 6.",
                "DISCUSSION We showed how DIDs may be extended to I-DIDs that enable online sequential decision-making in uncertain multiagent settings.",
                "Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent is three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs.",
                "I-DIDs extend NIDs to allow sequential decision-making over multiple time steps in the presence of other interacting agents.",
                "I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs.",
                "We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality.",
                "Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work.",
                "The first author would like to acknowledge the support of a UGARF grant. 7.",
                "REFERENCES [1] R. J. Aumann.",
                "Interactive epistemology i: Knowledge.",
                "International Journal of Game Theory, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron.",
                "The challenge of poker.",
                "AIJ, 2001. [3] A. Brandenburger and E. Dekel.",
                "Hierarchies of beliefs and common knowledge.",
                "Journal of Economic Theory, 59:189-198, 1993. [4] C. Camerer.",
                "Behavioral Game Theory: Experiments in Strategic Interaction.",
                "Princeton University Press, 2003. [5] E. Fehr and S. Gachter.",
                "Cooperation and punishment in public goods experiments.",
                "American Economic Review, 90(4):980-994, 2000. [6] D. Fudenberg and D. K. Levine.",
                "The Theory of Learning in Games.",
                "MIT Press, 1998. [7] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [8] Y. Gal and A. Pfeffer.",
                "A language for modeling agents decision-making processes in games.",
                "In AAMAS, 2003. [9] P. Gmytrasiewicz and P. Doshi.",
                "A framework for sequential planning in multiagent settings.",
                "JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz and E. Durfee.",
                "Rational coordination in multi-agent environments.",
                "JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi.",
                "Games with incomplete information played by bayesian players.",
                "Management Science, 14(3):159-182, 1967. [12] R. A. Howard and J. E. Matheson.",
                "Influence diagrams.",
                "In R. A. Howard and J. E. Matheson, editors, The Principles and Applications of Decision Analysis.",
                "Strategic Decisions Group, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman, and A. Cassandra.",
                "Planning and acting in partially observable stochastic domains.",
                "Artificial Intelligence Journal, 2, 1998. [14] D. Koller and B. Milch.",
                "Multi-agent influence diagrams for representing and solving games.",
                "In IJCAI, pages 1027-1034, 2001. [15] K. Polich and P. Gmytrasiewicz.",
                "Interactive dynamic influence diagrams.",
                "In GTDT Workshop, AAMAS, 2006. [16] B.",
                "Rathnas., P. Doshi, and P. J. Gmytrasiewicz.",
                "Exact solutions to interactive pomdps using behavioral equivalence.",
                "In Autonomous Agents and Multi-Agent Systems Conference (AAMAS), 2006. [17] S. Russell and P. Norvig.",
                "Artificial Intelligence: A Modern Approach (Second Edition).",
                "Prentice Hall, 2003. [18] R. D. Shachter.",
                "Evaluating influence diagrams.",
                "Operations Research, 34(6):871-882, 1986. [19] D. Suryadi and P. Gmytrasiewicz.",
                "Learning models of other agents using influence diagrams.",
                "In UM, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "interactive dynamic influence diagram": {
            "translated_key": "Diagrama de influencia dinámica interactiva",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Graphical Models for Online Solutions to Interactive POMDPs Prashant Doshi Dept.",
                "of Computer Science University of Georgia Athens, GA 30602, USA pdoshi@cs.uga.edu Yifeng Zeng Dept. of Computer Science Aalborg University DK-9220 Aalborg, Denmark yfzeng@cs.aau.edu Qiongyu Chen Dept. of Computer Science National Univ. of Singapore 117543, Singapore chenqy@comp.nus.edu.sg ABSTRACT We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation.",
                "These graphical models called <br>interactive dynamic influence diagram</br>s (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents.",
                "Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Theory 1.",
                "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in partially observable multiagent environments.",
                "They generalize POMDPs [13] to multiagent settings by including the other agents computable models in the state space along with the states of the physical environment.",
                "The models encompass all information influencing the agents behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11].",
                "I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a decision-makers perspective in the interaction.",
                "In [15], Polich and Gmytrasiewicz introduced <br>interactive dynamic influence diagram</br>s (I-DIDs) as the computational representations of I-POMDPs.",
                "I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs contribute to a growing line of work [19] that includes multi-agent influence diagrams (MAIDs) [14], and more recently, networks of influence diagrams (NIDs) [8].",
                "These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agents actions and chance nodes capturing the agents private information.",
                "MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the independence structure.",
                "NIDs extend MAIDs to include agents uncertainty over the game being played and over models of the other agents.",
                "Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent.",
                "Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently.",
                "However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games.",
                "Matters are more complex when we consider interactions that are extended over time, where predictions about others future actions must be made using models that change as the agents act and observe.",
                "I-DIDs address this gap by allowing the representation of other agents models as the values of a special model node.",
                "Both, other agents models and the original agents beliefs over these models are updated over time using special-purpose implementations.",
                "In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID.",
                "Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently.",
                "Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I-DID by [15], and show that it could be replaced by traditional dependency links.",
                "In the previous representation of the I-DID, the update of the agents belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time.",
                "We explicate the semantics of this link by showing how it can be implemented using the traditional dependency links between the chance nodes that constitute the model nodes.",
                "The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs.",
                "We show how IDIDs may be used to model an agents uncertainty over others models, that may themselves be I-DIDs.",
                "Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others models.",
                "Analogous to DIDs, I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. 2.",
                "BACKGROUND: FINITELY NESTED IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents models as part of the state space [9].",
                "Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents models and their beliefs about others.",
                "For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j.",
                "A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment.",
                "Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj .",
                "Here, j is Bayes rational and OCj is js optimality criterion.",
                "SMj is the set of subintentional models of j.",
                "Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent.",
                "We give a recursive bottom-up construction of the interactive state space below.",
                "ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . .",
                "ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent is preferences over its interactive states.",
                "Usually only the physical states will matter.",
                "Agent is policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i.",
                "Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai). 2.1 Belief Update Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes.",
                "However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones.",
                "First, since the state of the physical environment depends on the actions of both agents, is prediction of how the physical state changes has to be made based on its prediction of js actions.",
                "Second, changes in js models have to be included in is belief update.",
                "Specifically, if j is intentional then an update of js beliefs due to its action and observation has to be included.",
                "In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief.",
                "If js model is subintentional, then js probable observations are appended to the observation history contained in the model.",
                "Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update.",
                "For a version of the belief update when js model is subintentional, see [9].",
                "If agent j is also modeled as an I-POMDP, then is belief update invokes js belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke is belief update and so on.",
                "This recursion in belief nesting bottoms out at the 0th level.",
                "At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9]. 2.2 Value Iteration Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)).",
                "Eq. 2 is a basis for value iteration in I-POMDPs.",
                "Agent is optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3.",
                "INTERACTIVEINFLUENCEDIAGRAMS A naive extension of influence diagrams (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes.",
                "However, this approach assumes that the agents actions are controlled using a probability distribution that does not change over time.",
                "Interactive influence diagrams (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node.",
                "We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon.",
                "We note that the probability distribution over the chance node, S, and the model node together represents agent is belief over its interactive states.",
                "In addition to the model 1 The 0th level model is a POMDP: Other agents actions are treated as exogenous events and folded into the T, O, and R functions.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j.",
                "The hexagon is the model node (Mj,l−1) whose structure we show in (b).",
                "Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ).",
                "Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the policy link in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model.",
                "In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs.",
                "The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2.",
                "Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional.",
                "Because the model node contains the alternative models of the other agent as its values, its representation is not trivial.",
                "In particular, some of the models within the node are I-IDs that when solved generate the agents optimal policy in their decision nodes.",
                "Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise.",
                "Borrowing insights from previous work [8], we observe that the model node and the dashed policy link that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b).",
                "The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj.",
                "Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent is beliefs.",
                "The conditional probability table of the chance node, Aj, is a multiplexer that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj].",
                "The values of Mod[Mj] denote the different models of j.",
                "In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1.",
                "The distribution over the node, Mod[Mj], is the agent is belief over the models of j given a physical state.",
                "For more agents, we will have as many model nodes as there are agents.",
                "Notice that Fig. 1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional dependency links.",
                "In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them.",
                "In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes.",
                "This allows I-IDs to be represented and implemented using conventional application tools that target IDs.",
                "Note that we may view the level l I-ID as a NID.",
                "Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2).",
                "If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID.",
                "Note that within the I-IDs (or IDs) at each level, there is only a single decision node.",
                "Thus, our NID does not contain any MAIDs.",
                "Figure 2: A level l I-ID represented as a NID.",
                "The probabilities assigned to the blocks of the NID are is beliefs over js models conditioned on a physical state. 3.2 Solution The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively.",
                "We start by solving the level 0 models, which, if intentional, are traditional IDs.",
                "Their solutions provide probability distributions over the other agents actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID.",
                "The mapping from the level 0 models decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c).",
                "During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj].",
                "As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent is belief over the models of j conditioned on the physical state.",
                "The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j.",
                "Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18].",
                "This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief.",
                "Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agents current belief is known. 4.",
                "INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic influence diagrams (I-DIDs) extend I-IDs (and NIDs) to allow sequential decision-making over several time steps.",
                "Just as DIDs are structured graphical representations of POMDPs, I-DIDs are the graphical online analogs for finitely nested I-POMDPs.",
                "I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents. 4.1 Syntax We depict a general two time-slice I-DID in Fig. 3(a).",
                "In addition to the model nodes and the dashed policy link, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a).",
                "We explained the semantics of the model node and the policy link in the previous section; we describe the model updates next.",
                "The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1.",
                "Recall from Section 2 that an agents intentional model includes its belief.",
                "Because the agents act and receive observations, their models are updated to reflect their changed beliefs.",
                "Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models.",
                "Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models.",
                "Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model.",
                "These steps are a part of agent is belief update formalized using Eq. 1.",
                "In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID.",
                "If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ).",
                "These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation.",
                "The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously.",
                "Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed.",
                "The probability that js updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node.",
                "In order to obtain the probability of js possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ].",
                "Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a multiplexer modulated by Mod[Mt j ].",
                "Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1.",
                "Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1.",
                "Notice that the model update link may be replaced by the dependency links between the chance nodes that constitute the model nodes in the two time slices.",
                "In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them.",
                "Chance nodes and dependency links that not in bold are standard, usually found in DIDs.",
                "Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents js observation at time t + 1.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links.",
                "We note that the possible set of models of the other agent j grows exponentially with the number of time steps.",
                "For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node. 4.2 Solution Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively.",
                "For the purpose of illustration, let l=1 and T=2.",
                "The solution method uses the standard look-ahead technique, projecting the agents action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step.",
                "Because agent i has a belief over js models as well, the lookahead includes finding out the possible models that j could have in the future.",
                "Consequently, each of js subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions.",
                "These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b).",
                "We note the recursive nature of this solution: in solving agent is level 1 I-DID, js level 0 DIDs must be solved.",
                "If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner.",
                "We briefly outline the recursive algorithm for solving agent is Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase 1.",
                "For t from 1 to T − 1 do 2.",
                "If l ≥ 1 then Populate Mt+1 j,l−1 3.",
                "For each mt j in Range(Mt j,l−1) do 4.",
                "Recursively call algorithm with the l − 1 I-ID (or ID) that represents mt j and the horizon, T − t + 1 5.",
                "Map the decision node of the solved I-ID (or ID), OPT(mt j), to a chance node Aj 6.",
                "For each aj in OPT(mt j) do 7.",
                "For each oj in Oj (part of mt j) do 8.",
                "Update js belief, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← New I-ID (or ID) with bt+1 j as the initial belief 10.",
                "Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11.",
                "Add the model node, Mt+1 j,l−1, and the dependency links between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b)) 12.",
                "Add the chance, decision, and utility nodes for t + 1 time slice and the dependency links between them 13.",
                "Establish the CPTs for each chance node and utility node Look-Ahead Phase 14.",
                "Apply the standard look-ahead and backup method to solve the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5.",
                "We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the dependency links and the conditional probability tables for each node.",
                "We particularly focus on establishing and populating the model nodes (lines 3-11).",
                "Note that Range(·) returns the values (lower level models) of the random variable given as input (model node).",
                "In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs.",
                "Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents.",
                "As we mentioned previously, the 0-th level models are the traditional DIDs.",
                "Their solutions provide probability distributions over actions of the agent modeled at that level to I-DIDs at level 1.",
                "Given probability distributions over other agents actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models.",
                "Assume that the number of models considered at each level is bound by a number, M. Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs. 5.",
                "EXAMPLE APPLICATIONS To illustrate the usefulness of I-DIDs, we apply them to three problem domains.",
                "We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it. 5.1 Followership-Leadership in the Multiagent Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9].",
                "The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L).",
                "In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agents opening one of the doors.",
                "When any door is opened, the tiger persists in its original location with a probability of 95%.",
                "Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%.",
                "Agent j, on the other hand, hears growls with a reliability of 95%.",
                "Thus, the setting is such that agent i hears agent j opening doors more reliably than the tigers growls.",
                "This suggests that i could use js actions as an indication of the location of the tiger, as we discuss below.",
                "Each agents preferences are as in the single agent game discussed in [13].",
                "The transition, observation, and reward functions are shown in [16].",
                "A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions.",
                "In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship.",
                "In particular, we analyze the situational and epistemological conditions sufficient for their emergence.",
                "The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the others actions in order to maximize its payoffs.",
                "Let us consider a particular setting of the tiger problem in which agent i believes that js preferences are aligned with its own - both of them just want to get the gold - and js hearing is more reliable in comparison to itself.",
                "As an example, suppose that j, on listening can discern the tigers location 95% of the times compared to is 65% accuracy.",
                "Additionally, agent i does not have any initial information about the tigers location.",
                "In other words, is single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger.",
                "In addition, i considers two models of j, which differ in js flat level 0 initial beliefs.",
                "This is represented in the level 1 I-ID shown in Fig. 6(a).",
                "According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)).",
                "Agent i is undecided on these two models of j.",
                "If we vary is hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior.",
                "If is probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow js actions: i opens the same door that j opened previously iff is own assessment of the tigers location confirms js pick.",
                "If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)).",
                "Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem.",
                "Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations.",
                "We observed that a single level of belief nesting - beliefs about the others models - was sufficient for followership to emerge in the tiger problem.",
                "However, the epistemological requirements for the emergence of leadership are more complex.",
                "For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i.",
                "As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow js actions over time.",
                "Agent j emerges as a leader if it believes that i will follow it, which implies that js belief must be nested two levels deep to enable it to recognize its leadership role.",
                "Realizing that i will follow presents j with an opportunity to influence is actions in the benefit of the collective good or its self-interest alone.",
                "For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original.",
                "If j alone selects the correct door, it gets the payoff of 10.",
                "On the other hand, if both agents pick the wrong door, their penalties are cut in half.",
                "In this setting, it is in both js best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader.",
                "However, consider a slightly different problem in which j gains from is loss and is penalized if i gains.",
                "Specifically, let is payoff be subtracted from js, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then is loss of 100 becomes js gain.",
                "Agent j believes that i incorrectly thinks that js preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is.",
                "Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following js actions.",
                "We show is normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a).",
                "The policy demonstrates that i will blindly follow js actions.",
                "Since the tiger persists in its original location with a probability of 0.95, i will select the same door again.",
                "If j begins the game with a 99% probability that the tiger is on the right, solving js I-DID nested two levels deep, results in the policy shown in Fig. 8(b).",
                "Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL.",
                "Agent js intention is to deceive i who, it believes, will follow js actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest.",
                "Figure 8: Emergence of deception between agents in the tiger problem.",
                "Behaviors of interest are in bold. * denotes as before. (a) Agent is policy demonstrating that it will blindly follow js actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i. 5.2 Altruism and Reciprocity in the Public Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves.",
                "Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot.",
                "However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes.",
                "Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others contributions.",
                "However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions.",
                "The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4].",
                "Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting.",
                "These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others.",
                "For simplicity, we assume that the game is played between M = 2 agents, i and j.",
                "Let each agent be initially endowed with XT amount of resources.",
                "While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions.",
                "Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute.",
                "The latter action is deThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D).",
                "We assume that the actions are not observable to others.",
                "The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return.",
                "We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain.",
                "Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment.",
                "In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment.",
                "Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent.",
                "For simplicity, we assume that the cost of punishing is same for both the agents.",
                "The one-shot PG game with punishment is shown in Table. 1.",
                "Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action.",
                "If P < XT − ciXT , then defection is the dominating action for both.",
                "If P = XT − ciXT , then the game is not dominance-solvable.",
                "Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a).",
                "We formulate a sequential version of the PG problem with punishment from the perspective of agent i.",
                "Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents.",
                "Each agent may contribute a fixed amount, xc, or defect.",
                "An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot.",
                "Notice that the observations are also indirectly indicative of agent js actions because the state of the public pot is influenced by them.",
                "The amount of resources in agent is private pot, is perfectly observable to i.",
                "The payoffs are analogous to Table. 1.",
                "Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)).",
                "Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect.",
                "Let xc = 1 and the level 0 agent be punished half the times it defects.",
                "With one action remaining, both types of agents choose to contribute to avoid being punished.",
                "With two actions to go, the altruistic type chooses to contribute, while the other defects.",
                "This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids.",
                "Because cj for the non-altruistic type is less, it prefers not to contribute.",
                "With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects.",
                "For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection.",
                "We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level 0 models, mentioned previously, to j (see Fig. 9).",
                "If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps.",
                "This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type.",
                "However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1.",
                "These results demonstrate that the behavior of our altruistic type resembles that found experimentally.",
                "The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic.",
                "We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection.",
                "The reciprocal types marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other.",
                "We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full.",
                "For this prior belief, i chooses to defect.",
                "On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)).",
                "This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from js action to contribute.",
                "Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step.",
                "Agent i therefore chooses to contribute to reciprocate js action.",
                "An analogous reasoning leads i to defect when it observes a meager pot.",
                "With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations.",
                "Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic). 5.3 Strategies in Two-Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2].",
                "Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck.",
                "While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands.",
                "To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit.",
                "Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot.",
                "During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to 8.",
                "Notice that, for example, if a lower numbered card is discarded, the probability of receiving a low card in exchange is now reduced.",
                "We show the level 1 I-ID for the simplified two-player Poker in Fig. 11.",
                "We considered two models (personality types) of agent j.",
                "The conservative type believes that it is likely that its opponent has a high numbered card in its hand.",
                "On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card.",
                "Thus, the two types differ in their beliefs over their opponents hand.",
                "In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution.",
                "With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand.",
                "This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange.",
                "The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one.",
                "Figure 11: (a) Level 1 I-ID of agent i.",
                "The observation reveals information about js hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a).",
                "The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in js hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. is own hand contains the card numbered 8.",
                "The agent starts by keeping its card.",
                "On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card.",
                "If i observes that j discarded its card into the L or H pile, i believes that j is aggressive.",
                "On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange.",
                "Because the probability of receiving a low card is high now, i chooses to keep its card.",
                "On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card.",
                "In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff.",
                "This is partly due to the fact that an observation of, say, L resets the agent is previous time step beliefs over js hand to the low numbered cards only. 6.",
                "DISCUSSION We showed how DIDs may be extended to I-DIDs that enable online sequential decision-making in uncertain multiagent settings.",
                "Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent is three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs.",
                "I-DIDs extend NIDs to allow sequential decision-making over multiple time steps in the presence of other interacting agents.",
                "I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs.",
                "We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality.",
                "Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work.",
                "The first author would like to acknowledge the support of a UGARF grant. 7.",
                "REFERENCES [1] R. J. Aumann.",
                "Interactive epistemology i: Knowledge.",
                "International Journal of Game Theory, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron.",
                "The challenge of poker.",
                "AIJ, 2001. [3] A. Brandenburger and E. Dekel.",
                "Hierarchies of beliefs and common knowledge.",
                "Journal of Economic Theory, 59:189-198, 1993. [4] C. Camerer.",
                "Behavioral Game Theory: Experiments in Strategic Interaction.",
                "Princeton University Press, 2003. [5] E. Fehr and S. Gachter.",
                "Cooperation and punishment in public goods experiments.",
                "American Economic Review, 90(4):980-994, 2000. [6] D. Fudenberg and D. K. Levine.",
                "The Theory of Learning in Games.",
                "MIT Press, 1998. [7] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [8] Y. Gal and A. Pfeffer.",
                "A language for modeling agents decision-making processes in games.",
                "In AAMAS, 2003. [9] P. Gmytrasiewicz and P. Doshi.",
                "A framework for sequential planning in multiagent settings.",
                "JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz and E. Durfee.",
                "Rational coordination in multi-agent environments.",
                "JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi.",
                "Games with incomplete information played by bayesian players.",
                "Management Science, 14(3):159-182, 1967. [12] R. A. Howard and J. E. Matheson.",
                "Influence diagrams.",
                "In R. A. Howard and J. E. Matheson, editors, The Principles and Applications of Decision Analysis.",
                "Strategic Decisions Group, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman, and A. Cassandra.",
                "Planning and acting in partially observable stochastic domains.",
                "Artificial Intelligence Journal, 2, 1998. [14] D. Koller and B. Milch.",
                "Multi-agent influence diagrams for representing and solving games.",
                "In IJCAI, pages 1027-1034, 2001. [15] K. Polich and P. Gmytrasiewicz.",
                "Interactive dynamic influence diagrams.",
                "In GTDT Workshop, AAMAS, 2006. [16] B.",
                "Rathnas., P. Doshi, and P. J. Gmytrasiewicz.",
                "Exact solutions to interactive pomdps using behavioral equivalence.",
                "In Autonomous Agents and Multi-Agent Systems Conference (AAMAS), 2006. [17] S. Russell and P. Norvig.",
                "Artificial Intelligence: A Modern Approach (Second Edition).",
                "Prentice Hall, 2003. [18] R. D. Shachter.",
                "Evaluating influence diagrams.",
                "Operations Research, 34(6):871-882, 1986. [19] D. Suryadi and P. Gmytrasiewicz.",
                "Learning models of other agents using influence diagrams.",
                "In UM, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Estos modelos gráficos llamados \"diagrama de influencia dinámica interactiva\" s (I-DID) buscan modelar explícitamente la estructura que a menudo está presente en problemas del mundo real al descomponer la situación en las variables de casualidad y decisión, y las dependencias entre las variables.",
                "En [15], Polich y Gmytrasiewicz introdujeron el \"diagrama de influencia dinámica interactiva\" s (I-DID) como las representaciones computacionales de I-POMDPS."
            ],
            "translated_text": "",
            "candidates": [
                "Diagrama de influencia dinámica interactiva",
                "diagrama de influencia dinámica interactiva",
                "Diagrama de influencia dinámica interactiva",
                "diagrama de influencia dinámica interactiva"
            ],
            "error": []
        },
        "sequential decision-making": {
            "translated_key": "toma de decisiones secuenciales",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Graphical Models for Online Solutions to Interactive POMDPs Prashant Doshi Dept.",
                "of Computer Science University of Georgia Athens, GA 30602, USA pdoshi@cs.uga.edu Yifeng Zeng Dept. of Computer Science Aalborg University DK-9220 Aalborg, Denmark yfzeng@cs.aau.edu Qiongyu Chen Dept. of Computer Science National Univ. of Singapore 117543, Singapore chenqy@comp.nus.edu.sg ABSTRACT We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation.",
                "These graphical models called interactive dynamic influence diagrams (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents.",
                "Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Theory 1.",
                "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for <br>sequential decision-making</br> in partially observable multiagent environments.",
                "They generalize POMDPs [13] to multiagent settings by including the other agents computable models in the state space along with the states of the physical environment.",
                "The models encompass all information influencing the agents behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11].",
                "I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a decision-makers perspective in the interaction.",
                "In [15], Polich and Gmytrasiewicz introduced interactive dynamic influence diagrams (I-DIDs) as the computational representations of I-POMDPs.",
                "I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs contribute to a growing line of work [19] that includes multi-agent influence diagrams (MAIDs) [14], and more recently, networks of influence diagrams (NIDs) [8].",
                "These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agents actions and chance nodes capturing the agents private information.",
                "MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the independence structure.",
                "NIDs extend MAIDs to include agents uncertainty over the game being played and over models of the other agents.",
                "Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent.",
                "Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently.",
                "However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games.",
                "Matters are more complex when we consider interactions that are extended over time, where predictions about others future actions must be made using models that change as the agents act and observe.",
                "I-DIDs address this gap by allowing the representation of other agents models as the values of a special model node.",
                "Both, other agents models and the original agents beliefs over these models are updated over time using special-purpose implementations.",
                "In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID.",
                "Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently.",
                "Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I-DID by [15], and show that it could be replaced by traditional dependency links.",
                "In the previous representation of the I-DID, the update of the agents belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time.",
                "We explicate the semantics of this link by showing how it can be implemented using the traditional dependency links between the chance nodes that constitute the model nodes.",
                "The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs.",
                "We show how IDIDs may be used to model an agents uncertainty over others models, that may themselves be I-DIDs.",
                "Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others models.",
                "Analogous to DIDs, I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. 2.",
                "BACKGROUND: FINITELY NESTED IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents models as part of the state space [9].",
                "Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents models and their beliefs about others.",
                "For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j.",
                "A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment.",
                "Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj .",
                "Here, j is Bayes rational and OCj is js optimality criterion.",
                "SMj is the set of subintentional models of j.",
                "Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent.",
                "We give a recursive bottom-up construction of the interactive state space below.",
                "ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . .",
                "ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent is preferences over its interactive states.",
                "Usually only the physical states will matter.",
                "Agent is policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i.",
                "Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai). 2.1 Belief Update Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes.",
                "However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones.",
                "First, since the state of the physical environment depends on the actions of both agents, is prediction of how the physical state changes has to be made based on its prediction of js actions.",
                "Second, changes in js models have to be included in is belief update.",
                "Specifically, if j is intentional then an update of js beliefs due to its action and observation has to be included.",
                "In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief.",
                "If js model is subintentional, then js probable observations are appended to the observation history contained in the model.",
                "Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update.",
                "For a version of the belief update when js model is subintentional, see [9].",
                "If agent j is also modeled as an I-POMDP, then is belief update invokes js belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke is belief update and so on.",
                "This recursion in belief nesting bottoms out at the 0th level.",
                "At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9]. 2.2 Value Iteration Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)).",
                "Eq. 2 is a basis for value iteration in I-POMDPs.",
                "Agent is optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3.",
                "INTERACTIVEINFLUENCEDIAGRAMS A naive extension of influence diagrams (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes.",
                "However, this approach assumes that the agents actions are controlled using a probability distribution that does not change over time.",
                "Interactive influence diagrams (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node.",
                "We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon.",
                "We note that the probability distribution over the chance node, S, and the model node together represents agent is belief over its interactive states.",
                "In addition to the model 1 The 0th level model is a POMDP: Other agents actions are treated as exogenous events and folded into the T, O, and R functions.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j.",
                "The hexagon is the model node (Mj,l−1) whose structure we show in (b).",
                "Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ).",
                "Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the policy link in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model.",
                "In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs.",
                "The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2.",
                "Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional.",
                "Because the model node contains the alternative models of the other agent as its values, its representation is not trivial.",
                "In particular, some of the models within the node are I-IDs that when solved generate the agents optimal policy in their decision nodes.",
                "Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise.",
                "Borrowing insights from previous work [8], we observe that the model node and the dashed policy link that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b).",
                "The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj.",
                "Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent is beliefs.",
                "The conditional probability table of the chance node, Aj, is a multiplexer that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj].",
                "The values of Mod[Mj] denote the different models of j.",
                "In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1.",
                "The distribution over the node, Mod[Mj], is the agent is belief over the models of j given a physical state.",
                "For more agents, we will have as many model nodes as there are agents.",
                "Notice that Fig. 1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional dependency links.",
                "In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them.",
                "In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes.",
                "This allows I-IDs to be represented and implemented using conventional application tools that target IDs.",
                "Note that we may view the level l I-ID as a NID.",
                "Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2).",
                "If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID.",
                "Note that within the I-IDs (or IDs) at each level, there is only a single decision node.",
                "Thus, our NID does not contain any MAIDs.",
                "Figure 2: A level l I-ID represented as a NID.",
                "The probabilities assigned to the blocks of the NID are is beliefs over js models conditioned on a physical state. 3.2 Solution The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively.",
                "We start by solving the level 0 models, which, if intentional, are traditional IDs.",
                "Their solutions provide probability distributions over the other agents actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID.",
                "The mapping from the level 0 models decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c).",
                "During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj].",
                "As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent is belief over the models of j conditioned on the physical state.",
                "The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j.",
                "Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18].",
                "This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief.",
                "Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agents current belief is known. 4.",
                "INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic influence diagrams (I-DIDs) extend I-IDs (and NIDs) to allow <br>sequential decision-making</br> over several time steps.",
                "Just as DIDs are structured graphical representations of POMDPs, I-DIDs are the graphical online analogs for finitely nested I-POMDPs.",
                "I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents. 4.1 Syntax We depict a general two time-slice I-DID in Fig. 3(a).",
                "In addition to the model nodes and the dashed policy link, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a).",
                "We explained the semantics of the model node and the policy link in the previous section; we describe the model updates next.",
                "The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1.",
                "Recall from Section 2 that an agents intentional model includes its belief.",
                "Because the agents act and receive observations, their models are updated to reflect their changed beliefs.",
                "Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models.",
                "Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models.",
                "Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model.",
                "These steps are a part of agent is belief update formalized using Eq. 1.",
                "In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID.",
                "If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ).",
                "These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation.",
                "The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously.",
                "Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed.",
                "The probability that js updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node.",
                "In order to obtain the probability of js possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ].",
                "Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a multiplexer modulated by Mod[Mt j ].",
                "Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1.",
                "Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1.",
                "Notice that the model update link may be replaced by the dependency links between the chance nodes that constitute the model nodes in the two time slices.",
                "In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them.",
                "Chance nodes and dependency links that not in bold are standard, usually found in DIDs.",
                "Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents js observation at time t + 1.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links.",
                "We note that the possible set of models of the other agent j grows exponentially with the number of time steps.",
                "For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node. 4.2 Solution Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively.",
                "For the purpose of illustration, let l=1 and T=2.",
                "The solution method uses the standard look-ahead technique, projecting the agents action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step.",
                "Because agent i has a belief over js models as well, the lookahead includes finding out the possible models that j could have in the future.",
                "Consequently, each of js subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions.",
                "These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b).",
                "We note the recursive nature of this solution: in solving agent is level 1 I-DID, js level 0 DIDs must be solved.",
                "If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner.",
                "We briefly outline the recursive algorithm for solving agent is Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase 1.",
                "For t from 1 to T − 1 do 2.",
                "If l ≥ 1 then Populate Mt+1 j,l−1 3.",
                "For each mt j in Range(Mt j,l−1) do 4.",
                "Recursively call algorithm with the l − 1 I-ID (or ID) that represents mt j and the horizon, T − t + 1 5.",
                "Map the decision node of the solved I-ID (or ID), OPT(mt j), to a chance node Aj 6.",
                "For each aj in OPT(mt j) do 7.",
                "For each oj in Oj (part of mt j) do 8.",
                "Update js belief, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← New I-ID (or ID) with bt+1 j as the initial belief 10.",
                "Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11.",
                "Add the model node, Mt+1 j,l−1, and the dependency links between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b)) 12.",
                "Add the chance, decision, and utility nodes for t + 1 time slice and the dependency links between them 13.",
                "Establish the CPTs for each chance node and utility node Look-Ahead Phase 14.",
                "Apply the standard look-ahead and backup method to solve the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5.",
                "We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the dependency links and the conditional probability tables for each node.",
                "We particularly focus on establishing and populating the model nodes (lines 3-11).",
                "Note that Range(·) returns the values (lower level models) of the random variable given as input (model node).",
                "In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs.",
                "Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents.",
                "As we mentioned previously, the 0-th level models are the traditional DIDs.",
                "Their solutions provide probability distributions over actions of the agent modeled at that level to I-DIDs at level 1.",
                "Given probability distributions over other agents actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models.",
                "Assume that the number of models considered at each level is bound by a number, M. Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs. 5.",
                "EXAMPLE APPLICATIONS To illustrate the usefulness of I-DIDs, we apply them to three problem domains.",
                "We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it. 5.1 Followership-Leadership in the Multiagent Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9].",
                "The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L).",
                "In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agents opening one of the doors.",
                "When any door is opened, the tiger persists in its original location with a probability of 95%.",
                "Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%.",
                "Agent j, on the other hand, hears growls with a reliability of 95%.",
                "Thus, the setting is such that agent i hears agent j opening doors more reliably than the tigers growls.",
                "This suggests that i could use js actions as an indication of the location of the tiger, as we discuss below.",
                "Each agents preferences are as in the single agent game discussed in [13].",
                "The transition, observation, and reward functions are shown in [16].",
                "A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions.",
                "In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship.",
                "In particular, we analyze the situational and epistemological conditions sufficient for their emergence.",
                "The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the others actions in order to maximize its payoffs.",
                "Let us consider a particular setting of the tiger problem in which agent i believes that js preferences are aligned with its own - both of them just want to get the gold - and js hearing is more reliable in comparison to itself.",
                "As an example, suppose that j, on listening can discern the tigers location 95% of the times compared to is 65% accuracy.",
                "Additionally, agent i does not have any initial information about the tigers location.",
                "In other words, is single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger.",
                "In addition, i considers two models of j, which differ in js flat level 0 initial beliefs.",
                "This is represented in the level 1 I-ID shown in Fig. 6(a).",
                "According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)).",
                "Agent i is undecided on these two models of j.",
                "If we vary is hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior.",
                "If is probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow js actions: i opens the same door that j opened previously iff is own assessment of the tigers location confirms js pick.",
                "If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)).",
                "Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem.",
                "Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations.",
                "We observed that a single level of belief nesting - beliefs about the others models - was sufficient for followership to emerge in the tiger problem.",
                "However, the epistemological requirements for the emergence of leadership are more complex.",
                "For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i.",
                "As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow js actions over time.",
                "Agent j emerges as a leader if it believes that i will follow it, which implies that js belief must be nested two levels deep to enable it to recognize its leadership role.",
                "Realizing that i will follow presents j with an opportunity to influence is actions in the benefit of the collective good or its self-interest alone.",
                "For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original.",
                "If j alone selects the correct door, it gets the payoff of 10.",
                "On the other hand, if both agents pick the wrong door, their penalties are cut in half.",
                "In this setting, it is in both js best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader.",
                "However, consider a slightly different problem in which j gains from is loss and is penalized if i gains.",
                "Specifically, let is payoff be subtracted from js, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then is loss of 100 becomes js gain.",
                "Agent j believes that i incorrectly thinks that js preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is.",
                "Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following js actions.",
                "We show is normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a).",
                "The policy demonstrates that i will blindly follow js actions.",
                "Since the tiger persists in its original location with a probability of 0.95, i will select the same door again.",
                "If j begins the game with a 99% probability that the tiger is on the right, solving js I-DID nested two levels deep, results in the policy shown in Fig. 8(b).",
                "Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL.",
                "Agent js intention is to deceive i who, it believes, will follow js actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest.",
                "Figure 8: Emergence of deception between agents in the tiger problem.",
                "Behaviors of interest are in bold. * denotes as before. (a) Agent is policy demonstrating that it will blindly follow js actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i. 5.2 Altruism and Reciprocity in the Public Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves.",
                "Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot.",
                "However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes.",
                "Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others contributions.",
                "However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions.",
                "The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4].",
                "Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting.",
                "These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others.",
                "For simplicity, we assume that the game is played between M = 2 agents, i and j.",
                "Let each agent be initially endowed with XT amount of resources.",
                "While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions.",
                "Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute.",
                "The latter action is deThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D).",
                "We assume that the actions are not observable to others.",
                "The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return.",
                "We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain.",
                "Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment.",
                "In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment.",
                "Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent.",
                "For simplicity, we assume that the cost of punishing is same for both the agents.",
                "The one-shot PG game with punishment is shown in Table. 1.",
                "Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action.",
                "If P < XT − ciXT , then defection is the dominating action for both.",
                "If P = XT − ciXT , then the game is not dominance-solvable.",
                "Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a).",
                "We formulate a sequential version of the PG problem with punishment from the perspective of agent i.",
                "Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents.",
                "Each agent may contribute a fixed amount, xc, or defect.",
                "An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot.",
                "Notice that the observations are also indirectly indicative of agent js actions because the state of the public pot is influenced by them.",
                "The amount of resources in agent is private pot, is perfectly observable to i.",
                "The payoffs are analogous to Table. 1.",
                "Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)).",
                "Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect.",
                "Let xc = 1 and the level 0 agent be punished half the times it defects.",
                "With one action remaining, both types of agents choose to contribute to avoid being punished.",
                "With two actions to go, the altruistic type chooses to contribute, while the other defects.",
                "This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids.",
                "Because cj for the non-altruistic type is less, it prefers not to contribute.",
                "With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects.",
                "For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection.",
                "We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level 0 models, mentioned previously, to j (see Fig. 9).",
                "If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps.",
                "This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type.",
                "However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1.",
                "These results demonstrate that the behavior of our altruistic type resembles that found experimentally.",
                "The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic.",
                "We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection.",
                "The reciprocal types marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other.",
                "We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full.",
                "For this prior belief, i chooses to defect.",
                "On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)).",
                "This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from js action to contribute.",
                "Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step.",
                "Agent i therefore chooses to contribute to reciprocate js action.",
                "An analogous reasoning leads i to defect when it observes a meager pot.",
                "With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations.",
                "Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic). 5.3 Strategies in Two-Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2].",
                "Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck.",
                "While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands.",
                "To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit.",
                "Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot.",
                "During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to 8.",
                "Notice that, for example, if a lower numbered card is discarded, the probability of receiving a low card in exchange is now reduced.",
                "We show the level 1 I-ID for the simplified two-player Poker in Fig. 11.",
                "We considered two models (personality types) of agent j.",
                "The conservative type believes that it is likely that its opponent has a high numbered card in its hand.",
                "On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card.",
                "Thus, the two types differ in their beliefs over their opponents hand.",
                "In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution.",
                "With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand.",
                "This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange.",
                "The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one.",
                "Figure 11: (a) Level 1 I-ID of agent i.",
                "The observation reveals information about js hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a).",
                "The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in js hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. is own hand contains the card numbered 8.",
                "The agent starts by keeping its card.",
                "On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card.",
                "If i observes that j discarded its card into the L or H pile, i believes that j is aggressive.",
                "On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange.",
                "Because the probability of receiving a low card is high now, i chooses to keep its card.",
                "On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card.",
                "In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff.",
                "This is partly due to the fact that an observation of, say, L resets the agent is previous time step beliefs over js hand to the low numbered cards only. 6.",
                "DISCUSSION We showed how DIDs may be extended to I-DIDs that enable online <br>sequential decision-making</br> in uncertain multiagent settings.",
                "Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent is three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs.",
                "I-DIDs extend NIDs to allow <br>sequential decision-making</br> over multiple time steps in the presence of other interacting agents.",
                "I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs.",
                "We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality.",
                "Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work.",
                "The first author would like to acknowledge the support of a UGARF grant. 7.",
                "REFERENCES [1] R. J. Aumann.",
                "Interactive epistemology i: Knowledge.",
                "International Journal of Game Theory, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron.",
                "The challenge of poker.",
                "AIJ, 2001. [3] A. Brandenburger and E. Dekel.",
                "Hierarchies of beliefs and common knowledge.",
                "Journal of Economic Theory, 59:189-198, 1993. [4] C. Camerer.",
                "Behavioral Game Theory: Experiments in Strategic Interaction.",
                "Princeton University Press, 2003. [5] E. Fehr and S. Gachter.",
                "Cooperation and punishment in public goods experiments.",
                "American Economic Review, 90(4):980-994, 2000. [6] D. Fudenberg and D. K. Levine.",
                "The Theory of Learning in Games.",
                "MIT Press, 1998. [7] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [8] Y. Gal and A. Pfeffer.",
                "A language for modeling agents decision-making processes in games.",
                "In AAMAS, 2003. [9] P. Gmytrasiewicz and P. Doshi.",
                "A framework for sequential planning in multiagent settings.",
                "JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz and E. Durfee.",
                "Rational coordination in multi-agent environments.",
                "JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi.",
                "Games with incomplete information played by bayesian players.",
                "Management Science, 14(3):159-182, 1967. [12] R. A. Howard and J. E. Matheson.",
                "Influence diagrams.",
                "In R. A. Howard and J. E. Matheson, editors, The Principles and Applications of Decision Analysis.",
                "Strategic Decisions Group, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman, and A. Cassandra.",
                "Planning and acting in partially observable stochastic domains.",
                "Artificial Intelligence Journal, 2, 1998. [14] D. Koller and B. Milch.",
                "Multi-agent influence diagrams for representing and solving games.",
                "In IJCAI, pages 1027-1034, 2001. [15] K. Polich and P. Gmytrasiewicz.",
                "Interactive dynamic influence diagrams.",
                "In GTDT Workshop, AAMAS, 2006. [16] B.",
                "Rathnas., P. Doshi, and P. J. Gmytrasiewicz.",
                "Exact solutions to interactive pomdps using behavioral equivalence.",
                "In Autonomous Agents and Multi-Agent Systems Conference (AAMAS), 2006. [17] S. Russell and P. Norvig.",
                "Artificial Intelligence: A Modern Approach (Second Edition).",
                "Prentice Hall, 2003. [18] R. D. Shachter.",
                "Evaluating influence diagrams.",
                "Operations Research, 34(6):871-882, 1986. [19] D. Suryadi and P. Gmytrasiewicz.",
                "Learning models of other agents using influence diagrams.",
                "In UM, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Introducción Los procesos de decisión Markov parcialmente observables interactivos (IPOMDPS) [9] proporcionan un marco para la \"toma de decisiones secuenciales\" en entornos multiagentes parcialmente observables.",
                "Diagramas de influencia dinámica interactiva Los diagramas de influencia dinámica interactiva (I-DID) extienden I-IDS (y NIDS) para permitir la \"toma de decisiones secuenciales\" en varios pasos de tiempo.",
                "Discusión Mostramos cómo los DIDS pueden extenderse a I-DID que permiten la \"toma de decisiones secuenciales\" en línea en entornos múltiples inciertos.",
                "Los I-DID extienden NIDS para permitir la \"toma de decisiones secuenciales\" en múltiples pasos de tiempo en presencia de otros agentes interactivos."
            ],
            "translated_text": "",
            "candidates": [
                "toma de decisiones secuenciales",
                "toma de decisiones secuenciales",
                "toma de decisiones secuenciales",
                "toma de decisiones secuenciales",
                "toma de decisiones secuenciales",
                "toma de decisiones secuenciales",
                "toma de decisiones secuenciales",
                "toma de decisiones secuenciales"
            ],
            "error": []
        },
        "partially observable multiagent environment": {
            "translated_key": "entorno multiagente parcialmente observable",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Graphical Models for Online Solutions to Interactive POMDPs Prashant Doshi Dept.",
                "of Computer Science University of Georgia Athens, GA 30602, USA pdoshi@cs.uga.edu Yifeng Zeng Dept. of Computer Science Aalborg University DK-9220 Aalborg, Denmark yfzeng@cs.aau.edu Qiongyu Chen Dept. of Computer Science National Univ. of Singapore 117543, Singapore chenqy@comp.nus.edu.sg ABSTRACT We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation.",
                "These graphical models called interactive dynamic influence diagrams (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents.",
                "Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Theory 1.",
                "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in <br>partially observable multiagent environment</br>s.",
                "They generalize POMDPs [13] to multiagent settings by including the other agents computable models in the state space along with the states of the physical environment.",
                "The models encompass all information influencing the agents behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11].",
                "I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a decision-makers perspective in the interaction.",
                "In [15], Polich and Gmytrasiewicz introduced interactive dynamic influence diagrams (I-DIDs) as the computational representations of I-POMDPs.",
                "I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs contribute to a growing line of work [19] that includes multi-agent influence diagrams (MAIDs) [14], and more recently, networks of influence diagrams (NIDs) [8].",
                "These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agents actions and chance nodes capturing the agents private information.",
                "MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the independence structure.",
                "NIDs extend MAIDs to include agents uncertainty over the game being played and over models of the other agents.",
                "Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent.",
                "Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently.",
                "However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games.",
                "Matters are more complex when we consider interactions that are extended over time, where predictions about others future actions must be made using models that change as the agents act and observe.",
                "I-DIDs address this gap by allowing the representation of other agents models as the values of a special model node.",
                "Both, other agents models and the original agents beliefs over these models are updated over time using special-purpose implementations.",
                "In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID.",
                "Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently.",
                "Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I-DID by [15], and show that it could be replaced by traditional dependency links.",
                "In the previous representation of the I-DID, the update of the agents belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time.",
                "We explicate the semantics of this link by showing how it can be implemented using the traditional dependency links between the chance nodes that constitute the model nodes.",
                "The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs.",
                "We show how IDIDs may be used to model an agents uncertainty over others models, that may themselves be I-DIDs.",
                "Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others models.",
                "Analogous to DIDs, I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. 2.",
                "BACKGROUND: FINITELY NESTED IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents models as part of the state space [9].",
                "Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents models and their beliefs about others.",
                "For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j.",
                "A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment.",
                "Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj .",
                "Here, j is Bayes rational and OCj is js optimality criterion.",
                "SMj is the set of subintentional models of j.",
                "Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent.",
                "We give a recursive bottom-up construction of the interactive state space below.",
                "ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . .",
                "ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent is preferences over its interactive states.",
                "Usually only the physical states will matter.",
                "Agent is policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i.",
                "Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai). 2.1 Belief Update Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes.",
                "However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones.",
                "First, since the state of the physical environment depends on the actions of both agents, is prediction of how the physical state changes has to be made based on its prediction of js actions.",
                "Second, changes in js models have to be included in is belief update.",
                "Specifically, if j is intentional then an update of js beliefs due to its action and observation has to be included.",
                "In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief.",
                "If js model is subintentional, then js probable observations are appended to the observation history contained in the model.",
                "Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update.",
                "For a version of the belief update when js model is subintentional, see [9].",
                "If agent j is also modeled as an I-POMDP, then is belief update invokes js belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke is belief update and so on.",
                "This recursion in belief nesting bottoms out at the 0th level.",
                "At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9]. 2.2 Value Iteration Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)).",
                "Eq. 2 is a basis for value iteration in I-POMDPs.",
                "Agent is optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3.",
                "INTERACTIVEINFLUENCEDIAGRAMS A naive extension of influence diagrams (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes.",
                "However, this approach assumes that the agents actions are controlled using a probability distribution that does not change over time.",
                "Interactive influence diagrams (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node.",
                "We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon.",
                "We note that the probability distribution over the chance node, S, and the model node together represents agent is belief over its interactive states.",
                "In addition to the model 1 The 0th level model is a POMDP: Other agents actions are treated as exogenous events and folded into the T, O, and R functions.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j.",
                "The hexagon is the model node (Mj,l−1) whose structure we show in (b).",
                "Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ).",
                "Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the policy link in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model.",
                "In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs.",
                "The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2.",
                "Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional.",
                "Because the model node contains the alternative models of the other agent as its values, its representation is not trivial.",
                "In particular, some of the models within the node are I-IDs that when solved generate the agents optimal policy in their decision nodes.",
                "Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise.",
                "Borrowing insights from previous work [8], we observe that the model node and the dashed policy link that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b).",
                "The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj.",
                "Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent is beliefs.",
                "The conditional probability table of the chance node, Aj, is a multiplexer that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj].",
                "The values of Mod[Mj] denote the different models of j.",
                "In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1.",
                "The distribution over the node, Mod[Mj], is the agent is belief over the models of j given a physical state.",
                "For more agents, we will have as many model nodes as there are agents.",
                "Notice that Fig. 1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional dependency links.",
                "In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them.",
                "In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes.",
                "This allows I-IDs to be represented and implemented using conventional application tools that target IDs.",
                "Note that we may view the level l I-ID as a NID.",
                "Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2).",
                "If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID.",
                "Note that within the I-IDs (or IDs) at each level, there is only a single decision node.",
                "Thus, our NID does not contain any MAIDs.",
                "Figure 2: A level l I-ID represented as a NID.",
                "The probabilities assigned to the blocks of the NID are is beliefs over js models conditioned on a physical state. 3.2 Solution The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively.",
                "We start by solving the level 0 models, which, if intentional, are traditional IDs.",
                "Their solutions provide probability distributions over the other agents actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID.",
                "The mapping from the level 0 models decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c).",
                "During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj].",
                "As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent is belief over the models of j conditioned on the physical state.",
                "The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j.",
                "Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18].",
                "This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief.",
                "Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agents current belief is known. 4.",
                "INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic influence diagrams (I-DIDs) extend I-IDs (and NIDs) to allow sequential decision-making over several time steps.",
                "Just as DIDs are structured graphical representations of POMDPs, I-DIDs are the graphical online analogs for finitely nested I-POMDPs.",
                "I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents. 4.1 Syntax We depict a general two time-slice I-DID in Fig. 3(a).",
                "In addition to the model nodes and the dashed policy link, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a).",
                "We explained the semantics of the model node and the policy link in the previous section; we describe the model updates next.",
                "The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1.",
                "Recall from Section 2 that an agents intentional model includes its belief.",
                "Because the agents act and receive observations, their models are updated to reflect their changed beliefs.",
                "Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models.",
                "Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models.",
                "Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model.",
                "These steps are a part of agent is belief update formalized using Eq. 1.",
                "In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID.",
                "If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ).",
                "These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation.",
                "The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously.",
                "Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed.",
                "The probability that js updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node.",
                "In order to obtain the probability of js possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ].",
                "Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a multiplexer modulated by Mod[Mt j ].",
                "Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1.",
                "Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1.",
                "Notice that the model update link may be replaced by the dependency links between the chance nodes that constitute the model nodes in the two time slices.",
                "In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them.",
                "Chance nodes and dependency links that not in bold are standard, usually found in DIDs.",
                "Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents js observation at time t + 1.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links.",
                "We note that the possible set of models of the other agent j grows exponentially with the number of time steps.",
                "For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node. 4.2 Solution Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively.",
                "For the purpose of illustration, let l=1 and T=2.",
                "The solution method uses the standard look-ahead technique, projecting the agents action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step.",
                "Because agent i has a belief over js models as well, the lookahead includes finding out the possible models that j could have in the future.",
                "Consequently, each of js subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions.",
                "These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b).",
                "We note the recursive nature of this solution: in solving agent is level 1 I-DID, js level 0 DIDs must be solved.",
                "If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner.",
                "We briefly outline the recursive algorithm for solving agent is Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase 1.",
                "For t from 1 to T − 1 do 2.",
                "If l ≥ 1 then Populate Mt+1 j,l−1 3.",
                "For each mt j in Range(Mt j,l−1) do 4.",
                "Recursively call algorithm with the l − 1 I-ID (or ID) that represents mt j and the horizon, T − t + 1 5.",
                "Map the decision node of the solved I-ID (or ID), OPT(mt j), to a chance node Aj 6.",
                "For each aj in OPT(mt j) do 7.",
                "For each oj in Oj (part of mt j) do 8.",
                "Update js belief, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← New I-ID (or ID) with bt+1 j as the initial belief 10.",
                "Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11.",
                "Add the model node, Mt+1 j,l−1, and the dependency links between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b)) 12.",
                "Add the chance, decision, and utility nodes for t + 1 time slice and the dependency links between them 13.",
                "Establish the CPTs for each chance node and utility node Look-Ahead Phase 14.",
                "Apply the standard look-ahead and backup method to solve the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5.",
                "We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the dependency links and the conditional probability tables for each node.",
                "We particularly focus on establishing and populating the model nodes (lines 3-11).",
                "Note that Range(·) returns the values (lower level models) of the random variable given as input (model node).",
                "In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs.",
                "Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents.",
                "As we mentioned previously, the 0-th level models are the traditional DIDs.",
                "Their solutions provide probability distributions over actions of the agent modeled at that level to I-DIDs at level 1.",
                "Given probability distributions over other agents actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models.",
                "Assume that the number of models considered at each level is bound by a number, M. Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs. 5.",
                "EXAMPLE APPLICATIONS To illustrate the usefulness of I-DIDs, we apply them to three problem domains.",
                "We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it. 5.1 Followership-Leadership in the Multiagent Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9].",
                "The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L).",
                "In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agents opening one of the doors.",
                "When any door is opened, the tiger persists in its original location with a probability of 95%.",
                "Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%.",
                "Agent j, on the other hand, hears growls with a reliability of 95%.",
                "Thus, the setting is such that agent i hears agent j opening doors more reliably than the tigers growls.",
                "This suggests that i could use js actions as an indication of the location of the tiger, as we discuss below.",
                "Each agents preferences are as in the single agent game discussed in [13].",
                "The transition, observation, and reward functions are shown in [16].",
                "A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions.",
                "In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship.",
                "In particular, we analyze the situational and epistemological conditions sufficient for their emergence.",
                "The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the others actions in order to maximize its payoffs.",
                "Let us consider a particular setting of the tiger problem in which agent i believes that js preferences are aligned with its own - both of them just want to get the gold - and js hearing is more reliable in comparison to itself.",
                "As an example, suppose that j, on listening can discern the tigers location 95% of the times compared to is 65% accuracy.",
                "Additionally, agent i does not have any initial information about the tigers location.",
                "In other words, is single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger.",
                "In addition, i considers two models of j, which differ in js flat level 0 initial beliefs.",
                "This is represented in the level 1 I-ID shown in Fig. 6(a).",
                "According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)).",
                "Agent i is undecided on these two models of j.",
                "If we vary is hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior.",
                "If is probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow js actions: i opens the same door that j opened previously iff is own assessment of the tigers location confirms js pick.",
                "If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)).",
                "Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem.",
                "Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations.",
                "We observed that a single level of belief nesting - beliefs about the others models - was sufficient for followership to emerge in the tiger problem.",
                "However, the epistemological requirements for the emergence of leadership are more complex.",
                "For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i.",
                "As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow js actions over time.",
                "Agent j emerges as a leader if it believes that i will follow it, which implies that js belief must be nested two levels deep to enable it to recognize its leadership role.",
                "Realizing that i will follow presents j with an opportunity to influence is actions in the benefit of the collective good or its self-interest alone.",
                "For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original.",
                "If j alone selects the correct door, it gets the payoff of 10.",
                "On the other hand, if both agents pick the wrong door, their penalties are cut in half.",
                "In this setting, it is in both js best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader.",
                "However, consider a slightly different problem in which j gains from is loss and is penalized if i gains.",
                "Specifically, let is payoff be subtracted from js, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then is loss of 100 becomes js gain.",
                "Agent j believes that i incorrectly thinks that js preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is.",
                "Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following js actions.",
                "We show is normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a).",
                "The policy demonstrates that i will blindly follow js actions.",
                "Since the tiger persists in its original location with a probability of 0.95, i will select the same door again.",
                "If j begins the game with a 99% probability that the tiger is on the right, solving js I-DID nested two levels deep, results in the policy shown in Fig. 8(b).",
                "Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL.",
                "Agent js intention is to deceive i who, it believes, will follow js actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest.",
                "Figure 8: Emergence of deception between agents in the tiger problem.",
                "Behaviors of interest are in bold. * denotes as before. (a) Agent is policy demonstrating that it will blindly follow js actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i. 5.2 Altruism and Reciprocity in the Public Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves.",
                "Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot.",
                "However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes.",
                "Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others contributions.",
                "However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions.",
                "The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4].",
                "Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting.",
                "These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others.",
                "For simplicity, we assume that the game is played between M = 2 agents, i and j.",
                "Let each agent be initially endowed with XT amount of resources.",
                "While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions.",
                "Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute.",
                "The latter action is deThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D).",
                "We assume that the actions are not observable to others.",
                "The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return.",
                "We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain.",
                "Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment.",
                "In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment.",
                "Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent.",
                "For simplicity, we assume that the cost of punishing is same for both the agents.",
                "The one-shot PG game with punishment is shown in Table. 1.",
                "Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action.",
                "If P < XT − ciXT , then defection is the dominating action for both.",
                "If P = XT − ciXT , then the game is not dominance-solvable.",
                "Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a).",
                "We formulate a sequential version of the PG problem with punishment from the perspective of agent i.",
                "Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents.",
                "Each agent may contribute a fixed amount, xc, or defect.",
                "An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot.",
                "Notice that the observations are also indirectly indicative of agent js actions because the state of the public pot is influenced by them.",
                "The amount of resources in agent is private pot, is perfectly observable to i.",
                "The payoffs are analogous to Table. 1.",
                "Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)).",
                "Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect.",
                "Let xc = 1 and the level 0 agent be punished half the times it defects.",
                "With one action remaining, both types of agents choose to contribute to avoid being punished.",
                "With two actions to go, the altruistic type chooses to contribute, while the other defects.",
                "This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids.",
                "Because cj for the non-altruistic type is less, it prefers not to contribute.",
                "With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects.",
                "For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection.",
                "We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level 0 models, mentioned previously, to j (see Fig. 9).",
                "If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps.",
                "This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type.",
                "However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1.",
                "These results demonstrate that the behavior of our altruistic type resembles that found experimentally.",
                "The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic.",
                "We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection.",
                "The reciprocal types marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other.",
                "We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full.",
                "For this prior belief, i chooses to defect.",
                "On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)).",
                "This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from js action to contribute.",
                "Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step.",
                "Agent i therefore chooses to contribute to reciprocate js action.",
                "An analogous reasoning leads i to defect when it observes a meager pot.",
                "With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations.",
                "Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic). 5.3 Strategies in Two-Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2].",
                "Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck.",
                "While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands.",
                "To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit.",
                "Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot.",
                "During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to 8.",
                "Notice that, for example, if a lower numbered card is discarded, the probability of receiving a low card in exchange is now reduced.",
                "We show the level 1 I-ID for the simplified two-player Poker in Fig. 11.",
                "We considered two models (personality types) of agent j.",
                "The conservative type believes that it is likely that its opponent has a high numbered card in its hand.",
                "On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card.",
                "Thus, the two types differ in their beliefs over their opponents hand.",
                "In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution.",
                "With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand.",
                "This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange.",
                "The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one.",
                "Figure 11: (a) Level 1 I-ID of agent i.",
                "The observation reveals information about js hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a).",
                "The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in js hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. is own hand contains the card numbered 8.",
                "The agent starts by keeping its card.",
                "On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card.",
                "If i observes that j discarded its card into the L or H pile, i believes that j is aggressive.",
                "On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange.",
                "Because the probability of receiving a low card is high now, i chooses to keep its card.",
                "On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card.",
                "In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff.",
                "This is partly due to the fact that an observation of, say, L resets the agent is previous time step beliefs over js hand to the low numbered cards only. 6.",
                "DISCUSSION We showed how DIDs may be extended to I-DIDs that enable online sequential decision-making in uncertain multiagent settings.",
                "Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent is three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs.",
                "I-DIDs extend NIDs to allow sequential decision-making over multiple time steps in the presence of other interacting agents.",
                "I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs.",
                "We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality.",
                "Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work.",
                "The first author would like to acknowledge the support of a UGARF grant. 7.",
                "REFERENCES [1] R. J. Aumann.",
                "Interactive epistemology i: Knowledge.",
                "International Journal of Game Theory, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron.",
                "The challenge of poker.",
                "AIJ, 2001. [3] A. Brandenburger and E. Dekel.",
                "Hierarchies of beliefs and common knowledge.",
                "Journal of Economic Theory, 59:189-198, 1993. [4] C. Camerer.",
                "Behavioral Game Theory: Experiments in Strategic Interaction.",
                "Princeton University Press, 2003. [5] E. Fehr and S. Gachter.",
                "Cooperation and punishment in public goods experiments.",
                "American Economic Review, 90(4):980-994, 2000. [6] D. Fudenberg and D. K. Levine.",
                "The Theory of Learning in Games.",
                "MIT Press, 1998. [7] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [8] Y. Gal and A. Pfeffer.",
                "A language for modeling agents decision-making processes in games.",
                "In AAMAS, 2003. [9] P. Gmytrasiewicz and P. Doshi.",
                "A framework for sequential planning in multiagent settings.",
                "JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz and E. Durfee.",
                "Rational coordination in multi-agent environments.",
                "JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi.",
                "Games with incomplete information played by bayesian players.",
                "Management Science, 14(3):159-182, 1967. [12] R. A. Howard and J. E. Matheson.",
                "Influence diagrams.",
                "In R. A. Howard and J. E. Matheson, editors, The Principles and Applications of Decision Analysis.",
                "Strategic Decisions Group, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman, and A. Cassandra.",
                "Planning and acting in partially observable stochastic domains.",
                "Artificial Intelligence Journal, 2, 1998. [14] D. Koller and B. Milch.",
                "Multi-agent influence diagrams for representing and solving games.",
                "In IJCAI, pages 1027-1034, 2001. [15] K. Polich and P. Gmytrasiewicz.",
                "Interactive dynamic influence diagrams.",
                "In GTDT Workshop, AAMAS, 2006. [16] B.",
                "Rathnas., P. Doshi, and P. J. Gmytrasiewicz.",
                "Exact solutions to interactive pomdps using behavioral equivalence.",
                "In Autonomous Agents and Multi-Agent Systems Conference (AAMAS), 2006. [17] S. Russell and P. Norvig.",
                "Artificial Intelligence: A Modern Approach (Second Edition).",
                "Prentice Hall, 2003. [18] R. D. Shachter.",
                "Evaluating influence diagrams.",
                "Operations Research, 34(6):871-882, 1986. [19] D. Suryadi and P. Gmytrasiewicz.",
                "Learning models of other agents using influence diagrams.",
                "In UM, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Introducción Los procesos de decisión de Markov parcialmente observables (IPOMDPS) [9] proporcionan un marco para la toma de decisiones secuenciales en el \"entorno multiagente parcialmente observable\"."
            ],
            "translated_text": "",
            "candidates": [
                "entorno multiagente parcialmente observable",
                "entorno multiagente parcialmente observable"
            ],
            "error": []
        },
        "multiagent environment": {
            "translated_key": "entorno multiagente",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Graphical Models for Online Solutions to Interactive POMDPs Prashant Doshi Dept.",
                "of Computer Science University of Georgia Athens, GA 30602, USA pdoshi@cs.uga.edu Yifeng Zeng Dept. of Computer Science Aalborg University DK-9220 Aalborg, Denmark yfzeng@cs.aau.edu Qiongyu Chen Dept. of Computer Science National Univ. of Singapore 117543, Singapore chenqy@comp.nus.edu.sg ABSTRACT We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation.",
                "These graphical models called interactive dynamic influence diagrams (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents.",
                "Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Theory 1.",
                "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in partially observable <br>multiagent environment</br>s.",
                "They generalize POMDPs [13] to multiagent settings by including the other agents computable models in the state space along with the states of the physical environment.",
                "The models encompass all information influencing the agents behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11].",
                "I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a decision-makers perspective in the interaction.",
                "In [15], Polich and Gmytrasiewicz introduced interactive dynamic influence diagrams (I-DIDs) as the computational representations of I-POMDPs.",
                "I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs contribute to a growing line of work [19] that includes multi-agent influence diagrams (MAIDs) [14], and more recently, networks of influence diagrams (NIDs) [8].",
                "These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agents actions and chance nodes capturing the agents private information.",
                "MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the independence structure.",
                "NIDs extend MAIDs to include agents uncertainty over the game being played and over models of the other agents.",
                "Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent.",
                "Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently.",
                "However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games.",
                "Matters are more complex when we consider interactions that are extended over time, where predictions about others future actions must be made using models that change as the agents act and observe.",
                "I-DIDs address this gap by allowing the representation of other agents models as the values of a special model node.",
                "Both, other agents models and the original agents beliefs over these models are updated over time using special-purpose implementations.",
                "In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID.",
                "Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently.",
                "Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I-DID by [15], and show that it could be replaced by traditional dependency links.",
                "In the previous representation of the I-DID, the update of the agents belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time.",
                "We explicate the semantics of this link by showing how it can be implemented using the traditional dependency links between the chance nodes that constitute the model nodes.",
                "The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs.",
                "We show how IDIDs may be used to model an agents uncertainty over others models, that may themselves be I-DIDs.",
                "Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others models.",
                "Analogous to DIDs, I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. 2.",
                "BACKGROUND: FINITELY NESTED IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents models as part of the state space [9].",
                "Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents models and their beliefs about others.",
                "For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j.",
                "A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment.",
                "Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj .",
                "Here, j is Bayes rational and OCj is js optimality criterion.",
                "SMj is the set of subintentional models of j.",
                "Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent.",
                "We give a recursive bottom-up construction of the interactive state space below.",
                "ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . .",
                "ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent is preferences over its interactive states.",
                "Usually only the physical states will matter.",
                "Agent is policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i.",
                "Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai). 2.1 Belief Update Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes.",
                "However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones.",
                "First, since the state of the physical environment depends on the actions of both agents, is prediction of how the physical state changes has to be made based on its prediction of js actions.",
                "Second, changes in js models have to be included in is belief update.",
                "Specifically, if j is intentional then an update of js beliefs due to its action and observation has to be included.",
                "In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief.",
                "If js model is subintentional, then js probable observations are appended to the observation history contained in the model.",
                "Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update.",
                "For a version of the belief update when js model is subintentional, see [9].",
                "If agent j is also modeled as an I-POMDP, then is belief update invokes js belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke is belief update and so on.",
                "This recursion in belief nesting bottoms out at the 0th level.",
                "At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9]. 2.2 Value Iteration Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)).",
                "Eq. 2 is a basis for value iteration in I-POMDPs.",
                "Agent is optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3.",
                "INTERACTIVEINFLUENCEDIAGRAMS A naive extension of influence diagrams (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes.",
                "However, this approach assumes that the agents actions are controlled using a probability distribution that does not change over time.",
                "Interactive influence diagrams (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node.",
                "We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon.",
                "We note that the probability distribution over the chance node, S, and the model node together represents agent is belief over its interactive states.",
                "In addition to the model 1 The 0th level model is a POMDP: Other agents actions are treated as exogenous events and folded into the T, O, and R functions.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j.",
                "The hexagon is the model node (Mj,l−1) whose structure we show in (b).",
                "Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ).",
                "Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the policy link in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model.",
                "In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs.",
                "The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2.",
                "Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional.",
                "Because the model node contains the alternative models of the other agent as its values, its representation is not trivial.",
                "In particular, some of the models within the node are I-IDs that when solved generate the agents optimal policy in their decision nodes.",
                "Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise.",
                "Borrowing insights from previous work [8], we observe that the model node and the dashed policy link that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b).",
                "The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj.",
                "Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent is beliefs.",
                "The conditional probability table of the chance node, Aj, is a multiplexer that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj].",
                "The values of Mod[Mj] denote the different models of j.",
                "In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1.",
                "The distribution over the node, Mod[Mj], is the agent is belief over the models of j given a physical state.",
                "For more agents, we will have as many model nodes as there are agents.",
                "Notice that Fig. 1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional dependency links.",
                "In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them.",
                "In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes.",
                "This allows I-IDs to be represented and implemented using conventional application tools that target IDs.",
                "Note that we may view the level l I-ID as a NID.",
                "Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2).",
                "If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID.",
                "Note that within the I-IDs (or IDs) at each level, there is only a single decision node.",
                "Thus, our NID does not contain any MAIDs.",
                "Figure 2: A level l I-ID represented as a NID.",
                "The probabilities assigned to the blocks of the NID are is beliefs over js models conditioned on a physical state. 3.2 Solution The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively.",
                "We start by solving the level 0 models, which, if intentional, are traditional IDs.",
                "Their solutions provide probability distributions over the other agents actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID.",
                "The mapping from the level 0 models decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c).",
                "During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj].",
                "As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent is belief over the models of j conditioned on the physical state.",
                "The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j.",
                "Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18].",
                "This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief.",
                "Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agents current belief is known. 4.",
                "INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic influence diagrams (I-DIDs) extend I-IDs (and NIDs) to allow sequential decision-making over several time steps.",
                "Just as DIDs are structured graphical representations of POMDPs, I-DIDs are the graphical online analogs for finitely nested I-POMDPs.",
                "I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents. 4.1 Syntax We depict a general two time-slice I-DID in Fig. 3(a).",
                "In addition to the model nodes and the dashed policy link, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a).",
                "We explained the semantics of the model node and the policy link in the previous section; we describe the model updates next.",
                "The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1.",
                "Recall from Section 2 that an agents intentional model includes its belief.",
                "Because the agents act and receive observations, their models are updated to reflect their changed beliefs.",
                "Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models.",
                "Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models.",
                "Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model.",
                "These steps are a part of agent is belief update formalized using Eq. 1.",
                "In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID.",
                "If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ).",
                "These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation.",
                "The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously.",
                "Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed.",
                "The probability that js updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node.",
                "In order to obtain the probability of js possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ].",
                "Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a multiplexer modulated by Mod[Mt j ].",
                "Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1.",
                "Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1.",
                "Notice that the model update link may be replaced by the dependency links between the chance nodes that constitute the model nodes in the two time slices.",
                "In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them.",
                "Chance nodes and dependency links that not in bold are standard, usually found in DIDs.",
                "Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents js observation at time t + 1.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links.",
                "We note that the possible set of models of the other agent j grows exponentially with the number of time steps.",
                "For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node. 4.2 Solution Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively.",
                "For the purpose of illustration, let l=1 and T=2.",
                "The solution method uses the standard look-ahead technique, projecting the agents action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step.",
                "Because agent i has a belief over js models as well, the lookahead includes finding out the possible models that j could have in the future.",
                "Consequently, each of js subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions.",
                "These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b).",
                "We note the recursive nature of this solution: in solving agent is level 1 I-DID, js level 0 DIDs must be solved.",
                "If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner.",
                "We briefly outline the recursive algorithm for solving agent is Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase 1.",
                "For t from 1 to T − 1 do 2.",
                "If l ≥ 1 then Populate Mt+1 j,l−1 3.",
                "For each mt j in Range(Mt j,l−1) do 4.",
                "Recursively call algorithm with the l − 1 I-ID (or ID) that represents mt j and the horizon, T − t + 1 5.",
                "Map the decision node of the solved I-ID (or ID), OPT(mt j), to a chance node Aj 6.",
                "For each aj in OPT(mt j) do 7.",
                "For each oj in Oj (part of mt j) do 8.",
                "Update js belief, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← New I-ID (or ID) with bt+1 j as the initial belief 10.",
                "Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11.",
                "Add the model node, Mt+1 j,l−1, and the dependency links between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b)) 12.",
                "Add the chance, decision, and utility nodes for t + 1 time slice and the dependency links between them 13.",
                "Establish the CPTs for each chance node and utility node Look-Ahead Phase 14.",
                "Apply the standard look-ahead and backup method to solve the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5.",
                "We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the dependency links and the conditional probability tables for each node.",
                "We particularly focus on establishing and populating the model nodes (lines 3-11).",
                "Note that Range(·) returns the values (lower level models) of the random variable given as input (model node).",
                "In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs.",
                "Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents.",
                "As we mentioned previously, the 0-th level models are the traditional DIDs.",
                "Their solutions provide probability distributions over actions of the agent modeled at that level to I-DIDs at level 1.",
                "Given probability distributions over other agents actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models.",
                "Assume that the number of models considered at each level is bound by a number, M. Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs. 5.",
                "EXAMPLE APPLICATIONS To illustrate the usefulness of I-DIDs, we apply them to three problem domains.",
                "We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it. 5.1 Followership-Leadership in the Multiagent Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9].",
                "The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L).",
                "In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agents opening one of the doors.",
                "When any door is opened, the tiger persists in its original location with a probability of 95%.",
                "Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%.",
                "Agent j, on the other hand, hears growls with a reliability of 95%.",
                "Thus, the setting is such that agent i hears agent j opening doors more reliably than the tigers growls.",
                "This suggests that i could use js actions as an indication of the location of the tiger, as we discuss below.",
                "Each agents preferences are as in the single agent game discussed in [13].",
                "The transition, observation, and reward functions are shown in [16].",
                "A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions.",
                "In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship.",
                "In particular, we analyze the situational and epistemological conditions sufficient for their emergence.",
                "The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the others actions in order to maximize its payoffs.",
                "Let us consider a particular setting of the tiger problem in which agent i believes that js preferences are aligned with its own - both of them just want to get the gold - and js hearing is more reliable in comparison to itself.",
                "As an example, suppose that j, on listening can discern the tigers location 95% of the times compared to is 65% accuracy.",
                "Additionally, agent i does not have any initial information about the tigers location.",
                "In other words, is single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger.",
                "In addition, i considers two models of j, which differ in js flat level 0 initial beliefs.",
                "This is represented in the level 1 I-ID shown in Fig. 6(a).",
                "According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)).",
                "Agent i is undecided on these two models of j.",
                "If we vary is hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior.",
                "If is probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow js actions: i opens the same door that j opened previously iff is own assessment of the tigers location confirms js pick.",
                "If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)).",
                "Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem.",
                "Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations.",
                "We observed that a single level of belief nesting - beliefs about the others models - was sufficient for followership to emerge in the tiger problem.",
                "However, the epistemological requirements for the emergence of leadership are more complex.",
                "For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i.",
                "As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow js actions over time.",
                "Agent j emerges as a leader if it believes that i will follow it, which implies that js belief must be nested two levels deep to enable it to recognize its leadership role.",
                "Realizing that i will follow presents j with an opportunity to influence is actions in the benefit of the collective good or its self-interest alone.",
                "For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original.",
                "If j alone selects the correct door, it gets the payoff of 10.",
                "On the other hand, if both agents pick the wrong door, their penalties are cut in half.",
                "In this setting, it is in both js best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader.",
                "However, consider a slightly different problem in which j gains from is loss and is penalized if i gains.",
                "Specifically, let is payoff be subtracted from js, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then is loss of 100 becomes js gain.",
                "Agent j believes that i incorrectly thinks that js preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is.",
                "Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following js actions.",
                "We show is normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a).",
                "The policy demonstrates that i will blindly follow js actions.",
                "Since the tiger persists in its original location with a probability of 0.95, i will select the same door again.",
                "If j begins the game with a 99% probability that the tiger is on the right, solving js I-DID nested two levels deep, results in the policy shown in Fig. 8(b).",
                "Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL.",
                "Agent js intention is to deceive i who, it believes, will follow js actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest.",
                "Figure 8: Emergence of deception between agents in the tiger problem.",
                "Behaviors of interest are in bold. * denotes as before. (a) Agent is policy demonstrating that it will blindly follow js actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i. 5.2 Altruism and Reciprocity in the Public Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves.",
                "Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot.",
                "However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes.",
                "Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others contributions.",
                "However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions.",
                "The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4].",
                "Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting.",
                "These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others.",
                "For simplicity, we assume that the game is played between M = 2 agents, i and j.",
                "Let each agent be initially endowed with XT amount of resources.",
                "While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions.",
                "Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute.",
                "The latter action is deThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D).",
                "We assume that the actions are not observable to others.",
                "The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return.",
                "We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain.",
                "Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment.",
                "In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment.",
                "Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent.",
                "For simplicity, we assume that the cost of punishing is same for both the agents.",
                "The one-shot PG game with punishment is shown in Table. 1.",
                "Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action.",
                "If P < XT − ciXT , then defection is the dominating action for both.",
                "If P = XT − ciXT , then the game is not dominance-solvable.",
                "Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a).",
                "We formulate a sequential version of the PG problem with punishment from the perspective of agent i.",
                "Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents.",
                "Each agent may contribute a fixed amount, xc, or defect.",
                "An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot.",
                "Notice that the observations are also indirectly indicative of agent js actions because the state of the public pot is influenced by them.",
                "The amount of resources in agent is private pot, is perfectly observable to i.",
                "The payoffs are analogous to Table. 1.",
                "Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)).",
                "Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect.",
                "Let xc = 1 and the level 0 agent be punished half the times it defects.",
                "With one action remaining, both types of agents choose to contribute to avoid being punished.",
                "With two actions to go, the altruistic type chooses to contribute, while the other defects.",
                "This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids.",
                "Because cj for the non-altruistic type is less, it prefers not to contribute.",
                "With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects.",
                "For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection.",
                "We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level 0 models, mentioned previously, to j (see Fig. 9).",
                "If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps.",
                "This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type.",
                "However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1.",
                "These results demonstrate that the behavior of our altruistic type resembles that found experimentally.",
                "The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic.",
                "We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection.",
                "The reciprocal types marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other.",
                "We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full.",
                "For this prior belief, i chooses to defect.",
                "On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)).",
                "This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from js action to contribute.",
                "Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step.",
                "Agent i therefore chooses to contribute to reciprocate js action.",
                "An analogous reasoning leads i to defect when it observes a meager pot.",
                "With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations.",
                "Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic). 5.3 Strategies in Two-Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2].",
                "Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck.",
                "While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands.",
                "To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit.",
                "Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot.",
                "During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to 8.",
                "Notice that, for example, if a lower numbered card is discarded, the probability of receiving a low card in exchange is now reduced.",
                "We show the level 1 I-ID for the simplified two-player Poker in Fig. 11.",
                "We considered two models (personality types) of agent j.",
                "The conservative type believes that it is likely that its opponent has a high numbered card in its hand.",
                "On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card.",
                "Thus, the two types differ in their beliefs over their opponents hand.",
                "In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution.",
                "With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand.",
                "This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange.",
                "The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one.",
                "Figure 11: (a) Level 1 I-ID of agent i.",
                "The observation reveals information about js hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a).",
                "The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in js hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. is own hand contains the card numbered 8.",
                "The agent starts by keeping its card.",
                "On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card.",
                "If i observes that j discarded its card into the L or H pile, i believes that j is aggressive.",
                "On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange.",
                "Because the probability of receiving a low card is high now, i chooses to keep its card.",
                "On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card.",
                "In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff.",
                "This is partly due to the fact that an observation of, say, L resets the agent is previous time step beliefs over js hand to the low numbered cards only. 6.",
                "DISCUSSION We showed how DIDs may be extended to I-DIDs that enable online sequential decision-making in uncertain multiagent settings.",
                "Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent is three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs.",
                "I-DIDs extend NIDs to allow sequential decision-making over multiple time steps in the presence of other interacting agents.",
                "I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs.",
                "We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality.",
                "Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work.",
                "The first author would like to acknowledge the support of a UGARF grant. 7.",
                "REFERENCES [1] R. J. Aumann.",
                "Interactive epistemology i: Knowledge.",
                "International Journal of Game Theory, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron.",
                "The challenge of poker.",
                "AIJ, 2001. [3] A. Brandenburger and E. Dekel.",
                "Hierarchies of beliefs and common knowledge.",
                "Journal of Economic Theory, 59:189-198, 1993. [4] C. Camerer.",
                "Behavioral Game Theory: Experiments in Strategic Interaction.",
                "Princeton University Press, 2003. [5] E. Fehr and S. Gachter.",
                "Cooperation and punishment in public goods experiments.",
                "American Economic Review, 90(4):980-994, 2000. [6] D. Fudenberg and D. K. Levine.",
                "The Theory of Learning in Games.",
                "MIT Press, 1998. [7] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [8] Y. Gal and A. Pfeffer.",
                "A language for modeling agents decision-making processes in games.",
                "In AAMAS, 2003. [9] P. Gmytrasiewicz and P. Doshi.",
                "A framework for sequential planning in multiagent settings.",
                "JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz and E. Durfee.",
                "Rational coordination in multi-agent environments.",
                "JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi.",
                "Games with incomplete information played by bayesian players.",
                "Management Science, 14(3):159-182, 1967. [12] R. A. Howard and J. E. Matheson.",
                "Influence diagrams.",
                "In R. A. Howard and J. E. Matheson, editors, The Principles and Applications of Decision Analysis.",
                "Strategic Decisions Group, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman, and A. Cassandra.",
                "Planning and acting in partially observable stochastic domains.",
                "Artificial Intelligence Journal, 2, 1998. [14] D. Koller and B. Milch.",
                "Multi-agent influence diagrams for representing and solving games.",
                "In IJCAI, pages 1027-1034, 2001. [15] K. Polich and P. Gmytrasiewicz.",
                "Interactive dynamic influence diagrams.",
                "In GTDT Workshop, AAMAS, 2006. [16] B.",
                "Rathnas., P. Doshi, and P. J. Gmytrasiewicz.",
                "Exact solutions to interactive pomdps using behavioral equivalence.",
                "In Autonomous Agents and Multi-Agent Systems Conference (AAMAS), 2006. [17] S. Russell and P. Norvig.",
                "Artificial Intelligence: A Modern Approach (Second Edition).",
                "Prentice Hall, 2003. [18] R. D. Shachter.",
                "Evaluating influence diagrams.",
                "Operations Research, 34(6):871-882, 1986. [19] D. Suryadi and P. Gmytrasiewicz.",
                "Learning models of other agents using influence diagrams.",
                "In UM, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Introducción Los procesos de decisión de Markov parcialmente observables (IPOMDPS) [9] proporcionan un marco para la toma de decisiones secuenciales en el \"entorno multiagente\" parcialmente observable."
            ],
            "translated_text": "",
            "candidates": [
                "entorno multiagente",
                "entorno multiagente"
            ],
            "error": []
        },
        "nash equilibrium profile": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Graphical Models for Online Solutions to Interactive POMDPs Prashant Doshi Dept.",
                "of Computer Science University of Georgia Athens, GA 30602, USA pdoshi@cs.uga.edu Yifeng Zeng Dept. of Computer Science Aalborg University DK-9220 Aalborg, Denmark yfzeng@cs.aau.edu Qiongyu Chen Dept. of Computer Science National Univ. of Singapore 117543, Singapore chenqy@comp.nus.edu.sg ABSTRACT We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation.",
                "These graphical models called interactive dynamic influence diagrams (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents.",
                "Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Theory 1.",
                "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in partially observable multiagent environments.",
                "They generalize POMDPs [13] to multiagent settings by including the other agents computable models in the state space along with the states of the physical environment.",
                "The models encompass all information influencing the agents behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11].",
                "I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a decision-makers perspective in the interaction.",
                "In [15], Polich and Gmytrasiewicz introduced interactive dynamic influence diagrams (I-DIDs) as the computational representations of I-POMDPs.",
                "I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs contribute to a growing line of work [19] that includes multi-agent influence diagrams (MAIDs) [14], and more recently, networks of influence diagrams (NIDs) [8].",
                "These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agents actions and chance nodes capturing the agents private information.",
                "MAIDs objectively analyze the game, efficiently computing the <br>nash equilibrium profile</br> by exploiting the independence structure.",
                "NIDs extend MAIDs to include agents uncertainty over the game being played and over models of the other agents.",
                "Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent.",
                "Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently.",
                "However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games.",
                "Matters are more complex when we consider interactions that are extended over time, where predictions about others future actions must be made using models that change as the agents act and observe.",
                "I-DIDs address this gap by allowing the representation of other agents models as the values of a special model node.",
                "Both, other agents models and the original agents beliefs over these models are updated over time using special-purpose implementations.",
                "In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID.",
                "Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently.",
                "Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I-DID by [15], and show that it could be replaced by traditional dependency links.",
                "In the previous representation of the I-DID, the update of the agents belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time.",
                "We explicate the semantics of this link by showing how it can be implemented using the traditional dependency links between the chance nodes that constitute the model nodes.",
                "The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs.",
                "We show how IDIDs may be used to model an agents uncertainty over others models, that may themselves be I-DIDs.",
                "Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others models.",
                "Analogous to DIDs, I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. 2.",
                "BACKGROUND: FINITELY NESTED IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents models as part of the state space [9].",
                "Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents models and their beliefs about others.",
                "For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j.",
                "A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment.",
                "Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj .",
                "Here, j is Bayes rational and OCj is js optimality criterion.",
                "SMj is the set of subintentional models of j.",
                "Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent.",
                "We give a recursive bottom-up construction of the interactive state space below.",
                "ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . .",
                "ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent is preferences over its interactive states.",
                "Usually only the physical states will matter.",
                "Agent is policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i.",
                "Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai). 2.1 Belief Update Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes.",
                "However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones.",
                "First, since the state of the physical environment depends on the actions of both agents, is prediction of how the physical state changes has to be made based on its prediction of js actions.",
                "Second, changes in js models have to be included in is belief update.",
                "Specifically, if j is intentional then an update of js beliefs due to its action and observation has to be included.",
                "In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief.",
                "If js model is subintentional, then js probable observations are appended to the observation history contained in the model.",
                "Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update.",
                "For a version of the belief update when js model is subintentional, see [9].",
                "If agent j is also modeled as an I-POMDP, then is belief update invokes js belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke is belief update and so on.",
                "This recursion in belief nesting bottoms out at the 0th level.",
                "At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9]. 2.2 Value Iteration Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)).",
                "Eq. 2 is a basis for value iteration in I-POMDPs.",
                "Agent is optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3.",
                "INTERACTIVEINFLUENCEDIAGRAMS A naive extension of influence diagrams (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes.",
                "However, this approach assumes that the agents actions are controlled using a probability distribution that does not change over time.",
                "Interactive influence diagrams (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node.",
                "We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon.",
                "We note that the probability distribution over the chance node, S, and the model node together represents agent is belief over its interactive states.",
                "In addition to the model 1 The 0th level model is a POMDP: Other agents actions are treated as exogenous events and folded into the T, O, and R functions.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j.",
                "The hexagon is the model node (Mj,l−1) whose structure we show in (b).",
                "Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ).",
                "Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the policy link in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model.",
                "In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs.",
                "The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2.",
                "Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional.",
                "Because the model node contains the alternative models of the other agent as its values, its representation is not trivial.",
                "In particular, some of the models within the node are I-IDs that when solved generate the agents optimal policy in their decision nodes.",
                "Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise.",
                "Borrowing insights from previous work [8], we observe that the model node and the dashed policy link that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b).",
                "The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj.",
                "Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent is beliefs.",
                "The conditional probability table of the chance node, Aj, is a multiplexer that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj].",
                "The values of Mod[Mj] denote the different models of j.",
                "In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1.",
                "The distribution over the node, Mod[Mj], is the agent is belief over the models of j given a physical state.",
                "For more agents, we will have as many model nodes as there are agents.",
                "Notice that Fig. 1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional dependency links.",
                "In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them.",
                "In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes.",
                "This allows I-IDs to be represented and implemented using conventional application tools that target IDs.",
                "Note that we may view the level l I-ID as a NID.",
                "Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2).",
                "If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID.",
                "Note that within the I-IDs (or IDs) at each level, there is only a single decision node.",
                "Thus, our NID does not contain any MAIDs.",
                "Figure 2: A level l I-ID represented as a NID.",
                "The probabilities assigned to the blocks of the NID are is beliefs over js models conditioned on a physical state. 3.2 Solution The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively.",
                "We start by solving the level 0 models, which, if intentional, are traditional IDs.",
                "Their solutions provide probability distributions over the other agents actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID.",
                "The mapping from the level 0 models decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c).",
                "During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj].",
                "As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent is belief over the models of j conditioned on the physical state.",
                "The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j.",
                "Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18].",
                "This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief.",
                "Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agents current belief is known. 4.",
                "INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic influence diagrams (I-DIDs) extend I-IDs (and NIDs) to allow sequential decision-making over several time steps.",
                "Just as DIDs are structured graphical representations of POMDPs, I-DIDs are the graphical online analogs for finitely nested I-POMDPs.",
                "I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents. 4.1 Syntax We depict a general two time-slice I-DID in Fig. 3(a).",
                "In addition to the model nodes and the dashed policy link, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a).",
                "We explained the semantics of the model node and the policy link in the previous section; we describe the model updates next.",
                "The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1.",
                "Recall from Section 2 that an agents intentional model includes its belief.",
                "Because the agents act and receive observations, their models are updated to reflect their changed beliefs.",
                "Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models.",
                "Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models.",
                "Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model.",
                "These steps are a part of agent is belief update formalized using Eq. 1.",
                "In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID.",
                "If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ).",
                "These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation.",
                "The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously.",
                "Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed.",
                "The probability that js updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node.",
                "In order to obtain the probability of js possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ].",
                "Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a multiplexer modulated by Mod[Mt j ].",
                "Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1.",
                "Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1.",
                "Notice that the model update link may be replaced by the dependency links between the chance nodes that constitute the model nodes in the two time slices.",
                "In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them.",
                "Chance nodes and dependency links that not in bold are standard, usually found in DIDs.",
                "Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents js observation at time t + 1.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links.",
                "We note that the possible set of models of the other agent j grows exponentially with the number of time steps.",
                "For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node. 4.2 Solution Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively.",
                "For the purpose of illustration, let l=1 and T=2.",
                "The solution method uses the standard look-ahead technique, projecting the agents action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step.",
                "Because agent i has a belief over js models as well, the lookahead includes finding out the possible models that j could have in the future.",
                "Consequently, each of js subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions.",
                "These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b).",
                "We note the recursive nature of this solution: in solving agent is level 1 I-DID, js level 0 DIDs must be solved.",
                "If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner.",
                "We briefly outline the recursive algorithm for solving agent is Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase 1.",
                "For t from 1 to T − 1 do 2.",
                "If l ≥ 1 then Populate Mt+1 j,l−1 3.",
                "For each mt j in Range(Mt j,l−1) do 4.",
                "Recursively call algorithm with the l − 1 I-ID (or ID) that represents mt j and the horizon, T − t + 1 5.",
                "Map the decision node of the solved I-ID (or ID), OPT(mt j), to a chance node Aj 6.",
                "For each aj in OPT(mt j) do 7.",
                "For each oj in Oj (part of mt j) do 8.",
                "Update js belief, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← New I-ID (or ID) with bt+1 j as the initial belief 10.",
                "Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11.",
                "Add the model node, Mt+1 j,l−1, and the dependency links between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b)) 12.",
                "Add the chance, decision, and utility nodes for t + 1 time slice and the dependency links between them 13.",
                "Establish the CPTs for each chance node and utility node Look-Ahead Phase 14.",
                "Apply the standard look-ahead and backup method to solve the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5.",
                "We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the dependency links and the conditional probability tables for each node.",
                "We particularly focus on establishing and populating the model nodes (lines 3-11).",
                "Note that Range(·) returns the values (lower level models) of the random variable given as input (model node).",
                "In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs.",
                "Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents.",
                "As we mentioned previously, the 0-th level models are the traditional DIDs.",
                "Their solutions provide probability distributions over actions of the agent modeled at that level to I-DIDs at level 1.",
                "Given probability distributions over other agents actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models.",
                "Assume that the number of models considered at each level is bound by a number, M. Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs. 5.",
                "EXAMPLE APPLICATIONS To illustrate the usefulness of I-DIDs, we apply them to three problem domains.",
                "We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it. 5.1 Followership-Leadership in the Multiagent Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9].",
                "The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L).",
                "In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agents opening one of the doors.",
                "When any door is opened, the tiger persists in its original location with a probability of 95%.",
                "Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%.",
                "Agent j, on the other hand, hears growls with a reliability of 95%.",
                "Thus, the setting is such that agent i hears agent j opening doors more reliably than the tigers growls.",
                "This suggests that i could use js actions as an indication of the location of the tiger, as we discuss below.",
                "Each agents preferences are as in the single agent game discussed in [13].",
                "The transition, observation, and reward functions are shown in [16].",
                "A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions.",
                "In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship.",
                "In particular, we analyze the situational and epistemological conditions sufficient for their emergence.",
                "The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the others actions in order to maximize its payoffs.",
                "Let us consider a particular setting of the tiger problem in which agent i believes that js preferences are aligned with its own - both of them just want to get the gold - and js hearing is more reliable in comparison to itself.",
                "As an example, suppose that j, on listening can discern the tigers location 95% of the times compared to is 65% accuracy.",
                "Additionally, agent i does not have any initial information about the tigers location.",
                "In other words, is single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger.",
                "In addition, i considers two models of j, which differ in js flat level 0 initial beliefs.",
                "This is represented in the level 1 I-ID shown in Fig. 6(a).",
                "According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)).",
                "Agent i is undecided on these two models of j.",
                "If we vary is hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior.",
                "If is probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow js actions: i opens the same door that j opened previously iff is own assessment of the tigers location confirms js pick.",
                "If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)).",
                "Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem.",
                "Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations.",
                "We observed that a single level of belief nesting - beliefs about the others models - was sufficient for followership to emerge in the tiger problem.",
                "However, the epistemological requirements for the emergence of leadership are more complex.",
                "For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i.",
                "As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow js actions over time.",
                "Agent j emerges as a leader if it believes that i will follow it, which implies that js belief must be nested two levels deep to enable it to recognize its leadership role.",
                "Realizing that i will follow presents j with an opportunity to influence is actions in the benefit of the collective good or its self-interest alone.",
                "For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original.",
                "If j alone selects the correct door, it gets the payoff of 10.",
                "On the other hand, if both agents pick the wrong door, their penalties are cut in half.",
                "In this setting, it is in both js best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader.",
                "However, consider a slightly different problem in which j gains from is loss and is penalized if i gains.",
                "Specifically, let is payoff be subtracted from js, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then is loss of 100 becomes js gain.",
                "Agent j believes that i incorrectly thinks that js preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is.",
                "Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following js actions.",
                "We show is normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a).",
                "The policy demonstrates that i will blindly follow js actions.",
                "Since the tiger persists in its original location with a probability of 0.95, i will select the same door again.",
                "If j begins the game with a 99% probability that the tiger is on the right, solving js I-DID nested two levels deep, results in the policy shown in Fig. 8(b).",
                "Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL.",
                "Agent js intention is to deceive i who, it believes, will follow js actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest.",
                "Figure 8: Emergence of deception between agents in the tiger problem.",
                "Behaviors of interest are in bold. * denotes as before. (a) Agent is policy demonstrating that it will blindly follow js actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i. 5.2 Altruism and Reciprocity in the Public Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves.",
                "Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot.",
                "However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes.",
                "Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others contributions.",
                "However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions.",
                "The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4].",
                "Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting.",
                "These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others.",
                "For simplicity, we assume that the game is played between M = 2 agents, i and j.",
                "Let each agent be initially endowed with XT amount of resources.",
                "While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions.",
                "Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute.",
                "The latter action is deThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D).",
                "We assume that the actions are not observable to others.",
                "The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return.",
                "We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain.",
                "Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment.",
                "In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment.",
                "Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent.",
                "For simplicity, we assume that the cost of punishing is same for both the agents.",
                "The one-shot PG game with punishment is shown in Table. 1.",
                "Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action.",
                "If P < XT − ciXT , then defection is the dominating action for both.",
                "If P = XT − ciXT , then the game is not dominance-solvable.",
                "Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a).",
                "We formulate a sequential version of the PG problem with punishment from the perspective of agent i.",
                "Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents.",
                "Each agent may contribute a fixed amount, xc, or defect.",
                "An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot.",
                "Notice that the observations are also indirectly indicative of agent js actions because the state of the public pot is influenced by them.",
                "The amount of resources in agent is private pot, is perfectly observable to i.",
                "The payoffs are analogous to Table. 1.",
                "Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)).",
                "Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect.",
                "Let xc = 1 and the level 0 agent be punished half the times it defects.",
                "With one action remaining, both types of agents choose to contribute to avoid being punished.",
                "With two actions to go, the altruistic type chooses to contribute, while the other defects.",
                "This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids.",
                "Because cj for the non-altruistic type is less, it prefers not to contribute.",
                "With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects.",
                "For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection.",
                "We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level 0 models, mentioned previously, to j (see Fig. 9).",
                "If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps.",
                "This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type.",
                "However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1.",
                "These results demonstrate that the behavior of our altruistic type resembles that found experimentally.",
                "The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic.",
                "We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection.",
                "The reciprocal types marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other.",
                "We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full.",
                "For this prior belief, i chooses to defect.",
                "On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)).",
                "This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from js action to contribute.",
                "Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step.",
                "Agent i therefore chooses to contribute to reciprocate js action.",
                "An analogous reasoning leads i to defect when it observes a meager pot.",
                "With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations.",
                "Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic). 5.3 Strategies in Two-Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2].",
                "Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck.",
                "While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands.",
                "To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit.",
                "Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot.",
                "During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to 8.",
                "Notice that, for example, if a lower numbered card is discarded, the probability of receiving a low card in exchange is now reduced.",
                "We show the level 1 I-ID for the simplified two-player Poker in Fig. 11.",
                "We considered two models (personality types) of agent j.",
                "The conservative type believes that it is likely that its opponent has a high numbered card in its hand.",
                "On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card.",
                "Thus, the two types differ in their beliefs over their opponents hand.",
                "In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution.",
                "With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand.",
                "This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange.",
                "The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one.",
                "Figure 11: (a) Level 1 I-ID of agent i.",
                "The observation reveals information about js hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a).",
                "The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in js hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. is own hand contains the card numbered 8.",
                "The agent starts by keeping its card.",
                "On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card.",
                "If i observes that j discarded its card into the L or H pile, i believes that j is aggressive.",
                "On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange.",
                "Because the probability of receiving a low card is high now, i chooses to keep its card.",
                "On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card.",
                "In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff.",
                "This is partly due to the fact that an observation of, say, L resets the agent is previous time step beliefs over js hand to the low numbered cards only. 6.",
                "DISCUSSION We showed how DIDs may be extended to I-DIDs that enable online sequential decision-making in uncertain multiagent settings.",
                "Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent is three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs.",
                "I-DIDs extend NIDs to allow sequential decision-making over multiple time steps in the presence of other interacting agents.",
                "I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs.",
                "We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality.",
                "Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work.",
                "The first author would like to acknowledge the support of a UGARF grant. 7.",
                "REFERENCES [1] R. J. Aumann.",
                "Interactive epistemology i: Knowledge.",
                "International Journal of Game Theory, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron.",
                "The challenge of poker.",
                "AIJ, 2001. [3] A. Brandenburger and E. Dekel.",
                "Hierarchies of beliefs and common knowledge.",
                "Journal of Economic Theory, 59:189-198, 1993. [4] C. Camerer.",
                "Behavioral Game Theory: Experiments in Strategic Interaction.",
                "Princeton University Press, 2003. [5] E. Fehr and S. Gachter.",
                "Cooperation and punishment in public goods experiments.",
                "American Economic Review, 90(4):980-994, 2000. [6] D. Fudenberg and D. K. Levine.",
                "The Theory of Learning in Games.",
                "MIT Press, 1998. [7] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [8] Y. Gal and A. Pfeffer.",
                "A language for modeling agents decision-making processes in games.",
                "In AAMAS, 2003. [9] P. Gmytrasiewicz and P. Doshi.",
                "A framework for sequential planning in multiagent settings.",
                "JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz and E. Durfee.",
                "Rational coordination in multi-agent environments.",
                "JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi.",
                "Games with incomplete information played by bayesian players.",
                "Management Science, 14(3):159-182, 1967. [12] R. A. Howard and J. E. Matheson.",
                "Influence diagrams.",
                "In R. A. Howard and J. E. Matheson, editors, The Principles and Applications of Decision Analysis.",
                "Strategic Decisions Group, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman, and A. Cassandra.",
                "Planning and acting in partially observable stochastic domains.",
                "Artificial Intelligence Journal, 2, 1998. [14] D. Koller and B. Milch.",
                "Multi-agent influence diagrams for representing and solving games.",
                "In IJCAI, pages 1027-1034, 2001. [15] K. Polich and P. Gmytrasiewicz.",
                "Interactive dynamic influence diagrams.",
                "In GTDT Workshop, AAMAS, 2006. [16] B.",
                "Rathnas., P. Doshi, and P. J. Gmytrasiewicz.",
                "Exact solutions to interactive pomdps using behavioral equivalence.",
                "In Autonomous Agents and Multi-Agent Systems Conference (AAMAS), 2006. [17] S. Russell and P. Norvig.",
                "Artificial Intelligence: A Modern Approach (Second Edition).",
                "Prentice Hall, 2003. [18] R. D. Shachter.",
                "Evaluating influence diagrams.",
                "Operations Research, 34(6):871-882, 1986. [19] D. Suryadi and P. Gmytrasiewicz.",
                "Learning models of other agents using influence diagrams.",
                "In UM, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Las criadas analizan objetivamente el juego, calculando eficientemente el \"perfil de equilibrio Nash\" explotando la estructura de independencia."
            ],
            "translated_text": "",
            "candidates": [
                "Perfil de equilibrio de Nash",
                "perfil de equilibrio Nash"
            ],
            "error": []
        },
        "independence structure": {
            "translated_key": "estructura de independencia",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Graphical Models for Online Solutions to Interactive POMDPs Prashant Doshi Dept.",
                "of Computer Science University of Georgia Athens, GA 30602, USA pdoshi@cs.uga.edu Yifeng Zeng Dept. of Computer Science Aalborg University DK-9220 Aalborg, Denmark yfzeng@cs.aau.edu Qiongyu Chen Dept. of Computer Science National Univ. of Singapore 117543, Singapore chenqy@comp.nus.edu.sg ABSTRACT We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation.",
                "These graphical models called interactive dynamic influence diagrams (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents.",
                "Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Theory 1.",
                "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in partially observable multiagent environments.",
                "They generalize POMDPs [13] to multiagent settings by including the other agents computable models in the state space along with the states of the physical environment.",
                "The models encompass all information influencing the agents behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11].",
                "I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a decision-makers perspective in the interaction.",
                "In [15], Polich and Gmytrasiewicz introduced interactive dynamic influence diagrams (I-DIDs) as the computational representations of I-POMDPs.",
                "I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs contribute to a growing line of work [19] that includes multi-agent influence diagrams (MAIDs) [14], and more recently, networks of influence diagrams (NIDs) [8].",
                "These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agents actions and chance nodes capturing the agents private information.",
                "MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the <br>independence structure</br>.",
                "NIDs extend MAIDs to include agents uncertainty over the game being played and over models of the other agents.",
                "Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent.",
                "Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently.",
                "However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games.",
                "Matters are more complex when we consider interactions that are extended over time, where predictions about others future actions must be made using models that change as the agents act and observe.",
                "I-DIDs address this gap by allowing the representation of other agents models as the values of a special model node.",
                "Both, other agents models and the original agents beliefs over these models are updated over time using special-purpose implementations.",
                "In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID.",
                "Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently.",
                "Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I-DID by [15], and show that it could be replaced by traditional dependency links.",
                "In the previous representation of the I-DID, the update of the agents belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time.",
                "We explicate the semantics of this link by showing how it can be implemented using the traditional dependency links between the chance nodes that constitute the model nodes.",
                "The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs.",
                "We show how IDIDs may be used to model an agents uncertainty over others models, that may themselves be I-DIDs.",
                "Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others models.",
                "Analogous to DIDs, I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. 2.",
                "BACKGROUND: FINITELY NESTED IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents models as part of the state space [9].",
                "Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents models and their beliefs about others.",
                "For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j.",
                "A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment.",
                "Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj .",
                "Here, j is Bayes rational and OCj is js optimality criterion.",
                "SMj is the set of subintentional models of j.",
                "Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent.",
                "We give a recursive bottom-up construction of the interactive state space below.",
                "ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . .",
                "ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent is preferences over its interactive states.",
                "Usually only the physical states will matter.",
                "Agent is policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i.",
                "Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai). 2.1 Belief Update Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes.",
                "However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones.",
                "First, since the state of the physical environment depends on the actions of both agents, is prediction of how the physical state changes has to be made based on its prediction of js actions.",
                "Second, changes in js models have to be included in is belief update.",
                "Specifically, if j is intentional then an update of js beliefs due to its action and observation has to be included.",
                "In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief.",
                "If js model is subintentional, then js probable observations are appended to the observation history contained in the model.",
                "Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update.",
                "For a version of the belief update when js model is subintentional, see [9].",
                "If agent j is also modeled as an I-POMDP, then is belief update invokes js belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke is belief update and so on.",
                "This recursion in belief nesting bottoms out at the 0th level.",
                "At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9]. 2.2 Value Iteration Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)).",
                "Eq. 2 is a basis for value iteration in I-POMDPs.",
                "Agent is optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3.",
                "INTERACTIVEINFLUENCEDIAGRAMS A naive extension of influence diagrams (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes.",
                "However, this approach assumes that the agents actions are controlled using a probability distribution that does not change over time.",
                "Interactive influence diagrams (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node.",
                "We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon.",
                "We note that the probability distribution over the chance node, S, and the model node together represents agent is belief over its interactive states.",
                "In addition to the model 1 The 0th level model is a POMDP: Other agents actions are treated as exogenous events and folded into the T, O, and R functions.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j.",
                "The hexagon is the model node (Mj,l−1) whose structure we show in (b).",
                "Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ).",
                "Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the policy link in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model.",
                "In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs.",
                "The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2.",
                "Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional.",
                "Because the model node contains the alternative models of the other agent as its values, its representation is not trivial.",
                "In particular, some of the models within the node are I-IDs that when solved generate the agents optimal policy in their decision nodes.",
                "Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise.",
                "Borrowing insights from previous work [8], we observe that the model node and the dashed policy link that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b).",
                "The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj.",
                "Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent is beliefs.",
                "The conditional probability table of the chance node, Aj, is a multiplexer that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj].",
                "The values of Mod[Mj] denote the different models of j.",
                "In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1.",
                "The distribution over the node, Mod[Mj], is the agent is belief over the models of j given a physical state.",
                "For more agents, we will have as many model nodes as there are agents.",
                "Notice that Fig. 1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional dependency links.",
                "In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them.",
                "In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes.",
                "This allows I-IDs to be represented and implemented using conventional application tools that target IDs.",
                "Note that we may view the level l I-ID as a NID.",
                "Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2).",
                "If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID.",
                "Note that within the I-IDs (or IDs) at each level, there is only a single decision node.",
                "Thus, our NID does not contain any MAIDs.",
                "Figure 2: A level l I-ID represented as a NID.",
                "The probabilities assigned to the blocks of the NID are is beliefs over js models conditioned on a physical state. 3.2 Solution The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively.",
                "We start by solving the level 0 models, which, if intentional, are traditional IDs.",
                "Their solutions provide probability distributions over the other agents actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID.",
                "The mapping from the level 0 models decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c).",
                "During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj].",
                "As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent is belief over the models of j conditioned on the physical state.",
                "The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j.",
                "Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18].",
                "This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief.",
                "Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agents current belief is known. 4.",
                "INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic influence diagrams (I-DIDs) extend I-IDs (and NIDs) to allow sequential decision-making over several time steps.",
                "Just as DIDs are structured graphical representations of POMDPs, I-DIDs are the graphical online analogs for finitely nested I-POMDPs.",
                "I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents. 4.1 Syntax We depict a general two time-slice I-DID in Fig. 3(a).",
                "In addition to the model nodes and the dashed policy link, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a).",
                "We explained the semantics of the model node and the policy link in the previous section; we describe the model updates next.",
                "The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1.",
                "Recall from Section 2 that an agents intentional model includes its belief.",
                "Because the agents act and receive observations, their models are updated to reflect their changed beliefs.",
                "Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models.",
                "Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models.",
                "Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model.",
                "These steps are a part of agent is belief update formalized using Eq. 1.",
                "In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID.",
                "If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ).",
                "These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation.",
                "The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously.",
                "Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed.",
                "The probability that js updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node.",
                "In order to obtain the probability of js possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ].",
                "Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a multiplexer modulated by Mod[Mt j ].",
                "Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1.",
                "Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1.",
                "Notice that the model update link may be replaced by the dependency links between the chance nodes that constitute the model nodes in the two time slices.",
                "In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them.",
                "Chance nodes and dependency links that not in bold are standard, usually found in DIDs.",
                "Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents js observation at time t + 1.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links.",
                "We note that the possible set of models of the other agent j grows exponentially with the number of time steps.",
                "For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node. 4.2 Solution Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively.",
                "For the purpose of illustration, let l=1 and T=2.",
                "The solution method uses the standard look-ahead technique, projecting the agents action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step.",
                "Because agent i has a belief over js models as well, the lookahead includes finding out the possible models that j could have in the future.",
                "Consequently, each of js subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions.",
                "These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b).",
                "We note the recursive nature of this solution: in solving agent is level 1 I-DID, js level 0 DIDs must be solved.",
                "If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner.",
                "We briefly outline the recursive algorithm for solving agent is Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase 1.",
                "For t from 1 to T − 1 do 2.",
                "If l ≥ 1 then Populate Mt+1 j,l−1 3.",
                "For each mt j in Range(Mt j,l−1) do 4.",
                "Recursively call algorithm with the l − 1 I-ID (or ID) that represents mt j and the horizon, T − t + 1 5.",
                "Map the decision node of the solved I-ID (or ID), OPT(mt j), to a chance node Aj 6.",
                "For each aj in OPT(mt j) do 7.",
                "For each oj in Oj (part of mt j) do 8.",
                "Update js belief, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← New I-ID (or ID) with bt+1 j as the initial belief 10.",
                "Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11.",
                "Add the model node, Mt+1 j,l−1, and the dependency links between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b)) 12.",
                "Add the chance, decision, and utility nodes for t + 1 time slice and the dependency links between them 13.",
                "Establish the CPTs for each chance node and utility node Look-Ahead Phase 14.",
                "Apply the standard look-ahead and backup method to solve the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5.",
                "We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the dependency links and the conditional probability tables for each node.",
                "We particularly focus on establishing and populating the model nodes (lines 3-11).",
                "Note that Range(·) returns the values (lower level models) of the random variable given as input (model node).",
                "In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs.",
                "Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents.",
                "As we mentioned previously, the 0-th level models are the traditional DIDs.",
                "Their solutions provide probability distributions over actions of the agent modeled at that level to I-DIDs at level 1.",
                "Given probability distributions over other agents actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models.",
                "Assume that the number of models considered at each level is bound by a number, M. Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs. 5.",
                "EXAMPLE APPLICATIONS To illustrate the usefulness of I-DIDs, we apply them to three problem domains.",
                "We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it. 5.1 Followership-Leadership in the Multiagent Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9].",
                "The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L).",
                "In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agents opening one of the doors.",
                "When any door is opened, the tiger persists in its original location with a probability of 95%.",
                "Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%.",
                "Agent j, on the other hand, hears growls with a reliability of 95%.",
                "Thus, the setting is such that agent i hears agent j opening doors more reliably than the tigers growls.",
                "This suggests that i could use js actions as an indication of the location of the tiger, as we discuss below.",
                "Each agents preferences are as in the single agent game discussed in [13].",
                "The transition, observation, and reward functions are shown in [16].",
                "A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions.",
                "In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship.",
                "In particular, we analyze the situational and epistemological conditions sufficient for their emergence.",
                "The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the others actions in order to maximize its payoffs.",
                "Let us consider a particular setting of the tiger problem in which agent i believes that js preferences are aligned with its own - both of them just want to get the gold - and js hearing is more reliable in comparison to itself.",
                "As an example, suppose that j, on listening can discern the tigers location 95% of the times compared to is 65% accuracy.",
                "Additionally, agent i does not have any initial information about the tigers location.",
                "In other words, is single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger.",
                "In addition, i considers two models of j, which differ in js flat level 0 initial beliefs.",
                "This is represented in the level 1 I-ID shown in Fig. 6(a).",
                "According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)).",
                "Agent i is undecided on these two models of j.",
                "If we vary is hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior.",
                "If is probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow js actions: i opens the same door that j opened previously iff is own assessment of the tigers location confirms js pick.",
                "If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)).",
                "Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem.",
                "Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations.",
                "We observed that a single level of belief nesting - beliefs about the others models - was sufficient for followership to emerge in the tiger problem.",
                "However, the epistemological requirements for the emergence of leadership are more complex.",
                "For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i.",
                "As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow js actions over time.",
                "Agent j emerges as a leader if it believes that i will follow it, which implies that js belief must be nested two levels deep to enable it to recognize its leadership role.",
                "Realizing that i will follow presents j with an opportunity to influence is actions in the benefit of the collective good or its self-interest alone.",
                "For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original.",
                "If j alone selects the correct door, it gets the payoff of 10.",
                "On the other hand, if both agents pick the wrong door, their penalties are cut in half.",
                "In this setting, it is in both js best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader.",
                "However, consider a slightly different problem in which j gains from is loss and is penalized if i gains.",
                "Specifically, let is payoff be subtracted from js, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then is loss of 100 becomes js gain.",
                "Agent j believes that i incorrectly thinks that js preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is.",
                "Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following js actions.",
                "We show is normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a).",
                "The policy demonstrates that i will blindly follow js actions.",
                "Since the tiger persists in its original location with a probability of 0.95, i will select the same door again.",
                "If j begins the game with a 99% probability that the tiger is on the right, solving js I-DID nested two levels deep, results in the policy shown in Fig. 8(b).",
                "Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL.",
                "Agent js intention is to deceive i who, it believes, will follow js actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest.",
                "Figure 8: Emergence of deception between agents in the tiger problem.",
                "Behaviors of interest are in bold. * denotes as before. (a) Agent is policy demonstrating that it will blindly follow js actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i. 5.2 Altruism and Reciprocity in the Public Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves.",
                "Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot.",
                "However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes.",
                "Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others contributions.",
                "However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions.",
                "The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4].",
                "Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting.",
                "These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others.",
                "For simplicity, we assume that the game is played between M = 2 agents, i and j.",
                "Let each agent be initially endowed with XT amount of resources.",
                "While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions.",
                "Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute.",
                "The latter action is deThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D).",
                "We assume that the actions are not observable to others.",
                "The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return.",
                "We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain.",
                "Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment.",
                "In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment.",
                "Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent.",
                "For simplicity, we assume that the cost of punishing is same for both the agents.",
                "The one-shot PG game with punishment is shown in Table. 1.",
                "Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action.",
                "If P < XT − ciXT , then defection is the dominating action for both.",
                "If P = XT − ciXT , then the game is not dominance-solvable.",
                "Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a).",
                "We formulate a sequential version of the PG problem with punishment from the perspective of agent i.",
                "Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents.",
                "Each agent may contribute a fixed amount, xc, or defect.",
                "An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot.",
                "Notice that the observations are also indirectly indicative of agent js actions because the state of the public pot is influenced by them.",
                "The amount of resources in agent is private pot, is perfectly observable to i.",
                "The payoffs are analogous to Table. 1.",
                "Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)).",
                "Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect.",
                "Let xc = 1 and the level 0 agent be punished half the times it defects.",
                "With one action remaining, both types of agents choose to contribute to avoid being punished.",
                "With two actions to go, the altruistic type chooses to contribute, while the other defects.",
                "This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids.",
                "Because cj for the non-altruistic type is less, it prefers not to contribute.",
                "With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects.",
                "For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection.",
                "We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level 0 models, mentioned previously, to j (see Fig. 9).",
                "If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps.",
                "This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type.",
                "However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1.",
                "These results demonstrate that the behavior of our altruistic type resembles that found experimentally.",
                "The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic.",
                "We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection.",
                "The reciprocal types marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other.",
                "We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full.",
                "For this prior belief, i chooses to defect.",
                "On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)).",
                "This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from js action to contribute.",
                "Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step.",
                "Agent i therefore chooses to contribute to reciprocate js action.",
                "An analogous reasoning leads i to defect when it observes a meager pot.",
                "With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations.",
                "Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic). 5.3 Strategies in Two-Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2].",
                "Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck.",
                "While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands.",
                "To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit.",
                "Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot.",
                "During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to 8.",
                "Notice that, for example, if a lower numbered card is discarded, the probability of receiving a low card in exchange is now reduced.",
                "We show the level 1 I-ID for the simplified two-player Poker in Fig. 11.",
                "We considered two models (personality types) of agent j.",
                "The conservative type believes that it is likely that its opponent has a high numbered card in its hand.",
                "On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card.",
                "Thus, the two types differ in their beliefs over their opponents hand.",
                "In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution.",
                "With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand.",
                "This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange.",
                "The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one.",
                "Figure 11: (a) Level 1 I-ID of agent i.",
                "The observation reveals information about js hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a).",
                "The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in js hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. is own hand contains the card numbered 8.",
                "The agent starts by keeping its card.",
                "On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card.",
                "If i observes that j discarded its card into the L or H pile, i believes that j is aggressive.",
                "On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange.",
                "Because the probability of receiving a low card is high now, i chooses to keep its card.",
                "On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card.",
                "In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff.",
                "This is partly due to the fact that an observation of, say, L resets the agent is previous time step beliefs over js hand to the low numbered cards only. 6.",
                "DISCUSSION We showed how DIDs may be extended to I-DIDs that enable online sequential decision-making in uncertain multiagent settings.",
                "Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent is three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs.",
                "I-DIDs extend NIDs to allow sequential decision-making over multiple time steps in the presence of other interacting agents.",
                "I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs.",
                "We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality.",
                "Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work.",
                "The first author would like to acknowledge the support of a UGARF grant. 7.",
                "REFERENCES [1] R. J. Aumann.",
                "Interactive epistemology i: Knowledge.",
                "International Journal of Game Theory, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron.",
                "The challenge of poker.",
                "AIJ, 2001. [3] A. Brandenburger and E. Dekel.",
                "Hierarchies of beliefs and common knowledge.",
                "Journal of Economic Theory, 59:189-198, 1993. [4] C. Camerer.",
                "Behavioral Game Theory: Experiments in Strategic Interaction.",
                "Princeton University Press, 2003. [5] E. Fehr and S. Gachter.",
                "Cooperation and punishment in public goods experiments.",
                "American Economic Review, 90(4):980-994, 2000. [6] D. Fudenberg and D. K. Levine.",
                "The Theory of Learning in Games.",
                "MIT Press, 1998. [7] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [8] Y. Gal and A. Pfeffer.",
                "A language for modeling agents decision-making processes in games.",
                "In AAMAS, 2003. [9] P. Gmytrasiewicz and P. Doshi.",
                "A framework for sequential planning in multiagent settings.",
                "JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz and E. Durfee.",
                "Rational coordination in multi-agent environments.",
                "JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi.",
                "Games with incomplete information played by bayesian players.",
                "Management Science, 14(3):159-182, 1967. [12] R. A. Howard and J. E. Matheson.",
                "Influence diagrams.",
                "In R. A. Howard and J. E. Matheson, editors, The Principles and Applications of Decision Analysis.",
                "Strategic Decisions Group, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman, and A. Cassandra.",
                "Planning and acting in partially observable stochastic domains.",
                "Artificial Intelligence Journal, 2, 1998. [14] D. Koller and B. Milch.",
                "Multi-agent influence diagrams for representing and solving games.",
                "In IJCAI, pages 1027-1034, 2001. [15] K. Polich and P. Gmytrasiewicz.",
                "Interactive dynamic influence diagrams.",
                "In GTDT Workshop, AAMAS, 2006. [16] B.",
                "Rathnas., P. Doshi, and P. J. Gmytrasiewicz.",
                "Exact solutions to interactive pomdps using behavioral equivalence.",
                "In Autonomous Agents and Multi-Agent Systems Conference (AAMAS), 2006. [17] S. Russell and P. Norvig.",
                "Artificial Intelligence: A Modern Approach (Second Edition).",
                "Prentice Hall, 2003. [18] R. D. Shachter.",
                "Evaluating influence diagrams.",
                "Operations Research, 34(6):871-882, 1986. [19] D. Suryadi and P. Gmytrasiewicz.",
                "Learning models of other agents using influence diagrams.",
                "In UM, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Las criadas analizan objetivamente el juego, calculando eficientemente el perfil de equilibrio Nash explotando la \"estructura de independencia\"."
            ],
            "translated_text": "",
            "candidates": [
                "estructura de independencia",
                "estructura de independencia"
            ],
            "error": []
        },
        "multi-agent influence diagram": {
            "translated_key": "Diagrama de influencia de múltiples agentes",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Graphical Models for Online Solutions to Interactive POMDPs Prashant Doshi Dept.",
                "of Computer Science University of Georgia Athens, GA 30602, USA pdoshi@cs.uga.edu Yifeng Zeng Dept. of Computer Science Aalborg University DK-9220 Aalborg, Denmark yfzeng@cs.aau.edu Qiongyu Chen Dept. of Computer Science National Univ. of Singapore 117543, Singapore chenqy@comp.nus.edu.sg ABSTRACT We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation.",
                "These graphical models called interactive dynamic influence diagrams (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents.",
                "Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Theory 1.",
                "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in partially observable multiagent environments.",
                "They generalize POMDPs [13] to multiagent settings by including the other agents computable models in the state space along with the states of the physical environment.",
                "The models encompass all information influencing the agents behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11].",
                "I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a decision-makers perspective in the interaction.",
                "In [15], Polich and Gmytrasiewicz introduced interactive dynamic influence diagrams (I-DIDs) as the computational representations of I-POMDPs.",
                "I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs contribute to a growing line of work [19] that includes <br>multi-agent influence diagram</br>s (MAIDs) [14], and more recently, networks of influence diagrams (NIDs) [8].",
                "These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agents actions and chance nodes capturing the agents private information.",
                "MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the independence structure.",
                "NIDs extend MAIDs to include agents uncertainty over the game being played and over models of the other agents.",
                "Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent.",
                "Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently.",
                "However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games.",
                "Matters are more complex when we consider interactions that are extended over time, where predictions about others future actions must be made using models that change as the agents act and observe.",
                "I-DIDs address this gap by allowing the representation of other agents models as the values of a special model node.",
                "Both, other agents models and the original agents beliefs over these models are updated over time using special-purpose implementations.",
                "In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID.",
                "Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently.",
                "Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I-DID by [15], and show that it could be replaced by traditional dependency links.",
                "In the previous representation of the I-DID, the update of the agents belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time.",
                "We explicate the semantics of this link by showing how it can be implemented using the traditional dependency links between the chance nodes that constitute the model nodes.",
                "The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs.",
                "We show how IDIDs may be used to model an agents uncertainty over others models, that may themselves be I-DIDs.",
                "Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others models.",
                "Analogous to DIDs, I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. 2.",
                "BACKGROUND: FINITELY NESTED IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents models as part of the state space [9].",
                "Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents models and their beliefs about others.",
                "For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j.",
                "A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment.",
                "Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj .",
                "Here, j is Bayes rational and OCj is js optimality criterion.",
                "SMj is the set of subintentional models of j.",
                "Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent.",
                "We give a recursive bottom-up construction of the interactive state space below.",
                "ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . .",
                "ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent is preferences over its interactive states.",
                "Usually only the physical states will matter.",
                "Agent is policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i.",
                "Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai). 2.1 Belief Update Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes.",
                "However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones.",
                "First, since the state of the physical environment depends on the actions of both agents, is prediction of how the physical state changes has to be made based on its prediction of js actions.",
                "Second, changes in js models have to be included in is belief update.",
                "Specifically, if j is intentional then an update of js beliefs due to its action and observation has to be included.",
                "In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief.",
                "If js model is subintentional, then js probable observations are appended to the observation history contained in the model.",
                "Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update.",
                "For a version of the belief update when js model is subintentional, see [9].",
                "If agent j is also modeled as an I-POMDP, then is belief update invokes js belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke is belief update and so on.",
                "This recursion in belief nesting bottoms out at the 0th level.",
                "At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9]. 2.2 Value Iteration Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)).",
                "Eq. 2 is a basis for value iteration in I-POMDPs.",
                "Agent is optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3.",
                "INTERACTIVEINFLUENCEDIAGRAMS A naive extension of influence diagrams (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes.",
                "However, this approach assumes that the agents actions are controlled using a probability distribution that does not change over time.",
                "Interactive influence diagrams (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node.",
                "We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon.",
                "We note that the probability distribution over the chance node, S, and the model node together represents agent is belief over its interactive states.",
                "In addition to the model 1 The 0th level model is a POMDP: Other agents actions are treated as exogenous events and folded into the T, O, and R functions.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j.",
                "The hexagon is the model node (Mj,l−1) whose structure we show in (b).",
                "Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ).",
                "Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the policy link in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model.",
                "In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs.",
                "The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2.",
                "Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional.",
                "Because the model node contains the alternative models of the other agent as its values, its representation is not trivial.",
                "In particular, some of the models within the node are I-IDs that when solved generate the agents optimal policy in their decision nodes.",
                "Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise.",
                "Borrowing insights from previous work [8], we observe that the model node and the dashed policy link that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b).",
                "The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj.",
                "Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent is beliefs.",
                "The conditional probability table of the chance node, Aj, is a multiplexer that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj].",
                "The values of Mod[Mj] denote the different models of j.",
                "In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1.",
                "The distribution over the node, Mod[Mj], is the agent is belief over the models of j given a physical state.",
                "For more agents, we will have as many model nodes as there are agents.",
                "Notice that Fig. 1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional dependency links.",
                "In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them.",
                "In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes.",
                "This allows I-IDs to be represented and implemented using conventional application tools that target IDs.",
                "Note that we may view the level l I-ID as a NID.",
                "Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2).",
                "If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID.",
                "Note that within the I-IDs (or IDs) at each level, there is only a single decision node.",
                "Thus, our NID does not contain any MAIDs.",
                "Figure 2: A level l I-ID represented as a NID.",
                "The probabilities assigned to the blocks of the NID are is beliefs over js models conditioned on a physical state. 3.2 Solution The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively.",
                "We start by solving the level 0 models, which, if intentional, are traditional IDs.",
                "Their solutions provide probability distributions over the other agents actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID.",
                "The mapping from the level 0 models decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c).",
                "During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj].",
                "As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent is belief over the models of j conditioned on the physical state.",
                "The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j.",
                "Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18].",
                "This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief.",
                "Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agents current belief is known. 4.",
                "INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic influence diagrams (I-DIDs) extend I-IDs (and NIDs) to allow sequential decision-making over several time steps.",
                "Just as DIDs are structured graphical representations of POMDPs, I-DIDs are the graphical online analogs for finitely nested I-POMDPs.",
                "I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents. 4.1 Syntax We depict a general two time-slice I-DID in Fig. 3(a).",
                "In addition to the model nodes and the dashed policy link, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a).",
                "We explained the semantics of the model node and the policy link in the previous section; we describe the model updates next.",
                "The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1.",
                "Recall from Section 2 that an agents intentional model includes its belief.",
                "Because the agents act and receive observations, their models are updated to reflect their changed beliefs.",
                "Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models.",
                "Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models.",
                "Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model.",
                "These steps are a part of agent is belief update formalized using Eq. 1.",
                "In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID.",
                "If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ).",
                "These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation.",
                "The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously.",
                "Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed.",
                "The probability that js updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node.",
                "In order to obtain the probability of js possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ].",
                "Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a multiplexer modulated by Mod[Mt j ].",
                "Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1.",
                "Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1.",
                "Notice that the model update link may be replaced by the dependency links between the chance nodes that constitute the model nodes in the two time slices.",
                "In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them.",
                "Chance nodes and dependency links that not in bold are standard, usually found in DIDs.",
                "Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents js observation at time t + 1.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links.",
                "We note that the possible set of models of the other agent j grows exponentially with the number of time steps.",
                "For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node. 4.2 Solution Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively.",
                "For the purpose of illustration, let l=1 and T=2.",
                "The solution method uses the standard look-ahead technique, projecting the agents action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step.",
                "Because agent i has a belief over js models as well, the lookahead includes finding out the possible models that j could have in the future.",
                "Consequently, each of js subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions.",
                "These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b).",
                "We note the recursive nature of this solution: in solving agent is level 1 I-DID, js level 0 DIDs must be solved.",
                "If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner.",
                "We briefly outline the recursive algorithm for solving agent is Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase 1.",
                "For t from 1 to T − 1 do 2.",
                "If l ≥ 1 then Populate Mt+1 j,l−1 3.",
                "For each mt j in Range(Mt j,l−1) do 4.",
                "Recursively call algorithm with the l − 1 I-ID (or ID) that represents mt j and the horizon, T − t + 1 5.",
                "Map the decision node of the solved I-ID (or ID), OPT(mt j), to a chance node Aj 6.",
                "For each aj in OPT(mt j) do 7.",
                "For each oj in Oj (part of mt j) do 8.",
                "Update js belief, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← New I-ID (or ID) with bt+1 j as the initial belief 10.",
                "Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11.",
                "Add the model node, Mt+1 j,l−1, and the dependency links between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b)) 12.",
                "Add the chance, decision, and utility nodes for t + 1 time slice and the dependency links between them 13.",
                "Establish the CPTs for each chance node and utility node Look-Ahead Phase 14.",
                "Apply the standard look-ahead and backup method to solve the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5.",
                "We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the dependency links and the conditional probability tables for each node.",
                "We particularly focus on establishing and populating the model nodes (lines 3-11).",
                "Note that Range(·) returns the values (lower level models) of the random variable given as input (model node).",
                "In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs.",
                "Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents.",
                "As we mentioned previously, the 0-th level models are the traditional DIDs.",
                "Their solutions provide probability distributions over actions of the agent modeled at that level to I-DIDs at level 1.",
                "Given probability distributions over other agents actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models.",
                "Assume that the number of models considered at each level is bound by a number, M. Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs. 5.",
                "EXAMPLE APPLICATIONS To illustrate the usefulness of I-DIDs, we apply them to three problem domains.",
                "We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it. 5.1 Followership-Leadership in the Multiagent Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9].",
                "The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L).",
                "In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agents opening one of the doors.",
                "When any door is opened, the tiger persists in its original location with a probability of 95%.",
                "Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%.",
                "Agent j, on the other hand, hears growls with a reliability of 95%.",
                "Thus, the setting is such that agent i hears agent j opening doors more reliably than the tigers growls.",
                "This suggests that i could use js actions as an indication of the location of the tiger, as we discuss below.",
                "Each agents preferences are as in the single agent game discussed in [13].",
                "The transition, observation, and reward functions are shown in [16].",
                "A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions.",
                "In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship.",
                "In particular, we analyze the situational and epistemological conditions sufficient for their emergence.",
                "The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the others actions in order to maximize its payoffs.",
                "Let us consider a particular setting of the tiger problem in which agent i believes that js preferences are aligned with its own - both of them just want to get the gold - and js hearing is more reliable in comparison to itself.",
                "As an example, suppose that j, on listening can discern the tigers location 95% of the times compared to is 65% accuracy.",
                "Additionally, agent i does not have any initial information about the tigers location.",
                "In other words, is single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger.",
                "In addition, i considers two models of j, which differ in js flat level 0 initial beliefs.",
                "This is represented in the level 1 I-ID shown in Fig. 6(a).",
                "According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)).",
                "Agent i is undecided on these two models of j.",
                "If we vary is hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior.",
                "If is probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow js actions: i opens the same door that j opened previously iff is own assessment of the tigers location confirms js pick.",
                "If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)).",
                "Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem.",
                "Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations.",
                "We observed that a single level of belief nesting - beliefs about the others models - was sufficient for followership to emerge in the tiger problem.",
                "However, the epistemological requirements for the emergence of leadership are more complex.",
                "For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i.",
                "As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow js actions over time.",
                "Agent j emerges as a leader if it believes that i will follow it, which implies that js belief must be nested two levels deep to enable it to recognize its leadership role.",
                "Realizing that i will follow presents j with an opportunity to influence is actions in the benefit of the collective good or its self-interest alone.",
                "For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original.",
                "If j alone selects the correct door, it gets the payoff of 10.",
                "On the other hand, if both agents pick the wrong door, their penalties are cut in half.",
                "In this setting, it is in both js best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader.",
                "However, consider a slightly different problem in which j gains from is loss and is penalized if i gains.",
                "Specifically, let is payoff be subtracted from js, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then is loss of 100 becomes js gain.",
                "Agent j believes that i incorrectly thinks that js preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is.",
                "Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following js actions.",
                "We show is normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a).",
                "The policy demonstrates that i will blindly follow js actions.",
                "Since the tiger persists in its original location with a probability of 0.95, i will select the same door again.",
                "If j begins the game with a 99% probability that the tiger is on the right, solving js I-DID nested two levels deep, results in the policy shown in Fig. 8(b).",
                "Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL.",
                "Agent js intention is to deceive i who, it believes, will follow js actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest.",
                "Figure 8: Emergence of deception between agents in the tiger problem.",
                "Behaviors of interest are in bold. * denotes as before. (a) Agent is policy demonstrating that it will blindly follow js actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i. 5.2 Altruism and Reciprocity in the Public Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves.",
                "Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot.",
                "However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes.",
                "Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others contributions.",
                "However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions.",
                "The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4].",
                "Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting.",
                "These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others.",
                "For simplicity, we assume that the game is played between M = 2 agents, i and j.",
                "Let each agent be initially endowed with XT amount of resources.",
                "While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions.",
                "Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute.",
                "The latter action is deThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D).",
                "We assume that the actions are not observable to others.",
                "The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return.",
                "We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain.",
                "Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment.",
                "In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment.",
                "Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent.",
                "For simplicity, we assume that the cost of punishing is same for both the agents.",
                "The one-shot PG game with punishment is shown in Table. 1.",
                "Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action.",
                "If P < XT − ciXT , then defection is the dominating action for both.",
                "If P = XT − ciXT , then the game is not dominance-solvable.",
                "Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a).",
                "We formulate a sequential version of the PG problem with punishment from the perspective of agent i.",
                "Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents.",
                "Each agent may contribute a fixed amount, xc, or defect.",
                "An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot.",
                "Notice that the observations are also indirectly indicative of agent js actions because the state of the public pot is influenced by them.",
                "The amount of resources in agent is private pot, is perfectly observable to i.",
                "The payoffs are analogous to Table. 1.",
                "Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)).",
                "Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect.",
                "Let xc = 1 and the level 0 agent be punished half the times it defects.",
                "With one action remaining, both types of agents choose to contribute to avoid being punished.",
                "With two actions to go, the altruistic type chooses to contribute, while the other defects.",
                "This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids.",
                "Because cj for the non-altruistic type is less, it prefers not to contribute.",
                "With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects.",
                "For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection.",
                "We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level 0 models, mentioned previously, to j (see Fig. 9).",
                "If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps.",
                "This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type.",
                "However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1.",
                "These results demonstrate that the behavior of our altruistic type resembles that found experimentally.",
                "The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic.",
                "We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection.",
                "The reciprocal types marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other.",
                "We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full.",
                "For this prior belief, i chooses to defect.",
                "On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)).",
                "This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from js action to contribute.",
                "Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step.",
                "Agent i therefore chooses to contribute to reciprocate js action.",
                "An analogous reasoning leads i to defect when it observes a meager pot.",
                "With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations.",
                "Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic). 5.3 Strategies in Two-Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2].",
                "Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck.",
                "While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands.",
                "To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit.",
                "Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot.",
                "During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to 8.",
                "Notice that, for example, if a lower numbered card is discarded, the probability of receiving a low card in exchange is now reduced.",
                "We show the level 1 I-ID for the simplified two-player Poker in Fig. 11.",
                "We considered two models (personality types) of agent j.",
                "The conservative type believes that it is likely that its opponent has a high numbered card in its hand.",
                "On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card.",
                "Thus, the two types differ in their beliefs over their opponents hand.",
                "In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution.",
                "With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand.",
                "This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange.",
                "The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one.",
                "Figure 11: (a) Level 1 I-ID of agent i.",
                "The observation reveals information about js hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a).",
                "The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in js hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. is own hand contains the card numbered 8.",
                "The agent starts by keeping its card.",
                "On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card.",
                "If i observes that j discarded its card into the L or H pile, i believes that j is aggressive.",
                "On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange.",
                "Because the probability of receiving a low card is high now, i chooses to keep its card.",
                "On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card.",
                "In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff.",
                "This is partly due to the fact that an observation of, say, L resets the agent is previous time step beliefs over js hand to the low numbered cards only. 6.",
                "DISCUSSION We showed how DIDs may be extended to I-DIDs that enable online sequential decision-making in uncertain multiagent settings.",
                "Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent is three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs.",
                "I-DIDs extend NIDs to allow sequential decision-making over multiple time steps in the presence of other interacting agents.",
                "I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs.",
                "We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality.",
                "Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work.",
                "The first author would like to acknowledge the support of a UGARF grant. 7.",
                "REFERENCES [1] R. J. Aumann.",
                "Interactive epistemology i: Knowledge.",
                "International Journal of Game Theory, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron.",
                "The challenge of poker.",
                "AIJ, 2001. [3] A. Brandenburger and E. Dekel.",
                "Hierarchies of beliefs and common knowledge.",
                "Journal of Economic Theory, 59:189-198, 1993. [4] C. Camerer.",
                "Behavioral Game Theory: Experiments in Strategic Interaction.",
                "Princeton University Press, 2003. [5] E. Fehr and S. Gachter.",
                "Cooperation and punishment in public goods experiments.",
                "American Economic Review, 90(4):980-994, 2000. [6] D. Fudenberg and D. K. Levine.",
                "The Theory of Learning in Games.",
                "MIT Press, 1998. [7] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [8] Y. Gal and A. Pfeffer.",
                "A language for modeling agents decision-making processes in games.",
                "In AAMAS, 2003. [9] P. Gmytrasiewicz and P. Doshi.",
                "A framework for sequential planning in multiagent settings.",
                "JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz and E. Durfee.",
                "Rational coordination in multi-agent environments.",
                "JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi.",
                "Games with incomplete information played by bayesian players.",
                "Management Science, 14(3):159-182, 1967. [12] R. A. Howard and J. E. Matheson.",
                "Influence diagrams.",
                "In R. A. Howard and J. E. Matheson, editors, The Principles and Applications of Decision Analysis.",
                "Strategic Decisions Group, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman, and A. Cassandra.",
                "Planning and acting in partially observable stochastic domains.",
                "Artificial Intelligence Journal, 2, 1998. [14] D. Koller and B. Milch.",
                "Multi-agent influence diagrams for representing and solving games.",
                "In IJCAI, pages 1027-1034, 2001. [15] K. Polich and P. Gmytrasiewicz.",
                "Interactive dynamic influence diagrams.",
                "In GTDT Workshop, AAMAS, 2006. [16] B.",
                "Rathnas., P. Doshi, and P. J. Gmytrasiewicz.",
                "Exact solutions to interactive pomdps using behavioral equivalence.",
                "In Autonomous Agents and Multi-Agent Systems Conference (AAMAS), 2006. [17] S. Russell and P. Norvig.",
                "Artificial Intelligence: A Modern Approach (Second Edition).",
                "Prentice Hall, 2003. [18] R. D. Shachter.",
                "Evaluating influence diagrams.",
                "Operations Research, 34(6):871-882, 1986. [19] D. Suryadi and P. Gmytrasiewicz.",
                "Learning models of other agents using influence diagrams.",
                "In UM, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Los I-Did contribuyen a una línea de trabajo creciente [19] que incluye \"Diagrama de influencia de múltiples agentes\" s (mucamas) [14], y más recientemente, redes de diagramas de influencia (NID) [8]."
            ],
            "translated_text": "",
            "candidates": [
                "Diagrama de influencia de múltiples agentes",
                "Diagrama de influencia de múltiples agentes"
            ],
            "error": []
        },
        "networks of influence diagrams": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Graphical Models for Online Solutions to Interactive POMDPs Prashant Doshi Dept.",
                "of Computer Science University of Georgia Athens, GA 30602, USA pdoshi@cs.uga.edu Yifeng Zeng Dept. of Computer Science Aalborg University DK-9220 Aalborg, Denmark yfzeng@cs.aau.edu Qiongyu Chen Dept. of Computer Science National Univ. of Singapore 117543, Singapore chenqy@comp.nus.edu.sg ABSTRACT We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation.",
                "These graphical models called interactive dynamic influence diagrams (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents.",
                "Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Theory 1.",
                "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in partially observable multiagent environments.",
                "They generalize POMDPs [13] to multiagent settings by including the other agents computable models in the state space along with the states of the physical environment.",
                "The models encompass all information influencing the agents behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11].",
                "I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a decision-makers perspective in the interaction.",
                "In [15], Polich and Gmytrasiewicz introduced interactive dynamic influence diagrams (I-DIDs) as the computational representations of I-POMDPs.",
                "I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs contribute to a growing line of work [19] that includes multi-agent influence diagrams (MAIDs) [14], and more recently, <br>networks of influence diagrams</br> (NIDs) [8].",
                "These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agents actions and chance nodes capturing the agents private information.",
                "MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the independence structure.",
                "NIDs extend MAIDs to include agents uncertainty over the game being played and over models of the other agents.",
                "Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent.",
                "Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently.",
                "However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games.",
                "Matters are more complex when we consider interactions that are extended over time, where predictions about others future actions must be made using models that change as the agents act and observe.",
                "I-DIDs address this gap by allowing the representation of other agents models as the values of a special model node.",
                "Both, other agents models and the original agents beliefs over these models are updated over time using special-purpose implementations.",
                "In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID.",
                "Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently.",
                "Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I-DID by [15], and show that it could be replaced by traditional dependency links.",
                "In the previous representation of the I-DID, the update of the agents belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time.",
                "We explicate the semantics of this link by showing how it can be implemented using the traditional dependency links between the chance nodes that constitute the model nodes.",
                "The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs.",
                "We show how IDIDs may be used to model an agents uncertainty over others models, that may themselves be I-DIDs.",
                "Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others models.",
                "Analogous to DIDs, I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. 2.",
                "BACKGROUND: FINITELY NESTED IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents models as part of the state space [9].",
                "Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents models and their beliefs about others.",
                "For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j.",
                "A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment.",
                "Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj .",
                "Here, j is Bayes rational and OCj is js optimality criterion.",
                "SMj is the set of subintentional models of j.",
                "Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent.",
                "We give a recursive bottom-up construction of the interactive state space below.",
                "ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . .",
                "ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent is preferences over its interactive states.",
                "Usually only the physical states will matter.",
                "Agent is policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i.",
                "Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai). 2.1 Belief Update Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes.",
                "However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones.",
                "First, since the state of the physical environment depends on the actions of both agents, is prediction of how the physical state changes has to be made based on its prediction of js actions.",
                "Second, changes in js models have to be included in is belief update.",
                "Specifically, if j is intentional then an update of js beliefs due to its action and observation has to be included.",
                "In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief.",
                "If js model is subintentional, then js probable observations are appended to the observation history contained in the model.",
                "Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update.",
                "For a version of the belief update when js model is subintentional, see [9].",
                "If agent j is also modeled as an I-POMDP, then is belief update invokes js belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke is belief update and so on.",
                "This recursion in belief nesting bottoms out at the 0th level.",
                "At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9]. 2.2 Value Iteration Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)).",
                "Eq. 2 is a basis for value iteration in I-POMDPs.",
                "Agent is optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3.",
                "INTERACTIVEINFLUENCEDIAGRAMS A naive extension of influence diagrams (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes.",
                "However, this approach assumes that the agents actions are controlled using a probability distribution that does not change over time.",
                "Interactive influence diagrams (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node.",
                "We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon.",
                "We note that the probability distribution over the chance node, S, and the model node together represents agent is belief over its interactive states.",
                "In addition to the model 1 The 0th level model is a POMDP: Other agents actions are treated as exogenous events and folded into the T, O, and R functions.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j.",
                "The hexagon is the model node (Mj,l−1) whose structure we show in (b).",
                "Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ).",
                "Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the policy link in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model.",
                "In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs.",
                "The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2.",
                "Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional.",
                "Because the model node contains the alternative models of the other agent as its values, its representation is not trivial.",
                "In particular, some of the models within the node are I-IDs that when solved generate the agents optimal policy in their decision nodes.",
                "Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise.",
                "Borrowing insights from previous work [8], we observe that the model node and the dashed policy link that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b).",
                "The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj.",
                "Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent is beliefs.",
                "The conditional probability table of the chance node, Aj, is a multiplexer that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj].",
                "The values of Mod[Mj] denote the different models of j.",
                "In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1.",
                "The distribution over the node, Mod[Mj], is the agent is belief over the models of j given a physical state.",
                "For more agents, we will have as many model nodes as there are agents.",
                "Notice that Fig. 1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional dependency links.",
                "In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them.",
                "In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes.",
                "This allows I-IDs to be represented and implemented using conventional application tools that target IDs.",
                "Note that we may view the level l I-ID as a NID.",
                "Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2).",
                "If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID.",
                "Note that within the I-IDs (or IDs) at each level, there is only a single decision node.",
                "Thus, our NID does not contain any MAIDs.",
                "Figure 2: A level l I-ID represented as a NID.",
                "The probabilities assigned to the blocks of the NID are is beliefs over js models conditioned on a physical state. 3.2 Solution The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively.",
                "We start by solving the level 0 models, which, if intentional, are traditional IDs.",
                "Their solutions provide probability distributions over the other agents actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID.",
                "The mapping from the level 0 models decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c).",
                "During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj].",
                "As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent is belief over the models of j conditioned on the physical state.",
                "The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j.",
                "Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18].",
                "This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief.",
                "Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agents current belief is known. 4.",
                "INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic influence diagrams (I-DIDs) extend I-IDs (and NIDs) to allow sequential decision-making over several time steps.",
                "Just as DIDs are structured graphical representations of POMDPs, I-DIDs are the graphical online analogs for finitely nested I-POMDPs.",
                "I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents. 4.1 Syntax We depict a general two time-slice I-DID in Fig. 3(a).",
                "In addition to the model nodes and the dashed policy link, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a).",
                "We explained the semantics of the model node and the policy link in the previous section; we describe the model updates next.",
                "The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1.",
                "Recall from Section 2 that an agents intentional model includes its belief.",
                "Because the agents act and receive observations, their models are updated to reflect their changed beliefs.",
                "Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models.",
                "Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models.",
                "Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model.",
                "These steps are a part of agent is belief update formalized using Eq. 1.",
                "In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID.",
                "If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ).",
                "These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation.",
                "The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously.",
                "Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed.",
                "The probability that js updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node.",
                "In order to obtain the probability of js possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ].",
                "Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a multiplexer modulated by Mod[Mt j ].",
                "Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1.",
                "Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1.",
                "Notice that the model update link may be replaced by the dependency links between the chance nodes that constitute the model nodes in the two time slices.",
                "In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them.",
                "Chance nodes and dependency links that not in bold are standard, usually found in DIDs.",
                "Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents js observation at time t + 1.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links.",
                "We note that the possible set of models of the other agent j grows exponentially with the number of time steps.",
                "For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node. 4.2 Solution Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively.",
                "For the purpose of illustration, let l=1 and T=2.",
                "The solution method uses the standard look-ahead technique, projecting the agents action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step.",
                "Because agent i has a belief over js models as well, the lookahead includes finding out the possible models that j could have in the future.",
                "Consequently, each of js subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions.",
                "These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b).",
                "We note the recursive nature of this solution: in solving agent is level 1 I-DID, js level 0 DIDs must be solved.",
                "If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner.",
                "We briefly outline the recursive algorithm for solving agent is Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase 1.",
                "For t from 1 to T − 1 do 2.",
                "If l ≥ 1 then Populate Mt+1 j,l−1 3.",
                "For each mt j in Range(Mt j,l−1) do 4.",
                "Recursively call algorithm with the l − 1 I-ID (or ID) that represents mt j and the horizon, T − t + 1 5.",
                "Map the decision node of the solved I-ID (or ID), OPT(mt j), to a chance node Aj 6.",
                "For each aj in OPT(mt j) do 7.",
                "For each oj in Oj (part of mt j) do 8.",
                "Update js belief, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← New I-ID (or ID) with bt+1 j as the initial belief 10.",
                "Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11.",
                "Add the model node, Mt+1 j,l−1, and the dependency links between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b)) 12.",
                "Add the chance, decision, and utility nodes for t + 1 time slice and the dependency links between them 13.",
                "Establish the CPTs for each chance node and utility node Look-Ahead Phase 14.",
                "Apply the standard look-ahead and backup method to solve the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5.",
                "We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the dependency links and the conditional probability tables for each node.",
                "We particularly focus on establishing and populating the model nodes (lines 3-11).",
                "Note that Range(·) returns the values (lower level models) of the random variable given as input (model node).",
                "In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs.",
                "Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents.",
                "As we mentioned previously, the 0-th level models are the traditional DIDs.",
                "Their solutions provide probability distributions over actions of the agent modeled at that level to I-DIDs at level 1.",
                "Given probability distributions over other agents actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models.",
                "Assume that the number of models considered at each level is bound by a number, M. Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs. 5.",
                "EXAMPLE APPLICATIONS To illustrate the usefulness of I-DIDs, we apply them to three problem domains.",
                "We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it. 5.1 Followership-Leadership in the Multiagent Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9].",
                "The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L).",
                "In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agents opening one of the doors.",
                "When any door is opened, the tiger persists in its original location with a probability of 95%.",
                "Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%.",
                "Agent j, on the other hand, hears growls with a reliability of 95%.",
                "Thus, the setting is such that agent i hears agent j opening doors more reliably than the tigers growls.",
                "This suggests that i could use js actions as an indication of the location of the tiger, as we discuss below.",
                "Each agents preferences are as in the single agent game discussed in [13].",
                "The transition, observation, and reward functions are shown in [16].",
                "A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions.",
                "In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship.",
                "In particular, we analyze the situational and epistemological conditions sufficient for their emergence.",
                "The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the others actions in order to maximize its payoffs.",
                "Let us consider a particular setting of the tiger problem in which agent i believes that js preferences are aligned with its own - both of them just want to get the gold - and js hearing is more reliable in comparison to itself.",
                "As an example, suppose that j, on listening can discern the tigers location 95% of the times compared to is 65% accuracy.",
                "Additionally, agent i does not have any initial information about the tigers location.",
                "In other words, is single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger.",
                "In addition, i considers two models of j, which differ in js flat level 0 initial beliefs.",
                "This is represented in the level 1 I-ID shown in Fig. 6(a).",
                "According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)).",
                "Agent i is undecided on these two models of j.",
                "If we vary is hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior.",
                "If is probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow js actions: i opens the same door that j opened previously iff is own assessment of the tigers location confirms js pick.",
                "If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)).",
                "Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem.",
                "Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations.",
                "We observed that a single level of belief nesting - beliefs about the others models - was sufficient for followership to emerge in the tiger problem.",
                "However, the epistemological requirements for the emergence of leadership are more complex.",
                "For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i.",
                "As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow js actions over time.",
                "Agent j emerges as a leader if it believes that i will follow it, which implies that js belief must be nested two levels deep to enable it to recognize its leadership role.",
                "Realizing that i will follow presents j with an opportunity to influence is actions in the benefit of the collective good or its self-interest alone.",
                "For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original.",
                "If j alone selects the correct door, it gets the payoff of 10.",
                "On the other hand, if both agents pick the wrong door, their penalties are cut in half.",
                "In this setting, it is in both js best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader.",
                "However, consider a slightly different problem in which j gains from is loss and is penalized if i gains.",
                "Specifically, let is payoff be subtracted from js, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then is loss of 100 becomes js gain.",
                "Agent j believes that i incorrectly thinks that js preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is.",
                "Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following js actions.",
                "We show is normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a).",
                "The policy demonstrates that i will blindly follow js actions.",
                "Since the tiger persists in its original location with a probability of 0.95, i will select the same door again.",
                "If j begins the game with a 99% probability that the tiger is on the right, solving js I-DID nested two levels deep, results in the policy shown in Fig. 8(b).",
                "Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL.",
                "Agent js intention is to deceive i who, it believes, will follow js actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest.",
                "Figure 8: Emergence of deception between agents in the tiger problem.",
                "Behaviors of interest are in bold. * denotes as before. (a) Agent is policy demonstrating that it will blindly follow js actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i. 5.2 Altruism and Reciprocity in the Public Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves.",
                "Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot.",
                "However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes.",
                "Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others contributions.",
                "However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions.",
                "The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4].",
                "Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting.",
                "These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others.",
                "For simplicity, we assume that the game is played between M = 2 agents, i and j.",
                "Let each agent be initially endowed with XT amount of resources.",
                "While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions.",
                "Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute.",
                "The latter action is deThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D).",
                "We assume that the actions are not observable to others.",
                "The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return.",
                "We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain.",
                "Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment.",
                "In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment.",
                "Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent.",
                "For simplicity, we assume that the cost of punishing is same for both the agents.",
                "The one-shot PG game with punishment is shown in Table. 1.",
                "Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action.",
                "If P < XT − ciXT , then defection is the dominating action for both.",
                "If P = XT − ciXT , then the game is not dominance-solvable.",
                "Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a).",
                "We formulate a sequential version of the PG problem with punishment from the perspective of agent i.",
                "Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents.",
                "Each agent may contribute a fixed amount, xc, or defect.",
                "An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot.",
                "Notice that the observations are also indirectly indicative of agent js actions because the state of the public pot is influenced by them.",
                "The amount of resources in agent is private pot, is perfectly observable to i.",
                "The payoffs are analogous to Table. 1.",
                "Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)).",
                "Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect.",
                "Let xc = 1 and the level 0 agent be punished half the times it defects.",
                "With one action remaining, both types of agents choose to contribute to avoid being punished.",
                "With two actions to go, the altruistic type chooses to contribute, while the other defects.",
                "This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids.",
                "Because cj for the non-altruistic type is less, it prefers not to contribute.",
                "With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects.",
                "For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection.",
                "We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level 0 models, mentioned previously, to j (see Fig. 9).",
                "If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps.",
                "This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type.",
                "However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1.",
                "These results demonstrate that the behavior of our altruistic type resembles that found experimentally.",
                "The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic.",
                "We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection.",
                "The reciprocal types marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other.",
                "We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full.",
                "For this prior belief, i chooses to defect.",
                "On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)).",
                "This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from js action to contribute.",
                "Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step.",
                "Agent i therefore chooses to contribute to reciprocate js action.",
                "An analogous reasoning leads i to defect when it observes a meager pot.",
                "With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations.",
                "Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic). 5.3 Strategies in Two-Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2].",
                "Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck.",
                "While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands.",
                "To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit.",
                "Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot.",
                "During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to 8.",
                "Notice that, for example, if a lower numbered card is discarded, the probability of receiving a low card in exchange is now reduced.",
                "We show the level 1 I-ID for the simplified two-player Poker in Fig. 11.",
                "We considered two models (personality types) of agent j.",
                "The conservative type believes that it is likely that its opponent has a high numbered card in its hand.",
                "On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card.",
                "Thus, the two types differ in their beliefs over their opponents hand.",
                "In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution.",
                "With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand.",
                "This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange.",
                "The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one.",
                "Figure 11: (a) Level 1 I-ID of agent i.",
                "The observation reveals information about js hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a).",
                "The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in js hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. is own hand contains the card numbered 8.",
                "The agent starts by keeping its card.",
                "On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card.",
                "If i observes that j discarded its card into the L or H pile, i believes that j is aggressive.",
                "On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange.",
                "Because the probability of receiving a low card is high now, i chooses to keep its card.",
                "On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card.",
                "In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff.",
                "This is partly due to the fact that an observation of, say, L resets the agent is previous time step beliefs over js hand to the low numbered cards only. 6.",
                "DISCUSSION We showed how DIDs may be extended to I-DIDs that enable online sequential decision-making in uncertain multiagent settings.",
                "Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent is three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs.",
                "I-DIDs extend NIDs to allow sequential decision-making over multiple time steps in the presence of other interacting agents.",
                "I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs.",
                "We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality.",
                "Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work.",
                "The first author would like to acknowledge the support of a UGARF grant. 7.",
                "REFERENCES [1] R. J. Aumann.",
                "Interactive epistemology i: Knowledge.",
                "International Journal of Game Theory, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron.",
                "The challenge of poker.",
                "AIJ, 2001. [3] A. Brandenburger and E. Dekel.",
                "Hierarchies of beliefs and common knowledge.",
                "Journal of Economic Theory, 59:189-198, 1993. [4] C. Camerer.",
                "Behavioral Game Theory: Experiments in Strategic Interaction.",
                "Princeton University Press, 2003. [5] E. Fehr and S. Gachter.",
                "Cooperation and punishment in public goods experiments.",
                "American Economic Review, 90(4):980-994, 2000. [6] D. Fudenberg and D. K. Levine.",
                "The Theory of Learning in Games.",
                "MIT Press, 1998. [7] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [8] Y. Gal and A. Pfeffer.",
                "A language for modeling agents decision-making processes in games.",
                "In AAMAS, 2003. [9] P. Gmytrasiewicz and P. Doshi.",
                "A framework for sequential planning in multiagent settings.",
                "JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz and E. Durfee.",
                "Rational coordination in multi-agent environments.",
                "JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi.",
                "Games with incomplete information played by bayesian players.",
                "Management Science, 14(3):159-182, 1967. [12] R. A. Howard and J. E. Matheson.",
                "Influence diagrams.",
                "In R. A. Howard and J. E. Matheson, editors, The Principles and Applications of Decision Analysis.",
                "Strategic Decisions Group, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman, and A. Cassandra.",
                "Planning and acting in partially observable stochastic domains.",
                "Artificial Intelligence Journal, 2, 1998. [14] D. Koller and B. Milch.",
                "Multi-agent influence diagrams for representing and solving games.",
                "In IJCAI, pages 1027-1034, 2001. [15] K. Polich and P. Gmytrasiewicz.",
                "Interactive dynamic influence diagrams.",
                "In GTDT Workshop, AAMAS, 2006. [16] B.",
                "Rathnas., P. Doshi, and P. J. Gmytrasiewicz.",
                "Exact solutions to interactive pomdps using behavioral equivalence.",
                "In Autonomous Agents and Multi-Agent Systems Conference (AAMAS), 2006. [17] S. Russell and P. Norvig.",
                "Artificial Intelligence: A Modern Approach (Second Edition).",
                "Prentice Hall, 2003. [18] R. D. Shachter.",
                "Evaluating influence diagrams.",
                "Operations Research, 34(6):871-882, 1986. [19] D. Suryadi and P. Gmytrasiewicz.",
                "Learning models of other agents using influence diagrams.",
                "In UM, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Los I-Did contribuyen a una línea de trabajo creciente [19] que incluye diagramas de influencia de múltiples agentes (criadas) [14], y más recientemente, \"redes de influencia de redes\" (NID) [8]."
            ],
            "translated_text": "",
            "candidates": [
                "Diagramas de redes de influencia",
                "redes de influencia de redes"
            ],
            "error": []
        },
        "influence diagram network": {
            "translated_key": "Red de diagrama de influencia",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Graphical Models for Online Solutions to Interactive POMDPs Prashant Doshi Dept.",
                "of Computer Science University of Georgia Athens, GA 30602, USA pdoshi@cs.uga.edu Yifeng Zeng Dept. of Computer Science Aalborg University DK-9220 Aalborg, Denmark yfzeng@cs.aau.edu Qiongyu Chen Dept. of Computer Science National Univ. of Singapore 117543, Singapore chenqy@comp.nus.edu.sg ABSTRACT We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation.",
                "These graphical models called interactive dynamic influence diagrams (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents.",
                "Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Theory 1.",
                "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in partially observable multiagent environments.",
                "They generalize POMDPs [13] to multiagent settings by including the other agents computable models in the state space along with the states of the physical environment.",
                "The models encompass all information influencing the agents behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11].",
                "I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a decision-makers perspective in the interaction.",
                "In [15], Polich and Gmytrasiewicz introduced interactive dynamic influence diagrams (I-DIDs) as the computational representations of I-POMDPs.",
                "I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs contribute to a growing line of work [19] that includes multi-agent influence diagrams (MAIDs) [14], and more recently, networks of influence diagrams (NIDs) [8].",
                "These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agents actions and chance nodes capturing the agents private information.",
                "MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the independence structure.",
                "NIDs extend MAIDs to include agents uncertainty over the game being played and over models of the other agents.",
                "Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent.",
                "Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently.",
                "However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games.",
                "Matters are more complex when we consider interactions that are extended over time, where predictions about others future actions must be made using models that change as the agents act and observe.",
                "I-DIDs address this gap by allowing the representation of other agents models as the values of a special model node.",
                "Both, other agents models and the original agents beliefs over these models are updated over time using special-purpose implementations.",
                "In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID.",
                "Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently.",
                "Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I-DID by [15], and show that it could be replaced by traditional dependency links.",
                "In the previous representation of the I-DID, the update of the agents belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time.",
                "We explicate the semantics of this link by showing how it can be implemented using the traditional dependency links between the chance nodes that constitute the model nodes.",
                "The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs.",
                "We show how IDIDs may be used to model an agents uncertainty over others models, that may themselves be I-DIDs.",
                "Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others models.",
                "Analogous to DIDs, I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. 2.",
                "BACKGROUND: FINITELY NESTED IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents models as part of the state space [9].",
                "Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents models and their beliefs about others.",
                "For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j.",
                "A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment.",
                "Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj .",
                "Here, j is Bayes rational and OCj is js optimality criterion.",
                "SMj is the set of subintentional models of j.",
                "Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent.",
                "We give a recursive bottom-up construction of the interactive state space below.",
                "ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . .",
                "ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent is preferences over its interactive states.",
                "Usually only the physical states will matter.",
                "Agent is policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i.",
                "Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai). 2.1 Belief Update Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes.",
                "However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones.",
                "First, since the state of the physical environment depends on the actions of both agents, is prediction of how the physical state changes has to be made based on its prediction of js actions.",
                "Second, changes in js models have to be included in is belief update.",
                "Specifically, if j is intentional then an update of js beliefs due to its action and observation has to be included.",
                "In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief.",
                "If js model is subintentional, then js probable observations are appended to the observation history contained in the model.",
                "Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update.",
                "For a version of the belief update when js model is subintentional, see [9].",
                "If agent j is also modeled as an I-POMDP, then is belief update invokes js belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke is belief update and so on.",
                "This recursion in belief nesting bottoms out at the 0th level.",
                "At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9]. 2.2 Value Iteration Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)).",
                "Eq. 2 is a basis for value iteration in I-POMDPs.",
                "Agent is optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3.",
                "INTERACTIVEINFLUENCEDIAGRAMS A naive extension of influence diagrams (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes.",
                "However, this approach assumes that the agents actions are controlled using a probability distribution that does not change over time.",
                "Interactive influence diagrams (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node.",
                "We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon.",
                "We note that the probability distribution over the chance node, S, and the model node together represents agent is belief over its interactive states.",
                "In addition to the model 1 The 0th level model is a POMDP: Other agents actions are treated as exogenous events and folded into the T, O, and R functions.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j.",
                "The hexagon is the model node (Mj,l−1) whose structure we show in (b).",
                "Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ).",
                "Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the policy link in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model.",
                "In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs.",
                "The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2.",
                "Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional.",
                "Because the model node contains the alternative models of the other agent as its values, its representation is not trivial.",
                "In particular, some of the models within the node are I-IDs that when solved generate the agents optimal policy in their decision nodes.",
                "Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise.",
                "Borrowing insights from previous work [8], we observe that the model node and the dashed policy link that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b).",
                "The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj.",
                "Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent is beliefs.",
                "The conditional probability table of the chance node, Aj, is a multiplexer that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj].",
                "The values of Mod[Mj] denote the different models of j.",
                "In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1.",
                "The distribution over the node, Mod[Mj], is the agent is belief over the models of j given a physical state.",
                "For more agents, we will have as many model nodes as there are agents.",
                "Notice that Fig. 1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional dependency links.",
                "In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them.",
                "In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes.",
                "This allows I-IDs to be represented and implemented using conventional application tools that target IDs.",
                "Note that we may view the level l I-ID as a NID.",
                "Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2).",
                "If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID.",
                "Note that within the I-IDs (or IDs) at each level, there is only a single decision node.",
                "Thus, our NID does not contain any MAIDs.",
                "Figure 2: A level l I-ID represented as a NID.",
                "The probabilities assigned to the blocks of the NID are is beliefs over js models conditioned on a physical state. 3.2 Solution The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively.",
                "We start by solving the level 0 models, which, if intentional, are traditional IDs.",
                "Their solutions provide probability distributions over the other agents actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID.",
                "The mapping from the level 0 models decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c).",
                "During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj].",
                "As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent is belief over the models of j conditioned on the physical state.",
                "The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j.",
                "Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18].",
                "This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief.",
                "Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agents current belief is known. 4.",
                "INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic influence diagrams (I-DIDs) extend I-IDs (and NIDs) to allow sequential decision-making over several time steps.",
                "Just as DIDs are structured graphical representations of POMDPs, I-DIDs are the graphical online analogs for finitely nested I-POMDPs.",
                "I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents. 4.1 Syntax We depict a general two time-slice I-DID in Fig. 3(a).",
                "In addition to the model nodes and the dashed policy link, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a).",
                "We explained the semantics of the model node and the policy link in the previous section; we describe the model updates next.",
                "The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1.",
                "Recall from Section 2 that an agents intentional model includes its belief.",
                "Because the agents act and receive observations, their models are updated to reflect their changed beliefs.",
                "Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models.",
                "Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models.",
                "Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model.",
                "These steps are a part of agent is belief update formalized using Eq. 1.",
                "In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID.",
                "If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ).",
                "These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation.",
                "The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously.",
                "Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed.",
                "The probability that js updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node.",
                "In order to obtain the probability of js possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ].",
                "Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a multiplexer modulated by Mod[Mt j ].",
                "Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1.",
                "Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1.",
                "Notice that the model update link may be replaced by the dependency links between the chance nodes that constitute the model nodes in the two time slices.",
                "In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them.",
                "Chance nodes and dependency links that not in bold are standard, usually found in DIDs.",
                "Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents js observation at time t + 1.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links.",
                "We note that the possible set of models of the other agent j grows exponentially with the number of time steps.",
                "For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node. 4.2 Solution Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively.",
                "For the purpose of illustration, let l=1 and T=2.",
                "The solution method uses the standard look-ahead technique, projecting the agents action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step.",
                "Because agent i has a belief over js models as well, the lookahead includes finding out the possible models that j could have in the future.",
                "Consequently, each of js subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions.",
                "These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b).",
                "We note the recursive nature of this solution: in solving agent is level 1 I-DID, js level 0 DIDs must be solved.",
                "If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner.",
                "We briefly outline the recursive algorithm for solving agent is Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase 1.",
                "For t from 1 to T − 1 do 2.",
                "If l ≥ 1 then Populate Mt+1 j,l−1 3.",
                "For each mt j in Range(Mt j,l−1) do 4.",
                "Recursively call algorithm with the l − 1 I-ID (or ID) that represents mt j and the horizon, T − t + 1 5.",
                "Map the decision node of the solved I-ID (or ID), OPT(mt j), to a chance node Aj 6.",
                "For each aj in OPT(mt j) do 7.",
                "For each oj in Oj (part of mt j) do 8.",
                "Update js belief, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← New I-ID (or ID) with bt+1 j as the initial belief 10.",
                "Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11.",
                "Add the model node, Mt+1 j,l−1, and the dependency links between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b)) 12.",
                "Add the chance, decision, and utility nodes for t + 1 time slice and the dependency links between them 13.",
                "Establish the CPTs for each chance node and utility node Look-Ahead Phase 14.",
                "Apply the standard look-ahead and backup method to solve the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5.",
                "We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the dependency links and the conditional probability tables for each node.",
                "We particularly focus on establishing and populating the model nodes (lines 3-11).",
                "Note that Range(·) returns the values (lower level models) of the random variable given as input (model node).",
                "In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs.",
                "Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents.",
                "As we mentioned previously, the 0-th level models are the traditional DIDs.",
                "Their solutions provide probability distributions over actions of the agent modeled at that level to I-DIDs at level 1.",
                "Given probability distributions over other agents actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models.",
                "Assume that the number of models considered at each level is bound by a number, M. Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs. 5.",
                "EXAMPLE APPLICATIONS To illustrate the usefulness of I-DIDs, we apply them to three problem domains.",
                "We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it. 5.1 Followership-Leadership in the Multiagent Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9].",
                "The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L).",
                "In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agents opening one of the doors.",
                "When any door is opened, the tiger persists in its original location with a probability of 95%.",
                "Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%.",
                "Agent j, on the other hand, hears growls with a reliability of 95%.",
                "Thus, the setting is such that agent i hears agent j opening doors more reliably than the tigers growls.",
                "This suggests that i could use js actions as an indication of the location of the tiger, as we discuss below.",
                "Each agents preferences are as in the single agent game discussed in [13].",
                "The transition, observation, and reward functions are shown in [16].",
                "A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions.",
                "In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship.",
                "In particular, we analyze the situational and epistemological conditions sufficient for their emergence.",
                "The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the others actions in order to maximize its payoffs.",
                "Let us consider a particular setting of the tiger problem in which agent i believes that js preferences are aligned with its own - both of them just want to get the gold - and js hearing is more reliable in comparison to itself.",
                "As an example, suppose that j, on listening can discern the tigers location 95% of the times compared to is 65% accuracy.",
                "Additionally, agent i does not have any initial information about the tigers location.",
                "In other words, is single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger.",
                "In addition, i considers two models of j, which differ in js flat level 0 initial beliefs.",
                "This is represented in the level 1 I-ID shown in Fig. 6(a).",
                "According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)).",
                "Agent i is undecided on these two models of j.",
                "If we vary is hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior.",
                "If is probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow js actions: i opens the same door that j opened previously iff is own assessment of the tigers location confirms js pick.",
                "If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)).",
                "Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem.",
                "Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations.",
                "We observed that a single level of belief nesting - beliefs about the others models - was sufficient for followership to emerge in the tiger problem.",
                "However, the epistemological requirements for the emergence of leadership are more complex.",
                "For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i.",
                "As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow js actions over time.",
                "Agent j emerges as a leader if it believes that i will follow it, which implies that js belief must be nested two levels deep to enable it to recognize its leadership role.",
                "Realizing that i will follow presents j with an opportunity to influence is actions in the benefit of the collective good or its self-interest alone.",
                "For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original.",
                "If j alone selects the correct door, it gets the payoff of 10.",
                "On the other hand, if both agents pick the wrong door, their penalties are cut in half.",
                "In this setting, it is in both js best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader.",
                "However, consider a slightly different problem in which j gains from is loss and is penalized if i gains.",
                "Specifically, let is payoff be subtracted from js, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then is loss of 100 becomes js gain.",
                "Agent j believes that i incorrectly thinks that js preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is.",
                "Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following js actions.",
                "We show is normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a).",
                "The policy demonstrates that i will blindly follow js actions.",
                "Since the tiger persists in its original location with a probability of 0.95, i will select the same door again.",
                "If j begins the game with a 99% probability that the tiger is on the right, solving js I-DID nested two levels deep, results in the policy shown in Fig. 8(b).",
                "Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL.",
                "Agent js intention is to deceive i who, it believes, will follow js actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest.",
                "Figure 8: Emergence of deception between agents in the tiger problem.",
                "Behaviors of interest are in bold. * denotes as before. (a) Agent is policy demonstrating that it will blindly follow js actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i. 5.2 Altruism and Reciprocity in the Public Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves.",
                "Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot.",
                "However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes.",
                "Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others contributions.",
                "However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions.",
                "The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4].",
                "Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting.",
                "These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others.",
                "For simplicity, we assume that the game is played between M = 2 agents, i and j.",
                "Let each agent be initially endowed with XT amount of resources.",
                "While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions.",
                "Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute.",
                "The latter action is deThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D).",
                "We assume that the actions are not observable to others.",
                "The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return.",
                "We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain.",
                "Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment.",
                "In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment.",
                "Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent.",
                "For simplicity, we assume that the cost of punishing is same for both the agents.",
                "The one-shot PG game with punishment is shown in Table. 1.",
                "Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action.",
                "If P < XT − ciXT , then defection is the dominating action for both.",
                "If P = XT − ciXT , then the game is not dominance-solvable.",
                "Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a).",
                "We formulate a sequential version of the PG problem with punishment from the perspective of agent i.",
                "Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents.",
                "Each agent may contribute a fixed amount, xc, or defect.",
                "An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot.",
                "Notice that the observations are also indirectly indicative of agent js actions because the state of the public pot is influenced by them.",
                "The amount of resources in agent is private pot, is perfectly observable to i.",
                "The payoffs are analogous to Table. 1.",
                "Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)).",
                "Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect.",
                "Let xc = 1 and the level 0 agent be punished half the times it defects.",
                "With one action remaining, both types of agents choose to contribute to avoid being punished.",
                "With two actions to go, the altruistic type chooses to contribute, while the other defects.",
                "This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids.",
                "Because cj for the non-altruistic type is less, it prefers not to contribute.",
                "With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects.",
                "For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection.",
                "We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level 0 models, mentioned previously, to j (see Fig. 9).",
                "If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps.",
                "This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type.",
                "However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1.",
                "These results demonstrate that the behavior of our altruistic type resembles that found experimentally.",
                "The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic.",
                "We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection.",
                "The reciprocal types marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other.",
                "We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full.",
                "For this prior belief, i chooses to defect.",
                "On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)).",
                "This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from js action to contribute.",
                "Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step.",
                "Agent i therefore chooses to contribute to reciprocate js action.",
                "An analogous reasoning leads i to defect when it observes a meager pot.",
                "With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations.",
                "Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic). 5.3 Strategies in Two-Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2].",
                "Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck.",
                "While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands.",
                "To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit.",
                "Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot.",
                "During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to 8.",
                "Notice that, for example, if a lower numbered card is discarded, the probability of receiving a low card in exchange is now reduced.",
                "We show the level 1 I-ID for the simplified two-player Poker in Fig. 11.",
                "We considered two models (personality types) of agent j.",
                "The conservative type believes that it is likely that its opponent has a high numbered card in its hand.",
                "On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card.",
                "Thus, the two types differ in their beliefs over their opponents hand.",
                "In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution.",
                "With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand.",
                "This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange.",
                "The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one.",
                "Figure 11: (a) Level 1 I-ID of agent i.",
                "The observation reveals information about js hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a).",
                "The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in js hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. is own hand contains the card numbered 8.",
                "The agent starts by keeping its card.",
                "On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card.",
                "If i observes that j discarded its card into the L or H pile, i believes that j is aggressive.",
                "On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange.",
                "Because the probability of receiving a low card is high now, i chooses to keep its card.",
                "On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card.",
                "In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff.",
                "This is partly due to the fact that an observation of, say, L resets the agent is previous time step beliefs over js hand to the low numbered cards only. 6.",
                "DISCUSSION We showed how DIDs may be extended to I-DIDs that enable online sequential decision-making in uncertain multiagent settings.",
                "Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent is three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs.",
                "I-DIDs extend NIDs to allow sequential decision-making over multiple time steps in the presence of other interacting agents.",
                "I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs.",
                "We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality.",
                "Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work.",
                "The first author would like to acknowledge the support of a UGARF grant. 7.",
                "REFERENCES [1] R. J. Aumann.",
                "Interactive epistemology i: Knowledge.",
                "International Journal of Game Theory, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron.",
                "The challenge of poker.",
                "AIJ, 2001. [3] A. Brandenburger and E. Dekel.",
                "Hierarchies of beliefs and common knowledge.",
                "Journal of Economic Theory, 59:189-198, 1993. [4] C. Camerer.",
                "Behavioral Game Theory: Experiments in Strategic Interaction.",
                "Princeton University Press, 2003. [5] E. Fehr and S. Gachter.",
                "Cooperation and punishment in public goods experiments.",
                "American Economic Review, 90(4):980-994, 2000. [6] D. Fudenberg and D. K. Levine.",
                "The Theory of Learning in Games.",
                "MIT Press, 1998. [7] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [8] Y. Gal and A. Pfeffer.",
                "A language for modeling agents decision-making processes in games.",
                "In AAMAS, 2003. [9] P. Gmytrasiewicz and P. Doshi.",
                "A framework for sequential planning in multiagent settings.",
                "JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz and E. Durfee.",
                "Rational coordination in multi-agent environments.",
                "JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi.",
                "Games with incomplete information played by bayesian players.",
                "Management Science, 14(3):159-182, 1967. [12] R. A. Howard and J. E. Matheson.",
                "Influence diagrams.",
                "In R. A. Howard and J. E. Matheson, editors, The Principles and Applications of Decision Analysis.",
                "Strategic Decisions Group, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman, and A. Cassandra.",
                "Planning and acting in partially observable stochastic domains.",
                "Artificial Intelligence Journal, 2, 1998. [14] D. Koller and B. Milch.",
                "Multi-agent influence diagrams for representing and solving games.",
                "In IJCAI, pages 1027-1034, 2001. [15] K. Polich and P. Gmytrasiewicz.",
                "Interactive dynamic influence diagrams.",
                "In GTDT Workshop, AAMAS, 2006. [16] B.",
                "Rathnas., P. Doshi, and P. J. Gmytrasiewicz.",
                "Exact solutions to interactive pomdps using behavioral equivalence.",
                "In Autonomous Agents and Multi-Agent Systems Conference (AAMAS), 2006. [17] S. Russell and P. Norvig.",
                "Artificial Intelligence: A Modern Approach (Second Edition).",
                "Prentice Hall, 2003. [18] R. D. Shachter.",
                "Evaluating influence diagrams.",
                "Operations Research, 34(6):871-882, 1986. [19] D. Suryadi and P. Gmytrasiewicz.",
                "Learning models of other agents using influence diagrams.",
                "In UM, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "multiplexer": {
            "translated_key": "multiplexor",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Graphical Models for Online Solutions to Interactive POMDPs Prashant Doshi Dept.",
                "of Computer Science University of Georgia Athens, GA 30602, USA pdoshi@cs.uga.edu Yifeng Zeng Dept. of Computer Science Aalborg University DK-9220 Aalborg, Denmark yfzeng@cs.aau.edu Qiongyu Chen Dept. of Computer Science National Univ. of Singapore 117543, Singapore chenqy@comp.nus.edu.sg ABSTRACT We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation.",
                "These graphical models called interactive dynamic influence diagrams (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents.",
                "Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Theory 1.",
                "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in partially observable multiagent environments.",
                "They generalize POMDPs [13] to multiagent settings by including the other agents computable models in the state space along with the states of the physical environment.",
                "The models encompass all information influencing the agents behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11].",
                "I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a decision-makers perspective in the interaction.",
                "In [15], Polich and Gmytrasiewicz introduced interactive dynamic influence diagrams (I-DIDs) as the computational representations of I-POMDPs.",
                "I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs contribute to a growing line of work [19] that includes multi-agent influence diagrams (MAIDs) [14], and more recently, networks of influence diagrams (NIDs) [8].",
                "These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agents actions and chance nodes capturing the agents private information.",
                "MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the independence structure.",
                "NIDs extend MAIDs to include agents uncertainty over the game being played and over models of the other agents.",
                "Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent.",
                "Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently.",
                "However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games.",
                "Matters are more complex when we consider interactions that are extended over time, where predictions about others future actions must be made using models that change as the agents act and observe.",
                "I-DIDs address this gap by allowing the representation of other agents models as the values of a special model node.",
                "Both, other agents models and the original agents beliefs over these models are updated over time using special-purpose implementations.",
                "In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID.",
                "Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently.",
                "Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I-DID by [15], and show that it could be replaced by traditional dependency links.",
                "In the previous representation of the I-DID, the update of the agents belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time.",
                "We explicate the semantics of this link by showing how it can be implemented using the traditional dependency links between the chance nodes that constitute the model nodes.",
                "The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs.",
                "We show how IDIDs may be used to model an agents uncertainty over others models, that may themselves be I-DIDs.",
                "Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others models.",
                "Analogous to DIDs, I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. 2.",
                "BACKGROUND: FINITELY NESTED IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents models as part of the state space [9].",
                "Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents models and their beliefs about others.",
                "For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j.",
                "A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment.",
                "Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj .",
                "Here, j is Bayes rational and OCj is js optimality criterion.",
                "SMj is the set of subintentional models of j.",
                "Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent.",
                "We give a recursive bottom-up construction of the interactive state space below.",
                "ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . .",
                "ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent is preferences over its interactive states.",
                "Usually only the physical states will matter.",
                "Agent is policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i.",
                "Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai). 2.1 Belief Update Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes.",
                "However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones.",
                "First, since the state of the physical environment depends on the actions of both agents, is prediction of how the physical state changes has to be made based on its prediction of js actions.",
                "Second, changes in js models have to be included in is belief update.",
                "Specifically, if j is intentional then an update of js beliefs due to its action and observation has to be included.",
                "In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief.",
                "If js model is subintentional, then js probable observations are appended to the observation history contained in the model.",
                "Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update.",
                "For a version of the belief update when js model is subintentional, see [9].",
                "If agent j is also modeled as an I-POMDP, then is belief update invokes js belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke is belief update and so on.",
                "This recursion in belief nesting bottoms out at the 0th level.",
                "At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9]. 2.2 Value Iteration Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)).",
                "Eq. 2 is a basis for value iteration in I-POMDPs.",
                "Agent is optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3.",
                "INTERACTIVEINFLUENCEDIAGRAMS A naive extension of influence diagrams (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes.",
                "However, this approach assumes that the agents actions are controlled using a probability distribution that does not change over time.",
                "Interactive influence diagrams (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node.",
                "We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon.",
                "We note that the probability distribution over the chance node, S, and the model node together represents agent is belief over its interactive states.",
                "In addition to the model 1 The 0th level model is a POMDP: Other agents actions are treated as exogenous events and folded into the T, O, and R functions.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j.",
                "The hexagon is the model node (Mj,l−1) whose structure we show in (b).",
                "Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ).",
                "Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the policy link in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model.",
                "In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs.",
                "The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2.",
                "Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional.",
                "Because the model node contains the alternative models of the other agent as its values, its representation is not trivial.",
                "In particular, some of the models within the node are I-IDs that when solved generate the agents optimal policy in their decision nodes.",
                "Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise.",
                "Borrowing insights from previous work [8], we observe that the model node and the dashed policy link that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b).",
                "The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj.",
                "Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent is beliefs.",
                "The conditional probability table of the chance node, Aj, is a <br>multiplexer</br> that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj].",
                "The values of Mod[Mj] denote the different models of j.",
                "In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1.",
                "The distribution over the node, Mod[Mj], is the agent is belief over the models of j given a physical state.",
                "For more agents, we will have as many model nodes as there are agents.",
                "Notice that Fig. 1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional dependency links.",
                "In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them.",
                "In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes.",
                "This allows I-IDs to be represented and implemented using conventional application tools that target IDs.",
                "Note that we may view the level l I-ID as a NID.",
                "Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2).",
                "If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID.",
                "Note that within the I-IDs (or IDs) at each level, there is only a single decision node.",
                "Thus, our NID does not contain any MAIDs.",
                "Figure 2: A level l I-ID represented as a NID.",
                "The probabilities assigned to the blocks of the NID are is beliefs over js models conditioned on a physical state. 3.2 Solution The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively.",
                "We start by solving the level 0 models, which, if intentional, are traditional IDs.",
                "Their solutions provide probability distributions over the other agents actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID.",
                "The mapping from the level 0 models decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c).",
                "During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj].",
                "As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent is belief over the models of j conditioned on the physical state.",
                "The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j.",
                "Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18].",
                "This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief.",
                "Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agents current belief is known. 4.",
                "INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic influence diagrams (I-DIDs) extend I-IDs (and NIDs) to allow sequential decision-making over several time steps.",
                "Just as DIDs are structured graphical representations of POMDPs, I-DIDs are the graphical online analogs for finitely nested I-POMDPs.",
                "I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents. 4.1 Syntax We depict a general two time-slice I-DID in Fig. 3(a).",
                "In addition to the model nodes and the dashed policy link, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a).",
                "We explained the semantics of the model node and the policy link in the previous section; we describe the model updates next.",
                "The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1.",
                "Recall from Section 2 that an agents intentional model includes its belief.",
                "Because the agents act and receive observations, their models are updated to reflect their changed beliefs.",
                "Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models.",
                "Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models.",
                "Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model.",
                "These steps are a part of agent is belief update formalized using Eq. 1.",
                "In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID.",
                "If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ).",
                "These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation.",
                "The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously.",
                "Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed.",
                "The probability that js updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node.",
                "In order to obtain the probability of js possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ].",
                "Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a <br>multiplexer</br> modulated by Mod[Mt j ].",
                "Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1.",
                "Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1.",
                "Notice that the model update link may be replaced by the dependency links between the chance nodes that constitute the model nodes in the two time slices.",
                "In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them.",
                "Chance nodes and dependency links that not in bold are standard, usually found in DIDs.",
                "Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents js observation at time t + 1.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links.",
                "We note that the possible set of models of the other agent j grows exponentially with the number of time steps.",
                "For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node. 4.2 Solution Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively.",
                "For the purpose of illustration, let l=1 and T=2.",
                "The solution method uses the standard look-ahead technique, projecting the agents action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step.",
                "Because agent i has a belief over js models as well, the lookahead includes finding out the possible models that j could have in the future.",
                "Consequently, each of js subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions.",
                "These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b).",
                "We note the recursive nature of this solution: in solving agent is level 1 I-DID, js level 0 DIDs must be solved.",
                "If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner.",
                "We briefly outline the recursive algorithm for solving agent is Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase 1.",
                "For t from 1 to T − 1 do 2.",
                "If l ≥ 1 then Populate Mt+1 j,l−1 3.",
                "For each mt j in Range(Mt j,l−1) do 4.",
                "Recursively call algorithm with the l − 1 I-ID (or ID) that represents mt j and the horizon, T − t + 1 5.",
                "Map the decision node of the solved I-ID (or ID), OPT(mt j), to a chance node Aj 6.",
                "For each aj in OPT(mt j) do 7.",
                "For each oj in Oj (part of mt j) do 8.",
                "Update js belief, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← New I-ID (or ID) with bt+1 j as the initial belief 10.",
                "Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11.",
                "Add the model node, Mt+1 j,l−1, and the dependency links between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b)) 12.",
                "Add the chance, decision, and utility nodes for t + 1 time slice and the dependency links between them 13.",
                "Establish the CPTs for each chance node and utility node Look-Ahead Phase 14.",
                "Apply the standard look-ahead and backup method to solve the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5.",
                "We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the dependency links and the conditional probability tables for each node.",
                "We particularly focus on establishing and populating the model nodes (lines 3-11).",
                "Note that Range(·) returns the values (lower level models) of the random variable given as input (model node).",
                "In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs.",
                "Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents.",
                "As we mentioned previously, the 0-th level models are the traditional DIDs.",
                "Their solutions provide probability distributions over actions of the agent modeled at that level to I-DIDs at level 1.",
                "Given probability distributions over other agents actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models.",
                "Assume that the number of models considered at each level is bound by a number, M. Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs. 5.",
                "EXAMPLE APPLICATIONS To illustrate the usefulness of I-DIDs, we apply them to three problem domains.",
                "We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it. 5.1 Followership-Leadership in the Multiagent Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9].",
                "The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L).",
                "In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agents opening one of the doors.",
                "When any door is opened, the tiger persists in its original location with a probability of 95%.",
                "Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%.",
                "Agent j, on the other hand, hears growls with a reliability of 95%.",
                "Thus, the setting is such that agent i hears agent j opening doors more reliably than the tigers growls.",
                "This suggests that i could use js actions as an indication of the location of the tiger, as we discuss below.",
                "Each agents preferences are as in the single agent game discussed in [13].",
                "The transition, observation, and reward functions are shown in [16].",
                "A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions.",
                "In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship.",
                "In particular, we analyze the situational and epistemological conditions sufficient for their emergence.",
                "The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the others actions in order to maximize its payoffs.",
                "Let us consider a particular setting of the tiger problem in which agent i believes that js preferences are aligned with its own - both of them just want to get the gold - and js hearing is more reliable in comparison to itself.",
                "As an example, suppose that j, on listening can discern the tigers location 95% of the times compared to is 65% accuracy.",
                "Additionally, agent i does not have any initial information about the tigers location.",
                "In other words, is single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger.",
                "In addition, i considers two models of j, which differ in js flat level 0 initial beliefs.",
                "This is represented in the level 1 I-ID shown in Fig. 6(a).",
                "According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)).",
                "Agent i is undecided on these two models of j.",
                "If we vary is hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior.",
                "If is probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow js actions: i opens the same door that j opened previously iff is own assessment of the tigers location confirms js pick.",
                "If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)).",
                "Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem.",
                "Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations.",
                "We observed that a single level of belief nesting - beliefs about the others models - was sufficient for followership to emerge in the tiger problem.",
                "However, the epistemological requirements for the emergence of leadership are more complex.",
                "For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i.",
                "As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow js actions over time.",
                "Agent j emerges as a leader if it believes that i will follow it, which implies that js belief must be nested two levels deep to enable it to recognize its leadership role.",
                "Realizing that i will follow presents j with an opportunity to influence is actions in the benefit of the collective good or its self-interest alone.",
                "For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original.",
                "If j alone selects the correct door, it gets the payoff of 10.",
                "On the other hand, if both agents pick the wrong door, their penalties are cut in half.",
                "In this setting, it is in both js best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader.",
                "However, consider a slightly different problem in which j gains from is loss and is penalized if i gains.",
                "Specifically, let is payoff be subtracted from js, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then is loss of 100 becomes js gain.",
                "Agent j believes that i incorrectly thinks that js preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is.",
                "Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following js actions.",
                "We show is normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a).",
                "The policy demonstrates that i will blindly follow js actions.",
                "Since the tiger persists in its original location with a probability of 0.95, i will select the same door again.",
                "If j begins the game with a 99% probability that the tiger is on the right, solving js I-DID nested two levels deep, results in the policy shown in Fig. 8(b).",
                "Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL.",
                "Agent js intention is to deceive i who, it believes, will follow js actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest.",
                "Figure 8: Emergence of deception between agents in the tiger problem.",
                "Behaviors of interest are in bold. * denotes as before. (a) Agent is policy demonstrating that it will blindly follow js actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i. 5.2 Altruism and Reciprocity in the Public Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves.",
                "Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot.",
                "However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes.",
                "Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others contributions.",
                "However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions.",
                "The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4].",
                "Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting.",
                "These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others.",
                "For simplicity, we assume that the game is played between M = 2 agents, i and j.",
                "Let each agent be initially endowed with XT amount of resources.",
                "While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions.",
                "Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute.",
                "The latter action is deThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D).",
                "We assume that the actions are not observable to others.",
                "The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return.",
                "We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain.",
                "Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment.",
                "In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment.",
                "Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent.",
                "For simplicity, we assume that the cost of punishing is same for both the agents.",
                "The one-shot PG game with punishment is shown in Table. 1.",
                "Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action.",
                "If P < XT − ciXT , then defection is the dominating action for both.",
                "If P = XT − ciXT , then the game is not dominance-solvable.",
                "Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a).",
                "We formulate a sequential version of the PG problem with punishment from the perspective of agent i.",
                "Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents.",
                "Each agent may contribute a fixed amount, xc, or defect.",
                "An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot.",
                "Notice that the observations are also indirectly indicative of agent js actions because the state of the public pot is influenced by them.",
                "The amount of resources in agent is private pot, is perfectly observable to i.",
                "The payoffs are analogous to Table. 1.",
                "Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)).",
                "Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect.",
                "Let xc = 1 and the level 0 agent be punished half the times it defects.",
                "With one action remaining, both types of agents choose to contribute to avoid being punished.",
                "With two actions to go, the altruistic type chooses to contribute, while the other defects.",
                "This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids.",
                "Because cj for the non-altruistic type is less, it prefers not to contribute.",
                "With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects.",
                "For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection.",
                "We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level 0 models, mentioned previously, to j (see Fig. 9).",
                "If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps.",
                "This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type.",
                "However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1.",
                "These results demonstrate that the behavior of our altruistic type resembles that found experimentally.",
                "The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic.",
                "We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection.",
                "The reciprocal types marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other.",
                "We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full.",
                "For this prior belief, i chooses to defect.",
                "On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)).",
                "This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from js action to contribute.",
                "Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step.",
                "Agent i therefore chooses to contribute to reciprocate js action.",
                "An analogous reasoning leads i to defect when it observes a meager pot.",
                "With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations.",
                "Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic). 5.3 Strategies in Two-Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2].",
                "Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck.",
                "While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands.",
                "To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit.",
                "Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot.",
                "During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to 8.",
                "Notice that, for example, if a lower numbered card is discarded, the probability of receiving a low card in exchange is now reduced.",
                "We show the level 1 I-ID for the simplified two-player Poker in Fig. 11.",
                "We considered two models (personality types) of agent j.",
                "The conservative type believes that it is likely that its opponent has a high numbered card in its hand.",
                "On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card.",
                "Thus, the two types differ in their beliefs over their opponents hand.",
                "In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution.",
                "With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand.",
                "This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange.",
                "The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one.",
                "Figure 11: (a) Level 1 I-ID of agent i.",
                "The observation reveals information about js hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a).",
                "The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in js hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. is own hand contains the card numbered 8.",
                "The agent starts by keeping its card.",
                "On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card.",
                "If i observes that j discarded its card into the L or H pile, i believes that j is aggressive.",
                "On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange.",
                "Because the probability of receiving a low card is high now, i chooses to keep its card.",
                "On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card.",
                "In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff.",
                "This is partly due to the fact that an observation of, say, L resets the agent is previous time step beliefs over js hand to the low numbered cards only. 6.",
                "DISCUSSION We showed how DIDs may be extended to I-DIDs that enable online sequential decision-making in uncertain multiagent settings.",
                "Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent is three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs.",
                "I-DIDs extend NIDs to allow sequential decision-making over multiple time steps in the presence of other interacting agents.",
                "I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs.",
                "We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality.",
                "Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work.",
                "The first author would like to acknowledge the support of a UGARF grant. 7.",
                "REFERENCES [1] R. J. Aumann.",
                "Interactive epistemology i: Knowledge.",
                "International Journal of Game Theory, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron.",
                "The challenge of poker.",
                "AIJ, 2001. [3] A. Brandenburger and E. Dekel.",
                "Hierarchies of beliefs and common knowledge.",
                "Journal of Economic Theory, 59:189-198, 1993. [4] C. Camerer.",
                "Behavioral Game Theory: Experiments in Strategic Interaction.",
                "Princeton University Press, 2003. [5] E. Fehr and S. Gachter.",
                "Cooperation and punishment in public goods experiments.",
                "American Economic Review, 90(4):980-994, 2000. [6] D. Fudenberg and D. K. Levine.",
                "The Theory of Learning in Games.",
                "MIT Press, 1998. [7] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [8] Y. Gal and A. Pfeffer.",
                "A language for modeling agents decision-making processes in games.",
                "In AAMAS, 2003. [9] P. Gmytrasiewicz and P. Doshi.",
                "A framework for sequential planning in multiagent settings.",
                "JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz and E. Durfee.",
                "Rational coordination in multi-agent environments.",
                "JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi.",
                "Games with incomplete information played by bayesian players.",
                "Management Science, 14(3):159-182, 1967. [12] R. A. Howard and J. E. Matheson.",
                "Influence diagrams.",
                "In R. A. Howard and J. E. Matheson, editors, The Principles and Applications of Decision Analysis.",
                "Strategic Decisions Group, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman, and A. Cassandra.",
                "Planning and acting in partially observable stochastic domains.",
                "Artificial Intelligence Journal, 2, 1998. [14] D. Koller and B. Milch.",
                "Multi-agent influence diagrams for representing and solving games.",
                "In IJCAI, pages 1027-1034, 2001. [15] K. Polich and P. Gmytrasiewicz.",
                "Interactive dynamic influence diagrams.",
                "In GTDT Workshop, AAMAS, 2006. [16] B.",
                "Rathnas., P. Doshi, and P. J. Gmytrasiewicz.",
                "Exact solutions to interactive pomdps using behavioral equivalence.",
                "In Autonomous Agents and Multi-Agent Systems Conference (AAMAS), 2006. [17] S. Russell and P. Norvig.",
                "Artificial Intelligence: A Modern Approach (Second Edition).",
                "Prentice Hall, 2003. [18] R. D. Shachter.",
                "Evaluating influence diagrams.",
                "Operations Research, 34(6):871-882, 1986. [19] D. Suryadi and P. Gmytrasiewicz.",
                "Learning models of other agents using influence diagrams.",
                "In UM, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "La tabla de probabilidad condicional del nodo casual, AJ, es un \"multiplexor\" que asume la distribución de cada uno de los nodos de acción (A1 J, A2 J) dependiendo del valor de Mod [MJ].",
                "Debido a que la probabilidad de observaciones JS depende del estado físico y de las acciones conjuntas de ambos agentes, el nodo OJ está vinculado con ST+1, en J, y en i.2 Análogo a AT J, la tabla de probabilidad condicional de OJ también es un \"multiplexor\" modulado por MOD [MT J]."
            ],
            "translated_text": "",
            "candidates": [
                "multiplexor",
                "multiplexor",
                "multiplexor",
                "multiplexor"
            ],
            "error": []
        },
        "policy link": {
            "translated_key": "enlace de política",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Graphical Models for Online Solutions to Interactive POMDPs Prashant Doshi Dept.",
                "of Computer Science University of Georgia Athens, GA 30602, USA pdoshi@cs.uga.edu Yifeng Zeng Dept. of Computer Science Aalborg University DK-9220 Aalborg, Denmark yfzeng@cs.aau.edu Qiongyu Chen Dept. of Computer Science National Univ. of Singapore 117543, Singapore chenqy@comp.nus.edu.sg ABSTRACT We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation.",
                "These graphical models called interactive dynamic influence diagrams (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents.",
                "Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Theory 1.",
                "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in partially observable multiagent environments.",
                "They generalize POMDPs [13] to multiagent settings by including the other agents computable models in the state space along with the states of the physical environment.",
                "The models encompass all information influencing the agents behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11].",
                "I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a decision-makers perspective in the interaction.",
                "In [15], Polich and Gmytrasiewicz introduced interactive dynamic influence diagrams (I-DIDs) as the computational representations of I-POMDPs.",
                "I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs contribute to a growing line of work [19] that includes multi-agent influence diagrams (MAIDs) [14], and more recently, networks of influence diagrams (NIDs) [8].",
                "These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agents actions and chance nodes capturing the agents private information.",
                "MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the independence structure.",
                "NIDs extend MAIDs to include agents uncertainty over the game being played and over models of the other agents.",
                "Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent.",
                "Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently.",
                "However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games.",
                "Matters are more complex when we consider interactions that are extended over time, where predictions about others future actions must be made using models that change as the agents act and observe.",
                "I-DIDs address this gap by allowing the representation of other agents models as the values of a special model node.",
                "Both, other agents models and the original agents beliefs over these models are updated over time using special-purpose implementations.",
                "In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID.",
                "Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently.",
                "Furthermore, we clarify the semantics of the special purpose <br>policy link</br> introduced in the representation of I-DID by [15], and show that it could be replaced by traditional dependency links.",
                "In the previous representation of the I-DID, the update of the agents belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time.",
                "We explicate the semantics of this link by showing how it can be implemented using the traditional dependency links between the chance nodes that constitute the model nodes.",
                "The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs.",
                "We show how IDIDs may be used to model an agents uncertainty over others models, that may themselves be I-DIDs.",
                "Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others models.",
                "Analogous to DIDs, I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. 2.",
                "BACKGROUND: FINITELY NESTED IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents models as part of the state space [9].",
                "Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents models and their beliefs about others.",
                "For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j.",
                "A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment.",
                "Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj .",
                "Here, j is Bayes rational and OCj is js optimality criterion.",
                "SMj is the set of subintentional models of j.",
                "Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent.",
                "We give a recursive bottom-up construction of the interactive state space below.",
                "ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . .",
                "ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent is preferences over its interactive states.",
                "Usually only the physical states will matter.",
                "Agent is policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i.",
                "Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai). 2.1 Belief Update Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes.",
                "However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones.",
                "First, since the state of the physical environment depends on the actions of both agents, is prediction of how the physical state changes has to be made based on its prediction of js actions.",
                "Second, changes in js models have to be included in is belief update.",
                "Specifically, if j is intentional then an update of js beliefs due to its action and observation has to be included.",
                "In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief.",
                "If js model is subintentional, then js probable observations are appended to the observation history contained in the model.",
                "Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update.",
                "For a version of the belief update when js model is subintentional, see [9].",
                "If agent j is also modeled as an I-POMDP, then is belief update invokes js belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke is belief update and so on.",
                "This recursion in belief nesting bottoms out at the 0th level.",
                "At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9]. 2.2 Value Iteration Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)).",
                "Eq. 2 is a basis for value iteration in I-POMDPs.",
                "Agent is optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3.",
                "INTERACTIVEINFLUENCEDIAGRAMS A naive extension of influence diagrams (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes.",
                "However, this approach assumes that the agents actions are controlled using a probability distribution that does not change over time.",
                "Interactive influence diagrams (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node.",
                "We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon.",
                "We note that the probability distribution over the chance node, S, and the model node together represents agent is belief over its interactive states.",
                "In addition to the model 1 The 0th level model is a POMDP: Other agents actions are treated as exogenous events and folded into the T, O, and R functions.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j.",
                "The hexagon is the model node (Mj,l−1) whose structure we show in (b).",
                "Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ).",
                "Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the <br>policy link</br> in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model.",
                "In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs.",
                "The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2.",
                "Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional.",
                "Because the model node contains the alternative models of the other agent as its values, its representation is not trivial.",
                "In particular, some of the models within the node are I-IDs that when solved generate the agents optimal policy in their decision nodes.",
                "Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise.",
                "Borrowing insights from previous work [8], we observe that the model node and the dashed <br>policy link</br> that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b).",
                "The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj.",
                "Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent is beliefs.",
                "The conditional probability table of the chance node, Aj, is a multiplexer that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj].",
                "The values of Mod[Mj] denote the different models of j.",
                "In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1.",
                "The distribution over the node, Mod[Mj], is the agent is belief over the models of j given a physical state.",
                "For more agents, we will have as many model nodes as there are agents.",
                "Notice that Fig. 1(b) clarifies the semantics of the <br>policy link</br>, and shows how it can be represented using the traditional dependency links.",
                "In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them.",
                "In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes.",
                "This allows I-IDs to be represented and implemented using conventional application tools that target IDs.",
                "Note that we may view the level l I-ID as a NID.",
                "Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2).",
                "If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID.",
                "Note that within the I-IDs (or IDs) at each level, there is only a single decision node.",
                "Thus, our NID does not contain any MAIDs.",
                "Figure 2: A level l I-ID represented as a NID.",
                "The probabilities assigned to the blocks of the NID are is beliefs over js models conditioned on a physical state. 3.2 Solution The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively.",
                "We start by solving the level 0 models, which, if intentional, are traditional IDs.",
                "Their solutions provide probability distributions over the other agents actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID.",
                "The mapping from the level 0 models decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c).",
                "During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj].",
                "As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent is belief over the models of j conditioned on the physical state.",
                "The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j.",
                "Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18].",
                "This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief.",
                "Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agents current belief is known. 4.",
                "INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic influence diagrams (I-DIDs) extend I-IDs (and NIDs) to allow sequential decision-making over several time steps.",
                "Just as DIDs are structured graphical representations of POMDPs, I-DIDs are the graphical online analogs for finitely nested I-POMDPs.",
                "I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents. 4.1 Syntax We depict a general two time-slice I-DID in Fig. 3(a).",
                "In addition to the model nodes and the dashed <br>policy link</br>, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a).",
                "We explained the semantics of the model node and the <br>policy link</br> in the previous section; we describe the model updates next.",
                "The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1.",
                "Recall from Section 2 that an agents intentional model includes its belief.",
                "Because the agents act and receive observations, their models are updated to reflect their changed beliefs.",
                "Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models.",
                "Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models.",
                "Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model.",
                "These steps are a part of agent is belief update formalized using Eq. 1.",
                "In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID.",
                "If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ).",
                "These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation.",
                "The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously.",
                "Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed.",
                "The probability that js updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node.",
                "In order to obtain the probability of js possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ].",
                "Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a multiplexer modulated by Mod[Mt j ].",
                "Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1.",
                "Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1.",
                "Notice that the model update link may be replaced by the dependency links between the chance nodes that constitute the model nodes in the two time slices.",
                "In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them.",
                "Chance nodes and dependency links that not in bold are standard, usually found in DIDs.",
                "Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents js observation at time t + 1.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links.",
                "We note that the possible set of models of the other agent j grows exponentially with the number of time steps.",
                "For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node. 4.2 Solution Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively.",
                "For the purpose of illustration, let l=1 and T=2.",
                "The solution method uses the standard look-ahead technique, projecting the agents action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step.",
                "Because agent i has a belief over js models as well, the lookahead includes finding out the possible models that j could have in the future.",
                "Consequently, each of js subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions.",
                "These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b).",
                "We note the recursive nature of this solution: in solving agent is level 1 I-DID, js level 0 DIDs must be solved.",
                "If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner.",
                "We briefly outline the recursive algorithm for solving agent is Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase 1.",
                "For t from 1 to T − 1 do 2.",
                "If l ≥ 1 then Populate Mt+1 j,l−1 3.",
                "For each mt j in Range(Mt j,l−1) do 4.",
                "Recursively call algorithm with the l − 1 I-ID (or ID) that represents mt j and the horizon, T − t + 1 5.",
                "Map the decision node of the solved I-ID (or ID), OPT(mt j), to a chance node Aj 6.",
                "For each aj in OPT(mt j) do 7.",
                "For each oj in Oj (part of mt j) do 8.",
                "Update js belief, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← New I-ID (or ID) with bt+1 j as the initial belief 10.",
                "Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11.",
                "Add the model node, Mt+1 j,l−1, and the dependency links between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b)) 12.",
                "Add the chance, decision, and utility nodes for t + 1 time slice and the dependency links between them 13.",
                "Establish the CPTs for each chance node and utility node Look-Ahead Phase 14.",
                "Apply the standard look-ahead and backup method to solve the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5.",
                "We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the dependency links and the conditional probability tables for each node.",
                "We particularly focus on establishing and populating the model nodes (lines 3-11).",
                "Note that Range(·) returns the values (lower level models) of the random variable given as input (model node).",
                "In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs.",
                "Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents.",
                "As we mentioned previously, the 0-th level models are the traditional DIDs.",
                "Their solutions provide probability distributions over actions of the agent modeled at that level to I-DIDs at level 1.",
                "Given probability distributions over other agents actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models.",
                "Assume that the number of models considered at each level is bound by a number, M. Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs. 5.",
                "EXAMPLE APPLICATIONS To illustrate the usefulness of I-DIDs, we apply them to three problem domains.",
                "We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it. 5.1 Followership-Leadership in the Multiagent Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9].",
                "The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L).",
                "In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agents opening one of the doors.",
                "When any door is opened, the tiger persists in its original location with a probability of 95%.",
                "Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%.",
                "Agent j, on the other hand, hears growls with a reliability of 95%.",
                "Thus, the setting is such that agent i hears agent j opening doors more reliably than the tigers growls.",
                "This suggests that i could use js actions as an indication of the location of the tiger, as we discuss below.",
                "Each agents preferences are as in the single agent game discussed in [13].",
                "The transition, observation, and reward functions are shown in [16].",
                "A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions.",
                "In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship.",
                "In particular, we analyze the situational and epistemological conditions sufficient for their emergence.",
                "The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the others actions in order to maximize its payoffs.",
                "Let us consider a particular setting of the tiger problem in which agent i believes that js preferences are aligned with its own - both of them just want to get the gold - and js hearing is more reliable in comparison to itself.",
                "As an example, suppose that j, on listening can discern the tigers location 95% of the times compared to is 65% accuracy.",
                "Additionally, agent i does not have any initial information about the tigers location.",
                "In other words, is single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger.",
                "In addition, i considers two models of j, which differ in js flat level 0 initial beliefs.",
                "This is represented in the level 1 I-ID shown in Fig. 6(a).",
                "According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)).",
                "Agent i is undecided on these two models of j.",
                "If we vary is hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior.",
                "If is probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow js actions: i opens the same door that j opened previously iff is own assessment of the tigers location confirms js pick.",
                "If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)).",
                "Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem.",
                "Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations.",
                "We observed that a single level of belief nesting - beliefs about the others models - was sufficient for followership to emerge in the tiger problem.",
                "However, the epistemological requirements for the emergence of leadership are more complex.",
                "For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i.",
                "As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow js actions over time.",
                "Agent j emerges as a leader if it believes that i will follow it, which implies that js belief must be nested two levels deep to enable it to recognize its leadership role.",
                "Realizing that i will follow presents j with an opportunity to influence is actions in the benefit of the collective good or its self-interest alone.",
                "For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original.",
                "If j alone selects the correct door, it gets the payoff of 10.",
                "On the other hand, if both agents pick the wrong door, their penalties are cut in half.",
                "In this setting, it is in both js best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader.",
                "However, consider a slightly different problem in which j gains from is loss and is penalized if i gains.",
                "Specifically, let is payoff be subtracted from js, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then is loss of 100 becomes js gain.",
                "Agent j believes that i incorrectly thinks that js preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is.",
                "Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following js actions.",
                "We show is normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a).",
                "The policy demonstrates that i will blindly follow js actions.",
                "Since the tiger persists in its original location with a probability of 0.95, i will select the same door again.",
                "If j begins the game with a 99% probability that the tiger is on the right, solving js I-DID nested two levels deep, results in the policy shown in Fig. 8(b).",
                "Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL.",
                "Agent js intention is to deceive i who, it believes, will follow js actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest.",
                "Figure 8: Emergence of deception between agents in the tiger problem.",
                "Behaviors of interest are in bold. * denotes as before. (a) Agent is policy demonstrating that it will blindly follow js actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i. 5.2 Altruism and Reciprocity in the Public Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves.",
                "Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot.",
                "However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes.",
                "Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others contributions.",
                "However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions.",
                "The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4].",
                "Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting.",
                "These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others.",
                "For simplicity, we assume that the game is played between M = 2 agents, i and j.",
                "Let each agent be initially endowed with XT amount of resources.",
                "While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions.",
                "Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute.",
                "The latter action is deThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D).",
                "We assume that the actions are not observable to others.",
                "The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return.",
                "We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain.",
                "Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment.",
                "In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment.",
                "Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent.",
                "For simplicity, we assume that the cost of punishing is same for both the agents.",
                "The one-shot PG game with punishment is shown in Table. 1.",
                "Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action.",
                "If P < XT − ciXT , then defection is the dominating action for both.",
                "If P = XT − ciXT , then the game is not dominance-solvable.",
                "Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a).",
                "We formulate a sequential version of the PG problem with punishment from the perspective of agent i.",
                "Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents.",
                "Each agent may contribute a fixed amount, xc, or defect.",
                "An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot.",
                "Notice that the observations are also indirectly indicative of agent js actions because the state of the public pot is influenced by them.",
                "The amount of resources in agent is private pot, is perfectly observable to i.",
                "The payoffs are analogous to Table. 1.",
                "Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)).",
                "Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect.",
                "Let xc = 1 and the level 0 agent be punished half the times it defects.",
                "With one action remaining, both types of agents choose to contribute to avoid being punished.",
                "With two actions to go, the altruistic type chooses to contribute, while the other defects.",
                "This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids.",
                "Because cj for the non-altruistic type is less, it prefers not to contribute.",
                "With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects.",
                "For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection.",
                "We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level 0 models, mentioned previously, to j (see Fig. 9).",
                "If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps.",
                "This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type.",
                "However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1.",
                "These results demonstrate that the behavior of our altruistic type resembles that found experimentally.",
                "The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic.",
                "We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection.",
                "The reciprocal types marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other.",
                "We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full.",
                "For this prior belief, i chooses to defect.",
                "On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)).",
                "This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from js action to contribute.",
                "Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step.",
                "Agent i therefore chooses to contribute to reciprocate js action.",
                "An analogous reasoning leads i to defect when it observes a meager pot.",
                "With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations.",
                "Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic). 5.3 Strategies in Two-Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2].",
                "Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck.",
                "While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands.",
                "To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit.",
                "Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot.",
                "During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to 8.",
                "Notice that, for example, if a lower numbered card is discarded, the probability of receiving a low card in exchange is now reduced.",
                "We show the level 1 I-ID for the simplified two-player Poker in Fig. 11.",
                "We considered two models (personality types) of agent j.",
                "The conservative type believes that it is likely that its opponent has a high numbered card in its hand.",
                "On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card.",
                "Thus, the two types differ in their beliefs over their opponents hand.",
                "In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution.",
                "With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand.",
                "This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange.",
                "The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one.",
                "Figure 11: (a) Level 1 I-ID of agent i.",
                "The observation reveals information about js hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a).",
                "The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in js hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. is own hand contains the card numbered 8.",
                "The agent starts by keeping its card.",
                "On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card.",
                "If i observes that j discarded its card into the L or H pile, i believes that j is aggressive.",
                "On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange.",
                "Because the probability of receiving a low card is high now, i chooses to keep its card.",
                "On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card.",
                "In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff.",
                "This is partly due to the fact that an observation of, say, L resets the agent is previous time step beliefs over js hand to the low numbered cards only. 6.",
                "DISCUSSION We showed how DIDs may be extended to I-DIDs that enable online sequential decision-making in uncertain multiagent settings.",
                "Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent is three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs.",
                "I-DIDs extend NIDs to allow sequential decision-making over multiple time steps in the presence of other interacting agents.",
                "I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs.",
                "We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality.",
                "Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work.",
                "The first author would like to acknowledge the support of a UGARF grant. 7.",
                "REFERENCES [1] R. J. Aumann.",
                "Interactive epistemology i: Knowledge.",
                "International Journal of Game Theory, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron.",
                "The challenge of poker.",
                "AIJ, 2001. [3] A. Brandenburger and E. Dekel.",
                "Hierarchies of beliefs and common knowledge.",
                "Journal of Economic Theory, 59:189-198, 1993. [4] C. Camerer.",
                "Behavioral Game Theory: Experiments in Strategic Interaction.",
                "Princeton University Press, 2003. [5] E. Fehr and S. Gachter.",
                "Cooperation and punishment in public goods experiments.",
                "American Economic Review, 90(4):980-994, 2000. [6] D. Fudenberg and D. K. Levine.",
                "The Theory of Learning in Games.",
                "MIT Press, 1998. [7] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [8] Y. Gal and A. Pfeffer.",
                "A language for modeling agents decision-making processes in games.",
                "In AAMAS, 2003. [9] P. Gmytrasiewicz and P. Doshi.",
                "A framework for sequential planning in multiagent settings.",
                "JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz and E. Durfee.",
                "Rational coordination in multi-agent environments.",
                "JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi.",
                "Games with incomplete information played by bayesian players.",
                "Management Science, 14(3):159-182, 1967. [12] R. A. Howard and J. E. Matheson.",
                "Influence diagrams.",
                "In R. A. Howard and J. E. Matheson, editors, The Principles and Applications of Decision Analysis.",
                "Strategic Decisions Group, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman, and A. Cassandra.",
                "Planning and acting in partially observable stochastic domains.",
                "Artificial Intelligence Journal, 2, 1998. [14] D. Koller and B. Milch.",
                "Multi-agent influence diagrams for representing and solving games.",
                "In IJCAI, pages 1027-1034, 2001. [15] K. Polich and P. Gmytrasiewicz.",
                "Interactive dynamic influence diagrams.",
                "In GTDT Workshop, AAMAS, 2006. [16] B.",
                "Rathnas., P. Doshi, and P. J. Gmytrasiewicz.",
                "Exact solutions to interactive pomdps using behavioral equivalence.",
                "In Autonomous Agents and Multi-Agent Systems Conference (AAMAS), 2006. [17] S. Russell and P. Norvig.",
                "Artificial Intelligence: A Modern Approach (Second Edition).",
                "Prentice Hall, 2003. [18] R. D. Shachter.",
                "Evaluating influence diagrams.",
                "Operations Research, 34(6):871-882, 1986. [19] D. Suryadi and P. Gmytrasiewicz.",
                "Learning models of other agents using influence diagrams.",
                "In UM, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Además, aclaramos la semántica del \"enlace de política\" de propósito especial introducido en la representación de I-Did por [15], y mostramos que podría ser reemplazado por enlaces de dependencia tradicionales.",
                "Dependiendo del valor del nodo, Mod [MJ], la distribución de cada uno de los nodos de posibilidades se asigna al nodo AJ.(c) El I-ID transformado con el nodo modelo reemplazado por los nodos casuales y las relaciones entre ellos.Nodo, I-IDS difieren de las ID al tener un enlace discontinuo (llamado \"enlace de política\" en [15]) entre el nodo modelo y un nodo casual, AJ, que representa la distribución sobre las acciones de otros agentes dado su modelo.",
                "Pediendo prestados ideas de trabajos anteriores [8], observamos que el nodo modelo y el \"enlace de política\" discontinuo que lo conecta al nodo casual, AJ, podría representarse como se muestra en la Fig. 1 (b).",
                "Observe que la figura 1 (b) aclara la semántica del \"enlace de política\" y muestra cómo se puede representar utilizando los enlaces de dependencia tradicionales.",
                "Además de los nodos modelo y el \"enlace de política\" discontinuo, lo que diferencia a un I-DID de A DO es el enlace de actualización del modelo que se muestra como una flecha punteada en la Fig. 3 (a).",
                "Explicamos la semántica del nodo modelo y el \"enlace de política\" en la sección anterior;Describimos las actualizaciones del modelo a continuación."
            ],
            "translated_text": "",
            "candidates": [
                "enlace de política",
                "enlace de política",
                "enlace de política",
                "enlace de política",
                "enlace de política",
                "enlace de política",
                "enlace de política",
                "enlace de política",
                "enlace de política",
                "enlace de política",
                "enlace de política",
                "enlace de política"
            ],
            "error": []
        },
        "dependency link": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Graphical Models for Online Solutions to Interactive POMDPs Prashant Doshi Dept.",
                "of Computer Science University of Georgia Athens, GA 30602, USA pdoshi@cs.uga.edu Yifeng Zeng Dept. of Computer Science Aalborg University DK-9220 Aalborg, Denmark yfzeng@cs.aau.edu Qiongyu Chen Dept. of Computer Science National Univ. of Singapore 117543, Singapore chenqy@comp.nus.edu.sg ABSTRACT We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation.",
                "These graphical models called interactive dynamic influence diagrams (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents.",
                "Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Theory 1.",
                "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in partially observable multiagent environments.",
                "They generalize POMDPs [13] to multiagent settings by including the other agents computable models in the state space along with the states of the physical environment.",
                "The models encompass all information influencing the agents behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11].",
                "I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a decision-makers perspective in the interaction.",
                "In [15], Polich and Gmytrasiewicz introduced interactive dynamic influence diagrams (I-DIDs) as the computational representations of I-POMDPs.",
                "I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs contribute to a growing line of work [19] that includes multi-agent influence diagrams (MAIDs) [14], and more recently, networks of influence diagrams (NIDs) [8].",
                "These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agents actions and chance nodes capturing the agents private information.",
                "MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the independence structure.",
                "NIDs extend MAIDs to include agents uncertainty over the game being played and over models of the other agents.",
                "Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent.",
                "Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently.",
                "However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games.",
                "Matters are more complex when we consider interactions that are extended over time, where predictions about others future actions must be made using models that change as the agents act and observe.",
                "I-DIDs address this gap by allowing the representation of other agents models as the values of a special model node.",
                "Both, other agents models and the original agents beliefs over these models are updated over time using special-purpose implementations.",
                "In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID.",
                "Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently.",
                "Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I-DID by [15], and show that it could be replaced by traditional <br>dependency link</br>s.",
                "In the previous representation of the I-DID, the update of the agents belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time.",
                "We explicate the semantics of this link by showing how it can be implemented using the traditional <br>dependency link</br>s between the chance nodes that constitute the model nodes.",
                "The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs.",
                "We show how IDIDs may be used to model an agents uncertainty over others models, that may themselves be I-DIDs.",
                "Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others models.",
                "Analogous to DIDs, I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. 2.",
                "BACKGROUND: FINITELY NESTED IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents models as part of the state space [9].",
                "Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents models and their beliefs about others.",
                "For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j.",
                "A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment.",
                "Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj .",
                "Here, j is Bayes rational and OCj is js optimality criterion.",
                "SMj is the set of subintentional models of j.",
                "Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent.",
                "We give a recursive bottom-up construction of the interactive state space below.",
                "ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . .",
                "ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent is preferences over its interactive states.",
                "Usually only the physical states will matter.",
                "Agent is policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i.",
                "Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai). 2.1 Belief Update Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes.",
                "However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones.",
                "First, since the state of the physical environment depends on the actions of both agents, is prediction of how the physical state changes has to be made based on its prediction of js actions.",
                "Second, changes in js models have to be included in is belief update.",
                "Specifically, if j is intentional then an update of js beliefs due to its action and observation has to be included.",
                "In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief.",
                "If js model is subintentional, then js probable observations are appended to the observation history contained in the model.",
                "Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update.",
                "For a version of the belief update when js model is subintentional, see [9].",
                "If agent j is also modeled as an I-POMDP, then is belief update invokes js belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke is belief update and so on.",
                "This recursion in belief nesting bottoms out at the 0th level.",
                "At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9]. 2.2 Value Iteration Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)).",
                "Eq. 2 is a basis for value iteration in I-POMDPs.",
                "Agent is optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3.",
                "INTERACTIVEINFLUENCEDIAGRAMS A naive extension of influence diagrams (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes.",
                "However, this approach assumes that the agents actions are controlled using a probability distribution that does not change over time.",
                "Interactive influence diagrams (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node.",
                "We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon.",
                "We note that the probability distribution over the chance node, S, and the model node together represents agent is belief over its interactive states.",
                "In addition to the model 1 The 0th level model is a POMDP: Other agents actions are treated as exogenous events and folded into the T, O, and R functions.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j.",
                "The hexagon is the model node (Mj,l−1) whose structure we show in (b).",
                "Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ).",
                "Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the policy link in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model.",
                "In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs.",
                "The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2.",
                "Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional.",
                "Because the model node contains the alternative models of the other agent as its values, its representation is not trivial.",
                "In particular, some of the models within the node are I-IDs that when solved generate the agents optimal policy in their decision nodes.",
                "Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise.",
                "Borrowing insights from previous work [8], we observe that the model node and the dashed policy link that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b).",
                "The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj.",
                "Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent is beliefs.",
                "The conditional probability table of the chance node, Aj, is a multiplexer that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj].",
                "The values of Mod[Mj] denote the different models of j.",
                "In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1.",
                "The distribution over the node, Mod[Mj], is the agent is belief over the models of j given a physical state.",
                "For more agents, we will have as many model nodes as there are agents.",
                "Notice that Fig. 1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional <br>dependency link</br>s.",
                "In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them.",
                "In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes.",
                "This allows I-IDs to be represented and implemented using conventional application tools that target IDs.",
                "Note that we may view the level l I-ID as a NID.",
                "Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2).",
                "If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID.",
                "Note that within the I-IDs (or IDs) at each level, there is only a single decision node.",
                "Thus, our NID does not contain any MAIDs.",
                "Figure 2: A level l I-ID represented as a NID.",
                "The probabilities assigned to the blocks of the NID are is beliefs over js models conditioned on a physical state. 3.2 Solution The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively.",
                "We start by solving the level 0 models, which, if intentional, are traditional IDs.",
                "Their solutions provide probability distributions over the other agents actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID.",
                "The mapping from the level 0 models decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c).",
                "During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj].",
                "As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent is belief over the models of j conditioned on the physical state.",
                "The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j.",
                "Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18].",
                "This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief.",
                "Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agents current belief is known. 4.",
                "INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic influence diagrams (I-DIDs) extend I-IDs (and NIDs) to allow sequential decision-making over several time steps.",
                "Just as DIDs are structured graphical representations of POMDPs, I-DIDs are the graphical online analogs for finitely nested I-POMDPs.",
                "I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents. 4.1 Syntax We depict a general two time-slice I-DID in Fig. 3(a).",
                "In addition to the model nodes and the dashed policy link, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a).",
                "We explained the semantics of the model node and the policy link in the previous section; we describe the model updates next.",
                "The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1.",
                "Recall from Section 2 that an agents intentional model includes its belief.",
                "Because the agents act and receive observations, their models are updated to reflect their changed beliefs.",
                "Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models.",
                "Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models.",
                "Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model.",
                "These steps are a part of agent is belief update formalized using Eq. 1.",
                "In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID.",
                "If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ).",
                "These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation.",
                "The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously.",
                "Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed.",
                "The probability that js updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node.",
                "In order to obtain the probability of js possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ].",
                "Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a multiplexer modulated by Mod[Mt j ].",
                "Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1.",
                "Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1.",
                "Notice that the model update link may be replaced by the <br>dependency link</br>s between the chance nodes that constitute the model nodes in the two time slices.",
                "In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them.",
                "Chance nodes and <br>dependency link</br>s that not in bold are standard, usually found in DIDs.",
                "Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents js observation at time t + 1.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links.",
                "We note that the possible set of models of the other agent j grows exponentially with the number of time steps.",
                "For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node. 4.2 Solution Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively.",
                "For the purpose of illustration, let l=1 and T=2.",
                "The solution method uses the standard look-ahead technique, projecting the agents action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step.",
                "Because agent i has a belief over js models as well, the lookahead includes finding out the possible models that j could have in the future.",
                "Consequently, each of js subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions.",
                "These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b).",
                "We note the recursive nature of this solution: in solving agent is level 1 I-DID, js level 0 DIDs must be solved.",
                "If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner.",
                "We briefly outline the recursive algorithm for solving agent is Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase 1.",
                "For t from 1 to T − 1 do 2.",
                "If l ≥ 1 then Populate Mt+1 j,l−1 3.",
                "For each mt j in Range(Mt j,l−1) do 4.",
                "Recursively call algorithm with the l − 1 I-ID (or ID) that represents mt j and the horizon, T − t + 1 5.",
                "Map the decision node of the solved I-ID (or ID), OPT(mt j), to a chance node Aj 6.",
                "For each aj in OPT(mt j) do 7.",
                "For each oj in Oj (part of mt j) do 8.",
                "Update js belief, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← New I-ID (or ID) with bt+1 j as the initial belief 10.",
                "Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11.",
                "Add the model node, Mt+1 j,l−1, and the <br>dependency link</br>s between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b)) 12.",
                "Add the chance, decision, and utility nodes for t + 1 time slice and the <br>dependency link</br>s between them 13.",
                "Establish the CPTs for each chance node and utility node Look-Ahead Phase 14.",
                "Apply the standard look-ahead and backup method to solve the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5.",
                "We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the <br>dependency link</br>s and the conditional probability tables for each node.",
                "We particularly focus on establishing and populating the model nodes (lines 3-11).",
                "Note that Range(·) returns the values (lower level models) of the random variable given as input (model node).",
                "In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs.",
                "Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents.",
                "As we mentioned previously, the 0-th level models are the traditional DIDs.",
                "Their solutions provide probability distributions over actions of the agent modeled at that level to I-DIDs at level 1.",
                "Given probability distributions over other agents actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models.",
                "Assume that the number of models considered at each level is bound by a number, M. Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs. 5.",
                "EXAMPLE APPLICATIONS To illustrate the usefulness of I-DIDs, we apply them to three problem domains.",
                "We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it. 5.1 Followership-Leadership in the Multiagent Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9].",
                "The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L).",
                "In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agents opening one of the doors.",
                "When any door is opened, the tiger persists in its original location with a probability of 95%.",
                "Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%.",
                "Agent j, on the other hand, hears growls with a reliability of 95%.",
                "Thus, the setting is such that agent i hears agent j opening doors more reliably than the tigers growls.",
                "This suggests that i could use js actions as an indication of the location of the tiger, as we discuss below.",
                "Each agents preferences are as in the single agent game discussed in [13].",
                "The transition, observation, and reward functions are shown in [16].",
                "A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions.",
                "In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship.",
                "In particular, we analyze the situational and epistemological conditions sufficient for their emergence.",
                "The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the others actions in order to maximize its payoffs.",
                "Let us consider a particular setting of the tiger problem in which agent i believes that js preferences are aligned with its own - both of them just want to get the gold - and js hearing is more reliable in comparison to itself.",
                "As an example, suppose that j, on listening can discern the tigers location 95% of the times compared to is 65% accuracy.",
                "Additionally, agent i does not have any initial information about the tigers location.",
                "In other words, is single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger.",
                "In addition, i considers two models of j, which differ in js flat level 0 initial beliefs.",
                "This is represented in the level 1 I-ID shown in Fig. 6(a).",
                "According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)).",
                "Agent i is undecided on these two models of j.",
                "If we vary is hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior.",
                "If is probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow js actions: i opens the same door that j opened previously iff is own assessment of the tigers location confirms js pick.",
                "If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)).",
                "Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem.",
                "Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations.",
                "We observed that a single level of belief nesting - beliefs about the others models - was sufficient for followership to emerge in the tiger problem.",
                "However, the epistemological requirements for the emergence of leadership are more complex.",
                "For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i.",
                "As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow js actions over time.",
                "Agent j emerges as a leader if it believes that i will follow it, which implies that js belief must be nested two levels deep to enable it to recognize its leadership role.",
                "Realizing that i will follow presents j with an opportunity to influence is actions in the benefit of the collective good or its self-interest alone.",
                "For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original.",
                "If j alone selects the correct door, it gets the payoff of 10.",
                "On the other hand, if both agents pick the wrong door, their penalties are cut in half.",
                "In this setting, it is in both js best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader.",
                "However, consider a slightly different problem in which j gains from is loss and is penalized if i gains.",
                "Specifically, let is payoff be subtracted from js, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then is loss of 100 becomes js gain.",
                "Agent j believes that i incorrectly thinks that js preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is.",
                "Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following js actions.",
                "We show is normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a).",
                "The policy demonstrates that i will blindly follow js actions.",
                "Since the tiger persists in its original location with a probability of 0.95, i will select the same door again.",
                "If j begins the game with a 99% probability that the tiger is on the right, solving js I-DID nested two levels deep, results in the policy shown in Fig. 8(b).",
                "Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL.",
                "Agent js intention is to deceive i who, it believes, will follow js actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest.",
                "Figure 8: Emergence of deception between agents in the tiger problem.",
                "Behaviors of interest are in bold. * denotes as before. (a) Agent is policy demonstrating that it will blindly follow js actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i. 5.2 Altruism and Reciprocity in the Public Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves.",
                "Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot.",
                "However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes.",
                "Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others contributions.",
                "However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions.",
                "The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4].",
                "Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting.",
                "These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others.",
                "For simplicity, we assume that the game is played between M = 2 agents, i and j.",
                "Let each agent be initially endowed with XT amount of resources.",
                "While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions.",
                "Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute.",
                "The latter action is deThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D).",
                "We assume that the actions are not observable to others.",
                "The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return.",
                "We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain.",
                "Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment.",
                "In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment.",
                "Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent.",
                "For simplicity, we assume that the cost of punishing is same for both the agents.",
                "The one-shot PG game with punishment is shown in Table. 1.",
                "Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action.",
                "If P < XT − ciXT , then defection is the dominating action for both.",
                "If P = XT − ciXT , then the game is not dominance-solvable.",
                "Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a).",
                "We formulate a sequential version of the PG problem with punishment from the perspective of agent i.",
                "Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents.",
                "Each agent may contribute a fixed amount, xc, or defect.",
                "An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot.",
                "Notice that the observations are also indirectly indicative of agent js actions because the state of the public pot is influenced by them.",
                "The amount of resources in agent is private pot, is perfectly observable to i.",
                "The payoffs are analogous to Table. 1.",
                "Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)).",
                "Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect.",
                "Let xc = 1 and the level 0 agent be punished half the times it defects.",
                "With one action remaining, both types of agents choose to contribute to avoid being punished.",
                "With two actions to go, the altruistic type chooses to contribute, while the other defects.",
                "This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids.",
                "Because cj for the non-altruistic type is less, it prefers not to contribute.",
                "With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects.",
                "For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection.",
                "We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level 0 models, mentioned previously, to j (see Fig. 9).",
                "If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps.",
                "This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type.",
                "However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1.",
                "These results demonstrate that the behavior of our altruistic type resembles that found experimentally.",
                "The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic.",
                "We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection.",
                "The reciprocal types marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other.",
                "We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full.",
                "For this prior belief, i chooses to defect.",
                "On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)).",
                "This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from js action to contribute.",
                "Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step.",
                "Agent i therefore chooses to contribute to reciprocate js action.",
                "An analogous reasoning leads i to defect when it observes a meager pot.",
                "With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations.",
                "Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic). 5.3 Strategies in Two-Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2].",
                "Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck.",
                "While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands.",
                "To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit.",
                "Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot.",
                "During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to 8.",
                "Notice that, for example, if a lower numbered card is discarded, the probability of receiving a low card in exchange is now reduced.",
                "We show the level 1 I-ID for the simplified two-player Poker in Fig. 11.",
                "We considered two models (personality types) of agent j.",
                "The conservative type believes that it is likely that its opponent has a high numbered card in its hand.",
                "On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card.",
                "Thus, the two types differ in their beliefs over their opponents hand.",
                "In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution.",
                "With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand.",
                "This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange.",
                "The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one.",
                "Figure 11: (a) Level 1 I-ID of agent i.",
                "The observation reveals information about js hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a).",
                "The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in js hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. is own hand contains the card numbered 8.",
                "The agent starts by keeping its card.",
                "On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card.",
                "If i observes that j discarded its card into the L or H pile, i believes that j is aggressive.",
                "On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange.",
                "Because the probability of receiving a low card is high now, i chooses to keep its card.",
                "On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card.",
                "In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff.",
                "This is partly due to the fact that an observation of, say, L resets the agent is previous time step beliefs over js hand to the low numbered cards only. 6.",
                "DISCUSSION We showed how DIDs may be extended to I-DIDs that enable online sequential decision-making in uncertain multiagent settings.",
                "Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent is three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs.",
                "I-DIDs extend NIDs to allow sequential decision-making over multiple time steps in the presence of other interacting agents.",
                "I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs.",
                "We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality.",
                "Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work.",
                "The first author would like to acknowledge the support of a UGARF grant. 7.",
                "REFERENCES [1] R. J. Aumann.",
                "Interactive epistemology i: Knowledge.",
                "International Journal of Game Theory, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron.",
                "The challenge of poker.",
                "AIJ, 2001. [3] A. Brandenburger and E. Dekel.",
                "Hierarchies of beliefs and common knowledge.",
                "Journal of Economic Theory, 59:189-198, 1993. [4] C. Camerer.",
                "Behavioral Game Theory: Experiments in Strategic Interaction.",
                "Princeton University Press, 2003. [5] E. Fehr and S. Gachter.",
                "Cooperation and punishment in public goods experiments.",
                "American Economic Review, 90(4):980-994, 2000. [6] D. Fudenberg and D. K. Levine.",
                "The Theory of Learning in Games.",
                "MIT Press, 1998. [7] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [8] Y. Gal and A. Pfeffer.",
                "A language for modeling agents decision-making processes in games.",
                "In AAMAS, 2003. [9] P. Gmytrasiewicz and P. Doshi.",
                "A framework for sequential planning in multiagent settings.",
                "JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz and E. Durfee.",
                "Rational coordination in multi-agent environments.",
                "JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi.",
                "Games with incomplete information played by bayesian players.",
                "Management Science, 14(3):159-182, 1967. [12] R. A. Howard and J. E. Matheson.",
                "Influence diagrams.",
                "In R. A. Howard and J. E. Matheson, editors, The Principles and Applications of Decision Analysis.",
                "Strategic Decisions Group, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman, and A. Cassandra.",
                "Planning and acting in partially observable stochastic domains.",
                "Artificial Intelligence Journal, 2, 1998. [14] D. Koller and B. Milch.",
                "Multi-agent influence diagrams for representing and solving games.",
                "In IJCAI, pages 1027-1034, 2001. [15] K. Polich and P. Gmytrasiewicz.",
                "Interactive dynamic influence diagrams.",
                "In GTDT Workshop, AAMAS, 2006. [16] B.",
                "Rathnas., P. Doshi, and P. J. Gmytrasiewicz.",
                "Exact solutions to interactive pomdps using behavioral equivalence.",
                "In Autonomous Agents and Multi-Agent Systems Conference (AAMAS), 2006. [17] S. Russell and P. Norvig.",
                "Artificial Intelligence: A Modern Approach (Second Edition).",
                "Prentice Hall, 2003. [18] R. D. Shachter.",
                "Evaluating influence diagrams.",
                "Operations Research, 34(6):871-882, 1986. [19] D. Suryadi and P. Gmytrasiewicz.",
                "Learning models of other agents using influence diagrams.",
                "In UM, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Además, aclaramos la semántica del enlace de política de propósito especial introducido en la representación de I-Did por [15], y mostramos que podría ser reemplazado por el \"enlace de dependencia\" tradicional.",
                "Explicamos la semántica de este enlace mostrando cómo se puede implementar utilizando el \"enlace de dependencia\" tradicional entre los nodos casuales que constituyen los nodos modelo.",
                "Observe que la Fig. 1 (b) aclara la semántica del enlace de la política y muestra cómo se puede representar utilizando el tradicional \"enlace de dependencia\" s.",
                "Observe que el enlace de actualización del modelo puede ser reemplazado por el \"enlace de dependencia\" entre los nodos casuales que constituyen los nodos modelo en los dos cortes de tiempo.",
                "Los nodos casuales y el \"enlace de dependencia\" que no están en negrita son estándar, generalmente se encuentran en DIDS.",
                "Agregue el nodo modelo, MT+1 J, L - 1 y el \"enlace de dependencia\" s entre MT J, L - 1 y Mt+1 J, L - 1 (que se muestra en la Fig. 3 (b)) 12.",
                "Agregue la oportunidad, la decisión y los nodos de utilidad para T + 1 Time Slice y el \"enlace de dependencia\" entre ellos 13.",
                "Adoptamos un enfoque de dos fases: dado un I-ID de nivel L (descrito anteriormente en la Sección 3) con todos los modelos de nivel inferior también representados como I-ID o IDS (IF Nivel 0), el primer paso es expandir el nivelL I-ID sobre t Pasos de tiempo Agregar el \"enlace de dependencia\" S y las tablas de probabilidad condicional para cada nodo."
            ],
            "translated_text": "",
            "candidates": [
                "enlace de dependencia",
                "enlace de dependencia",
                "enlace de dependencia",
                "enlace de dependencia",
                "enlace de dependencia",
                "enlace de dependencia",
                "enlace de dependencia",
                "enlace de dependencia",
                "enlace de dependencia",
                "enlace de dependencia",
                "Dependenciaenlace",
                "enlace de dependencia",
                "Link de dependencia",
                "enlace de dependencia",
                "enlace de dependencia",
                "enlace de dependencia"
            ],
            "error": []
        },
        "influence diagram": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Graphical Models for Online Solutions to Interactive POMDPs Prashant Doshi Dept.",
                "of Computer Science University of Georgia Athens, GA 30602, USA pdoshi@cs.uga.edu Yifeng Zeng Dept. of Computer Science Aalborg University DK-9220 Aalborg, Denmark yfzeng@cs.aau.edu Qiongyu Chen Dept. of Computer Science National Univ. of Singapore 117543, Singapore chenqy@comp.nus.edu.sg ABSTRACT We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation.",
                "These graphical models called interactive dynamic <br>influence diagram</br>s (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents.",
                "Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Theory 1.",
                "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in partially observable multiagent environments.",
                "They generalize POMDPs [13] to multiagent settings by including the other agents computable models in the state space along with the states of the physical environment.",
                "The models encompass all information influencing the agents behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11].",
                "I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a decision-makers perspective in the interaction.",
                "In [15], Polich and Gmytrasiewicz introduced interactive dynamic <br>influence diagram</br>s (I-DIDs) as the computational representations of I-POMDPs.",
                "I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs contribute to a growing line of work [19] that includes multi-agent <br>influence diagram</br>s (MAIDs) [14], and more recently, networks of <br>influence diagram</br>s (NIDs) [8].",
                "These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agents actions and chance nodes capturing the agents private information.",
                "MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the independence structure.",
                "NIDs extend MAIDs to include agents uncertainty over the game being played and over models of the other agents.",
                "Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent.",
                "Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently.",
                "However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games.",
                "Matters are more complex when we consider interactions that are extended over time, where predictions about others future actions must be made using models that change as the agents act and observe.",
                "I-DIDs address this gap by allowing the representation of other agents models as the values of a special model node.",
                "Both, other agents models and the original agents beliefs over these models are updated over time using special-purpose implementations.",
                "In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID.",
                "Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently.",
                "Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I-DID by [15], and show that it could be replaced by traditional dependency links.",
                "In the previous representation of the I-DID, the update of the agents belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time.",
                "We explicate the semantics of this link by showing how it can be implemented using the traditional dependency links between the chance nodes that constitute the model nodes.",
                "The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs.",
                "We show how IDIDs may be used to model an agents uncertainty over others models, that may themselves be I-DIDs.",
                "Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others models.",
                "Analogous to DIDs, I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. 2.",
                "BACKGROUND: FINITELY NESTED IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents models as part of the state space [9].",
                "Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents models and their beliefs about others.",
                "For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j.",
                "A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment.",
                "Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj .",
                "Here, j is Bayes rational and OCj is js optimality criterion.",
                "SMj is the set of subintentional models of j.",
                "Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent.",
                "We give a recursive bottom-up construction of the interactive state space below.",
                "ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . .",
                "ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent is preferences over its interactive states.",
                "Usually only the physical states will matter.",
                "Agent is policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i.",
                "Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai). 2.1 Belief Update Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes.",
                "However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones.",
                "First, since the state of the physical environment depends on the actions of both agents, is prediction of how the physical state changes has to be made based on its prediction of js actions.",
                "Second, changes in js models have to be included in is belief update.",
                "Specifically, if j is intentional then an update of js beliefs due to its action and observation has to be included.",
                "In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief.",
                "If js model is subintentional, then js probable observations are appended to the observation history contained in the model.",
                "Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update.",
                "For a version of the belief update when js model is subintentional, see [9].",
                "If agent j is also modeled as an I-POMDP, then is belief update invokes js belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke is belief update and so on.",
                "This recursion in belief nesting bottoms out at the 0th level.",
                "At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9]. 2.2 Value Iteration Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)).",
                "Eq. 2 is a basis for value iteration in I-POMDPs.",
                "Agent is optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3.",
                "INTERACTIVEINFLUENCEDIAGRAMS A naive extension of <br>influence diagram</br>s (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes.",
                "However, this approach assumes that the agents actions are controlled using a probability distribution that does not change over time.",
                "Interactive <br>influence diagram</br>s (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node.",
                "We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon.",
                "We note that the probability distribution over the chance node, S, and the model node together represents agent is belief over its interactive states.",
                "In addition to the model 1 The 0th level model is a POMDP: Other agents actions are treated as exogenous events and folded into the T, O, and R functions.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j.",
                "The hexagon is the model node (Mj,l−1) whose structure we show in (b).",
                "Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ).",
                "Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the policy link in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model.",
                "In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs.",
                "The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2.",
                "Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional.",
                "Because the model node contains the alternative models of the other agent as its values, its representation is not trivial.",
                "In particular, some of the models within the node are I-IDs that when solved generate the agents optimal policy in their decision nodes.",
                "Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise.",
                "Borrowing insights from previous work [8], we observe that the model node and the dashed policy link that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b).",
                "The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj.",
                "Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent is beliefs.",
                "The conditional probability table of the chance node, Aj, is a multiplexer that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj].",
                "The values of Mod[Mj] denote the different models of j.",
                "In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1.",
                "The distribution over the node, Mod[Mj], is the agent is belief over the models of j given a physical state.",
                "For more agents, we will have as many model nodes as there are agents.",
                "Notice that Fig. 1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional dependency links.",
                "In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them.",
                "In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes.",
                "This allows I-IDs to be represented and implemented using conventional application tools that target IDs.",
                "Note that we may view the level l I-ID as a NID.",
                "Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2).",
                "If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID.",
                "Note that within the I-IDs (or IDs) at each level, there is only a single decision node.",
                "Thus, our NID does not contain any MAIDs.",
                "Figure 2: A level l I-ID represented as a NID.",
                "The probabilities assigned to the blocks of the NID are is beliefs over js models conditioned on a physical state. 3.2 Solution The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively.",
                "We start by solving the level 0 models, which, if intentional, are traditional IDs.",
                "Their solutions provide probability distributions over the other agents actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID.",
                "The mapping from the level 0 models decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c).",
                "During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj].",
                "As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent is belief over the models of j conditioned on the physical state.",
                "The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j.",
                "Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18].",
                "This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief.",
                "Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agents current belief is known. 4.",
                "INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic <br>influence diagram</br>s (I-DIDs) extend I-IDs (and NIDs) to allow sequential decision-making over several time steps.",
                "Just as DIDs are structured graphical representations of POMDPs, I-DIDs are the graphical online analogs for finitely nested I-POMDPs.",
                "I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents. 4.1 Syntax We depict a general two time-slice I-DID in Fig. 3(a).",
                "In addition to the model nodes and the dashed policy link, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a).",
                "We explained the semantics of the model node and the policy link in the previous section; we describe the model updates next.",
                "The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1.",
                "Recall from Section 2 that an agents intentional model includes its belief.",
                "Because the agents act and receive observations, their models are updated to reflect their changed beliefs.",
                "Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models.",
                "Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models.",
                "Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model.",
                "These steps are a part of agent is belief update formalized using Eq. 1.",
                "In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID.",
                "If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ).",
                "These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation.",
                "The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously.",
                "Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed.",
                "The probability that js updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node.",
                "In order to obtain the probability of js possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ].",
                "Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a multiplexer modulated by Mod[Mt j ].",
                "Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1.",
                "Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1.",
                "Notice that the model update link may be replaced by the dependency links between the chance nodes that constitute the model nodes in the two time slices.",
                "In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them.",
                "Chance nodes and dependency links that not in bold are standard, usually found in DIDs.",
                "Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents js observation at time t + 1.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links.",
                "We note that the possible set of models of the other agent j grows exponentially with the number of time steps.",
                "For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node. 4.2 Solution Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively.",
                "For the purpose of illustration, let l=1 and T=2.",
                "The solution method uses the standard look-ahead technique, projecting the agents action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step.",
                "Because agent i has a belief over js models as well, the lookahead includes finding out the possible models that j could have in the future.",
                "Consequently, each of js subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions.",
                "These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b).",
                "We note the recursive nature of this solution: in solving agent is level 1 I-DID, js level 0 DIDs must be solved.",
                "If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner.",
                "We briefly outline the recursive algorithm for solving agent is Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase 1.",
                "For t from 1 to T − 1 do 2.",
                "If l ≥ 1 then Populate Mt+1 j,l−1 3.",
                "For each mt j in Range(Mt j,l−1) do 4.",
                "Recursively call algorithm with the l − 1 I-ID (or ID) that represents mt j and the horizon, T − t + 1 5.",
                "Map the decision node of the solved I-ID (or ID), OPT(mt j), to a chance node Aj 6.",
                "For each aj in OPT(mt j) do 7.",
                "For each oj in Oj (part of mt j) do 8.",
                "Update js belief, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← New I-ID (or ID) with bt+1 j as the initial belief 10.",
                "Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11.",
                "Add the model node, Mt+1 j,l−1, and the dependency links between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b)) 12.",
                "Add the chance, decision, and utility nodes for t + 1 time slice and the dependency links between them 13.",
                "Establish the CPTs for each chance node and utility node Look-Ahead Phase 14.",
                "Apply the standard look-ahead and backup method to solve the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5.",
                "We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the dependency links and the conditional probability tables for each node.",
                "We particularly focus on establishing and populating the model nodes (lines 3-11).",
                "Note that Range(·) returns the values (lower level models) of the random variable given as input (model node).",
                "In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs.",
                "Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents.",
                "As we mentioned previously, the 0-th level models are the traditional DIDs.",
                "Their solutions provide probability distributions over actions of the agent modeled at that level to I-DIDs at level 1.",
                "Given probability distributions over other agents actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models.",
                "Assume that the number of models considered at each level is bound by a number, M. Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs. 5.",
                "EXAMPLE APPLICATIONS To illustrate the usefulness of I-DIDs, we apply them to three problem domains.",
                "We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it. 5.1 Followership-Leadership in the Multiagent Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9].",
                "The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L).",
                "In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agents opening one of the doors.",
                "When any door is opened, the tiger persists in its original location with a probability of 95%.",
                "Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%.",
                "Agent j, on the other hand, hears growls with a reliability of 95%.",
                "Thus, the setting is such that agent i hears agent j opening doors more reliably than the tigers growls.",
                "This suggests that i could use js actions as an indication of the location of the tiger, as we discuss below.",
                "Each agents preferences are as in the single agent game discussed in [13].",
                "The transition, observation, and reward functions are shown in [16].",
                "A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions.",
                "In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship.",
                "In particular, we analyze the situational and epistemological conditions sufficient for their emergence.",
                "The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the others actions in order to maximize its payoffs.",
                "Let us consider a particular setting of the tiger problem in which agent i believes that js preferences are aligned with its own - both of them just want to get the gold - and js hearing is more reliable in comparison to itself.",
                "As an example, suppose that j, on listening can discern the tigers location 95% of the times compared to is 65% accuracy.",
                "Additionally, agent i does not have any initial information about the tigers location.",
                "In other words, is single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger.",
                "In addition, i considers two models of j, which differ in js flat level 0 initial beliefs.",
                "This is represented in the level 1 I-ID shown in Fig. 6(a).",
                "According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)).",
                "Agent i is undecided on these two models of j.",
                "If we vary is hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior.",
                "If is probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow js actions: i opens the same door that j opened previously iff is own assessment of the tigers location confirms js pick.",
                "If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)).",
                "Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem.",
                "Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations.",
                "We observed that a single level of belief nesting - beliefs about the others models - was sufficient for followership to emerge in the tiger problem.",
                "However, the epistemological requirements for the emergence of leadership are more complex.",
                "For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i.",
                "As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow js actions over time.",
                "Agent j emerges as a leader if it believes that i will follow it, which implies that js belief must be nested two levels deep to enable it to recognize its leadership role.",
                "Realizing that i will follow presents j with an opportunity to influence is actions in the benefit of the collective good or its self-interest alone.",
                "For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original.",
                "If j alone selects the correct door, it gets the payoff of 10.",
                "On the other hand, if both agents pick the wrong door, their penalties are cut in half.",
                "In this setting, it is in both js best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader.",
                "However, consider a slightly different problem in which j gains from is loss and is penalized if i gains.",
                "Specifically, let is payoff be subtracted from js, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then is loss of 100 becomes js gain.",
                "Agent j believes that i incorrectly thinks that js preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is.",
                "Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following js actions.",
                "We show is normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a).",
                "The policy demonstrates that i will blindly follow js actions.",
                "Since the tiger persists in its original location with a probability of 0.95, i will select the same door again.",
                "If j begins the game with a 99% probability that the tiger is on the right, solving js I-DID nested two levels deep, results in the policy shown in Fig. 8(b).",
                "Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL.",
                "Agent js intention is to deceive i who, it believes, will follow js actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest.",
                "Figure 8: Emergence of deception between agents in the tiger problem.",
                "Behaviors of interest are in bold. * denotes as before. (a) Agent is policy demonstrating that it will blindly follow js actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i. 5.2 Altruism and Reciprocity in the Public Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves.",
                "Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot.",
                "However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes.",
                "Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others contributions.",
                "However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions.",
                "The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4].",
                "Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting.",
                "These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others.",
                "For simplicity, we assume that the game is played between M = 2 agents, i and j.",
                "Let each agent be initially endowed with XT amount of resources.",
                "While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions.",
                "Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute.",
                "The latter action is deThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D).",
                "We assume that the actions are not observable to others.",
                "The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return.",
                "We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain.",
                "Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment.",
                "In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment.",
                "Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent.",
                "For simplicity, we assume that the cost of punishing is same for both the agents.",
                "The one-shot PG game with punishment is shown in Table. 1.",
                "Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action.",
                "If P < XT − ciXT , then defection is the dominating action for both.",
                "If P = XT − ciXT , then the game is not dominance-solvable.",
                "Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a).",
                "We formulate a sequential version of the PG problem with punishment from the perspective of agent i.",
                "Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents.",
                "Each agent may contribute a fixed amount, xc, or defect.",
                "An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot.",
                "Notice that the observations are also indirectly indicative of agent js actions because the state of the public pot is influenced by them.",
                "The amount of resources in agent is private pot, is perfectly observable to i.",
                "The payoffs are analogous to Table. 1.",
                "Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)).",
                "Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect.",
                "Let xc = 1 and the level 0 agent be punished half the times it defects.",
                "With one action remaining, both types of agents choose to contribute to avoid being punished.",
                "With two actions to go, the altruistic type chooses to contribute, while the other defects.",
                "This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids.",
                "Because cj for the non-altruistic type is less, it prefers not to contribute.",
                "With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects.",
                "For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection.",
                "We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level 0 models, mentioned previously, to j (see Fig. 9).",
                "If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps.",
                "This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type.",
                "However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1.",
                "These results demonstrate that the behavior of our altruistic type resembles that found experimentally.",
                "The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic.",
                "We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection.",
                "The reciprocal types marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other.",
                "We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full.",
                "For this prior belief, i chooses to defect.",
                "On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)).",
                "This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from js action to contribute.",
                "Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step.",
                "Agent i therefore chooses to contribute to reciprocate js action.",
                "An analogous reasoning leads i to defect when it observes a meager pot.",
                "With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations.",
                "Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic). 5.3 Strategies in Two-Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2].",
                "Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck.",
                "While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands.",
                "To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit.",
                "Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot.",
                "During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to 8.",
                "Notice that, for example, if a lower numbered card is discarded, the probability of receiving a low card in exchange is now reduced.",
                "We show the level 1 I-ID for the simplified two-player Poker in Fig. 11.",
                "We considered two models (personality types) of agent j.",
                "The conservative type believes that it is likely that its opponent has a high numbered card in its hand.",
                "On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card.",
                "Thus, the two types differ in their beliefs over their opponents hand.",
                "In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution.",
                "With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand.",
                "This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange.",
                "The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one.",
                "Figure 11: (a) Level 1 I-ID of agent i.",
                "The observation reveals information about js hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a).",
                "The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in js hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. is own hand contains the card numbered 8.",
                "The agent starts by keeping its card.",
                "On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card.",
                "If i observes that j discarded its card into the L or H pile, i believes that j is aggressive.",
                "On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange.",
                "Because the probability of receiving a low card is high now, i chooses to keep its card.",
                "On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card.",
                "In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff.",
                "This is partly due to the fact that an observation of, say, L resets the agent is previous time step beliefs over js hand to the low numbered cards only. 6.",
                "DISCUSSION We showed how DIDs may be extended to I-DIDs that enable online sequential decision-making in uncertain multiagent settings.",
                "Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent is three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs.",
                "I-DIDs extend NIDs to allow sequential decision-making over multiple time steps in the presence of other interacting agents.",
                "I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs.",
                "We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality.",
                "Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work.",
                "The first author would like to acknowledge the support of a UGARF grant. 7.",
                "REFERENCES [1] R. J. Aumann.",
                "Interactive epistemology i: Knowledge.",
                "International Journal of Game Theory, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron.",
                "The challenge of poker.",
                "AIJ, 2001. [3] A. Brandenburger and E. Dekel.",
                "Hierarchies of beliefs and common knowledge.",
                "Journal of Economic Theory, 59:189-198, 1993. [4] C. Camerer.",
                "Behavioral Game Theory: Experiments in Strategic Interaction.",
                "Princeton University Press, 2003. [5] E. Fehr and S. Gachter.",
                "Cooperation and punishment in public goods experiments.",
                "American Economic Review, 90(4):980-994, 2000. [6] D. Fudenberg and D. K. Levine.",
                "The Theory of Learning in Games.",
                "MIT Press, 1998. [7] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [8] Y. Gal and A. Pfeffer.",
                "A language for modeling agents decision-making processes in games.",
                "In AAMAS, 2003. [9] P. Gmytrasiewicz and P. Doshi.",
                "A framework for sequential planning in multiagent settings.",
                "JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz and E. Durfee.",
                "Rational coordination in multi-agent environments.",
                "JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi.",
                "Games with incomplete information played by bayesian players.",
                "Management Science, 14(3):159-182, 1967. [12] R. A. Howard and J. E. Matheson.",
                "Influence diagrams.",
                "In R. A. Howard and J. E. Matheson, editors, The Principles and Applications of Decision Analysis.",
                "Strategic Decisions Group, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman, and A. Cassandra.",
                "Planning and acting in partially observable stochastic domains.",
                "Artificial Intelligence Journal, 2, 1998. [14] D. Koller and B. Milch.",
                "Multi-agent <br>influence diagram</br>s for representing and solving games.",
                "In IJCAI, pages 1027-1034, 2001. [15] K. Polich and P. Gmytrasiewicz.",
                "Interactive dynamic <br>influence diagram</br>s.",
                "In GTDT Workshop, AAMAS, 2006. [16] B.",
                "Rathnas., P. Doshi, and P. J. Gmytrasiewicz.",
                "Exact solutions to interactive pomdps using behavioral equivalence.",
                "In Autonomous Agents and Multi-Agent Systems Conference (AAMAS), 2006. [17] S. Russell and P. Norvig.",
                "Artificial Intelligence: A Modern Approach (Second Edition).",
                "Prentice Hall, 2003. [18] R. D. Shachter.",
                "Evaluating <br>influence diagram</br>s.",
                "Operations Research, 34(6):871-882, 1986. [19] D. Suryadi and P. Gmytrasiewicz.",
                "Learning models of other agents using <br>influence diagram</br>s.",
                "In UM, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Estos modelos gráficos llamados \"diagrama de influencia\" dinámico interactivo (I-DID) buscan modelar explícitamente la estructura que a menudo está presente en los problemas del mundo real al descomponer la situación en las variables de casualidad y decisión, y las dependencias entre las variables.",
                "En [15], Polich y Gmytrasiewicz introdujeron el \"diagrama de influencia\" dinámico interactivo (I-DID) como las representaciones computacionales de I-POMDPS.",
                "Los I-Did contribuyen a una línea de trabajo creciente [19] que incluye \"diagrama de influencia\" de múltiples agentes (Maids) [14], y más recientemente, redes de \"diagrama de influencia\" s (NIDS) [8].",
                "InteractiveInfluencedAgrams Una extensión ingenua de \"Diagrama de influencia\" S (IDS) a configuraciones pobladas por múltiples agentes es posible tratando a otros agentes como autómatas, representados usando nodos de casas.",
                "El \"diagrama de influencia\" interactivo (I-IDS) adoptan un enfoque más sofisticado al generalizar las IDS para que sean aplicables a la configuración compartida con otros agentes que pueden actuar y observar, y actualizar sus creencias.3.1 Sintaxis Además de la oportunidad habitual, la decisión y los nodos de utilidad, los IID incluyen un nuevo tipo de nodo llamado nodo modelo.",
                "Diagramas de influencia dinámica interactiva Los diagramas interactivos de \"influencia\" dinámicos (I-DID) extienden I-IDS (y NIDS) para permitir la toma de decisiones secuenciales en varios pasos de tiempo.",
                "\"Diagrama de influencia\" de múltiples agentes para representar y resolver juegos.",
                "Dinámica interactiva \"Diagrama de influencia\" s.",
                "Evaluación del \"diagrama de influencia\" s.",
                "Modelos de aprendizaje de otros agentes que usan \"diagrama de influencia\" s."
            ],
            "translated_text": "",
            "candidates": [
                "Diagrama de influencia",
                "diagrama de influencia",
                "Diagrama de influencia",
                "diagrama de influencia",
                "Diagrama de influencia",
                "diagrama de influencia",
                "diagrama de influencia",
                "Diagrama de influencia",
                "Diagrama de influencia",
                "Diagrama de influencia",
                "diagrama de influencia",
                "Diagrama de influencia",
                "influencia",
                "Diagrama de influencia",
                "Diagrama de influencia",
                "Diagrama de influencia",
                "Diagrama de influencia",
                "Diagrama de influencia",
                "diagrama de influencia",
                "Diagrama de influencia",
                "diagrama de influencia"
            ],
            "error": []
        },
        "interactive influence diagram": {
            "translated_key": "Diagrama de influencia interactiva",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Graphical Models for Online Solutions to Interactive POMDPs Prashant Doshi Dept.",
                "of Computer Science University of Georgia Athens, GA 30602, USA pdoshi@cs.uga.edu Yifeng Zeng Dept. of Computer Science Aalborg University DK-9220 Aalborg, Denmark yfzeng@cs.aau.edu Qiongyu Chen Dept. of Computer Science National Univ. of Singapore 117543, Singapore chenqy@comp.nus.edu.sg ABSTRACT We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation.",
                "These graphical models called interactive dynamic influence diagrams (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents.",
                "Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Theory 1.",
                "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in partially observable multiagent environments.",
                "They generalize POMDPs [13] to multiagent settings by including the other agents computable models in the state space along with the states of the physical environment.",
                "The models encompass all information influencing the agents behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11].",
                "I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a decision-makers perspective in the interaction.",
                "In [15], Polich and Gmytrasiewicz introduced interactive dynamic influence diagrams (I-DIDs) as the computational representations of I-POMDPs.",
                "I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs contribute to a growing line of work [19] that includes multi-agent influence diagrams (MAIDs) [14], and more recently, networks of influence diagrams (NIDs) [8].",
                "These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agents actions and chance nodes capturing the agents private information.",
                "MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the independence structure.",
                "NIDs extend MAIDs to include agents uncertainty over the game being played and over models of the other agents.",
                "Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent.",
                "Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently.",
                "However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games.",
                "Matters are more complex when we consider interactions that are extended over time, where predictions about others future actions must be made using models that change as the agents act and observe.",
                "I-DIDs address this gap by allowing the representation of other agents models as the values of a special model node.",
                "Both, other agents models and the original agents beliefs over these models are updated over time using special-purpose implementations.",
                "In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID.",
                "Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently.",
                "Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I-DID by [15], and show that it could be replaced by traditional dependency links.",
                "In the previous representation of the I-DID, the update of the agents belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time.",
                "We explicate the semantics of this link by showing how it can be implemented using the traditional dependency links between the chance nodes that constitute the model nodes.",
                "The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs.",
                "We show how IDIDs may be used to model an agents uncertainty over others models, that may themselves be I-DIDs.",
                "Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others models.",
                "Analogous to DIDs, I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. 2.",
                "BACKGROUND: FINITELY NESTED IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents models as part of the state space [9].",
                "Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents models and their beliefs about others.",
                "For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j.",
                "A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment.",
                "Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj .",
                "Here, j is Bayes rational and OCj is js optimality criterion.",
                "SMj is the set of subintentional models of j.",
                "Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent.",
                "We give a recursive bottom-up construction of the interactive state space below.",
                "ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . .",
                "ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent is preferences over its interactive states.",
                "Usually only the physical states will matter.",
                "Agent is policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i.",
                "Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai). 2.1 Belief Update Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes.",
                "However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones.",
                "First, since the state of the physical environment depends on the actions of both agents, is prediction of how the physical state changes has to be made based on its prediction of js actions.",
                "Second, changes in js models have to be included in is belief update.",
                "Specifically, if j is intentional then an update of js beliefs due to its action and observation has to be included.",
                "In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief.",
                "If js model is subintentional, then js probable observations are appended to the observation history contained in the model.",
                "Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update.",
                "For a version of the belief update when js model is subintentional, see [9].",
                "If agent j is also modeled as an I-POMDP, then is belief update invokes js belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke is belief update and so on.",
                "This recursion in belief nesting bottoms out at the 0th level.",
                "At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9]. 2.2 Value Iteration Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)).",
                "Eq. 2 is a basis for value iteration in I-POMDPs.",
                "Agent is optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3.",
                "INTERACTIVEINFLUENCEDIAGRAMS A naive extension of influence diagrams (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes.",
                "However, this approach assumes that the agents actions are controlled using a probability distribution that does not change over time.",
                "Interactive influence diagrams (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node.",
                "We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon.",
                "We note that the probability distribution over the chance node, S, and the model node together represents agent is belief over its interactive states.",
                "In addition to the model 1 The 0th level model is a POMDP: Other agents actions are treated as exogenous events and folded into the T, O, and R functions.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j.",
                "The hexagon is the model node (Mj,l−1) whose structure we show in (b).",
                "Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ).",
                "Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the policy link in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model.",
                "In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs.",
                "The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2.",
                "Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional.",
                "Because the model node contains the alternative models of the other agent as its values, its representation is not trivial.",
                "In particular, some of the models within the node are I-IDs that when solved generate the agents optimal policy in their decision nodes.",
                "Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise.",
                "Borrowing insights from previous work [8], we observe that the model node and the dashed policy link that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b).",
                "The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj.",
                "Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent is beliefs.",
                "The conditional probability table of the chance node, Aj, is a multiplexer that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj].",
                "The values of Mod[Mj] denote the different models of j.",
                "In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1.",
                "The distribution over the node, Mod[Mj], is the agent is belief over the models of j given a physical state.",
                "For more agents, we will have as many model nodes as there are agents.",
                "Notice that Fig. 1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional dependency links.",
                "In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them.",
                "In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes.",
                "This allows I-IDs to be represented and implemented using conventional application tools that target IDs.",
                "Note that we may view the level l I-ID as a NID.",
                "Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2).",
                "If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID.",
                "Note that within the I-IDs (or IDs) at each level, there is only a single decision node.",
                "Thus, our NID does not contain any MAIDs.",
                "Figure 2: A level l I-ID represented as a NID.",
                "The probabilities assigned to the blocks of the NID are is beliefs over js models conditioned on a physical state. 3.2 Solution The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively.",
                "We start by solving the level 0 models, which, if intentional, are traditional IDs.",
                "Their solutions provide probability distributions over the other agents actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID.",
                "The mapping from the level 0 models decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c).",
                "During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj].",
                "As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent is belief over the models of j conditioned on the physical state.",
                "The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j.",
                "Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18].",
                "This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief.",
                "Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agents current belief is known. 4.",
                "INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic influence diagrams (I-DIDs) extend I-IDs (and NIDs) to allow sequential decision-making over several time steps.",
                "Just as DIDs are structured graphical representations of POMDPs, I-DIDs are the graphical online analogs for finitely nested I-POMDPs.",
                "I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents. 4.1 Syntax We depict a general two time-slice I-DID in Fig. 3(a).",
                "In addition to the model nodes and the dashed policy link, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a).",
                "We explained the semantics of the model node and the policy link in the previous section; we describe the model updates next.",
                "The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1.",
                "Recall from Section 2 that an agents intentional model includes its belief.",
                "Because the agents act and receive observations, their models are updated to reflect their changed beliefs.",
                "Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models.",
                "Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models.",
                "Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model.",
                "These steps are a part of agent is belief update formalized using Eq. 1.",
                "In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID.",
                "If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ).",
                "These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation.",
                "The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously.",
                "Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed.",
                "The probability that js updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node.",
                "In order to obtain the probability of js possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ].",
                "Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a multiplexer modulated by Mod[Mt j ].",
                "Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1.",
                "Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1.",
                "Notice that the model update link may be replaced by the dependency links between the chance nodes that constitute the model nodes in the two time slices.",
                "In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them.",
                "Chance nodes and dependency links that not in bold are standard, usually found in DIDs.",
                "Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents js observation at time t + 1.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links.",
                "We note that the possible set of models of the other agent j grows exponentially with the number of time steps.",
                "For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node. 4.2 Solution Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively.",
                "For the purpose of illustration, let l=1 and T=2.",
                "The solution method uses the standard look-ahead technique, projecting the agents action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step.",
                "Because agent i has a belief over js models as well, the lookahead includes finding out the possible models that j could have in the future.",
                "Consequently, each of js subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions.",
                "These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b).",
                "We note the recursive nature of this solution: in solving agent is level 1 I-DID, js level 0 DIDs must be solved.",
                "If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner.",
                "We briefly outline the recursive algorithm for solving agent is Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase 1.",
                "For t from 1 to T − 1 do 2.",
                "If l ≥ 1 then Populate Mt+1 j,l−1 3.",
                "For each mt j in Range(Mt j,l−1) do 4.",
                "Recursively call algorithm with the l − 1 I-ID (or ID) that represents mt j and the horizon, T − t + 1 5.",
                "Map the decision node of the solved I-ID (or ID), OPT(mt j), to a chance node Aj 6.",
                "For each aj in OPT(mt j) do 7.",
                "For each oj in Oj (part of mt j) do 8.",
                "Update js belief, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← New I-ID (or ID) with bt+1 j as the initial belief 10.",
                "Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11.",
                "Add the model node, Mt+1 j,l−1, and the dependency links between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b)) 12.",
                "Add the chance, decision, and utility nodes for t + 1 time slice and the dependency links between them 13.",
                "Establish the CPTs for each chance node and utility node Look-Ahead Phase 14.",
                "Apply the standard look-ahead and backup method to solve the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5.",
                "We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the dependency links and the conditional probability tables for each node.",
                "We particularly focus on establishing and populating the model nodes (lines 3-11).",
                "Note that Range(·) returns the values (lower level models) of the random variable given as input (model node).",
                "In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs.",
                "Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents.",
                "As we mentioned previously, the 0-th level models are the traditional DIDs.",
                "Their solutions provide probability distributions over actions of the agent modeled at that level to I-DIDs at level 1.",
                "Given probability distributions over other agents actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models.",
                "Assume that the number of models considered at each level is bound by a number, M. Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs. 5.",
                "EXAMPLE APPLICATIONS To illustrate the usefulness of I-DIDs, we apply them to three problem domains.",
                "We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it. 5.1 Followership-Leadership in the Multiagent Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9].",
                "The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L).",
                "In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agents opening one of the doors.",
                "When any door is opened, the tiger persists in its original location with a probability of 95%.",
                "Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%.",
                "Agent j, on the other hand, hears growls with a reliability of 95%.",
                "Thus, the setting is such that agent i hears agent j opening doors more reliably than the tigers growls.",
                "This suggests that i could use js actions as an indication of the location of the tiger, as we discuss below.",
                "Each agents preferences are as in the single agent game discussed in [13].",
                "The transition, observation, and reward functions are shown in [16].",
                "A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions.",
                "In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship.",
                "In particular, we analyze the situational and epistemological conditions sufficient for their emergence.",
                "The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the others actions in order to maximize its payoffs.",
                "Let us consider a particular setting of the tiger problem in which agent i believes that js preferences are aligned with its own - both of them just want to get the gold - and js hearing is more reliable in comparison to itself.",
                "As an example, suppose that j, on listening can discern the tigers location 95% of the times compared to is 65% accuracy.",
                "Additionally, agent i does not have any initial information about the tigers location.",
                "In other words, is single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger.",
                "In addition, i considers two models of j, which differ in js flat level 0 initial beliefs.",
                "This is represented in the level 1 I-ID shown in Fig. 6(a).",
                "According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)).",
                "Agent i is undecided on these two models of j.",
                "If we vary is hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior.",
                "If is probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow js actions: i opens the same door that j opened previously iff is own assessment of the tigers location confirms js pick.",
                "If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)).",
                "Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem.",
                "Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations.",
                "We observed that a single level of belief nesting - beliefs about the others models - was sufficient for followership to emerge in the tiger problem.",
                "However, the epistemological requirements for the emergence of leadership are more complex.",
                "For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i.",
                "As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow js actions over time.",
                "Agent j emerges as a leader if it believes that i will follow it, which implies that js belief must be nested two levels deep to enable it to recognize its leadership role.",
                "Realizing that i will follow presents j with an opportunity to influence is actions in the benefit of the collective good or its self-interest alone.",
                "For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original.",
                "If j alone selects the correct door, it gets the payoff of 10.",
                "On the other hand, if both agents pick the wrong door, their penalties are cut in half.",
                "In this setting, it is in both js best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader.",
                "However, consider a slightly different problem in which j gains from is loss and is penalized if i gains.",
                "Specifically, let is payoff be subtracted from js, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then is loss of 100 becomes js gain.",
                "Agent j believes that i incorrectly thinks that js preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is.",
                "Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following js actions.",
                "We show is normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a).",
                "The policy demonstrates that i will blindly follow js actions.",
                "Since the tiger persists in its original location with a probability of 0.95, i will select the same door again.",
                "If j begins the game with a 99% probability that the tiger is on the right, solving js I-DID nested two levels deep, results in the policy shown in Fig. 8(b).",
                "Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL.",
                "Agent js intention is to deceive i who, it believes, will follow js actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest.",
                "Figure 8: Emergence of deception between agents in the tiger problem.",
                "Behaviors of interest are in bold. * denotes as before. (a) Agent is policy demonstrating that it will blindly follow js actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i. 5.2 Altruism and Reciprocity in the Public Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves.",
                "Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot.",
                "However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes.",
                "Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others contributions.",
                "However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions.",
                "The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4].",
                "Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting.",
                "These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others.",
                "For simplicity, we assume that the game is played between M = 2 agents, i and j.",
                "Let each agent be initially endowed with XT amount of resources.",
                "While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions.",
                "Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute.",
                "The latter action is deThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D).",
                "We assume that the actions are not observable to others.",
                "The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return.",
                "We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain.",
                "Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment.",
                "In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment.",
                "Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent.",
                "For simplicity, we assume that the cost of punishing is same for both the agents.",
                "The one-shot PG game with punishment is shown in Table. 1.",
                "Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action.",
                "If P < XT − ciXT , then defection is the dominating action for both.",
                "If P = XT − ciXT , then the game is not dominance-solvable.",
                "Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a).",
                "We formulate a sequential version of the PG problem with punishment from the perspective of agent i.",
                "Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents.",
                "Each agent may contribute a fixed amount, xc, or defect.",
                "An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot.",
                "Notice that the observations are also indirectly indicative of agent js actions because the state of the public pot is influenced by them.",
                "The amount of resources in agent is private pot, is perfectly observable to i.",
                "The payoffs are analogous to Table. 1.",
                "Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)).",
                "Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect.",
                "Let xc = 1 and the level 0 agent be punished half the times it defects.",
                "With one action remaining, both types of agents choose to contribute to avoid being punished.",
                "With two actions to go, the altruistic type chooses to contribute, while the other defects.",
                "This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids.",
                "Because cj for the non-altruistic type is less, it prefers not to contribute.",
                "With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects.",
                "For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection.",
                "We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level 0 models, mentioned previously, to j (see Fig. 9).",
                "If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps.",
                "This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type.",
                "However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1.",
                "These results demonstrate that the behavior of our altruistic type resembles that found experimentally.",
                "The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic.",
                "We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection.",
                "The reciprocal types marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other.",
                "We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full.",
                "For this prior belief, i chooses to defect.",
                "On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)).",
                "This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from js action to contribute.",
                "Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step.",
                "Agent i therefore chooses to contribute to reciprocate js action.",
                "An analogous reasoning leads i to defect when it observes a meager pot.",
                "With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations.",
                "Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic). 5.3 Strategies in Two-Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2].",
                "Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck.",
                "While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands.",
                "To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit.",
                "Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot.",
                "During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to 8.",
                "Notice that, for example, if a lower numbered card is discarded, the probability of receiving a low card in exchange is now reduced.",
                "We show the level 1 I-ID for the simplified two-player Poker in Fig. 11.",
                "We considered two models (personality types) of agent j.",
                "The conservative type believes that it is likely that its opponent has a high numbered card in its hand.",
                "On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card.",
                "Thus, the two types differ in their beliefs over their opponents hand.",
                "In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution.",
                "With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand.",
                "This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange.",
                "The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one.",
                "Figure 11: (a) Level 1 I-ID of agent i.",
                "The observation reveals information about js hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a).",
                "The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in js hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. is own hand contains the card numbered 8.",
                "The agent starts by keeping its card.",
                "On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card.",
                "If i observes that j discarded its card into the L or H pile, i believes that j is aggressive.",
                "On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange.",
                "Because the probability of receiving a low card is high now, i chooses to keep its card.",
                "On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card.",
                "In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff.",
                "This is partly due to the fact that an observation of, say, L resets the agent is previous time step beliefs over js hand to the low numbered cards only. 6.",
                "DISCUSSION We showed how DIDs may be extended to I-DIDs that enable online sequential decision-making in uncertain multiagent settings.",
                "Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent is three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs.",
                "I-DIDs extend NIDs to allow sequential decision-making over multiple time steps in the presence of other interacting agents.",
                "I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs.",
                "We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality.",
                "Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work.",
                "The first author would like to acknowledge the support of a UGARF grant. 7.",
                "REFERENCES [1] R. J. Aumann.",
                "Interactive epistemology i: Knowledge.",
                "International Journal of Game Theory, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron.",
                "The challenge of poker.",
                "AIJ, 2001. [3] A. Brandenburger and E. Dekel.",
                "Hierarchies of beliefs and common knowledge.",
                "Journal of Economic Theory, 59:189-198, 1993. [4] C. Camerer.",
                "Behavioral Game Theory: Experiments in Strategic Interaction.",
                "Princeton University Press, 2003. [5] E. Fehr and S. Gachter.",
                "Cooperation and punishment in public goods experiments.",
                "American Economic Review, 90(4):980-994, 2000. [6] D. Fudenberg and D. K. Levine.",
                "The Theory of Learning in Games.",
                "MIT Press, 1998. [7] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [8] Y. Gal and A. Pfeffer.",
                "A language for modeling agents decision-making processes in games.",
                "In AAMAS, 2003. [9] P. Gmytrasiewicz and P. Doshi.",
                "A framework for sequential planning in multiagent settings.",
                "JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz and E. Durfee.",
                "Rational coordination in multi-agent environments.",
                "JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi.",
                "Games with incomplete information played by bayesian players.",
                "Management Science, 14(3):159-182, 1967. [12] R. A. Howard and J. E. Matheson.",
                "Influence diagrams.",
                "In R. A. Howard and J. E. Matheson, editors, The Principles and Applications of Decision Analysis.",
                "Strategic Decisions Group, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman, and A. Cassandra.",
                "Planning and acting in partially observable stochastic domains.",
                "Artificial Intelligence Journal, 2, 1998. [14] D. Koller and B. Milch.",
                "Multi-agent influence diagrams for representing and solving games.",
                "In IJCAI, pages 1027-1034, 2001. [15] K. Polich and P. Gmytrasiewicz.",
                "Interactive dynamic influence diagrams.",
                "In GTDT Workshop, AAMAS, 2006. [16] B.",
                "Rathnas., P. Doshi, and P. J. Gmytrasiewicz.",
                "Exact solutions to interactive pomdps using behavioral equivalence.",
                "In Autonomous Agents and Multi-Agent Systems Conference (AAMAS), 2006. [17] S. Russell and P. Norvig.",
                "Artificial Intelligence: A Modern Approach (Second Edition).",
                "Prentice Hall, 2003. [18] R. D. Shachter.",
                "Evaluating influence diagrams.",
                "Operations Research, 34(6):871-882, 1986. [19] D. Suryadi and P. Gmytrasiewicz.",
                "Learning models of other agents using influence diagrams.",
                "In UM, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "online sequential decision-making": {
            "translated_key": "toma de decisiones secuenciales en línea",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Graphical Models for Online Solutions to Interactive POMDPs Prashant Doshi Dept.",
                "of Computer Science University of Georgia Athens, GA 30602, USA pdoshi@cs.uga.edu Yifeng Zeng Dept. of Computer Science Aalborg University DK-9220 Aalborg, Denmark yfzeng@cs.aau.edu Qiongyu Chen Dept. of Computer Science National Univ. of Singapore 117543, Singapore chenqy@comp.nus.edu.sg ABSTRACT We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation.",
                "These graphical models called interactive dynamic influence diagrams (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents.",
                "Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Theory 1.",
                "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in partially observable multiagent environments.",
                "They generalize POMDPs [13] to multiagent settings by including the other agents computable models in the state space along with the states of the physical environment.",
                "The models encompass all information influencing the agents behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11].",
                "I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a decision-makers perspective in the interaction.",
                "In [15], Polich and Gmytrasiewicz introduced interactive dynamic influence diagrams (I-DIDs) as the computational representations of I-POMDPs.",
                "I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs contribute to a growing line of work [19] that includes multi-agent influence diagrams (MAIDs) [14], and more recently, networks of influence diagrams (NIDs) [8].",
                "These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agents actions and chance nodes capturing the agents private information.",
                "MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the independence structure.",
                "NIDs extend MAIDs to include agents uncertainty over the game being played and over models of the other agents.",
                "Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent.",
                "Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently.",
                "However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games.",
                "Matters are more complex when we consider interactions that are extended over time, where predictions about others future actions must be made using models that change as the agents act and observe.",
                "I-DIDs address this gap by allowing the representation of other agents models as the values of a special model node.",
                "Both, other agents models and the original agents beliefs over these models are updated over time using special-purpose implementations.",
                "In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID.",
                "Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently.",
                "Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I-DID by [15], and show that it could be replaced by traditional dependency links.",
                "In the previous representation of the I-DID, the update of the agents belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time.",
                "We explicate the semantics of this link by showing how it can be implemented using the traditional dependency links between the chance nodes that constitute the model nodes.",
                "The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs.",
                "We show how IDIDs may be used to model an agents uncertainty over others models, that may themselves be I-DIDs.",
                "Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others models.",
                "Analogous to DIDs, I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. 2.",
                "BACKGROUND: FINITELY NESTED IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents models as part of the state space [9].",
                "Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents models and their beliefs about others.",
                "For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j.",
                "A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment.",
                "Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj .",
                "Here, j is Bayes rational and OCj is js optimality criterion.",
                "SMj is the set of subintentional models of j.",
                "Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent.",
                "We give a recursive bottom-up construction of the interactive state space below.",
                "ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . .",
                "ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent is preferences over its interactive states.",
                "Usually only the physical states will matter.",
                "Agent is policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i.",
                "Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai). 2.1 Belief Update Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes.",
                "However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones.",
                "First, since the state of the physical environment depends on the actions of both agents, is prediction of how the physical state changes has to be made based on its prediction of js actions.",
                "Second, changes in js models have to be included in is belief update.",
                "Specifically, if j is intentional then an update of js beliefs due to its action and observation has to be included.",
                "In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief.",
                "If js model is subintentional, then js probable observations are appended to the observation history contained in the model.",
                "Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update.",
                "For a version of the belief update when js model is subintentional, see [9].",
                "If agent j is also modeled as an I-POMDP, then is belief update invokes js belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke is belief update and so on.",
                "This recursion in belief nesting bottoms out at the 0th level.",
                "At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9]. 2.2 Value Iteration Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)).",
                "Eq. 2 is a basis for value iteration in I-POMDPs.",
                "Agent is optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3.",
                "INTERACTIVEINFLUENCEDIAGRAMS A naive extension of influence diagrams (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes.",
                "However, this approach assumes that the agents actions are controlled using a probability distribution that does not change over time.",
                "Interactive influence diagrams (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node.",
                "We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon.",
                "We note that the probability distribution over the chance node, S, and the model node together represents agent is belief over its interactive states.",
                "In addition to the model 1 The 0th level model is a POMDP: Other agents actions are treated as exogenous events and folded into the T, O, and R functions.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j.",
                "The hexagon is the model node (Mj,l−1) whose structure we show in (b).",
                "Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ).",
                "Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the policy link in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model.",
                "In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs.",
                "The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2.",
                "Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional.",
                "Because the model node contains the alternative models of the other agent as its values, its representation is not trivial.",
                "In particular, some of the models within the node are I-IDs that when solved generate the agents optimal policy in their decision nodes.",
                "Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise.",
                "Borrowing insights from previous work [8], we observe that the model node and the dashed policy link that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b).",
                "The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj.",
                "Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent is beliefs.",
                "The conditional probability table of the chance node, Aj, is a multiplexer that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj].",
                "The values of Mod[Mj] denote the different models of j.",
                "In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1.",
                "The distribution over the node, Mod[Mj], is the agent is belief over the models of j given a physical state.",
                "For more agents, we will have as many model nodes as there are agents.",
                "Notice that Fig. 1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional dependency links.",
                "In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them.",
                "In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes.",
                "This allows I-IDs to be represented and implemented using conventional application tools that target IDs.",
                "Note that we may view the level l I-ID as a NID.",
                "Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2).",
                "If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID.",
                "Note that within the I-IDs (or IDs) at each level, there is only a single decision node.",
                "Thus, our NID does not contain any MAIDs.",
                "Figure 2: A level l I-ID represented as a NID.",
                "The probabilities assigned to the blocks of the NID are is beliefs over js models conditioned on a physical state. 3.2 Solution The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively.",
                "We start by solving the level 0 models, which, if intentional, are traditional IDs.",
                "Their solutions provide probability distributions over the other agents actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID.",
                "The mapping from the level 0 models decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c).",
                "During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj].",
                "As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent is belief over the models of j conditioned on the physical state.",
                "The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j.",
                "Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18].",
                "This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief.",
                "Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agents current belief is known. 4.",
                "INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic influence diagrams (I-DIDs) extend I-IDs (and NIDs) to allow sequential decision-making over several time steps.",
                "Just as DIDs are structured graphical representations of POMDPs, I-DIDs are the graphical online analogs for finitely nested I-POMDPs.",
                "I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents. 4.1 Syntax We depict a general two time-slice I-DID in Fig. 3(a).",
                "In addition to the model nodes and the dashed policy link, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a).",
                "We explained the semantics of the model node and the policy link in the previous section; we describe the model updates next.",
                "The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1.",
                "Recall from Section 2 that an agents intentional model includes its belief.",
                "Because the agents act and receive observations, their models are updated to reflect their changed beliefs.",
                "Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models.",
                "Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models.",
                "Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model.",
                "These steps are a part of agent is belief update formalized using Eq. 1.",
                "In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID.",
                "If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ).",
                "These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation.",
                "The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously.",
                "Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed.",
                "The probability that js updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node.",
                "In order to obtain the probability of js possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ].",
                "Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a multiplexer modulated by Mod[Mt j ].",
                "Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1.",
                "Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1.",
                "Notice that the model update link may be replaced by the dependency links between the chance nodes that constitute the model nodes in the two time slices.",
                "In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them.",
                "Chance nodes and dependency links that not in bold are standard, usually found in DIDs.",
                "Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents js observation at time t + 1.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links.",
                "We note that the possible set of models of the other agent j grows exponentially with the number of time steps.",
                "For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node. 4.2 Solution Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively.",
                "For the purpose of illustration, let l=1 and T=2.",
                "The solution method uses the standard look-ahead technique, projecting the agents action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step.",
                "Because agent i has a belief over js models as well, the lookahead includes finding out the possible models that j could have in the future.",
                "Consequently, each of js subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions.",
                "These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b).",
                "We note the recursive nature of this solution: in solving agent is level 1 I-DID, js level 0 DIDs must be solved.",
                "If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner.",
                "We briefly outline the recursive algorithm for solving agent is Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase 1.",
                "For t from 1 to T − 1 do 2.",
                "If l ≥ 1 then Populate Mt+1 j,l−1 3.",
                "For each mt j in Range(Mt j,l−1) do 4.",
                "Recursively call algorithm with the l − 1 I-ID (or ID) that represents mt j and the horizon, T − t + 1 5.",
                "Map the decision node of the solved I-ID (or ID), OPT(mt j), to a chance node Aj 6.",
                "For each aj in OPT(mt j) do 7.",
                "For each oj in Oj (part of mt j) do 8.",
                "Update js belief, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← New I-ID (or ID) with bt+1 j as the initial belief 10.",
                "Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11.",
                "Add the model node, Mt+1 j,l−1, and the dependency links between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b)) 12.",
                "Add the chance, decision, and utility nodes for t + 1 time slice and the dependency links between them 13.",
                "Establish the CPTs for each chance node and utility node Look-Ahead Phase 14.",
                "Apply the standard look-ahead and backup method to solve the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5.",
                "We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the dependency links and the conditional probability tables for each node.",
                "We particularly focus on establishing and populating the model nodes (lines 3-11).",
                "Note that Range(·) returns the values (lower level models) of the random variable given as input (model node).",
                "In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs.",
                "Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents.",
                "As we mentioned previously, the 0-th level models are the traditional DIDs.",
                "Their solutions provide probability distributions over actions of the agent modeled at that level to I-DIDs at level 1.",
                "Given probability distributions over other agents actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models.",
                "Assume that the number of models considered at each level is bound by a number, M. Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs. 5.",
                "EXAMPLE APPLICATIONS To illustrate the usefulness of I-DIDs, we apply them to three problem domains.",
                "We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it. 5.1 Followership-Leadership in the Multiagent Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9].",
                "The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L).",
                "In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agents opening one of the doors.",
                "When any door is opened, the tiger persists in its original location with a probability of 95%.",
                "Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%.",
                "Agent j, on the other hand, hears growls with a reliability of 95%.",
                "Thus, the setting is such that agent i hears agent j opening doors more reliably than the tigers growls.",
                "This suggests that i could use js actions as an indication of the location of the tiger, as we discuss below.",
                "Each agents preferences are as in the single agent game discussed in [13].",
                "The transition, observation, and reward functions are shown in [16].",
                "A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions.",
                "In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship.",
                "In particular, we analyze the situational and epistemological conditions sufficient for their emergence.",
                "The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the others actions in order to maximize its payoffs.",
                "Let us consider a particular setting of the tiger problem in which agent i believes that js preferences are aligned with its own - both of them just want to get the gold - and js hearing is more reliable in comparison to itself.",
                "As an example, suppose that j, on listening can discern the tigers location 95% of the times compared to is 65% accuracy.",
                "Additionally, agent i does not have any initial information about the tigers location.",
                "In other words, is single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger.",
                "In addition, i considers two models of j, which differ in js flat level 0 initial beliefs.",
                "This is represented in the level 1 I-ID shown in Fig. 6(a).",
                "According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)).",
                "Agent i is undecided on these two models of j.",
                "If we vary is hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior.",
                "If is probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow js actions: i opens the same door that j opened previously iff is own assessment of the tigers location confirms js pick.",
                "If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)).",
                "Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem.",
                "Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations.",
                "We observed that a single level of belief nesting - beliefs about the others models - was sufficient for followership to emerge in the tiger problem.",
                "However, the epistemological requirements for the emergence of leadership are more complex.",
                "For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i.",
                "As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow js actions over time.",
                "Agent j emerges as a leader if it believes that i will follow it, which implies that js belief must be nested two levels deep to enable it to recognize its leadership role.",
                "Realizing that i will follow presents j with an opportunity to influence is actions in the benefit of the collective good or its self-interest alone.",
                "For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original.",
                "If j alone selects the correct door, it gets the payoff of 10.",
                "On the other hand, if both agents pick the wrong door, their penalties are cut in half.",
                "In this setting, it is in both js best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader.",
                "However, consider a slightly different problem in which j gains from is loss and is penalized if i gains.",
                "Specifically, let is payoff be subtracted from js, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then is loss of 100 becomes js gain.",
                "Agent j believes that i incorrectly thinks that js preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is.",
                "Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following js actions.",
                "We show is normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a).",
                "The policy demonstrates that i will blindly follow js actions.",
                "Since the tiger persists in its original location with a probability of 0.95, i will select the same door again.",
                "If j begins the game with a 99% probability that the tiger is on the right, solving js I-DID nested two levels deep, results in the policy shown in Fig. 8(b).",
                "Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL.",
                "Agent js intention is to deceive i who, it believes, will follow js actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest.",
                "Figure 8: Emergence of deception between agents in the tiger problem.",
                "Behaviors of interest are in bold. * denotes as before. (a) Agent is policy demonstrating that it will blindly follow js actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i. 5.2 Altruism and Reciprocity in the Public Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves.",
                "Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot.",
                "However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes.",
                "Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others contributions.",
                "However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions.",
                "The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4].",
                "Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting.",
                "These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others.",
                "For simplicity, we assume that the game is played between M = 2 agents, i and j.",
                "Let each agent be initially endowed with XT amount of resources.",
                "While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions.",
                "Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute.",
                "The latter action is deThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D).",
                "We assume that the actions are not observable to others.",
                "The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return.",
                "We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain.",
                "Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment.",
                "In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment.",
                "Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent.",
                "For simplicity, we assume that the cost of punishing is same for both the agents.",
                "The one-shot PG game with punishment is shown in Table. 1.",
                "Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action.",
                "If P < XT − ciXT , then defection is the dominating action for both.",
                "If P = XT − ciXT , then the game is not dominance-solvable.",
                "Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a).",
                "We formulate a sequential version of the PG problem with punishment from the perspective of agent i.",
                "Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents.",
                "Each agent may contribute a fixed amount, xc, or defect.",
                "An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot.",
                "Notice that the observations are also indirectly indicative of agent js actions because the state of the public pot is influenced by them.",
                "The amount of resources in agent is private pot, is perfectly observable to i.",
                "The payoffs are analogous to Table. 1.",
                "Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)).",
                "Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect.",
                "Let xc = 1 and the level 0 agent be punished half the times it defects.",
                "With one action remaining, both types of agents choose to contribute to avoid being punished.",
                "With two actions to go, the altruistic type chooses to contribute, while the other defects.",
                "This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids.",
                "Because cj for the non-altruistic type is less, it prefers not to contribute.",
                "With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects.",
                "For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection.",
                "We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level 0 models, mentioned previously, to j (see Fig. 9).",
                "If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps.",
                "This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type.",
                "However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1.",
                "These results demonstrate that the behavior of our altruistic type resembles that found experimentally.",
                "The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic.",
                "We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection.",
                "The reciprocal types marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other.",
                "We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full.",
                "For this prior belief, i chooses to defect.",
                "On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)).",
                "This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from js action to contribute.",
                "Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step.",
                "Agent i therefore chooses to contribute to reciprocate js action.",
                "An analogous reasoning leads i to defect when it observes a meager pot.",
                "With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations.",
                "Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic). 5.3 Strategies in Two-Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2].",
                "Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck.",
                "While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands.",
                "To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit.",
                "Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot.",
                "During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to 8.",
                "Notice that, for example, if a lower numbered card is discarded, the probability of receiving a low card in exchange is now reduced.",
                "We show the level 1 I-ID for the simplified two-player Poker in Fig. 11.",
                "We considered two models (personality types) of agent j.",
                "The conservative type believes that it is likely that its opponent has a high numbered card in its hand.",
                "On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card.",
                "Thus, the two types differ in their beliefs over their opponents hand.",
                "In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution.",
                "With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand.",
                "This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange.",
                "The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one.",
                "Figure 11: (a) Level 1 I-ID of agent i.",
                "The observation reveals information about js hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a).",
                "The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in js hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. is own hand contains the card numbered 8.",
                "The agent starts by keeping its card.",
                "On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card.",
                "If i observes that j discarded its card into the L or H pile, i believes that j is aggressive.",
                "On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange.",
                "Because the probability of receiving a low card is high now, i chooses to keep its card.",
                "On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card.",
                "In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff.",
                "This is partly due to the fact that an observation of, say, L resets the agent is previous time step beliefs over js hand to the low numbered cards only. 6.",
                "DISCUSSION We showed how DIDs may be extended to I-DIDs that enable <br>online sequential decision-making</br> in uncertain multiagent settings.",
                "Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent is three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs.",
                "I-DIDs extend NIDs to allow sequential decision-making over multiple time steps in the presence of other interacting agents.",
                "I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs.",
                "We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality.",
                "Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work.",
                "The first author would like to acknowledge the support of a UGARF grant. 7.",
                "REFERENCES [1] R. J. Aumann.",
                "Interactive epistemology i: Knowledge.",
                "International Journal of Game Theory, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron.",
                "The challenge of poker.",
                "AIJ, 2001. [3] A. Brandenburger and E. Dekel.",
                "Hierarchies of beliefs and common knowledge.",
                "Journal of Economic Theory, 59:189-198, 1993. [4] C. Camerer.",
                "Behavioral Game Theory: Experiments in Strategic Interaction.",
                "Princeton University Press, 2003. [5] E. Fehr and S. Gachter.",
                "Cooperation and punishment in public goods experiments.",
                "American Economic Review, 90(4):980-994, 2000. [6] D. Fudenberg and D. K. Levine.",
                "The Theory of Learning in Games.",
                "MIT Press, 1998. [7] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [8] Y. Gal and A. Pfeffer.",
                "A language for modeling agents decision-making processes in games.",
                "In AAMAS, 2003. [9] P. Gmytrasiewicz and P. Doshi.",
                "A framework for sequential planning in multiagent settings.",
                "JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz and E. Durfee.",
                "Rational coordination in multi-agent environments.",
                "JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi.",
                "Games with incomplete information played by bayesian players.",
                "Management Science, 14(3):159-182, 1967. [12] R. A. Howard and J. E. Matheson.",
                "Influence diagrams.",
                "In R. A. Howard and J. E. Matheson, editors, The Principles and Applications of Decision Analysis.",
                "Strategic Decisions Group, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman, and A. Cassandra.",
                "Planning and acting in partially observable stochastic domains.",
                "Artificial Intelligence Journal, 2, 1998. [14] D. Koller and B. Milch.",
                "Multi-agent influence diagrams for representing and solving games.",
                "In IJCAI, pages 1027-1034, 2001. [15] K. Polich and P. Gmytrasiewicz.",
                "Interactive dynamic influence diagrams.",
                "In GTDT Workshop, AAMAS, 2006. [16] B.",
                "Rathnas., P. Doshi, and P. J. Gmytrasiewicz.",
                "Exact solutions to interactive pomdps using behavioral equivalence.",
                "In Autonomous Agents and Multi-Agent Systems Conference (AAMAS), 2006. [17] S. Russell and P. Norvig.",
                "Artificial Intelligence: A Modern Approach (Second Edition).",
                "Prentice Hall, 2003. [18] R. D. Shachter.",
                "Evaluating influence diagrams.",
                "Operations Research, 34(6):871-882, 1986. [19] D. Suryadi and P. Gmytrasiewicz.",
                "Learning models of other agents using influence diagrams.",
                "In UM, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Discusión Mostramos cómo los DIDS pueden extenderse a I-DID que permiten la \"toma de decisiones secuenciales en línea\" en entornos múltiples inciertos."
            ],
            "translated_text": "",
            "candidates": [
                "toma de decisiones secuenciales en línea",
                "toma de decisiones secuenciales en línea"
            ],
            "error": []
        },
        "dynamic inﬂuence diagram": {
            "translated_key": "Diagrama de influencia dinámica",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Graphical Models for Online Solutions to Interactive POMDPs Prashant Doshi Dept.",
                "of Computer Science University of Georgia Athens, GA 30602, USA pdoshi@cs.uga.edu Yifeng Zeng Dept. of Computer Science Aalborg University DK-9220 Aalborg, Denmark yfzeng@cs.aau.edu Qiongyu Chen Dept. of Computer Science National Univ. of Singapore 117543, Singapore chenqy@comp.nus.edu.sg ABSTRACT We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation.",
                "These graphical models called interactive dynamic influence diagrams (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents.",
                "Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Theory 1.",
                "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in partially observable multiagent environments.",
                "They generalize POMDPs [13] to multiagent settings by including the other agents computable models in the state space along with the states of the physical environment.",
                "The models encompass all information influencing the agents behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11].",
                "I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a decision-makers perspective in the interaction.",
                "In [15], Polich and Gmytrasiewicz introduced interactive dynamic influence diagrams (I-DIDs) as the computational representations of I-POMDPs.",
                "I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs contribute to a growing line of work [19] that includes multi-agent influence diagrams (MAIDs) [14], and more recently, networks of influence diagrams (NIDs) [8].",
                "These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agents actions and chance nodes capturing the agents private information.",
                "MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the independence structure.",
                "NIDs extend MAIDs to include agents uncertainty over the game being played and over models of the other agents.",
                "Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent.",
                "Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently.",
                "However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games.",
                "Matters are more complex when we consider interactions that are extended over time, where predictions about others future actions must be made using models that change as the agents act and observe.",
                "I-DIDs address this gap by allowing the representation of other agents models as the values of a special model node.",
                "Both, other agents models and the original agents beliefs over these models are updated over time using special-purpose implementations.",
                "In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID.",
                "Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently.",
                "Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I-DID by [15], and show that it could be replaced by traditional dependency links.",
                "In the previous representation of the I-DID, the update of the agents belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time.",
                "We explicate the semantics of this link by showing how it can be implemented using the traditional dependency links between the chance nodes that constitute the model nodes.",
                "The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs.",
                "We show how IDIDs may be used to model an agents uncertainty over others models, that may themselves be I-DIDs.",
                "Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others models.",
                "Analogous to DIDs, I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. 2.",
                "BACKGROUND: FINITELY NESTED IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents models as part of the state space [9].",
                "Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents models and their beliefs about others.",
                "For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j.",
                "A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment.",
                "Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj .",
                "Here, j is Bayes rational and OCj is js optimality criterion.",
                "SMj is the set of subintentional models of j.",
                "Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent.",
                "We give a recursive bottom-up construction of the interactive state space below.",
                "ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . .",
                "ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent is preferences over its interactive states.",
                "Usually only the physical states will matter.",
                "Agent is policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i.",
                "Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai). 2.1 Belief Update Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes.",
                "However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones.",
                "First, since the state of the physical environment depends on the actions of both agents, is prediction of how the physical state changes has to be made based on its prediction of js actions.",
                "Second, changes in js models have to be included in is belief update.",
                "Specifically, if j is intentional then an update of js beliefs due to its action and observation has to be included.",
                "In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief.",
                "If js model is subintentional, then js probable observations are appended to the observation history contained in the model.",
                "Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update.",
                "For a version of the belief update when js model is subintentional, see [9].",
                "If agent j is also modeled as an I-POMDP, then is belief update invokes js belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke is belief update and so on.",
                "This recursion in belief nesting bottoms out at the 0th level.",
                "At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9]. 2.2 Value Iteration Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)).",
                "Eq. 2 is a basis for value iteration in I-POMDPs.",
                "Agent is optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3.",
                "INTERACTIVEINFLUENCEDIAGRAMS A naive extension of influence diagrams (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes.",
                "However, this approach assumes that the agents actions are controlled using a probability distribution that does not change over time.",
                "Interactive influence diagrams (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node.",
                "We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon.",
                "We note that the probability distribution over the chance node, S, and the model node together represents agent is belief over its interactive states.",
                "In addition to the model 1 The 0th level model is a POMDP: Other agents actions are treated as exogenous events and folded into the T, O, and R functions.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j.",
                "The hexagon is the model node (Mj,l−1) whose structure we show in (b).",
                "Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ).",
                "Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the policy link in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model.",
                "In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs.",
                "The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2.",
                "Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional.",
                "Because the model node contains the alternative models of the other agent as its values, its representation is not trivial.",
                "In particular, some of the models within the node are I-IDs that when solved generate the agents optimal policy in their decision nodes.",
                "Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise.",
                "Borrowing insights from previous work [8], we observe that the model node and the dashed policy link that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b).",
                "The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj.",
                "Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent is beliefs.",
                "The conditional probability table of the chance node, Aj, is a multiplexer that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj].",
                "The values of Mod[Mj] denote the different models of j.",
                "In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1.",
                "The distribution over the node, Mod[Mj], is the agent is belief over the models of j given a physical state.",
                "For more agents, we will have as many model nodes as there are agents.",
                "Notice that Fig. 1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional dependency links.",
                "In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them.",
                "In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes.",
                "This allows I-IDs to be represented and implemented using conventional application tools that target IDs.",
                "Note that we may view the level l I-ID as a NID.",
                "Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2).",
                "If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID.",
                "Note that within the I-IDs (or IDs) at each level, there is only a single decision node.",
                "Thus, our NID does not contain any MAIDs.",
                "Figure 2: A level l I-ID represented as a NID.",
                "The probabilities assigned to the blocks of the NID are is beliefs over js models conditioned on a physical state. 3.2 Solution The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively.",
                "We start by solving the level 0 models, which, if intentional, are traditional IDs.",
                "Their solutions provide probability distributions over the other agents actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID.",
                "The mapping from the level 0 models decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c).",
                "During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj].",
                "As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent is belief over the models of j conditioned on the physical state.",
                "The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j.",
                "Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18].",
                "This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief.",
                "Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agents current belief is known. 4.",
                "INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic influence diagrams (I-DIDs) extend I-IDs (and NIDs) to allow sequential decision-making over several time steps.",
                "Just as DIDs are structured graphical representations of POMDPs, I-DIDs are the graphical online analogs for finitely nested I-POMDPs.",
                "I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents. 4.1 Syntax We depict a general two time-slice I-DID in Fig. 3(a).",
                "In addition to the model nodes and the dashed policy link, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a).",
                "We explained the semantics of the model node and the policy link in the previous section; we describe the model updates next.",
                "The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1.",
                "Recall from Section 2 that an agents intentional model includes its belief.",
                "Because the agents act and receive observations, their models are updated to reflect their changed beliefs.",
                "Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models.",
                "Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models.",
                "Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model.",
                "These steps are a part of agent is belief update formalized using Eq. 1.",
                "In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID.",
                "If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ).",
                "These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation.",
                "The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously.",
                "Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed.",
                "The probability that js updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node.",
                "In order to obtain the probability of js possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ].",
                "Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a multiplexer modulated by Mod[Mt j ].",
                "Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1.",
                "Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1.",
                "Notice that the model update link may be replaced by the dependency links between the chance nodes that constitute the model nodes in the two time slices.",
                "In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them.",
                "Chance nodes and dependency links that not in bold are standard, usually found in DIDs.",
                "Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents js observation at time t + 1.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links.",
                "We note that the possible set of models of the other agent j grows exponentially with the number of time steps.",
                "For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node. 4.2 Solution Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively.",
                "For the purpose of illustration, let l=1 and T=2.",
                "The solution method uses the standard look-ahead technique, projecting the agents action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step.",
                "Because agent i has a belief over js models as well, the lookahead includes finding out the possible models that j could have in the future.",
                "Consequently, each of js subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions.",
                "These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b).",
                "We note the recursive nature of this solution: in solving agent is level 1 I-DID, js level 0 DIDs must be solved.",
                "If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner.",
                "We briefly outline the recursive algorithm for solving agent is Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase 1.",
                "For t from 1 to T − 1 do 2.",
                "If l ≥ 1 then Populate Mt+1 j,l−1 3.",
                "For each mt j in Range(Mt j,l−1) do 4.",
                "Recursively call algorithm with the l − 1 I-ID (or ID) that represents mt j and the horizon, T − t + 1 5.",
                "Map the decision node of the solved I-ID (or ID), OPT(mt j), to a chance node Aj 6.",
                "For each aj in OPT(mt j) do 7.",
                "For each oj in Oj (part of mt j) do 8.",
                "Update js belief, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← New I-ID (or ID) with bt+1 j as the initial belief 10.",
                "Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11.",
                "Add the model node, Mt+1 j,l−1, and the dependency links between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b)) 12.",
                "Add the chance, decision, and utility nodes for t + 1 time slice and the dependency links between them 13.",
                "Establish the CPTs for each chance node and utility node Look-Ahead Phase 14.",
                "Apply the standard look-ahead and backup method to solve the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5.",
                "We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the dependency links and the conditional probability tables for each node.",
                "We particularly focus on establishing and populating the model nodes (lines 3-11).",
                "Note that Range(·) returns the values (lower level models) of the random variable given as input (model node).",
                "In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs.",
                "Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents.",
                "As we mentioned previously, the 0-th level models are the traditional DIDs.",
                "Their solutions provide probability distributions over actions of the agent modeled at that level to I-DIDs at level 1.",
                "Given probability distributions over other agents actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models.",
                "Assume that the number of models considered at each level is bound by a number, M. Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs. 5.",
                "EXAMPLE APPLICATIONS To illustrate the usefulness of I-DIDs, we apply them to three problem domains.",
                "We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it. 5.1 Followership-Leadership in the Multiagent Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9].",
                "The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L).",
                "In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agents opening one of the doors.",
                "When any door is opened, the tiger persists in its original location with a probability of 95%.",
                "Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%.",
                "Agent j, on the other hand, hears growls with a reliability of 95%.",
                "Thus, the setting is such that agent i hears agent j opening doors more reliably than the tigers growls.",
                "This suggests that i could use js actions as an indication of the location of the tiger, as we discuss below.",
                "Each agents preferences are as in the single agent game discussed in [13].",
                "The transition, observation, and reward functions are shown in [16].",
                "A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions.",
                "In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship.",
                "In particular, we analyze the situational and epistemological conditions sufficient for their emergence.",
                "The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the others actions in order to maximize its payoffs.",
                "Let us consider a particular setting of the tiger problem in which agent i believes that js preferences are aligned with its own - both of them just want to get the gold - and js hearing is more reliable in comparison to itself.",
                "As an example, suppose that j, on listening can discern the tigers location 95% of the times compared to is 65% accuracy.",
                "Additionally, agent i does not have any initial information about the tigers location.",
                "In other words, is single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger.",
                "In addition, i considers two models of j, which differ in js flat level 0 initial beliefs.",
                "This is represented in the level 1 I-ID shown in Fig. 6(a).",
                "According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)).",
                "Agent i is undecided on these two models of j.",
                "If we vary is hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior.",
                "If is probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow js actions: i opens the same door that j opened previously iff is own assessment of the tigers location confirms js pick.",
                "If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)).",
                "Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem.",
                "Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations.",
                "We observed that a single level of belief nesting - beliefs about the others models - was sufficient for followership to emerge in the tiger problem.",
                "However, the epistemological requirements for the emergence of leadership are more complex.",
                "For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i.",
                "As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow js actions over time.",
                "Agent j emerges as a leader if it believes that i will follow it, which implies that js belief must be nested two levels deep to enable it to recognize its leadership role.",
                "Realizing that i will follow presents j with an opportunity to influence is actions in the benefit of the collective good or its self-interest alone.",
                "For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original.",
                "If j alone selects the correct door, it gets the payoff of 10.",
                "On the other hand, if both agents pick the wrong door, their penalties are cut in half.",
                "In this setting, it is in both js best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader.",
                "However, consider a slightly different problem in which j gains from is loss and is penalized if i gains.",
                "Specifically, let is payoff be subtracted from js, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then is loss of 100 becomes js gain.",
                "Agent j believes that i incorrectly thinks that js preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is.",
                "Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following js actions.",
                "We show is normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a).",
                "The policy demonstrates that i will blindly follow js actions.",
                "Since the tiger persists in its original location with a probability of 0.95, i will select the same door again.",
                "If j begins the game with a 99% probability that the tiger is on the right, solving js I-DID nested two levels deep, results in the policy shown in Fig. 8(b).",
                "Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL.",
                "Agent js intention is to deceive i who, it believes, will follow js actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest.",
                "Figure 8: Emergence of deception between agents in the tiger problem.",
                "Behaviors of interest are in bold. * denotes as before. (a) Agent is policy demonstrating that it will blindly follow js actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i. 5.2 Altruism and Reciprocity in the Public Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves.",
                "Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot.",
                "However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes.",
                "Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others contributions.",
                "However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions.",
                "The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4].",
                "Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting.",
                "These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others.",
                "For simplicity, we assume that the game is played between M = 2 agents, i and j.",
                "Let each agent be initially endowed with XT amount of resources.",
                "While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions.",
                "Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute.",
                "The latter action is deThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D).",
                "We assume that the actions are not observable to others.",
                "The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return.",
                "We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain.",
                "Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment.",
                "In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment.",
                "Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent.",
                "For simplicity, we assume that the cost of punishing is same for both the agents.",
                "The one-shot PG game with punishment is shown in Table. 1.",
                "Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action.",
                "If P < XT − ciXT , then defection is the dominating action for both.",
                "If P = XT − ciXT , then the game is not dominance-solvable.",
                "Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a).",
                "We formulate a sequential version of the PG problem with punishment from the perspective of agent i.",
                "Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents.",
                "Each agent may contribute a fixed amount, xc, or defect.",
                "An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot.",
                "Notice that the observations are also indirectly indicative of agent js actions because the state of the public pot is influenced by them.",
                "The amount of resources in agent is private pot, is perfectly observable to i.",
                "The payoffs are analogous to Table. 1.",
                "Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)).",
                "Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect.",
                "Let xc = 1 and the level 0 agent be punished half the times it defects.",
                "With one action remaining, both types of agents choose to contribute to avoid being punished.",
                "With two actions to go, the altruistic type chooses to contribute, while the other defects.",
                "This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids.",
                "Because cj for the non-altruistic type is less, it prefers not to contribute.",
                "With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects.",
                "For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection.",
                "We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level 0 models, mentioned previously, to j (see Fig. 9).",
                "If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps.",
                "This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type.",
                "However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1.",
                "These results demonstrate that the behavior of our altruistic type resembles that found experimentally.",
                "The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic.",
                "We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection.",
                "The reciprocal types marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other.",
                "We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full.",
                "For this prior belief, i chooses to defect.",
                "On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)).",
                "This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from js action to contribute.",
                "Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step.",
                "Agent i therefore chooses to contribute to reciprocate js action.",
                "An analogous reasoning leads i to defect when it observes a meager pot.",
                "With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations.",
                "Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic). 5.3 Strategies in Two-Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2].",
                "Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck.",
                "While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands.",
                "To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit.",
                "Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot.",
                "During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to 8.",
                "Notice that, for example, if a lower numbered card is discarded, the probability of receiving a low card in exchange is now reduced.",
                "We show the level 1 I-ID for the simplified two-player Poker in Fig. 11.",
                "We considered two models (personality types) of agent j.",
                "The conservative type believes that it is likely that its opponent has a high numbered card in its hand.",
                "On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card.",
                "Thus, the two types differ in their beliefs over their opponents hand.",
                "In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution.",
                "With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand.",
                "This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange.",
                "The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one.",
                "Figure 11: (a) Level 1 I-ID of agent i.",
                "The observation reveals information about js hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a).",
                "The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in js hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. is own hand contains the card numbered 8.",
                "The agent starts by keeping its card.",
                "On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card.",
                "If i observes that j discarded its card into the L or H pile, i believes that j is aggressive.",
                "On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange.",
                "Because the probability of receiving a low card is high now, i chooses to keep its card.",
                "On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card.",
                "In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff.",
                "This is partly due to the fact that an observation of, say, L resets the agent is previous time step beliefs over js hand to the low numbered cards only. 6.",
                "DISCUSSION We showed how DIDs may be extended to I-DIDs that enable online sequential decision-making in uncertain multiagent settings.",
                "Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent is three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs.",
                "I-DIDs extend NIDs to allow sequential decision-making over multiple time steps in the presence of other interacting agents.",
                "I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs.",
                "We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality.",
                "Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work.",
                "The first author would like to acknowledge the support of a UGARF grant. 7.",
                "REFERENCES [1] R. J. Aumann.",
                "Interactive epistemology i: Knowledge.",
                "International Journal of Game Theory, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron.",
                "The challenge of poker.",
                "AIJ, 2001. [3] A. Brandenburger and E. Dekel.",
                "Hierarchies of beliefs and common knowledge.",
                "Journal of Economic Theory, 59:189-198, 1993. [4] C. Camerer.",
                "Behavioral Game Theory: Experiments in Strategic Interaction.",
                "Princeton University Press, 2003. [5] E. Fehr and S. Gachter.",
                "Cooperation and punishment in public goods experiments.",
                "American Economic Review, 90(4):980-994, 2000. [6] D. Fudenberg and D. K. Levine.",
                "The Theory of Learning in Games.",
                "MIT Press, 1998. [7] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [8] Y. Gal and A. Pfeffer.",
                "A language for modeling agents decision-making processes in games.",
                "In AAMAS, 2003. [9] P. Gmytrasiewicz and P. Doshi.",
                "A framework for sequential planning in multiagent settings.",
                "JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz and E. Durfee.",
                "Rational coordination in multi-agent environments.",
                "JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi.",
                "Games with incomplete information played by bayesian players.",
                "Management Science, 14(3):159-182, 1967. [12] R. A. Howard and J. E. Matheson.",
                "Influence diagrams.",
                "In R. A. Howard and J. E. Matheson, editors, The Principles and Applications of Decision Analysis.",
                "Strategic Decisions Group, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman, and A. Cassandra.",
                "Planning and acting in partially observable stochastic domains.",
                "Artificial Intelligence Journal, 2, 1998. [14] D. Koller and B. Milch.",
                "Multi-agent influence diagrams for representing and solving games.",
                "In IJCAI, pages 1027-1034, 2001. [15] K. Polich and P. Gmytrasiewicz.",
                "Interactive dynamic influence diagrams.",
                "In GTDT Workshop, AAMAS, 2006. [16] B.",
                "Rathnas., P. Doshi, and P. J. Gmytrasiewicz.",
                "Exact solutions to interactive pomdps using behavioral equivalence.",
                "In Autonomous Agents and Multi-Agent Systems Conference (AAMAS), 2006. [17] S. Russell and P. Norvig.",
                "Artificial Intelligence: A Modern Approach (Second Edition).",
                "Prentice Hall, 2003. [18] R. D. Shachter.",
                "Evaluating influence diagrams.",
                "Operations Research, 34(6):871-882, 1986. [19] D. Suryadi and P. Gmytrasiewicz.",
                "Learning models of other agents using influence diagrams.",
                "In UM, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "decision-make": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Graphical Models for Online Solutions to Interactive POMDPs Prashant Doshi Dept.",
                "of Computer Science University of Georgia Athens, GA 30602, USA pdoshi@cs.uga.edu Yifeng Zeng Dept. of Computer Science Aalborg University DK-9220 Aalborg, Denmark yfzeng@cs.aau.edu Qiongyu Chen Dept. of Computer Science National Univ. of Singapore 117543, Singapore chenqy@comp.nus.edu.sg ABSTRACT We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation.",
                "These graphical models called interactive dynamic influence diagrams (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents.",
                "Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Theory 1.",
                "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in partially observable multiagent environments.",
                "They generalize POMDPs [13] to multiagent settings by including the other agents computable models in the state space along with the states of the physical environment.",
                "The models encompass all information influencing the agents behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11].",
                "I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a <br>decision-make</br>rs perspective in the interaction.",
                "In [15], Polich and Gmytrasiewicz introduced interactive dynamic influence diagrams (I-DIDs) as the computational representations of I-POMDPs.",
                "I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs contribute to a growing line of work [19] that includes multi-agent influence diagrams (MAIDs) [14], and more recently, networks of influence diagrams (NIDs) [8].",
                "These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agents actions and chance nodes capturing the agents private information.",
                "MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the independence structure.",
                "NIDs extend MAIDs to include agents uncertainty over the game being played and over models of the other agents.",
                "Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent.",
                "Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently.",
                "However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games.",
                "Matters are more complex when we consider interactions that are extended over time, where predictions about others future actions must be made using models that change as the agents act and observe.",
                "I-DIDs address this gap by allowing the representation of other agents models as the values of a special model node.",
                "Both, other agents models and the original agents beliefs over these models are updated over time using special-purpose implementations.",
                "In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID.",
                "Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently.",
                "Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I-DID by [15], and show that it could be replaced by traditional dependency links.",
                "In the previous representation of the I-DID, the update of the agents belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time.",
                "We explicate the semantics of this link by showing how it can be implemented using the traditional dependency links between the chance nodes that constitute the model nodes.",
                "The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs.",
                "We show how IDIDs may be used to model an agents uncertainty over others models, that may themselves be I-DIDs.",
                "Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others models.",
                "Analogous to DIDs, I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. 2.",
                "BACKGROUND: FINITELY NESTED IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents models as part of the state space [9].",
                "Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents models and their beliefs about others.",
                "For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j.",
                "A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment.",
                "Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj .",
                "Here, j is Bayes rational and OCj is js optimality criterion.",
                "SMj is the set of subintentional models of j.",
                "Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent.",
                "We give a recursive bottom-up construction of the interactive state space below.",
                "ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . .",
                "ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent is preferences over its interactive states.",
                "Usually only the physical states will matter.",
                "Agent is policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i.",
                "Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai). 2.1 Belief Update Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes.",
                "However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones.",
                "First, since the state of the physical environment depends on the actions of both agents, is prediction of how the physical state changes has to be made based on its prediction of js actions.",
                "Second, changes in js models have to be included in is belief update.",
                "Specifically, if j is intentional then an update of js beliefs due to its action and observation has to be included.",
                "In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief.",
                "If js model is subintentional, then js probable observations are appended to the observation history contained in the model.",
                "Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update.",
                "For a version of the belief update when js model is subintentional, see [9].",
                "If agent j is also modeled as an I-POMDP, then is belief update invokes js belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke is belief update and so on.",
                "This recursion in belief nesting bottoms out at the 0th level.",
                "At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9]. 2.2 Value Iteration Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)).",
                "Eq. 2 is a basis for value iteration in I-POMDPs.",
                "Agent is optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3.",
                "INTERACTIVEINFLUENCEDIAGRAMS A naive extension of influence diagrams (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes.",
                "However, this approach assumes that the agents actions are controlled using a probability distribution that does not change over time.",
                "Interactive influence diagrams (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node.",
                "We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon.",
                "We note that the probability distribution over the chance node, S, and the model node together represents agent is belief over its interactive states.",
                "In addition to the model 1 The 0th level model is a POMDP: Other agents actions are treated as exogenous events and folded into the T, O, and R functions.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j.",
                "The hexagon is the model node (Mj,l−1) whose structure we show in (b).",
                "Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ).",
                "Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the policy link in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model.",
                "In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs.",
                "The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2.",
                "Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional.",
                "Because the model node contains the alternative models of the other agent as its values, its representation is not trivial.",
                "In particular, some of the models within the node are I-IDs that when solved generate the agents optimal policy in their decision nodes.",
                "Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise.",
                "Borrowing insights from previous work [8], we observe that the model node and the dashed policy link that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b).",
                "The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj.",
                "Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent is beliefs.",
                "The conditional probability table of the chance node, Aj, is a multiplexer that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj].",
                "The values of Mod[Mj] denote the different models of j.",
                "In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1.",
                "The distribution over the node, Mod[Mj], is the agent is belief over the models of j given a physical state.",
                "For more agents, we will have as many model nodes as there are agents.",
                "Notice that Fig. 1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional dependency links.",
                "In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them.",
                "In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes.",
                "This allows I-IDs to be represented and implemented using conventional application tools that target IDs.",
                "Note that we may view the level l I-ID as a NID.",
                "Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2).",
                "If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID.",
                "Note that within the I-IDs (or IDs) at each level, there is only a single decision node.",
                "Thus, our NID does not contain any MAIDs.",
                "Figure 2: A level l I-ID represented as a NID.",
                "The probabilities assigned to the blocks of the NID are is beliefs over js models conditioned on a physical state. 3.2 Solution The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively.",
                "We start by solving the level 0 models, which, if intentional, are traditional IDs.",
                "Their solutions provide probability distributions over the other agents actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID.",
                "The mapping from the level 0 models decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c).",
                "During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj].",
                "As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent is belief over the models of j conditioned on the physical state.",
                "The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j.",
                "Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18].",
                "This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief.",
                "Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agents current belief is known. 4.",
                "INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic influence diagrams (I-DIDs) extend I-IDs (and NIDs) to allow sequential decision-making over several time steps.",
                "Just as DIDs are structured graphical representations of POMDPs, I-DIDs are the graphical online analogs for finitely nested I-POMDPs.",
                "I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents. 4.1 Syntax We depict a general two time-slice I-DID in Fig. 3(a).",
                "In addition to the model nodes and the dashed policy link, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a).",
                "We explained the semantics of the model node and the policy link in the previous section; we describe the model updates next.",
                "The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1.",
                "Recall from Section 2 that an agents intentional model includes its belief.",
                "Because the agents act and receive observations, their models are updated to reflect their changed beliefs.",
                "Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models.",
                "Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models.",
                "Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model.",
                "These steps are a part of agent is belief update formalized using Eq. 1.",
                "In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID.",
                "If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ).",
                "These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation.",
                "The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously.",
                "Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed.",
                "The probability that js updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node.",
                "In order to obtain the probability of js possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ].",
                "Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a multiplexer modulated by Mod[Mt j ].",
                "Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1.",
                "Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1.",
                "Notice that the model update link may be replaced by the dependency links between the chance nodes that constitute the model nodes in the two time slices.",
                "In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them.",
                "Chance nodes and dependency links that not in bold are standard, usually found in DIDs.",
                "Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents js observation at time t + 1.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links.",
                "We note that the possible set of models of the other agent j grows exponentially with the number of time steps.",
                "For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node. 4.2 Solution Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively.",
                "For the purpose of illustration, let l=1 and T=2.",
                "The solution method uses the standard look-ahead technique, projecting the agents action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step.",
                "Because agent i has a belief over js models as well, the lookahead includes finding out the possible models that j could have in the future.",
                "Consequently, each of js subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions.",
                "These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b).",
                "We note the recursive nature of this solution: in solving agent is level 1 I-DID, js level 0 DIDs must be solved.",
                "If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner.",
                "We briefly outline the recursive algorithm for solving agent is Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase 1.",
                "For t from 1 to T − 1 do 2.",
                "If l ≥ 1 then Populate Mt+1 j,l−1 3.",
                "For each mt j in Range(Mt j,l−1) do 4.",
                "Recursively call algorithm with the l − 1 I-ID (or ID) that represents mt j and the horizon, T − t + 1 5.",
                "Map the decision node of the solved I-ID (or ID), OPT(mt j), to a chance node Aj 6.",
                "For each aj in OPT(mt j) do 7.",
                "For each oj in Oj (part of mt j) do 8.",
                "Update js belief, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← New I-ID (or ID) with bt+1 j as the initial belief 10.",
                "Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11.",
                "Add the model node, Mt+1 j,l−1, and the dependency links between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b)) 12.",
                "Add the chance, decision, and utility nodes for t + 1 time slice and the dependency links between them 13.",
                "Establish the CPTs for each chance node and utility node Look-Ahead Phase 14.",
                "Apply the standard look-ahead and backup method to solve the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5.",
                "We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the dependency links and the conditional probability tables for each node.",
                "We particularly focus on establishing and populating the model nodes (lines 3-11).",
                "Note that Range(·) returns the values (lower level models) of the random variable given as input (model node).",
                "In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs.",
                "Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents.",
                "As we mentioned previously, the 0-th level models are the traditional DIDs.",
                "Their solutions provide probability distributions over actions of the agent modeled at that level to I-DIDs at level 1.",
                "Given probability distributions over other agents actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models.",
                "Assume that the number of models considered at each level is bound by a number, M. Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs. 5.",
                "EXAMPLE APPLICATIONS To illustrate the usefulness of I-DIDs, we apply them to three problem domains.",
                "We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it. 5.1 Followership-Leadership in the Multiagent Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9].",
                "The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L).",
                "In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agents opening one of the doors.",
                "When any door is opened, the tiger persists in its original location with a probability of 95%.",
                "Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%.",
                "Agent j, on the other hand, hears growls with a reliability of 95%.",
                "Thus, the setting is such that agent i hears agent j opening doors more reliably than the tigers growls.",
                "This suggests that i could use js actions as an indication of the location of the tiger, as we discuss below.",
                "Each agents preferences are as in the single agent game discussed in [13].",
                "The transition, observation, and reward functions are shown in [16].",
                "A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions.",
                "In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship.",
                "In particular, we analyze the situational and epistemological conditions sufficient for their emergence.",
                "The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the others actions in order to maximize its payoffs.",
                "Let us consider a particular setting of the tiger problem in which agent i believes that js preferences are aligned with its own - both of them just want to get the gold - and js hearing is more reliable in comparison to itself.",
                "As an example, suppose that j, on listening can discern the tigers location 95% of the times compared to is 65% accuracy.",
                "Additionally, agent i does not have any initial information about the tigers location.",
                "In other words, is single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger.",
                "In addition, i considers two models of j, which differ in js flat level 0 initial beliefs.",
                "This is represented in the level 1 I-ID shown in Fig. 6(a).",
                "According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)).",
                "Agent i is undecided on these two models of j.",
                "If we vary is hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior.",
                "If is probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow js actions: i opens the same door that j opened previously iff is own assessment of the tigers location confirms js pick.",
                "If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)).",
                "Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem.",
                "Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations.",
                "We observed that a single level of belief nesting - beliefs about the others models - was sufficient for followership to emerge in the tiger problem.",
                "However, the epistemological requirements for the emergence of leadership are more complex.",
                "For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i.",
                "As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow js actions over time.",
                "Agent j emerges as a leader if it believes that i will follow it, which implies that js belief must be nested two levels deep to enable it to recognize its leadership role.",
                "Realizing that i will follow presents j with an opportunity to influence is actions in the benefit of the collective good or its self-interest alone.",
                "For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original.",
                "If j alone selects the correct door, it gets the payoff of 10.",
                "On the other hand, if both agents pick the wrong door, their penalties are cut in half.",
                "In this setting, it is in both js best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader.",
                "However, consider a slightly different problem in which j gains from is loss and is penalized if i gains.",
                "Specifically, let is payoff be subtracted from js, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then is loss of 100 becomes js gain.",
                "Agent j believes that i incorrectly thinks that js preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is.",
                "Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following js actions.",
                "We show is normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a).",
                "The policy demonstrates that i will blindly follow js actions.",
                "Since the tiger persists in its original location with a probability of 0.95, i will select the same door again.",
                "If j begins the game with a 99% probability that the tiger is on the right, solving js I-DID nested two levels deep, results in the policy shown in Fig. 8(b).",
                "Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL.",
                "Agent js intention is to deceive i who, it believes, will follow js actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest.",
                "Figure 8: Emergence of deception between agents in the tiger problem.",
                "Behaviors of interest are in bold. * denotes as before. (a) Agent is policy demonstrating that it will blindly follow js actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i. 5.2 Altruism and Reciprocity in the Public Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves.",
                "Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot.",
                "However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes.",
                "Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others contributions.",
                "However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions.",
                "The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4].",
                "Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting.",
                "These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others.",
                "For simplicity, we assume that the game is played between M = 2 agents, i and j.",
                "Let each agent be initially endowed with XT amount of resources.",
                "While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions.",
                "Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute.",
                "The latter action is deThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D).",
                "We assume that the actions are not observable to others.",
                "The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return.",
                "We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain.",
                "Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment.",
                "In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment.",
                "Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent.",
                "For simplicity, we assume that the cost of punishing is same for both the agents.",
                "The one-shot PG game with punishment is shown in Table. 1.",
                "Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action.",
                "If P < XT − ciXT , then defection is the dominating action for both.",
                "If P = XT − ciXT , then the game is not dominance-solvable.",
                "Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a).",
                "We formulate a sequential version of the PG problem with punishment from the perspective of agent i.",
                "Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents.",
                "Each agent may contribute a fixed amount, xc, or defect.",
                "An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot.",
                "Notice that the observations are also indirectly indicative of agent js actions because the state of the public pot is influenced by them.",
                "The amount of resources in agent is private pot, is perfectly observable to i.",
                "The payoffs are analogous to Table. 1.",
                "Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)).",
                "Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect.",
                "Let xc = 1 and the level 0 agent be punished half the times it defects.",
                "With one action remaining, both types of agents choose to contribute to avoid being punished.",
                "With two actions to go, the altruistic type chooses to contribute, while the other defects.",
                "This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids.",
                "Because cj for the non-altruistic type is less, it prefers not to contribute.",
                "With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects.",
                "For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection.",
                "We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level 0 models, mentioned previously, to j (see Fig. 9).",
                "If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps.",
                "This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type.",
                "However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1.",
                "These results demonstrate that the behavior of our altruistic type resembles that found experimentally.",
                "The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic.",
                "We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection.",
                "The reciprocal types marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other.",
                "We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full.",
                "For this prior belief, i chooses to defect.",
                "On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)).",
                "This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from js action to contribute.",
                "Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step.",
                "Agent i therefore chooses to contribute to reciprocate js action.",
                "An analogous reasoning leads i to defect when it observes a meager pot.",
                "With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations.",
                "Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic). 5.3 Strategies in Two-Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2].",
                "Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck.",
                "While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands.",
                "To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit.",
                "Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot.",
                "During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to 8.",
                "Notice that, for example, if a lower numbered card is discarded, the probability of receiving a low card in exchange is now reduced.",
                "We show the level 1 I-ID for the simplified two-player Poker in Fig. 11.",
                "We considered two models (personality types) of agent j.",
                "The conservative type believes that it is likely that its opponent has a high numbered card in its hand.",
                "On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card.",
                "Thus, the two types differ in their beliefs over their opponents hand.",
                "In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution.",
                "With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand.",
                "This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange.",
                "The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one.",
                "Figure 11: (a) Level 1 I-ID of agent i.",
                "The observation reveals information about js hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a).",
                "The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in js hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. is own hand contains the card numbered 8.",
                "The agent starts by keeping its card.",
                "On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card.",
                "If i observes that j discarded its card into the L or H pile, i believes that j is aggressive.",
                "On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange.",
                "Because the probability of receiving a low card is high now, i chooses to keep its card.",
                "On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card.",
                "In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff.",
                "This is partly due to the fact that an observation of, say, L resets the agent is previous time step beliefs over js hand to the low numbered cards only. 6.",
                "DISCUSSION We showed how DIDs may be extended to I-DIDs that enable online sequential decision-making in uncertain multiagent settings.",
                "Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent is three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs.",
                "I-DIDs extend NIDs to allow sequential decision-making over multiple time steps in the presence of other interacting agents.",
                "I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs.",
                "We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality.",
                "Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work.",
                "The first author would like to acknowledge the support of a UGARF grant. 7.",
                "REFERENCES [1] R. J. Aumann.",
                "Interactive epistemology i: Knowledge.",
                "International Journal of Game Theory, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron.",
                "The challenge of poker.",
                "AIJ, 2001. [3] A. Brandenburger and E. Dekel.",
                "Hierarchies of beliefs and common knowledge.",
                "Journal of Economic Theory, 59:189-198, 1993. [4] C. Camerer.",
                "Behavioral Game Theory: Experiments in Strategic Interaction.",
                "Princeton University Press, 2003. [5] E. Fehr and S. Gachter.",
                "Cooperation and punishment in public goods experiments.",
                "American Economic Review, 90(4):980-994, 2000. [6] D. Fudenberg and D. K. Levine.",
                "The Theory of Learning in Games.",
                "MIT Press, 1998. [7] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [8] Y. Gal and A. Pfeffer.",
                "A language for modeling agents decision-making processes in games.",
                "In AAMAS, 2003. [9] P. Gmytrasiewicz and P. Doshi.",
                "A framework for sequential planning in multiagent settings.",
                "JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz and E. Durfee.",
                "Rational coordination in multi-agent environments.",
                "JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi.",
                "Games with incomplete information played by bayesian players.",
                "Management Science, 14(3):159-182, 1967. [12] R. A. Howard and J. E. Matheson.",
                "Influence diagrams.",
                "In R. A. Howard and J. E. Matheson, editors, The Principles and Applications of Decision Analysis.",
                "Strategic Decisions Group, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman, and A. Cassandra.",
                "Planning and acting in partially observable stochastic domains.",
                "Artificial Intelligence Journal, 2, 1998. [14] D. Koller and B. Milch.",
                "Multi-agent influence diagrams for representing and solving games.",
                "In IJCAI, pages 1027-1034, 2001. [15] K. Polich and P. Gmytrasiewicz.",
                "Interactive dynamic influence diagrams.",
                "In GTDT Workshop, AAMAS, 2006. [16] B.",
                "Rathnas., P. Doshi, and P. J. Gmytrasiewicz.",
                "Exact solutions to interactive pomdps using behavioral equivalence.",
                "In Autonomous Agents and Multi-Agent Systems Conference (AAMAS), 2006. [17] S. Russell and P. Norvig.",
                "Artificial Intelligence: A Modern Approach (Second Edition).",
                "Prentice Hall, 2003. [18] R. D. Shachter.",
                "Evaluating influence diagrams.",
                "Operations Research, 34(6):871-882, 1986. [19] D. Suryadi and P. Gmytrasiewicz.",
                "Learning models of other agents using influence diagrams.",
                "In UM, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Los I POMDP adoptan un enfoque subjetivo para comprender el comportamiento estratégico, arraigado en un marco teórico de decisión que toma una perspectiva RS de \"toma de decisiones\" en la interacción."
            ],
            "translated_text": "",
            "candidates": [
                "tomar decisiones",
                "toma de decisiones"
            ],
            "error": []
        },
        "agent model": {
            "translated_key": "Modelo de agente",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Graphical Models for Online Solutions to Interactive POMDPs Prashant Doshi Dept.",
                "of Computer Science University of Georgia Athens, GA 30602, USA pdoshi@cs.uga.edu Yifeng Zeng Dept. of Computer Science Aalborg University DK-9220 Aalborg, Denmark yfzeng@cs.aau.edu Qiongyu Chen Dept. of Computer Science National Univ. of Singapore 117543, Singapore chenqy@comp.nus.edu.sg ABSTRACT We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation.",
                "These graphical models called interactive dynamic influence diagrams (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents.",
                "Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Theory 1.",
                "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in partially observable multiagent environments.",
                "They generalize POMDPs [13] to multiagent settings by including the other agents computable models in the state space along with the states of the physical environment.",
                "The models encompass all information influencing the agents behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11].",
                "I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a decision-makers perspective in the interaction.",
                "In [15], Polich and Gmytrasiewicz introduced interactive dynamic influence diagrams (I-DIDs) as the computational representations of I-POMDPs.",
                "I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs contribute to a growing line of work [19] that includes multi-agent influence diagrams (MAIDs) [14], and more recently, networks of influence diagrams (NIDs) [8].",
                "These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agents actions and chance nodes capturing the agents private information.",
                "MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the independence structure.",
                "NIDs extend MAIDs to include agents uncertainty over the game being played and over models of the other agents.",
                "Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent.",
                "Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently.",
                "However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games.",
                "Matters are more complex when we consider interactions that are extended over time, where predictions about others future actions must be made using models that change as the agents act and observe.",
                "I-DIDs address this gap by allowing the representation of other agents models as the values of a special model node.",
                "Both, other agents models and the original agents beliefs over these models are updated over time using special-purpose implementations.",
                "In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID.",
                "Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently.",
                "Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I-DID by [15], and show that it could be replaced by traditional dependency links.",
                "In the previous representation of the I-DID, the update of the agents belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time.",
                "We explicate the semantics of this link by showing how it can be implemented using the traditional dependency links between the chance nodes that constitute the model nodes.",
                "The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs.",
                "We show how IDIDs may be used to model an agents uncertainty over others models, that may themselves be I-DIDs.",
                "Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others models.",
                "Analogous to DIDs, I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. 2.",
                "BACKGROUND: FINITELY NESTED IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents models as part of the state space [9].",
                "Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents models and their beliefs about others.",
                "For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j.",
                "A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment.",
                "Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj .",
                "Here, j is Bayes rational and OCj is js optimality criterion.",
                "SMj is the set of subintentional models of j.",
                "Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent.",
                "We give a recursive bottom-up construction of the interactive state space below.",
                "ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . .",
                "ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent is preferences over its interactive states.",
                "Usually only the physical states will matter.",
                "Agent is policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i.",
                "Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai). 2.1 Belief Update Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes.",
                "However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones.",
                "First, since the state of the physical environment depends on the actions of both agents, is prediction of how the physical state changes has to be made based on its prediction of js actions.",
                "Second, changes in js models have to be included in is belief update.",
                "Specifically, if j is intentional then an update of js beliefs due to its action and observation has to be included.",
                "In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief.",
                "If js model is subintentional, then js probable observations are appended to the observation history contained in the model.",
                "Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update.",
                "For a version of the belief update when js model is subintentional, see [9].",
                "If agent j is also modeled as an I-POMDP, then is belief update invokes js belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke is belief update and so on.",
                "This recursion in belief nesting bottoms out at the 0th level.",
                "At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9]. 2.2 Value Iteration Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)).",
                "Eq. 2 is a basis for value iteration in I-POMDPs.",
                "Agent is optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3.",
                "INTERACTIVEINFLUENCEDIAGRAMS A naive extension of influence diagrams (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes.",
                "However, this approach assumes that the agents actions are controlled using a probability distribution that does not change over time.",
                "Interactive influence diagrams (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node.",
                "We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon.",
                "We note that the probability distribution over the chance node, S, and the model node together represents agent is belief over its interactive states.",
                "In addition to the model 1 The 0th level model is a POMDP: Other agents actions are treated as exogenous events and folded into the T, O, and R functions.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j.",
                "The hexagon is the model node (Mj,l−1) whose structure we show in (b).",
                "Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ).",
                "Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the policy link in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model.",
                "In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs.",
                "The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2.",
                "Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional.",
                "Because the model node contains the alternative models of the other agent as its values, its representation is not trivial.",
                "In particular, some of the models within the node are I-IDs that when solved generate the agents optimal policy in their decision nodes.",
                "Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise.",
                "Borrowing insights from previous work [8], we observe that the model node and the dashed policy link that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b).",
                "The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj.",
                "Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent is beliefs.",
                "The conditional probability table of the chance node, Aj, is a multiplexer that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj].",
                "The values of Mod[Mj] denote the different models of j.",
                "In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1.",
                "The distribution over the node, Mod[Mj], is the agent is belief over the models of j given a physical state.",
                "For more agents, we will have as many model nodes as there are agents.",
                "Notice that Fig. 1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional dependency links.",
                "In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them.",
                "In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes.",
                "This allows I-IDs to be represented and implemented using conventional application tools that target IDs.",
                "Note that we may view the level l I-ID as a NID.",
                "Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2).",
                "If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID.",
                "Note that within the I-IDs (or IDs) at each level, there is only a single decision node.",
                "Thus, our NID does not contain any MAIDs.",
                "Figure 2: A level l I-ID represented as a NID.",
                "The probabilities assigned to the blocks of the NID are is beliefs over js models conditioned on a physical state. 3.2 Solution The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively.",
                "We start by solving the level 0 models, which, if intentional, are traditional IDs.",
                "Their solutions provide probability distributions over the other agents actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID.",
                "The mapping from the level 0 models decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c).",
                "During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj].",
                "As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent is belief over the models of j conditioned on the physical state.",
                "The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j.",
                "Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18].",
                "This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief.",
                "Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agents current belief is known. 4.",
                "INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic influence diagrams (I-DIDs) extend I-IDs (and NIDs) to allow sequential decision-making over several time steps.",
                "Just as DIDs are structured graphical representations of POMDPs, I-DIDs are the graphical online analogs for finitely nested I-POMDPs.",
                "I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents. 4.1 Syntax We depict a general two time-slice I-DID in Fig. 3(a).",
                "In addition to the model nodes and the dashed policy link, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a).",
                "We explained the semantics of the model node and the policy link in the previous section; we describe the model updates next.",
                "The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1.",
                "Recall from Section 2 that an agents intentional model includes its belief.",
                "Because the agents act and receive observations, their models are updated to reflect their changed beliefs.",
                "Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models.",
                "Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models.",
                "Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model.",
                "These steps are a part of agent is belief update formalized using Eq. 1.",
                "In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID.",
                "If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ).",
                "These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation.",
                "The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously.",
                "Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed.",
                "The probability that js updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node.",
                "In order to obtain the probability of js possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ].",
                "Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a multiplexer modulated by Mod[Mt j ].",
                "Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1.",
                "Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1.",
                "Notice that the model update link may be replaced by the dependency links between the chance nodes that constitute the model nodes in the two time slices.",
                "In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them.",
                "Chance nodes and dependency links that not in bold are standard, usually found in DIDs.",
                "Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents js observation at time t + 1.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links.",
                "We note that the possible set of models of the other agent j grows exponentially with the number of time steps.",
                "For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node. 4.2 Solution Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively.",
                "For the purpose of illustration, let l=1 and T=2.",
                "The solution method uses the standard look-ahead technique, projecting the agents action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step.",
                "Because agent i has a belief over js models as well, the lookahead includes finding out the possible models that j could have in the future.",
                "Consequently, each of js subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions.",
                "These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b).",
                "We note the recursive nature of this solution: in solving agent is level 1 I-DID, js level 0 DIDs must be solved.",
                "If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner.",
                "We briefly outline the recursive algorithm for solving agent is Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase 1.",
                "For t from 1 to T − 1 do 2.",
                "If l ≥ 1 then Populate Mt+1 j,l−1 3.",
                "For each mt j in Range(Mt j,l−1) do 4.",
                "Recursively call algorithm with the l − 1 I-ID (or ID) that represents mt j and the horizon, T − t + 1 5.",
                "Map the decision node of the solved I-ID (or ID), OPT(mt j), to a chance node Aj 6.",
                "For each aj in OPT(mt j) do 7.",
                "For each oj in Oj (part of mt j) do 8.",
                "Update js belief, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← New I-ID (or ID) with bt+1 j as the initial belief 10.",
                "Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11.",
                "Add the model node, Mt+1 j,l−1, and the dependency links between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b)) 12.",
                "Add the chance, decision, and utility nodes for t + 1 time slice and the dependency links between them 13.",
                "Establish the CPTs for each chance node and utility node Look-Ahead Phase 14.",
                "Apply the standard look-ahead and backup method to solve the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5.",
                "We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the dependency links and the conditional probability tables for each node.",
                "We particularly focus on establishing and populating the model nodes (lines 3-11).",
                "Note that Range(·) returns the values (lower level models) of the random variable given as input (model node).",
                "In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs.",
                "Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents.",
                "As we mentioned previously, the 0-th level models are the traditional DIDs.",
                "Their solutions provide probability distributions over actions of the <br>agent model</br>ed at that level to I-DIDs at level 1.",
                "Given probability distributions over other agents actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models.",
                "Assume that the number of models considered at each level is bound by a number, M. Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs. 5.",
                "EXAMPLE APPLICATIONS To illustrate the usefulness of I-DIDs, we apply them to three problem domains.",
                "We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it. 5.1 Followership-Leadership in the Multiagent Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9].",
                "The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L).",
                "In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agents opening one of the doors.",
                "When any door is opened, the tiger persists in its original location with a probability of 95%.",
                "Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%.",
                "Agent j, on the other hand, hears growls with a reliability of 95%.",
                "Thus, the setting is such that agent i hears agent j opening doors more reliably than the tigers growls.",
                "This suggests that i could use js actions as an indication of the location of the tiger, as we discuss below.",
                "Each agents preferences are as in the single agent game discussed in [13].",
                "The transition, observation, and reward functions are shown in [16].",
                "A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions.",
                "In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship.",
                "In particular, we analyze the situational and epistemological conditions sufficient for their emergence.",
                "The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the others actions in order to maximize its payoffs.",
                "Let us consider a particular setting of the tiger problem in which agent i believes that js preferences are aligned with its own - both of them just want to get the gold - and js hearing is more reliable in comparison to itself.",
                "As an example, suppose that j, on listening can discern the tigers location 95% of the times compared to is 65% accuracy.",
                "Additionally, agent i does not have any initial information about the tigers location.",
                "In other words, is single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger.",
                "In addition, i considers two models of j, which differ in js flat level 0 initial beliefs.",
                "This is represented in the level 1 I-ID shown in Fig. 6(a).",
                "According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)).",
                "Agent i is undecided on these two models of j.",
                "If we vary is hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior.",
                "If is probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow js actions: i opens the same door that j opened previously iff is own assessment of the tigers location confirms js pick.",
                "If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)).",
                "Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem.",
                "Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations.",
                "We observed that a single level of belief nesting - beliefs about the others models - was sufficient for followership to emerge in the tiger problem.",
                "However, the epistemological requirements for the emergence of leadership are more complex.",
                "For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i.",
                "As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow js actions over time.",
                "Agent j emerges as a leader if it believes that i will follow it, which implies that js belief must be nested two levels deep to enable it to recognize its leadership role.",
                "Realizing that i will follow presents j with an opportunity to influence is actions in the benefit of the collective good or its self-interest alone.",
                "For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original.",
                "If j alone selects the correct door, it gets the payoff of 10.",
                "On the other hand, if both agents pick the wrong door, their penalties are cut in half.",
                "In this setting, it is in both js best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader.",
                "However, consider a slightly different problem in which j gains from is loss and is penalized if i gains.",
                "Specifically, let is payoff be subtracted from js, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then is loss of 100 becomes js gain.",
                "Agent j believes that i incorrectly thinks that js preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is.",
                "Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following js actions.",
                "We show is normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a).",
                "The policy demonstrates that i will blindly follow js actions.",
                "Since the tiger persists in its original location with a probability of 0.95, i will select the same door again.",
                "If j begins the game with a 99% probability that the tiger is on the right, solving js I-DID nested two levels deep, results in the policy shown in Fig. 8(b).",
                "Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL.",
                "Agent js intention is to deceive i who, it believes, will follow js actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest.",
                "Figure 8: Emergence of deception between agents in the tiger problem.",
                "Behaviors of interest are in bold. * denotes as before. (a) Agent is policy demonstrating that it will blindly follow js actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i. 5.2 Altruism and Reciprocity in the Public Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves.",
                "Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot.",
                "However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes.",
                "Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others contributions.",
                "However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions.",
                "The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4].",
                "Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting.",
                "These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others.",
                "For simplicity, we assume that the game is played between M = 2 agents, i and j.",
                "Let each agent be initially endowed with XT amount of resources.",
                "While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions.",
                "Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute.",
                "The latter action is deThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D).",
                "We assume that the actions are not observable to others.",
                "The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return.",
                "We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain.",
                "Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment.",
                "In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment.",
                "Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent.",
                "For simplicity, we assume that the cost of punishing is same for both the agents.",
                "The one-shot PG game with punishment is shown in Table. 1.",
                "Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action.",
                "If P < XT − ciXT , then defection is the dominating action for both.",
                "If P = XT − ciXT , then the game is not dominance-solvable.",
                "Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a).",
                "We formulate a sequential version of the PG problem with punishment from the perspective of agent i.",
                "Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents.",
                "Each agent may contribute a fixed amount, xc, or defect.",
                "An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot.",
                "Notice that the observations are also indirectly indicative of agent js actions because the state of the public pot is influenced by them.",
                "The amount of resources in agent is private pot, is perfectly observable to i.",
                "The payoffs are analogous to Table. 1.",
                "Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)).",
                "Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect.",
                "Let xc = 1 and the level 0 agent be punished half the times it defects.",
                "With one action remaining, both types of agents choose to contribute to avoid being punished.",
                "With two actions to go, the altruistic type chooses to contribute, while the other defects.",
                "This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids.",
                "Because cj for the non-altruistic type is less, it prefers not to contribute.",
                "With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects.",
                "For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection.",
                "We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level 0 models, mentioned previously, to j (see Fig. 9).",
                "If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps.",
                "This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type.",
                "However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1.",
                "These results demonstrate that the behavior of our altruistic type resembles that found experimentally.",
                "The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic.",
                "We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection.",
                "The reciprocal types marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other.",
                "We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full.",
                "For this prior belief, i chooses to defect.",
                "On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)).",
                "This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from js action to contribute.",
                "Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step.",
                "Agent i therefore chooses to contribute to reciprocate js action.",
                "An analogous reasoning leads i to defect when it observes a meager pot.",
                "With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations.",
                "Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic). 5.3 Strategies in Two-Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2].",
                "Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck.",
                "While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands.",
                "To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit.",
                "Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot.",
                "During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to 8.",
                "Notice that, for example, if a lower numbered card is discarded, the probability of receiving a low card in exchange is now reduced.",
                "We show the level 1 I-ID for the simplified two-player Poker in Fig. 11.",
                "We considered two models (personality types) of agent j.",
                "The conservative type believes that it is likely that its opponent has a high numbered card in its hand.",
                "On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card.",
                "Thus, the two types differ in their beliefs over their opponents hand.",
                "In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution.",
                "With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand.",
                "This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange.",
                "The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one.",
                "Figure 11: (a) Level 1 I-ID of agent i.",
                "The observation reveals information about js hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a).",
                "The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in js hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. is own hand contains the card numbered 8.",
                "The agent starts by keeping its card.",
                "On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card.",
                "If i observes that j discarded its card into the L or H pile, i believes that j is aggressive.",
                "On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange.",
                "Because the probability of receiving a low card is high now, i chooses to keep its card.",
                "On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card.",
                "In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff.",
                "This is partly due to the fact that an observation of, say, L resets the agent is previous time step beliefs over js hand to the low numbered cards only. 6.",
                "DISCUSSION We showed how DIDs may be extended to I-DIDs that enable online sequential decision-making in uncertain multiagent settings.",
                "Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent is three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs.",
                "I-DIDs extend NIDs to allow sequential decision-making over multiple time steps in the presence of other interacting agents.",
                "I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs.",
                "We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality.",
                "Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work.",
                "The first author would like to acknowledge the support of a UGARF grant. 7.",
                "REFERENCES [1] R. J. Aumann.",
                "Interactive epistemology i: Knowledge.",
                "International Journal of Game Theory, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron.",
                "The challenge of poker.",
                "AIJ, 2001. [3] A. Brandenburger and E. Dekel.",
                "Hierarchies of beliefs and common knowledge.",
                "Journal of Economic Theory, 59:189-198, 1993. [4] C. Camerer.",
                "Behavioral Game Theory: Experiments in Strategic Interaction.",
                "Princeton University Press, 2003. [5] E. Fehr and S. Gachter.",
                "Cooperation and punishment in public goods experiments.",
                "American Economic Review, 90(4):980-994, 2000. [6] D. Fudenberg and D. K. Levine.",
                "The Theory of Learning in Games.",
                "MIT Press, 1998. [7] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [8] Y. Gal and A. Pfeffer.",
                "A language for modeling agents decision-making processes in games.",
                "In AAMAS, 2003. [9] P. Gmytrasiewicz and P. Doshi.",
                "A framework for sequential planning in multiagent settings.",
                "JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz and E. Durfee.",
                "Rational coordination in multi-agent environments.",
                "JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi.",
                "Games with incomplete information played by bayesian players.",
                "Management Science, 14(3):159-182, 1967. [12] R. A. Howard and J. E. Matheson.",
                "Influence diagrams.",
                "In R. A. Howard and J. E. Matheson, editors, The Principles and Applications of Decision Analysis.",
                "Strategic Decisions Group, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman, and A. Cassandra.",
                "Planning and acting in partially observable stochastic domains.",
                "Artificial Intelligence Journal, 2, 1998. [14] D. Koller and B. Milch.",
                "Multi-agent influence diagrams for representing and solving games.",
                "In IJCAI, pages 1027-1034, 2001. [15] K. Polich and P. Gmytrasiewicz.",
                "Interactive dynamic influence diagrams.",
                "In GTDT Workshop, AAMAS, 2006. [16] B.",
                "Rathnas., P. Doshi, and P. J. Gmytrasiewicz.",
                "Exact solutions to interactive pomdps using behavioral equivalence.",
                "In Autonomous Agents and Multi-Agent Systems Conference (AAMAS), 2006. [17] S. Russell and P. Norvig.",
                "Artificial Intelligence: A Modern Approach (Second Edition).",
                "Prentice Hall, 2003. [18] R. D. Shachter.",
                "Evaluating influence diagrams.",
                "Operations Research, 34(6):871-882, 1986. [19] D. Suryadi and P. Gmytrasiewicz.",
                "Learning models of other agents using influence diagrams.",
                "In UM, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Sus soluciones proporcionan distribuciones de probabilidad sobre las acciones del \"modelo de agente\" editado en ese nivel a I-DID en el nivel 1."
            ],
            "translated_text": "",
            "candidates": [
                "Modelo de agente",
                "modelo de agente"
            ],
            "error": []
        }
    }
}