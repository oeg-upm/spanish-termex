{
    "id": "J-44",
    "original_text": "Scouts, Promoters, and Connectors: The Roles of Ratings in Nearest Neighbor Collaborative Filtering Bharath Kumar Mohan Dept. of CSA Indian Institute of Science Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. of Computer Science Eastern Michigan University Ypsilanti, MI 48917, USA bkeller@emich.edu Naren Ramakrishnan Dept. of Computer Science Virginia Tech, Blacksburg VA 24061, USA naren@cs.vt.edu ABSTRACT Recommender systems aggregate individual user ratings into predictions of products or services that might interest visitors. The quality of this aggregation process crucially affects the user experience and hence the effectiveness of recommenders in e-commerce. We present a novel study that disaggregates global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearestneighbor collaborative filtering. In particular, we formulate three roles-scouts, promoters, and connectors-that capture how users receive recommendations, how items get recommended, and how ratings of these two types are themselves connected (resp.). These roles find direct uses in improving recommendations for users, in better targeting of items and, most importantly, in helping monitor the health of the system as a whole. For instance, they can be used to track the evolution of neighborhoods, to identify rating subspaces that do not contribute (or contribute negatively) to system performance, to enumerate users who are in danger of leaving, and to assess the susceptibility of the system to attacks such as shilling. We argue that the three rating roles presented here provide broad primitives to manage a recommender system and its community. Categories and Subject Descriptors H.4.2 [Information Systems Applications]: Types of Systems-Decision support; J.4 [Computer Applications]: Social and Behavioral Sciences General Terms Algorithms, Human Factors 1. INTRODUCTION Recommender systems have become integral to e-commerce, providing technology that suggests products to a visitor based on previous purchases or rating history. Collaborative filtering, a common form of recommendation, predicts a users rating for an item by combining (other) ratings of that user with other users ratings. Significant research has been conducted in implementing fast and accurate collaborative filtering algorithms [2, 7], designing interfaces for presenting recommendations to users [1], and studying the robustness of these algorithms [8]. However, with the exception of a few studies on the influence of users [10], little attention has been paid to unraveling the inner workings of a recommender in terms of the individual ratings and the roles they play in making (good) recommendations. Such an understanding will give an important handle to monitoring and managing a recommender system, to engineer mechanisms to sustain the recommender, and thereby ensure its continued success. Our motivation here is to disaggregate global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearest-neighbor collaborative filtering. We identify three possible roles: (scouts) to connect the user into the system to receive recommendations, (promoters) to connect an item into the system to be recommended, and (connectors) to connect ratings of these two kinds. Viewing ratings in this way, we can define the contribution of a rating in each role, both in terms of allowing recommendations to occur, and in terms of influence on the quality of recommendations. In turn, this capability helps support scenarios such as: 1. Situating users in better neighborhoods: A users ratings may inadvertently connect the user to a neighborhood for which the users tastes may not be a perfect match. Identifying ratings responsible for such bad recommendations and suggesting new items to rate can help situate the user in a better neighborhood. 2. Targeting items: Recommender systems suffer from lack of user participation, especially in cold-start scenarios [13] involving newly arrived items. Identifying users who can be encouraged to rate specific items helps ensure coverage of the recommender system. 3. Monitoring the evolution of the recommender system and its stakeholders: A recommender system is constantly under change: growing with new users and 250 items, shrinking with users leaving the system, items becoming irrelevant, and parts of the system under attack. Tracking the roles of a rating and its evolution over time provides many insights into the health of the system, and how it could be managed and improved. These include being able to identify rating subspaces that do not contribute (or contribute negatively) to system performance, and could be removed; to enumerate users who are in danger of leaving, or have left the system; and to assess the susceptibility of the system to attacks such as shilling [5]. As we show, the characterization of rating roles presented here provides broad primitives to manage a recommender system and its community. The rest of the paper is organized as follows. Background on nearest-neighbor collaborative filtering and algorithm evaluation is discussed in Section 2. Section 3 defines and discusses the roles of a rating, and Section 4 defines measures of the contribution of a rating in each of these roles. In Section 5, we illustrate the use of these roles to address the goals outlined above. 2. BACKGROUND 2.1 Algorithms Nearest-neighbor collaborative filtering algorithms either use neighborhoods of users or neighborhoods of items to compute a prediction. An algorithm of the first kind is called user-based, and one of the second kind is called itembased [12]. In both families of algorithms, neighborhoods are formed by first computing the similarity between all pairs of users (for user-based) or items (for item-based). Predictions are then computed by aggregating ratings, which in a user-based algorithm involves aggregating the ratings of the target item by the users neighbors and, in an item-based algorithm, involves aggregating the users ratings of items that are neighbors of the target item. Algorithms within these families differ in the definition of similarity, formation of neighborhoods, and the computation of predictions. We consider a user-based algorithm based on that defined for GroupLens [11] with variations from Herlocker et al. [2], and an item-based algorithm similar to that of Sarwar et al. [12]. The algorithm used by Resnick et al. [11] defines the similarity of two users u and v as the Pearson correlation of their common ratings: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2 , where Iu is the set of items rated by user u, ru,i is user us rating for item i, and ¯ru is the average rating of user u (similarly for v). Similarity computed in this manner is typically scaled by a factor proportional to the number of common ratings, to reduce the chance of making a recommendation made on weak connections: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), where γ ≈ 5 is a constant used as a lower limit in scaling [2]. These new similarities are then used to define a static neighborhood Nu for each user u consisting of the top K users most similar to user u. A prediction for user u and item i is computed by a weighted average of the ratings by the neighbors pu,i = ¯ru + P v∈V sim (u, v)(rv,i − ¯rv) P v∈V sim (u, v) (1) where V = Nu ∩ Ui is the set of users most similar to u who have rated i. The item-based algorithm we use is the one defined by Sarwar et al. [12]. In this algorithm, similarity is defined as the adjusted cosine measure sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) where Ui is the set of users who have rated item i. As for the user-based algorithm, the similarity weights are adjusted proportionally to the number of users that have rated the items in common sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). (3) Given the similarities, the neighborhood Ni of an item i is defined as the top K most similar items for that item. A prediction for user u and item i is computed as the weighted average pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) where J = Ni ∩ Iu is the set of items rated by u that are most similar to i. 2.2 Evaluation Recommender algorithms have typically been evaluated using measures of predictive accuracy and coverage [3]. Studies on recommender algorithms, notably Herlocker et al. [2] and Sarwar et al. [12], typically compute predictive accuracy by dividing a set of ratings into training and test sets, and compute the prediction for an item in the test set using the ratings in the training set. A standard measure of predictive accuracy is mean absolute error (MAE), which for a test set T = {(u, i)} is defined as, MAE = P (u,i)∈T |pu,i − ru,i| |T | . (5) Coverage has a number of definitions, but generally refers to the proportion of items that can be predicted by the algorithm [3]. A practical issue with predictive accuracy is that users typically are presented with recommendation lists, and not individual numeric predictions. Recommendation lists are lists of items in decreasing order of prediction (sometimes stated in terms of star-ratings), and so predictive accuracy may not be reflective of the accuracy of the list. So, instead we can measure recommendation or rank accuracy, which indicates the extent to which the list is in the correct order. Herlocker et al. [3] discuss a number of rank accuracy measures, which range from Kendalls Tau to measures that consider the fact that users tend to only look at a prefix of the list [5]. Kendalls Tau measures the number of inversions when comparing ordered pairs in the true user ordering of 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figure 1: Ratings in simple movie recommender. items and the recommended order, and is defined as τ = C − D p (C + D + TR)(C + D + TP) (6) where C is the number of pairs that the system predicts in the correct order, D the number of pairs the system predicts in the wrong order, TR the number of pairs in the true ordering that have the same ratings, and TP is the number of pairs in the predicted ordering that have the same ratings [3]. A shortcoming of the Tau metric is that it is oblivious to the position in the ordered list where the inversion occurs [3]. For instance, an inversion toward the end of the list is given the same weight as one in the beginning. One solution is to consider inversions only in the top few items in the recommended list or to weight inversions based on their position in the list. 3. ROLES OF A RATING Our basic observation is that each rating plays a different role in each prediction in which it is used. Consider a simplified movie recommender system with three users Jim, Jeff, and Tom and their ratings for a few movies, as shown in Fig. 1. (For this initial discussion we will not consider the rating values involved.) The recommender predicts whether Tom will like The Mask using the other already available ratings. How this is done depends on the algorithm: 1. An item-based collaborative filtering algorithm constructs a neighborhood of movies around The Mask by using the ratings of users who rated The Mask and other movies similarly (e.g., Jims ratings of The Matrix and The Mask; and Jeffs ratings of Star Wars and The Mask). Toms ratings of those movies are then used to make a prediction for The Mask. 2. A user-based collaborative filtering algorithm would construct a neighborhood around Tom by tracking other users whose rating behaviors are similar to Toms (e.g., Tom and Jeff have rated Star Wars; Tom and Jim have rated The Matrix). The prediction of Toms rating for The Mask is then based on the ratings of Jeff and Tim. Although the nearest-neighbor algorithms aggregate the ratings to form neighborhoods used to compute predictions, we can disaggregate the similarities to view the computation of a prediction as simultaneously following parallel paths of ratings. So, irrespective of the collaborative filtering algorithm used, we can visualize the prediction of Toms rating of The Mask as walking through a sequence of ratings. In Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figure 2: Ratings used to predict The Mask for Tom. Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figure 3: Prediction of The Mask for Tom in which a rating is used more than once. this example, two paths were used for this prediction as depicted in Fig. 2: (p1, p2, p3) and (q1, q2, q3). Note that these paths are undirected, and are all of length 3. Only the order in which the ratings are traversed is different between the item-based algorithm (e.g., (p3, p2, p1), (q3, q2, q1)) and the user-based algorithm (e.g., (p1, p2, p3), (q1, q2, q3).) A rating can be part of many paths for a single prediction as shown in Fig. 3, where three paths are used for a prediction, two of which follow p1: (p1, p2, p3) and (p1, r2, r3). Predictions in a collaborative filtering algorithms may involve thousands of such walks in parallel, each playing a part in influencing the predicted value. Each prediction path consists of three ratings, playing roles that we call scouts, promoters, and connectors. To illustrate these roles, consider the path (p1, p2, p3) in Fig. 2 used to make a prediction of The Mask for Tom: 1. The rating p1 (Tom → Star Wars) makes a connection from Tom to other ratings that can be used to predict Toms rating for The Mask. This rating serves as a scout in the bipartite graph of ratings to find a path that leads to The Mask. 2. The rating p2 (Jeff → Star Wars) helps the system recommend The Mask to Tom by connecting the scout to the promoter. 3. The rating p3 (Jeff → The Mask) allows connections to The Mask, and, therefore, promotes this movie to Tom. Formally, given a prediction pu,a of a target item a for user u, a scout for pu,a is a rating ru,i such that there exists a user v with ratings rv,a and rv,i for some item i; a promoter for pu,a is a rating rv,a for some user v, such that there exist ratings rv,i and ru,i for an item i, and; a connector for pu,a 252 Jim Tom Jeff Jerry My Cousin Vinny The Matrix Star Wars The Mask Jurasic Park Figure 4: Scouts, promoters, and connectors. is a rating rv,i by some user v and rating i, such that there exists ratings ru,i and rv,a. The scouts, connectors, and promoters for the prediction of Toms rating of The Mask are p1 and q1, p2 and q2, and p3 and q3 (respectively). Each of these roles has a value in the recommender to the user, the users neighborhood, and the system in terms of allowing recommendations to be made. 3.1 Roles in Detail Ratings that act as scouts tend to help the recommender system suggest more movies to the user, though the extent to which this is true depends on the rating behavior of other users. For example, in Fig. 4 the rating Tom → Star Wars helps the system recommend only The Mask to him, while Tom → The Matrix helps recommend The Mask, Jurassic Park, and My Cousin Vinny. Tom makes a connection to Jim who is a prolific user of the system, by rating The Matrix. However, this does not make The Matrix the best movie to rate for everyone. For example, Jim is benefited equally by both The Mask and The Matrix, which allow the system to recommend Star Wars to him. His rating of The Mask is the best scout for Jeff, and Jerrys only scout is his rating of Star Wars. This suggests that good scouts allow a user to build similarity with prolific users, and thereby ensure they get more from the system. While scouts represent beneficial ratings from the perspective of a user, promoters are their duals, and are of benefit to items. In Fig. 4, My Cousin Vinny benefits from Jims rating, since it allows recommendations to Jeff and Tom. The Mask is not so dependent on just one rating, since the ratings by Jim and Jeff help it. On the other hand, Jerrys rating of Star Wars does not help promote it to any other user. We conclude that a good promoter connects an item to a broader neighborhood of other items, and thereby ensures that it is recommended to more users. Connectors serve a crucial role in a recommender system that is not as obvious. The movies My Cousin Vinny and Jurassic Park have the highest recommendation potential since they can be recommended to Jeff, Jerry and Tom based on the linkage structure illustrated in Fig. 4. Beside the fact that Jim rated these movies, these recommendations are possible only because of the ratings Jim → The Matrix and Jim → The Mask, which are the best connectors. A connector improves the systems ability to make recommendations with no explicit gain for the user. Note that every rating can be of varied benefit in each of these roles. The rating Jim → My Cousin Vinny is a poor scout and connector, but is a very good promoter. The rating Jim → The Mask is a reasonably good scout, a very good connector, and a good promoter. Finally, the rating Jerry → Star Wars is a very good scout, but is of no value as a connector or promoter. As illustrated here, a rating can have different value in each of the three roles in terms of whether a recommendation can be made or not. We could measure this value by simply counting the number of times a rating is used in each role, which alone would be helpful in characterizing the behavior of a system. But we can also measure the contribution of each rating to the quality of recommendations or health of the system. Since every prediction is a combined effort of several recommendation paths, we are interested in discerning the influence of each rating (and, hence, each path) in the system towards the systems overall error. We can understand the dynamics of the system at a finer granularity by tracking the influence of a rating according to the role played. The next section describes the approach to measuring the values of a rating in each role. 4. CONTRIBUTIONS OF RATINGS As weve seen, a rating may play different roles in different predictions and, in each prediction, contribute to the quality of a prediction in different ways. Our approach can use any numeric measure of a property of system health, and assigns credit (or blame) to each rating proportional to its influence in the prediction. By tracking the role of each rating in a prediction, we can accumulate the credit for a rating in each of the three roles, and also track the evolution of the roles of rating over time in the system. This section defines the methodology for computing the contribution of ratings by first defining the influence of a rating, and then instantiating the approach for predictive accuracy, and then rank accuracy. We also demonstrate how these contributions can be aggregated to study the neighborhood of ratings involved in computing a users recommendations. Note that although our general formulation for rating influence is algorithm independent, due to space considerations, we present the approach for only item-based collaborative filtering. The definition for user-based algorithms is similar and will be presented in an expanded version of this paper. 4.1 Influence of Ratings Recall that an item-based approach to collaborative filtering relies on building item neighborhoods using the similarity of ratings by the same user. As described earlier, similarity is defined by the adjusted cosine measure (Equations (2) and (3)). A set of the top K neighbors is maintained for all items for space and computational efficiency. A prediction of item i for a user u is computed as the weighted deviation from the items mean rating as shown in Equation (4). The list of recommendations for a user is then the list of items sorted in descending order of their predicted values. We first define impact(a, i, j), the impact a user a has in determining the similarity between two items i and j. This is the change in the similarity between i and j when as rating is removed, and is defined as impact(a, i, j) = |sim (i, j) − sim¯a(i, j)| P w∈Cij |sim (i, j) − sim ¯w(i, j)| where Cij = {u ∈ U | ∃ ru,i, ru,j ∈ R(u)} is the set of coraters 253 of items i and j (users who rate both i and j), R(u) is the set of ratings provided by user u, and sim¯a(i, j) is the similarity of i and j when the ratings of user a are removed sim¯a(i, j) = P v∈U\\{a} (ru,i − ¯ru)(ru,j − ¯ru) qP u∈U\\{a}(ru,i − ¯ru)2 qP u∈U\\{a}(ru,j − ¯ru)2 , and adjusted for the number of raters sim¯a(i, j) = max(|Ui ∩ Uj| − 1, γ) γ · sim(i, j). If all coraters of i and j rate them identically, we define the impact as impact(a, i, j) = 1 |Cij| since P w∈Cij |sim (i, j) − sim ¯w(i, j)| = 0. The influence of each path (u, j, v, i) = [ru,j, rv,j, rv,i] in the prediction of pu,i is given by influence(u, j, v, i) = sim (i, j) P l∈Ni∩Iu sim (i, l) · impact(v, i, j) It follows that the sum of influences over all such paths, for a given set of endpoints, is 1. 4.2 Role Values for Predictive Accuracy The value of a rating in each role is computed from the influence depending on the evaluation measure employed. Here we illustrate the approach using predictive accuracy as the evaluation metric. In general, the goodness of a prediction decides whether the ratings involved must be credited or discredited for their role. For predictive accuracy, the error in prediction e = |pu,i − ru,i| is mapped to a comfort level using a mapping function M(e). Anecdotal evidence suggests that users are unable to discern errors less than 1.0 (for a rating scale of 1 to 5) [4], and so an error less than 1.0 is considered acceptable, but anything larger is not. We hence define M(e) as (1 − e) binned to an appropriate value in [−1, −0.5, 0.5, 1]. For each prediction pu,i, M(e) is attributed to all the paths that assisted the computation of pu,i, proportional to their influences. This tribute, M(e)∗influence(u, j, v, i), is in turn inherited by each of the ratings in the path [ru,j, rv,j, rv,i], with the credit/blame accumulating to the respective roles of ru,j as a scout, rv,j as a connector, and rv,i as a promoter. In other words, the scout value SF(ru,j), the connector value CF(rv,j) and the promoter value PF(rv,i) are all incremented by the tribute amount. Over a large number of predictions, scouts that have repeatedly resulted in big error rates have a big negative scout value, and vice versa (similarly with the other roles). Every rating is thus summarized by its triple [SF, CF, PF]. 4.3 Role Values for Rank Accuracy We now define the computation of the contribution of ratings to observed rank accuracy. For this computation, we must know the users preference order for a set of items for which predictions can be computed. We assume that we have a test set of the users ratings of the items presented in the recommendation list. For every pair of items rated by a user in the test data, we check whether the predicted order is concordant with his preference. We say a pair (i, j) is concordant (with error ) whenever one of the following holds: • if (ru,i < ru,j) then (pu,i − pu,j < ); • if (ru,i > ru,j) then (pu,i − pu,j > ); or • if (ru,i = ru,j) then (|pu,i − pu,j| ≤ ). Similarly, a pair (i, j) is discordant (with error ) if it is not concordant. Our experiments described below use an error tolerance of = 0.1. All paths involved in the prediction of the two items in a concordant pair are credited, and the paths involved in a discordant pair are discredited. The credit assigned to a pair of items (i, j) in the recommendation list for user u is computed as c(i, j) = ( t T · 1 C+D if (i, j) are concordant − t T · 1 C+D if (i, j) are discordant (7) where t is the number of items in the users test set whose ratings could be predicted, T is the number of items rated by user u in the test set, C is the number of concordances and D is the number of discordances. The credit c is then divided among all paths responsible for predicting pu,i and pu,j proportional to their influences. We again add the role values obtained from all the experiments to form a triple [SF, CF, PF] for each rating. 4.4 Aggregating rating roles After calculating the role values for individual ratings, we can also use these values to study neighborhoods and the system. Here we consider how we can use the role values to characterize the health of a neighborhood. Consider the list of top recommendations presented to a user at a specific point in time. The collaborative filtering algorithm traversed many paths in his neighborhood through his scouts and other connectors and promoters to make these recommendations. We call these ratings the recommender neighborhood of the user. The user implicitly chooses this neighborhood of ratings through the items he rates. Apart from the collaborative filtering algorithm, the health of this neighborhood completely influences a users satisfaction with the system. We can characterize a users recommender neighborhood by aggregating the individual role values of the ratings involved, weighted by the influence of individual ratings in determining his recommended list. Different sections of the users neighborhood wield varied influence on his recommendation list. For instance, ratings reachable through highly rated items have a bigger say in the recommended items. Our aim is to study the system and classify users with respect to their positioning in a healthy or unhealthy neighborhood. A user can have a good set of scouts, but may be exposed to a neighborhood with bad connectors and promoters. He can have a good neighborhood, but his bad scouts may ensure the neighborhoods potential is rendered useless. We expect that users with good scouts and good neighborhoods will be most satisfied with the system in the future. A users neighborhood is characterized by a triple that represents the weighted sum of the role values of individual ratings involved in making recommendations. Consider a user u and his ordered list of recommendations L. An item i 254 in the list is weighted inversely, as K(i), depending on its position in the list. In our studies we use K(i) = p position(i). Several paths of ratings [ru,j, rv,j, rv,i] are involved in predicting pu,i which ultimately decides its position in L, each with influence(u, j, v, i). The recommender neighborhood of a user u is characterized by the triple, [SFN(u), CFN(u), PFN(u)] where SFN(u) = X i∈L P [ru,j ,rv,j ,rv,i] SF(ru,j)influence(u, j, v, i) K(i) ! CFN(u) and PFN(u) are defined similarly. This triple estimates the quality of us recommendations based on the past track record of the ratings involved in their respective roles. 5. EXPERIMENTATION As we have seen, we can assign role values to each rating when evaluating a collaborative filtering system. In this section, we demonstrate the use of this approach to our overall goal of defining an approach to monitor and manage the health of a recommender system through experiments done on the MovieLens million rating dataset. In particular, we discuss results relating to identifying good scouts, promoters, and connectors; the evolution of rating roles; and the characterization of user neighborhoods. 5.1 Methodology Our experiments use the MovieLens million rating dataset, which consists of ratings by 6040 users of 3952 movies. The ratings are in the range 1 to 5, and are labeled with the time the rating was given. As discussed before, we consider only the item-based algorithm here (with item neighborhoods of size 30) and, due to space considerations, only present role value results for rank accuracy. Since we are interested in the evolution of the rating role values over time, the model of the recommender system is built by processing ratings in their arrival order. The timestamping provided by MovieLens is hence crucial for the analyses presented here. We make assessments of rating roles at intervals of 10,000 ratings and processed the first 200,000 ratings in the dataset (giving rise to 20 snapshots). We incrementally update the role values as the time ordered ratings are merged into the model. To keep the experiment computationally manageable, we define a test dataset for each user. As the time ordered ratings are merged into the model, we label a small randomly selected percentage (20%) as test data. At discrete epochs, i.e., after processing every 10,000 ratings, we compute the predictions for the ratings in the test data, and then compute the role values for the ratings used in the predictions. One potential criticism of this methodology is that the ratings in the test set are never evaluated for their roles. We overcome this concern by repeating the experiment, using different random seeds. The probability that every rating is considered for evaluation is then considerably high: 1 − 0.2n , where n is the number of times the experiment is repeated with different random seeds. The results here are based on n = 4 repetitions. The item-based collaborative filtering algorithms performance was ordinary with respect to rank accuracy. Fig. 5 shows a plot of the precision and recall as ratings were merged in time order into the model. The recall was always high, but the average precision was just about 53%. 0 0.2 0.4 0.6 0.8 1 1.2 10000 30000 50000 70000 90000110000130000150000 Ratings merged into model Value Precision Recall Figure 5: Precision and recall for the item-based collaborative filtering algorithm. 5.2 Inducing good scouts The ratings of a user that serve as scouts are those that allow the user to receive recommendations. We claim that users with ratings that have respectable scout values will be happier with the system than those with ratings with low scout values. Note that the item-based algorithm discussed here produces recommendation lists with nearly half of the pairs in the list discordant from the users preference. Whether all of these discordant pairs are observable by the user is unclear, however, certainly this suggests that there is a need to be able to direct users to items whose ratings would improve the lists. The distribution of the scout values for most users ratings are Gaussian with mean zero. Fig. 6 shows the frequency distribution of scout values for a sample user at a given snapshot. We observe that a large number of ratings never serve as scouts for their users. A relatable scenario is when Amazons recommender makes suggestions of books or items based on other items that were purchased as gifts. With simple relevance feedback from the user, such ratings can be isolated as bad scouts and discounted from future predictions. Removing bad scouts was found to be worthwhile for individual users but the overall performance improvement was only marginal. An obvious question is whether good scouts can be formed by merely rating popular movies as suggested by Rashid et al. [9]. They show that a mix of popularity and rating entropy identifies the best items to suggest to new users when evaluated using MAE. Following their intuition, we would expect to see a higher correlation between popularityentropy and good scouts. We measured the Pearson correlation coefficient between aggregated scout values for a movie with the popularity of a movie (number of times it is rated); and with its popularity*variance measure at different snapshots of the system. Note that the scout values were initially anti-correlated with popularity (Fig. 7), but became moderately correlated as the system evolved. Both popularity and popularity*variance performed similarly. A possible explanation is that there has been insufficient time for the popular movies to accumulate ratings. 255 -10 0 10 20 30 40 50 60 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 Scout Value Frequency Figure 6: Distribution of scout values for a sample user. -0.4 -0.2 0 0.2 0.4 0.6 0.8 30000 60000 90000 120000 150000 180000 Popularity Pop*Var Figure 7: Correlation between aggregated scout value and item popularity (computed at different intervals). -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 30000 60000 90000 120000 150000 180000 Figure 8: Correlation between aggregated promoter value and user prolificity (computed at different intervals). Table 1: Movies forming the best scouts. Best Scouts Conf. Pop. Being John Malkovich (1999) 1.00 445 Star Wars: Episode IV - A New Hope (1977) 0.92 623 Princess Bride, The (1987) 0.85 477 Sixth Sense, The (1999) 0.85 617 Matrix, The (1999) 0.77 522 Ghostbusters (1984) 0.77 441 Casablanca (1942) 0.77 384 Insider, The (1999) 0.77 235 American Beauty (1999) 0.69 624 Terminator 2: Judgment Day (1991) 0.69 503 Fight Club (1999) 0.69 235 Shawshank Redemption, The (1994) 0.69 445 Run Lola Run (Lola rennt) (1998) 0.69 220 Terminator, The (1984) 0.62 450 Usual Suspects, The (1995) 0.62 326 Aliens (1986) 0.62 385 North by Northwest (1959) 0.62 245 Fugitive, The (1993) 0.62 402 End of Days (1999) 0.62 132 Raiders of the Lost Ark (1981) 0.54 540 Schindlers List (1993) 0.54 453 Back to the Future (1985) 0.54 543 Toy Story (1995) 0.54 419 Alien (1979) 0.54 415 Abyss, The (1989) 0.54 345 2001: A Space Odyssey (1968) 0.54 358 Dogma (1999) 0.54 228 Little Mermaid, The (1989) 0.54 203 Table 2: Movies forming the worst scouts. Worst scouts Conf. Pop. Harold and Maude (1971) 0.46 141 Grifters, The (1990) 0.46 180 Sting, The (1973) 0.38 244 Godfather: Part III, The (1990) 0.38 154 Lawrence of Arabia (1962) 0.38 167 High Noon (1952) 0.38 84 Women on the Verge of a... (1988) 0.38 113 Grapes of Wrath, The (1940) 0.38 115 Duck Soup (1933) 0.38 131 Arsenic and Old Lace (1944) 0.38 138 Midnight Cowboy (1969) 0.38 137 To Kill a Mockingbird (1962) 0.31 195 Four Weddings and a Funeral (1994) 0.31 271 Good, The Bad and The Ugly, The (1966) 0.31 156 Its a Wonderful Life (1946) 0.31 146 Player, The (1992) 0.31 220 Jackie Brown (1997) 0.31 118 Boat, The (Das Boot) (1981) 0.31 210 Manhattan (1979) 0.31 158 Truth About Cats & Dogs, The (1996) 0.31 143 Ghost (1990) 0.31 227 Lone Star (1996) 0.31 125 Big Chill, The (1983) 0.31 184 256 By studying the evolution of scout values, we can identify movies that consistently feature in good scouts over time. We claim these movies will make viable scouts for other users. We found the aggregated scout values for all movies in intervals of 10,000 ratings each. A movie is said to induce a good scout if the movie was in the top 100 of the sorted list, and to induce a bad scout if it was in bottom 100 of the same list. Movies appearing consistently high over time are expected to remain up there in the future. The effective confidence in a movie m is Cm = Tm − Bm N (8) where Tm is the number of times it appeared in the top 100, Bm the number of times it appeared in the bottom 100, and N is the number of intervals considered. Using this measure, the top few movies expected to induce the best scouts are shown in Table 1. Movies that would be bad scout choices are shown in Table 2 with their associated confidences. The popularities of the movies are also displayed. Although more popular movies appear in the list of good scouts, these tables show that a blind choice of scout based on popularity alone can be potentially dangerous. Interestingly, the best scout-Being John Malkovich-is about a puppeteer who discovers a portal into a movie star, a movie that has been described variously on amazon.com as makes you feel giddy, seriously weird, comedy with depth, silly, strange, and inventive. Indicating whether someone likes this movie or not goes a long way toward situating the user in a suitable neighborhood, with similar preferences. On the other hand, several factors may have made a movie a bad scout, like the sharp variance in user preferences in the neighborhood of a movie. Two users may have the same opinion about Lawrence of Arabia, but they may differ sharply about how they felt about the other movies they saw. Bad scouts ensue when there is deviation in behavior around a common synchronization point. 5.3 Inducing good promoters Ratings that serve to promote items in a collaborative filtering system are critical to allowing a new item be recommended to users. So, inducing good promoters is important for cold-start recommendation. We note that the frequency distribution of promoter values for a sample movies ratings is also Gaussian (similar to Fig. 6). This indicates that the promotion of a movie is benefited most by the ratings of a few users, and are unaffected by the ratings of most users. We find a strong correlation between a users number of ratings and his aggregated promoter value. Fig. 8 depicts the evolution of the Pearson correlation co-efficient between the prolificity of a user (number of ratings) versus his aggregated promoter value. We expect that conspicuous shills, by recommending wrong movies to users, will be discredited with negative aggregate promoter values and should be identifiable easily. Given this observation, the obvious rule to follow when introducing a new movie is to have it rated directly by prolific users who posses high aggregated promoter values. A new movie is thus cast into the neighborhood of many other movies improving its visibility. Note, though, that a user may have long stopped using the system. Tracking promoter values consistently allows only the most active recent users to be considered. 5.4 Inducing good connectors Given the way scouts, connectors, and promoters are characterized, it follows that the movies that are part of the best scouts are also part of the best connectors. Similarly, the users that constitute the best promoters are also part of the best connectors. Good connectors are induced by ensuring a user with a high promoter value rates a movie with a high scout value. In our experiments, we find that a ratings longest standing role is often as a connector. A rating with a poor connector value is often seen due to its user being a bad promoter, or its movie being a bad scout. Such ratings can be removed from the prediction process to bring marginal improvements to recommendations. In some selected experiments, we observed that removing a set of badly behaving connectors helped improve the systems overall performance by 1.5%. The effect was even higher on a few select users who observed an improvement of above 10% in precision without much loss in recall. 5.5 Monitoring the evolution of rating roles One of the more significant contributions of our work is the ability to model the evolution of recommender systems, by studying the changing roles of ratings over time. The role and value of a rating can change depending on many factors like user behavior, redundancy, shilling effects or properties of the collaborative filtering algorithm used. Studying the dynamics of rating roles in terms of transitions between good, bad, and negligible values can provide insights into the functioning of the recommender system. We believe that a continuous visualization of these transitions will improve the ability to manage a recommender system. We classify different rating states as good, bad, or negligible. Consider a user who has rated 100 movies in a particular interval, of which 20 are part of the test set. If a scout has a value greater than 0.005, it indicates that it is uniquely involved in at least 2 concordant predictions, which we will say is good. Thus, a threshold of 0.005 is chosen to bin a rating as good, bad or negligible in terms of its scout, connector and promoter value. For instance, a rating r, at time t with role value triple [0.1, 0.001, −0.01] is classified as [scout +, connector 0, promoter −], where + indicates good, 0 indicates negligible, and − indicates bad. The positive credit held by a rating is a measure of its contribution to the betterment of the system, and the discredit is a measure of its contribution to the detriment of the system. Even though the positive roles (and the negative roles) make up a very small percentage of all ratings, their contribution supersedes their size. For example, even though only 1.7% of all ratings were classified as good scouts, they hold 79% of all positive credit in the system! Similarly, the bad scouts were just 1.4% of all ratings but hold 82% of all discredit. Note that good and bad scouts, together, comprise only 1.4% + 1.7% = 3.1% of the ratings, implying that the majority of the ratings are negligible role players as scouts (more on this later). Likewise, good connectors were 1.2% of the system, and hold 30% of all positive credit. The bad connectors (0.8% of the system) hold 36% of all discredit. Good promoters (3% of the system) hold 46% of all credit, while bad promoters (2%) hold 50% of all discredit. This reiterates that a few ratings influence most of the systems performance. Hence it is important to track transitions between them regardless of their small numbers. 257 Across different snapshots, a rating can remain in the same state or change. A good scout can become a bad scout, a good promoter can become a good connector, good and bad scouts can become vestigial, and so on. It is not practical to expect a recommender system to have no ratings in bad roles. However, it suffices to see ratings in bad roles either convert to good or vestigial roles. Similarly, observing a large number of good roles become bad ones is a sign of imminent failure of the system. We employ the principle of non-overlapping episodes [6] to count such transitions. A sequence such as: [+, 0, 0] → [+, 0, 0] → [0, +, 0] → [0, 0, 0] is interpreted as the transitions [+, 0, 0] ; [0, +, 0] : 1 [+, 0, 0] ; [0, 0, 0] : 1 [0, +, 0] ; [0, 0, 0] : 1 instead of [+, 0, 0] ; [0, +, 0] : 2 [+, 0, 0] ; [0, 0, 0] : 2 [0, +, 0] ; [0, 0, 0] : 1. See [6] for further details about this counting procedure. Thus, a rating can be in one of 27 possible states, and there are about 272 possible transitions. We make a further simplification and utilize only 9 states, indicating whether the rating is a scout, promoter, or connector, and whether it has a positive, negative, or negligible role. Ratings that serve multiple purposes are counted using multiple episode instantiations but the states themselves are not duplicated beyond the 9 restricted states. In this model, a transition such as [+, 0, +] ; [0, +, 0] : 1 is counted as [scout+] ; [scout0] : 1 [scout+] ; [connector+] : 1 [scout+] ; [promoter0] : 1 [connector0] ; [scout0] : 1 [connector0] ; [scout+] : 1 [connector0] ; [promoter0] : 1 [promoter+] ; [scout0] : 1 [promoter+] ; [connector+] : 1 [promoter+] ; [promoter0] : 1 Of these, transitions like [pX] ; [q0] where p = q, X ∈ {+, 0, −} are considered uninteresting, and only the rest are counted. Fig. 9 depicts the major transitions counted while processing the first 200,000 ratings from the MovieLens dataset. Only transitions with frequency greater than or equal to 3% are shown. The percentages for each state indicates the number of ratings that were found to be in those states. We consider transitions from any state to a good state as healthy, from any state to a bad state as unhealthy, and from any state to a vestigial state as decaying. From Fig. 9, we can observe: • The bulk of the ratings have negligible values, irrespective of their role. The majority of the transitions involve both good and bad ratings becoming negligible. Scout + (2%) Scout(1.5%) Scout 0 (96.5%) Connector + (1.2%) Connector(0.8%) Connector 0 (98%) Promoter + (3%) Promoter(2%) Promoter 0 (95%) 84% 84% 81% 74% 10% 6% 11% 77% 8% 7% 8% 82% 4% 86% 4% 68% 15% 13% 5% 5% 77% 11% 7% 5% 4% 3% 3% 3% Healthy Unhealthy Decaying Figure 9: Transitions among rating roles. • The number of good ratings is comparable to the bad ratings, and ratings are seen to switch states often, except in the case of scouts (see below). • The negative and positive scout states are not reachable through any transition, indicating that these ratings must begin as such, and cannot be coerced into these roles. • Good promoters and good connectors have a much longer survival period than scouts. Transitions that recur to these states have frequencies of 10% and 15% when compared to just 4% for scouts. Good connectors are the slowest to decay whereas (good) scouts decay the fastest. • Healthy percentages are seen on transitions between promoters and connectors. As indicated earlier, there are hardly any transitions from promoters/connectors to scouts. This indicates that, over the long run, a users rating is more useful to others (movies or other users) than to the user himself. • The percentages of healthy transitions outweigh the unhealthy ones - this hints that the system is healthy, albeit only marginally. Note that these results are conditioned by the static nature of the dataset, which is a set of ratings over a fixed window of time. However a diagram such as Fig. 9 is clearly useful for monitoring the health of a recommender system. For instance, acceptable limits can be imposed on different types of transitions and, if a transition fails to meet the threshold, the recommender system or a part of it can be brought under closer scrutiny. Furthermore, the role state transition diagram would also be the ideal place to study the effects of shilling, a topic we will consider in future research. 5.6 Characterizing neighborhoods Earlier we saw that we can characterize the neighborhood of ratings involved in creating a recommendation list L for 258 a user. In our experiment, we consider lists of length 30, and sample the lists of about 5% of users through the evolution of the model (at intervals of 10,000 ratings each) and compute their neighborhood characteristics. To simplify our presentation, we consider the percentage of the sample that fall into one of the following categories: 1. Inactive user: (SFN(u) = 0) 2. Good scouts, Good neighborhood: (SFN(u) > 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 3. Good scouts, Bad neighborhood: (SFN(u) > 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) 4. Bad scouts, Good neighborhood: (SFN(u) < 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 5. Bad scouts, Bad neighborhood: (SFN(u) < 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) From our sample set of 561 users, we found that 476 users were inactive. Of the remaining 85 users, we found 26 users had good scouts and a good neighborhood, 6 had bad scouts and a good neighborhood, 29 had good scouts and a bad neighborhood, and 24 had bad scouts and a bad neighborhood. Thus, we conjecture that 59 users (29+24+6) are in danger of leaving the system. As a remedy, users with bad scouts and a good neighborhood can be asked to reconsider rating of some movies hoping to improve the systems recommendations. The system can be expected to deliver more if they engineer some good scouts. Users with good scouts and a bad neighborhood are harder to address; this situation might entail selectively removing some connector-promoter pairs that are causing the damage. Handling users with bad scouts and bad neighborhoods is a more difficult challenge. Such a classification allows the use of different strategies to better a users experience with the system depending on his context. In future work, we intend to conduct field studies and study the improvement in performance of different strategies for different contexts. 6. CONCLUSIONS To further recommender system acceptance and deployment, we require new tools and methodologies to manage an installed recommender and develop insights into the roles played by ratings. A fine-grained characterization in terms of rating roles such as scouts, promoters, and connectors, as done here, helps such an endeavor. Although we have presented results on only the item-based algorithm with list rank accuracy as the metric, the same approach outlined here applies to user-based algorithms and other metrics. In future research, we plan to systematically study the many algorithmic parameters, tolerances, and cutoff thresholds employed here and reason about their effects on the downstream conclusions. We also aim to extend our formulation to other collaborative filtering algorithms, study the effect of shilling in altering rating roles, conduct field studies, and evaluate improvements in user experience by tweaking ratings based on their role values. Finally, we plan to develop the idea of mining the evolution of rating role patterns into a reporting and tracking system for all aspects of recommender system health. 7. REFERENCES [1] Cosley, D., Lam, S., Albert, I., Konstan, J., and Riedl, J. Is Seeing Believing?: How Recommender System Interfaces Affect Users Opinions. In Proc. CHI (2001), pp. 585-592. [2] Herlocker, J. L., Konstan, J. A., Borchers, A., and Riedl, J. An Algorithmic Framework for Performing Collaborative Filtering. In Proc. SIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J. A., Terveen, L. G., and Riedl, J. T. Evaluating Collaborative Filtering Recommender Systems. ACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J. A. Personal communication. 2003. [5] Lam, S. K., and Riedl, J. Shilling Recommender Systems for Fun and Profit. In Proceedings of the 13th International World Wide Web Conference (2004), ACM Press, pp. 393-402. [6] Laxman, S., Sastry, P. S., and Unnikrishnan, K. P. Discovering Frequent Episodes and Learning Hidden Markov Models: A Formal Connection. IEEE Transactions on Knowledge and Data Engineering Vol. 17, 11 (2005), 1505-1517. [7] McLaughlin, M. R., and Herlocker, J. L. A Collaborative Filtering Algorithm and Evaluation Metric that Accurately Model the User Experience. In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (2004), pp. 329 - 336. [8] OMahony, M., Hurley, N. J., Kushmerick, N., and Silvestre, G. Collaborative Recommendation: A Robustness Analysis. ACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J. A., and Riedl, J. Getting to Know You: Learning New User Preferences in Recommender Systems. In Proceedings of the 2002 Conference on Intelligent User Interfaces (IUI 2002) (2002), pp. 127-134. [10] Rashid, A. M., Karypis, G., and Riedl, J. Influence in Ratings-Based Recommender Systems: An Algorithm-Independent Approach. In Proc. of the SIAM International Conference on Data Mining (2005). [11] Resnick, P., Iacovou, N., Sushak, M., Bergstrom, P., and Riedl, J. GroupLens: An Open Architecture for Collaborative Filtering of Netnews. In Proceedings of the Conference on Computer Supported Collaborative Work (CSCW94) (1994), ACM Press, pp. 175-186. [12] Sarwar, B., Karypis, G., Konstan, J., and Reidl, J. Item-Based Collaborative Filtering Recommendation Algorithms. In Proceedings of the Tenth International World Wide Web Conference (WWW10) (2001), pp. 285-295. [13] Schein, A., Popescu, A., Ungar, L., and Pennock, D. Methods and Metrics for Cold-Start Recommendation. In Proc. SIGIR (2002), pp. 253-260. 259",
    "original_translation": "Scouts, promotores y conectores: los roles de las clasificaciones en el Filtrado Collaborativo de Bharath Kumar Mohan del vecino más cercano, del Departamento de Bharath Kumar Mohan. del Instituto Indio de Ciencias de CSA Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Departamento de Ciencias de la Computación del Este de la Universidad de Michigan Ypsilanti, MI 48917, EE. UU.Virginia Tech, Blacksburg VA 24061, EE. UU. Naren@cs.vt.edu Resumen Los sistemas de recomendación de recomendaciones agregan las calificaciones individuales de los usuarios en predicciones de productos o servicios que podrían interesar a los visitantes. La calidad de este proceso de agregación afecta de manera crucial la experiencia del usuario y, por lo tanto, la efectividad de los recomendadores en el comercio electrónico. Presentamos un estudio novedoso que desagrega las métricas de rendimiento de recomendaciones globales en las contribuciones realizadas por cada calificación individual, lo que nos permite caracterizar los muchos roles desempeñados por las calificaciones en el filtrado colaborativo más cercano de Newbor. En particular, formulamos tres calificaciones de roles, promotores y conectores, que capturan cómo los usuarios reciben recomendaciones, cómo se recomiendan los elementos y cómo las calificaciones de estos dos tipos están conectadas (resp.). Estos roles encuentran usos directos para mejorar las recomendaciones para los usuarios, en una mejor orientación de elementos y, lo más importante, para ayudar a monitorear la salud del sistema en su conjunto. Por ejemplo, pueden usarse para rastrear la evolución de los vecindarios, identificar subespacios de calificación que no contribuyen (o contribuyen negativamente) al rendimiento del sistema, enumeren a los usuarios que están en peligro de irse y evaluar la susceptibilidad del sistema aataques como chelines. Argumentamos que los tres roles de calificación presentados aquí proporcionan primitivas amplias para administrar un sistema de recomendación y su comunidad. Categorías y descriptores de sujetos H.4.2 [Aplicaciones de sistemas de información]: Tipos de soporte de decisión de sistemas;J.4 [Aplicaciones informáticas]: Ciencias sociales y de comportamiento Algoritmos de términos generales, Factores humanos 1. Introducción Los sistemas de recomendación se han vuelto integrales para el comercio electrónico, proporcionando tecnología que sugiere productos a un visitante basado en compras anteriores o historial de calificación. El filtrado colaborativo, una forma común de recomendación, predice una calificación de usuarios para un elemento combinando (otras) calificaciones de ese usuario con las calificaciones de otros usuarios. Se han realizado una investigación significativa en la implementación de algoritmos de filtrado colaborativos rápidos y precisos [2, 7], diseñando interfaces para presentar recomendaciones a los usuarios [1] y estudiar la robustez de estos algoritmos [8]. Sin embargo, con la excepción de algunos estudios sobre la influencia de los usuarios [10], se ha prestado poca atención al desentrañar el funcionamiento interno de un recomendador en términos de las calificaciones individuales y los roles que juegan al hacer (buenas) recomendaciones. Tal comprensión dará un mango importante para el monitoreo y la gestión de un sistema de recomendación, para diseñar mecanismos para mantener al recomendador y, por lo tanto, garantizar su éxito continuo. Nuestra motivación aquí es desglosar las métricas de rendimiento de recomendaciones globales en las contribuciones hechas por cada calificación individual, lo que nos permite caracterizar los muchos roles desempeñados por las clasificaciones en el filtrado colaborativo de vecino más cercano. Identificamos tres posibles roles: (Scouts) para conectar al usuario al sistema para recibir recomendaciones, (promotores) para conectar un elemento al sistema a recomendar y (conectores) para conectar las calificaciones de estos dos tipos. Viendo las calificaciones de esta manera, podemos definir la contribución de una calificación en cada rol, tanto en términos de permitir que ocurran recomendaciones como en términos de influencia en la calidad de las recomendaciones. A su vez, esta capacidad ayuda a apoyar escenarios como: 1. Situando usuarios en mejores vecindarios: las calificaciones de los usuarios pueden conectar inadvertidamente al usuario a un vecindario para el que los gustos de los usuarios pueden no ser una coincidencia perfecta. Identificar las calificaciones responsables de tales malas recomendaciones y sugerir nuevos elementos para calificar puede ayudar a ubicar al usuario en un mejor vecindario.2. Elementos de orientación: los sistemas de recomendación sufren de falta de participación del usuario, especialmente en escenarios de arranque en frío [13] que involucran artículos recién llegados. Identificar a los usuarios que pueden ser alentados a calificar elementos específicos ayuda a garantizar la cobertura del sistema de recomendación.3. Monitoreo de la evolución del sistema de recomendación y sus partes interesadas: un sistema de recomendación está constantemente bajo cambio: crecer con nuevos usuarios y 250 elementos, reducirse con los usuarios que abandonan el sistema, los elementos se vuelven irrelevantes y partes del sistema bajo ataque. El seguimiento de los roles de una calificación y su evolución a lo largo del tiempo proporciona muchas ideas sobre la salud del sistema y cómo podría administrarse y mejorar. Estos incluyen poder identificar subespacios de calificación que no contribuyen (o contribuyen negativamente) al rendimiento del sistema, y podrían eliminarse;enumerar a los usuarios que están en peligro de irse o dejar el sistema;y para evaluar la susceptibilidad del sistema a ataques como el chelín [5]. Como mostramos, la caracterización de los roles de calificación presentados aquí proporciona primitivas amplias para administrar un sistema de recomendación y su comunidad. El resto del documento está organizado de la siguiente manera. Los antecedentes sobre el filtrado colaborativo de vecino más cercano y la evaluación de algoritmos se analizan en la Sección 2. La Sección 3 define y discute los roles de una calificación, y la Sección 4 define las medidas de la contribución de una calificación en cada uno de estos roles. En la Sección 5, ilustramos el uso de estos roles para abordar los objetivos descritos anteriormente.2. Antecedentes 2.1 Algoritmos más cercanos Los algoritmos de filtrado colaborativo de vecino más cercano usan vecindarios de usuarios o vecindarios de elementos para calcular una predicción. Un algoritmo del primer tipo se llama basado en el usuario, y uno de los segundos se llama elemento [12]. En ambas familias de algoritmos, los vecindarios se forman primero calculando la similitud entre todos los pares de usuarios (para usuarios) o elementos (para artículos basados en elementos). Las predicciones se calculan luego agregando las calificaciones, que en un algoritmo basado en el usuario implica agregar las calificaciones del elemento objetivo por los vecinos de los usuarios y, en un algoritmo basado en elementos, implica agregar las calificaciones de los usuarios de elementos que son vecinos del elemento objetivo. Los algoritmos dentro de estas familias difieren en la definición de similitud, formación de vecindarios y el cálculo de las predicciones. Consideramos un algoritmo basado en el usuario basado en el definido para Grouplens [11] con variaciones de Herlocker et al.[2], y un algoritmo basado en elementos similar al de Sarwar et al.[12]. El algoritmo utilizado por Resnick et al.[11] define la similitud de dos usuarios U y V como la correlación de Pearson de sus calificaciones comunes: Sim (U, V) = P i∈Iu∩iv (Ru, I - ¯ru) (RV, I - ¯RV)qp i∈Iu (ru, i - ¯ru) 2 qp i∈Iv (rv, i - ¯rv) 2, donde iu es el conjunto de elementos clasificados por el usuario u, ru, es la calificación del usuario para el elemento i,y ¯RU es la calificación promedio de User U (de manera similar para V). La similitud calculada de esta manera generalmente se escala por un factor proporcional al número de clasificaciones comunes, para reducir la posibilidad de hacer una recomendación hecha en conexiones débiles: Sim (U, V) = Max (| IU ∩ IV |, γ) γ· Sim (U, V), donde γ ≈ 5 es una constante utilizada como límite inferior en la escala [2]. Estas nuevas similitudes se utilizan para definir una NU de vecindad estática para cada usuario U que consta de los principales usuarios de K más similares al usuario U. Una predicción para el usuario u y el elemento I se calcula por un promedio ponderado de las calificaciones por los vecinos pu, i = ¯ru + p v∈V sim (u, v) (rv, i - ¯rv) p v∈V sim(u, v) (1) donde v = nu ∩ ui es el conjunto de usuarios más similares a los que han calificado i. El algoritmo basado en elementos que utilizamos es el definido por Sarwar et al.[12]. En este algoritmo, la similitud se define como la medida del coseno ajustado SIM (I, J) = P u∈Ui∩uj (ru, i - ¯ru) (ru, j - ¯ru) qp u∈Ui (ru, i -¯ru) 2 qp u∈UJ (ru, j - ¯ru) 2 (2) donde UI es el conjunto de usuarios que tienen el elemento nominal i. En cuanto al algoritmo basado en el usuario, los pesos de similitud se ajustan proporcionalmente al número de usuarios que han calificado los elementos en SIM (i, j) = max (| ui ∩ uj |, γ) γ · SIM (I, J).(3) Dadas las similitudes, el Ni del vecindario de un artículo I se define como los mejores elementos K más similares para ese artículo. Una predicción para el usuario U y el elemento I se calcula como el promedio ponderado PU, i = ¯ri + p j∈J sim (i, j) (ru, j - ¯rj) p j∈J sim (i, j) (4) donde j = ni ∩ iu es el conjunto de elementos clasificados por u que son más similares a i.2.2 Evaluación Los algoritmos de recomendación se han evaluado típicamente utilizando medidas de precisión predictiva y cobertura [3]. Estudios sobre algoritmos de recomendación, especialmente Herlocker et al.[2] y Sarwar et al.[12], típicamente calcula la precisión predictiva dividiendo un conjunto de clasificaciones en conjuntos de entrenamiento y prueba, y calcula la predicción de un elemento en el conjunto de pruebas utilizando las clasificaciones en el conjunto de entrenamiento. Una medida estándar de precisión predictiva es el error absoluto medio (MAE), que para un conjunto de prueba T = {(u, i)} se define como, mae = p (u, i) ∈T | pu, i - ru, i i|| T |.(5) La cobertura tiene una serie de definiciones, pero generalmente se refiere a la proporción de elementos que pueden predecir el algoritmo [3]. Un problema práctico con la precisión predictiva es que los usuarios generalmente reciben listas de recomendaciones y no predicciones numéricas individuales. Las listas de recomendaciones son listas de elementos en orden decreciente de predicción (a veces establecido en términos de ratas de estrellas), por lo que la precisión predictiva puede no reflejar la precisión de la lista. Entonces, en su lugar, podemos medir la recomendación o la precisión de rango, lo que indica el grado en que la lista está en el orden correcto. Herlocker et al.[3] discuten una serie de medidas de precisión de rango, que van desde Kendalls Tau hasta medidas que consideran el hecho de que los usuarios tienden a mirar solo un prefijo de la lista [5]. Kendalls Tau mide el número de inversiones al comparar pares ordenados en el verdadero pedido de usuarios de 251 Jim Tom Jeff mi primo Vinny The Matrix Star Wars The Mask Figura 1: Calificaciones en una simple recomendación de películas.ítems y el orden recomendado, y se define como τ = c - d p (c + d + tr) (c + d + tp) (6) donde c es el número de pares que el sistema predice en el orden correcto, DEl número de pares que predice el sistema en el orden incorrecto, TR el número de pares en el pedido verdadero que tienen las mismas clasificaciones, y TP es el número de pares en el orden previsto que tienen las mismas clasificaciones [3]. Una deficiencia de la métrica Tau es que es ajeno a la posición en la lista ordenada donde ocurre la inversión [3]. Por ejemplo, una inversión hacia el final de la lista tiene el mismo peso que uno al principio. Una solución es considerar las inversiones solo en los pocos elementos principales de la lista recomendada o las inversiones de peso en función de su posición en la lista.3. Los roles de una calificación de nuestra observación básica es que cada calificación juega un papel diferente en cada predicción en la que se usa. Considere un sistema de recomendación de películas simplificado con tres usuarios Jim, Jeff y Tom y sus calificaciones para algunas películas, como se muestra en la Fig. 1. (Para esta discusión inicial no consideraremos los valores de calificación involucrados). El recomendador predice si a Tom le gustará la máscara utilizando las otras calificaciones ya disponibles. Cómo se hace esto depende del algoritmo: 1. Un algoritmo de filtrado colaborativo basado en elementos construye un vecindario de películas alrededor de la máscara utilizando las clasificaciones de usuarios que calificaron la máscara y otras películas de manera similar (por ejemplo, clasificaciones de Jims de la matriz y la máscara; y las clasificaciones de Jeff de Star Wars y la máscara). Las clasificaciones de Toms de esas películas se utilizan para hacer una predicción para la máscara.2. Un algoritmo de filtrado colaborativo basado en el usuario construiría un vecindario alrededor de Tom al rastrear a otros usuarios cuyos comportamientos de calificación son similares a Toms (por ejemplo, Tom y Jeff han calificado Star Wars; Tom y Jim han calificado la matriz). La predicción de la clasificación TOMS para la máscara se basa en las clasificaciones de Jeff y Tim. Aunque los algoritmos del vecino más cercano agregan las clasificaciones para formar vecindarios utilizados para calcular predicciones, podemos desglosar las similitudes para ver el cálculo de una predicción como simultáneamente siguiendo las rutas paralelas de las calificaciones. Por lo tanto, independientemente del algoritmo de filtrado colaborativo utilizado, podemos visualizar la predicción de la clasificación TOMS de la máscara como caminar a través de una secuencia de clasificaciones. En Jim Tom Jeff, la Matrix Star Wars, la máscara Q1 Q2 Q2 P1 P2 P3 Figura 2: Calificaciones utilizadas para predecir la máscara para Tom. Jim Tom Jeff The Matrix Star Wars The Mask Q1 Q2 Q3 P1 P2 P3 Jerry R2 R3 Figura 3: Predicción de la máscara para Tom en la que se usa una calificación más de una vez.Este ejemplo, se usaron dos rutas para esta predicción como se muestra en la Fig. 2: (p1, p2, p3) y (Q1, Q2, Q3). Tenga en cuenta que estos caminos no están dirigidos y son todos de longitud 3. Solo el orden en que se atraviesan las clasificaciones es diferente entre el algoritmo basado en elementos (por ejemplo, (p3, p2, p1), (Q3, Q2, Q1)) y el algoritmo basado en el usuario (por ejemplo, (P1, P2,P3), (Q1, Q2, Q3).) Una calificación puede ser parte de muchas rutas para una sola predicción como se muestra en la Fig. 3, donde se usan tres rutas para una predicción, dos de los cuales siguen P1: (P1, P2, P3) y (P1, R2, R3). Las predicciones en un algoritmos de filtrado colaborativo pueden involucrar miles de tales caminatas en paralelo, cada una jugando un papel en la influencia del valor predicho. Cada ruta de predicción consta de tres calificaciones, que juegan roles que llamamos exploradores, promotores y conectores. Para ilustrar estos roles, considere la ruta (p1, p2, p3) en la Fig. 2 utilizada para hacer una predicción de la máscara para Tom: 1. La calificación P1 (Tom → Star Wars) establece una conexión de Tom a otras clasificaciones que se pueden usar para predecir la calificación TOMS para la máscara. Esta calificación sirve como un explorador en el gráfico bipartito de las clasificaciones para encontrar un camino que conduzca a la máscara.2. La calificación P2 (Jeff → Star Wars) ayuda al sistema a recomendar la máscara a Tom conectando el Scout al promotor.3. La calificación P3 (Jeff → la máscara) permite conexiones a la máscara y, por lo tanto, promueve esta película a Tom. Formalmente, dada una predicción PU, un elemento objetivo A para el usuario U, un explorador para PU, A es una calificación RU, yo tal que existe un usuario V con calificaciones RV, A y RV, para algunos elementos I;Un promotor para PU, A es un RV de calificación, A para algún usuario V, de modo que existan calificaciones RV, I y Ru, I para un artículo I y;Un conector para PU, un 252 Jim Tom Jeff Jerry, mi primo Vinny, el Matrix Star Wars The Mask Jurasic Park Figura 4: Scouts, promotores y conectores.es una calificación RV, I por algún usuario V y calificación I, de modo que exista las calificaciones Ru, I y RV, a. Los exploradores, conectores y promotores para la predicción de la clasificación TOMS de la máscara son P1 y Q1, P2 y Q2, y P3 y Q3 (respectivamente). Cada uno de estos roles tiene un valor en el recomendador para el usuario, el vecindario de los usuarios y el sistema en términos de permitir que se hagan recomendaciones.3.1 Roles en las clasificaciones detalladas que actúan como Scouts tienden a ayudar al sistema de recomendación sugerir más películas para el usuario, aunque la medida en que esto es cierto depende del comportamiento de calificación de otros usuarios. Por ejemplo, en la Fig. 4, la calificación Tom → Star Wars ayuda al sistema a recomendarle solo la máscara, mientras que Tom → la matriz ayuda a recomendar la máscara, Jurassic Park y mi primo Vinny. Tom hace una conexión con Jim, que es un usuario prolífico del sistema, calificando la matriz. Sin embargo, esto no hace que la matriz sea la mejor película para calificar para todos. Por ejemplo, Jim se beneficia por igual por la máscara y la matriz, que permiten al sistema recomendarle Star Wars. Su calificación de la máscara es el mejor explorador para Jeff, y Jerrys Only Scout es su calificación de Star Wars. Esto sugiere que los buenos exploradores permiten a un usuario generar similitud con usuarios prolíficos y, por lo tanto, asegurarse de obtener más del sistema. Si bien los exploradores representan calificaciones beneficiosas desde la perspectiva de un usuario, los promotores son sus duales y son beneficiosos para los artículos. En la Fig. 4, mi primo Vinny se beneficia de la calificación de JIMS, ya que permite recomendaciones a Jeff y Tom. La máscara no depende tan de una sola calificación, ya que las calificaciones de Jim y Jeff la ayudan. Por otro lado, la calificación de Jerrys de Star Wars no ayuda a promoverla a ningún otro usuario. Concluimos que un buen promotor conecta un elemento con un vecindario más amplio de otros artículos y, por lo tanto, asegura que se recomiende a más usuarios. Los conectores cumplen un papel crucial en un sistema de recomendación que no es tan obvio. Las películas de mi primo Vinny y Jurassic Park tienen el potencial de recomendación más alto, ya que se pueden recomendar a Jeff, Jerry y Tom en función de la estructura de enlace ilustrada en la figura 4. Además del hecho de que Jim calificó estas películas, estas recomendaciones son posibles solo debido a las clasificaciones Jim → The Matrix y Jim → The Mask, que son los mejores conectores. Un conector mejora la capacidad de los sistemas para hacer recomendaciones sin ganancia explícita para el usuario. Tenga en cuenta que cada calificación puede ser de variado beneficio en cada uno de estos roles. La calificación Jim → Mi primo Vinny es un pobre explorador y conector, pero es un muy buen promotor. La calificación Jim → la máscara es un explorador razonablemente bueno, un muy buen conector y un buen promotor. Finalmente, la calificación Jerry → Star Wars es un muy buen explorador, pero no tiene ningún valor como conector o promotor. Como se ilustra aquí, una calificación puede tener un valor diferente en cada uno de los tres roles en términos de si se puede hacer una recomendación o no. Podríamos medir este valor simplemente contando el número de veces que se usa una calificación en cada rol, lo que solo sería útil para caracterizar el comportamiento de un sistema. Pero también podemos medir la contribución de cada calificación a la calidad de las recomendaciones o la salud del sistema. Dado que cada predicción es un esfuerzo combinado de varias rutas de recomendación, estamos interesados en discernir la influencia de cada calificación (y, por lo tanto, cada ruta) en el sistema hacia el error general de los sistemas. Podemos entender la dinámica del sistema con una granularidad más fina al rastrear la influencia de una calificación de acuerdo con el papel jugado. La siguiente sección describe el enfoque para medir los valores de una calificación en cada rol.4. Las contribuciones de las calificaciones, como hemos visto, una calificación puede desempeñar diferentes roles en diferentes predicciones y, en cada predicción, contribuir a la calidad de una predicción de diferentes maneras. Nuestro enfoque puede usar cualquier medida numérica de una propiedad de la salud del sistema y asigna crédito (o culpa) a cada calificación proporcional a su influencia en la predicción. Al rastrear el papel de cada calificación en una predicción, podemos acumular el crédito por una calificación en cada uno de los tres roles, y también rastrear la evolución de los roles de la calificación a lo largo del tiempo en el sistema. Esta sección define la metodología para calcular la contribución de las calificaciones definiendo primero la influencia de una calificación, y luego instanciando el enfoque para la precisión predictiva, y luego la precisión de rango. También demostramos cómo estas contribuciones pueden agregarse para estudiar el vecindario de las calificaciones involucradas en la calculación de las recomendaciones de los usuarios. Tenga en cuenta que aunque nuestra formulación general para la influencia de calificación es independiente del algoritmo, debido a las consideraciones de espacio, presentamos el enfoque solo para el filtrado colaborativo basado en elementos. La definición de algoritmos basados en el usuario es similar y se presentará en una versión ampliada de este documento.4.1 La influencia de las calificaciones recuerda que un enfoque basado en elementos para el filtrado colaborativo se basa en la construcción de vecindarios de elementos utilizando la similitud de las calificaciones por el mismo usuario. Como se describió anteriormente, la similitud se define por la medida de coseno ajustado (ecuaciones (2) y (3)). Se mantiene un conjunto de los mejores vecinos K para todos los elementos para el espacio y la eficiencia computacional. Una predicción del elemento I para un usuario U se calcula como la desviación ponderada de los elementos calificación media como se muestra en la ecuación (4). La lista de recomendaciones para un usuario es la lista de elementos ordenados en orden descendente de sus valores predichos. Primero definimos el impacto (A, I, J), el impacto que tiene un usuario A para determinar la similitud entre dos ítems I y J. Este es el cambio en la similitud entre I y J cuando se elimina la calificación AS, y se define como impacto (a, i, j) = | sim (i, j) - sim¯a (i, j) |P w∈Cij | sim (i, j) - sim ¯w (i, j) |donde cij = {u ∈ U |∃ ru, i, ru, j ∈ R (u)} es el conjunto de corras 253 de los elementos I y J (usuarios que califican tanto i como j), r (u) es el conjunto de calificaciones proporcionadas por el usuario u, ySim¯a (i, j) es la similitud de I y J cuando las calificaciones del usuario A se eliminan Sim¯a (i, j) = P v∈U \\ {a} (ru, i - ¯ru) (ru, j - ¯ru) qp u∈U \\ {a} (ru, i - ¯ru) 2 qp u∈U \\ {a} (ru, j - ¯ru) 2, y ajustado para el número de evaluadores sim¯a (i, j) = max (| ui ∩ uj | - 1, γ) γ · sim (i, j). Si todas las coratas de I y J califican de manera idéntica, definimos el impacto como impacto (A, I, J) = 1 | CIJ |Dado que p w∈Cij | sim (i, j) - sim ¯w (i, j) |= 0. La influencia de cada ruta (u, j, v, i) = [ru, j, rv, j, rv, i] en la predicción de PU, i es dada por influencia (u, j, v, i) = sim(i, j) p l∈Ni∩iu sim (i, l) · impacto (v, j) se deduce que la suma de influencias en todas esas rutas, para un conjunto determinado de puntos finales, es 1. 4.2Valores para la precisión predictiva El valor de una calificación en cada rol se calcula a partir de la influencia dependiendo de la medida de evaluación empleada. Aquí ilustramos el enfoque utilizando la precisión predictiva como métrica de evaluación. En general, la bondad de una predicción decide si las calificaciones involucradas deben ser acreditadas o desacreditadas por su papel. Para la precisión predictiva, el error en la predicción e = | pu, i - ru, i |se asigna a un nivel de comodidad utilizando una función de mapeo M (e). La evidencia anecdótica sugiere que los usuarios no pueden discernir errores inferiores a 1.0 (para una escala de calificación de 1 a 5) [4], por lo que un error inferior a 1.0 se considera aceptable, pero cualquier cosa más grande no lo es. Por lo tanto, definimos m (e) como (1 - e) agrupado a un valor apropiado en [−1, −0.5, 0.5, 1]. Para cada predicción PU, I, M (e) se atribuye a todos los caminos que ayudaron al cálculo de PU, I, proporcional a sus influencias. Este tributo, m (e) ∗ influencia (u, j, v, i), es a su vez heredado por cada una de las calificaciones en el camino [ru, j, rv, j, rv, i], con el crédito/culpaacumulando a los roles respectivos de Ru, J como Scout, RV, J como conector y RV, yo como promotor. En otras palabras, el valor Scout SF (Ru, J), el valor del conector CF (RV, J) y el valor del promotor PF (RV, I) están incrementados por el monto del tributo. En una gran cantidad de predicciones, los exploradores que han resultado repetidamente en grandes tasas de error tienen un gran valor de exploración negativa, y viceversa (de manera similar con los otros roles). Cada calificación se resume así por su triple [SF, CF, PF].4.3 Valores de roles para la precisión de rango ahora definimos el cálculo de la contribución de las calificaciones a la precisión de rango observada. Para este cálculo, debemos conocer el orden de preferencia de los usuarios para un conjunto de elementos para los cuales se pueden calcular predicciones. Suponemos que tenemos un conjunto de pruebas de las calificaciones de los usuarios de los elementos presentados en la lista de recomendaciones. Para cada par de elementos calificados por un usuario en los datos de la prueba, verificamos si el orden predicho es concordante con su preferencia. Decimos que un par (i, j) es concordante (con error) cada vez que uno de los siguientes se mantiene: • if (ru, i <ru, j) entonces (pu, i - pu, j <);• if (ru, i> ru, j) entonces (pu, i - pu, j>);o • if (ru, i = ru, j) entonces (| pu, i - pu, j | ≤). Del mismo modo, un par (i, j) es discordante (con error) si no es concordante. Nuestros experimentos descritos a continuación utilizan una tolerancia de error de = 0.1. Se acreditan todos los caminos involucrados en la predicción de los dos elementos en un par concordante, y los caminos involucrados en un par discordante están desacreditados. El crédito asignado a un par de elementos (i, j) en la lista de recomendaciones para el usuario u se calcula como c (i, j) = (t t · 1 c+d if (i, j) son concordantes - t t ·1 c+d if (i, j) son discordantes (7) donde t es el número de elementos en el conjunto de pruebas de usuarios cuyas calificaciones podrían ser predicha, t es el número de elementos calificados por el usuario u en el conjunto de pruebas, c esEl número de concordancias y D es el número de discordancias. El crédito C se divide entre todos los caminos responsables de predecir PU, I y PU, J proporcional a sus influencias. Nuevamente agregamos los valores de roles obtenidos de todos los experimentos para formar un triple [SF, CF, PF] para cada calificación.4.4 Agregando roles de calificación Después de calcular los valores de roles para las calificaciones individuales, también podemos usar estos valores para estudiar vecindarios y el sistema. Aquí consideramos cómo podemos usar los valores de roles para caracterizar la salud de un vecindario. Considere la lista de las principales recomendaciones presentadas a un usuario en un momento específico. El algoritmo de filtrado colaborativo atravesó muchos caminos en su vecindario a través de sus exploradores y otros conectores y promotores para hacer estas recomendaciones. Llamamos a estas calificaciones el vecindario de recomendación del usuario. El usuario elige implícitamente este vecindario de calificaciones a través de los artículos que califica. Además del algoritmo de filtrado colaborativo, la salud de este vecindario influye por completo en la satisfacción de los usuarios con el sistema. Podemos caracterizar un vecindario de recomendación de usuarios agregando los valores de roles individuales de las calificaciones involucradas, ponderadas por la influencia de las calificaciones individuales para determinar su lista recomendada. Diferentes secciones del vecindario de los usuarios ejercieron una influencia variada en su lista de recomendaciones. Por ejemplo, las calificaciones accesibles a través de elementos altamente calificados tienen una voz mayor en los elementos recomendados. Nuestro objetivo es estudiar el sistema y clasificar a los usuarios con respecto a su posicionamiento en un vecindario saludable o poco saludable. Un usuario puede tener un buen conjunto de exploradores, pero puede estar expuesto a un vecindario con malos conectores y promotores. Puede tener un buen vecindario, pero sus malos exploradores pueden garantizar que el potencial de los vecindarios se vuelva inútil. Esperamos que los usuarios con buenos exploradores y buenos vecindarios estén más satisfechos con el sistema en el futuro. Un vecindario de usuarios se caracteriza por un triple que representa la suma ponderada de los valores de roles de las calificaciones individuales involucradas en la realización de recomendaciones. Considere un usuario U y su lista ordenada de recomendaciones L. Un elemento I 254 en la lista se ponderó inversamente, como K (i), dependiendo de su posición en la lista. En nuestros estudios usamos K (i) = P Posición (I). Varios caminos de calificaciones [Ru, J, RV, J, RV, I] están involucrados en la predicción de PU, I, que finalmente decide su posición en L, cada una con influencia (U, J, V, I). El vecindario de recomendación de un usuario U se caracteriza por el triple, [SFN (U), CFN (U), Pfn (U)] donde Sfn (u) = x i∈L p [ru, j, rv, j, rv,I] sf (ru, j) influencia (u, j, v, i) k (i)! CFN (U) y PFN (U) se definen de manera similar. Este triple estima la calidad de las recomendaciones estadounidenses basadas en el historial pasado de las calificaciones involucradas en sus respectivos roles.5. Experimentación Como hemos visto, podemos asignar valores de roles a cada calificación al evaluar un sistema de filtrado colaborativo. En esta sección, demostramos el uso de este enfoque de nuestro objetivo general de definir un enfoque para monitorear y administrar la salud de un sistema de recomendación a través de experimentos realizados en el conjunto de datos de calificación de Mobielens Million. En particular, discutimos los resultados relacionados con la identificación de buenos exploradores, promotores y conectores;la evolución de los roles de calificación;y la caracterización de los vecindarios de los usuarios.5.1 Metodología Nuestros experimentos usan el conjunto de datos de calificación de Movielens Million, que consiste en calificaciones de 6040 usuarios de 3952 películas. Las clasificaciones están en el rango 1 a 5, y están etiquetadas con el tiempo que se dio la calificación. Como se discutió anteriormente, consideramos solo el algoritmo basado en ítems aquí (con vecindarios de elementos del tamaño 30) y, debido a consideraciones de espacio, solo los resultados del valor del rol presentan para la precisión de rango. Dado que estamos interesados en la evolución de los valores de rol de calificación a lo largo del tiempo, el modelo del sistema de recomendación se basa en las calificaciones de procesamiento en su orden de llegada. La campaña de tiempo proporcionada por Movielens es, por lo tanto, crucial para los análisis presentados aquí. Hacemos evaluaciones de roles de calificación a intervalos de 10,000 calificaciones y procesamos las primeras 200,000 calificaciones en el conjunto de datos (dando lugar a 20 instantáneas). Actualizamos incrementalmente los valores de roles a medida que las clasificaciones ordenadas se fusionan en el modelo. Para mantener el experimento computacionalmente manejable, definimos un conjunto de datos de prueba para cada usuario. A medida que las clasificaciones de tiempo ordenadas se fusionan en el modelo, etiquetamos un pequeño porcentaje seleccionado al azar (20%) como datos de prueba. En épocas discretas, es decir, después de procesar cada 10,000 calificaciones, calculamos las predicciones para las calificaciones en los datos de prueba y luego calculamos los valores de roles para las calificaciones utilizadas en las predicciones. Una posible crítica de esta metodología es que las calificaciones en el conjunto de pruebas nunca se evalúan para sus roles. Superamos esta preocupación repitiendo el experimento, usando diferentes semillas aleatorias. La probabilidad de que cada calificación se considera para la evaluación es considerablemente alta: 1 - 0.2N, donde n es el número de veces que el experimento se repite con diferentes semillas aleatorias. Los resultados aquí se basan en n = 4 repeticiones. El rendimiento de los algoritmos de filtrado colaborativos basados en elementos fue ordinario con respecto a la precisión de rango. La figura 5 muestra una gráfica de la precisión y el recuerdo, ya que las clasificaciones se fusionaron en el orden de tiempo en el modelo. El retiro siempre fue alto, pero la precisión promedio era de aproximadamente 53%.0 0.2 0.4 0.6 0.8 1 1.2 10000 30000 50000 70000 90000110000130000150000 Clasificaciones fusionadas en el valor del modelo Recuerdo de precisión Figura 5: Precisión y recuperación para el algoritmo de filtrado colaborativo basado en elementos.5.2 Inducir buenos exploradores Las calificaciones de un usuario que sirven como exploradores son aquellas que permiten al usuario recibir recomendaciones. Afirmamos que los usuarios con calificaciones que tienen valores de exploración respetables estarán más felices con el sistema que aquellos con calificaciones con valores de exploración bajos. Tenga en cuenta que el algoritmo basado en elementos discutido aquí produce listas de recomendaciones con casi la mitad de los pares en la lista discordante de la preferencia de los usuarios. Sin embargo, no está claro si todos estos pares discordantes son observables por el usuario, sin duda, esto sugiere que es necesario poder dirigir a los usuarios a elementos cuyas calificaciones mejorarían las listas. La distribución de los valores de exploración para la mayoría de las calificaciones de los usuarios son gaussianos con media cero. La figura 6 muestra la distribución de frecuencia de los valores de exploración para un usuario de muestra en una instantánea dada. Observamos que una gran cantidad de calificaciones nunca sirven como exploradores para sus usuarios. Un escenario relatable es cuando Amazons Recomender hace sugerencias de libros o artículos basados en otros artículos que se compraron como regalos. Con comentarios de relevancia simple del usuario, tales calificaciones pueden aislarse como malos exploradores y descontarse de futuras predicciones. Se descubrió que la eliminación de exploradores malos valía la pena para los usuarios individuales, pero la mejora general del rendimiento fue solo marginal. Una pregunta obvia es si se pueden formar buenos exploradores calificando simplemente películas populares como lo sugieren Rashid et al.[9]. Muestran que una mezcla de popularidad y entropía de calificación identifica los mejores elementos para sugerir a los nuevos usuarios cuando se evalúa usando MAE. Después de su intuición, esperaríamos ver una mayor correlación entre la popularidad de la atención y los buenos exploradores. Medimos el coeficiente de correlación de Pearson entre los valores de exploración agregados para una película con la popularidad de una película (número de veces que está clasificada);y con su popularidad*medida de varianza en diferentes instantáneas del sistema. Tenga en cuenta que los valores de los exploradores estaban inicialmente anticorrelacionados con popularidad (Fig. 7), pero se correlacionó moderadamente a medida que el sistema evolucionó. La varianza de popularidad y popularidad*se desempeñó de manera similar. Una posible explicación es que no ha habido tiempo suficiente para que las películas populares acumulen calificaciones.255-10 0 10 20 20 30 40 50 60 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 FRIDUENCIA DE VALOR DE SCOUT Figura 6: Distribución de valores de exploración para un usuario de muestra.-0.4 -0.2 0 0.2 0.4 0.6 0.8 30000 60000 90000 120000 150000 180000 Popularidad Pop*VAR Figura 7: Correlación entre el valor de exploración agregado y la popularidad del artículo (calculado a diferentes intervalos).-0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 30000 60000 90000 120000 150000 180000 Figura 8: Correlación entre el valor del promotor agregado y la prolificidad del usuario (calculado a diferentes intervalos). Tabla 1: Películas que forman los mejores exploradores. Siendo John Malkovich (1999) 1.00 445 Star Wars: Episodio IV - A New Hope (1977) 0.92 623 Princess Bride, The (1987) 0.85 477 Sixth Sense, The (1999) 0.85 617 Matrix, The (1999) 0.77 522 Ghostbusters (1984) 0.77 441 Casablanca (1942) 0.77 384 Insider, The (1999) 0.77 235 American Beauty (1999) 0.69 624 Terminator 2: Judgment Day (1991) 0.69 503 Fight Club (1999) 0.69 235 Shawshank Redemption, The (1994) 0.69445 Run Lola Run (Lola Rennt) (1998) 0.69 220 Terminator, The (1984) 0.62 450 Sospechosos habituales, The (1995) 0.62 326 Aliens (1986) 0.62 385 Norte por Noroeste (1959) 0.62 245 Fugitivo, el (1993)0.62 402 Fin de los días (1999) 0.62 132 Raiders of the Lost Ark (1981) 0.54 540 Lista de Schindlers (1993) 0.54 453 Volver al futuro (1985) 0.54 543 Toy Story (1995) 0.54 419 Alien (1979) 0.54 415 Abys Abys, The (1989) 0.54 345 2001: A Space Odyssey (1968) 0.54 358 Dogma (1999) 0.54 228 Little Mermaid, The (1989) 0.54 203 Tabla 2: Películas que forman los peores exploradores. Harold y Maude (1971) 0.46 141 Grifters, The (1990) 0.46 180 Sting, The (1973) 0.38 244 Godfather: Parte III, The (1990) 0.38 154 Lawrence of Arabia (1962) 0.38 167 High Noon (1952) 0.38 84Mujeres al borde de A ... (1988) 0.38 113 uvas de ira, The (1940) 0.38 115 Duck Soup (1933) 0.38 131 Arsénico y Old Lace (1944) 0.38 138 Vaquero de medianoche (1969) 0.38 137 para matar AMockingbird (1962) 0.31 195 Cuatro bodas y un funeral (1994) 0.31 271 Good, The Bad and the Ugly, The (1966) 0.31 156 Es una vida maravillosa (1946) 0.31 146 Jugador, The (1992) 0.31 220 Jackie Brown (1997) 0.31 118 Boat, The (Das Boot) (1981) 0.31 210 Manhattan (1979) 0.31 158 Verdad Acerca de Cats & Dogs, The (1996) 0.31 143 Ghost (1990) 0.31 227 Lone Star (1996) 0.31 125 Big Chill,El (1983) 0.31 184 256 Al estudiar la evolución de los valores de los exploradores, podemos identificar películas que aparecen constantemente en buenos exploradores con el tiempo. Afirmamos que estas películas harán exploradores viables para otros usuarios. Encontramos los valores de exploración agregados para todas las películas en intervalos de 10,000 calificaciones cada uno. Se dice que una película induce un buen explorador si la película estaba en el top 100 de la lista ordenada, e induce un mal explorador si estaba en los 100 inferiores de la misma lista. Se espera que las películas que aparecen constantemente altas con el tiempo permanecen allí en el futuro. La confianza efectiva en una película m es cm = tm - bm n (8) donde tm es el número de veces que apareció en el top 100, el número de veces que apareció en las 100 inferiores, y n es el número de intervalosconsideró. Usando esta medida, las mejores películas que se esperan inducir los mejores exploradores se muestran en la Tabla 1. Las películas que serían las opciones de Scouts se muestran en la Tabla 2 con sus confidencias asociadas. También se muestran las popularidades de las películas. Aunque las películas más populares aparecen en la lista de buenos exploradores, estas tablas muestran que una elección a ciegas de explorador basada solo en la popularidad puede ser potencialmente peligrosa. Curiosamente, el mejor explorador John Malkovich-se trata de un titiritero que descubre un portal en una estrella de cine, una película que se ha descrito de manera diversa en Amazon.com como te hace sentir vertiginoso, muy extraño, comedia con profundidad, tonta, extraña, e inventivo. Indicando si a alguien le gusta esta película o no es muy útil para situar al usuario en un vecindario adecuado, con preferencias similares. Por otro lado, varios factores pueden haber hecho de una película un mal explorador, como la fuerte variación en las preferencias de los usuarios en el vecindario de una película. Dos usuarios pueden tener la misma opinión sobre Lawrence of Arabia, pero pueden diferir considerablemente sobre cómo se sintieron sobre las otras películas que vieron. Se producen exploradores malos cuando hay una desviación en el comportamiento en torno a un punto de sincronización común.5.3 La inducción de buenas clasificaciones de promotores que sirven para promover elementos en un sistema de filtrado colaborativo son fundamentales para permitir que se recomiende un nuevo elemento a los usuarios. Por lo tanto, inducir buenos promotores es importante para la recomendación de arranque en frío. Observamos que la distribución de frecuencia de los valores del promotor para las clasificaciones de películas de muestra también es gaussiana (similar a la Fig. 6). Esto indica que la promoción de una película se beneficia más por las calificaciones de algunos usuarios, y no se ven afectados por las calificaciones de la mayoría de los usuarios. Encontramos una fuerte correlación entre el número de calificaciones de los usuarios y su valor promotor agregado. La figura 8 muestra la evolución de la correlación de Pearson coeficiente entre la prolificidad de un usuario (número de calificaciones) versus su valor promotor agregado. Esperamos que las chelines conspicuas, al recomendar películas incorrectas a los usuarios, se desacrediten con los valores negativos del promotor agregado y deben ser identificables fácilmente. Dada esta observación, la regla obvia a seguir al introducir una nueva película es tenerla clasificada directamente por usuarios prolíficos que poseen altos valores de promotores agregados. Por lo tanto, se emite una nueva película en el vecindario de muchas otras películas que mejora su visibilidad. Sin embargo, tenga en cuenta que un usuario puede haber dejado de usar el sistema. El seguimiento de los valores del promotor de manera consistente permite que solo se consideren los usuarios recientes más activos.5.4 Inducir buenos conectores dada la forma en que se caracterizan los exploradores, conectores y promotores, se deduce que las películas que son parte de los mejores exploradores también son parte de los mejores conectores. Del mismo modo, los usuarios que constituyen los mejores promotores también forman parte de los mejores conectores. Los buenos conectores se inducen al garantizar que un usuario con un alto valor del promotor califique una película con un alto valor de exploración. En nuestros experimentos, encontramos que un rol más largo de calificación a menudo es como un conector. A menudo se ve una calificación con un valor de mal conector debido a que su usuario es un mal promotor o su película es un mal explorador. Dichas calificaciones se pueden eliminar del proceso de predicción para llevar mejoras marginales a las recomendaciones. En algunos experimentos seleccionados, observamos que eliminar un conjunto de conectores que se comportan mal ayudó a mejorar el rendimiento general de los sistemas en un 1,5%. El efecto fue aún mayor en algunos usuarios seleccionados que observaron una mejora de más del 10% en precisión sin mucha pérdida en el retiro.5.5 Monitoreo de la evolución de los roles de calificación Una de las contribuciones más significativas de nuestro trabajo es la capacidad de modelar la evolución de los sistemas de recomendación, al estudiar los roles cambiantes de las calificaciones a lo largo del tiempo. El papel y el valor de una calificación pueden cambiar dependiendo de muchos factores como el comportamiento del usuario, la redundancia, los efectos de chelín o las propiedades del algoritmo de filtrado colaborativo utilizado. Estudiar la dinámica de los roles de calificación en términos de transiciones entre valores buenos, malos y insignificantes puede proporcionar información sobre el funcionamiento del sistema de recomendación. Creemos que una visualización continua de estas transiciones mejorará la capacidad de administrar un sistema de recomendación. Clasificamos diferentes estados de calificación como buenos, malos o insignificantes. Considere a un usuario que ha calificado 100 películas en un intervalo particular, de las cuales 20 son parte del conjunto de pruebas. Si un explorador tiene un valor superior a 0.005, indica que está involucrado de manera única en al menos 2 predicciones concordantes, lo que diremos que es bueno. Por lo tanto, se elige un umbral de 0.005 para reducir una calificación como buena, mala o insignificante en términos de su explorador, conector y valor promotor. Por ejemplo, una calificación R, en el tiempo t con el valor de rol triple [0.1, 0.001, −0.01] se clasifica como [Scout +, conector 0, promotor -], donde + indica bueno, 0 indica negligible y - indica mal. El crédito positivo en poder de una calificación es una medida de su contribución al mejoramiento del sistema, y el desacredit es una medida de su contribución al detrimento del sistema. Aunque los roles positivos (y los roles negativos) representan un porcentaje muy pequeño de todas las calificaciones, su contribución reemplaza a su tamaño. Por ejemplo, a pesar de que solo el 1.7% de todas las calificaciones se clasificaron como buenos exploradores, ¡tienen el 79% de todo el crédito positivo en el sistema! Del mismo modo, los malos exploradores fueron solo el 1.4% de todas las calificaciones, pero poseen el 82% de todos los desacreditados. Tenga en cuenta que los exploradores buenos y malos, juntos, comprenden solo 1.4% + 1.7% = 3.1% de las calificaciones, lo que implica que la mayoría de las calificaciones son jugadores de roles insignificantes como exploradores (más sobre esto más adelante). Del mismo modo, los buenos conectores fueron 1.2% del sistema y poseen el 30% de todo el crédito positivo. Los conectores malos (0.8% del sistema) poseen el 36% de todos los desacreditados. Los buenos promotores (3% del sistema) poseen el 46% de todo el crédito, mientras que los promotores incobrables (2%) tienen el 50% de todos los desacreditados. Esto reitera que algunas calificaciones influyen en la mayoría del rendimiento del sistema. Por lo tanto, es importante rastrear las transiciones entre ellas independientemente de sus pequeños números.257 En diferentes instantáneas, una calificación puede permanecer en el mismo estado o cambio. Un buen explorador puede convertirse en un mal explorador, un buen promotor puede convertirse en un buen conector, los exploradores buenos y malos pueden volverse vestigiales, y así sucesivamente. No es práctico esperar que un sistema de recomendación no tenga calificaciones en los malos roles. Sin embargo, es suficiente ver que las calificaciones en roles malos se convierten en roles buenos o vestigiales. Del mismo modo, observar una gran cantidad de buenos roles se convierten en malos es un signo de falla inminente del sistema. Empleamos el principio de los episodios no superpuestos [6] para contar tales transiciones. Una secuencia como: [+, 0, 0] → [+, 0, 0] → [0,+, 0] → [0, 0, 0] se interpreta como las transiciones [+, 0, 0];[0, +, 0]: 1 [ +, 0, 0];[0, 0, 0]: 1 [0, +, 0];[0, 0, 0]: 1 en lugar de [+, 0, 0];[0, +, 0]: 2 [ +, 0, 0];[0, 0, 0]: 2 [0, +, 0];[0, 0, 0]: 1. Consulte [6] para obtener más detalles sobre este procedimiento de conteo. Por lo tanto, una calificación puede estar en uno de los 27 estados posibles, y hay alrededor de 272 transiciones posibles. Hacemos una simplificación adicional y utilizamos solo 9 estados, lo que indica si la calificación es un explorador, promotor o conector, y si tiene un papel positivo, negativo o insignificante. Las calificaciones que sirven múltiples propósitos se cuentan utilizando instancias de múltiples episodios, pero los estados mismos no están duplicados más allá de los 9 estados restringidos. En este modelo, una transición como [ +, 0, +];[0, +, 0]: 1 se cuenta como [Scout +];[Scout0]: 1 [Scout+];[conector+]: 1 [Scout+];[promotor0]: 1 [conector0];[Scout0]: 1 [conector0];[Scout+]: 1 [conector0];[promotor0]: 1 [promotor+];[Scout0]: 1 [promotor+];[conector+]: 1 [promotor+];[Promoter0]: 1 de estos, transiciones como [PX];[Q0] donde p = q, x ∈ {+, 0, -} se consideran poco interesantes, y solo se cuentan el resto. La figura 9 muestra las principales transiciones contadas mientras procesan las primeras 200,000 calificaciones del conjunto de datos Movielens. Solo se muestran transiciones con frecuencia mayor o igual al 3%. Los porcentajes para cada estado indican el número de calificaciones que se encontró en esos estados.",
    "original_sentences": [
        "Scouts, Promoters, and Connectors: The Roles of Ratings in Nearest Neighbor Collaborative Filtering Bharath Kumar Mohan Dept.",
        "of CSA Indian Institute of Science Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. of Computer Science Eastern Michigan University Ypsilanti, MI 48917, USA bkeller@emich.edu Naren Ramakrishnan Dept. of Computer Science Virginia Tech, Blacksburg VA 24061, USA naren@cs.vt.edu ABSTRACT Recommender systems aggregate individual user ratings into predictions of products or services that might interest visitors.",
        "The quality of this aggregation process crucially affects the user experience and hence the effectiveness of recommenders in e-commerce.",
        "We present a novel study that disaggregates global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearestneighbor collaborative filtering.",
        "In particular, we formulate three roles-scouts, promoters, and connectors-that capture how users receive recommendations, how items get recommended, and how ratings of these two types are themselves connected (resp.).",
        "These roles find direct uses in improving recommendations for users, in better targeting of items and, most importantly, in helping monitor the health of the system as a whole.",
        "For instance, they can be used to track the evolution of neighborhoods, to identify rating subspaces that do not contribute (or contribute negatively) to system performance, to enumerate users who are in danger of leaving, and to assess the susceptibility of the system to attacks such as shilling.",
        "We argue that the three rating roles presented here provide broad primitives to manage a recommender system and its community.",
        "Categories and Subject Descriptors H.4.2 [Information Systems Applications]: Types of Systems-Decision support; J.4 [Computer Applications]: Social and Behavioral Sciences General Terms Algorithms, Human Factors 1.",
        "INTRODUCTION Recommender systems have become integral to e-commerce, providing technology that suggests products to a visitor based on previous purchases or rating history.",
        "Collaborative filtering, a common form of recommendation, predicts a users rating for an item by combining (other) ratings of that user with other users ratings.",
        "Significant research has been conducted in implementing fast and accurate collaborative filtering algorithms [2, 7], designing interfaces for presenting recommendations to users [1], and studying the robustness of these algorithms [8].",
        "However, with the exception of a few studies on the influence of users [10], little attention has been paid to unraveling the inner workings of a recommender in terms of the individual ratings and the roles they play in making (good) recommendations.",
        "Such an understanding will give an important handle to monitoring and managing a recommender system, to engineer mechanisms to sustain the recommender, and thereby ensure its continued success.",
        "Our motivation here is to disaggregate global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearest-neighbor collaborative filtering.",
        "We identify three possible roles: (scouts) to connect the user into the system to receive recommendations, (promoters) to connect an item into the system to be recommended, and (connectors) to connect ratings of these two kinds.",
        "Viewing ratings in this way, we can define the contribution of a rating in each role, both in terms of allowing recommendations to occur, and in terms of influence on the quality of recommendations.",
        "In turn, this capability helps support scenarios such as: 1.",
        "Situating users in better neighborhoods: A users ratings may inadvertently connect the user to a neighborhood for which the users tastes may not be a perfect match.",
        "Identifying ratings responsible for such bad recommendations and suggesting new items to rate can help situate the user in a better neighborhood. 2.",
        "Targeting items: Recommender systems suffer from lack of user participation, especially in cold-start scenarios [13] involving newly arrived items.",
        "Identifying users who can be encouraged to rate specific items helps ensure coverage of the recommender system. 3.",
        "Monitoring the evolution of the recommender system and its stakeholders: A recommender system is constantly under change: growing with new users and 250 items, shrinking with users leaving the system, items becoming irrelevant, and parts of the system under attack.",
        "Tracking the roles of a rating and its evolution over time provides many insights into the health of the system, and how it could be managed and improved.",
        "These include being able to identify rating subspaces that do not contribute (or contribute negatively) to system performance, and could be removed; to enumerate users who are in danger of leaving, or have left the system; and to assess the susceptibility of the system to attacks such as shilling [5].",
        "As we show, the characterization of rating roles presented here provides broad primitives to manage a recommender system and its community.",
        "The rest of the paper is organized as follows.",
        "Background on nearest-neighbor collaborative filtering and algorithm evaluation is discussed in Section 2.",
        "Section 3 defines and discusses the roles of a rating, and Section 4 defines measures of the contribution of a rating in each of these roles.",
        "In Section 5, we illustrate the use of these roles to address the goals outlined above. 2.",
        "BACKGROUND 2.1 Algorithms Nearest-neighbor collaborative filtering algorithms either use neighborhoods of users or neighborhoods of items to compute a prediction.",
        "An algorithm of the first kind is called user-based, and one of the second kind is called itembased [12].",
        "In both families of algorithms, neighborhoods are formed by first computing the similarity between all pairs of users (for user-based) or items (for item-based).",
        "Predictions are then computed by aggregating ratings, which in a user-based algorithm involves aggregating the ratings of the target item by the users neighbors and, in an item-based algorithm, involves aggregating the users ratings of items that are neighbors of the target item.",
        "Algorithms within these families differ in the definition of similarity, formation of neighborhoods, and the computation of predictions.",
        "We consider a user-based algorithm based on that defined for GroupLens [11] with variations from Herlocker et al. [2], and an item-based algorithm similar to that of Sarwar et al. [12].",
        "The algorithm used by Resnick et al. [11] defines the similarity of two users u and v as the Pearson correlation of their common ratings: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2 , where Iu is the set of items rated by user u, ru,i is user us rating for item i, and ¯ru is the average rating of user u (similarly for v).",
        "Similarity computed in this manner is typically scaled by a factor proportional to the number of common ratings, to reduce the chance of making a recommendation made on weak connections: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), where γ ≈ 5 is a constant used as a lower limit in scaling [2].",
        "These new similarities are then used to define a static neighborhood Nu for each user u consisting of the top K users most similar to user u.",
        "A prediction for user u and item i is computed by a weighted average of the ratings by the neighbors pu,i = ¯ru + P v∈V sim (u, v)(rv,i − ¯rv) P v∈V sim (u, v) (1) where V = Nu ∩ Ui is the set of users most similar to u who have rated i.",
        "The item-based algorithm we use is the one defined by Sarwar et al. [12].",
        "In this algorithm, similarity is defined as the adjusted cosine measure sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) where Ui is the set of users who have rated item i.",
        "As for the user-based algorithm, the similarity weights are adjusted proportionally to the number of users that have rated the items in common sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). (3) Given the similarities, the neighborhood Ni of an item i is defined as the top K most similar items for that item.",
        "A prediction for user u and item i is computed as the weighted average pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) where J = Ni ∩ Iu is the set of items rated by u that are most similar to i. 2.2 Evaluation Recommender algorithms have typically been evaluated using measures of predictive accuracy and coverage [3].",
        "Studies on recommender algorithms, notably Herlocker et al. [2] and Sarwar et al. [12], typically compute predictive accuracy by dividing a set of ratings into training and test sets, and compute the prediction for an item in the test set using the ratings in the training set.",
        "A standard measure of predictive accuracy is mean absolute error (MAE), which for a test set T = {(u, i)} is defined as, MAE = P (u,i)∈T |pu,i − ru,i| |T | . (5) Coverage has a number of definitions, but generally refers to the proportion of items that can be predicted by the algorithm [3].",
        "A practical issue with predictive accuracy is that users typically are presented with recommendation lists, and not individual numeric predictions.",
        "Recommendation lists are lists of items in decreasing order of prediction (sometimes stated in terms of star-ratings), and so predictive accuracy may not be reflective of the accuracy of the list.",
        "So, instead we can measure recommendation or rank accuracy, which indicates the extent to which the list is in the correct order.",
        "Herlocker et al. [3] discuss a number of rank accuracy measures, which range from Kendalls Tau to measures that consider the fact that users tend to only look at a prefix of the list [5].",
        "Kendalls Tau measures the number of inversions when comparing ordered pairs in the true user ordering of 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figure 1: Ratings in simple movie recommender. items and the recommended order, and is defined as τ = C − D p (C + D + TR)(C + D + TP) (6) where C is the number of pairs that the system predicts in the correct order, D the number of pairs the system predicts in the wrong order, TR the number of pairs in the true ordering that have the same ratings, and TP is the number of pairs in the predicted ordering that have the same ratings [3].",
        "A shortcoming of the Tau metric is that it is oblivious to the position in the ordered list where the inversion occurs [3].",
        "For instance, an inversion toward the end of the list is given the same weight as one in the beginning.",
        "One solution is to consider inversions only in the top few items in the recommended list or to weight inversions based on their position in the list. 3.",
        "ROLES OF A RATING Our basic observation is that each rating plays a different role in each prediction in which it is used.",
        "Consider a simplified movie recommender system with three users Jim, Jeff, and Tom and their ratings for a few movies, as shown in Fig. 1. (For this initial discussion we will not consider the rating values involved.)",
        "The recommender predicts whether Tom will like The Mask using the other already available ratings.",
        "How this is done depends on the algorithm: 1.",
        "An item-based collaborative filtering algorithm constructs a neighborhood of movies around The Mask by using the ratings of users who rated The Mask and other movies similarly (e.g., Jims ratings of The Matrix and The Mask; and Jeffs ratings of Star Wars and The Mask).",
        "Toms ratings of those movies are then used to make a prediction for The Mask. 2.",
        "A user-based collaborative filtering algorithm would construct a neighborhood around Tom by tracking other users whose rating behaviors are similar to Toms (e.g., Tom and Jeff have rated Star Wars; Tom and Jim have rated The Matrix).",
        "The prediction of Toms rating for The Mask is then based on the ratings of Jeff and Tim.",
        "Although the nearest-neighbor algorithms aggregate the ratings to form neighborhoods used to compute predictions, we can disaggregate the similarities to view the computation of a prediction as simultaneously following parallel paths of ratings.",
        "So, irrespective of the collaborative filtering algorithm used, we can visualize the prediction of Toms rating of The Mask as walking through a sequence of ratings.",
        "In Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figure 2: Ratings used to predict The Mask for Tom.",
        "Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figure 3: Prediction of The Mask for Tom in which a rating is used more than once. this example, two paths were used for this prediction as depicted in Fig. 2: (p1, p2, p3) and (q1, q2, q3).",
        "Note that these paths are undirected, and are all of length 3.",
        "Only the order in which the ratings are traversed is different between the item-based algorithm (e.g., (p3, p2, p1), (q3, q2, q1)) and the user-based algorithm (e.g., (p1, p2, p3), (q1, q2, q3).)",
        "A rating can be part of many paths for a single prediction as shown in Fig. 3, where three paths are used for a prediction, two of which follow p1: (p1, p2, p3) and (p1, r2, r3).",
        "Predictions in a collaborative filtering algorithms may involve thousands of such walks in parallel, each playing a part in influencing the predicted value.",
        "Each prediction path consists of three ratings, playing roles that we call scouts, promoters, and connectors.",
        "To illustrate these roles, consider the path (p1, p2, p3) in Fig. 2 used to make a prediction of The Mask for Tom: 1.",
        "The rating p1 (Tom → Star Wars) makes a connection from Tom to other ratings that can be used to predict Toms rating for The Mask.",
        "This rating serves as a scout in the bipartite graph of ratings to find a path that leads to The Mask. 2.",
        "The rating p2 (Jeff → Star Wars) helps the system recommend The Mask to Tom by connecting the scout to the promoter. 3.",
        "The rating p3 (Jeff → The Mask) allows connections to The Mask, and, therefore, promotes this movie to Tom.",
        "Formally, given a prediction pu,a of a target item a for user u, a scout for pu,a is a rating ru,i such that there exists a user v with ratings rv,a and rv,i for some item i; a promoter for pu,a is a rating rv,a for some user v, such that there exist ratings rv,i and ru,i for an item i, and; a connector for pu,a 252 Jim Tom Jeff Jerry My Cousin Vinny The Matrix Star Wars The Mask Jurasic Park Figure 4: Scouts, promoters, and connectors. is a rating rv,i by some user v and rating i, such that there exists ratings ru,i and rv,a.",
        "The scouts, connectors, and promoters for the prediction of Toms rating of The Mask are p1 and q1, p2 and q2, and p3 and q3 (respectively).",
        "Each of these roles has a value in the recommender to the user, the users neighborhood, and the system in terms of allowing recommendations to be made. 3.1 Roles in Detail Ratings that act as scouts tend to help the recommender system suggest more movies to the user, though the extent to which this is true depends on the rating behavior of other users.",
        "For example, in Fig. 4 the rating Tom → Star Wars helps the system recommend only The Mask to him, while Tom → The Matrix helps recommend The Mask, Jurassic Park, and My Cousin Vinny.",
        "Tom makes a connection to Jim who is a prolific user of the system, by rating The Matrix.",
        "However, this does not make The Matrix the best movie to rate for everyone.",
        "For example, Jim is benefited equally by both The Mask and The Matrix, which allow the system to recommend Star Wars to him.",
        "His rating of The Mask is the best scout for Jeff, and Jerrys only scout is his rating of Star Wars.",
        "This suggests that good scouts allow a user to build similarity with prolific users, and thereby ensure they get more from the system.",
        "While scouts represent beneficial ratings from the perspective of a user, promoters are their duals, and are of benefit to items.",
        "In Fig. 4, My Cousin Vinny benefits from Jims rating, since it allows recommendations to Jeff and Tom.",
        "The Mask is not so dependent on just one rating, since the ratings by Jim and Jeff help it.",
        "On the other hand, Jerrys rating of Star Wars does not help promote it to any other user.",
        "We conclude that a good promoter connects an item to a broader neighborhood of other items, and thereby ensures that it is recommended to more users.",
        "Connectors serve a crucial role in a recommender system that is not as obvious.",
        "The movies My Cousin Vinny and Jurassic Park have the highest recommendation potential since they can be recommended to Jeff, Jerry and Tom based on the linkage structure illustrated in Fig. 4.",
        "Beside the fact that Jim rated these movies, these recommendations are possible only because of the ratings Jim → The Matrix and Jim → The Mask, which are the best connectors.",
        "A connector improves the systems ability to make recommendations with no explicit gain for the user.",
        "Note that every rating can be of varied benefit in each of these roles.",
        "The rating Jim → My Cousin Vinny is a poor scout and connector, but is a very good promoter.",
        "The rating Jim → The Mask is a reasonably good scout, a very good connector, and a good promoter.",
        "Finally, the rating Jerry → Star Wars is a very good scout, but is of no value as a connector or promoter.",
        "As illustrated here, a rating can have different value in each of the three roles in terms of whether a recommendation can be made or not.",
        "We could measure this value by simply counting the number of times a rating is used in each role, which alone would be helpful in characterizing the behavior of a system.",
        "But we can also measure the contribution of each rating to the quality of recommendations or health of the system.",
        "Since every prediction is a combined effort of several recommendation paths, we are interested in discerning the influence of each rating (and, hence, each path) in the system towards the systems overall error.",
        "We can understand the dynamics of the system at a finer granularity by tracking the influence of a rating according to the role played.",
        "The next section describes the approach to measuring the values of a rating in each role. 4.",
        "CONTRIBUTIONS OF RATINGS As weve seen, a rating may play different roles in different predictions and, in each prediction, contribute to the quality of a prediction in different ways.",
        "Our approach can use any numeric measure of a property of system health, and assigns credit (or blame) to each rating proportional to its influence in the prediction.",
        "By tracking the role of each rating in a prediction, we can accumulate the credit for a rating in each of the three roles, and also track the evolution of the roles of rating over time in the system.",
        "This section defines the methodology for computing the contribution of ratings by first defining the influence of a rating, and then instantiating the approach for predictive accuracy, and then rank accuracy.",
        "We also demonstrate how these contributions can be aggregated to study the neighborhood of ratings involved in computing a users recommendations.",
        "Note that although our general formulation for rating influence is algorithm independent, due to space considerations, we present the approach for only item-based collaborative filtering.",
        "The definition for user-based algorithms is similar and will be presented in an expanded version of this paper. 4.1 Influence of Ratings Recall that an item-based approach to collaborative filtering relies on building item neighborhoods using the similarity of ratings by the same user.",
        "As described earlier, similarity is defined by the adjusted cosine measure (Equations (2) and (3)).",
        "A set of the top K neighbors is maintained for all items for space and computational efficiency.",
        "A prediction of item i for a user u is computed as the weighted deviation from the items mean rating as shown in Equation (4).",
        "The list of recommendations for a user is then the list of items sorted in descending order of their predicted values.",
        "We first define impact(a, i, j), the impact a user a has in determining the similarity between two items i and j.",
        "This is the change in the similarity between i and j when as rating is removed, and is defined as impact(a, i, j) = |sim (i, j) − sim¯a(i, j)| P w∈Cij |sim (i, j) − sim ¯w(i, j)| where Cij = {u ∈ U | ∃ ru,i, ru,j ∈ R(u)} is the set of coraters 253 of items i and j (users who rate both i and j), R(u) is the set of ratings provided by user u, and sim¯a(i, j) is the similarity of i and j when the ratings of user a are removed sim¯a(i, j) = P v∈U\\{a} (ru,i − ¯ru)(ru,j − ¯ru) qP u∈U\\{a}(ru,i − ¯ru)2 qP u∈U\\{a}(ru,j − ¯ru)2 , and adjusted for the number of raters sim¯a(i, j) = max(|Ui ∩ Uj| − 1, γ) γ · sim(i, j).",
        "If all coraters of i and j rate them identically, we define the impact as impact(a, i, j) = 1 |Cij| since P w∈Cij |sim (i, j) − sim ¯w(i, j)| = 0.",
        "The influence of each path (u, j, v, i) = [ru,j, rv,j, rv,i] in the prediction of pu,i is given by influence(u, j, v, i) = sim (i, j) P l∈Ni∩Iu sim (i, l) · impact(v, i, j) It follows that the sum of influences over all such paths, for a given set of endpoints, is 1. 4.2 Role Values for Predictive Accuracy The value of a rating in each role is computed from the influence depending on the evaluation measure employed.",
        "Here we illustrate the approach using predictive accuracy as the evaluation metric.",
        "In general, the goodness of a prediction decides whether the ratings involved must be credited or discredited for their role.",
        "For predictive accuracy, the error in prediction e = |pu,i − ru,i| is mapped to a comfort level using a mapping function M(e).",
        "Anecdotal evidence suggests that users are unable to discern errors less than 1.0 (for a rating scale of 1 to 5) [4], and so an error less than 1.0 is considered acceptable, but anything larger is not.",
        "We hence define M(e) as (1 − e) binned to an appropriate value in [−1, −0.5, 0.5, 1].",
        "For each prediction pu,i, M(e) is attributed to all the paths that assisted the computation of pu,i, proportional to their influences.",
        "This tribute, M(e)∗influence(u, j, v, i), is in turn inherited by each of the ratings in the path [ru,j, rv,j, rv,i], with the credit/blame accumulating to the respective roles of ru,j as a scout, rv,j as a connector, and rv,i as a promoter.",
        "In other words, the scout value SF(ru,j), the connector value CF(rv,j) and the promoter value PF(rv,i) are all incremented by the tribute amount.",
        "Over a large number of predictions, scouts that have repeatedly resulted in big error rates have a big negative scout value, and vice versa (similarly with the other roles).",
        "Every rating is thus summarized by its triple [SF, CF, PF]. 4.3 Role Values for Rank Accuracy We now define the computation of the contribution of ratings to observed rank accuracy.",
        "For this computation, we must know the users preference order for a set of items for which predictions can be computed.",
        "We assume that we have a test set of the users ratings of the items presented in the recommendation list.",
        "For every pair of items rated by a user in the test data, we check whether the predicted order is concordant with his preference.",
        "We say a pair (i, j) is concordant (with error ) whenever one of the following holds: • if (ru,i < ru,j) then (pu,i − pu,j < ); • if (ru,i > ru,j) then (pu,i − pu,j > ); or • if (ru,i = ru,j) then (|pu,i − pu,j| ≤ ).",
        "Similarly, a pair (i, j) is discordant (with error ) if it is not concordant.",
        "Our experiments described below use an error tolerance of = 0.1.",
        "All paths involved in the prediction of the two items in a concordant pair are credited, and the paths involved in a discordant pair are discredited.",
        "The credit assigned to a pair of items (i, j) in the recommendation list for user u is computed as c(i, j) = ( t T · 1 C+D if (i, j) are concordant − t T · 1 C+D if (i, j) are discordant (7) where t is the number of items in the users test set whose ratings could be predicted, T is the number of items rated by user u in the test set, C is the number of concordances and D is the number of discordances.",
        "The credit c is then divided among all paths responsible for predicting pu,i and pu,j proportional to their influences.",
        "We again add the role values obtained from all the experiments to form a triple [SF, CF, PF] for each rating. 4.4 Aggregating rating roles After calculating the role values for individual ratings, we can also use these values to study neighborhoods and the system.",
        "Here we consider how we can use the role values to characterize the health of a neighborhood.",
        "Consider the list of top recommendations presented to a user at a specific point in time.",
        "The collaborative filtering algorithm traversed many paths in his neighborhood through his scouts and other connectors and promoters to make these recommendations.",
        "We call these ratings the recommender neighborhood of the user.",
        "The user implicitly chooses this neighborhood of ratings through the items he rates.",
        "Apart from the collaborative filtering algorithm, the health of this neighborhood completely influences a users satisfaction with the system.",
        "We can characterize a users recommender neighborhood by aggregating the individual role values of the ratings involved, weighted by the influence of individual ratings in determining his recommended list.",
        "Different sections of the users neighborhood wield varied influence on his recommendation list.",
        "For instance, ratings reachable through highly rated items have a bigger say in the recommended items.",
        "Our aim is to study the system and classify users with respect to their positioning in a healthy or unhealthy neighborhood.",
        "A user can have a good set of scouts, but may be exposed to a neighborhood with bad connectors and promoters.",
        "He can have a good neighborhood, but his bad scouts may ensure the neighborhoods potential is rendered useless.",
        "We expect that users with good scouts and good neighborhoods will be most satisfied with the system in the future.",
        "A users neighborhood is characterized by a triple that represents the weighted sum of the role values of individual ratings involved in making recommendations.",
        "Consider a user u and his ordered list of recommendations L. An item i 254 in the list is weighted inversely, as K(i), depending on its position in the list.",
        "In our studies we use K(i) = p position(i).",
        "Several paths of ratings [ru,j, rv,j, rv,i] are involved in predicting pu,i which ultimately decides its position in L, each with influence(u, j, v, i).",
        "The recommender neighborhood of a user u is characterized by the triple, [SFN(u), CFN(u), PFN(u)] where SFN(u) = X i∈L P [ru,j ,rv,j ,rv,i] SF(ru,j)influence(u, j, v, i) K(i) !",
        "CFN(u) and PFN(u) are defined similarly.",
        "This triple estimates the quality of us recommendations based on the past track record of the ratings involved in their respective roles. 5.",
        "EXPERIMENTATION As we have seen, we can assign role values to each rating when evaluating a collaborative filtering system.",
        "In this section, we demonstrate the use of this approach to our overall goal of defining an approach to monitor and manage the health of a recommender system through experiments done on the MovieLens million rating dataset.",
        "In particular, we discuss results relating to identifying good scouts, promoters, and connectors; the evolution of rating roles; and the characterization of user neighborhoods. 5.1 Methodology Our experiments use the MovieLens million rating dataset, which consists of ratings by 6040 users of 3952 movies.",
        "The ratings are in the range 1 to 5, and are labeled with the time the rating was given.",
        "As discussed before, we consider only the item-based algorithm here (with item neighborhoods of size 30) and, due to space considerations, only present role value results for rank accuracy.",
        "Since we are interested in the evolution of the rating role values over time, the model of the recommender system is built by processing ratings in their arrival order.",
        "The timestamping provided by MovieLens is hence crucial for the analyses presented here.",
        "We make assessments of rating roles at intervals of 10,000 ratings and processed the first 200,000 ratings in the dataset (giving rise to 20 snapshots).",
        "We incrementally update the role values as the time ordered ratings are merged into the model.",
        "To keep the experiment computationally manageable, we define a test dataset for each user.",
        "As the time ordered ratings are merged into the model, we label a small randomly selected percentage (20%) as test data.",
        "At discrete epochs, i.e., after processing every 10,000 ratings, we compute the predictions for the ratings in the test data, and then compute the role values for the ratings used in the predictions.",
        "One potential criticism of this methodology is that the ratings in the test set are never evaluated for their roles.",
        "We overcome this concern by repeating the experiment, using different random seeds.",
        "The probability that every rating is considered for evaluation is then considerably high: 1 − 0.2n , where n is the number of times the experiment is repeated with different random seeds.",
        "The results here are based on n = 4 repetitions.",
        "The item-based collaborative filtering algorithms performance was ordinary with respect to rank accuracy.",
        "Fig. 5 shows a plot of the precision and recall as ratings were merged in time order into the model.",
        "The recall was always high, but the average precision was just about 53%. 0 0.2 0.4 0.6 0.8 1 1.2 10000 30000 50000 70000 90000110000130000150000 Ratings merged into model Value Precision Recall Figure 5: Precision and recall for the item-based collaborative filtering algorithm. 5.2 Inducing good scouts The ratings of a user that serve as scouts are those that allow the user to receive recommendations.",
        "We claim that users with ratings that have respectable scout values will be happier with the system than those with ratings with low scout values.",
        "Note that the item-based algorithm discussed here produces recommendation lists with nearly half of the pairs in the list discordant from the users preference.",
        "Whether all of these discordant pairs are observable by the user is unclear, however, certainly this suggests that there is a need to be able to direct users to items whose ratings would improve the lists.",
        "The distribution of the scout values for most users ratings are Gaussian with mean zero.",
        "Fig. 6 shows the frequency distribution of scout values for a sample user at a given snapshot.",
        "We observe that a large number of ratings never serve as scouts for their users.",
        "A relatable scenario is when Amazons recommender makes suggestions of books or items based on other items that were purchased as gifts.",
        "With simple relevance feedback from the user, such ratings can be isolated as bad scouts and discounted from future predictions.",
        "Removing bad scouts was found to be worthwhile for individual users but the overall performance improvement was only marginal.",
        "An obvious question is whether good scouts can be formed by merely rating popular movies as suggested by Rashid et al. [9].",
        "They show that a mix of popularity and rating entropy identifies the best items to suggest to new users when evaluated using MAE.",
        "Following their intuition, we would expect to see a higher correlation between popularityentropy and good scouts.",
        "We measured the Pearson correlation coefficient between aggregated scout values for a movie with the popularity of a movie (number of times it is rated); and with its popularity*variance measure at different snapshots of the system.",
        "Note that the scout values were initially anti-correlated with popularity (Fig. 7), but became moderately correlated as the system evolved.",
        "Both popularity and popularity*variance performed similarly.",
        "A possible explanation is that there has been insufficient time for the popular movies to accumulate ratings. 255 -10 0 10 20 30 40 50 60 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 Scout Value Frequency Figure 6: Distribution of scout values for a sample user. -0.4 -0.2 0 0.2 0.4 0.6 0.8 30000 60000 90000 120000 150000 180000 Popularity Pop*Var Figure 7: Correlation between aggregated scout value and item popularity (computed at different intervals). -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 30000 60000 90000 120000 150000 180000 Figure 8: Correlation between aggregated promoter value and user prolificity (computed at different intervals).",
        "Table 1: Movies forming the best scouts.",
        "Best Scouts Conf.",
        "Pop.",
        "Being John Malkovich (1999) 1.00 445 Star Wars: Episode IV - A New Hope (1977) 0.92 623 Princess Bride, The (1987) 0.85 477 Sixth Sense, The (1999) 0.85 617 Matrix, The (1999) 0.77 522 Ghostbusters (1984) 0.77 441 Casablanca (1942) 0.77 384 Insider, The (1999) 0.77 235 American Beauty (1999) 0.69 624 Terminator 2: Judgment Day (1991) 0.69 503 Fight Club (1999) 0.69 235 Shawshank Redemption, The (1994) 0.69 445 Run Lola Run (Lola rennt) (1998) 0.69 220 Terminator, The (1984) 0.62 450 Usual Suspects, The (1995) 0.62 326 Aliens (1986) 0.62 385 North by Northwest (1959) 0.62 245 Fugitive, The (1993) 0.62 402 End of Days (1999) 0.62 132 Raiders of the Lost Ark (1981) 0.54 540 Schindlers List (1993) 0.54 453 Back to the Future (1985) 0.54 543 Toy Story (1995) 0.54 419 Alien (1979) 0.54 415 Abyss, The (1989) 0.54 345 2001: A Space Odyssey (1968) 0.54 358 Dogma (1999) 0.54 228 Little Mermaid, The (1989) 0.54 203 Table 2: Movies forming the worst scouts.",
        "Worst scouts Conf.",
        "Pop.",
        "Harold and Maude (1971) 0.46 141 Grifters, The (1990) 0.46 180 Sting, The (1973) 0.38 244 Godfather: Part III, The (1990) 0.38 154 Lawrence of Arabia (1962) 0.38 167 High Noon (1952) 0.38 84 Women on the Verge of a... (1988) 0.38 113 Grapes of Wrath, The (1940) 0.38 115 Duck Soup (1933) 0.38 131 Arsenic and Old Lace (1944) 0.38 138 Midnight Cowboy (1969) 0.38 137 To Kill a Mockingbird (1962) 0.31 195 Four Weddings and a Funeral (1994) 0.31 271 Good, The Bad and The Ugly, The (1966) 0.31 156 Its a Wonderful Life (1946) 0.31 146 Player, The (1992) 0.31 220 Jackie Brown (1997) 0.31 118 Boat, The (Das Boot) (1981) 0.31 210 Manhattan (1979) 0.31 158 Truth About Cats & Dogs, The (1996) 0.31 143 Ghost (1990) 0.31 227 Lone Star (1996) 0.31 125 Big Chill, The (1983) 0.31 184 256 By studying the evolution of scout values, we can identify movies that consistently feature in good scouts over time.",
        "We claim these movies will make viable scouts for other users.",
        "We found the aggregated scout values for all movies in intervals of 10,000 ratings each.",
        "A movie is said to induce a good scout if the movie was in the top 100 of the sorted list, and to induce a bad scout if it was in bottom 100 of the same list.",
        "Movies appearing consistently high over time are expected to remain up there in the future.",
        "The effective confidence in a movie m is Cm = Tm − Bm N (8) where Tm is the number of times it appeared in the top 100, Bm the number of times it appeared in the bottom 100, and N is the number of intervals considered.",
        "Using this measure, the top few movies expected to induce the best scouts are shown in Table 1.",
        "Movies that would be bad scout choices are shown in Table 2 with their associated confidences.",
        "The popularities of the movies are also displayed.",
        "Although more popular movies appear in the list of good scouts, these tables show that a blind choice of scout based on popularity alone can be potentially dangerous.",
        "Interestingly, the best scout-Being John Malkovich-is about a puppeteer who discovers a portal into a movie star, a movie that has been described variously on amazon.com as makes you feel giddy, seriously weird, comedy with depth, silly, strange, and inventive.",
        "Indicating whether someone likes this movie or not goes a long way toward situating the user in a suitable neighborhood, with similar preferences.",
        "On the other hand, several factors may have made a movie a bad scout, like the sharp variance in user preferences in the neighborhood of a movie.",
        "Two users may have the same opinion about Lawrence of Arabia, but they may differ sharply about how they felt about the other movies they saw.",
        "Bad scouts ensue when there is deviation in behavior around a common synchronization point. 5.3 Inducing good promoters Ratings that serve to promote items in a collaborative filtering system are critical to allowing a new item be recommended to users.",
        "So, inducing good promoters is important for cold-start recommendation.",
        "We note that the frequency distribution of promoter values for a sample movies ratings is also Gaussian (similar to Fig. 6).",
        "This indicates that the promotion of a movie is benefited most by the ratings of a few users, and are unaffected by the ratings of most users.",
        "We find a strong correlation between a users number of ratings and his aggregated promoter value.",
        "Fig. 8 depicts the evolution of the Pearson correlation co-efficient between the prolificity of a user (number of ratings) versus his aggregated promoter value.",
        "We expect that conspicuous shills, by recommending wrong movies to users, will be discredited with negative aggregate promoter values and should be identifiable easily.",
        "Given this observation, the obvious rule to follow when introducing a new movie is to have it rated directly by prolific users who posses high aggregated promoter values.",
        "A new movie is thus cast into the neighborhood of many other movies improving its visibility.",
        "Note, though, that a user may have long stopped using the system.",
        "Tracking promoter values consistently allows only the most active recent users to be considered. 5.4 Inducing good connectors Given the way scouts, connectors, and promoters are characterized, it follows that the movies that are part of the best scouts are also part of the best connectors.",
        "Similarly, the users that constitute the best promoters are also part of the best connectors.",
        "Good connectors are induced by ensuring a user with a high promoter value rates a movie with a high scout value.",
        "In our experiments, we find that a ratings longest standing role is often as a connector.",
        "A rating with a poor connector value is often seen due to its user being a bad promoter, or its movie being a bad scout.",
        "Such ratings can be removed from the prediction process to bring marginal improvements to recommendations.",
        "In some selected experiments, we observed that removing a set of badly behaving connectors helped improve the systems overall performance by 1.5%.",
        "The effect was even higher on a few select users who observed an improvement of above 10% in precision without much loss in recall. 5.5 Monitoring the evolution of rating roles One of the more significant contributions of our work is the ability to model the evolution of recommender systems, by studying the changing roles of ratings over time.",
        "The role and value of a rating can change depending on many factors like user behavior, redundancy, shilling effects or properties of the collaborative filtering algorithm used.",
        "Studying the dynamics of rating roles in terms of transitions between good, bad, and negligible values can provide insights into the functioning of the recommender system.",
        "We believe that a continuous visualization of these transitions will improve the ability to manage a recommender system.",
        "We classify different rating states as good, bad, or negligible.",
        "Consider a user who has rated 100 movies in a particular interval, of which 20 are part of the test set.",
        "If a scout has a value greater than 0.005, it indicates that it is uniquely involved in at least 2 concordant predictions, which we will say is good.",
        "Thus, a threshold of 0.005 is chosen to bin a rating as good, bad or negligible in terms of its scout, connector and promoter value.",
        "For instance, a rating r, at time t with role value triple [0.1, 0.001, −0.01] is classified as [scout +, connector 0, promoter −], where + indicates good, 0 indicates negligible, and − indicates bad.",
        "The positive credit held by a rating is a measure of its contribution to the betterment of the system, and the discredit is a measure of its contribution to the detriment of the system.",
        "Even though the positive roles (and the negative roles) make up a very small percentage of all ratings, their contribution supersedes their size.",
        "For example, even though only 1.7% of all ratings were classified as good scouts, they hold 79% of all positive credit in the system!",
        "Similarly, the bad scouts were just 1.4% of all ratings but hold 82% of all discredit.",
        "Note that good and bad scouts, together, comprise only 1.4% + 1.7% = 3.1% of the ratings, implying that the majority of the ratings are negligible role players as scouts (more on this later).",
        "Likewise, good connectors were 1.2% of the system, and hold 30% of all positive credit.",
        "The bad connectors (0.8% of the system) hold 36% of all discredit.",
        "Good promoters (3% of the system) hold 46% of all credit, while bad promoters (2%) hold 50% of all discredit.",
        "This reiterates that a few ratings influence most of the systems performance.",
        "Hence it is important to track transitions between them regardless of their small numbers. 257 Across different snapshots, a rating can remain in the same state or change.",
        "A good scout can become a bad scout, a good promoter can become a good connector, good and bad scouts can become vestigial, and so on.",
        "It is not practical to expect a recommender system to have no ratings in bad roles.",
        "However, it suffices to see ratings in bad roles either convert to good or vestigial roles.",
        "Similarly, observing a large number of good roles become bad ones is a sign of imminent failure of the system.",
        "We employ the principle of non-overlapping episodes [6] to count such transitions.",
        "A sequence such as: [+, 0, 0] → [+, 0, 0] → [0, +, 0] → [0, 0, 0] is interpreted as the transitions [+, 0, 0] ; [0, +, 0] : 1 [+, 0, 0] ; [0, 0, 0] : 1 [0, +, 0] ; [0, 0, 0] : 1 instead of [+, 0, 0] ; [0, +, 0] : 2 [+, 0, 0] ; [0, 0, 0] : 2 [0, +, 0] ; [0, 0, 0] : 1.",
        "See [6] for further details about this counting procedure.",
        "Thus, a rating can be in one of 27 possible states, and there are about 272 possible transitions.",
        "We make a further simplification and utilize only 9 states, indicating whether the rating is a scout, promoter, or connector, and whether it has a positive, negative, or negligible role.",
        "Ratings that serve multiple purposes are counted using multiple episode instantiations but the states themselves are not duplicated beyond the 9 restricted states.",
        "In this model, a transition such as [+, 0, +] ; [0, +, 0] : 1 is counted as [scout+] ; [scout0] : 1 [scout+] ; [connector+] : 1 [scout+] ; [promoter0] : 1 [connector0] ; [scout0] : 1 [connector0] ; [scout+] : 1 [connector0] ; [promoter0] : 1 [promoter+] ; [scout0] : 1 [promoter+] ; [connector+] : 1 [promoter+] ; [promoter0] : 1 Of these, transitions like [pX] ; [q0] where p = q, X ∈ {+, 0, −} are considered uninteresting, and only the rest are counted.",
        "Fig. 9 depicts the major transitions counted while processing the first 200,000 ratings from the MovieLens dataset.",
        "Only transitions with frequency greater than or equal to 3% are shown.",
        "The percentages for each state indicates the number of ratings that were found to be in those states.",
        "We consider transitions from any state to a good state as healthy, from any state to a bad state as unhealthy, and from any state to a vestigial state as decaying.",
        "From Fig. 9, we can observe: • The bulk of the ratings have negligible values, irrespective of their role.",
        "The majority of the transitions involve both good and bad ratings becoming negligible.",
        "Scout + (2%) Scout(1.5%) Scout 0 (96.5%) Connector + (1.2%) Connector(0.8%) Connector 0 (98%) Promoter + (3%) Promoter(2%) Promoter 0 (95%) 84% 84% 81% 74% 10% 6% 11% 77% 8% 7% 8% 82% 4% 86% 4% 68% 15% 13% 5% 5% 77% 11% 7% 5% 4% 3% 3% 3% Healthy Unhealthy Decaying Figure 9: Transitions among rating roles. • The number of good ratings is comparable to the bad ratings, and ratings are seen to switch states often, except in the case of scouts (see below). • The negative and positive scout states are not reachable through any transition, indicating that these ratings must begin as such, and cannot be coerced into these roles. • Good promoters and good connectors have a much longer survival period than scouts.",
        "Transitions that recur to these states have frequencies of 10% and 15% when compared to just 4% for scouts.",
        "Good connectors are the slowest to decay whereas (good) scouts decay the fastest. • Healthy percentages are seen on transitions between promoters and connectors.",
        "As indicated earlier, there are hardly any transitions from promoters/connectors to scouts.",
        "This indicates that, over the long run, a users rating is more useful to others (movies or other users) than to the user himself. • The percentages of healthy transitions outweigh the unhealthy ones - this hints that the system is healthy, albeit only marginally.",
        "Note that these results are conditioned by the static nature of the dataset, which is a set of ratings over a fixed window of time.",
        "However a diagram such as Fig. 9 is clearly useful for monitoring the health of a recommender system.",
        "For instance, acceptable limits can be imposed on different types of transitions and, if a transition fails to meet the threshold, the recommender system or a part of it can be brought under closer scrutiny.",
        "Furthermore, the role state transition diagram would also be the ideal place to study the effects of shilling, a topic we will consider in future research. 5.6 Characterizing neighborhoods Earlier we saw that we can characterize the neighborhood of ratings involved in creating a recommendation list L for 258 a user.",
        "In our experiment, we consider lists of length 30, and sample the lists of about 5% of users through the evolution of the model (at intervals of 10,000 ratings each) and compute their neighborhood characteristics.",
        "To simplify our presentation, we consider the percentage of the sample that fall into one of the following categories: 1.",
        "Inactive user: (SFN(u) = 0) 2.",
        "Good scouts, Good neighborhood: (SFN(u) > 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 3.",
        "Good scouts, Bad neighborhood: (SFN(u) > 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) 4.",
        "Bad scouts, Good neighborhood: (SFN(u) < 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 5.",
        "Bad scouts, Bad neighborhood: (SFN(u) < 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) From our sample set of 561 users, we found that 476 users were inactive.",
        "Of the remaining 85 users, we found 26 users had good scouts and a good neighborhood, 6 had bad scouts and a good neighborhood, 29 had good scouts and a bad neighborhood, and 24 had bad scouts and a bad neighborhood.",
        "Thus, we conjecture that 59 users (29+24+6) are in danger of leaving the system.",
        "As a remedy, users with bad scouts and a good neighborhood can be asked to reconsider rating of some movies hoping to improve the systems recommendations.",
        "The system can be expected to deliver more if they engineer some good scouts.",
        "Users with good scouts and a bad neighborhood are harder to address; this situation might entail selectively removing some connector-promoter pairs that are causing the damage.",
        "Handling users with bad scouts and bad neighborhoods is a more difficult challenge.",
        "Such a classification allows the use of different strategies to better a users experience with the system depending on his context.",
        "In future work, we intend to conduct field studies and study the improvement in performance of different strategies for different contexts. 6.",
        "CONCLUSIONS To further recommender system acceptance and deployment, we require new tools and methodologies to manage an installed recommender and develop insights into the roles played by ratings.",
        "A fine-grained characterization in terms of rating roles such as scouts, promoters, and connectors, as done here, helps such an endeavor.",
        "Although we have presented results on only the item-based algorithm with list rank accuracy as the metric, the same approach outlined here applies to user-based algorithms and other metrics.",
        "In future research, we plan to systematically study the many algorithmic parameters, tolerances, and cutoff thresholds employed here and reason about their effects on the downstream conclusions.",
        "We also aim to extend our formulation to other collaborative filtering algorithms, study the effect of shilling in altering rating roles, conduct field studies, and evaluate improvements in user experience by tweaking ratings based on their role values.",
        "Finally, we plan to develop the idea of mining the evolution of rating role patterns into a reporting and tracking system for all aspects of recommender system health. 7.",
        "REFERENCES [1] Cosley, D., Lam, S., Albert, I., Konstan, J., and Riedl, J.",
        "Is Seeing Believing? : How Recommender System Interfaces Affect Users Opinions.",
        "In Proc.",
        "CHI (2001), pp. 585-592. [2] Herlocker, J. L., Konstan, J.",
        "A., Borchers, A., and Riedl, J.",
        "An Algorithmic Framework for Performing Collaborative Filtering.",
        "In Proc.",
        "SIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J.",
        "A., Terveen, L. G., and Riedl, J. T. Evaluating Collaborative Filtering Recommender Systems.",
        "ACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J.",
        "A.",
        "Personal communication. 2003. [5] Lam, S. K., and Riedl, J. Shilling Recommender Systems for Fun and Profit.",
        "In Proceedings of the 13th International World Wide Web Conference (2004), ACM Press, pp. 393-402. [6] Laxman, S., Sastry, P. S., and Unnikrishnan, K. P. Discovering Frequent Episodes and Learning Hidden Markov Models: A Formal Connection.",
        "IEEE Transactions on Knowledge and Data Engineering Vol. 17, 11 (2005), 1505-1517. [7] McLaughlin, M. R., and Herlocker, J. L. A Collaborative Filtering Algorithm and Evaluation Metric that Accurately Model the User Experience.",
        "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (2004), pp. 329 - 336. [8] OMahony, M., Hurley, N. J., Kushmerick, N., and Silvestre, G. Collaborative Recommendation: A Robustness Analysis.",
        "ACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J.",
        "A., and Riedl, J.",
        "Getting to Know You: Learning New User Preferences in Recommender Systems.",
        "In Proceedings of the 2002 Conference on Intelligent User Interfaces (IUI 2002) (2002), pp. 127-134. [10] Rashid, A. M., Karypis, G., and Riedl, J.",
        "Influence in Ratings-Based Recommender Systems: An Algorithm-Independent Approach.",
        "In Proc. of the SIAM International Conference on Data Mining (2005). [11] Resnick, P., Iacovou, N., Sushak, M., Bergstrom, P., and Riedl, J. GroupLens: An Open Architecture for Collaborative Filtering of Netnews.",
        "In Proceedings of the Conference on Computer Supported Collaborative Work (CSCW94) (1994), ACM Press, pp. 175-186. [12] Sarwar, B., Karypis, G., Konstan, J., and Reidl, J. Item-Based Collaborative Filtering Recommendation Algorithms.",
        "In Proceedings of the Tenth International World Wide Web Conference (WWW10) (2001), pp. 285-295. [13] Schein, A., Popescu, A., Ungar, L., and Pennock, D. Methods and Metrics for Cold-Start Recommendation.",
        "In Proc.",
        "SIGIR (2002), pp. 253-260. 259"
    ],
    "error_count": 0,
    "keys": {
        "nearest neighbor": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Scouts, Promoters, and Connectors: The Roles of Ratings in <br>nearest neighbor</br> Collaborative Filtering Bharath Kumar Mohan Dept.",
                "of CSA Indian Institute of Science Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. of Computer Science Eastern Michigan University Ypsilanti, MI 48917, USA bkeller@emich.edu Naren Ramakrishnan Dept. of Computer Science Virginia Tech, Blacksburg VA 24061, USA naren@cs.vt.edu ABSTRACT Recommender systems aggregate individual user ratings into predictions of products or services that might interest visitors.",
                "The quality of this aggregation process crucially affects the user experience and hence the effectiveness of recommenders in e-commerce.",
                "We present a novel study that disaggregates global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearestneighbor collaborative filtering.",
                "In particular, we formulate three roles-scouts, promoters, and connectors-that capture how users receive recommendations, how items get recommended, and how ratings of these two types are themselves connected (resp.).",
                "These roles find direct uses in improving recommendations for users, in better targeting of items and, most importantly, in helping monitor the health of the system as a whole.",
                "For instance, they can be used to track the evolution of neighborhoods, to identify rating subspaces that do not contribute (or contribute negatively) to system performance, to enumerate users who are in danger of leaving, and to assess the susceptibility of the system to attacks such as shilling.",
                "We argue that the three rating roles presented here provide broad primitives to manage a recommender system and its community.",
                "Categories and Subject Descriptors H.4.2 [Information Systems Applications]: Types of Systems-Decision support; J.4 [Computer Applications]: Social and Behavioral Sciences General Terms Algorithms, Human Factors 1.",
                "INTRODUCTION Recommender systems have become integral to e-commerce, providing technology that suggests products to a visitor based on previous purchases or rating history.",
                "Collaborative filtering, a common form of recommendation, predicts a users rating for an item by combining (other) ratings of that user with other users ratings.",
                "Significant research has been conducted in implementing fast and accurate collaborative filtering algorithms [2, 7], designing interfaces for presenting recommendations to users [1], and studying the robustness of these algorithms [8].",
                "However, with the exception of a few studies on the influence of users [10], little attention has been paid to unraveling the inner workings of a recommender in terms of the individual ratings and the roles they play in making (good) recommendations.",
                "Such an understanding will give an important handle to monitoring and managing a recommender system, to engineer mechanisms to sustain the recommender, and thereby ensure its continued success.",
                "Our motivation here is to disaggregate global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearest-neighbor collaborative filtering.",
                "We identify three possible roles: (scouts) to connect the user into the system to receive recommendations, (promoters) to connect an item into the system to be recommended, and (connectors) to connect ratings of these two kinds.",
                "Viewing ratings in this way, we can define the contribution of a rating in each role, both in terms of allowing recommendations to occur, and in terms of influence on the quality of recommendations.",
                "In turn, this capability helps support scenarios such as: 1.",
                "Situating users in better neighborhoods: A users ratings may inadvertently connect the user to a neighborhood for which the users tastes may not be a perfect match.",
                "Identifying ratings responsible for such bad recommendations and suggesting new items to rate can help situate the user in a better neighborhood. 2.",
                "Targeting items: Recommender systems suffer from lack of user participation, especially in cold-start scenarios [13] involving newly arrived items.",
                "Identifying users who can be encouraged to rate specific items helps ensure coverage of the recommender system. 3.",
                "Monitoring the evolution of the recommender system and its stakeholders: A recommender system is constantly under change: growing with new users and 250 items, shrinking with users leaving the system, items becoming irrelevant, and parts of the system under attack.",
                "Tracking the roles of a rating and its evolution over time provides many insights into the health of the system, and how it could be managed and improved.",
                "These include being able to identify rating subspaces that do not contribute (or contribute negatively) to system performance, and could be removed; to enumerate users who are in danger of leaving, or have left the system; and to assess the susceptibility of the system to attacks such as shilling [5].",
                "As we show, the characterization of rating roles presented here provides broad primitives to manage a recommender system and its community.",
                "The rest of the paper is organized as follows.",
                "Background on nearest-neighbor collaborative filtering and algorithm evaluation is discussed in Section 2.",
                "Section 3 defines and discusses the roles of a rating, and Section 4 defines measures of the contribution of a rating in each of these roles.",
                "In Section 5, we illustrate the use of these roles to address the goals outlined above. 2.",
                "BACKGROUND 2.1 Algorithms Nearest-neighbor collaborative filtering algorithms either use neighborhoods of users or neighborhoods of items to compute a prediction.",
                "An algorithm of the first kind is called user-based, and one of the second kind is called itembased [12].",
                "In both families of algorithms, neighborhoods are formed by first computing the similarity between all pairs of users (for user-based) or items (for item-based).",
                "Predictions are then computed by aggregating ratings, which in a user-based algorithm involves aggregating the ratings of the target item by the users neighbors and, in an item-based algorithm, involves aggregating the users ratings of items that are neighbors of the target item.",
                "Algorithms within these families differ in the definition of similarity, formation of neighborhoods, and the computation of predictions.",
                "We consider a user-based algorithm based on that defined for GroupLens [11] with variations from Herlocker et al. [2], and an item-based algorithm similar to that of Sarwar et al. [12].",
                "The algorithm used by Resnick et al. [11] defines the similarity of two users u and v as the Pearson correlation of their common ratings: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2 , where Iu is the set of items rated by user u, ru,i is user us rating for item i, and ¯ru is the average rating of user u (similarly for v).",
                "Similarity computed in this manner is typically scaled by a factor proportional to the number of common ratings, to reduce the chance of making a recommendation made on weak connections: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), where γ ≈ 5 is a constant used as a lower limit in scaling [2].",
                "These new similarities are then used to define a static neighborhood Nu for each user u consisting of the top K users most similar to user u.",
                "A prediction for user u and item i is computed by a weighted average of the ratings by the neighbors pu,i = ¯ru + P v∈V sim (u, v)(rv,i − ¯rv) P v∈V sim (u, v) (1) where V = Nu ∩ Ui is the set of users most similar to u who have rated i.",
                "The item-based algorithm we use is the one defined by Sarwar et al. [12].",
                "In this algorithm, similarity is defined as the adjusted cosine measure sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) where Ui is the set of users who have rated item i.",
                "As for the user-based algorithm, the similarity weights are adjusted proportionally to the number of users that have rated the items in common sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). (3) Given the similarities, the neighborhood Ni of an item i is defined as the top K most similar items for that item.",
                "A prediction for user u and item i is computed as the weighted average pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) where J = Ni ∩ Iu is the set of items rated by u that are most similar to i. 2.2 Evaluation Recommender algorithms have typically been evaluated using measures of predictive accuracy and coverage [3].",
                "Studies on recommender algorithms, notably Herlocker et al. [2] and Sarwar et al. [12], typically compute predictive accuracy by dividing a set of ratings into training and test sets, and compute the prediction for an item in the test set using the ratings in the training set.",
                "A standard measure of predictive accuracy is mean absolute error (MAE), which for a test set T = {(u, i)} is defined as, MAE = P (u,i)∈T |pu,i − ru,i| |T | . (5) Coverage has a number of definitions, but generally refers to the proportion of items that can be predicted by the algorithm [3].",
                "A practical issue with predictive accuracy is that users typically are presented with recommendation lists, and not individual numeric predictions.",
                "Recommendation lists are lists of items in decreasing order of prediction (sometimes stated in terms of star-ratings), and so predictive accuracy may not be reflective of the accuracy of the list.",
                "So, instead we can measure recommendation or rank accuracy, which indicates the extent to which the list is in the correct order.",
                "Herlocker et al. [3] discuss a number of rank accuracy measures, which range from Kendalls Tau to measures that consider the fact that users tend to only look at a prefix of the list [5].",
                "Kendalls Tau measures the number of inversions when comparing ordered pairs in the true user ordering of 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figure 1: Ratings in simple movie recommender. items and the recommended order, and is defined as τ = C − D p (C + D + TR)(C + D + TP) (6) where C is the number of pairs that the system predicts in the correct order, D the number of pairs the system predicts in the wrong order, TR the number of pairs in the true ordering that have the same ratings, and TP is the number of pairs in the predicted ordering that have the same ratings [3].",
                "A shortcoming of the Tau metric is that it is oblivious to the position in the ordered list where the inversion occurs [3].",
                "For instance, an inversion toward the end of the list is given the same weight as one in the beginning.",
                "One solution is to consider inversions only in the top few items in the recommended list or to weight inversions based on their position in the list. 3.",
                "ROLES OF A RATING Our basic observation is that each rating plays a different role in each prediction in which it is used.",
                "Consider a simplified movie recommender system with three users Jim, Jeff, and Tom and their ratings for a few movies, as shown in Fig. 1. (For this initial discussion we will not consider the rating values involved.)",
                "The recommender predicts whether Tom will like The Mask using the other already available ratings.",
                "How this is done depends on the algorithm: 1.",
                "An item-based collaborative filtering algorithm constructs a neighborhood of movies around The Mask by using the ratings of users who rated The Mask and other movies similarly (e.g., Jims ratings of The Matrix and The Mask; and Jeffs ratings of Star Wars and The Mask).",
                "Toms ratings of those movies are then used to make a prediction for The Mask. 2.",
                "A user-based collaborative filtering algorithm would construct a neighborhood around Tom by tracking other users whose rating behaviors are similar to Toms (e.g., Tom and Jeff have rated Star Wars; Tom and Jim have rated The Matrix).",
                "The prediction of Toms rating for The Mask is then based on the ratings of Jeff and Tim.",
                "Although the nearest-neighbor algorithms aggregate the ratings to form neighborhoods used to compute predictions, we can disaggregate the similarities to view the computation of a prediction as simultaneously following parallel paths of ratings.",
                "So, irrespective of the collaborative filtering algorithm used, we can visualize the prediction of Toms rating of The Mask as walking through a sequence of ratings.",
                "In Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figure 2: Ratings used to predict The Mask for Tom.",
                "Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figure 3: Prediction of The Mask for Tom in which a rating is used more than once. this example, two paths were used for this prediction as depicted in Fig. 2: (p1, p2, p3) and (q1, q2, q3).",
                "Note that these paths are undirected, and are all of length 3.",
                "Only the order in which the ratings are traversed is different between the item-based algorithm (e.g., (p3, p2, p1), (q3, q2, q1)) and the user-based algorithm (e.g., (p1, p2, p3), (q1, q2, q3).)",
                "A rating can be part of many paths for a single prediction as shown in Fig. 3, where three paths are used for a prediction, two of which follow p1: (p1, p2, p3) and (p1, r2, r3).",
                "Predictions in a collaborative filtering algorithms may involve thousands of such walks in parallel, each playing a part in influencing the predicted value.",
                "Each prediction path consists of three ratings, playing roles that we call scouts, promoters, and connectors.",
                "To illustrate these roles, consider the path (p1, p2, p3) in Fig. 2 used to make a prediction of The Mask for Tom: 1.",
                "The rating p1 (Tom → Star Wars) makes a connection from Tom to other ratings that can be used to predict Toms rating for The Mask.",
                "This rating serves as a scout in the bipartite graph of ratings to find a path that leads to The Mask. 2.",
                "The rating p2 (Jeff → Star Wars) helps the system recommend The Mask to Tom by connecting the scout to the promoter. 3.",
                "The rating p3 (Jeff → The Mask) allows connections to The Mask, and, therefore, promotes this movie to Tom.",
                "Formally, given a prediction pu,a of a target item a for user u, a scout for pu,a is a rating ru,i such that there exists a user v with ratings rv,a and rv,i for some item i; a promoter for pu,a is a rating rv,a for some user v, such that there exist ratings rv,i and ru,i for an item i, and; a connector for pu,a 252 Jim Tom Jeff Jerry My Cousin Vinny The Matrix Star Wars The Mask Jurasic Park Figure 4: Scouts, promoters, and connectors. is a rating rv,i by some user v and rating i, such that there exists ratings ru,i and rv,a.",
                "The scouts, connectors, and promoters for the prediction of Toms rating of The Mask are p1 and q1, p2 and q2, and p3 and q3 (respectively).",
                "Each of these roles has a value in the recommender to the user, the users neighborhood, and the system in terms of allowing recommendations to be made. 3.1 Roles in Detail Ratings that act as scouts tend to help the recommender system suggest more movies to the user, though the extent to which this is true depends on the rating behavior of other users.",
                "For example, in Fig. 4 the rating Tom → Star Wars helps the system recommend only The Mask to him, while Tom → The Matrix helps recommend The Mask, Jurassic Park, and My Cousin Vinny.",
                "Tom makes a connection to Jim who is a prolific user of the system, by rating The Matrix.",
                "However, this does not make The Matrix the best movie to rate for everyone.",
                "For example, Jim is benefited equally by both The Mask and The Matrix, which allow the system to recommend Star Wars to him.",
                "His rating of The Mask is the best scout for Jeff, and Jerrys only scout is his rating of Star Wars.",
                "This suggests that good scouts allow a user to build similarity with prolific users, and thereby ensure they get more from the system.",
                "While scouts represent beneficial ratings from the perspective of a user, promoters are their duals, and are of benefit to items.",
                "In Fig. 4, My Cousin Vinny benefits from Jims rating, since it allows recommendations to Jeff and Tom.",
                "The Mask is not so dependent on just one rating, since the ratings by Jim and Jeff help it.",
                "On the other hand, Jerrys rating of Star Wars does not help promote it to any other user.",
                "We conclude that a good promoter connects an item to a broader neighborhood of other items, and thereby ensures that it is recommended to more users.",
                "Connectors serve a crucial role in a recommender system that is not as obvious.",
                "The movies My Cousin Vinny and Jurassic Park have the highest recommendation potential since they can be recommended to Jeff, Jerry and Tom based on the linkage structure illustrated in Fig. 4.",
                "Beside the fact that Jim rated these movies, these recommendations are possible only because of the ratings Jim → The Matrix and Jim → The Mask, which are the best connectors.",
                "A connector improves the systems ability to make recommendations with no explicit gain for the user.",
                "Note that every rating can be of varied benefit in each of these roles.",
                "The rating Jim → My Cousin Vinny is a poor scout and connector, but is a very good promoter.",
                "The rating Jim → The Mask is a reasonably good scout, a very good connector, and a good promoter.",
                "Finally, the rating Jerry → Star Wars is a very good scout, but is of no value as a connector or promoter.",
                "As illustrated here, a rating can have different value in each of the three roles in terms of whether a recommendation can be made or not.",
                "We could measure this value by simply counting the number of times a rating is used in each role, which alone would be helpful in characterizing the behavior of a system.",
                "But we can also measure the contribution of each rating to the quality of recommendations or health of the system.",
                "Since every prediction is a combined effort of several recommendation paths, we are interested in discerning the influence of each rating (and, hence, each path) in the system towards the systems overall error.",
                "We can understand the dynamics of the system at a finer granularity by tracking the influence of a rating according to the role played.",
                "The next section describes the approach to measuring the values of a rating in each role. 4.",
                "CONTRIBUTIONS OF RATINGS As weve seen, a rating may play different roles in different predictions and, in each prediction, contribute to the quality of a prediction in different ways.",
                "Our approach can use any numeric measure of a property of system health, and assigns credit (or blame) to each rating proportional to its influence in the prediction.",
                "By tracking the role of each rating in a prediction, we can accumulate the credit for a rating in each of the three roles, and also track the evolution of the roles of rating over time in the system.",
                "This section defines the methodology for computing the contribution of ratings by first defining the influence of a rating, and then instantiating the approach for predictive accuracy, and then rank accuracy.",
                "We also demonstrate how these contributions can be aggregated to study the neighborhood of ratings involved in computing a users recommendations.",
                "Note that although our general formulation for rating influence is algorithm independent, due to space considerations, we present the approach for only item-based collaborative filtering.",
                "The definition for user-based algorithms is similar and will be presented in an expanded version of this paper. 4.1 Influence of Ratings Recall that an item-based approach to collaborative filtering relies on building item neighborhoods using the similarity of ratings by the same user.",
                "As described earlier, similarity is defined by the adjusted cosine measure (Equations (2) and (3)).",
                "A set of the top K neighbors is maintained for all items for space and computational efficiency.",
                "A prediction of item i for a user u is computed as the weighted deviation from the items mean rating as shown in Equation (4).",
                "The list of recommendations for a user is then the list of items sorted in descending order of their predicted values.",
                "We first define impact(a, i, j), the impact a user a has in determining the similarity between two items i and j.",
                "This is the change in the similarity between i and j when as rating is removed, and is defined as impact(a, i, j) = |sim (i, j) − sim¯a(i, j)| P w∈Cij |sim (i, j) − sim ¯w(i, j)| where Cij = {u ∈ U | ∃ ru,i, ru,j ∈ R(u)} is the set of coraters 253 of items i and j (users who rate both i and j), R(u) is the set of ratings provided by user u, and sim¯a(i, j) is the similarity of i and j when the ratings of user a are removed sim¯a(i, j) = P v∈U\\{a} (ru,i − ¯ru)(ru,j − ¯ru) qP u∈U\\{a}(ru,i − ¯ru)2 qP u∈U\\{a}(ru,j − ¯ru)2 , and adjusted for the number of raters sim¯a(i, j) = max(|Ui ∩ Uj| − 1, γ) γ · sim(i, j).",
                "If all coraters of i and j rate them identically, we define the impact as impact(a, i, j) = 1 |Cij| since P w∈Cij |sim (i, j) − sim ¯w(i, j)| = 0.",
                "The influence of each path (u, j, v, i) = [ru,j, rv,j, rv,i] in the prediction of pu,i is given by influence(u, j, v, i) = sim (i, j) P l∈Ni∩Iu sim (i, l) · impact(v, i, j) It follows that the sum of influences over all such paths, for a given set of endpoints, is 1. 4.2 Role Values for Predictive Accuracy The value of a rating in each role is computed from the influence depending on the evaluation measure employed.",
                "Here we illustrate the approach using predictive accuracy as the evaluation metric.",
                "In general, the goodness of a prediction decides whether the ratings involved must be credited or discredited for their role.",
                "For predictive accuracy, the error in prediction e = |pu,i − ru,i| is mapped to a comfort level using a mapping function M(e).",
                "Anecdotal evidence suggests that users are unable to discern errors less than 1.0 (for a rating scale of 1 to 5) [4], and so an error less than 1.0 is considered acceptable, but anything larger is not.",
                "We hence define M(e) as (1 − e) binned to an appropriate value in [−1, −0.5, 0.5, 1].",
                "For each prediction pu,i, M(e) is attributed to all the paths that assisted the computation of pu,i, proportional to their influences.",
                "This tribute, M(e)∗influence(u, j, v, i), is in turn inherited by each of the ratings in the path [ru,j, rv,j, rv,i], with the credit/blame accumulating to the respective roles of ru,j as a scout, rv,j as a connector, and rv,i as a promoter.",
                "In other words, the scout value SF(ru,j), the connector value CF(rv,j) and the promoter value PF(rv,i) are all incremented by the tribute amount.",
                "Over a large number of predictions, scouts that have repeatedly resulted in big error rates have a big negative scout value, and vice versa (similarly with the other roles).",
                "Every rating is thus summarized by its triple [SF, CF, PF]. 4.3 Role Values for Rank Accuracy We now define the computation of the contribution of ratings to observed rank accuracy.",
                "For this computation, we must know the users preference order for a set of items for which predictions can be computed.",
                "We assume that we have a test set of the users ratings of the items presented in the recommendation list.",
                "For every pair of items rated by a user in the test data, we check whether the predicted order is concordant with his preference.",
                "We say a pair (i, j) is concordant (with error ) whenever one of the following holds: • if (ru,i < ru,j) then (pu,i − pu,j < ); • if (ru,i > ru,j) then (pu,i − pu,j > ); or • if (ru,i = ru,j) then (|pu,i − pu,j| ≤ ).",
                "Similarly, a pair (i, j) is discordant (with error ) if it is not concordant.",
                "Our experiments described below use an error tolerance of = 0.1.",
                "All paths involved in the prediction of the two items in a concordant pair are credited, and the paths involved in a discordant pair are discredited.",
                "The credit assigned to a pair of items (i, j) in the recommendation list for user u is computed as c(i, j) = ( t T · 1 C+D if (i, j) are concordant − t T · 1 C+D if (i, j) are discordant (7) where t is the number of items in the users test set whose ratings could be predicted, T is the number of items rated by user u in the test set, C is the number of concordances and D is the number of discordances.",
                "The credit c is then divided among all paths responsible for predicting pu,i and pu,j proportional to their influences.",
                "We again add the role values obtained from all the experiments to form a triple [SF, CF, PF] for each rating. 4.4 Aggregating rating roles After calculating the role values for individual ratings, we can also use these values to study neighborhoods and the system.",
                "Here we consider how we can use the role values to characterize the health of a neighborhood.",
                "Consider the list of top recommendations presented to a user at a specific point in time.",
                "The collaborative filtering algorithm traversed many paths in his neighborhood through his scouts and other connectors and promoters to make these recommendations.",
                "We call these ratings the recommender neighborhood of the user.",
                "The user implicitly chooses this neighborhood of ratings through the items he rates.",
                "Apart from the collaborative filtering algorithm, the health of this neighborhood completely influences a users satisfaction with the system.",
                "We can characterize a users recommender neighborhood by aggregating the individual role values of the ratings involved, weighted by the influence of individual ratings in determining his recommended list.",
                "Different sections of the users neighborhood wield varied influence on his recommendation list.",
                "For instance, ratings reachable through highly rated items have a bigger say in the recommended items.",
                "Our aim is to study the system and classify users with respect to their positioning in a healthy or unhealthy neighborhood.",
                "A user can have a good set of scouts, but may be exposed to a neighborhood with bad connectors and promoters.",
                "He can have a good neighborhood, but his bad scouts may ensure the neighborhoods potential is rendered useless.",
                "We expect that users with good scouts and good neighborhoods will be most satisfied with the system in the future.",
                "A users neighborhood is characterized by a triple that represents the weighted sum of the role values of individual ratings involved in making recommendations.",
                "Consider a user u and his ordered list of recommendations L. An item i 254 in the list is weighted inversely, as K(i), depending on its position in the list.",
                "In our studies we use K(i) = p position(i).",
                "Several paths of ratings [ru,j, rv,j, rv,i] are involved in predicting pu,i which ultimately decides its position in L, each with influence(u, j, v, i).",
                "The recommender neighborhood of a user u is characterized by the triple, [SFN(u), CFN(u), PFN(u)] where SFN(u) = X i∈L P [ru,j ,rv,j ,rv,i] SF(ru,j)influence(u, j, v, i) K(i) !",
                "CFN(u) and PFN(u) are defined similarly.",
                "This triple estimates the quality of us recommendations based on the past track record of the ratings involved in their respective roles. 5.",
                "EXPERIMENTATION As we have seen, we can assign role values to each rating when evaluating a collaborative filtering system.",
                "In this section, we demonstrate the use of this approach to our overall goal of defining an approach to monitor and manage the health of a recommender system through experiments done on the MovieLens million rating dataset.",
                "In particular, we discuss results relating to identifying good scouts, promoters, and connectors; the evolution of rating roles; and the characterization of user neighborhoods. 5.1 Methodology Our experiments use the MovieLens million rating dataset, which consists of ratings by 6040 users of 3952 movies.",
                "The ratings are in the range 1 to 5, and are labeled with the time the rating was given.",
                "As discussed before, we consider only the item-based algorithm here (with item neighborhoods of size 30) and, due to space considerations, only present role value results for rank accuracy.",
                "Since we are interested in the evolution of the rating role values over time, the model of the recommender system is built by processing ratings in their arrival order.",
                "The timestamping provided by MovieLens is hence crucial for the analyses presented here.",
                "We make assessments of rating roles at intervals of 10,000 ratings and processed the first 200,000 ratings in the dataset (giving rise to 20 snapshots).",
                "We incrementally update the role values as the time ordered ratings are merged into the model.",
                "To keep the experiment computationally manageable, we define a test dataset for each user.",
                "As the time ordered ratings are merged into the model, we label a small randomly selected percentage (20%) as test data.",
                "At discrete epochs, i.e., after processing every 10,000 ratings, we compute the predictions for the ratings in the test data, and then compute the role values for the ratings used in the predictions.",
                "One potential criticism of this methodology is that the ratings in the test set are never evaluated for their roles.",
                "We overcome this concern by repeating the experiment, using different random seeds.",
                "The probability that every rating is considered for evaluation is then considerably high: 1 − 0.2n , where n is the number of times the experiment is repeated with different random seeds.",
                "The results here are based on n = 4 repetitions.",
                "The item-based collaborative filtering algorithms performance was ordinary with respect to rank accuracy.",
                "Fig. 5 shows a plot of the precision and recall as ratings were merged in time order into the model.",
                "The recall was always high, but the average precision was just about 53%. 0 0.2 0.4 0.6 0.8 1 1.2 10000 30000 50000 70000 90000110000130000150000 Ratings merged into model Value Precision Recall Figure 5: Precision and recall for the item-based collaborative filtering algorithm. 5.2 Inducing good scouts The ratings of a user that serve as scouts are those that allow the user to receive recommendations.",
                "We claim that users with ratings that have respectable scout values will be happier with the system than those with ratings with low scout values.",
                "Note that the item-based algorithm discussed here produces recommendation lists with nearly half of the pairs in the list discordant from the users preference.",
                "Whether all of these discordant pairs are observable by the user is unclear, however, certainly this suggests that there is a need to be able to direct users to items whose ratings would improve the lists.",
                "The distribution of the scout values for most users ratings are Gaussian with mean zero.",
                "Fig. 6 shows the frequency distribution of scout values for a sample user at a given snapshot.",
                "We observe that a large number of ratings never serve as scouts for their users.",
                "A relatable scenario is when Amazons recommender makes suggestions of books or items based on other items that were purchased as gifts.",
                "With simple relevance feedback from the user, such ratings can be isolated as bad scouts and discounted from future predictions.",
                "Removing bad scouts was found to be worthwhile for individual users but the overall performance improvement was only marginal.",
                "An obvious question is whether good scouts can be formed by merely rating popular movies as suggested by Rashid et al. [9].",
                "They show that a mix of popularity and rating entropy identifies the best items to suggest to new users when evaluated using MAE.",
                "Following their intuition, we would expect to see a higher correlation between popularityentropy and good scouts.",
                "We measured the Pearson correlation coefficient between aggregated scout values for a movie with the popularity of a movie (number of times it is rated); and with its popularity*variance measure at different snapshots of the system.",
                "Note that the scout values were initially anti-correlated with popularity (Fig. 7), but became moderately correlated as the system evolved.",
                "Both popularity and popularity*variance performed similarly.",
                "A possible explanation is that there has been insufficient time for the popular movies to accumulate ratings. 255 -10 0 10 20 30 40 50 60 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 Scout Value Frequency Figure 6: Distribution of scout values for a sample user. -0.4 -0.2 0 0.2 0.4 0.6 0.8 30000 60000 90000 120000 150000 180000 Popularity Pop*Var Figure 7: Correlation between aggregated scout value and item popularity (computed at different intervals). -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 30000 60000 90000 120000 150000 180000 Figure 8: Correlation between aggregated promoter value and user prolificity (computed at different intervals).",
                "Table 1: Movies forming the best scouts.",
                "Best Scouts Conf.",
                "Pop.",
                "Being John Malkovich (1999) 1.00 445 Star Wars: Episode IV - A New Hope (1977) 0.92 623 Princess Bride, The (1987) 0.85 477 Sixth Sense, The (1999) 0.85 617 Matrix, The (1999) 0.77 522 Ghostbusters (1984) 0.77 441 Casablanca (1942) 0.77 384 Insider, The (1999) 0.77 235 American Beauty (1999) 0.69 624 Terminator 2: Judgment Day (1991) 0.69 503 Fight Club (1999) 0.69 235 Shawshank Redemption, The (1994) 0.69 445 Run Lola Run (Lola rennt) (1998) 0.69 220 Terminator, The (1984) 0.62 450 Usual Suspects, The (1995) 0.62 326 Aliens (1986) 0.62 385 North by Northwest (1959) 0.62 245 Fugitive, The (1993) 0.62 402 End of Days (1999) 0.62 132 Raiders of the Lost Ark (1981) 0.54 540 Schindlers List (1993) 0.54 453 Back to the Future (1985) 0.54 543 Toy Story (1995) 0.54 419 Alien (1979) 0.54 415 Abyss, The (1989) 0.54 345 2001: A Space Odyssey (1968) 0.54 358 Dogma (1999) 0.54 228 Little Mermaid, The (1989) 0.54 203 Table 2: Movies forming the worst scouts.",
                "Worst scouts Conf.",
                "Pop.",
                "Harold and Maude (1971) 0.46 141 Grifters, The (1990) 0.46 180 Sting, The (1973) 0.38 244 Godfather: Part III, The (1990) 0.38 154 Lawrence of Arabia (1962) 0.38 167 High Noon (1952) 0.38 84 Women on the Verge of a... (1988) 0.38 113 Grapes of Wrath, The (1940) 0.38 115 Duck Soup (1933) 0.38 131 Arsenic and Old Lace (1944) 0.38 138 Midnight Cowboy (1969) 0.38 137 To Kill a Mockingbird (1962) 0.31 195 Four Weddings and a Funeral (1994) 0.31 271 Good, The Bad and The Ugly, The (1966) 0.31 156 Its a Wonderful Life (1946) 0.31 146 Player, The (1992) 0.31 220 Jackie Brown (1997) 0.31 118 Boat, The (Das Boot) (1981) 0.31 210 Manhattan (1979) 0.31 158 Truth About Cats & Dogs, The (1996) 0.31 143 Ghost (1990) 0.31 227 Lone Star (1996) 0.31 125 Big Chill, The (1983) 0.31 184 256 By studying the evolution of scout values, we can identify movies that consistently feature in good scouts over time.",
                "We claim these movies will make viable scouts for other users.",
                "We found the aggregated scout values for all movies in intervals of 10,000 ratings each.",
                "A movie is said to induce a good scout if the movie was in the top 100 of the sorted list, and to induce a bad scout if it was in bottom 100 of the same list.",
                "Movies appearing consistently high over time are expected to remain up there in the future.",
                "The effective confidence in a movie m is Cm = Tm − Bm N (8) where Tm is the number of times it appeared in the top 100, Bm the number of times it appeared in the bottom 100, and N is the number of intervals considered.",
                "Using this measure, the top few movies expected to induce the best scouts are shown in Table 1.",
                "Movies that would be bad scout choices are shown in Table 2 with their associated confidences.",
                "The popularities of the movies are also displayed.",
                "Although more popular movies appear in the list of good scouts, these tables show that a blind choice of scout based on popularity alone can be potentially dangerous.",
                "Interestingly, the best scout-Being John Malkovich-is about a puppeteer who discovers a portal into a movie star, a movie that has been described variously on amazon.com as makes you feel giddy, seriously weird, comedy with depth, silly, strange, and inventive.",
                "Indicating whether someone likes this movie or not goes a long way toward situating the user in a suitable neighborhood, with similar preferences.",
                "On the other hand, several factors may have made a movie a bad scout, like the sharp variance in user preferences in the neighborhood of a movie.",
                "Two users may have the same opinion about Lawrence of Arabia, but they may differ sharply about how they felt about the other movies they saw.",
                "Bad scouts ensue when there is deviation in behavior around a common synchronization point. 5.3 Inducing good promoters Ratings that serve to promote items in a collaborative filtering system are critical to allowing a new item be recommended to users.",
                "So, inducing good promoters is important for cold-start recommendation.",
                "We note that the frequency distribution of promoter values for a sample movies ratings is also Gaussian (similar to Fig. 6).",
                "This indicates that the promotion of a movie is benefited most by the ratings of a few users, and are unaffected by the ratings of most users.",
                "We find a strong correlation between a users number of ratings and his aggregated promoter value.",
                "Fig. 8 depicts the evolution of the Pearson correlation co-efficient between the prolificity of a user (number of ratings) versus his aggregated promoter value.",
                "We expect that conspicuous shills, by recommending wrong movies to users, will be discredited with negative aggregate promoter values and should be identifiable easily.",
                "Given this observation, the obvious rule to follow when introducing a new movie is to have it rated directly by prolific users who posses high aggregated promoter values.",
                "A new movie is thus cast into the neighborhood of many other movies improving its visibility.",
                "Note, though, that a user may have long stopped using the system.",
                "Tracking promoter values consistently allows only the most active recent users to be considered. 5.4 Inducing good connectors Given the way scouts, connectors, and promoters are characterized, it follows that the movies that are part of the best scouts are also part of the best connectors.",
                "Similarly, the users that constitute the best promoters are also part of the best connectors.",
                "Good connectors are induced by ensuring a user with a high promoter value rates a movie with a high scout value.",
                "In our experiments, we find that a ratings longest standing role is often as a connector.",
                "A rating with a poor connector value is often seen due to its user being a bad promoter, or its movie being a bad scout.",
                "Such ratings can be removed from the prediction process to bring marginal improvements to recommendations.",
                "In some selected experiments, we observed that removing a set of badly behaving connectors helped improve the systems overall performance by 1.5%.",
                "The effect was even higher on a few select users who observed an improvement of above 10% in precision without much loss in recall. 5.5 Monitoring the evolution of rating roles One of the more significant contributions of our work is the ability to model the evolution of recommender systems, by studying the changing roles of ratings over time.",
                "The role and value of a rating can change depending on many factors like user behavior, redundancy, shilling effects or properties of the collaborative filtering algorithm used.",
                "Studying the dynamics of rating roles in terms of transitions between good, bad, and negligible values can provide insights into the functioning of the recommender system.",
                "We believe that a continuous visualization of these transitions will improve the ability to manage a recommender system.",
                "We classify different rating states as good, bad, or negligible.",
                "Consider a user who has rated 100 movies in a particular interval, of which 20 are part of the test set.",
                "If a scout has a value greater than 0.005, it indicates that it is uniquely involved in at least 2 concordant predictions, which we will say is good.",
                "Thus, a threshold of 0.005 is chosen to bin a rating as good, bad or negligible in terms of its scout, connector and promoter value.",
                "For instance, a rating r, at time t with role value triple [0.1, 0.001, −0.01] is classified as [scout +, connector 0, promoter −], where + indicates good, 0 indicates negligible, and − indicates bad.",
                "The positive credit held by a rating is a measure of its contribution to the betterment of the system, and the discredit is a measure of its contribution to the detriment of the system.",
                "Even though the positive roles (and the negative roles) make up a very small percentage of all ratings, their contribution supersedes their size.",
                "For example, even though only 1.7% of all ratings were classified as good scouts, they hold 79% of all positive credit in the system!",
                "Similarly, the bad scouts were just 1.4% of all ratings but hold 82% of all discredit.",
                "Note that good and bad scouts, together, comprise only 1.4% + 1.7% = 3.1% of the ratings, implying that the majority of the ratings are negligible role players as scouts (more on this later).",
                "Likewise, good connectors were 1.2% of the system, and hold 30% of all positive credit.",
                "The bad connectors (0.8% of the system) hold 36% of all discredit.",
                "Good promoters (3% of the system) hold 46% of all credit, while bad promoters (2%) hold 50% of all discredit.",
                "This reiterates that a few ratings influence most of the systems performance.",
                "Hence it is important to track transitions between them regardless of their small numbers. 257 Across different snapshots, a rating can remain in the same state or change.",
                "A good scout can become a bad scout, a good promoter can become a good connector, good and bad scouts can become vestigial, and so on.",
                "It is not practical to expect a recommender system to have no ratings in bad roles.",
                "However, it suffices to see ratings in bad roles either convert to good or vestigial roles.",
                "Similarly, observing a large number of good roles become bad ones is a sign of imminent failure of the system.",
                "We employ the principle of non-overlapping episodes [6] to count such transitions.",
                "A sequence such as: [+, 0, 0] → [+, 0, 0] → [0, +, 0] → [0, 0, 0] is interpreted as the transitions [+, 0, 0] ; [0, +, 0] : 1 [+, 0, 0] ; [0, 0, 0] : 1 [0, +, 0] ; [0, 0, 0] : 1 instead of [+, 0, 0] ; [0, +, 0] : 2 [+, 0, 0] ; [0, 0, 0] : 2 [0, +, 0] ; [0, 0, 0] : 1.",
                "See [6] for further details about this counting procedure.",
                "Thus, a rating can be in one of 27 possible states, and there are about 272 possible transitions.",
                "We make a further simplification and utilize only 9 states, indicating whether the rating is a scout, promoter, or connector, and whether it has a positive, negative, or negligible role.",
                "Ratings that serve multiple purposes are counted using multiple episode instantiations but the states themselves are not duplicated beyond the 9 restricted states.",
                "In this model, a transition such as [+, 0, +] ; [0, +, 0] : 1 is counted as [scout+] ; [scout0] : 1 [scout+] ; [connector+] : 1 [scout+] ; [promoter0] : 1 [connector0] ; [scout0] : 1 [connector0] ; [scout+] : 1 [connector0] ; [promoter0] : 1 [promoter+] ; [scout0] : 1 [promoter+] ; [connector+] : 1 [promoter+] ; [promoter0] : 1 Of these, transitions like [pX] ; [q0] where p = q, X ∈ {+, 0, −} are considered uninteresting, and only the rest are counted.",
                "Fig. 9 depicts the major transitions counted while processing the first 200,000 ratings from the MovieLens dataset.",
                "Only transitions with frequency greater than or equal to 3% are shown.",
                "The percentages for each state indicates the number of ratings that were found to be in those states.",
                "We consider transitions from any state to a good state as healthy, from any state to a bad state as unhealthy, and from any state to a vestigial state as decaying.",
                "From Fig. 9, we can observe: • The bulk of the ratings have negligible values, irrespective of their role.",
                "The majority of the transitions involve both good and bad ratings becoming negligible.",
                "Scout + (2%) Scout(1.5%) Scout 0 (96.5%) Connector + (1.2%) Connector(0.8%) Connector 0 (98%) Promoter + (3%) Promoter(2%) Promoter 0 (95%) 84% 84% 81% 74% 10% 6% 11% 77% 8% 7% 8% 82% 4% 86% 4% 68% 15% 13% 5% 5% 77% 11% 7% 5% 4% 3% 3% 3% Healthy Unhealthy Decaying Figure 9: Transitions among rating roles. • The number of good ratings is comparable to the bad ratings, and ratings are seen to switch states often, except in the case of scouts (see below). • The negative and positive scout states are not reachable through any transition, indicating that these ratings must begin as such, and cannot be coerced into these roles. • Good promoters and good connectors have a much longer survival period than scouts.",
                "Transitions that recur to these states have frequencies of 10% and 15% when compared to just 4% for scouts.",
                "Good connectors are the slowest to decay whereas (good) scouts decay the fastest. • Healthy percentages are seen on transitions between promoters and connectors.",
                "As indicated earlier, there are hardly any transitions from promoters/connectors to scouts.",
                "This indicates that, over the long run, a users rating is more useful to others (movies or other users) than to the user himself. • The percentages of healthy transitions outweigh the unhealthy ones - this hints that the system is healthy, albeit only marginally.",
                "Note that these results are conditioned by the static nature of the dataset, which is a set of ratings over a fixed window of time.",
                "However a diagram such as Fig. 9 is clearly useful for monitoring the health of a recommender system.",
                "For instance, acceptable limits can be imposed on different types of transitions and, if a transition fails to meet the threshold, the recommender system or a part of it can be brought under closer scrutiny.",
                "Furthermore, the role state transition diagram would also be the ideal place to study the effects of shilling, a topic we will consider in future research. 5.6 Characterizing neighborhoods Earlier we saw that we can characterize the neighborhood of ratings involved in creating a recommendation list L for 258 a user.",
                "In our experiment, we consider lists of length 30, and sample the lists of about 5% of users through the evolution of the model (at intervals of 10,000 ratings each) and compute their neighborhood characteristics.",
                "To simplify our presentation, we consider the percentage of the sample that fall into one of the following categories: 1.",
                "Inactive user: (SFN(u) = 0) 2.",
                "Good scouts, Good neighborhood: (SFN(u) > 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 3.",
                "Good scouts, Bad neighborhood: (SFN(u) > 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) 4.",
                "Bad scouts, Good neighborhood: (SFN(u) < 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 5.",
                "Bad scouts, Bad neighborhood: (SFN(u) < 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) From our sample set of 561 users, we found that 476 users were inactive.",
                "Of the remaining 85 users, we found 26 users had good scouts and a good neighborhood, 6 had bad scouts and a good neighborhood, 29 had good scouts and a bad neighborhood, and 24 had bad scouts and a bad neighborhood.",
                "Thus, we conjecture that 59 users (29+24+6) are in danger of leaving the system.",
                "As a remedy, users with bad scouts and a good neighborhood can be asked to reconsider rating of some movies hoping to improve the systems recommendations.",
                "The system can be expected to deliver more if they engineer some good scouts.",
                "Users with good scouts and a bad neighborhood are harder to address; this situation might entail selectively removing some connector-promoter pairs that are causing the damage.",
                "Handling users with bad scouts and bad neighborhoods is a more difficult challenge.",
                "Such a classification allows the use of different strategies to better a users experience with the system depending on his context.",
                "In future work, we intend to conduct field studies and study the improvement in performance of different strategies for different contexts. 6.",
                "CONCLUSIONS To further recommender system acceptance and deployment, we require new tools and methodologies to manage an installed recommender and develop insights into the roles played by ratings.",
                "A fine-grained characterization in terms of rating roles such as scouts, promoters, and connectors, as done here, helps such an endeavor.",
                "Although we have presented results on only the item-based algorithm with list rank accuracy as the metric, the same approach outlined here applies to user-based algorithms and other metrics.",
                "In future research, we plan to systematically study the many algorithmic parameters, tolerances, and cutoff thresholds employed here and reason about their effects on the downstream conclusions.",
                "We also aim to extend our formulation to other collaborative filtering algorithms, study the effect of shilling in altering rating roles, conduct field studies, and evaluate improvements in user experience by tweaking ratings based on their role values.",
                "Finally, we plan to develop the idea of mining the evolution of rating role patterns into a reporting and tracking system for all aspects of recommender system health. 7.",
                "REFERENCES [1] Cosley, D., Lam, S., Albert, I., Konstan, J., and Riedl, J.",
                "Is Seeing Believing? : How Recommender System Interfaces Affect Users Opinions.",
                "In Proc.",
                "CHI (2001), pp. 585-592. [2] Herlocker, J. L., Konstan, J.",
                "A., Borchers, A., and Riedl, J.",
                "An Algorithmic Framework for Performing Collaborative Filtering.",
                "In Proc.",
                "SIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J.",
                "A., Terveen, L. G., and Riedl, J. T. Evaluating Collaborative Filtering Recommender Systems.",
                "ACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J.",
                "A.",
                "Personal communication. 2003. [5] Lam, S. K., and Riedl, J. Shilling Recommender Systems for Fun and Profit.",
                "In Proceedings of the 13th International World Wide Web Conference (2004), ACM Press, pp. 393-402. [6] Laxman, S., Sastry, P. S., and Unnikrishnan, K. P. Discovering Frequent Episodes and Learning Hidden Markov Models: A Formal Connection.",
                "IEEE Transactions on Knowledge and Data Engineering Vol. 17, 11 (2005), 1505-1517. [7] McLaughlin, M. R., and Herlocker, J. L. A Collaborative Filtering Algorithm and Evaluation Metric that Accurately Model the User Experience.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (2004), pp. 329 - 336. [8] OMahony, M., Hurley, N. J., Kushmerick, N., and Silvestre, G. Collaborative Recommendation: A Robustness Analysis.",
                "ACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J.",
                "A., and Riedl, J.",
                "Getting to Know You: Learning New User Preferences in Recommender Systems.",
                "In Proceedings of the 2002 Conference on Intelligent User Interfaces (IUI 2002) (2002), pp. 127-134. [10] Rashid, A. M., Karypis, G., and Riedl, J.",
                "Influence in Ratings-Based Recommender Systems: An Algorithm-Independent Approach.",
                "In Proc. of the SIAM International Conference on Data Mining (2005). [11] Resnick, P., Iacovou, N., Sushak, M., Bergstrom, P., and Riedl, J. GroupLens: An Open Architecture for Collaborative Filtering of Netnews.",
                "In Proceedings of the Conference on Computer Supported Collaborative Work (CSCW94) (1994), ACM Press, pp. 175-186. [12] Sarwar, B., Karypis, G., Konstan, J., and Reidl, J. Item-Based Collaborative Filtering Recommendation Algorithms.",
                "In Proceedings of the Tenth International World Wide Web Conference (WWW10) (2001), pp. 285-295. [13] Schein, A., Popescu, A., Ungar, L., and Pennock, D. Methods and Metrics for Cold-Start Recommendation.",
                "In Proc.",
                "SIGIR (2002), pp. 253-260. 259"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Scouts,"
            ],
            "translated_text": "",
            "candidates": [
                "promotores y conectores: los roles de las clasificaciones en el filtrado colaborativo \"más cercano\" Filtrado Bharath Kumar Mohan Vecino más cercano",
                "más cercano"
            ],
            "error": []
        },
        "collaborative filtering algorithm": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Scouts, Promoters, and Connectors: The Roles of Ratings in Nearest Neighbor Collaborative Filtering Bharath Kumar Mohan Dept.",
                "of CSA Indian Institute of Science Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. of Computer Science Eastern Michigan University Ypsilanti, MI 48917, USA bkeller@emich.edu Naren Ramakrishnan Dept. of Computer Science Virginia Tech, Blacksburg VA 24061, USA naren@cs.vt.edu ABSTRACT Recommender systems aggregate individual user ratings into predictions of products or services that might interest visitors.",
                "The quality of this aggregation process crucially affects the user experience and hence the effectiveness of recommenders in e-commerce.",
                "We present a novel study that disaggregates global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearestneighbor collaborative filtering.",
                "In particular, we formulate three roles-scouts, promoters, and connectors-that capture how users receive recommendations, how items get recommended, and how ratings of these two types are themselves connected (resp.).",
                "These roles find direct uses in improving recommendations for users, in better targeting of items and, most importantly, in helping monitor the health of the system as a whole.",
                "For instance, they can be used to track the evolution of neighborhoods, to identify rating subspaces that do not contribute (or contribute negatively) to system performance, to enumerate users who are in danger of leaving, and to assess the susceptibility of the system to attacks such as shilling.",
                "We argue that the three rating roles presented here provide broad primitives to manage a recommender system and its community.",
                "Categories and Subject Descriptors H.4.2 [Information Systems Applications]: Types of Systems-Decision support; J.4 [Computer Applications]: Social and Behavioral Sciences General Terms Algorithms, Human Factors 1.",
                "INTRODUCTION Recommender systems have become integral to e-commerce, providing technology that suggests products to a visitor based on previous purchases or rating history.",
                "Collaborative filtering, a common form of recommendation, predicts a users rating for an item by combining (other) ratings of that user with other users ratings.",
                "Significant research has been conducted in implementing fast and accurate collaborative filtering algorithms [2, 7], designing interfaces for presenting recommendations to users [1], and studying the robustness of these algorithms [8].",
                "However, with the exception of a few studies on the influence of users [10], little attention has been paid to unraveling the inner workings of a recommender in terms of the individual ratings and the roles they play in making (good) recommendations.",
                "Such an understanding will give an important handle to monitoring and managing a recommender system, to engineer mechanisms to sustain the recommender, and thereby ensure its continued success.",
                "Our motivation here is to disaggregate global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearest-neighbor collaborative filtering.",
                "We identify three possible roles: (scouts) to connect the user into the system to receive recommendations, (promoters) to connect an item into the system to be recommended, and (connectors) to connect ratings of these two kinds.",
                "Viewing ratings in this way, we can define the contribution of a rating in each role, both in terms of allowing recommendations to occur, and in terms of influence on the quality of recommendations.",
                "In turn, this capability helps support scenarios such as: 1.",
                "Situating users in better neighborhoods: A users ratings may inadvertently connect the user to a neighborhood for which the users tastes may not be a perfect match.",
                "Identifying ratings responsible for such bad recommendations and suggesting new items to rate can help situate the user in a better neighborhood. 2.",
                "Targeting items: Recommender systems suffer from lack of user participation, especially in cold-start scenarios [13] involving newly arrived items.",
                "Identifying users who can be encouraged to rate specific items helps ensure coverage of the recommender system. 3.",
                "Monitoring the evolution of the recommender system and its stakeholders: A recommender system is constantly under change: growing with new users and 250 items, shrinking with users leaving the system, items becoming irrelevant, and parts of the system under attack.",
                "Tracking the roles of a rating and its evolution over time provides many insights into the health of the system, and how it could be managed and improved.",
                "These include being able to identify rating subspaces that do not contribute (or contribute negatively) to system performance, and could be removed; to enumerate users who are in danger of leaving, or have left the system; and to assess the susceptibility of the system to attacks such as shilling [5].",
                "As we show, the characterization of rating roles presented here provides broad primitives to manage a recommender system and its community.",
                "The rest of the paper is organized as follows.",
                "Background on nearest-neighbor collaborative filtering and algorithm evaluation is discussed in Section 2.",
                "Section 3 defines and discusses the roles of a rating, and Section 4 defines measures of the contribution of a rating in each of these roles.",
                "In Section 5, we illustrate the use of these roles to address the goals outlined above. 2.",
                "BACKGROUND 2.1 Algorithms Nearest-neighbor collaborative filtering algorithms either use neighborhoods of users or neighborhoods of items to compute a prediction.",
                "An algorithm of the first kind is called user-based, and one of the second kind is called itembased [12].",
                "In both families of algorithms, neighborhoods are formed by first computing the similarity between all pairs of users (for user-based) or items (for item-based).",
                "Predictions are then computed by aggregating ratings, which in a user-based algorithm involves aggregating the ratings of the target item by the users neighbors and, in an item-based algorithm, involves aggregating the users ratings of items that are neighbors of the target item.",
                "Algorithms within these families differ in the definition of similarity, formation of neighborhoods, and the computation of predictions.",
                "We consider a user-based algorithm based on that defined for GroupLens [11] with variations from Herlocker et al. [2], and an item-based algorithm similar to that of Sarwar et al. [12].",
                "The algorithm used by Resnick et al. [11] defines the similarity of two users u and v as the Pearson correlation of their common ratings: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2 , where Iu is the set of items rated by user u, ru,i is user us rating for item i, and ¯ru is the average rating of user u (similarly for v).",
                "Similarity computed in this manner is typically scaled by a factor proportional to the number of common ratings, to reduce the chance of making a recommendation made on weak connections: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), where γ ≈ 5 is a constant used as a lower limit in scaling [2].",
                "These new similarities are then used to define a static neighborhood Nu for each user u consisting of the top K users most similar to user u.",
                "A prediction for user u and item i is computed by a weighted average of the ratings by the neighbors pu,i = ¯ru + P v∈V sim (u, v)(rv,i − ¯rv) P v∈V sim (u, v) (1) where V = Nu ∩ Ui is the set of users most similar to u who have rated i.",
                "The item-based algorithm we use is the one defined by Sarwar et al. [12].",
                "In this algorithm, similarity is defined as the adjusted cosine measure sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) where Ui is the set of users who have rated item i.",
                "As for the user-based algorithm, the similarity weights are adjusted proportionally to the number of users that have rated the items in common sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). (3) Given the similarities, the neighborhood Ni of an item i is defined as the top K most similar items for that item.",
                "A prediction for user u and item i is computed as the weighted average pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) where J = Ni ∩ Iu is the set of items rated by u that are most similar to i. 2.2 Evaluation Recommender algorithms have typically been evaluated using measures of predictive accuracy and coverage [3].",
                "Studies on recommender algorithms, notably Herlocker et al. [2] and Sarwar et al. [12], typically compute predictive accuracy by dividing a set of ratings into training and test sets, and compute the prediction for an item in the test set using the ratings in the training set.",
                "A standard measure of predictive accuracy is mean absolute error (MAE), which for a test set T = {(u, i)} is defined as, MAE = P (u,i)∈T |pu,i − ru,i| |T | . (5) Coverage has a number of definitions, but generally refers to the proportion of items that can be predicted by the algorithm [3].",
                "A practical issue with predictive accuracy is that users typically are presented with recommendation lists, and not individual numeric predictions.",
                "Recommendation lists are lists of items in decreasing order of prediction (sometimes stated in terms of star-ratings), and so predictive accuracy may not be reflective of the accuracy of the list.",
                "So, instead we can measure recommendation or rank accuracy, which indicates the extent to which the list is in the correct order.",
                "Herlocker et al. [3] discuss a number of rank accuracy measures, which range from Kendalls Tau to measures that consider the fact that users tend to only look at a prefix of the list [5].",
                "Kendalls Tau measures the number of inversions when comparing ordered pairs in the true user ordering of 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figure 1: Ratings in simple movie recommender. items and the recommended order, and is defined as τ = C − D p (C + D + TR)(C + D + TP) (6) where C is the number of pairs that the system predicts in the correct order, D the number of pairs the system predicts in the wrong order, TR the number of pairs in the true ordering that have the same ratings, and TP is the number of pairs in the predicted ordering that have the same ratings [3].",
                "A shortcoming of the Tau metric is that it is oblivious to the position in the ordered list where the inversion occurs [3].",
                "For instance, an inversion toward the end of the list is given the same weight as one in the beginning.",
                "One solution is to consider inversions only in the top few items in the recommended list or to weight inversions based on their position in the list. 3.",
                "ROLES OF A RATING Our basic observation is that each rating plays a different role in each prediction in which it is used.",
                "Consider a simplified movie recommender system with three users Jim, Jeff, and Tom and their ratings for a few movies, as shown in Fig. 1. (For this initial discussion we will not consider the rating values involved.)",
                "The recommender predicts whether Tom will like The Mask using the other already available ratings.",
                "How this is done depends on the algorithm: 1.",
                "An item-based <br>collaborative filtering algorithm</br> constructs a neighborhood of movies around The Mask by using the ratings of users who rated The Mask and other movies similarly (e.g., Jims ratings of The Matrix and The Mask; and Jeffs ratings of Star Wars and The Mask).",
                "Toms ratings of those movies are then used to make a prediction for The Mask. 2.",
                "A user-based <br>collaborative filtering algorithm</br> would construct a neighborhood around Tom by tracking other users whose rating behaviors are similar to Toms (e.g., Tom and Jeff have rated Star Wars; Tom and Jim have rated The Matrix).",
                "The prediction of Toms rating for The Mask is then based on the ratings of Jeff and Tim.",
                "Although the nearest-neighbor algorithms aggregate the ratings to form neighborhoods used to compute predictions, we can disaggregate the similarities to view the computation of a prediction as simultaneously following parallel paths of ratings.",
                "So, irrespective of the <br>collaborative filtering algorithm</br> used, we can visualize the prediction of Toms rating of The Mask as walking through a sequence of ratings.",
                "In Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figure 2: Ratings used to predict The Mask for Tom.",
                "Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figure 3: Prediction of The Mask for Tom in which a rating is used more than once. this example, two paths were used for this prediction as depicted in Fig. 2: (p1, p2, p3) and (q1, q2, q3).",
                "Note that these paths are undirected, and are all of length 3.",
                "Only the order in which the ratings are traversed is different between the item-based algorithm (e.g., (p3, p2, p1), (q3, q2, q1)) and the user-based algorithm (e.g., (p1, p2, p3), (q1, q2, q3).)",
                "A rating can be part of many paths for a single prediction as shown in Fig. 3, where three paths are used for a prediction, two of which follow p1: (p1, p2, p3) and (p1, r2, r3).",
                "Predictions in a collaborative filtering algorithms may involve thousands of such walks in parallel, each playing a part in influencing the predicted value.",
                "Each prediction path consists of three ratings, playing roles that we call scouts, promoters, and connectors.",
                "To illustrate these roles, consider the path (p1, p2, p3) in Fig. 2 used to make a prediction of The Mask for Tom: 1.",
                "The rating p1 (Tom → Star Wars) makes a connection from Tom to other ratings that can be used to predict Toms rating for The Mask.",
                "This rating serves as a scout in the bipartite graph of ratings to find a path that leads to The Mask. 2.",
                "The rating p2 (Jeff → Star Wars) helps the system recommend The Mask to Tom by connecting the scout to the promoter. 3.",
                "The rating p3 (Jeff → The Mask) allows connections to The Mask, and, therefore, promotes this movie to Tom.",
                "Formally, given a prediction pu,a of a target item a for user u, a scout for pu,a is a rating ru,i such that there exists a user v with ratings rv,a and rv,i for some item i; a promoter for pu,a is a rating rv,a for some user v, such that there exist ratings rv,i and ru,i for an item i, and; a connector for pu,a 252 Jim Tom Jeff Jerry My Cousin Vinny The Matrix Star Wars The Mask Jurasic Park Figure 4: Scouts, promoters, and connectors. is a rating rv,i by some user v and rating i, such that there exists ratings ru,i and rv,a.",
                "The scouts, connectors, and promoters for the prediction of Toms rating of The Mask are p1 and q1, p2 and q2, and p3 and q3 (respectively).",
                "Each of these roles has a value in the recommender to the user, the users neighborhood, and the system in terms of allowing recommendations to be made. 3.1 Roles in Detail Ratings that act as scouts tend to help the recommender system suggest more movies to the user, though the extent to which this is true depends on the rating behavior of other users.",
                "For example, in Fig. 4 the rating Tom → Star Wars helps the system recommend only The Mask to him, while Tom → The Matrix helps recommend The Mask, Jurassic Park, and My Cousin Vinny.",
                "Tom makes a connection to Jim who is a prolific user of the system, by rating The Matrix.",
                "However, this does not make The Matrix the best movie to rate for everyone.",
                "For example, Jim is benefited equally by both The Mask and The Matrix, which allow the system to recommend Star Wars to him.",
                "His rating of The Mask is the best scout for Jeff, and Jerrys only scout is his rating of Star Wars.",
                "This suggests that good scouts allow a user to build similarity with prolific users, and thereby ensure they get more from the system.",
                "While scouts represent beneficial ratings from the perspective of a user, promoters are their duals, and are of benefit to items.",
                "In Fig. 4, My Cousin Vinny benefits from Jims rating, since it allows recommendations to Jeff and Tom.",
                "The Mask is not so dependent on just one rating, since the ratings by Jim and Jeff help it.",
                "On the other hand, Jerrys rating of Star Wars does not help promote it to any other user.",
                "We conclude that a good promoter connects an item to a broader neighborhood of other items, and thereby ensures that it is recommended to more users.",
                "Connectors serve a crucial role in a recommender system that is not as obvious.",
                "The movies My Cousin Vinny and Jurassic Park have the highest recommendation potential since they can be recommended to Jeff, Jerry and Tom based on the linkage structure illustrated in Fig. 4.",
                "Beside the fact that Jim rated these movies, these recommendations are possible only because of the ratings Jim → The Matrix and Jim → The Mask, which are the best connectors.",
                "A connector improves the systems ability to make recommendations with no explicit gain for the user.",
                "Note that every rating can be of varied benefit in each of these roles.",
                "The rating Jim → My Cousin Vinny is a poor scout and connector, but is a very good promoter.",
                "The rating Jim → The Mask is a reasonably good scout, a very good connector, and a good promoter.",
                "Finally, the rating Jerry → Star Wars is a very good scout, but is of no value as a connector or promoter.",
                "As illustrated here, a rating can have different value in each of the three roles in terms of whether a recommendation can be made or not.",
                "We could measure this value by simply counting the number of times a rating is used in each role, which alone would be helpful in characterizing the behavior of a system.",
                "But we can also measure the contribution of each rating to the quality of recommendations or health of the system.",
                "Since every prediction is a combined effort of several recommendation paths, we are interested in discerning the influence of each rating (and, hence, each path) in the system towards the systems overall error.",
                "We can understand the dynamics of the system at a finer granularity by tracking the influence of a rating according to the role played.",
                "The next section describes the approach to measuring the values of a rating in each role. 4.",
                "CONTRIBUTIONS OF RATINGS As weve seen, a rating may play different roles in different predictions and, in each prediction, contribute to the quality of a prediction in different ways.",
                "Our approach can use any numeric measure of a property of system health, and assigns credit (or blame) to each rating proportional to its influence in the prediction.",
                "By tracking the role of each rating in a prediction, we can accumulate the credit for a rating in each of the three roles, and also track the evolution of the roles of rating over time in the system.",
                "This section defines the methodology for computing the contribution of ratings by first defining the influence of a rating, and then instantiating the approach for predictive accuracy, and then rank accuracy.",
                "We also demonstrate how these contributions can be aggregated to study the neighborhood of ratings involved in computing a users recommendations.",
                "Note that although our general formulation for rating influence is algorithm independent, due to space considerations, we present the approach for only item-based collaborative filtering.",
                "The definition for user-based algorithms is similar and will be presented in an expanded version of this paper. 4.1 Influence of Ratings Recall that an item-based approach to collaborative filtering relies on building item neighborhoods using the similarity of ratings by the same user.",
                "As described earlier, similarity is defined by the adjusted cosine measure (Equations (2) and (3)).",
                "A set of the top K neighbors is maintained for all items for space and computational efficiency.",
                "A prediction of item i for a user u is computed as the weighted deviation from the items mean rating as shown in Equation (4).",
                "The list of recommendations for a user is then the list of items sorted in descending order of their predicted values.",
                "We first define impact(a, i, j), the impact a user a has in determining the similarity between two items i and j.",
                "This is the change in the similarity between i and j when as rating is removed, and is defined as impact(a, i, j) = |sim (i, j) − sim¯a(i, j)| P w∈Cij |sim (i, j) − sim ¯w(i, j)| where Cij = {u ∈ U | ∃ ru,i, ru,j ∈ R(u)} is the set of coraters 253 of items i and j (users who rate both i and j), R(u) is the set of ratings provided by user u, and sim¯a(i, j) is the similarity of i and j when the ratings of user a are removed sim¯a(i, j) = P v∈U\\{a} (ru,i − ¯ru)(ru,j − ¯ru) qP u∈U\\{a}(ru,i − ¯ru)2 qP u∈U\\{a}(ru,j − ¯ru)2 , and adjusted for the number of raters sim¯a(i, j) = max(|Ui ∩ Uj| − 1, γ) γ · sim(i, j).",
                "If all coraters of i and j rate them identically, we define the impact as impact(a, i, j) = 1 |Cij| since P w∈Cij |sim (i, j) − sim ¯w(i, j)| = 0.",
                "The influence of each path (u, j, v, i) = [ru,j, rv,j, rv,i] in the prediction of pu,i is given by influence(u, j, v, i) = sim (i, j) P l∈Ni∩Iu sim (i, l) · impact(v, i, j) It follows that the sum of influences over all such paths, for a given set of endpoints, is 1. 4.2 Role Values for Predictive Accuracy The value of a rating in each role is computed from the influence depending on the evaluation measure employed.",
                "Here we illustrate the approach using predictive accuracy as the evaluation metric.",
                "In general, the goodness of a prediction decides whether the ratings involved must be credited or discredited for their role.",
                "For predictive accuracy, the error in prediction e = |pu,i − ru,i| is mapped to a comfort level using a mapping function M(e).",
                "Anecdotal evidence suggests that users are unable to discern errors less than 1.0 (for a rating scale of 1 to 5) [4], and so an error less than 1.0 is considered acceptable, but anything larger is not.",
                "We hence define M(e) as (1 − e) binned to an appropriate value in [−1, −0.5, 0.5, 1].",
                "For each prediction pu,i, M(e) is attributed to all the paths that assisted the computation of pu,i, proportional to their influences.",
                "This tribute, M(e)∗influence(u, j, v, i), is in turn inherited by each of the ratings in the path [ru,j, rv,j, rv,i], with the credit/blame accumulating to the respective roles of ru,j as a scout, rv,j as a connector, and rv,i as a promoter.",
                "In other words, the scout value SF(ru,j), the connector value CF(rv,j) and the promoter value PF(rv,i) are all incremented by the tribute amount.",
                "Over a large number of predictions, scouts that have repeatedly resulted in big error rates have a big negative scout value, and vice versa (similarly with the other roles).",
                "Every rating is thus summarized by its triple [SF, CF, PF]. 4.3 Role Values for Rank Accuracy We now define the computation of the contribution of ratings to observed rank accuracy.",
                "For this computation, we must know the users preference order for a set of items for which predictions can be computed.",
                "We assume that we have a test set of the users ratings of the items presented in the recommendation list.",
                "For every pair of items rated by a user in the test data, we check whether the predicted order is concordant with his preference.",
                "We say a pair (i, j) is concordant (with error ) whenever one of the following holds: • if (ru,i < ru,j) then (pu,i − pu,j < ); • if (ru,i > ru,j) then (pu,i − pu,j > ); or • if (ru,i = ru,j) then (|pu,i − pu,j| ≤ ).",
                "Similarly, a pair (i, j) is discordant (with error ) if it is not concordant.",
                "Our experiments described below use an error tolerance of = 0.1.",
                "All paths involved in the prediction of the two items in a concordant pair are credited, and the paths involved in a discordant pair are discredited.",
                "The credit assigned to a pair of items (i, j) in the recommendation list for user u is computed as c(i, j) = ( t T · 1 C+D if (i, j) are concordant − t T · 1 C+D if (i, j) are discordant (7) where t is the number of items in the users test set whose ratings could be predicted, T is the number of items rated by user u in the test set, C is the number of concordances and D is the number of discordances.",
                "The credit c is then divided among all paths responsible for predicting pu,i and pu,j proportional to their influences.",
                "We again add the role values obtained from all the experiments to form a triple [SF, CF, PF] for each rating. 4.4 Aggregating rating roles After calculating the role values for individual ratings, we can also use these values to study neighborhoods and the system.",
                "Here we consider how we can use the role values to characterize the health of a neighborhood.",
                "Consider the list of top recommendations presented to a user at a specific point in time.",
                "The <br>collaborative filtering algorithm</br> traversed many paths in his neighborhood through his scouts and other connectors and promoters to make these recommendations.",
                "We call these ratings the recommender neighborhood of the user.",
                "The user implicitly chooses this neighborhood of ratings through the items he rates.",
                "Apart from the <br>collaborative filtering algorithm</br>, the health of this neighborhood completely influences a users satisfaction with the system.",
                "We can characterize a users recommender neighborhood by aggregating the individual role values of the ratings involved, weighted by the influence of individual ratings in determining his recommended list.",
                "Different sections of the users neighborhood wield varied influence on his recommendation list.",
                "For instance, ratings reachable through highly rated items have a bigger say in the recommended items.",
                "Our aim is to study the system and classify users with respect to their positioning in a healthy or unhealthy neighborhood.",
                "A user can have a good set of scouts, but may be exposed to a neighborhood with bad connectors and promoters.",
                "He can have a good neighborhood, but his bad scouts may ensure the neighborhoods potential is rendered useless.",
                "We expect that users with good scouts and good neighborhoods will be most satisfied with the system in the future.",
                "A users neighborhood is characterized by a triple that represents the weighted sum of the role values of individual ratings involved in making recommendations.",
                "Consider a user u and his ordered list of recommendations L. An item i 254 in the list is weighted inversely, as K(i), depending on its position in the list.",
                "In our studies we use K(i) = p position(i).",
                "Several paths of ratings [ru,j, rv,j, rv,i] are involved in predicting pu,i which ultimately decides its position in L, each with influence(u, j, v, i).",
                "The recommender neighborhood of a user u is characterized by the triple, [SFN(u), CFN(u), PFN(u)] where SFN(u) = X i∈L P [ru,j ,rv,j ,rv,i] SF(ru,j)influence(u, j, v, i) K(i) !",
                "CFN(u) and PFN(u) are defined similarly.",
                "This triple estimates the quality of us recommendations based on the past track record of the ratings involved in their respective roles. 5.",
                "EXPERIMENTATION As we have seen, we can assign role values to each rating when evaluating a collaborative filtering system.",
                "In this section, we demonstrate the use of this approach to our overall goal of defining an approach to monitor and manage the health of a recommender system through experiments done on the MovieLens million rating dataset.",
                "In particular, we discuss results relating to identifying good scouts, promoters, and connectors; the evolution of rating roles; and the characterization of user neighborhoods. 5.1 Methodology Our experiments use the MovieLens million rating dataset, which consists of ratings by 6040 users of 3952 movies.",
                "The ratings are in the range 1 to 5, and are labeled with the time the rating was given.",
                "As discussed before, we consider only the item-based algorithm here (with item neighborhoods of size 30) and, due to space considerations, only present role value results for rank accuracy.",
                "Since we are interested in the evolution of the rating role values over time, the model of the recommender system is built by processing ratings in their arrival order.",
                "The timestamping provided by MovieLens is hence crucial for the analyses presented here.",
                "We make assessments of rating roles at intervals of 10,000 ratings and processed the first 200,000 ratings in the dataset (giving rise to 20 snapshots).",
                "We incrementally update the role values as the time ordered ratings are merged into the model.",
                "To keep the experiment computationally manageable, we define a test dataset for each user.",
                "As the time ordered ratings are merged into the model, we label a small randomly selected percentage (20%) as test data.",
                "At discrete epochs, i.e., after processing every 10,000 ratings, we compute the predictions for the ratings in the test data, and then compute the role values for the ratings used in the predictions.",
                "One potential criticism of this methodology is that the ratings in the test set are never evaluated for their roles.",
                "We overcome this concern by repeating the experiment, using different random seeds.",
                "The probability that every rating is considered for evaluation is then considerably high: 1 − 0.2n , where n is the number of times the experiment is repeated with different random seeds.",
                "The results here are based on n = 4 repetitions.",
                "The item-based collaborative filtering algorithms performance was ordinary with respect to rank accuracy.",
                "Fig. 5 shows a plot of the precision and recall as ratings were merged in time order into the model.",
                "The recall was always high, but the average precision was just about 53%. 0 0.2 0.4 0.6 0.8 1 1.2 10000 30000 50000 70000 90000110000130000150000 Ratings merged into model Value Precision Recall Figure 5: Precision and recall for the item-based <br>collaborative filtering algorithm</br>. 5.2 Inducing good scouts The ratings of a user that serve as scouts are those that allow the user to receive recommendations.",
                "We claim that users with ratings that have respectable scout values will be happier with the system than those with ratings with low scout values.",
                "Note that the item-based algorithm discussed here produces recommendation lists with nearly half of the pairs in the list discordant from the users preference.",
                "Whether all of these discordant pairs are observable by the user is unclear, however, certainly this suggests that there is a need to be able to direct users to items whose ratings would improve the lists.",
                "The distribution of the scout values for most users ratings are Gaussian with mean zero.",
                "Fig. 6 shows the frequency distribution of scout values for a sample user at a given snapshot.",
                "We observe that a large number of ratings never serve as scouts for their users.",
                "A relatable scenario is when Amazons recommender makes suggestions of books or items based on other items that were purchased as gifts.",
                "With simple relevance feedback from the user, such ratings can be isolated as bad scouts and discounted from future predictions.",
                "Removing bad scouts was found to be worthwhile for individual users but the overall performance improvement was only marginal.",
                "An obvious question is whether good scouts can be formed by merely rating popular movies as suggested by Rashid et al. [9].",
                "They show that a mix of popularity and rating entropy identifies the best items to suggest to new users when evaluated using MAE.",
                "Following their intuition, we would expect to see a higher correlation between popularityentropy and good scouts.",
                "We measured the Pearson correlation coefficient between aggregated scout values for a movie with the popularity of a movie (number of times it is rated); and with its popularity*variance measure at different snapshots of the system.",
                "Note that the scout values were initially anti-correlated with popularity (Fig. 7), but became moderately correlated as the system evolved.",
                "Both popularity and popularity*variance performed similarly.",
                "A possible explanation is that there has been insufficient time for the popular movies to accumulate ratings. 255 -10 0 10 20 30 40 50 60 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 Scout Value Frequency Figure 6: Distribution of scout values for a sample user. -0.4 -0.2 0 0.2 0.4 0.6 0.8 30000 60000 90000 120000 150000 180000 Popularity Pop*Var Figure 7: Correlation between aggregated scout value and item popularity (computed at different intervals). -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 30000 60000 90000 120000 150000 180000 Figure 8: Correlation between aggregated promoter value and user prolificity (computed at different intervals).",
                "Table 1: Movies forming the best scouts.",
                "Best Scouts Conf.",
                "Pop.",
                "Being John Malkovich (1999) 1.00 445 Star Wars: Episode IV - A New Hope (1977) 0.92 623 Princess Bride, The (1987) 0.85 477 Sixth Sense, The (1999) 0.85 617 Matrix, The (1999) 0.77 522 Ghostbusters (1984) 0.77 441 Casablanca (1942) 0.77 384 Insider, The (1999) 0.77 235 American Beauty (1999) 0.69 624 Terminator 2: Judgment Day (1991) 0.69 503 Fight Club (1999) 0.69 235 Shawshank Redemption, The (1994) 0.69 445 Run Lola Run (Lola rennt) (1998) 0.69 220 Terminator, The (1984) 0.62 450 Usual Suspects, The (1995) 0.62 326 Aliens (1986) 0.62 385 North by Northwest (1959) 0.62 245 Fugitive, The (1993) 0.62 402 End of Days (1999) 0.62 132 Raiders of the Lost Ark (1981) 0.54 540 Schindlers List (1993) 0.54 453 Back to the Future (1985) 0.54 543 Toy Story (1995) 0.54 419 Alien (1979) 0.54 415 Abyss, The (1989) 0.54 345 2001: A Space Odyssey (1968) 0.54 358 Dogma (1999) 0.54 228 Little Mermaid, The (1989) 0.54 203 Table 2: Movies forming the worst scouts.",
                "Worst scouts Conf.",
                "Pop.",
                "Harold and Maude (1971) 0.46 141 Grifters, The (1990) 0.46 180 Sting, The (1973) 0.38 244 Godfather: Part III, The (1990) 0.38 154 Lawrence of Arabia (1962) 0.38 167 High Noon (1952) 0.38 84 Women on the Verge of a... (1988) 0.38 113 Grapes of Wrath, The (1940) 0.38 115 Duck Soup (1933) 0.38 131 Arsenic and Old Lace (1944) 0.38 138 Midnight Cowboy (1969) 0.38 137 To Kill a Mockingbird (1962) 0.31 195 Four Weddings and a Funeral (1994) 0.31 271 Good, The Bad and The Ugly, The (1966) 0.31 156 Its a Wonderful Life (1946) 0.31 146 Player, The (1992) 0.31 220 Jackie Brown (1997) 0.31 118 Boat, The (Das Boot) (1981) 0.31 210 Manhattan (1979) 0.31 158 Truth About Cats & Dogs, The (1996) 0.31 143 Ghost (1990) 0.31 227 Lone Star (1996) 0.31 125 Big Chill, The (1983) 0.31 184 256 By studying the evolution of scout values, we can identify movies that consistently feature in good scouts over time.",
                "We claim these movies will make viable scouts for other users.",
                "We found the aggregated scout values for all movies in intervals of 10,000 ratings each.",
                "A movie is said to induce a good scout if the movie was in the top 100 of the sorted list, and to induce a bad scout if it was in bottom 100 of the same list.",
                "Movies appearing consistently high over time are expected to remain up there in the future.",
                "The effective confidence in a movie m is Cm = Tm − Bm N (8) where Tm is the number of times it appeared in the top 100, Bm the number of times it appeared in the bottom 100, and N is the number of intervals considered.",
                "Using this measure, the top few movies expected to induce the best scouts are shown in Table 1.",
                "Movies that would be bad scout choices are shown in Table 2 with their associated confidences.",
                "The popularities of the movies are also displayed.",
                "Although more popular movies appear in the list of good scouts, these tables show that a blind choice of scout based on popularity alone can be potentially dangerous.",
                "Interestingly, the best scout-Being John Malkovich-is about a puppeteer who discovers a portal into a movie star, a movie that has been described variously on amazon.com as makes you feel giddy, seriously weird, comedy with depth, silly, strange, and inventive.",
                "Indicating whether someone likes this movie or not goes a long way toward situating the user in a suitable neighborhood, with similar preferences.",
                "On the other hand, several factors may have made a movie a bad scout, like the sharp variance in user preferences in the neighborhood of a movie.",
                "Two users may have the same opinion about Lawrence of Arabia, but they may differ sharply about how they felt about the other movies they saw.",
                "Bad scouts ensue when there is deviation in behavior around a common synchronization point. 5.3 Inducing good promoters Ratings that serve to promote items in a collaborative filtering system are critical to allowing a new item be recommended to users.",
                "So, inducing good promoters is important for cold-start recommendation.",
                "We note that the frequency distribution of promoter values for a sample movies ratings is also Gaussian (similar to Fig. 6).",
                "This indicates that the promotion of a movie is benefited most by the ratings of a few users, and are unaffected by the ratings of most users.",
                "We find a strong correlation between a users number of ratings and his aggregated promoter value.",
                "Fig. 8 depicts the evolution of the Pearson correlation co-efficient between the prolificity of a user (number of ratings) versus his aggregated promoter value.",
                "We expect that conspicuous shills, by recommending wrong movies to users, will be discredited with negative aggregate promoter values and should be identifiable easily.",
                "Given this observation, the obvious rule to follow when introducing a new movie is to have it rated directly by prolific users who posses high aggregated promoter values.",
                "A new movie is thus cast into the neighborhood of many other movies improving its visibility.",
                "Note, though, that a user may have long stopped using the system.",
                "Tracking promoter values consistently allows only the most active recent users to be considered. 5.4 Inducing good connectors Given the way scouts, connectors, and promoters are characterized, it follows that the movies that are part of the best scouts are also part of the best connectors.",
                "Similarly, the users that constitute the best promoters are also part of the best connectors.",
                "Good connectors are induced by ensuring a user with a high promoter value rates a movie with a high scout value.",
                "In our experiments, we find that a ratings longest standing role is often as a connector.",
                "A rating with a poor connector value is often seen due to its user being a bad promoter, or its movie being a bad scout.",
                "Such ratings can be removed from the prediction process to bring marginal improvements to recommendations.",
                "In some selected experiments, we observed that removing a set of badly behaving connectors helped improve the systems overall performance by 1.5%.",
                "The effect was even higher on a few select users who observed an improvement of above 10% in precision without much loss in recall. 5.5 Monitoring the evolution of rating roles One of the more significant contributions of our work is the ability to model the evolution of recommender systems, by studying the changing roles of ratings over time.",
                "The role and value of a rating can change depending on many factors like user behavior, redundancy, shilling effects or properties of the <br>collaborative filtering algorithm</br> used.",
                "Studying the dynamics of rating roles in terms of transitions between good, bad, and negligible values can provide insights into the functioning of the recommender system.",
                "We believe that a continuous visualization of these transitions will improve the ability to manage a recommender system.",
                "We classify different rating states as good, bad, or negligible.",
                "Consider a user who has rated 100 movies in a particular interval, of which 20 are part of the test set.",
                "If a scout has a value greater than 0.005, it indicates that it is uniquely involved in at least 2 concordant predictions, which we will say is good.",
                "Thus, a threshold of 0.005 is chosen to bin a rating as good, bad or negligible in terms of its scout, connector and promoter value.",
                "For instance, a rating r, at time t with role value triple [0.1, 0.001, −0.01] is classified as [scout +, connector 0, promoter −], where + indicates good, 0 indicates negligible, and − indicates bad.",
                "The positive credit held by a rating is a measure of its contribution to the betterment of the system, and the discredit is a measure of its contribution to the detriment of the system.",
                "Even though the positive roles (and the negative roles) make up a very small percentage of all ratings, their contribution supersedes their size.",
                "For example, even though only 1.7% of all ratings were classified as good scouts, they hold 79% of all positive credit in the system!",
                "Similarly, the bad scouts were just 1.4% of all ratings but hold 82% of all discredit.",
                "Note that good and bad scouts, together, comprise only 1.4% + 1.7% = 3.1% of the ratings, implying that the majority of the ratings are negligible role players as scouts (more on this later).",
                "Likewise, good connectors were 1.2% of the system, and hold 30% of all positive credit.",
                "The bad connectors (0.8% of the system) hold 36% of all discredit.",
                "Good promoters (3% of the system) hold 46% of all credit, while bad promoters (2%) hold 50% of all discredit.",
                "This reiterates that a few ratings influence most of the systems performance.",
                "Hence it is important to track transitions between them regardless of their small numbers. 257 Across different snapshots, a rating can remain in the same state or change.",
                "A good scout can become a bad scout, a good promoter can become a good connector, good and bad scouts can become vestigial, and so on.",
                "It is not practical to expect a recommender system to have no ratings in bad roles.",
                "However, it suffices to see ratings in bad roles either convert to good or vestigial roles.",
                "Similarly, observing a large number of good roles become bad ones is a sign of imminent failure of the system.",
                "We employ the principle of non-overlapping episodes [6] to count such transitions.",
                "A sequence such as: [+, 0, 0] → [+, 0, 0] → [0, +, 0] → [0, 0, 0] is interpreted as the transitions [+, 0, 0] ; [0, +, 0] : 1 [+, 0, 0] ; [0, 0, 0] : 1 [0, +, 0] ; [0, 0, 0] : 1 instead of [+, 0, 0] ; [0, +, 0] : 2 [+, 0, 0] ; [0, 0, 0] : 2 [0, +, 0] ; [0, 0, 0] : 1.",
                "See [6] for further details about this counting procedure.",
                "Thus, a rating can be in one of 27 possible states, and there are about 272 possible transitions.",
                "We make a further simplification and utilize only 9 states, indicating whether the rating is a scout, promoter, or connector, and whether it has a positive, negative, or negligible role.",
                "Ratings that serve multiple purposes are counted using multiple episode instantiations but the states themselves are not duplicated beyond the 9 restricted states.",
                "In this model, a transition such as [+, 0, +] ; [0, +, 0] : 1 is counted as [scout+] ; [scout0] : 1 [scout+] ; [connector+] : 1 [scout+] ; [promoter0] : 1 [connector0] ; [scout0] : 1 [connector0] ; [scout+] : 1 [connector0] ; [promoter0] : 1 [promoter+] ; [scout0] : 1 [promoter+] ; [connector+] : 1 [promoter+] ; [promoter0] : 1 Of these, transitions like [pX] ; [q0] where p = q, X ∈ {+, 0, −} are considered uninteresting, and only the rest are counted.",
                "Fig. 9 depicts the major transitions counted while processing the first 200,000 ratings from the MovieLens dataset.",
                "Only transitions with frequency greater than or equal to 3% are shown.",
                "The percentages for each state indicates the number of ratings that were found to be in those states.",
                "We consider transitions from any state to a good state as healthy, from any state to a bad state as unhealthy, and from any state to a vestigial state as decaying.",
                "From Fig. 9, we can observe: • The bulk of the ratings have negligible values, irrespective of their role.",
                "The majority of the transitions involve both good and bad ratings becoming negligible.",
                "Scout + (2%) Scout(1.5%) Scout 0 (96.5%) Connector + (1.2%) Connector(0.8%) Connector 0 (98%) Promoter + (3%) Promoter(2%) Promoter 0 (95%) 84% 84% 81% 74% 10% 6% 11% 77% 8% 7% 8% 82% 4% 86% 4% 68% 15% 13% 5% 5% 77% 11% 7% 5% 4% 3% 3% 3% Healthy Unhealthy Decaying Figure 9: Transitions among rating roles. • The number of good ratings is comparable to the bad ratings, and ratings are seen to switch states often, except in the case of scouts (see below). • The negative and positive scout states are not reachable through any transition, indicating that these ratings must begin as such, and cannot be coerced into these roles. • Good promoters and good connectors have a much longer survival period than scouts.",
                "Transitions that recur to these states have frequencies of 10% and 15% when compared to just 4% for scouts.",
                "Good connectors are the slowest to decay whereas (good) scouts decay the fastest. • Healthy percentages are seen on transitions between promoters and connectors.",
                "As indicated earlier, there are hardly any transitions from promoters/connectors to scouts.",
                "This indicates that, over the long run, a users rating is more useful to others (movies or other users) than to the user himself. • The percentages of healthy transitions outweigh the unhealthy ones - this hints that the system is healthy, albeit only marginally.",
                "Note that these results are conditioned by the static nature of the dataset, which is a set of ratings over a fixed window of time.",
                "However a diagram such as Fig. 9 is clearly useful for monitoring the health of a recommender system.",
                "For instance, acceptable limits can be imposed on different types of transitions and, if a transition fails to meet the threshold, the recommender system or a part of it can be brought under closer scrutiny.",
                "Furthermore, the role state transition diagram would also be the ideal place to study the effects of shilling, a topic we will consider in future research. 5.6 Characterizing neighborhoods Earlier we saw that we can characterize the neighborhood of ratings involved in creating a recommendation list L for 258 a user.",
                "In our experiment, we consider lists of length 30, and sample the lists of about 5% of users through the evolution of the model (at intervals of 10,000 ratings each) and compute their neighborhood characteristics.",
                "To simplify our presentation, we consider the percentage of the sample that fall into one of the following categories: 1.",
                "Inactive user: (SFN(u) = 0) 2.",
                "Good scouts, Good neighborhood: (SFN(u) > 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 3.",
                "Good scouts, Bad neighborhood: (SFN(u) > 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) 4.",
                "Bad scouts, Good neighborhood: (SFN(u) < 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 5.",
                "Bad scouts, Bad neighborhood: (SFN(u) < 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) From our sample set of 561 users, we found that 476 users were inactive.",
                "Of the remaining 85 users, we found 26 users had good scouts and a good neighborhood, 6 had bad scouts and a good neighborhood, 29 had good scouts and a bad neighborhood, and 24 had bad scouts and a bad neighborhood.",
                "Thus, we conjecture that 59 users (29+24+6) are in danger of leaving the system.",
                "As a remedy, users with bad scouts and a good neighborhood can be asked to reconsider rating of some movies hoping to improve the systems recommendations.",
                "The system can be expected to deliver more if they engineer some good scouts.",
                "Users with good scouts and a bad neighborhood are harder to address; this situation might entail selectively removing some connector-promoter pairs that are causing the damage.",
                "Handling users with bad scouts and bad neighborhoods is a more difficult challenge.",
                "Such a classification allows the use of different strategies to better a users experience with the system depending on his context.",
                "In future work, we intend to conduct field studies and study the improvement in performance of different strategies for different contexts. 6.",
                "CONCLUSIONS To further recommender system acceptance and deployment, we require new tools and methodologies to manage an installed recommender and develop insights into the roles played by ratings.",
                "A fine-grained characterization in terms of rating roles such as scouts, promoters, and connectors, as done here, helps such an endeavor.",
                "Although we have presented results on only the item-based algorithm with list rank accuracy as the metric, the same approach outlined here applies to user-based algorithms and other metrics.",
                "In future research, we plan to systematically study the many algorithmic parameters, tolerances, and cutoff thresholds employed here and reason about their effects on the downstream conclusions.",
                "We also aim to extend our formulation to other collaborative filtering algorithms, study the effect of shilling in altering rating roles, conduct field studies, and evaluate improvements in user experience by tweaking ratings based on their role values.",
                "Finally, we plan to develop the idea of mining the evolution of rating role patterns into a reporting and tracking system for all aspects of recommender system health. 7.",
                "REFERENCES [1] Cosley, D., Lam, S., Albert, I., Konstan, J., and Riedl, J.",
                "Is Seeing Believing? : How Recommender System Interfaces Affect Users Opinions.",
                "In Proc.",
                "CHI (2001), pp. 585-592. [2] Herlocker, J. L., Konstan, J.",
                "A., Borchers, A., and Riedl, J.",
                "An Algorithmic Framework for Performing Collaborative Filtering.",
                "In Proc.",
                "SIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J.",
                "A., Terveen, L. G., and Riedl, J. T. Evaluating Collaborative Filtering Recommender Systems.",
                "ACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J.",
                "A.",
                "Personal communication. 2003. [5] Lam, S. K., and Riedl, J. Shilling Recommender Systems for Fun and Profit.",
                "In Proceedings of the 13th International World Wide Web Conference (2004), ACM Press, pp. 393-402. [6] Laxman, S., Sastry, P. S., and Unnikrishnan, K. P. Discovering Frequent Episodes and Learning Hidden Markov Models: A Formal Connection.",
                "IEEE Transactions on Knowledge and Data Engineering Vol. 17, 11 (2005), 1505-1517. [7] McLaughlin, M. R., and Herlocker, J. L. A <br>collaborative filtering algorithm</br> and Evaluation Metric that Accurately Model the User Experience.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (2004), pp. 329 - 336. [8] OMahony, M., Hurley, N. J., Kushmerick, N., and Silvestre, G. Collaborative Recommendation: A Robustness Analysis.",
                "ACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J.",
                "A., and Riedl, J.",
                "Getting to Know You: Learning New User Preferences in Recommender Systems.",
                "In Proceedings of the 2002 Conference on Intelligent User Interfaces (IUI 2002) (2002), pp. 127-134. [10] Rashid, A. M., Karypis, G., and Riedl, J.",
                "Influence in Ratings-Based Recommender Systems: An Algorithm-Independent Approach.",
                "In Proc. of the SIAM International Conference on Data Mining (2005). [11] Resnick, P., Iacovou, N., Sushak, M., Bergstrom, P., and Riedl, J. GroupLens: An Open Architecture for Collaborative Filtering of Netnews.",
                "In Proceedings of the Conference on Computer Supported Collaborative Work (CSCW94) (1994), ACM Press, pp. 175-186. [12] Sarwar, B., Karypis, G., Konstan, J., and Reidl, J. Item-Based Collaborative Filtering Recommendation Algorithms.",
                "In Proceedings of the Tenth International World Wide Web Conference (WWW10) (2001), pp. 285-295. [13] Schein, A., Popescu, A., Ungar, L., and Pennock, D. Methods and Metrics for Cold-Start Recommendation.",
                "In Proc.",
                "SIGIR (2002), pp. 253-260. 259"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Un \"algoritmo de filtrado colaborativo\" basado en elementos construye un vecindario de películas alrededor de la máscara utilizando las clasificaciones de usuarios que calificaron la máscara y otras películas de manera similar (por ejemplo, las clasificaciones de Jims de la matriz y la máscara; y las clasificaciones de Jeffs de Star Wars yLa máscara).",
                "Un \"algoritmo de filtrado colaborativo\" basado en el usuario construiría un vecindario alrededor de Tom al rastrear a otros usuarios cuyos comportamientos de calificación son similares a Toms (por ejemplo, Tom y Jeff han calificado Star Wars; Tom y Jim han calificado la matriz).",
                "Entonces, independientemente del \"algoritmo de filtrado colaborativo\" utilizado, podemos visualizar la predicción de la clasificación TOMS de la máscara como caminar a través de una secuencia de clasificaciones.",
                "El \"Algoritmo de filtrado colaborativo\" atravesó muchos caminos en su vecindario a través de sus exploradores y otros conectores y promotores para hacer estas recomendaciones.",
                "Además del \"algoritmo de filtrado colaborativo\", la salud de este vecindario influye por completo en la satisfacción de los usuarios con el sistema.",
                "El retiro siempre fue alto, pero la precisión promedio era de aproximadamente 53%.0 0.2 0.4 0.6 0.8 1 1.2 10000 30000 50000 70000 90000110000130000150000 Clasificaciones fusionadas en el valor del modelo Recuerdo de precisión Figura 5: Precisión y recuperación para el \"Algoritmo de filtrado colaborativo basado en el artículo\".5.2 Inducir buenos exploradores Las calificaciones de un usuario que sirven como exploradores son aquellas que permiten al usuario recibir recomendaciones.",
                "El papel y el valor de una calificación pueden cambiar dependiendo de muchos factores como el comportamiento del usuario, la redundancia, los efectos de chelín o las propiedades del \"algoritmo de filtrado colaborativo\" utilizado.",
                "Transacciones IEEE sobre conocimiento e ingeniería de datos vol.17, 11 (2005), 1505-1517.[7] McLaughlin, M. R. y Herlocker, J. L. Un \"algoritmo de filtrado colaborativo\" y una métrica de evaluación que modela con precisión la experiencia del usuario."
            ],
            "translated_text": "",
            "candidates": [
                "algoritmo de filtrado colaborativo",
                "algoritmo de filtrado colaborativo",
                "algoritmo de filtrado colaborativo",
                "algoritmo de filtrado colaborativo",
                "algoritmo de filtrado colaborativo",
                "algoritmo de filtrado colaborativo",
                "algoritmo de filtrado colaborativo",
                "Algoritmo de filtrado colaborativo",
                "algoritmo de filtrado colaborativo",
                "algoritmo de filtrado colaborativo",
                "algoritmo de filtrado colaborativo",
                "Algoritmo de filtrado colaborativo basado en el artículo",
                "algoritmo de filtrado colaborativo",
                "algoritmo de filtrado colaborativo",
                "algoritmo de filtrado colaborativo",
                "algoritmo de filtrado colaborativo"
            ],
            "error": []
        },
        "aggregation process": {
            "translated_key": "proceso de agregación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Scouts, Promoters, and Connectors: The Roles of Ratings in Nearest Neighbor Collaborative Filtering Bharath Kumar Mohan Dept.",
                "of CSA Indian Institute of Science Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. of Computer Science Eastern Michigan University Ypsilanti, MI 48917, USA bkeller@emich.edu Naren Ramakrishnan Dept. of Computer Science Virginia Tech, Blacksburg VA 24061, USA naren@cs.vt.edu ABSTRACT Recommender systems aggregate individual user ratings into predictions of products or services that might interest visitors.",
                "The quality of this <br>aggregation process</br> crucially affects the user experience and hence the effectiveness of recommenders in e-commerce.",
                "We present a novel study that disaggregates global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearestneighbor collaborative filtering.",
                "In particular, we formulate three roles-scouts, promoters, and connectors-that capture how users receive recommendations, how items get recommended, and how ratings of these two types are themselves connected (resp.).",
                "These roles find direct uses in improving recommendations for users, in better targeting of items and, most importantly, in helping monitor the health of the system as a whole.",
                "For instance, they can be used to track the evolution of neighborhoods, to identify rating subspaces that do not contribute (or contribute negatively) to system performance, to enumerate users who are in danger of leaving, and to assess the susceptibility of the system to attacks such as shilling.",
                "We argue that the three rating roles presented here provide broad primitives to manage a recommender system and its community.",
                "Categories and Subject Descriptors H.4.2 [Information Systems Applications]: Types of Systems-Decision support; J.4 [Computer Applications]: Social and Behavioral Sciences General Terms Algorithms, Human Factors 1.",
                "INTRODUCTION Recommender systems have become integral to e-commerce, providing technology that suggests products to a visitor based on previous purchases or rating history.",
                "Collaborative filtering, a common form of recommendation, predicts a users rating for an item by combining (other) ratings of that user with other users ratings.",
                "Significant research has been conducted in implementing fast and accurate collaborative filtering algorithms [2, 7], designing interfaces for presenting recommendations to users [1], and studying the robustness of these algorithms [8].",
                "However, with the exception of a few studies on the influence of users [10], little attention has been paid to unraveling the inner workings of a recommender in terms of the individual ratings and the roles they play in making (good) recommendations.",
                "Such an understanding will give an important handle to monitoring and managing a recommender system, to engineer mechanisms to sustain the recommender, and thereby ensure its continued success.",
                "Our motivation here is to disaggregate global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearest-neighbor collaborative filtering.",
                "We identify three possible roles: (scouts) to connect the user into the system to receive recommendations, (promoters) to connect an item into the system to be recommended, and (connectors) to connect ratings of these two kinds.",
                "Viewing ratings in this way, we can define the contribution of a rating in each role, both in terms of allowing recommendations to occur, and in terms of influence on the quality of recommendations.",
                "In turn, this capability helps support scenarios such as: 1.",
                "Situating users in better neighborhoods: A users ratings may inadvertently connect the user to a neighborhood for which the users tastes may not be a perfect match.",
                "Identifying ratings responsible for such bad recommendations and suggesting new items to rate can help situate the user in a better neighborhood. 2.",
                "Targeting items: Recommender systems suffer from lack of user participation, especially in cold-start scenarios [13] involving newly arrived items.",
                "Identifying users who can be encouraged to rate specific items helps ensure coverage of the recommender system. 3.",
                "Monitoring the evolution of the recommender system and its stakeholders: A recommender system is constantly under change: growing with new users and 250 items, shrinking with users leaving the system, items becoming irrelevant, and parts of the system under attack.",
                "Tracking the roles of a rating and its evolution over time provides many insights into the health of the system, and how it could be managed and improved.",
                "These include being able to identify rating subspaces that do not contribute (or contribute negatively) to system performance, and could be removed; to enumerate users who are in danger of leaving, or have left the system; and to assess the susceptibility of the system to attacks such as shilling [5].",
                "As we show, the characterization of rating roles presented here provides broad primitives to manage a recommender system and its community.",
                "The rest of the paper is organized as follows.",
                "Background on nearest-neighbor collaborative filtering and algorithm evaluation is discussed in Section 2.",
                "Section 3 defines and discusses the roles of a rating, and Section 4 defines measures of the contribution of a rating in each of these roles.",
                "In Section 5, we illustrate the use of these roles to address the goals outlined above. 2.",
                "BACKGROUND 2.1 Algorithms Nearest-neighbor collaborative filtering algorithms either use neighborhoods of users or neighborhoods of items to compute a prediction.",
                "An algorithm of the first kind is called user-based, and one of the second kind is called itembased [12].",
                "In both families of algorithms, neighborhoods are formed by first computing the similarity between all pairs of users (for user-based) or items (for item-based).",
                "Predictions are then computed by aggregating ratings, which in a user-based algorithm involves aggregating the ratings of the target item by the users neighbors and, in an item-based algorithm, involves aggregating the users ratings of items that are neighbors of the target item.",
                "Algorithms within these families differ in the definition of similarity, formation of neighborhoods, and the computation of predictions.",
                "We consider a user-based algorithm based on that defined for GroupLens [11] with variations from Herlocker et al. [2], and an item-based algorithm similar to that of Sarwar et al. [12].",
                "The algorithm used by Resnick et al. [11] defines the similarity of two users u and v as the Pearson correlation of their common ratings: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2 , where Iu is the set of items rated by user u, ru,i is user us rating for item i, and ¯ru is the average rating of user u (similarly for v).",
                "Similarity computed in this manner is typically scaled by a factor proportional to the number of common ratings, to reduce the chance of making a recommendation made on weak connections: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), where γ ≈ 5 is a constant used as a lower limit in scaling [2].",
                "These new similarities are then used to define a static neighborhood Nu for each user u consisting of the top K users most similar to user u.",
                "A prediction for user u and item i is computed by a weighted average of the ratings by the neighbors pu,i = ¯ru + P v∈V sim (u, v)(rv,i − ¯rv) P v∈V sim (u, v) (1) where V = Nu ∩ Ui is the set of users most similar to u who have rated i.",
                "The item-based algorithm we use is the one defined by Sarwar et al. [12].",
                "In this algorithm, similarity is defined as the adjusted cosine measure sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) where Ui is the set of users who have rated item i.",
                "As for the user-based algorithm, the similarity weights are adjusted proportionally to the number of users that have rated the items in common sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). (3) Given the similarities, the neighborhood Ni of an item i is defined as the top K most similar items for that item.",
                "A prediction for user u and item i is computed as the weighted average pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) where J = Ni ∩ Iu is the set of items rated by u that are most similar to i. 2.2 Evaluation Recommender algorithms have typically been evaluated using measures of predictive accuracy and coverage [3].",
                "Studies on recommender algorithms, notably Herlocker et al. [2] and Sarwar et al. [12], typically compute predictive accuracy by dividing a set of ratings into training and test sets, and compute the prediction for an item in the test set using the ratings in the training set.",
                "A standard measure of predictive accuracy is mean absolute error (MAE), which for a test set T = {(u, i)} is defined as, MAE = P (u,i)∈T |pu,i − ru,i| |T | . (5) Coverage has a number of definitions, but generally refers to the proportion of items that can be predicted by the algorithm [3].",
                "A practical issue with predictive accuracy is that users typically are presented with recommendation lists, and not individual numeric predictions.",
                "Recommendation lists are lists of items in decreasing order of prediction (sometimes stated in terms of star-ratings), and so predictive accuracy may not be reflective of the accuracy of the list.",
                "So, instead we can measure recommendation or rank accuracy, which indicates the extent to which the list is in the correct order.",
                "Herlocker et al. [3] discuss a number of rank accuracy measures, which range from Kendalls Tau to measures that consider the fact that users tend to only look at a prefix of the list [5].",
                "Kendalls Tau measures the number of inversions when comparing ordered pairs in the true user ordering of 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figure 1: Ratings in simple movie recommender. items and the recommended order, and is defined as τ = C − D p (C + D + TR)(C + D + TP) (6) where C is the number of pairs that the system predicts in the correct order, D the number of pairs the system predicts in the wrong order, TR the number of pairs in the true ordering that have the same ratings, and TP is the number of pairs in the predicted ordering that have the same ratings [3].",
                "A shortcoming of the Tau metric is that it is oblivious to the position in the ordered list where the inversion occurs [3].",
                "For instance, an inversion toward the end of the list is given the same weight as one in the beginning.",
                "One solution is to consider inversions only in the top few items in the recommended list or to weight inversions based on their position in the list. 3.",
                "ROLES OF A RATING Our basic observation is that each rating plays a different role in each prediction in which it is used.",
                "Consider a simplified movie recommender system with three users Jim, Jeff, and Tom and their ratings for a few movies, as shown in Fig. 1. (For this initial discussion we will not consider the rating values involved.)",
                "The recommender predicts whether Tom will like The Mask using the other already available ratings.",
                "How this is done depends on the algorithm: 1.",
                "An item-based collaborative filtering algorithm constructs a neighborhood of movies around The Mask by using the ratings of users who rated The Mask and other movies similarly (e.g., Jims ratings of The Matrix and The Mask; and Jeffs ratings of Star Wars and The Mask).",
                "Toms ratings of those movies are then used to make a prediction for The Mask. 2.",
                "A user-based collaborative filtering algorithm would construct a neighborhood around Tom by tracking other users whose rating behaviors are similar to Toms (e.g., Tom and Jeff have rated Star Wars; Tom and Jim have rated The Matrix).",
                "The prediction of Toms rating for The Mask is then based on the ratings of Jeff and Tim.",
                "Although the nearest-neighbor algorithms aggregate the ratings to form neighborhoods used to compute predictions, we can disaggregate the similarities to view the computation of a prediction as simultaneously following parallel paths of ratings.",
                "So, irrespective of the collaborative filtering algorithm used, we can visualize the prediction of Toms rating of The Mask as walking through a sequence of ratings.",
                "In Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figure 2: Ratings used to predict The Mask for Tom.",
                "Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figure 3: Prediction of The Mask for Tom in which a rating is used more than once. this example, two paths were used for this prediction as depicted in Fig. 2: (p1, p2, p3) and (q1, q2, q3).",
                "Note that these paths are undirected, and are all of length 3.",
                "Only the order in which the ratings are traversed is different between the item-based algorithm (e.g., (p3, p2, p1), (q3, q2, q1)) and the user-based algorithm (e.g., (p1, p2, p3), (q1, q2, q3).)",
                "A rating can be part of many paths for a single prediction as shown in Fig. 3, where three paths are used for a prediction, two of which follow p1: (p1, p2, p3) and (p1, r2, r3).",
                "Predictions in a collaborative filtering algorithms may involve thousands of such walks in parallel, each playing a part in influencing the predicted value.",
                "Each prediction path consists of three ratings, playing roles that we call scouts, promoters, and connectors.",
                "To illustrate these roles, consider the path (p1, p2, p3) in Fig. 2 used to make a prediction of The Mask for Tom: 1.",
                "The rating p1 (Tom → Star Wars) makes a connection from Tom to other ratings that can be used to predict Toms rating for The Mask.",
                "This rating serves as a scout in the bipartite graph of ratings to find a path that leads to The Mask. 2.",
                "The rating p2 (Jeff → Star Wars) helps the system recommend The Mask to Tom by connecting the scout to the promoter. 3.",
                "The rating p3 (Jeff → The Mask) allows connections to The Mask, and, therefore, promotes this movie to Tom.",
                "Formally, given a prediction pu,a of a target item a for user u, a scout for pu,a is a rating ru,i such that there exists a user v with ratings rv,a and rv,i for some item i; a promoter for pu,a is a rating rv,a for some user v, such that there exist ratings rv,i and ru,i for an item i, and; a connector for pu,a 252 Jim Tom Jeff Jerry My Cousin Vinny The Matrix Star Wars The Mask Jurasic Park Figure 4: Scouts, promoters, and connectors. is a rating rv,i by some user v and rating i, such that there exists ratings ru,i and rv,a.",
                "The scouts, connectors, and promoters for the prediction of Toms rating of The Mask are p1 and q1, p2 and q2, and p3 and q3 (respectively).",
                "Each of these roles has a value in the recommender to the user, the users neighborhood, and the system in terms of allowing recommendations to be made. 3.1 Roles in Detail Ratings that act as scouts tend to help the recommender system suggest more movies to the user, though the extent to which this is true depends on the rating behavior of other users.",
                "For example, in Fig. 4 the rating Tom → Star Wars helps the system recommend only The Mask to him, while Tom → The Matrix helps recommend The Mask, Jurassic Park, and My Cousin Vinny.",
                "Tom makes a connection to Jim who is a prolific user of the system, by rating The Matrix.",
                "However, this does not make The Matrix the best movie to rate for everyone.",
                "For example, Jim is benefited equally by both The Mask and The Matrix, which allow the system to recommend Star Wars to him.",
                "His rating of The Mask is the best scout for Jeff, and Jerrys only scout is his rating of Star Wars.",
                "This suggests that good scouts allow a user to build similarity with prolific users, and thereby ensure they get more from the system.",
                "While scouts represent beneficial ratings from the perspective of a user, promoters are their duals, and are of benefit to items.",
                "In Fig. 4, My Cousin Vinny benefits from Jims rating, since it allows recommendations to Jeff and Tom.",
                "The Mask is not so dependent on just one rating, since the ratings by Jim and Jeff help it.",
                "On the other hand, Jerrys rating of Star Wars does not help promote it to any other user.",
                "We conclude that a good promoter connects an item to a broader neighborhood of other items, and thereby ensures that it is recommended to more users.",
                "Connectors serve a crucial role in a recommender system that is not as obvious.",
                "The movies My Cousin Vinny and Jurassic Park have the highest recommendation potential since they can be recommended to Jeff, Jerry and Tom based on the linkage structure illustrated in Fig. 4.",
                "Beside the fact that Jim rated these movies, these recommendations are possible only because of the ratings Jim → The Matrix and Jim → The Mask, which are the best connectors.",
                "A connector improves the systems ability to make recommendations with no explicit gain for the user.",
                "Note that every rating can be of varied benefit in each of these roles.",
                "The rating Jim → My Cousin Vinny is a poor scout and connector, but is a very good promoter.",
                "The rating Jim → The Mask is a reasonably good scout, a very good connector, and a good promoter.",
                "Finally, the rating Jerry → Star Wars is a very good scout, but is of no value as a connector or promoter.",
                "As illustrated here, a rating can have different value in each of the three roles in terms of whether a recommendation can be made or not.",
                "We could measure this value by simply counting the number of times a rating is used in each role, which alone would be helpful in characterizing the behavior of a system.",
                "But we can also measure the contribution of each rating to the quality of recommendations or health of the system.",
                "Since every prediction is a combined effort of several recommendation paths, we are interested in discerning the influence of each rating (and, hence, each path) in the system towards the systems overall error.",
                "We can understand the dynamics of the system at a finer granularity by tracking the influence of a rating according to the role played.",
                "The next section describes the approach to measuring the values of a rating in each role. 4.",
                "CONTRIBUTIONS OF RATINGS As weve seen, a rating may play different roles in different predictions and, in each prediction, contribute to the quality of a prediction in different ways.",
                "Our approach can use any numeric measure of a property of system health, and assigns credit (or blame) to each rating proportional to its influence in the prediction.",
                "By tracking the role of each rating in a prediction, we can accumulate the credit for a rating in each of the three roles, and also track the evolution of the roles of rating over time in the system.",
                "This section defines the methodology for computing the contribution of ratings by first defining the influence of a rating, and then instantiating the approach for predictive accuracy, and then rank accuracy.",
                "We also demonstrate how these contributions can be aggregated to study the neighborhood of ratings involved in computing a users recommendations.",
                "Note that although our general formulation for rating influence is algorithm independent, due to space considerations, we present the approach for only item-based collaborative filtering.",
                "The definition for user-based algorithms is similar and will be presented in an expanded version of this paper. 4.1 Influence of Ratings Recall that an item-based approach to collaborative filtering relies on building item neighborhoods using the similarity of ratings by the same user.",
                "As described earlier, similarity is defined by the adjusted cosine measure (Equations (2) and (3)).",
                "A set of the top K neighbors is maintained for all items for space and computational efficiency.",
                "A prediction of item i for a user u is computed as the weighted deviation from the items mean rating as shown in Equation (4).",
                "The list of recommendations for a user is then the list of items sorted in descending order of their predicted values.",
                "We first define impact(a, i, j), the impact a user a has in determining the similarity between two items i and j.",
                "This is the change in the similarity between i and j when as rating is removed, and is defined as impact(a, i, j) = |sim (i, j) − sim¯a(i, j)| P w∈Cij |sim (i, j) − sim ¯w(i, j)| where Cij = {u ∈ U | ∃ ru,i, ru,j ∈ R(u)} is the set of coraters 253 of items i and j (users who rate both i and j), R(u) is the set of ratings provided by user u, and sim¯a(i, j) is the similarity of i and j when the ratings of user a are removed sim¯a(i, j) = P v∈U\\{a} (ru,i − ¯ru)(ru,j − ¯ru) qP u∈U\\{a}(ru,i − ¯ru)2 qP u∈U\\{a}(ru,j − ¯ru)2 , and adjusted for the number of raters sim¯a(i, j) = max(|Ui ∩ Uj| − 1, γ) γ · sim(i, j).",
                "If all coraters of i and j rate them identically, we define the impact as impact(a, i, j) = 1 |Cij| since P w∈Cij |sim (i, j) − sim ¯w(i, j)| = 0.",
                "The influence of each path (u, j, v, i) = [ru,j, rv,j, rv,i] in the prediction of pu,i is given by influence(u, j, v, i) = sim (i, j) P l∈Ni∩Iu sim (i, l) · impact(v, i, j) It follows that the sum of influences over all such paths, for a given set of endpoints, is 1. 4.2 Role Values for Predictive Accuracy The value of a rating in each role is computed from the influence depending on the evaluation measure employed.",
                "Here we illustrate the approach using predictive accuracy as the evaluation metric.",
                "In general, the goodness of a prediction decides whether the ratings involved must be credited or discredited for their role.",
                "For predictive accuracy, the error in prediction e = |pu,i − ru,i| is mapped to a comfort level using a mapping function M(e).",
                "Anecdotal evidence suggests that users are unable to discern errors less than 1.0 (for a rating scale of 1 to 5) [4], and so an error less than 1.0 is considered acceptable, but anything larger is not.",
                "We hence define M(e) as (1 − e) binned to an appropriate value in [−1, −0.5, 0.5, 1].",
                "For each prediction pu,i, M(e) is attributed to all the paths that assisted the computation of pu,i, proportional to their influences.",
                "This tribute, M(e)∗influence(u, j, v, i), is in turn inherited by each of the ratings in the path [ru,j, rv,j, rv,i], with the credit/blame accumulating to the respective roles of ru,j as a scout, rv,j as a connector, and rv,i as a promoter.",
                "In other words, the scout value SF(ru,j), the connector value CF(rv,j) and the promoter value PF(rv,i) are all incremented by the tribute amount.",
                "Over a large number of predictions, scouts that have repeatedly resulted in big error rates have a big negative scout value, and vice versa (similarly with the other roles).",
                "Every rating is thus summarized by its triple [SF, CF, PF]. 4.3 Role Values for Rank Accuracy We now define the computation of the contribution of ratings to observed rank accuracy.",
                "For this computation, we must know the users preference order for a set of items for which predictions can be computed.",
                "We assume that we have a test set of the users ratings of the items presented in the recommendation list.",
                "For every pair of items rated by a user in the test data, we check whether the predicted order is concordant with his preference.",
                "We say a pair (i, j) is concordant (with error ) whenever one of the following holds: • if (ru,i < ru,j) then (pu,i − pu,j < ); • if (ru,i > ru,j) then (pu,i − pu,j > ); or • if (ru,i = ru,j) then (|pu,i − pu,j| ≤ ).",
                "Similarly, a pair (i, j) is discordant (with error ) if it is not concordant.",
                "Our experiments described below use an error tolerance of = 0.1.",
                "All paths involved in the prediction of the two items in a concordant pair are credited, and the paths involved in a discordant pair are discredited.",
                "The credit assigned to a pair of items (i, j) in the recommendation list for user u is computed as c(i, j) = ( t T · 1 C+D if (i, j) are concordant − t T · 1 C+D if (i, j) are discordant (7) where t is the number of items in the users test set whose ratings could be predicted, T is the number of items rated by user u in the test set, C is the number of concordances and D is the number of discordances.",
                "The credit c is then divided among all paths responsible for predicting pu,i and pu,j proportional to their influences.",
                "We again add the role values obtained from all the experiments to form a triple [SF, CF, PF] for each rating. 4.4 Aggregating rating roles After calculating the role values for individual ratings, we can also use these values to study neighborhoods and the system.",
                "Here we consider how we can use the role values to characterize the health of a neighborhood.",
                "Consider the list of top recommendations presented to a user at a specific point in time.",
                "The collaborative filtering algorithm traversed many paths in his neighborhood through his scouts and other connectors and promoters to make these recommendations.",
                "We call these ratings the recommender neighborhood of the user.",
                "The user implicitly chooses this neighborhood of ratings through the items he rates.",
                "Apart from the collaborative filtering algorithm, the health of this neighborhood completely influences a users satisfaction with the system.",
                "We can characterize a users recommender neighborhood by aggregating the individual role values of the ratings involved, weighted by the influence of individual ratings in determining his recommended list.",
                "Different sections of the users neighborhood wield varied influence on his recommendation list.",
                "For instance, ratings reachable through highly rated items have a bigger say in the recommended items.",
                "Our aim is to study the system and classify users with respect to their positioning in a healthy or unhealthy neighborhood.",
                "A user can have a good set of scouts, but may be exposed to a neighborhood with bad connectors and promoters.",
                "He can have a good neighborhood, but his bad scouts may ensure the neighborhoods potential is rendered useless.",
                "We expect that users with good scouts and good neighborhoods will be most satisfied with the system in the future.",
                "A users neighborhood is characterized by a triple that represents the weighted sum of the role values of individual ratings involved in making recommendations.",
                "Consider a user u and his ordered list of recommendations L. An item i 254 in the list is weighted inversely, as K(i), depending on its position in the list.",
                "In our studies we use K(i) = p position(i).",
                "Several paths of ratings [ru,j, rv,j, rv,i] are involved in predicting pu,i which ultimately decides its position in L, each with influence(u, j, v, i).",
                "The recommender neighborhood of a user u is characterized by the triple, [SFN(u), CFN(u), PFN(u)] where SFN(u) = X i∈L P [ru,j ,rv,j ,rv,i] SF(ru,j)influence(u, j, v, i) K(i) !",
                "CFN(u) and PFN(u) are defined similarly.",
                "This triple estimates the quality of us recommendations based on the past track record of the ratings involved in their respective roles. 5.",
                "EXPERIMENTATION As we have seen, we can assign role values to each rating when evaluating a collaborative filtering system.",
                "In this section, we demonstrate the use of this approach to our overall goal of defining an approach to monitor and manage the health of a recommender system through experiments done on the MovieLens million rating dataset.",
                "In particular, we discuss results relating to identifying good scouts, promoters, and connectors; the evolution of rating roles; and the characterization of user neighborhoods. 5.1 Methodology Our experiments use the MovieLens million rating dataset, which consists of ratings by 6040 users of 3952 movies.",
                "The ratings are in the range 1 to 5, and are labeled with the time the rating was given.",
                "As discussed before, we consider only the item-based algorithm here (with item neighborhoods of size 30) and, due to space considerations, only present role value results for rank accuracy.",
                "Since we are interested in the evolution of the rating role values over time, the model of the recommender system is built by processing ratings in their arrival order.",
                "The timestamping provided by MovieLens is hence crucial for the analyses presented here.",
                "We make assessments of rating roles at intervals of 10,000 ratings and processed the first 200,000 ratings in the dataset (giving rise to 20 snapshots).",
                "We incrementally update the role values as the time ordered ratings are merged into the model.",
                "To keep the experiment computationally manageable, we define a test dataset for each user.",
                "As the time ordered ratings are merged into the model, we label a small randomly selected percentage (20%) as test data.",
                "At discrete epochs, i.e., after processing every 10,000 ratings, we compute the predictions for the ratings in the test data, and then compute the role values for the ratings used in the predictions.",
                "One potential criticism of this methodology is that the ratings in the test set are never evaluated for their roles.",
                "We overcome this concern by repeating the experiment, using different random seeds.",
                "The probability that every rating is considered for evaluation is then considerably high: 1 − 0.2n , where n is the number of times the experiment is repeated with different random seeds.",
                "The results here are based on n = 4 repetitions.",
                "The item-based collaborative filtering algorithms performance was ordinary with respect to rank accuracy.",
                "Fig. 5 shows a plot of the precision and recall as ratings were merged in time order into the model.",
                "The recall was always high, but the average precision was just about 53%. 0 0.2 0.4 0.6 0.8 1 1.2 10000 30000 50000 70000 90000110000130000150000 Ratings merged into model Value Precision Recall Figure 5: Precision and recall for the item-based collaborative filtering algorithm. 5.2 Inducing good scouts The ratings of a user that serve as scouts are those that allow the user to receive recommendations.",
                "We claim that users with ratings that have respectable scout values will be happier with the system than those with ratings with low scout values.",
                "Note that the item-based algorithm discussed here produces recommendation lists with nearly half of the pairs in the list discordant from the users preference.",
                "Whether all of these discordant pairs are observable by the user is unclear, however, certainly this suggests that there is a need to be able to direct users to items whose ratings would improve the lists.",
                "The distribution of the scout values for most users ratings are Gaussian with mean zero.",
                "Fig. 6 shows the frequency distribution of scout values for a sample user at a given snapshot.",
                "We observe that a large number of ratings never serve as scouts for their users.",
                "A relatable scenario is when Amazons recommender makes suggestions of books or items based on other items that were purchased as gifts.",
                "With simple relevance feedback from the user, such ratings can be isolated as bad scouts and discounted from future predictions.",
                "Removing bad scouts was found to be worthwhile for individual users but the overall performance improvement was only marginal.",
                "An obvious question is whether good scouts can be formed by merely rating popular movies as suggested by Rashid et al. [9].",
                "They show that a mix of popularity and rating entropy identifies the best items to suggest to new users when evaluated using MAE.",
                "Following their intuition, we would expect to see a higher correlation between popularityentropy and good scouts.",
                "We measured the Pearson correlation coefficient between aggregated scout values for a movie with the popularity of a movie (number of times it is rated); and with its popularity*variance measure at different snapshots of the system.",
                "Note that the scout values were initially anti-correlated with popularity (Fig. 7), but became moderately correlated as the system evolved.",
                "Both popularity and popularity*variance performed similarly.",
                "A possible explanation is that there has been insufficient time for the popular movies to accumulate ratings. 255 -10 0 10 20 30 40 50 60 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 Scout Value Frequency Figure 6: Distribution of scout values for a sample user. -0.4 -0.2 0 0.2 0.4 0.6 0.8 30000 60000 90000 120000 150000 180000 Popularity Pop*Var Figure 7: Correlation between aggregated scout value and item popularity (computed at different intervals). -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 30000 60000 90000 120000 150000 180000 Figure 8: Correlation between aggregated promoter value and user prolificity (computed at different intervals).",
                "Table 1: Movies forming the best scouts.",
                "Best Scouts Conf.",
                "Pop.",
                "Being John Malkovich (1999) 1.00 445 Star Wars: Episode IV - A New Hope (1977) 0.92 623 Princess Bride, The (1987) 0.85 477 Sixth Sense, The (1999) 0.85 617 Matrix, The (1999) 0.77 522 Ghostbusters (1984) 0.77 441 Casablanca (1942) 0.77 384 Insider, The (1999) 0.77 235 American Beauty (1999) 0.69 624 Terminator 2: Judgment Day (1991) 0.69 503 Fight Club (1999) 0.69 235 Shawshank Redemption, The (1994) 0.69 445 Run Lola Run (Lola rennt) (1998) 0.69 220 Terminator, The (1984) 0.62 450 Usual Suspects, The (1995) 0.62 326 Aliens (1986) 0.62 385 North by Northwest (1959) 0.62 245 Fugitive, The (1993) 0.62 402 End of Days (1999) 0.62 132 Raiders of the Lost Ark (1981) 0.54 540 Schindlers List (1993) 0.54 453 Back to the Future (1985) 0.54 543 Toy Story (1995) 0.54 419 Alien (1979) 0.54 415 Abyss, The (1989) 0.54 345 2001: A Space Odyssey (1968) 0.54 358 Dogma (1999) 0.54 228 Little Mermaid, The (1989) 0.54 203 Table 2: Movies forming the worst scouts.",
                "Worst scouts Conf.",
                "Pop.",
                "Harold and Maude (1971) 0.46 141 Grifters, The (1990) 0.46 180 Sting, The (1973) 0.38 244 Godfather: Part III, The (1990) 0.38 154 Lawrence of Arabia (1962) 0.38 167 High Noon (1952) 0.38 84 Women on the Verge of a... (1988) 0.38 113 Grapes of Wrath, The (1940) 0.38 115 Duck Soup (1933) 0.38 131 Arsenic and Old Lace (1944) 0.38 138 Midnight Cowboy (1969) 0.38 137 To Kill a Mockingbird (1962) 0.31 195 Four Weddings and a Funeral (1994) 0.31 271 Good, The Bad and The Ugly, The (1966) 0.31 156 Its a Wonderful Life (1946) 0.31 146 Player, The (1992) 0.31 220 Jackie Brown (1997) 0.31 118 Boat, The (Das Boot) (1981) 0.31 210 Manhattan (1979) 0.31 158 Truth About Cats & Dogs, The (1996) 0.31 143 Ghost (1990) 0.31 227 Lone Star (1996) 0.31 125 Big Chill, The (1983) 0.31 184 256 By studying the evolution of scout values, we can identify movies that consistently feature in good scouts over time.",
                "We claim these movies will make viable scouts for other users.",
                "We found the aggregated scout values for all movies in intervals of 10,000 ratings each.",
                "A movie is said to induce a good scout if the movie was in the top 100 of the sorted list, and to induce a bad scout if it was in bottom 100 of the same list.",
                "Movies appearing consistently high over time are expected to remain up there in the future.",
                "The effective confidence in a movie m is Cm = Tm − Bm N (8) where Tm is the number of times it appeared in the top 100, Bm the number of times it appeared in the bottom 100, and N is the number of intervals considered.",
                "Using this measure, the top few movies expected to induce the best scouts are shown in Table 1.",
                "Movies that would be bad scout choices are shown in Table 2 with their associated confidences.",
                "The popularities of the movies are also displayed.",
                "Although more popular movies appear in the list of good scouts, these tables show that a blind choice of scout based on popularity alone can be potentially dangerous.",
                "Interestingly, the best scout-Being John Malkovich-is about a puppeteer who discovers a portal into a movie star, a movie that has been described variously on amazon.com as makes you feel giddy, seriously weird, comedy with depth, silly, strange, and inventive.",
                "Indicating whether someone likes this movie or not goes a long way toward situating the user in a suitable neighborhood, with similar preferences.",
                "On the other hand, several factors may have made a movie a bad scout, like the sharp variance in user preferences in the neighborhood of a movie.",
                "Two users may have the same opinion about Lawrence of Arabia, but they may differ sharply about how they felt about the other movies they saw.",
                "Bad scouts ensue when there is deviation in behavior around a common synchronization point. 5.3 Inducing good promoters Ratings that serve to promote items in a collaborative filtering system are critical to allowing a new item be recommended to users.",
                "So, inducing good promoters is important for cold-start recommendation.",
                "We note that the frequency distribution of promoter values for a sample movies ratings is also Gaussian (similar to Fig. 6).",
                "This indicates that the promotion of a movie is benefited most by the ratings of a few users, and are unaffected by the ratings of most users.",
                "We find a strong correlation between a users number of ratings and his aggregated promoter value.",
                "Fig. 8 depicts the evolution of the Pearson correlation co-efficient between the prolificity of a user (number of ratings) versus his aggregated promoter value.",
                "We expect that conspicuous shills, by recommending wrong movies to users, will be discredited with negative aggregate promoter values and should be identifiable easily.",
                "Given this observation, the obvious rule to follow when introducing a new movie is to have it rated directly by prolific users who posses high aggregated promoter values.",
                "A new movie is thus cast into the neighborhood of many other movies improving its visibility.",
                "Note, though, that a user may have long stopped using the system.",
                "Tracking promoter values consistently allows only the most active recent users to be considered. 5.4 Inducing good connectors Given the way scouts, connectors, and promoters are characterized, it follows that the movies that are part of the best scouts are also part of the best connectors.",
                "Similarly, the users that constitute the best promoters are also part of the best connectors.",
                "Good connectors are induced by ensuring a user with a high promoter value rates a movie with a high scout value.",
                "In our experiments, we find that a ratings longest standing role is often as a connector.",
                "A rating with a poor connector value is often seen due to its user being a bad promoter, or its movie being a bad scout.",
                "Such ratings can be removed from the prediction process to bring marginal improvements to recommendations.",
                "In some selected experiments, we observed that removing a set of badly behaving connectors helped improve the systems overall performance by 1.5%.",
                "The effect was even higher on a few select users who observed an improvement of above 10% in precision without much loss in recall. 5.5 Monitoring the evolution of rating roles One of the more significant contributions of our work is the ability to model the evolution of recommender systems, by studying the changing roles of ratings over time.",
                "The role and value of a rating can change depending on many factors like user behavior, redundancy, shilling effects or properties of the collaborative filtering algorithm used.",
                "Studying the dynamics of rating roles in terms of transitions between good, bad, and negligible values can provide insights into the functioning of the recommender system.",
                "We believe that a continuous visualization of these transitions will improve the ability to manage a recommender system.",
                "We classify different rating states as good, bad, or negligible.",
                "Consider a user who has rated 100 movies in a particular interval, of which 20 are part of the test set.",
                "If a scout has a value greater than 0.005, it indicates that it is uniquely involved in at least 2 concordant predictions, which we will say is good.",
                "Thus, a threshold of 0.005 is chosen to bin a rating as good, bad or negligible in terms of its scout, connector and promoter value.",
                "For instance, a rating r, at time t with role value triple [0.1, 0.001, −0.01] is classified as [scout +, connector 0, promoter −], where + indicates good, 0 indicates negligible, and − indicates bad.",
                "The positive credit held by a rating is a measure of its contribution to the betterment of the system, and the discredit is a measure of its contribution to the detriment of the system.",
                "Even though the positive roles (and the negative roles) make up a very small percentage of all ratings, their contribution supersedes their size.",
                "For example, even though only 1.7% of all ratings were classified as good scouts, they hold 79% of all positive credit in the system!",
                "Similarly, the bad scouts were just 1.4% of all ratings but hold 82% of all discredit.",
                "Note that good and bad scouts, together, comprise only 1.4% + 1.7% = 3.1% of the ratings, implying that the majority of the ratings are negligible role players as scouts (more on this later).",
                "Likewise, good connectors were 1.2% of the system, and hold 30% of all positive credit.",
                "The bad connectors (0.8% of the system) hold 36% of all discredit.",
                "Good promoters (3% of the system) hold 46% of all credit, while bad promoters (2%) hold 50% of all discredit.",
                "This reiterates that a few ratings influence most of the systems performance.",
                "Hence it is important to track transitions between them regardless of their small numbers. 257 Across different snapshots, a rating can remain in the same state or change.",
                "A good scout can become a bad scout, a good promoter can become a good connector, good and bad scouts can become vestigial, and so on.",
                "It is not practical to expect a recommender system to have no ratings in bad roles.",
                "However, it suffices to see ratings in bad roles either convert to good or vestigial roles.",
                "Similarly, observing a large number of good roles become bad ones is a sign of imminent failure of the system.",
                "We employ the principle of non-overlapping episodes [6] to count such transitions.",
                "A sequence such as: [+, 0, 0] → [+, 0, 0] → [0, +, 0] → [0, 0, 0] is interpreted as the transitions [+, 0, 0] ; [0, +, 0] : 1 [+, 0, 0] ; [0, 0, 0] : 1 [0, +, 0] ; [0, 0, 0] : 1 instead of [+, 0, 0] ; [0, +, 0] : 2 [+, 0, 0] ; [0, 0, 0] : 2 [0, +, 0] ; [0, 0, 0] : 1.",
                "See [6] for further details about this counting procedure.",
                "Thus, a rating can be in one of 27 possible states, and there are about 272 possible transitions.",
                "We make a further simplification and utilize only 9 states, indicating whether the rating is a scout, promoter, or connector, and whether it has a positive, negative, or negligible role.",
                "Ratings that serve multiple purposes are counted using multiple episode instantiations but the states themselves are not duplicated beyond the 9 restricted states.",
                "In this model, a transition such as [+, 0, +] ; [0, +, 0] : 1 is counted as [scout+] ; [scout0] : 1 [scout+] ; [connector+] : 1 [scout+] ; [promoter0] : 1 [connector0] ; [scout0] : 1 [connector0] ; [scout+] : 1 [connector0] ; [promoter0] : 1 [promoter+] ; [scout0] : 1 [promoter+] ; [connector+] : 1 [promoter+] ; [promoter0] : 1 Of these, transitions like [pX] ; [q0] where p = q, X ∈ {+, 0, −} are considered uninteresting, and only the rest are counted.",
                "Fig. 9 depicts the major transitions counted while processing the first 200,000 ratings from the MovieLens dataset.",
                "Only transitions with frequency greater than or equal to 3% are shown.",
                "The percentages for each state indicates the number of ratings that were found to be in those states.",
                "We consider transitions from any state to a good state as healthy, from any state to a bad state as unhealthy, and from any state to a vestigial state as decaying.",
                "From Fig. 9, we can observe: • The bulk of the ratings have negligible values, irrespective of their role.",
                "The majority of the transitions involve both good and bad ratings becoming negligible.",
                "Scout + (2%) Scout(1.5%) Scout 0 (96.5%) Connector + (1.2%) Connector(0.8%) Connector 0 (98%) Promoter + (3%) Promoter(2%) Promoter 0 (95%) 84% 84% 81% 74% 10% 6% 11% 77% 8% 7% 8% 82% 4% 86% 4% 68% 15% 13% 5% 5% 77% 11% 7% 5% 4% 3% 3% 3% Healthy Unhealthy Decaying Figure 9: Transitions among rating roles. • The number of good ratings is comparable to the bad ratings, and ratings are seen to switch states often, except in the case of scouts (see below). • The negative and positive scout states are not reachable through any transition, indicating that these ratings must begin as such, and cannot be coerced into these roles. • Good promoters and good connectors have a much longer survival period than scouts.",
                "Transitions that recur to these states have frequencies of 10% and 15% when compared to just 4% for scouts.",
                "Good connectors are the slowest to decay whereas (good) scouts decay the fastest. • Healthy percentages are seen on transitions between promoters and connectors.",
                "As indicated earlier, there are hardly any transitions from promoters/connectors to scouts.",
                "This indicates that, over the long run, a users rating is more useful to others (movies or other users) than to the user himself. • The percentages of healthy transitions outweigh the unhealthy ones - this hints that the system is healthy, albeit only marginally.",
                "Note that these results are conditioned by the static nature of the dataset, which is a set of ratings over a fixed window of time.",
                "However a diagram such as Fig. 9 is clearly useful for monitoring the health of a recommender system.",
                "For instance, acceptable limits can be imposed on different types of transitions and, if a transition fails to meet the threshold, the recommender system or a part of it can be brought under closer scrutiny.",
                "Furthermore, the role state transition diagram would also be the ideal place to study the effects of shilling, a topic we will consider in future research. 5.6 Characterizing neighborhoods Earlier we saw that we can characterize the neighborhood of ratings involved in creating a recommendation list L for 258 a user.",
                "In our experiment, we consider lists of length 30, and sample the lists of about 5% of users through the evolution of the model (at intervals of 10,000 ratings each) and compute their neighborhood characteristics.",
                "To simplify our presentation, we consider the percentage of the sample that fall into one of the following categories: 1.",
                "Inactive user: (SFN(u) = 0) 2.",
                "Good scouts, Good neighborhood: (SFN(u) > 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 3.",
                "Good scouts, Bad neighborhood: (SFN(u) > 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) 4.",
                "Bad scouts, Good neighborhood: (SFN(u) < 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 5.",
                "Bad scouts, Bad neighborhood: (SFN(u) < 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) From our sample set of 561 users, we found that 476 users were inactive.",
                "Of the remaining 85 users, we found 26 users had good scouts and a good neighborhood, 6 had bad scouts and a good neighborhood, 29 had good scouts and a bad neighborhood, and 24 had bad scouts and a bad neighborhood.",
                "Thus, we conjecture that 59 users (29+24+6) are in danger of leaving the system.",
                "As a remedy, users with bad scouts and a good neighborhood can be asked to reconsider rating of some movies hoping to improve the systems recommendations.",
                "The system can be expected to deliver more if they engineer some good scouts.",
                "Users with good scouts and a bad neighborhood are harder to address; this situation might entail selectively removing some connector-promoter pairs that are causing the damage.",
                "Handling users with bad scouts and bad neighborhoods is a more difficult challenge.",
                "Such a classification allows the use of different strategies to better a users experience with the system depending on his context.",
                "In future work, we intend to conduct field studies and study the improvement in performance of different strategies for different contexts. 6.",
                "CONCLUSIONS To further recommender system acceptance and deployment, we require new tools and methodologies to manage an installed recommender and develop insights into the roles played by ratings.",
                "A fine-grained characterization in terms of rating roles such as scouts, promoters, and connectors, as done here, helps such an endeavor.",
                "Although we have presented results on only the item-based algorithm with list rank accuracy as the metric, the same approach outlined here applies to user-based algorithms and other metrics.",
                "In future research, we plan to systematically study the many algorithmic parameters, tolerances, and cutoff thresholds employed here and reason about their effects on the downstream conclusions.",
                "We also aim to extend our formulation to other collaborative filtering algorithms, study the effect of shilling in altering rating roles, conduct field studies, and evaluate improvements in user experience by tweaking ratings based on their role values.",
                "Finally, we plan to develop the idea of mining the evolution of rating role patterns into a reporting and tracking system for all aspects of recommender system health. 7.",
                "REFERENCES [1] Cosley, D., Lam, S., Albert, I., Konstan, J., and Riedl, J.",
                "Is Seeing Believing? : How Recommender System Interfaces Affect Users Opinions.",
                "In Proc.",
                "CHI (2001), pp. 585-592. [2] Herlocker, J. L., Konstan, J.",
                "A., Borchers, A., and Riedl, J.",
                "An Algorithmic Framework for Performing Collaborative Filtering.",
                "In Proc.",
                "SIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J.",
                "A., Terveen, L. G., and Riedl, J. T. Evaluating Collaborative Filtering Recommender Systems.",
                "ACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J.",
                "A.",
                "Personal communication. 2003. [5] Lam, S. K., and Riedl, J. Shilling Recommender Systems for Fun and Profit.",
                "In Proceedings of the 13th International World Wide Web Conference (2004), ACM Press, pp. 393-402. [6] Laxman, S., Sastry, P. S., and Unnikrishnan, K. P. Discovering Frequent Episodes and Learning Hidden Markov Models: A Formal Connection.",
                "IEEE Transactions on Knowledge and Data Engineering Vol. 17, 11 (2005), 1505-1517. [7] McLaughlin, M. R., and Herlocker, J. L. A Collaborative Filtering Algorithm and Evaluation Metric that Accurately Model the User Experience.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (2004), pp. 329 - 336. [8] OMahony, M., Hurley, N. J., Kushmerick, N., and Silvestre, G. Collaborative Recommendation: A Robustness Analysis.",
                "ACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J.",
                "A., and Riedl, J.",
                "Getting to Know You: Learning New User Preferences in Recommender Systems.",
                "In Proceedings of the 2002 Conference on Intelligent User Interfaces (IUI 2002) (2002), pp. 127-134. [10] Rashid, A. M., Karypis, G., and Riedl, J.",
                "Influence in Ratings-Based Recommender Systems: An Algorithm-Independent Approach.",
                "In Proc. of the SIAM International Conference on Data Mining (2005). [11] Resnick, P., Iacovou, N., Sushak, M., Bergstrom, P., and Riedl, J. GroupLens: An Open Architecture for Collaborative Filtering of Netnews.",
                "In Proceedings of the Conference on Computer Supported Collaborative Work (CSCW94) (1994), ACM Press, pp. 175-186. [12] Sarwar, B., Karypis, G., Konstan, J., and Reidl, J. Item-Based Collaborative Filtering Recommendation Algorithms.",
                "In Proceedings of the Tenth International World Wide Web Conference (WWW10) (2001), pp. 285-295. [13] Schein, A., Popescu, A., Ungar, L., and Pennock, D. Methods and Metrics for Cold-Start Recommendation.",
                "In Proc.",
                "SIGIR (2002), pp. 253-260. 259"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "La calidad de este \"proceso de agregación\" afecta de manera crucial la experiencia del usuario y, por lo tanto, la efectividad de los recomendadores en el comercio electrónico."
            ],
            "translated_text": "",
            "candidates": [
                "proceso de agregación",
                "proceso de agregación"
            ],
            "error": []
        },
        "recommender": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Scouts, Promoters, and Connectors: The Roles of Ratings in Nearest Neighbor Collaborative Filtering Bharath Kumar Mohan Dept.",
                "of CSA Indian Institute of Science Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. of Computer Science Eastern Michigan University Ypsilanti, MI 48917, USA bkeller@emich.edu Naren Ramakrishnan Dept. of Computer Science Virginia Tech, Blacksburg VA 24061, USA naren@cs.vt.edu ABSTRACT <br>recommender</br> systems aggregate individual user ratings into predictions of products or services that might interest visitors.",
                "The quality of this aggregation process crucially affects the user experience and hence the effectiveness of recommenders in e-commerce.",
                "We present a novel study that disaggregates global <br>recommender</br> performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearestneighbor collaborative filtering.",
                "In particular, we formulate three roles-scouts, promoters, and connectors-that capture how users receive recommendations, how items get recommended, and how ratings of these two types are themselves connected (resp.).",
                "These roles find direct uses in improving recommendations for users, in better targeting of items and, most importantly, in helping monitor the health of the system as a whole.",
                "For instance, they can be used to track the evolution of neighborhoods, to identify rating subspaces that do not contribute (or contribute negatively) to system performance, to enumerate users who are in danger of leaving, and to assess the susceptibility of the system to attacks such as shilling.",
                "We argue that the three rating roles presented here provide broad primitives to manage a <br>recommender</br> system and its community.",
                "Categories and Subject Descriptors H.4.2 [Information Systems Applications]: Types of Systems-Decision support; J.4 [Computer Applications]: Social and Behavioral Sciences General Terms Algorithms, Human Factors 1.",
                "INTRODUCTION <br>recommender</br> systems have become integral to e-commerce, providing technology that suggests products to a visitor based on previous purchases or rating history.",
                "Collaborative filtering, a common form of recommendation, predicts a users rating for an item by combining (other) ratings of that user with other users ratings.",
                "Significant research has been conducted in implementing fast and accurate collaborative filtering algorithms [2, 7], designing interfaces for presenting recommendations to users [1], and studying the robustness of these algorithms [8].",
                "However, with the exception of a few studies on the influence of users [10], little attention has been paid to unraveling the inner workings of a <br>recommender</br> in terms of the individual ratings and the roles they play in making (good) recommendations.",
                "Such an understanding will give an important handle to monitoring and managing a <br>recommender</br> system, to engineer mechanisms to sustain the <br>recommender</br>, and thereby ensure its continued success.",
                "Our motivation here is to disaggregate global <br>recommender</br> performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearest-neighbor collaborative filtering.",
                "We identify three possible roles: (scouts) to connect the user into the system to receive recommendations, (promoters) to connect an item into the system to be recommended, and (connectors) to connect ratings of these two kinds.",
                "Viewing ratings in this way, we can define the contribution of a rating in each role, both in terms of allowing recommendations to occur, and in terms of influence on the quality of recommendations.",
                "In turn, this capability helps support scenarios such as: 1.",
                "Situating users in better neighborhoods: A users ratings may inadvertently connect the user to a neighborhood for which the users tastes may not be a perfect match.",
                "Identifying ratings responsible for such bad recommendations and suggesting new items to rate can help situate the user in a better neighborhood. 2.",
                "Targeting items: <br>recommender</br> systems suffer from lack of user participation, especially in cold-start scenarios [13] involving newly arrived items.",
                "Identifying users who can be encouraged to rate specific items helps ensure coverage of the <br>recommender</br> system. 3.",
                "Monitoring the evolution of the <br>recommender</br> system and its stakeholders: A <br>recommender</br> system is constantly under change: growing with new users and 250 items, shrinking with users leaving the system, items becoming irrelevant, and parts of the system under attack.",
                "Tracking the roles of a rating and its evolution over time provides many insights into the health of the system, and how it could be managed and improved.",
                "These include being able to identify rating subspaces that do not contribute (or contribute negatively) to system performance, and could be removed; to enumerate users who are in danger of leaving, or have left the system; and to assess the susceptibility of the system to attacks such as shilling [5].",
                "As we show, the characterization of rating roles presented here provides broad primitives to manage a <br>recommender</br> system and its community.",
                "The rest of the paper is organized as follows.",
                "Background on nearest-neighbor collaborative filtering and algorithm evaluation is discussed in Section 2.",
                "Section 3 defines and discusses the roles of a rating, and Section 4 defines measures of the contribution of a rating in each of these roles.",
                "In Section 5, we illustrate the use of these roles to address the goals outlined above. 2.",
                "BACKGROUND 2.1 Algorithms Nearest-neighbor collaborative filtering algorithms either use neighborhoods of users or neighborhoods of items to compute a prediction.",
                "An algorithm of the first kind is called user-based, and one of the second kind is called itembased [12].",
                "In both families of algorithms, neighborhoods are formed by first computing the similarity between all pairs of users (for user-based) or items (for item-based).",
                "Predictions are then computed by aggregating ratings, which in a user-based algorithm involves aggregating the ratings of the target item by the users neighbors and, in an item-based algorithm, involves aggregating the users ratings of items that are neighbors of the target item.",
                "Algorithms within these families differ in the definition of similarity, formation of neighborhoods, and the computation of predictions.",
                "We consider a user-based algorithm based on that defined for GroupLens [11] with variations from Herlocker et al. [2], and an item-based algorithm similar to that of Sarwar et al. [12].",
                "The algorithm used by Resnick et al. [11] defines the similarity of two users u and v as the Pearson correlation of their common ratings: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2 , where Iu is the set of items rated by user u, ru,i is user us rating for item i, and ¯ru is the average rating of user u (similarly for v).",
                "Similarity computed in this manner is typically scaled by a factor proportional to the number of common ratings, to reduce the chance of making a recommendation made on weak connections: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), where γ ≈ 5 is a constant used as a lower limit in scaling [2].",
                "These new similarities are then used to define a static neighborhood Nu for each user u consisting of the top K users most similar to user u.",
                "A prediction for user u and item i is computed by a weighted average of the ratings by the neighbors pu,i = ¯ru + P v∈V sim (u, v)(rv,i − ¯rv) P v∈V sim (u, v) (1) where V = Nu ∩ Ui is the set of users most similar to u who have rated i.",
                "The item-based algorithm we use is the one defined by Sarwar et al. [12].",
                "In this algorithm, similarity is defined as the adjusted cosine measure sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) where Ui is the set of users who have rated item i.",
                "As for the user-based algorithm, the similarity weights are adjusted proportionally to the number of users that have rated the items in common sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). (3) Given the similarities, the neighborhood Ni of an item i is defined as the top K most similar items for that item.",
                "A prediction for user u and item i is computed as the weighted average pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) where J = Ni ∩ Iu is the set of items rated by u that are most similar to i. 2.2 Evaluation <br>recommender</br> algorithms have typically been evaluated using measures of predictive accuracy and coverage [3].",
                "Studies on <br>recommender</br> algorithms, notably Herlocker et al. [2] and Sarwar et al. [12], typically compute predictive accuracy by dividing a set of ratings into training and test sets, and compute the prediction for an item in the test set using the ratings in the training set.",
                "A standard measure of predictive accuracy is mean absolute error (MAE), which for a test set T = {(u, i)} is defined as, MAE = P (u,i)∈T |pu,i − ru,i| |T | . (5) Coverage has a number of definitions, but generally refers to the proportion of items that can be predicted by the algorithm [3].",
                "A practical issue with predictive accuracy is that users typically are presented with recommendation lists, and not individual numeric predictions.",
                "Recommendation lists are lists of items in decreasing order of prediction (sometimes stated in terms of star-ratings), and so predictive accuracy may not be reflective of the accuracy of the list.",
                "So, instead we can measure recommendation or rank accuracy, which indicates the extent to which the list is in the correct order.",
                "Herlocker et al. [3] discuss a number of rank accuracy measures, which range from Kendalls Tau to measures that consider the fact that users tend to only look at a prefix of the list [5].",
                "Kendalls Tau measures the number of inversions when comparing ordered pairs in the true user ordering of 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figure 1: Ratings in simple movie <br>recommender</br>. items and the recommended order, and is defined as τ = C − D p (C + D + TR)(C + D + TP) (6) where C is the number of pairs that the system predicts in the correct order, D the number of pairs the system predicts in the wrong order, TR the number of pairs in the true ordering that have the same ratings, and TP is the number of pairs in the predicted ordering that have the same ratings [3].",
                "A shortcoming of the Tau metric is that it is oblivious to the position in the ordered list where the inversion occurs [3].",
                "For instance, an inversion toward the end of the list is given the same weight as one in the beginning.",
                "One solution is to consider inversions only in the top few items in the recommended list or to weight inversions based on their position in the list. 3.",
                "ROLES OF A RATING Our basic observation is that each rating plays a different role in each prediction in which it is used.",
                "Consider a simplified movie <br>recommender</br> system with three users Jim, Jeff, and Tom and their ratings for a few movies, as shown in Fig. 1. (For this initial discussion we will not consider the rating values involved.)",
                "The <br>recommender</br> predicts whether Tom will like The Mask using the other already available ratings.",
                "How this is done depends on the algorithm: 1.",
                "An item-based collaborative filtering algorithm constructs a neighborhood of movies around The Mask by using the ratings of users who rated The Mask and other movies similarly (e.g., Jims ratings of The Matrix and The Mask; and Jeffs ratings of Star Wars and The Mask).",
                "Toms ratings of those movies are then used to make a prediction for The Mask. 2.",
                "A user-based collaborative filtering algorithm would construct a neighborhood around Tom by tracking other users whose rating behaviors are similar to Toms (e.g., Tom and Jeff have rated Star Wars; Tom and Jim have rated The Matrix).",
                "The prediction of Toms rating for The Mask is then based on the ratings of Jeff and Tim.",
                "Although the nearest-neighbor algorithms aggregate the ratings to form neighborhoods used to compute predictions, we can disaggregate the similarities to view the computation of a prediction as simultaneously following parallel paths of ratings.",
                "So, irrespective of the collaborative filtering algorithm used, we can visualize the prediction of Toms rating of The Mask as walking through a sequence of ratings.",
                "In Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figure 2: Ratings used to predict The Mask for Tom.",
                "Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figure 3: Prediction of The Mask for Tom in which a rating is used more than once. this example, two paths were used for this prediction as depicted in Fig. 2: (p1, p2, p3) and (q1, q2, q3).",
                "Note that these paths are undirected, and are all of length 3.",
                "Only the order in which the ratings are traversed is different between the item-based algorithm (e.g., (p3, p2, p1), (q3, q2, q1)) and the user-based algorithm (e.g., (p1, p2, p3), (q1, q2, q3).)",
                "A rating can be part of many paths for a single prediction as shown in Fig. 3, where three paths are used for a prediction, two of which follow p1: (p1, p2, p3) and (p1, r2, r3).",
                "Predictions in a collaborative filtering algorithms may involve thousands of such walks in parallel, each playing a part in influencing the predicted value.",
                "Each prediction path consists of three ratings, playing roles that we call scouts, promoters, and connectors.",
                "To illustrate these roles, consider the path (p1, p2, p3) in Fig. 2 used to make a prediction of The Mask for Tom: 1.",
                "The rating p1 (Tom → Star Wars) makes a connection from Tom to other ratings that can be used to predict Toms rating for The Mask.",
                "This rating serves as a scout in the bipartite graph of ratings to find a path that leads to The Mask. 2.",
                "The rating p2 (Jeff → Star Wars) helps the system recommend The Mask to Tom by connecting the scout to the promoter. 3.",
                "The rating p3 (Jeff → The Mask) allows connections to The Mask, and, therefore, promotes this movie to Tom.",
                "Formally, given a prediction pu,a of a target item a for user u, a scout for pu,a is a rating ru,i such that there exists a user v with ratings rv,a and rv,i for some item i; a promoter for pu,a is a rating rv,a for some user v, such that there exist ratings rv,i and ru,i for an item i, and; a connector for pu,a 252 Jim Tom Jeff Jerry My Cousin Vinny The Matrix Star Wars The Mask Jurasic Park Figure 4: Scouts, promoters, and connectors. is a rating rv,i by some user v and rating i, such that there exists ratings ru,i and rv,a.",
                "The scouts, connectors, and promoters for the prediction of Toms rating of The Mask are p1 and q1, p2 and q2, and p3 and q3 (respectively).",
                "Each of these roles has a value in the <br>recommender</br> to the user, the users neighborhood, and the system in terms of allowing recommendations to be made. 3.1 Roles in Detail Ratings that act as scouts tend to help the <br>recommender</br> system suggest more movies to the user, though the extent to which this is true depends on the rating behavior of other users.",
                "For example, in Fig. 4 the rating Tom → Star Wars helps the system recommend only The Mask to him, while Tom → The Matrix helps recommend The Mask, Jurassic Park, and My Cousin Vinny.",
                "Tom makes a connection to Jim who is a prolific user of the system, by rating The Matrix.",
                "However, this does not make The Matrix the best movie to rate for everyone.",
                "For example, Jim is benefited equally by both The Mask and The Matrix, which allow the system to recommend Star Wars to him.",
                "His rating of The Mask is the best scout for Jeff, and Jerrys only scout is his rating of Star Wars.",
                "This suggests that good scouts allow a user to build similarity with prolific users, and thereby ensure they get more from the system.",
                "While scouts represent beneficial ratings from the perspective of a user, promoters are their duals, and are of benefit to items.",
                "In Fig. 4, My Cousin Vinny benefits from Jims rating, since it allows recommendations to Jeff and Tom.",
                "The Mask is not so dependent on just one rating, since the ratings by Jim and Jeff help it.",
                "On the other hand, Jerrys rating of Star Wars does not help promote it to any other user.",
                "We conclude that a good promoter connects an item to a broader neighborhood of other items, and thereby ensures that it is recommended to more users.",
                "Connectors serve a crucial role in a <br>recommender</br> system that is not as obvious.",
                "The movies My Cousin Vinny and Jurassic Park have the highest recommendation potential since they can be recommended to Jeff, Jerry and Tom based on the linkage structure illustrated in Fig. 4.",
                "Beside the fact that Jim rated these movies, these recommendations are possible only because of the ratings Jim → The Matrix and Jim → The Mask, which are the best connectors.",
                "A connector improves the systems ability to make recommendations with no explicit gain for the user.",
                "Note that every rating can be of varied benefit in each of these roles.",
                "The rating Jim → My Cousin Vinny is a poor scout and connector, but is a very good promoter.",
                "The rating Jim → The Mask is a reasonably good scout, a very good connector, and a good promoter.",
                "Finally, the rating Jerry → Star Wars is a very good scout, but is of no value as a connector or promoter.",
                "As illustrated here, a rating can have different value in each of the three roles in terms of whether a recommendation can be made or not.",
                "We could measure this value by simply counting the number of times a rating is used in each role, which alone would be helpful in characterizing the behavior of a system.",
                "But we can also measure the contribution of each rating to the quality of recommendations or health of the system.",
                "Since every prediction is a combined effort of several recommendation paths, we are interested in discerning the influence of each rating (and, hence, each path) in the system towards the systems overall error.",
                "We can understand the dynamics of the system at a finer granularity by tracking the influence of a rating according to the role played.",
                "The next section describes the approach to measuring the values of a rating in each role. 4.",
                "CONTRIBUTIONS OF RATINGS As weve seen, a rating may play different roles in different predictions and, in each prediction, contribute to the quality of a prediction in different ways.",
                "Our approach can use any numeric measure of a property of system health, and assigns credit (or blame) to each rating proportional to its influence in the prediction.",
                "By tracking the role of each rating in a prediction, we can accumulate the credit for a rating in each of the three roles, and also track the evolution of the roles of rating over time in the system.",
                "This section defines the methodology for computing the contribution of ratings by first defining the influence of a rating, and then instantiating the approach for predictive accuracy, and then rank accuracy.",
                "We also demonstrate how these contributions can be aggregated to study the neighborhood of ratings involved in computing a users recommendations.",
                "Note that although our general formulation for rating influence is algorithm independent, due to space considerations, we present the approach for only item-based collaborative filtering.",
                "The definition for user-based algorithms is similar and will be presented in an expanded version of this paper. 4.1 Influence of Ratings Recall that an item-based approach to collaborative filtering relies on building item neighborhoods using the similarity of ratings by the same user.",
                "As described earlier, similarity is defined by the adjusted cosine measure (Equations (2) and (3)).",
                "A set of the top K neighbors is maintained for all items for space and computational efficiency.",
                "A prediction of item i for a user u is computed as the weighted deviation from the items mean rating as shown in Equation (4).",
                "The list of recommendations for a user is then the list of items sorted in descending order of their predicted values.",
                "We first define impact(a, i, j), the impact a user a has in determining the similarity between two items i and j.",
                "This is the change in the similarity between i and j when as rating is removed, and is defined as impact(a, i, j) = |sim (i, j) − sim¯a(i, j)| P w∈Cij |sim (i, j) − sim ¯w(i, j)| where Cij = {u ∈ U | ∃ ru,i, ru,j ∈ R(u)} is the set of coraters 253 of items i and j (users who rate both i and j), R(u) is the set of ratings provided by user u, and sim¯a(i, j) is the similarity of i and j when the ratings of user a are removed sim¯a(i, j) = P v∈U\\{a} (ru,i − ¯ru)(ru,j − ¯ru) qP u∈U\\{a}(ru,i − ¯ru)2 qP u∈U\\{a}(ru,j − ¯ru)2 , and adjusted for the number of raters sim¯a(i, j) = max(|Ui ∩ Uj| − 1, γ) γ · sim(i, j).",
                "If all coraters of i and j rate them identically, we define the impact as impact(a, i, j) = 1 |Cij| since P w∈Cij |sim (i, j) − sim ¯w(i, j)| = 0.",
                "The influence of each path (u, j, v, i) = [ru,j, rv,j, rv,i] in the prediction of pu,i is given by influence(u, j, v, i) = sim (i, j) P l∈Ni∩Iu sim (i, l) · impact(v, i, j) It follows that the sum of influences over all such paths, for a given set of endpoints, is 1. 4.2 Role Values for Predictive Accuracy The value of a rating in each role is computed from the influence depending on the evaluation measure employed.",
                "Here we illustrate the approach using predictive accuracy as the evaluation metric.",
                "In general, the goodness of a prediction decides whether the ratings involved must be credited or discredited for their role.",
                "For predictive accuracy, the error in prediction e = |pu,i − ru,i| is mapped to a comfort level using a mapping function M(e).",
                "Anecdotal evidence suggests that users are unable to discern errors less than 1.0 (for a rating scale of 1 to 5) [4], and so an error less than 1.0 is considered acceptable, but anything larger is not.",
                "We hence define M(e) as (1 − e) binned to an appropriate value in [−1, −0.5, 0.5, 1].",
                "For each prediction pu,i, M(e) is attributed to all the paths that assisted the computation of pu,i, proportional to their influences.",
                "This tribute, M(e)∗influence(u, j, v, i), is in turn inherited by each of the ratings in the path [ru,j, rv,j, rv,i], with the credit/blame accumulating to the respective roles of ru,j as a scout, rv,j as a connector, and rv,i as a promoter.",
                "In other words, the scout value SF(ru,j), the connector value CF(rv,j) and the promoter value PF(rv,i) are all incremented by the tribute amount.",
                "Over a large number of predictions, scouts that have repeatedly resulted in big error rates have a big negative scout value, and vice versa (similarly with the other roles).",
                "Every rating is thus summarized by its triple [SF, CF, PF]. 4.3 Role Values for Rank Accuracy We now define the computation of the contribution of ratings to observed rank accuracy.",
                "For this computation, we must know the users preference order for a set of items for which predictions can be computed.",
                "We assume that we have a test set of the users ratings of the items presented in the recommendation list.",
                "For every pair of items rated by a user in the test data, we check whether the predicted order is concordant with his preference.",
                "We say a pair (i, j) is concordant (with error ) whenever one of the following holds: • if (ru,i < ru,j) then (pu,i − pu,j < ); • if (ru,i > ru,j) then (pu,i − pu,j > ); or • if (ru,i = ru,j) then (|pu,i − pu,j| ≤ ).",
                "Similarly, a pair (i, j) is discordant (with error ) if it is not concordant.",
                "Our experiments described below use an error tolerance of = 0.1.",
                "All paths involved in the prediction of the two items in a concordant pair are credited, and the paths involved in a discordant pair are discredited.",
                "The credit assigned to a pair of items (i, j) in the recommendation list for user u is computed as c(i, j) = ( t T · 1 C+D if (i, j) are concordant − t T · 1 C+D if (i, j) are discordant (7) where t is the number of items in the users test set whose ratings could be predicted, T is the number of items rated by user u in the test set, C is the number of concordances and D is the number of discordances.",
                "The credit c is then divided among all paths responsible for predicting pu,i and pu,j proportional to their influences.",
                "We again add the role values obtained from all the experiments to form a triple [SF, CF, PF] for each rating. 4.4 Aggregating rating roles After calculating the role values for individual ratings, we can also use these values to study neighborhoods and the system.",
                "Here we consider how we can use the role values to characterize the health of a neighborhood.",
                "Consider the list of top recommendations presented to a user at a specific point in time.",
                "The collaborative filtering algorithm traversed many paths in his neighborhood through his scouts and other connectors and promoters to make these recommendations.",
                "We call these ratings the <br>recommender</br> neighborhood of the user.",
                "The user implicitly chooses this neighborhood of ratings through the items he rates.",
                "Apart from the collaborative filtering algorithm, the health of this neighborhood completely influences a users satisfaction with the system.",
                "We can characterize a users <br>recommender</br> neighborhood by aggregating the individual role values of the ratings involved, weighted by the influence of individual ratings in determining his recommended list.",
                "Different sections of the users neighborhood wield varied influence on his recommendation list.",
                "For instance, ratings reachable through highly rated items have a bigger say in the recommended items.",
                "Our aim is to study the system and classify users with respect to their positioning in a healthy or unhealthy neighborhood.",
                "A user can have a good set of scouts, but may be exposed to a neighborhood with bad connectors and promoters.",
                "He can have a good neighborhood, but his bad scouts may ensure the neighborhoods potential is rendered useless.",
                "We expect that users with good scouts and good neighborhoods will be most satisfied with the system in the future.",
                "A users neighborhood is characterized by a triple that represents the weighted sum of the role values of individual ratings involved in making recommendations.",
                "Consider a user u and his ordered list of recommendations L. An item i 254 in the list is weighted inversely, as K(i), depending on its position in the list.",
                "In our studies we use K(i) = p position(i).",
                "Several paths of ratings [ru,j, rv,j, rv,i] are involved in predicting pu,i which ultimately decides its position in L, each with influence(u, j, v, i).",
                "The <br>recommender</br> neighborhood of a user u is characterized by the triple, [SFN(u), CFN(u), PFN(u)] where SFN(u) = X i∈L P [ru,j ,rv,j ,rv,i] SF(ru,j)influence(u, j, v, i) K(i) !",
                "CFN(u) and PFN(u) are defined similarly.",
                "This triple estimates the quality of us recommendations based on the past track record of the ratings involved in their respective roles. 5.",
                "EXPERIMENTATION As we have seen, we can assign role values to each rating when evaluating a collaborative filtering system.",
                "In this section, we demonstrate the use of this approach to our overall goal of defining an approach to monitor and manage the health of a <br>recommender</br> system through experiments done on the MovieLens million rating dataset.",
                "In particular, we discuss results relating to identifying good scouts, promoters, and connectors; the evolution of rating roles; and the characterization of user neighborhoods. 5.1 Methodology Our experiments use the MovieLens million rating dataset, which consists of ratings by 6040 users of 3952 movies.",
                "The ratings are in the range 1 to 5, and are labeled with the time the rating was given.",
                "As discussed before, we consider only the item-based algorithm here (with item neighborhoods of size 30) and, due to space considerations, only present role value results for rank accuracy.",
                "Since we are interested in the evolution of the rating role values over time, the model of the <br>recommender</br> system is built by processing ratings in their arrival order.",
                "The timestamping provided by MovieLens is hence crucial for the analyses presented here.",
                "We make assessments of rating roles at intervals of 10,000 ratings and processed the first 200,000 ratings in the dataset (giving rise to 20 snapshots).",
                "We incrementally update the role values as the time ordered ratings are merged into the model.",
                "To keep the experiment computationally manageable, we define a test dataset for each user.",
                "As the time ordered ratings are merged into the model, we label a small randomly selected percentage (20%) as test data.",
                "At discrete epochs, i.e., after processing every 10,000 ratings, we compute the predictions for the ratings in the test data, and then compute the role values for the ratings used in the predictions.",
                "One potential criticism of this methodology is that the ratings in the test set are never evaluated for their roles.",
                "We overcome this concern by repeating the experiment, using different random seeds.",
                "The probability that every rating is considered for evaluation is then considerably high: 1 − 0.2n , where n is the number of times the experiment is repeated with different random seeds.",
                "The results here are based on n = 4 repetitions.",
                "The item-based collaborative filtering algorithms performance was ordinary with respect to rank accuracy.",
                "Fig. 5 shows a plot of the precision and recall as ratings were merged in time order into the model.",
                "The recall was always high, but the average precision was just about 53%. 0 0.2 0.4 0.6 0.8 1 1.2 10000 30000 50000 70000 90000110000130000150000 Ratings merged into model Value Precision Recall Figure 5: Precision and recall for the item-based collaborative filtering algorithm. 5.2 Inducing good scouts The ratings of a user that serve as scouts are those that allow the user to receive recommendations.",
                "We claim that users with ratings that have respectable scout values will be happier with the system than those with ratings with low scout values.",
                "Note that the item-based algorithm discussed here produces recommendation lists with nearly half of the pairs in the list discordant from the users preference.",
                "Whether all of these discordant pairs are observable by the user is unclear, however, certainly this suggests that there is a need to be able to direct users to items whose ratings would improve the lists.",
                "The distribution of the scout values for most users ratings are Gaussian with mean zero.",
                "Fig. 6 shows the frequency distribution of scout values for a sample user at a given snapshot.",
                "We observe that a large number of ratings never serve as scouts for their users.",
                "A relatable scenario is when Amazons <br>recommender</br> makes suggestions of books or items based on other items that were purchased as gifts.",
                "With simple relevance feedback from the user, such ratings can be isolated as bad scouts and discounted from future predictions.",
                "Removing bad scouts was found to be worthwhile for individual users but the overall performance improvement was only marginal.",
                "An obvious question is whether good scouts can be formed by merely rating popular movies as suggested by Rashid et al. [9].",
                "They show that a mix of popularity and rating entropy identifies the best items to suggest to new users when evaluated using MAE.",
                "Following their intuition, we would expect to see a higher correlation between popularityentropy and good scouts.",
                "We measured the Pearson correlation coefficient between aggregated scout values for a movie with the popularity of a movie (number of times it is rated); and with its popularity*variance measure at different snapshots of the system.",
                "Note that the scout values were initially anti-correlated with popularity (Fig. 7), but became moderately correlated as the system evolved.",
                "Both popularity and popularity*variance performed similarly.",
                "A possible explanation is that there has been insufficient time for the popular movies to accumulate ratings. 255 -10 0 10 20 30 40 50 60 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 Scout Value Frequency Figure 6: Distribution of scout values for a sample user. -0.4 -0.2 0 0.2 0.4 0.6 0.8 30000 60000 90000 120000 150000 180000 Popularity Pop*Var Figure 7: Correlation between aggregated scout value and item popularity (computed at different intervals). -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 30000 60000 90000 120000 150000 180000 Figure 8: Correlation between aggregated promoter value and user prolificity (computed at different intervals).",
                "Table 1: Movies forming the best scouts.",
                "Best Scouts Conf.",
                "Pop.",
                "Being John Malkovich (1999) 1.00 445 Star Wars: Episode IV - A New Hope (1977) 0.92 623 Princess Bride, The (1987) 0.85 477 Sixth Sense, The (1999) 0.85 617 Matrix, The (1999) 0.77 522 Ghostbusters (1984) 0.77 441 Casablanca (1942) 0.77 384 Insider, The (1999) 0.77 235 American Beauty (1999) 0.69 624 Terminator 2: Judgment Day (1991) 0.69 503 Fight Club (1999) 0.69 235 Shawshank Redemption, The (1994) 0.69 445 Run Lola Run (Lola rennt) (1998) 0.69 220 Terminator, The (1984) 0.62 450 Usual Suspects, The (1995) 0.62 326 Aliens (1986) 0.62 385 North by Northwest (1959) 0.62 245 Fugitive, The (1993) 0.62 402 End of Days (1999) 0.62 132 Raiders of the Lost Ark (1981) 0.54 540 Schindlers List (1993) 0.54 453 Back to the Future (1985) 0.54 543 Toy Story (1995) 0.54 419 Alien (1979) 0.54 415 Abyss, The (1989) 0.54 345 2001: A Space Odyssey (1968) 0.54 358 Dogma (1999) 0.54 228 Little Mermaid, The (1989) 0.54 203 Table 2: Movies forming the worst scouts.",
                "Worst scouts Conf.",
                "Pop.",
                "Harold and Maude (1971) 0.46 141 Grifters, The (1990) 0.46 180 Sting, The (1973) 0.38 244 Godfather: Part III, The (1990) 0.38 154 Lawrence of Arabia (1962) 0.38 167 High Noon (1952) 0.38 84 Women on the Verge of a... (1988) 0.38 113 Grapes of Wrath, The (1940) 0.38 115 Duck Soup (1933) 0.38 131 Arsenic and Old Lace (1944) 0.38 138 Midnight Cowboy (1969) 0.38 137 To Kill a Mockingbird (1962) 0.31 195 Four Weddings and a Funeral (1994) 0.31 271 Good, The Bad and The Ugly, The (1966) 0.31 156 Its a Wonderful Life (1946) 0.31 146 Player, The (1992) 0.31 220 Jackie Brown (1997) 0.31 118 Boat, The (Das Boot) (1981) 0.31 210 Manhattan (1979) 0.31 158 Truth About Cats & Dogs, The (1996) 0.31 143 Ghost (1990) 0.31 227 Lone Star (1996) 0.31 125 Big Chill, The (1983) 0.31 184 256 By studying the evolution of scout values, we can identify movies that consistently feature in good scouts over time.",
                "We claim these movies will make viable scouts for other users.",
                "We found the aggregated scout values for all movies in intervals of 10,000 ratings each.",
                "A movie is said to induce a good scout if the movie was in the top 100 of the sorted list, and to induce a bad scout if it was in bottom 100 of the same list.",
                "Movies appearing consistently high over time are expected to remain up there in the future.",
                "The effective confidence in a movie m is Cm = Tm − Bm N (8) where Tm is the number of times it appeared in the top 100, Bm the number of times it appeared in the bottom 100, and N is the number of intervals considered.",
                "Using this measure, the top few movies expected to induce the best scouts are shown in Table 1.",
                "Movies that would be bad scout choices are shown in Table 2 with their associated confidences.",
                "The popularities of the movies are also displayed.",
                "Although more popular movies appear in the list of good scouts, these tables show that a blind choice of scout based on popularity alone can be potentially dangerous.",
                "Interestingly, the best scout-Being John Malkovich-is about a puppeteer who discovers a portal into a movie star, a movie that has been described variously on amazon.com as makes you feel giddy, seriously weird, comedy with depth, silly, strange, and inventive.",
                "Indicating whether someone likes this movie or not goes a long way toward situating the user in a suitable neighborhood, with similar preferences.",
                "On the other hand, several factors may have made a movie a bad scout, like the sharp variance in user preferences in the neighborhood of a movie.",
                "Two users may have the same opinion about Lawrence of Arabia, but they may differ sharply about how they felt about the other movies they saw.",
                "Bad scouts ensue when there is deviation in behavior around a common synchronization point. 5.3 Inducing good promoters Ratings that serve to promote items in a collaborative filtering system are critical to allowing a new item be recommended to users.",
                "So, inducing good promoters is important for cold-start recommendation.",
                "We note that the frequency distribution of promoter values for a sample movies ratings is also Gaussian (similar to Fig. 6).",
                "This indicates that the promotion of a movie is benefited most by the ratings of a few users, and are unaffected by the ratings of most users.",
                "We find a strong correlation between a users number of ratings and his aggregated promoter value.",
                "Fig. 8 depicts the evolution of the Pearson correlation co-efficient between the prolificity of a user (number of ratings) versus his aggregated promoter value.",
                "We expect that conspicuous shills, by recommending wrong movies to users, will be discredited with negative aggregate promoter values and should be identifiable easily.",
                "Given this observation, the obvious rule to follow when introducing a new movie is to have it rated directly by prolific users who posses high aggregated promoter values.",
                "A new movie is thus cast into the neighborhood of many other movies improving its visibility.",
                "Note, though, that a user may have long stopped using the system.",
                "Tracking promoter values consistently allows only the most active recent users to be considered. 5.4 Inducing good connectors Given the way scouts, connectors, and promoters are characterized, it follows that the movies that are part of the best scouts are also part of the best connectors.",
                "Similarly, the users that constitute the best promoters are also part of the best connectors.",
                "Good connectors are induced by ensuring a user with a high promoter value rates a movie with a high scout value.",
                "In our experiments, we find that a ratings longest standing role is often as a connector.",
                "A rating with a poor connector value is often seen due to its user being a bad promoter, or its movie being a bad scout.",
                "Such ratings can be removed from the prediction process to bring marginal improvements to recommendations.",
                "In some selected experiments, we observed that removing a set of badly behaving connectors helped improve the systems overall performance by 1.5%.",
                "The effect was even higher on a few select users who observed an improvement of above 10% in precision without much loss in recall. 5.5 Monitoring the evolution of rating roles One of the more significant contributions of our work is the ability to model the evolution of <br>recommender</br> systems, by studying the changing roles of ratings over time.",
                "The role and value of a rating can change depending on many factors like user behavior, redundancy, shilling effects or properties of the collaborative filtering algorithm used.",
                "Studying the dynamics of rating roles in terms of transitions between good, bad, and negligible values can provide insights into the functioning of the <br>recommender</br> system.",
                "We believe that a continuous visualization of these transitions will improve the ability to manage a <br>recommender</br> system.",
                "We classify different rating states as good, bad, or negligible.",
                "Consider a user who has rated 100 movies in a particular interval, of which 20 are part of the test set.",
                "If a scout has a value greater than 0.005, it indicates that it is uniquely involved in at least 2 concordant predictions, which we will say is good.",
                "Thus, a threshold of 0.005 is chosen to bin a rating as good, bad or negligible in terms of its scout, connector and promoter value.",
                "For instance, a rating r, at time t with role value triple [0.1, 0.001, −0.01] is classified as [scout +, connector 0, promoter −], where + indicates good, 0 indicates negligible, and − indicates bad.",
                "The positive credit held by a rating is a measure of its contribution to the betterment of the system, and the discredit is a measure of its contribution to the detriment of the system.",
                "Even though the positive roles (and the negative roles) make up a very small percentage of all ratings, their contribution supersedes their size.",
                "For example, even though only 1.7% of all ratings were classified as good scouts, they hold 79% of all positive credit in the system!",
                "Similarly, the bad scouts were just 1.4% of all ratings but hold 82% of all discredit.",
                "Note that good and bad scouts, together, comprise only 1.4% + 1.7% = 3.1% of the ratings, implying that the majority of the ratings are negligible role players as scouts (more on this later).",
                "Likewise, good connectors were 1.2% of the system, and hold 30% of all positive credit.",
                "The bad connectors (0.8% of the system) hold 36% of all discredit.",
                "Good promoters (3% of the system) hold 46% of all credit, while bad promoters (2%) hold 50% of all discredit.",
                "This reiterates that a few ratings influence most of the systems performance.",
                "Hence it is important to track transitions between them regardless of their small numbers. 257 Across different snapshots, a rating can remain in the same state or change.",
                "A good scout can become a bad scout, a good promoter can become a good connector, good and bad scouts can become vestigial, and so on.",
                "It is not practical to expect a <br>recommender</br> system to have no ratings in bad roles.",
                "However, it suffices to see ratings in bad roles either convert to good or vestigial roles.",
                "Similarly, observing a large number of good roles become bad ones is a sign of imminent failure of the system.",
                "We employ the principle of non-overlapping episodes [6] to count such transitions.",
                "A sequence such as: [+, 0, 0] → [+, 0, 0] → [0, +, 0] → [0, 0, 0] is interpreted as the transitions [+, 0, 0] ; [0, +, 0] : 1 [+, 0, 0] ; [0, 0, 0] : 1 [0, +, 0] ; [0, 0, 0] : 1 instead of [+, 0, 0] ; [0, +, 0] : 2 [+, 0, 0] ; [0, 0, 0] : 2 [0, +, 0] ; [0, 0, 0] : 1.",
                "See [6] for further details about this counting procedure.",
                "Thus, a rating can be in one of 27 possible states, and there are about 272 possible transitions.",
                "We make a further simplification and utilize only 9 states, indicating whether the rating is a scout, promoter, or connector, and whether it has a positive, negative, or negligible role.",
                "Ratings that serve multiple purposes are counted using multiple episode instantiations but the states themselves are not duplicated beyond the 9 restricted states.",
                "In this model, a transition such as [+, 0, +] ; [0, +, 0] : 1 is counted as [scout+] ; [scout0] : 1 [scout+] ; [connector+] : 1 [scout+] ; [promoter0] : 1 [connector0] ; [scout0] : 1 [connector0] ; [scout+] : 1 [connector0] ; [promoter0] : 1 [promoter+] ; [scout0] : 1 [promoter+] ; [connector+] : 1 [promoter+] ; [promoter0] : 1 Of these, transitions like [pX] ; [q0] where p = q, X ∈ {+, 0, −} are considered uninteresting, and only the rest are counted.",
                "Fig. 9 depicts the major transitions counted while processing the first 200,000 ratings from the MovieLens dataset.",
                "Only transitions with frequency greater than or equal to 3% are shown.",
                "The percentages for each state indicates the number of ratings that were found to be in those states.",
                "We consider transitions from any state to a good state as healthy, from any state to a bad state as unhealthy, and from any state to a vestigial state as decaying.",
                "From Fig. 9, we can observe: • The bulk of the ratings have negligible values, irrespective of their role.",
                "The majority of the transitions involve both good and bad ratings becoming negligible.",
                "Scout + (2%) Scout(1.5%) Scout 0 (96.5%) Connector + (1.2%) Connector(0.8%) Connector 0 (98%) Promoter + (3%) Promoter(2%) Promoter 0 (95%) 84% 84% 81% 74% 10% 6% 11% 77% 8% 7% 8% 82% 4% 86% 4% 68% 15% 13% 5% 5% 77% 11% 7% 5% 4% 3% 3% 3% Healthy Unhealthy Decaying Figure 9: Transitions among rating roles. • The number of good ratings is comparable to the bad ratings, and ratings are seen to switch states often, except in the case of scouts (see below). • The negative and positive scout states are not reachable through any transition, indicating that these ratings must begin as such, and cannot be coerced into these roles. • Good promoters and good connectors have a much longer survival period than scouts.",
                "Transitions that recur to these states have frequencies of 10% and 15% when compared to just 4% for scouts.",
                "Good connectors are the slowest to decay whereas (good) scouts decay the fastest. • Healthy percentages are seen on transitions between promoters and connectors.",
                "As indicated earlier, there are hardly any transitions from promoters/connectors to scouts.",
                "This indicates that, over the long run, a users rating is more useful to others (movies or other users) than to the user himself. • The percentages of healthy transitions outweigh the unhealthy ones - this hints that the system is healthy, albeit only marginally.",
                "Note that these results are conditioned by the static nature of the dataset, which is a set of ratings over a fixed window of time.",
                "However a diagram such as Fig. 9 is clearly useful for monitoring the health of a <br>recommender</br> system.",
                "For instance, acceptable limits can be imposed on different types of transitions and, if a transition fails to meet the threshold, the <br>recommender</br> system or a part of it can be brought under closer scrutiny.",
                "Furthermore, the role state transition diagram would also be the ideal place to study the effects of shilling, a topic we will consider in future research. 5.6 Characterizing neighborhoods Earlier we saw that we can characterize the neighborhood of ratings involved in creating a recommendation list L for 258 a user.",
                "In our experiment, we consider lists of length 30, and sample the lists of about 5% of users through the evolution of the model (at intervals of 10,000 ratings each) and compute their neighborhood characteristics.",
                "To simplify our presentation, we consider the percentage of the sample that fall into one of the following categories: 1.",
                "Inactive user: (SFN(u) = 0) 2.",
                "Good scouts, Good neighborhood: (SFN(u) > 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 3.",
                "Good scouts, Bad neighborhood: (SFN(u) > 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) 4.",
                "Bad scouts, Good neighborhood: (SFN(u) < 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 5.",
                "Bad scouts, Bad neighborhood: (SFN(u) < 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) From our sample set of 561 users, we found that 476 users were inactive.",
                "Of the remaining 85 users, we found 26 users had good scouts and a good neighborhood, 6 had bad scouts and a good neighborhood, 29 had good scouts and a bad neighborhood, and 24 had bad scouts and a bad neighborhood.",
                "Thus, we conjecture that 59 users (29+24+6) are in danger of leaving the system.",
                "As a remedy, users with bad scouts and a good neighborhood can be asked to reconsider rating of some movies hoping to improve the systems recommendations.",
                "The system can be expected to deliver more if they engineer some good scouts.",
                "Users with good scouts and a bad neighborhood are harder to address; this situation might entail selectively removing some connector-promoter pairs that are causing the damage.",
                "Handling users with bad scouts and bad neighborhoods is a more difficult challenge.",
                "Such a classification allows the use of different strategies to better a users experience with the system depending on his context.",
                "In future work, we intend to conduct field studies and study the improvement in performance of different strategies for different contexts. 6.",
                "CONCLUSIONS To further <br>recommender</br> system acceptance and deployment, we require new tools and methodologies to manage an installed <br>recommender</br> and develop insights into the roles played by ratings.",
                "A fine-grained characterization in terms of rating roles such as scouts, promoters, and connectors, as done here, helps such an endeavor.",
                "Although we have presented results on only the item-based algorithm with list rank accuracy as the metric, the same approach outlined here applies to user-based algorithms and other metrics.",
                "In future research, we plan to systematically study the many algorithmic parameters, tolerances, and cutoff thresholds employed here and reason about their effects on the downstream conclusions.",
                "We also aim to extend our formulation to other collaborative filtering algorithms, study the effect of shilling in altering rating roles, conduct field studies, and evaluate improvements in user experience by tweaking ratings based on their role values.",
                "Finally, we plan to develop the idea of mining the evolution of rating role patterns into a reporting and tracking system for all aspects of <br>recommender</br> system health. 7.",
                "REFERENCES [1] Cosley, D., Lam, S., Albert, I., Konstan, J., and Riedl, J.",
                "Is Seeing Believing? : How <br>recommender</br> System Interfaces Affect Users Opinions.",
                "In Proc.",
                "CHI (2001), pp. 585-592. [2] Herlocker, J. L., Konstan, J.",
                "A., Borchers, A., and Riedl, J.",
                "An Algorithmic Framework for Performing Collaborative Filtering.",
                "In Proc.",
                "SIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J.",
                "A., Terveen, L. G., and Riedl, J. T. Evaluating Collaborative Filtering <br>recommender</br> Systems.",
                "ACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J.",
                "A.",
                "Personal communication. 2003. [5] Lam, S. K., and Riedl, J. Shilling <br>recommender</br> Systems for Fun and Profit.",
                "In Proceedings of the 13th International World Wide Web Conference (2004), ACM Press, pp. 393-402. [6] Laxman, S., Sastry, P. S., and Unnikrishnan, K. P. Discovering Frequent Episodes and Learning Hidden Markov Models: A Formal Connection.",
                "IEEE Transactions on Knowledge and Data Engineering Vol. 17, 11 (2005), 1505-1517. [7] McLaughlin, M. R., and Herlocker, J. L. A Collaborative Filtering Algorithm and Evaluation Metric that Accurately Model the User Experience.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (2004), pp. 329 - 336. [8] OMahony, M., Hurley, N. J., Kushmerick, N., and Silvestre, G. Collaborative Recommendation: A Robustness Analysis.",
                "ACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J.",
                "A., and Riedl, J.",
                "Getting to Know You: Learning New User Preferences in <br>recommender</br> Systems.",
                "In Proceedings of the 2002 Conference on Intelligent User Interfaces (IUI 2002) (2002), pp. 127-134. [10] Rashid, A. M., Karypis, G., and Riedl, J.",
                "Influence in Ratings-Based <br>recommender</br> Systems: An Algorithm-Independent Approach.",
                "In Proc. of the SIAM International Conference on Data Mining (2005). [11] Resnick, P., Iacovou, N., Sushak, M., Bergstrom, P., and Riedl, J. GroupLens: An Open Architecture for Collaborative Filtering of Netnews.",
                "In Proceedings of the Conference on Computer Supported Collaborative Work (CSCW94) (1994), ACM Press, pp. 175-186. [12] Sarwar, B., Karypis, G., Konstan, J., and Reidl, J. Item-Based Collaborative Filtering Recommendation Algorithms.",
                "In Proceedings of the Tenth International World Wide Web Conference (WWW10) (2001), pp. 285-295. [13] Schein, A., Popescu, A., Ungar, L., and Pennock, D. Methods and Metrics for Cold-Start Recommendation.",
                "In Proc.",
                "SIGIR (2002), pp. 253-260. 259"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "del Instituto Indio de Ciencias de CSA Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Departamento de Ciencias de la Computación del Este de la Universidad de Michigan Ypsilanti, MI 48917, EE. UU.Virginia Tech, Blacksburg VA 24061, EE. UU. Naren@cs.vt.edu Resumen Los sistemas \"Recomendar\" los sistemas agregan las calificaciones individuales de los usuarios en predicciones de productos o servicios que podrían interesar a los visitantes.",
                "Presentamos un estudio novedoso que desagrega las métricas de rendimiento de \"recomendador\" global en las contribuciones hechas por cada calificación individual, lo que nos permite caracterizar los muchos roles desempeñados por las calificaciones en el filtrado colaborativo más cercano de Newbor.",
                "Argumentamos que los tres roles de calificación presentados aquí proporcionan primitivas amplias para administrar un sistema de \"recomendación\" y su comunidad.",
                "Introducción Los sistemas \"Recomendador\" se han vuelto integrales para el comercio electrónico, proporcionando tecnología que sugiere productos a un visitante basado en compras anteriores o historial de calificación.",
                "Sin embargo, con la excepción de algunos estudios sobre la influencia de los usuarios [10], se ha prestado poca atención al desentrañar el funcionamiento interno de un \"recomendador\" en términos de las calificaciones individuales y los roles que juegan al hacer (buenas) recomendaciones.",
                "Tal comprensión dará un mango importante para monitorear y administrar un sistema de \"recomendador\", para diseñar mecanismos para mantener el \"recomendador\" y, por lo tanto, garantizar su éxito continuo.",
                "Nuestra motivación aquí es desagregarse las métricas de desempeño global de \"recomendación\" en las contribuciones hechas por cada calificación individual, lo que nos permite caracterizar los muchos roles desempeñados por las calificaciones en el filtrado colaborativo más cercano al vecino.",
                "Elementos de orientación: los sistemas \"recomendantes\" sufren de falta de participación del usuario, especialmente en escenarios de arranque en frío [13] que involucran artículos recién llegados.",
                "Identificar a los usuarios que pueden ser alentados a calificar elementos específicos ayuda a garantizar la cobertura del sistema de \"recomendación\".3.",
                "Monitoreo de la evolución del sistema \"recomendador\" y sus partes interesadas: un sistema de \"recomendación\" está constantemente bajo cambio: crecer con nuevos usuarios y 250 elementos, reducirse con los usuarios que abandonan el sistema, los elementos se vuelven irrelevantes y partes del sistema bajo ataque."
            ],
            "translated_text": "",
            "candidates": [
                "recomendante",
                "Recomendar",
                "recomendante",
                "recomendador",
                "recomendante",
                "recomendación",
                "recomendante",
                "Recomendador",
                "recomendante",
                "recomendador",
                "recomendante",
                "recomendador",
                "recomendador",
                "recomendante",
                "recomendación",
                "recomendante",
                "recomendantes",
                "Recomendar",
                "recomendación",
                "recomendante",
                "recomendador",
                "recomendación"
            ],
            "error": []
        },
        "rating": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Scouts, Promoters, and Connectors: The Roles of Ratings in Nearest Neighbor Collaborative Filtering Bharath Kumar Mohan Dept.",
                "of CSA Indian Institute of Science Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. of Computer Science Eastern Michigan University Ypsilanti, MI 48917, USA bkeller@emich.edu Naren Ramakrishnan Dept. of Computer Science Virginia Tech, Blacksburg VA 24061, USA naren@cs.vt.edu ABSTRACT Recommender systems aggregate individual user ratings into predictions of products or services that might interest visitors.",
                "The quality of this aggregation process crucially affects the user experience and hence the effectiveness of recommenders in e-commerce.",
                "We present a novel study that disaggregates global recommender performance metrics into contributions made by each individual <br>rating</br>, allowing us to characterize the many roles played by ratings in nearestneighbor collaborative filtering.",
                "In particular, we formulate three roles-scouts, promoters, and connectors-that capture how users receive recommendations, how items get recommended, and how ratings of these two types are themselves connected (resp.).",
                "These roles find direct uses in improving recommendations for users, in better targeting of items and, most importantly, in helping monitor the health of the system as a whole.",
                "For instance, they can be used to track the evolution of neighborhoods, to identify <br>rating</br> subspaces that do not contribute (or contribute negatively) to system performance, to enumerate users who are in danger of leaving, and to assess the susceptibility of the system to attacks such as shilling.",
                "We argue that the three <br>rating</br> roles presented here provide broad primitives to manage a recommender system and its community.",
                "Categories and Subject Descriptors H.4.2 [Information Systems Applications]: Types of Systems-Decision support; J.4 [Computer Applications]: Social and Behavioral Sciences General Terms Algorithms, Human Factors 1.",
                "INTRODUCTION Recommender systems have become integral to e-commerce, providing technology that suggests products to a visitor based on previous purchases or <br>rating</br> history.",
                "Collaborative filtering, a common form of recommendation, predicts a users <br>rating</br> for an item by combining (other) ratings of that user with other users ratings.",
                "Significant research has been conducted in implementing fast and accurate collaborative filtering algorithms [2, 7], designing interfaces for presenting recommendations to users [1], and studying the robustness of these algorithms [8].",
                "However, with the exception of a few studies on the influence of users [10], little attention has been paid to unraveling the inner workings of a recommender in terms of the individual ratings and the roles they play in making (good) recommendations.",
                "Such an understanding will give an important handle to monitoring and managing a recommender system, to engineer mechanisms to sustain the recommender, and thereby ensure its continued success.",
                "Our motivation here is to disaggregate global recommender performance metrics into contributions made by each individual <br>rating</br>, allowing us to characterize the many roles played by ratings in nearest-neighbor collaborative filtering.",
                "We identify three possible roles: (scouts) to connect the user into the system to receive recommendations, (promoters) to connect an item into the system to be recommended, and (connectors) to connect ratings of these two kinds.",
                "Viewing ratings in this way, we can define the contribution of a <br>rating</br> in each role, both in terms of allowing recommendations to occur, and in terms of influence on the quality of recommendations.",
                "In turn, this capability helps support scenarios such as: 1.",
                "Situating users in better neighborhoods: A users ratings may inadvertently connect the user to a neighborhood for which the users tastes may not be a perfect match.",
                "Identifying ratings responsible for such bad recommendations and suggesting new items to rate can help situate the user in a better neighborhood. 2.",
                "Targeting items: Recommender systems suffer from lack of user participation, especially in cold-start scenarios [13] involving newly arrived items.",
                "Identifying users who can be encouraged to rate specific items helps ensure coverage of the recommender system. 3.",
                "Monitoring the evolution of the recommender system and its stakeholders: A recommender system is constantly under change: growing with new users and 250 items, shrinking with users leaving the system, items becoming irrelevant, and parts of the system under attack.",
                "Tracking the roles of a <br>rating</br> and its evolution over time provides many insights into the health of the system, and how it could be managed and improved.",
                "These include being able to identify <br>rating</br> subspaces that do not contribute (or contribute negatively) to system performance, and could be removed; to enumerate users who are in danger of leaving, or have left the system; and to assess the susceptibility of the system to attacks such as shilling [5].",
                "As we show, the characterization of <br>rating</br> roles presented here provides broad primitives to manage a recommender system and its community.",
                "The rest of the paper is organized as follows.",
                "Background on nearest-neighbor collaborative filtering and algorithm evaluation is discussed in Section 2.",
                "Section 3 defines and discusses the roles of a <br>rating</br>, and Section 4 defines measures of the contribution of a <br>rating</br> in each of these roles.",
                "In Section 5, we illustrate the use of these roles to address the goals outlined above. 2.",
                "BACKGROUND 2.1 Algorithms Nearest-neighbor collaborative filtering algorithms either use neighborhoods of users or neighborhoods of items to compute a prediction.",
                "An algorithm of the first kind is called user-based, and one of the second kind is called itembased [12].",
                "In both families of algorithms, neighborhoods are formed by first computing the similarity between all pairs of users (for user-based) or items (for item-based).",
                "Predictions are then computed by aggregating ratings, which in a user-based algorithm involves aggregating the ratings of the target item by the users neighbors and, in an item-based algorithm, involves aggregating the users ratings of items that are neighbors of the target item.",
                "Algorithms within these families differ in the definition of similarity, formation of neighborhoods, and the computation of predictions.",
                "We consider a user-based algorithm based on that defined for GroupLens [11] with variations from Herlocker et al. [2], and an item-based algorithm similar to that of Sarwar et al. [12].",
                "The algorithm used by Resnick et al. [11] defines the similarity of two users u and v as the Pearson correlation of their common ratings: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2 , where Iu is the set of items rated by user u, ru,i is user us <br>rating</br> for item i, and ¯ru is the average <br>rating</br> of user u (similarly for v).",
                "Similarity computed in this manner is typically scaled by a factor proportional to the number of common ratings, to reduce the chance of making a recommendation made on weak connections: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), where γ ≈ 5 is a constant used as a lower limit in scaling [2].",
                "These new similarities are then used to define a static neighborhood Nu for each user u consisting of the top K users most similar to user u.",
                "A prediction for user u and item i is computed by a weighted average of the ratings by the neighbors pu,i = ¯ru + P v∈V sim (u, v)(rv,i − ¯rv) P v∈V sim (u, v) (1) where V = Nu ∩ Ui is the set of users most similar to u who have rated i.",
                "The item-based algorithm we use is the one defined by Sarwar et al. [12].",
                "In this algorithm, similarity is defined as the adjusted cosine measure sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) where Ui is the set of users who have rated item i.",
                "As for the user-based algorithm, the similarity weights are adjusted proportionally to the number of users that have rated the items in common sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). (3) Given the similarities, the neighborhood Ni of an item i is defined as the top K most similar items for that item.",
                "A prediction for user u and item i is computed as the weighted average pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) where J = Ni ∩ Iu is the set of items rated by u that are most similar to i. 2.2 Evaluation Recommender algorithms have typically been evaluated using measures of predictive accuracy and coverage [3].",
                "Studies on recommender algorithms, notably Herlocker et al. [2] and Sarwar et al. [12], typically compute predictive accuracy by dividing a set of ratings into training and test sets, and compute the prediction for an item in the test set using the ratings in the training set.",
                "A standard measure of predictive accuracy is mean absolute error (MAE), which for a test set T = {(u, i)} is defined as, MAE = P (u,i)∈T |pu,i − ru,i| |T | . (5) Coverage has a number of definitions, but generally refers to the proportion of items that can be predicted by the algorithm [3].",
                "A practical issue with predictive accuracy is that users typically are presented with recommendation lists, and not individual numeric predictions.",
                "Recommendation lists are lists of items in decreasing order of prediction (sometimes stated in terms of star-ratings), and so predictive accuracy may not be reflective of the accuracy of the list.",
                "So, instead we can measure recommendation or rank accuracy, which indicates the extent to which the list is in the correct order.",
                "Herlocker et al. [3] discuss a number of rank accuracy measures, which range from Kendalls Tau to measures that consider the fact that users tend to only look at a prefix of the list [5].",
                "Kendalls Tau measures the number of inversions when comparing ordered pairs in the true user ordering of 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figure 1: Ratings in simple movie recommender. items and the recommended order, and is defined as τ = C − D p (C + D + TR)(C + D + TP) (6) where C is the number of pairs that the system predicts in the correct order, D the number of pairs the system predicts in the wrong order, TR the number of pairs in the true ordering that have the same ratings, and TP is the number of pairs in the predicted ordering that have the same ratings [3].",
                "A shortcoming of the Tau metric is that it is oblivious to the position in the ordered list where the inversion occurs [3].",
                "For instance, an inversion toward the end of the list is given the same weight as one in the beginning.",
                "One solution is to consider inversions only in the top few items in the recommended list or to weight inversions based on their position in the list. 3.",
                "ROLES OF A <br>rating</br> Our basic observation is that each <br>rating</br> plays a different role in each prediction in which it is used.",
                "Consider a simplified movie recommender system with three users Jim, Jeff, and Tom and their ratings for a few movies, as shown in Fig. 1. (For this initial discussion we will not consider the <br>rating</br> values involved.)",
                "The recommender predicts whether Tom will like The Mask using the other already available ratings.",
                "How this is done depends on the algorithm: 1.",
                "An item-based collaborative filtering algorithm constructs a neighborhood of movies around The Mask by using the ratings of users who rated The Mask and other movies similarly (e.g., Jims ratings of The Matrix and The Mask; and Jeffs ratings of Star Wars and The Mask).",
                "Toms ratings of those movies are then used to make a prediction for The Mask. 2.",
                "A user-based collaborative filtering algorithm would construct a neighborhood around Tom by tracking other users whose <br>rating</br> behaviors are similar to Toms (e.g., Tom and Jeff have rated Star Wars; Tom and Jim have rated The Matrix).",
                "The prediction of Toms <br>rating</br> for The Mask is then based on the ratings of Jeff and Tim.",
                "Although the nearest-neighbor algorithms aggregate the ratings to form neighborhoods used to compute predictions, we can disaggregate the similarities to view the computation of a prediction as simultaneously following parallel paths of ratings.",
                "So, irrespective of the collaborative filtering algorithm used, we can visualize the prediction of Toms <br>rating</br> of The Mask as walking through a sequence of ratings.",
                "In Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figure 2: Ratings used to predict The Mask for Tom.",
                "Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figure 3: Prediction of The Mask for Tom in which a <br>rating</br> is used more than once. this example, two paths were used for this prediction as depicted in Fig. 2: (p1, p2, p3) and (q1, q2, q3).",
                "Note that these paths are undirected, and are all of length 3.",
                "Only the order in which the ratings are traversed is different between the item-based algorithm (e.g., (p3, p2, p1), (q3, q2, q1)) and the user-based algorithm (e.g., (p1, p2, p3), (q1, q2, q3).)",
                "A <br>rating</br> can be part of many paths for a single prediction as shown in Fig. 3, where three paths are used for a prediction, two of which follow p1: (p1, p2, p3) and (p1, r2, r3).",
                "Predictions in a collaborative filtering algorithms may involve thousands of such walks in parallel, each playing a part in influencing the predicted value.",
                "Each prediction path consists of three ratings, playing roles that we call scouts, promoters, and connectors.",
                "To illustrate these roles, consider the path (p1, p2, p3) in Fig. 2 used to make a prediction of The Mask for Tom: 1.",
                "The <br>rating</br> p1 (Tom → Star Wars) makes a connection from Tom to other ratings that can be used to predict Toms <br>rating</br> for The Mask.",
                "This <br>rating</br> serves as a scout in the bipartite graph of ratings to find a path that leads to The Mask. 2.",
                "The <br>rating</br> p2 (Jeff → Star Wars) helps the system recommend The Mask to Tom by connecting the scout to the promoter. 3.",
                "The <br>rating</br> p3 (Jeff → The Mask) allows connections to The Mask, and, therefore, promotes this movie to Tom.",
                "Formally, given a prediction pu,a of a target item a for user u, a scout for pu,a is a <br>rating</br> ru,i such that there exists a user v with ratings rv,a and rv,i for some item i; a promoter for pu,a is a <br>rating</br> rv,a for some user v, such that there exist ratings rv,i and ru,i for an item i, and; a connector for pu,a 252 Jim Tom Jeff Jerry My Cousin Vinny The Matrix Star Wars The Mask Jurasic Park Figure 4: Scouts, promoters, and connectors. is a rating rv,i by some user v and rating i, such that there exists ratings ru,i and rv,a.",
                "The scouts, connectors, and promoters for the prediction of Toms <br>rating</br> of The Mask are p1 and q1, p2 and q2, and p3 and q3 (respectively).",
                "Each of these roles has a value in the recommender to the user, the users neighborhood, and the system in terms of allowing recommendations to be made. 3.1 Roles in Detail Ratings that act as scouts tend to help the recommender system suggest more movies to the user, though the extent to which this is true depends on the <br>rating</br> behavior of other users.",
                "For example, in Fig. 4 the <br>rating</br> Tom → Star Wars helps the system recommend only The Mask to him, while Tom → The Matrix helps recommend The Mask, Jurassic Park, and My Cousin Vinny.",
                "Tom makes a connection to Jim who is a prolific user of the system, by <br>rating</br> The Matrix.",
                "However, this does not make The Matrix the best movie to rate for everyone.",
                "For example, Jim is benefited equally by both The Mask and The Matrix, which allow the system to recommend Star Wars to him.",
                "His <br>rating</br> of The Mask is the best scout for Jeff, and Jerrys only scout is his <br>rating</br> of Star Wars.",
                "This suggests that good scouts allow a user to build similarity with prolific users, and thereby ensure they get more from the system.",
                "While scouts represent beneficial ratings from the perspective of a user, promoters are their duals, and are of benefit to items.",
                "In Fig. 4, My Cousin Vinny benefits from Jims <br>rating</br>, since it allows recommendations to Jeff and Tom.",
                "The Mask is not so dependent on just one <br>rating</br>, since the ratings by Jim and Jeff help it.",
                "On the other hand, Jerrys <br>rating</br> of Star Wars does not help promote it to any other user.",
                "We conclude that a good promoter connects an item to a broader neighborhood of other items, and thereby ensures that it is recommended to more users.",
                "Connectors serve a crucial role in a recommender system that is not as obvious.",
                "The movies My Cousin Vinny and Jurassic Park have the highest recommendation potential since they can be recommended to Jeff, Jerry and Tom based on the linkage structure illustrated in Fig. 4.",
                "Beside the fact that Jim rated these movies, these recommendations are possible only because of the ratings Jim → The Matrix and Jim → The Mask, which are the best connectors.",
                "A connector improves the systems ability to make recommendations with no explicit gain for the user.",
                "Note that every <br>rating</br> can be of varied benefit in each of these roles.",
                "The <br>rating</br> Jim → My Cousin Vinny is a poor scout and connector, but is a very good promoter.",
                "The <br>rating</br> Jim → The Mask is a reasonably good scout, a very good connector, and a good promoter.",
                "Finally, the <br>rating</br> Jerry → Star Wars is a very good scout, but is of no value as a connector or promoter.",
                "As illustrated here, a <br>rating</br> can have different value in each of the three roles in terms of whether a recommendation can be made or not.",
                "We could measure this value by simply counting the number of times a <br>rating</br> is used in each role, which alone would be helpful in characterizing the behavior of a system.",
                "But we can also measure the contribution of each <br>rating</br> to the quality of recommendations or health of the system.",
                "Since every prediction is a combined effort of several recommendation paths, we are interested in discerning the influence of each <br>rating</br> (and, hence, each path) in the system towards the systems overall error.",
                "We can understand the dynamics of the system at a finer granularity by tracking the influence of a <br>rating</br> according to the role played.",
                "The next section describes the approach to measuring the values of a <br>rating</br> in each role. 4.",
                "CONTRIBUTIONS OF RATINGS As weve seen, a <br>rating</br> may play different roles in different predictions and, in each prediction, contribute to the quality of a prediction in different ways.",
                "Our approach can use any numeric measure of a property of system health, and assigns credit (or blame) to each <br>rating</br> proportional to its influence in the prediction.",
                "By tracking the role of each <br>rating</br> in a prediction, we can accumulate the credit for a <br>rating</br> in each of the three roles, and also track the evolution of the roles of rating over time in the system.",
                "This section defines the methodology for computing the contribution of ratings by first defining the influence of a <br>rating</br>, and then instantiating the approach for predictive accuracy, and then rank accuracy.",
                "We also demonstrate how these contributions can be aggregated to study the neighborhood of ratings involved in computing a users recommendations.",
                "Note that although our general formulation for <br>rating</br> influence is algorithm independent, due to space considerations, we present the approach for only item-based collaborative filtering.",
                "The definition for user-based algorithms is similar and will be presented in an expanded version of this paper. 4.1 Influence of Ratings Recall that an item-based approach to collaborative filtering relies on building item neighborhoods using the similarity of ratings by the same user.",
                "As described earlier, similarity is defined by the adjusted cosine measure (Equations (2) and (3)).",
                "A set of the top K neighbors is maintained for all items for space and computational efficiency.",
                "A prediction of item i for a user u is computed as the weighted deviation from the items mean <br>rating</br> as shown in Equation (4).",
                "The list of recommendations for a user is then the list of items sorted in descending order of their predicted values.",
                "We first define impact(a, i, j), the impact a user a has in determining the similarity between two items i and j.",
                "This is the change in the similarity between i and j when as <br>rating</br> is removed, and is defined as impact(a, i, j) = |sim (i, j) − sim¯a(i, j)| P w∈Cij |sim (i, j) − sim ¯w(i, j)| where Cij = {u ∈ U | ∃ ru,i, ru,j ∈ R(u)} is the set of coraters 253 of items i and j (users who rate both i and j), R(u) is the set of ratings provided by user u, and sim¯a(i, j) is the similarity of i and j when the ratings of user a are removed sim¯a(i, j) = P v∈U\\{a} (ru,i − ¯ru)(ru,j − ¯ru) qP u∈U\\{a}(ru,i − ¯ru)2 qP u∈U\\{a}(ru,j − ¯ru)2 , and adjusted for the number of raters sim¯a(i, j) = max(|Ui ∩ Uj| − 1, γ) γ · sim(i, j).",
                "If all coraters of i and j rate them identically, we define the impact as impact(a, i, j) = 1 |Cij| since P w∈Cij |sim (i, j) − sim ¯w(i, j)| = 0.",
                "The influence of each path (u, j, v, i) = [ru,j, rv,j, rv,i] in the prediction of pu,i is given by influence(u, j, v, i) = sim (i, j) P l∈Ni∩Iu sim (i, l) · impact(v, i, j) It follows that the sum of influences over all such paths, for a given set of endpoints, is 1. 4.2 Role Values for Predictive Accuracy The value of a <br>rating</br> in each role is computed from the influence depending on the evaluation measure employed.",
                "Here we illustrate the approach using predictive accuracy as the evaluation metric.",
                "In general, the goodness of a prediction decides whether the ratings involved must be credited or discredited for their role.",
                "For predictive accuracy, the error in prediction e = |pu,i − ru,i| is mapped to a comfort level using a mapping function M(e).",
                "Anecdotal evidence suggests that users are unable to discern errors less than 1.0 (for a <br>rating</br> scale of 1 to 5) [4], and so an error less than 1.0 is considered acceptable, but anything larger is not.",
                "We hence define M(e) as (1 − e) binned to an appropriate value in [−1, −0.5, 0.5, 1].",
                "For each prediction pu,i, M(e) is attributed to all the paths that assisted the computation of pu,i, proportional to their influences.",
                "This tribute, M(e)∗influence(u, j, v, i), is in turn inherited by each of the ratings in the path [ru,j, rv,j, rv,i], with the credit/blame accumulating to the respective roles of ru,j as a scout, rv,j as a connector, and rv,i as a promoter.",
                "In other words, the scout value SF(ru,j), the connector value CF(rv,j) and the promoter value PF(rv,i) are all incremented by the tribute amount.",
                "Over a large number of predictions, scouts that have repeatedly resulted in big error rates have a big negative scout value, and vice versa (similarly with the other roles).",
                "Every <br>rating</br> is thus summarized by its triple [SF, CF, PF]. 4.3 Role Values for Rank Accuracy We now define the computation of the contribution of ratings to observed rank accuracy.",
                "For this computation, we must know the users preference order for a set of items for which predictions can be computed.",
                "We assume that we have a test set of the users ratings of the items presented in the recommendation list.",
                "For every pair of items rated by a user in the test data, we check whether the predicted order is concordant with his preference.",
                "We say a pair (i, j) is concordant (with error ) whenever one of the following holds: • if (ru,i < ru,j) then (pu,i − pu,j < ); • if (ru,i > ru,j) then (pu,i − pu,j > ); or • if (ru,i = ru,j) then (|pu,i − pu,j| ≤ ).",
                "Similarly, a pair (i, j) is discordant (with error ) if it is not concordant.",
                "Our experiments described below use an error tolerance of = 0.1.",
                "All paths involved in the prediction of the two items in a concordant pair are credited, and the paths involved in a discordant pair are discredited.",
                "The credit assigned to a pair of items (i, j) in the recommendation list for user u is computed as c(i, j) = ( t T · 1 C+D if (i, j) are concordant − t T · 1 C+D if (i, j) are discordant (7) where t is the number of items in the users test set whose ratings could be predicted, T is the number of items rated by user u in the test set, C is the number of concordances and D is the number of discordances.",
                "The credit c is then divided among all paths responsible for predicting pu,i and pu,j proportional to their influences.",
                "We again add the role values obtained from all the experiments to form a triple [SF, CF, PF] for each <br>rating</br>. 4.4 Aggregating <br>rating</br> roles After calculating the role values for individual ratings, we can also use these values to study neighborhoods and the system.",
                "Here we consider how we can use the role values to characterize the health of a neighborhood.",
                "Consider the list of top recommendations presented to a user at a specific point in time.",
                "The collaborative filtering algorithm traversed many paths in his neighborhood through his scouts and other connectors and promoters to make these recommendations.",
                "We call these ratings the recommender neighborhood of the user.",
                "The user implicitly chooses this neighborhood of ratings through the items he rates.",
                "Apart from the collaborative filtering algorithm, the health of this neighborhood completely influences a users satisfaction with the system.",
                "We can characterize a users recommender neighborhood by aggregating the individual role values of the ratings involved, weighted by the influence of individual ratings in determining his recommended list.",
                "Different sections of the users neighborhood wield varied influence on his recommendation list.",
                "For instance, ratings reachable through highly rated items have a bigger say in the recommended items.",
                "Our aim is to study the system and classify users with respect to their positioning in a healthy or unhealthy neighborhood.",
                "A user can have a good set of scouts, but may be exposed to a neighborhood with bad connectors and promoters.",
                "He can have a good neighborhood, but his bad scouts may ensure the neighborhoods potential is rendered useless.",
                "We expect that users with good scouts and good neighborhoods will be most satisfied with the system in the future.",
                "A users neighborhood is characterized by a triple that represents the weighted sum of the role values of individual ratings involved in making recommendations.",
                "Consider a user u and his ordered list of recommendations L. An item i 254 in the list is weighted inversely, as K(i), depending on its position in the list.",
                "In our studies we use K(i) = p position(i).",
                "Several paths of ratings [ru,j, rv,j, rv,i] are involved in predicting pu,i which ultimately decides its position in L, each with influence(u, j, v, i).",
                "The recommender neighborhood of a user u is characterized by the triple, [SFN(u), CFN(u), PFN(u)] where SFN(u) = X i∈L P [ru,j ,rv,j ,rv,i] SF(ru,j)influence(u, j, v, i) K(i) !",
                "CFN(u) and PFN(u) are defined similarly.",
                "This triple estimates the quality of us recommendations based on the past track record of the ratings involved in their respective roles. 5.",
                "EXPERIMENTATION As we have seen, we can assign role values to each <br>rating</br> when evaluating a collaborative filtering system.",
                "In this section, we demonstrate the use of this approach to our overall goal of defining an approach to monitor and manage the health of a recommender system through experiments done on the MovieLens million <br>rating</br> dataset.",
                "In particular, we discuss results relating to identifying good scouts, promoters, and connectors; the evolution of <br>rating</br> roles; and the characterization of user neighborhoods. 5.1 Methodology Our experiments use the MovieLens million <br>rating</br> dataset, which consists of ratings by 6040 users of 3952 movies.",
                "The ratings are in the range 1 to 5, and are labeled with the time the <br>rating</br> was given.",
                "As discussed before, we consider only the item-based algorithm here (with item neighborhoods of size 30) and, due to space considerations, only present role value results for rank accuracy.",
                "Since we are interested in the evolution of the <br>rating</br> role values over time, the model of the recommender system is built by processing ratings in their arrival order.",
                "The timestamping provided by MovieLens is hence crucial for the analyses presented here.",
                "We make assessments of <br>rating</br> roles at intervals of 10,000 ratings and processed the first 200,000 ratings in the dataset (giving rise to 20 snapshots).",
                "We incrementally update the role values as the time ordered ratings are merged into the model.",
                "To keep the experiment computationally manageable, we define a test dataset for each user.",
                "As the time ordered ratings are merged into the model, we label a small randomly selected percentage (20%) as test data.",
                "At discrete epochs, i.e., after processing every 10,000 ratings, we compute the predictions for the ratings in the test data, and then compute the role values for the ratings used in the predictions.",
                "One potential criticism of this methodology is that the ratings in the test set are never evaluated for their roles.",
                "We overcome this concern by repeating the experiment, using different random seeds.",
                "The probability that every <br>rating</br> is considered for evaluation is then considerably high: 1 − 0.2n , where n is the number of times the experiment is repeated with different random seeds.",
                "The results here are based on n = 4 repetitions.",
                "The item-based collaborative filtering algorithms performance was ordinary with respect to rank accuracy.",
                "Fig. 5 shows a plot of the precision and recall as ratings were merged in time order into the model.",
                "The recall was always high, but the average precision was just about 53%. 0 0.2 0.4 0.6 0.8 1 1.2 10000 30000 50000 70000 90000110000130000150000 Ratings merged into model Value Precision Recall Figure 5: Precision and recall for the item-based collaborative filtering algorithm. 5.2 Inducing good scouts The ratings of a user that serve as scouts are those that allow the user to receive recommendations.",
                "We claim that users with ratings that have respectable scout values will be happier with the system than those with ratings with low scout values.",
                "Note that the item-based algorithm discussed here produces recommendation lists with nearly half of the pairs in the list discordant from the users preference.",
                "Whether all of these discordant pairs are observable by the user is unclear, however, certainly this suggests that there is a need to be able to direct users to items whose ratings would improve the lists.",
                "The distribution of the scout values for most users ratings are Gaussian with mean zero.",
                "Fig. 6 shows the frequency distribution of scout values for a sample user at a given snapshot.",
                "We observe that a large number of ratings never serve as scouts for their users.",
                "A relatable scenario is when Amazons recommender makes suggestions of books or items based on other items that were purchased as gifts.",
                "With simple relevance feedback from the user, such ratings can be isolated as bad scouts and discounted from future predictions.",
                "Removing bad scouts was found to be worthwhile for individual users but the overall performance improvement was only marginal.",
                "An obvious question is whether good scouts can be formed by merely <br>rating</br> popular movies as suggested by Rashid et al. [9].",
                "They show that a mix of popularity and <br>rating</br> entropy identifies the best items to suggest to new users when evaluated using MAE.",
                "Following their intuition, we would expect to see a higher correlation between popularityentropy and good scouts.",
                "We measured the Pearson correlation coefficient between aggregated scout values for a movie with the popularity of a movie (number of times it is rated); and with its popularity*variance measure at different snapshots of the system.",
                "Note that the scout values were initially anti-correlated with popularity (Fig. 7), but became moderately correlated as the system evolved.",
                "Both popularity and popularity*variance performed similarly.",
                "A possible explanation is that there has been insufficient time for the popular movies to accumulate ratings. 255 -10 0 10 20 30 40 50 60 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 Scout Value Frequency Figure 6: Distribution of scout values for a sample user. -0.4 -0.2 0 0.2 0.4 0.6 0.8 30000 60000 90000 120000 150000 180000 Popularity Pop*Var Figure 7: Correlation between aggregated scout value and item popularity (computed at different intervals). -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 30000 60000 90000 120000 150000 180000 Figure 8: Correlation between aggregated promoter value and user prolificity (computed at different intervals).",
                "Table 1: Movies forming the best scouts.",
                "Best Scouts Conf.",
                "Pop.",
                "Being John Malkovich (1999) 1.00 445 Star Wars: Episode IV - A New Hope (1977) 0.92 623 Princess Bride, The (1987) 0.85 477 Sixth Sense, The (1999) 0.85 617 Matrix, The (1999) 0.77 522 Ghostbusters (1984) 0.77 441 Casablanca (1942) 0.77 384 Insider, The (1999) 0.77 235 American Beauty (1999) 0.69 624 Terminator 2: Judgment Day (1991) 0.69 503 Fight Club (1999) 0.69 235 Shawshank Redemption, The (1994) 0.69 445 Run Lola Run (Lola rennt) (1998) 0.69 220 Terminator, The (1984) 0.62 450 Usual Suspects, The (1995) 0.62 326 Aliens (1986) 0.62 385 North by Northwest (1959) 0.62 245 Fugitive, The (1993) 0.62 402 End of Days (1999) 0.62 132 Raiders of the Lost Ark (1981) 0.54 540 Schindlers List (1993) 0.54 453 Back to the Future (1985) 0.54 543 Toy Story (1995) 0.54 419 Alien (1979) 0.54 415 Abyss, The (1989) 0.54 345 2001: A Space Odyssey (1968) 0.54 358 Dogma (1999) 0.54 228 Little Mermaid, The (1989) 0.54 203 Table 2: Movies forming the worst scouts.",
                "Worst scouts Conf.",
                "Pop.",
                "Harold and Maude (1971) 0.46 141 Grifters, The (1990) 0.46 180 Sting, The (1973) 0.38 244 Godfather: Part III, The (1990) 0.38 154 Lawrence of Arabia (1962) 0.38 167 High Noon (1952) 0.38 84 Women on the Verge of a... (1988) 0.38 113 Grapes of Wrath, The (1940) 0.38 115 Duck Soup (1933) 0.38 131 Arsenic and Old Lace (1944) 0.38 138 Midnight Cowboy (1969) 0.38 137 To Kill a Mockingbird (1962) 0.31 195 Four Weddings and a Funeral (1994) 0.31 271 Good, The Bad and The Ugly, The (1966) 0.31 156 Its a Wonderful Life (1946) 0.31 146 Player, The (1992) 0.31 220 Jackie Brown (1997) 0.31 118 Boat, The (Das Boot) (1981) 0.31 210 Manhattan (1979) 0.31 158 Truth About Cats & Dogs, The (1996) 0.31 143 Ghost (1990) 0.31 227 Lone Star (1996) 0.31 125 Big Chill, The (1983) 0.31 184 256 By studying the evolution of scout values, we can identify movies that consistently feature in good scouts over time.",
                "We claim these movies will make viable scouts for other users.",
                "We found the aggregated scout values for all movies in intervals of 10,000 ratings each.",
                "A movie is said to induce a good scout if the movie was in the top 100 of the sorted list, and to induce a bad scout if it was in bottom 100 of the same list.",
                "Movies appearing consistently high over time are expected to remain up there in the future.",
                "The effective confidence in a movie m is Cm = Tm − Bm N (8) where Tm is the number of times it appeared in the top 100, Bm the number of times it appeared in the bottom 100, and N is the number of intervals considered.",
                "Using this measure, the top few movies expected to induce the best scouts are shown in Table 1.",
                "Movies that would be bad scout choices are shown in Table 2 with their associated confidences.",
                "The popularities of the movies are also displayed.",
                "Although more popular movies appear in the list of good scouts, these tables show that a blind choice of scout based on popularity alone can be potentially dangerous.",
                "Interestingly, the best scout-Being John Malkovich-is about a puppeteer who discovers a portal into a movie star, a movie that has been described variously on amazon.com as makes you feel giddy, seriously weird, comedy with depth, silly, strange, and inventive.",
                "Indicating whether someone likes this movie or not goes a long way toward situating the user in a suitable neighborhood, with similar preferences.",
                "On the other hand, several factors may have made a movie a bad scout, like the sharp variance in user preferences in the neighborhood of a movie.",
                "Two users may have the same opinion about Lawrence of Arabia, but they may differ sharply about how they felt about the other movies they saw.",
                "Bad scouts ensue when there is deviation in behavior around a common synchronization point. 5.3 Inducing good promoters Ratings that serve to promote items in a collaborative filtering system are critical to allowing a new item be recommended to users.",
                "So, inducing good promoters is important for cold-start recommendation.",
                "We note that the frequency distribution of promoter values for a sample movies ratings is also Gaussian (similar to Fig. 6).",
                "This indicates that the promotion of a movie is benefited most by the ratings of a few users, and are unaffected by the ratings of most users.",
                "We find a strong correlation between a users number of ratings and his aggregated promoter value.",
                "Fig. 8 depicts the evolution of the Pearson correlation co-efficient between the prolificity of a user (number of ratings) versus his aggregated promoter value.",
                "We expect that conspicuous shills, by recommending wrong movies to users, will be discredited with negative aggregate promoter values and should be identifiable easily.",
                "Given this observation, the obvious rule to follow when introducing a new movie is to have it rated directly by prolific users who posses high aggregated promoter values.",
                "A new movie is thus cast into the neighborhood of many other movies improving its visibility.",
                "Note, though, that a user may have long stopped using the system.",
                "Tracking promoter values consistently allows only the most active recent users to be considered. 5.4 Inducing good connectors Given the way scouts, connectors, and promoters are characterized, it follows that the movies that are part of the best scouts are also part of the best connectors.",
                "Similarly, the users that constitute the best promoters are also part of the best connectors.",
                "Good connectors are induced by ensuring a user with a high promoter value rates a movie with a high scout value.",
                "In our experiments, we find that a ratings longest standing role is often as a connector.",
                "A <br>rating</br> with a poor connector value is often seen due to its user being a bad promoter, or its movie being a bad scout.",
                "Such ratings can be removed from the prediction process to bring marginal improvements to recommendations.",
                "In some selected experiments, we observed that removing a set of badly behaving connectors helped improve the systems overall performance by 1.5%.",
                "The effect was even higher on a few select users who observed an improvement of above 10% in precision without much loss in recall. 5.5 Monitoring the evolution of <br>rating</br> roles One of the more significant contributions of our work is the ability to model the evolution of recommender systems, by studying the changing roles of ratings over time.",
                "The role and value of a <br>rating</br> can change depending on many factors like user behavior, redundancy, shilling effects or properties of the collaborative filtering algorithm used.",
                "Studying the dynamics of <br>rating</br> roles in terms of transitions between good, bad, and negligible values can provide insights into the functioning of the recommender system.",
                "We believe that a continuous visualization of these transitions will improve the ability to manage a recommender system.",
                "We classify different <br>rating</br> states as good, bad, or negligible.",
                "Consider a user who has rated 100 movies in a particular interval, of which 20 are part of the test set.",
                "If a scout has a value greater than 0.005, it indicates that it is uniquely involved in at least 2 concordant predictions, which we will say is good.",
                "Thus, a threshold of 0.005 is chosen to bin a <br>rating</br> as good, bad or negligible in terms of its scout, connector and promoter value.",
                "For instance, a <br>rating</br> r, at time t with role value triple [0.1, 0.001, −0.01] is classified as [scout +, connector 0, promoter −], where + indicates good, 0 indicates negligible, and − indicates bad.",
                "The positive credit held by a <br>rating</br> is a measure of its contribution to the betterment of the system, and the discredit is a measure of its contribution to the detriment of the system.",
                "Even though the positive roles (and the negative roles) make up a very small percentage of all ratings, their contribution supersedes their size.",
                "For example, even though only 1.7% of all ratings were classified as good scouts, they hold 79% of all positive credit in the system!",
                "Similarly, the bad scouts were just 1.4% of all ratings but hold 82% of all discredit.",
                "Note that good and bad scouts, together, comprise only 1.4% + 1.7% = 3.1% of the ratings, implying that the majority of the ratings are negligible role players as scouts (more on this later).",
                "Likewise, good connectors were 1.2% of the system, and hold 30% of all positive credit.",
                "The bad connectors (0.8% of the system) hold 36% of all discredit.",
                "Good promoters (3% of the system) hold 46% of all credit, while bad promoters (2%) hold 50% of all discredit.",
                "This reiterates that a few ratings influence most of the systems performance.",
                "Hence it is important to track transitions between them regardless of their small numbers. 257 Across different snapshots, a <br>rating</br> can remain in the same state or change.",
                "A good scout can become a bad scout, a good promoter can become a good connector, good and bad scouts can become vestigial, and so on.",
                "It is not practical to expect a recommender system to have no ratings in bad roles.",
                "However, it suffices to see ratings in bad roles either convert to good or vestigial roles.",
                "Similarly, observing a large number of good roles become bad ones is a sign of imminent failure of the system.",
                "We employ the principle of non-overlapping episodes [6] to count such transitions.",
                "A sequence such as: [+, 0, 0] → [+, 0, 0] → [0, +, 0] → [0, 0, 0] is interpreted as the transitions [+, 0, 0] ; [0, +, 0] : 1 [+, 0, 0] ; [0, 0, 0] : 1 [0, +, 0] ; [0, 0, 0] : 1 instead of [+, 0, 0] ; [0, +, 0] : 2 [+, 0, 0] ; [0, 0, 0] : 2 [0, +, 0] ; [0, 0, 0] : 1.",
                "See [6] for further details about this counting procedure.",
                "Thus, a <br>rating</br> can be in one of 27 possible states, and there are about 272 possible transitions.",
                "We make a further simplification and utilize only 9 states, indicating whether the <br>rating</br> is a scout, promoter, or connector, and whether it has a positive, negative, or negligible role.",
                "Ratings that serve multiple purposes are counted using multiple episode instantiations but the states themselves are not duplicated beyond the 9 restricted states.",
                "In this model, a transition such as [+, 0, +] ; [0, +, 0] : 1 is counted as [scout+] ; [scout0] : 1 [scout+] ; [connector+] : 1 [scout+] ; [promoter0] : 1 [connector0] ; [scout0] : 1 [connector0] ; [scout+] : 1 [connector0] ; [promoter0] : 1 [promoter+] ; [scout0] : 1 [promoter+] ; [connector+] : 1 [promoter+] ; [promoter0] : 1 Of these, transitions like [pX] ; [q0] where p = q, X ∈ {+, 0, −} are considered uninteresting, and only the rest are counted.",
                "Fig. 9 depicts the major transitions counted while processing the first 200,000 ratings from the MovieLens dataset.",
                "Only transitions with frequency greater than or equal to 3% are shown.",
                "The percentages for each state indicates the number of ratings that were found to be in those states.",
                "We consider transitions from any state to a good state as healthy, from any state to a bad state as unhealthy, and from any state to a vestigial state as decaying.",
                "From Fig. 9, we can observe: • The bulk of the ratings have negligible values, irrespective of their role.",
                "The majority of the transitions involve both good and bad ratings becoming negligible.",
                "Scout + (2%) Scout(1.5%) Scout 0 (96.5%) Connector + (1.2%) Connector(0.8%) Connector 0 (98%) Promoter + (3%) Promoter(2%) Promoter 0 (95%) 84% 84% 81% 74% 10% 6% 11% 77% 8% 7% 8% 82% 4% 86% 4% 68% 15% 13% 5% 5% 77% 11% 7% 5% 4% 3% 3% 3% Healthy Unhealthy Decaying Figure 9: Transitions among <br>rating</br> roles. • The number of good ratings is comparable to the bad ratings, and ratings are seen to switch states often, except in the case of scouts (see below). • The negative and positive scout states are not reachable through any transition, indicating that these ratings must begin as such, and cannot be coerced into these roles. • Good promoters and good connectors have a much longer survival period than scouts.",
                "Transitions that recur to these states have frequencies of 10% and 15% when compared to just 4% for scouts.",
                "Good connectors are the slowest to decay whereas (good) scouts decay the fastest. • Healthy percentages are seen on transitions between promoters and connectors.",
                "As indicated earlier, there are hardly any transitions from promoters/connectors to scouts.",
                "This indicates that, over the long run, a users <br>rating</br> is more useful to others (movies or other users) than to the user himself. • The percentages of healthy transitions outweigh the unhealthy ones - this hints that the system is healthy, albeit only marginally.",
                "Note that these results are conditioned by the static nature of the dataset, which is a set of ratings over a fixed window of time.",
                "However a diagram such as Fig. 9 is clearly useful for monitoring the health of a recommender system.",
                "For instance, acceptable limits can be imposed on different types of transitions and, if a transition fails to meet the threshold, the recommender system or a part of it can be brought under closer scrutiny.",
                "Furthermore, the role state transition diagram would also be the ideal place to study the effects of shilling, a topic we will consider in future research. 5.6 Characterizing neighborhoods Earlier we saw that we can characterize the neighborhood of ratings involved in creating a recommendation list L for 258 a user.",
                "In our experiment, we consider lists of length 30, and sample the lists of about 5% of users through the evolution of the model (at intervals of 10,000 ratings each) and compute their neighborhood characteristics.",
                "To simplify our presentation, we consider the percentage of the sample that fall into one of the following categories: 1.",
                "Inactive user: (SFN(u) = 0) 2.",
                "Good scouts, Good neighborhood: (SFN(u) > 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 3.",
                "Good scouts, Bad neighborhood: (SFN(u) > 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) 4.",
                "Bad scouts, Good neighborhood: (SFN(u) < 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 5.",
                "Bad scouts, Bad neighborhood: (SFN(u) < 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) From our sample set of 561 users, we found that 476 users were inactive.",
                "Of the remaining 85 users, we found 26 users had good scouts and a good neighborhood, 6 had bad scouts and a good neighborhood, 29 had good scouts and a bad neighborhood, and 24 had bad scouts and a bad neighborhood.",
                "Thus, we conjecture that 59 users (29+24+6) are in danger of leaving the system.",
                "As a remedy, users with bad scouts and a good neighborhood can be asked to reconsider <br>rating</br> of some movies hoping to improve the systems recommendations.",
                "The system can be expected to deliver more if they engineer some good scouts.",
                "Users with good scouts and a bad neighborhood are harder to address; this situation might entail selectively removing some connector-promoter pairs that are causing the damage.",
                "Handling users with bad scouts and bad neighborhoods is a more difficult challenge.",
                "Such a classification allows the use of different strategies to better a users experience with the system depending on his context.",
                "In future work, we intend to conduct field studies and study the improvement in performance of different strategies for different contexts. 6.",
                "CONCLUSIONS To further recommender system acceptance and deployment, we require new tools and methodologies to manage an installed recommender and develop insights into the roles played by ratings.",
                "A fine-grained characterization in terms of <br>rating</br> roles such as scouts, promoters, and connectors, as done here, helps such an endeavor.",
                "Although we have presented results on only the item-based algorithm with list rank accuracy as the metric, the same approach outlined here applies to user-based algorithms and other metrics.",
                "In future research, we plan to systematically study the many algorithmic parameters, tolerances, and cutoff thresholds employed here and reason about their effects on the downstream conclusions.",
                "We also aim to extend our formulation to other collaborative filtering algorithms, study the effect of shilling in altering <br>rating</br> roles, conduct field studies, and evaluate improvements in user experience by tweaking ratings based on their role values.",
                "Finally, we plan to develop the idea of mining the evolution of <br>rating</br> role patterns into a reporting and tracking system for all aspects of recommender system health. 7.",
                "REFERENCES [1] Cosley, D., Lam, S., Albert, I., Konstan, J., and Riedl, J.",
                "Is Seeing Believing? : How Recommender System Interfaces Affect Users Opinions.",
                "In Proc.",
                "CHI (2001), pp. 585-592. [2] Herlocker, J. L., Konstan, J.",
                "A., Borchers, A., and Riedl, J.",
                "An Algorithmic Framework for Performing Collaborative Filtering.",
                "In Proc.",
                "SIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J.",
                "A., Terveen, L. G., and Riedl, J. T. Evaluating Collaborative Filtering Recommender Systems.",
                "ACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J.",
                "A.",
                "Personal communication. 2003. [5] Lam, S. K., and Riedl, J. Shilling Recommender Systems for Fun and Profit.",
                "In Proceedings of the 13th International World Wide Web Conference (2004), ACM Press, pp. 393-402. [6] Laxman, S., Sastry, P. S., and Unnikrishnan, K. P. Discovering Frequent Episodes and Learning Hidden Markov Models: A Formal Connection.",
                "IEEE Transactions on Knowledge and Data Engineering Vol. 17, 11 (2005), 1505-1517. [7] McLaughlin, M. R., and Herlocker, J. L. A Collaborative Filtering Algorithm and Evaluation Metric that Accurately Model the User Experience.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (2004), pp. 329 - 336. [8] OMahony, M., Hurley, N. J., Kushmerick, N., and Silvestre, G. Collaborative Recommendation: A Robustness Analysis.",
                "ACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J.",
                "A., and Riedl, J.",
                "Getting to Know You: Learning New User Preferences in Recommender Systems.",
                "In Proceedings of the 2002 Conference on Intelligent User Interfaces (IUI 2002) (2002), pp. 127-134. [10] Rashid, A. M., Karypis, G., and Riedl, J.",
                "Influence in Ratings-Based Recommender Systems: An Algorithm-Independent Approach.",
                "In Proc. of the SIAM International Conference on Data Mining (2005). [11] Resnick, P., Iacovou, N., Sushak, M., Bergstrom, P., and Riedl, J. GroupLens: An Open Architecture for Collaborative Filtering of Netnews.",
                "In Proceedings of the Conference on Computer Supported Collaborative Work (CSCW94) (1994), ACM Press, pp. 175-186. [12] Sarwar, B., Karypis, G., Konstan, J., and Reidl, J. Item-Based Collaborative Filtering Recommendation Algorithms.",
                "In Proceedings of the Tenth International World Wide Web Conference (WWW10) (2001), pp. 285-295. [13] Schein, A., Popescu, A., Ungar, L., and Pennock, D. Methods and Metrics for Cold-Start Recommendation.",
                "In Proc.",
                "SIGIR (2002), pp. 253-260. 259"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Presentamos un estudio novedoso que desagrega las métricas de rendimiento de recomendaciones globales en las contribuciones hechas por cada \"calificación\" individual, lo que nos permite caracterizar los muchos roles desempeñados por las calificaciones en el filtrado colaborativo más cercano de Newbor.",
                "Por ejemplo, se pueden usar para rastrear la evolución de los vecindarios, para identificar subespacios de \"calificación\" que no contribuyen (o contribuyen negativamente) al rendimiento del sistema, a enumerar a los usuarios que están en peligro de irse y evaluar la susceptibilidad de lossistema a ataques como chelines.",
                "Argumentamos que los tres roles de \"calificación\" presentados aquí proporcionan primitivas amplias para administrar un sistema de recomendación y su comunidad.",
                "Introducción Los sistemas de recomendación se han vuelto integrales para el comercio electrónico, proporcionando tecnología que sugiere productos a un visitante basado en compras anteriores o historial de \"calificación\".",
                "El filtrado colaborativo, una forma común de recomendación, predice que los usuarios \"califican\" para un elemento combinando (otras) calificaciones de ese usuario con las calificaciones de otros usuarios.",
                "Nuestra motivación aquí es desglosar métricas de rendimiento de recomendación global en contribuciones hechas por cada \"calificación\" individual, lo que nos permite caracterizar los muchos roles desempeñados por las calificaciones en el filtrado colaborativo más cercano al vecino.",
                "Viendo las calificaciones de esta manera, podemos definir la contribución de una \"calificación\" en cada rol, tanto en términos de permitir que ocurran recomendaciones como en términos de influencia en la calidad de las recomendaciones.",
                "El seguimiento de los roles de una \"calificación\" y su evolución a lo largo del tiempo proporciona muchas ideas sobre la salud del sistema y cómo podría manejarse y mejorarse.",
                "Estos incluyen poder identificar subespacios de \"calificación\" que no contribuyen (o contribuyen negativamente) al rendimiento del sistema, y podrían eliminarse;enumerar a los usuarios que están en peligro de irse o dejar el sistema;y para evaluar la susceptibilidad del sistema a ataques como el chelín [5].",
                "Como mostramos, la caracterización de los roles de \"calificación\" presentados aquí proporciona primitivas amplias para administrar un sistema de recomendación y su comunidad."
            ],
            "translated_text": "",
            "candidates": [
                "clasificación",
                "calificación",
                "clasificación",
                "calificación",
                "clasificación",
                "calificación",
                "clasificación",
                "calificación",
                "clasificación",
                "califican",
                "clasificación",
                "calificación",
                "clasificación",
                "calificación",
                "clasificación",
                "calificación",
                "clasificación",
                "calificación",
                "clasificación",
                "calificación"
            ],
            "error": []
        },
        "purchase": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Scouts, Promoters, and Connectors: The Roles of Ratings in Nearest Neighbor Collaborative Filtering Bharath Kumar Mohan Dept.",
                "of CSA Indian Institute of Science Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. of Computer Science Eastern Michigan University Ypsilanti, MI 48917, USA bkeller@emich.edu Naren Ramakrishnan Dept. of Computer Science Virginia Tech, Blacksburg VA 24061, USA naren@cs.vt.edu ABSTRACT Recommender systems aggregate individual user ratings into predictions of products or services that might interest visitors.",
                "The quality of this aggregation process crucially affects the user experience and hence the effectiveness of recommenders in e-commerce.",
                "We present a novel study that disaggregates global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearestneighbor collaborative filtering.",
                "In particular, we formulate three roles-scouts, promoters, and connectors-that capture how users receive recommendations, how items get recommended, and how ratings of these two types are themselves connected (resp.).",
                "These roles find direct uses in improving recommendations for users, in better targeting of items and, most importantly, in helping monitor the health of the system as a whole.",
                "For instance, they can be used to track the evolution of neighborhoods, to identify rating subspaces that do not contribute (or contribute negatively) to system performance, to enumerate users who are in danger of leaving, and to assess the susceptibility of the system to attacks such as shilling.",
                "We argue that the three rating roles presented here provide broad primitives to manage a recommender system and its community.",
                "Categories and Subject Descriptors H.4.2 [Information Systems Applications]: Types of Systems-Decision support; J.4 [Computer Applications]: Social and Behavioral Sciences General Terms Algorithms, Human Factors 1.",
                "INTRODUCTION Recommender systems have become integral to e-commerce, providing technology that suggests products to a visitor based on previous <br>purchase</br>s or rating history.",
                "Collaborative filtering, a common form of recommendation, predicts a users rating for an item by combining (other) ratings of that user with other users ratings.",
                "Significant research has been conducted in implementing fast and accurate collaborative filtering algorithms [2, 7], designing interfaces for presenting recommendations to users [1], and studying the robustness of these algorithms [8].",
                "However, with the exception of a few studies on the influence of users [10], little attention has been paid to unraveling the inner workings of a recommender in terms of the individual ratings and the roles they play in making (good) recommendations.",
                "Such an understanding will give an important handle to monitoring and managing a recommender system, to engineer mechanisms to sustain the recommender, and thereby ensure its continued success.",
                "Our motivation here is to disaggregate global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearest-neighbor collaborative filtering.",
                "We identify three possible roles: (scouts) to connect the user into the system to receive recommendations, (promoters) to connect an item into the system to be recommended, and (connectors) to connect ratings of these two kinds.",
                "Viewing ratings in this way, we can define the contribution of a rating in each role, both in terms of allowing recommendations to occur, and in terms of influence on the quality of recommendations.",
                "In turn, this capability helps support scenarios such as: 1.",
                "Situating users in better neighborhoods: A users ratings may inadvertently connect the user to a neighborhood for which the users tastes may not be a perfect match.",
                "Identifying ratings responsible for such bad recommendations and suggesting new items to rate can help situate the user in a better neighborhood. 2.",
                "Targeting items: Recommender systems suffer from lack of user participation, especially in cold-start scenarios [13] involving newly arrived items.",
                "Identifying users who can be encouraged to rate specific items helps ensure coverage of the recommender system. 3.",
                "Monitoring the evolution of the recommender system and its stakeholders: A recommender system is constantly under change: growing with new users and 250 items, shrinking with users leaving the system, items becoming irrelevant, and parts of the system under attack.",
                "Tracking the roles of a rating and its evolution over time provides many insights into the health of the system, and how it could be managed and improved.",
                "These include being able to identify rating subspaces that do not contribute (or contribute negatively) to system performance, and could be removed; to enumerate users who are in danger of leaving, or have left the system; and to assess the susceptibility of the system to attacks such as shilling [5].",
                "As we show, the characterization of rating roles presented here provides broad primitives to manage a recommender system and its community.",
                "The rest of the paper is organized as follows.",
                "Background on nearest-neighbor collaborative filtering and algorithm evaluation is discussed in Section 2.",
                "Section 3 defines and discusses the roles of a rating, and Section 4 defines measures of the contribution of a rating in each of these roles.",
                "In Section 5, we illustrate the use of these roles to address the goals outlined above. 2.",
                "BACKGROUND 2.1 Algorithms Nearest-neighbor collaborative filtering algorithms either use neighborhoods of users or neighborhoods of items to compute a prediction.",
                "An algorithm of the first kind is called user-based, and one of the second kind is called itembased [12].",
                "In both families of algorithms, neighborhoods are formed by first computing the similarity between all pairs of users (for user-based) or items (for item-based).",
                "Predictions are then computed by aggregating ratings, which in a user-based algorithm involves aggregating the ratings of the target item by the users neighbors and, in an item-based algorithm, involves aggregating the users ratings of items that are neighbors of the target item.",
                "Algorithms within these families differ in the definition of similarity, formation of neighborhoods, and the computation of predictions.",
                "We consider a user-based algorithm based on that defined for GroupLens [11] with variations from Herlocker et al. [2], and an item-based algorithm similar to that of Sarwar et al. [12].",
                "The algorithm used by Resnick et al. [11] defines the similarity of two users u and v as the Pearson correlation of their common ratings: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2 , where Iu is the set of items rated by user u, ru,i is user us rating for item i, and ¯ru is the average rating of user u (similarly for v).",
                "Similarity computed in this manner is typically scaled by a factor proportional to the number of common ratings, to reduce the chance of making a recommendation made on weak connections: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), where γ ≈ 5 is a constant used as a lower limit in scaling [2].",
                "These new similarities are then used to define a static neighborhood Nu for each user u consisting of the top K users most similar to user u.",
                "A prediction for user u and item i is computed by a weighted average of the ratings by the neighbors pu,i = ¯ru + P v∈V sim (u, v)(rv,i − ¯rv) P v∈V sim (u, v) (1) where V = Nu ∩ Ui is the set of users most similar to u who have rated i.",
                "The item-based algorithm we use is the one defined by Sarwar et al. [12].",
                "In this algorithm, similarity is defined as the adjusted cosine measure sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) where Ui is the set of users who have rated item i.",
                "As for the user-based algorithm, the similarity weights are adjusted proportionally to the number of users that have rated the items in common sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). (3) Given the similarities, the neighborhood Ni of an item i is defined as the top K most similar items for that item.",
                "A prediction for user u and item i is computed as the weighted average pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) where J = Ni ∩ Iu is the set of items rated by u that are most similar to i. 2.2 Evaluation Recommender algorithms have typically been evaluated using measures of predictive accuracy and coverage [3].",
                "Studies on recommender algorithms, notably Herlocker et al. [2] and Sarwar et al. [12], typically compute predictive accuracy by dividing a set of ratings into training and test sets, and compute the prediction for an item in the test set using the ratings in the training set.",
                "A standard measure of predictive accuracy is mean absolute error (MAE), which for a test set T = {(u, i)} is defined as, MAE = P (u,i)∈T |pu,i − ru,i| |T | . (5) Coverage has a number of definitions, but generally refers to the proportion of items that can be predicted by the algorithm [3].",
                "A practical issue with predictive accuracy is that users typically are presented with recommendation lists, and not individual numeric predictions.",
                "Recommendation lists are lists of items in decreasing order of prediction (sometimes stated in terms of star-ratings), and so predictive accuracy may not be reflective of the accuracy of the list.",
                "So, instead we can measure recommendation or rank accuracy, which indicates the extent to which the list is in the correct order.",
                "Herlocker et al. [3] discuss a number of rank accuracy measures, which range from Kendalls Tau to measures that consider the fact that users tend to only look at a prefix of the list [5].",
                "Kendalls Tau measures the number of inversions when comparing ordered pairs in the true user ordering of 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figure 1: Ratings in simple movie recommender. items and the recommended order, and is defined as τ = C − D p (C + D + TR)(C + D + TP) (6) where C is the number of pairs that the system predicts in the correct order, D the number of pairs the system predicts in the wrong order, TR the number of pairs in the true ordering that have the same ratings, and TP is the number of pairs in the predicted ordering that have the same ratings [3].",
                "A shortcoming of the Tau metric is that it is oblivious to the position in the ordered list where the inversion occurs [3].",
                "For instance, an inversion toward the end of the list is given the same weight as one in the beginning.",
                "One solution is to consider inversions only in the top few items in the recommended list or to weight inversions based on their position in the list. 3.",
                "ROLES OF A RATING Our basic observation is that each rating plays a different role in each prediction in which it is used.",
                "Consider a simplified movie recommender system with three users Jim, Jeff, and Tom and their ratings for a few movies, as shown in Fig. 1. (For this initial discussion we will not consider the rating values involved.)",
                "The recommender predicts whether Tom will like The Mask using the other already available ratings.",
                "How this is done depends on the algorithm: 1.",
                "An item-based collaborative filtering algorithm constructs a neighborhood of movies around The Mask by using the ratings of users who rated The Mask and other movies similarly (e.g., Jims ratings of The Matrix and The Mask; and Jeffs ratings of Star Wars and The Mask).",
                "Toms ratings of those movies are then used to make a prediction for The Mask. 2.",
                "A user-based collaborative filtering algorithm would construct a neighborhood around Tom by tracking other users whose rating behaviors are similar to Toms (e.g., Tom and Jeff have rated Star Wars; Tom and Jim have rated The Matrix).",
                "The prediction of Toms rating for The Mask is then based on the ratings of Jeff and Tim.",
                "Although the nearest-neighbor algorithms aggregate the ratings to form neighborhoods used to compute predictions, we can disaggregate the similarities to view the computation of a prediction as simultaneously following parallel paths of ratings.",
                "So, irrespective of the collaborative filtering algorithm used, we can visualize the prediction of Toms rating of The Mask as walking through a sequence of ratings.",
                "In Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figure 2: Ratings used to predict The Mask for Tom.",
                "Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figure 3: Prediction of The Mask for Tom in which a rating is used more than once. this example, two paths were used for this prediction as depicted in Fig. 2: (p1, p2, p3) and (q1, q2, q3).",
                "Note that these paths are undirected, and are all of length 3.",
                "Only the order in which the ratings are traversed is different between the item-based algorithm (e.g., (p3, p2, p1), (q3, q2, q1)) and the user-based algorithm (e.g., (p1, p2, p3), (q1, q2, q3).)",
                "A rating can be part of many paths for a single prediction as shown in Fig. 3, where three paths are used for a prediction, two of which follow p1: (p1, p2, p3) and (p1, r2, r3).",
                "Predictions in a collaborative filtering algorithms may involve thousands of such walks in parallel, each playing a part in influencing the predicted value.",
                "Each prediction path consists of three ratings, playing roles that we call scouts, promoters, and connectors.",
                "To illustrate these roles, consider the path (p1, p2, p3) in Fig. 2 used to make a prediction of The Mask for Tom: 1.",
                "The rating p1 (Tom → Star Wars) makes a connection from Tom to other ratings that can be used to predict Toms rating for The Mask.",
                "This rating serves as a scout in the bipartite graph of ratings to find a path that leads to The Mask. 2.",
                "The rating p2 (Jeff → Star Wars) helps the system recommend The Mask to Tom by connecting the scout to the promoter. 3.",
                "The rating p3 (Jeff → The Mask) allows connections to The Mask, and, therefore, promotes this movie to Tom.",
                "Formally, given a prediction pu,a of a target item a for user u, a scout for pu,a is a rating ru,i such that there exists a user v with ratings rv,a and rv,i for some item i; a promoter for pu,a is a rating rv,a for some user v, such that there exist ratings rv,i and ru,i for an item i, and; a connector for pu,a 252 Jim Tom Jeff Jerry My Cousin Vinny The Matrix Star Wars The Mask Jurasic Park Figure 4: Scouts, promoters, and connectors. is a rating rv,i by some user v and rating i, such that there exists ratings ru,i and rv,a.",
                "The scouts, connectors, and promoters for the prediction of Toms rating of The Mask are p1 and q1, p2 and q2, and p3 and q3 (respectively).",
                "Each of these roles has a value in the recommender to the user, the users neighborhood, and the system in terms of allowing recommendations to be made. 3.1 Roles in Detail Ratings that act as scouts tend to help the recommender system suggest more movies to the user, though the extent to which this is true depends on the rating behavior of other users.",
                "For example, in Fig. 4 the rating Tom → Star Wars helps the system recommend only The Mask to him, while Tom → The Matrix helps recommend The Mask, Jurassic Park, and My Cousin Vinny.",
                "Tom makes a connection to Jim who is a prolific user of the system, by rating The Matrix.",
                "However, this does not make The Matrix the best movie to rate for everyone.",
                "For example, Jim is benefited equally by both The Mask and The Matrix, which allow the system to recommend Star Wars to him.",
                "His rating of The Mask is the best scout for Jeff, and Jerrys only scout is his rating of Star Wars.",
                "This suggests that good scouts allow a user to build similarity with prolific users, and thereby ensure they get more from the system.",
                "While scouts represent beneficial ratings from the perspective of a user, promoters are their duals, and are of benefit to items.",
                "In Fig. 4, My Cousin Vinny benefits from Jims rating, since it allows recommendations to Jeff and Tom.",
                "The Mask is not so dependent on just one rating, since the ratings by Jim and Jeff help it.",
                "On the other hand, Jerrys rating of Star Wars does not help promote it to any other user.",
                "We conclude that a good promoter connects an item to a broader neighborhood of other items, and thereby ensures that it is recommended to more users.",
                "Connectors serve a crucial role in a recommender system that is not as obvious.",
                "The movies My Cousin Vinny and Jurassic Park have the highest recommendation potential since they can be recommended to Jeff, Jerry and Tom based on the linkage structure illustrated in Fig. 4.",
                "Beside the fact that Jim rated these movies, these recommendations are possible only because of the ratings Jim → The Matrix and Jim → The Mask, which are the best connectors.",
                "A connector improves the systems ability to make recommendations with no explicit gain for the user.",
                "Note that every rating can be of varied benefit in each of these roles.",
                "The rating Jim → My Cousin Vinny is a poor scout and connector, but is a very good promoter.",
                "The rating Jim → The Mask is a reasonably good scout, a very good connector, and a good promoter.",
                "Finally, the rating Jerry → Star Wars is a very good scout, but is of no value as a connector or promoter.",
                "As illustrated here, a rating can have different value in each of the three roles in terms of whether a recommendation can be made or not.",
                "We could measure this value by simply counting the number of times a rating is used in each role, which alone would be helpful in characterizing the behavior of a system.",
                "But we can also measure the contribution of each rating to the quality of recommendations or health of the system.",
                "Since every prediction is a combined effort of several recommendation paths, we are interested in discerning the influence of each rating (and, hence, each path) in the system towards the systems overall error.",
                "We can understand the dynamics of the system at a finer granularity by tracking the influence of a rating according to the role played.",
                "The next section describes the approach to measuring the values of a rating in each role. 4.",
                "CONTRIBUTIONS OF RATINGS As weve seen, a rating may play different roles in different predictions and, in each prediction, contribute to the quality of a prediction in different ways.",
                "Our approach can use any numeric measure of a property of system health, and assigns credit (or blame) to each rating proportional to its influence in the prediction.",
                "By tracking the role of each rating in a prediction, we can accumulate the credit for a rating in each of the three roles, and also track the evolution of the roles of rating over time in the system.",
                "This section defines the methodology for computing the contribution of ratings by first defining the influence of a rating, and then instantiating the approach for predictive accuracy, and then rank accuracy.",
                "We also demonstrate how these contributions can be aggregated to study the neighborhood of ratings involved in computing a users recommendations.",
                "Note that although our general formulation for rating influence is algorithm independent, due to space considerations, we present the approach for only item-based collaborative filtering.",
                "The definition for user-based algorithms is similar and will be presented in an expanded version of this paper. 4.1 Influence of Ratings Recall that an item-based approach to collaborative filtering relies on building item neighborhoods using the similarity of ratings by the same user.",
                "As described earlier, similarity is defined by the adjusted cosine measure (Equations (2) and (3)).",
                "A set of the top K neighbors is maintained for all items for space and computational efficiency.",
                "A prediction of item i for a user u is computed as the weighted deviation from the items mean rating as shown in Equation (4).",
                "The list of recommendations for a user is then the list of items sorted in descending order of their predicted values.",
                "We first define impact(a, i, j), the impact a user a has in determining the similarity between two items i and j.",
                "This is the change in the similarity between i and j when as rating is removed, and is defined as impact(a, i, j) = |sim (i, j) − sim¯a(i, j)| P w∈Cij |sim (i, j) − sim ¯w(i, j)| where Cij = {u ∈ U | ∃ ru,i, ru,j ∈ R(u)} is the set of coraters 253 of items i and j (users who rate both i and j), R(u) is the set of ratings provided by user u, and sim¯a(i, j) is the similarity of i and j when the ratings of user a are removed sim¯a(i, j) = P v∈U\\{a} (ru,i − ¯ru)(ru,j − ¯ru) qP u∈U\\{a}(ru,i − ¯ru)2 qP u∈U\\{a}(ru,j − ¯ru)2 , and adjusted for the number of raters sim¯a(i, j) = max(|Ui ∩ Uj| − 1, γ) γ · sim(i, j).",
                "If all coraters of i and j rate them identically, we define the impact as impact(a, i, j) = 1 |Cij| since P w∈Cij |sim (i, j) − sim ¯w(i, j)| = 0.",
                "The influence of each path (u, j, v, i) = [ru,j, rv,j, rv,i] in the prediction of pu,i is given by influence(u, j, v, i) = sim (i, j) P l∈Ni∩Iu sim (i, l) · impact(v, i, j) It follows that the sum of influences over all such paths, for a given set of endpoints, is 1. 4.2 Role Values for Predictive Accuracy The value of a rating in each role is computed from the influence depending on the evaluation measure employed.",
                "Here we illustrate the approach using predictive accuracy as the evaluation metric.",
                "In general, the goodness of a prediction decides whether the ratings involved must be credited or discredited for their role.",
                "For predictive accuracy, the error in prediction e = |pu,i − ru,i| is mapped to a comfort level using a mapping function M(e).",
                "Anecdotal evidence suggests that users are unable to discern errors less than 1.0 (for a rating scale of 1 to 5) [4], and so an error less than 1.0 is considered acceptable, but anything larger is not.",
                "We hence define M(e) as (1 − e) binned to an appropriate value in [−1, −0.5, 0.5, 1].",
                "For each prediction pu,i, M(e) is attributed to all the paths that assisted the computation of pu,i, proportional to their influences.",
                "This tribute, M(e)∗influence(u, j, v, i), is in turn inherited by each of the ratings in the path [ru,j, rv,j, rv,i], with the credit/blame accumulating to the respective roles of ru,j as a scout, rv,j as a connector, and rv,i as a promoter.",
                "In other words, the scout value SF(ru,j), the connector value CF(rv,j) and the promoter value PF(rv,i) are all incremented by the tribute amount.",
                "Over a large number of predictions, scouts that have repeatedly resulted in big error rates have a big negative scout value, and vice versa (similarly with the other roles).",
                "Every rating is thus summarized by its triple [SF, CF, PF]. 4.3 Role Values for Rank Accuracy We now define the computation of the contribution of ratings to observed rank accuracy.",
                "For this computation, we must know the users preference order for a set of items for which predictions can be computed.",
                "We assume that we have a test set of the users ratings of the items presented in the recommendation list.",
                "For every pair of items rated by a user in the test data, we check whether the predicted order is concordant with his preference.",
                "We say a pair (i, j) is concordant (with error ) whenever one of the following holds: • if (ru,i < ru,j) then (pu,i − pu,j < ); • if (ru,i > ru,j) then (pu,i − pu,j > ); or • if (ru,i = ru,j) then (|pu,i − pu,j| ≤ ).",
                "Similarly, a pair (i, j) is discordant (with error ) if it is not concordant.",
                "Our experiments described below use an error tolerance of = 0.1.",
                "All paths involved in the prediction of the two items in a concordant pair are credited, and the paths involved in a discordant pair are discredited.",
                "The credit assigned to a pair of items (i, j) in the recommendation list for user u is computed as c(i, j) = ( t T · 1 C+D if (i, j) are concordant − t T · 1 C+D if (i, j) are discordant (7) where t is the number of items in the users test set whose ratings could be predicted, T is the number of items rated by user u in the test set, C is the number of concordances and D is the number of discordances.",
                "The credit c is then divided among all paths responsible for predicting pu,i and pu,j proportional to their influences.",
                "We again add the role values obtained from all the experiments to form a triple [SF, CF, PF] for each rating. 4.4 Aggregating rating roles After calculating the role values for individual ratings, we can also use these values to study neighborhoods and the system.",
                "Here we consider how we can use the role values to characterize the health of a neighborhood.",
                "Consider the list of top recommendations presented to a user at a specific point in time.",
                "The collaborative filtering algorithm traversed many paths in his neighborhood through his scouts and other connectors and promoters to make these recommendations.",
                "We call these ratings the recommender neighborhood of the user.",
                "The user implicitly chooses this neighborhood of ratings through the items he rates.",
                "Apart from the collaborative filtering algorithm, the health of this neighborhood completely influences a users satisfaction with the system.",
                "We can characterize a users recommender neighborhood by aggregating the individual role values of the ratings involved, weighted by the influence of individual ratings in determining his recommended list.",
                "Different sections of the users neighborhood wield varied influence on his recommendation list.",
                "For instance, ratings reachable through highly rated items have a bigger say in the recommended items.",
                "Our aim is to study the system and classify users with respect to their positioning in a healthy or unhealthy neighborhood.",
                "A user can have a good set of scouts, but may be exposed to a neighborhood with bad connectors and promoters.",
                "He can have a good neighborhood, but his bad scouts may ensure the neighborhoods potential is rendered useless.",
                "We expect that users with good scouts and good neighborhoods will be most satisfied with the system in the future.",
                "A users neighborhood is characterized by a triple that represents the weighted sum of the role values of individual ratings involved in making recommendations.",
                "Consider a user u and his ordered list of recommendations L. An item i 254 in the list is weighted inversely, as K(i), depending on its position in the list.",
                "In our studies we use K(i) = p position(i).",
                "Several paths of ratings [ru,j, rv,j, rv,i] are involved in predicting pu,i which ultimately decides its position in L, each with influence(u, j, v, i).",
                "The recommender neighborhood of a user u is characterized by the triple, [SFN(u), CFN(u), PFN(u)] where SFN(u) = X i∈L P [ru,j ,rv,j ,rv,i] SF(ru,j)influence(u, j, v, i) K(i) !",
                "CFN(u) and PFN(u) are defined similarly.",
                "This triple estimates the quality of us recommendations based on the past track record of the ratings involved in their respective roles. 5.",
                "EXPERIMENTATION As we have seen, we can assign role values to each rating when evaluating a collaborative filtering system.",
                "In this section, we demonstrate the use of this approach to our overall goal of defining an approach to monitor and manage the health of a recommender system through experiments done on the MovieLens million rating dataset.",
                "In particular, we discuss results relating to identifying good scouts, promoters, and connectors; the evolution of rating roles; and the characterization of user neighborhoods. 5.1 Methodology Our experiments use the MovieLens million rating dataset, which consists of ratings by 6040 users of 3952 movies.",
                "The ratings are in the range 1 to 5, and are labeled with the time the rating was given.",
                "As discussed before, we consider only the item-based algorithm here (with item neighborhoods of size 30) and, due to space considerations, only present role value results for rank accuracy.",
                "Since we are interested in the evolution of the rating role values over time, the model of the recommender system is built by processing ratings in their arrival order.",
                "The timestamping provided by MovieLens is hence crucial for the analyses presented here.",
                "We make assessments of rating roles at intervals of 10,000 ratings and processed the first 200,000 ratings in the dataset (giving rise to 20 snapshots).",
                "We incrementally update the role values as the time ordered ratings are merged into the model.",
                "To keep the experiment computationally manageable, we define a test dataset for each user.",
                "As the time ordered ratings are merged into the model, we label a small randomly selected percentage (20%) as test data.",
                "At discrete epochs, i.e., after processing every 10,000 ratings, we compute the predictions for the ratings in the test data, and then compute the role values for the ratings used in the predictions.",
                "One potential criticism of this methodology is that the ratings in the test set are never evaluated for their roles.",
                "We overcome this concern by repeating the experiment, using different random seeds.",
                "The probability that every rating is considered for evaluation is then considerably high: 1 − 0.2n , where n is the number of times the experiment is repeated with different random seeds.",
                "The results here are based on n = 4 repetitions.",
                "The item-based collaborative filtering algorithms performance was ordinary with respect to rank accuracy.",
                "Fig. 5 shows a plot of the precision and recall as ratings were merged in time order into the model.",
                "The recall was always high, but the average precision was just about 53%. 0 0.2 0.4 0.6 0.8 1 1.2 10000 30000 50000 70000 90000110000130000150000 Ratings merged into model Value Precision Recall Figure 5: Precision and recall for the item-based collaborative filtering algorithm. 5.2 Inducing good scouts The ratings of a user that serve as scouts are those that allow the user to receive recommendations.",
                "We claim that users with ratings that have respectable scout values will be happier with the system than those with ratings with low scout values.",
                "Note that the item-based algorithm discussed here produces recommendation lists with nearly half of the pairs in the list discordant from the users preference.",
                "Whether all of these discordant pairs are observable by the user is unclear, however, certainly this suggests that there is a need to be able to direct users to items whose ratings would improve the lists.",
                "The distribution of the scout values for most users ratings are Gaussian with mean zero.",
                "Fig. 6 shows the frequency distribution of scout values for a sample user at a given snapshot.",
                "We observe that a large number of ratings never serve as scouts for their users.",
                "A relatable scenario is when Amazons recommender makes suggestions of books or items based on other items that were <br>purchase</br>d as gifts.",
                "With simple relevance feedback from the user, such ratings can be isolated as bad scouts and discounted from future predictions.",
                "Removing bad scouts was found to be worthwhile for individual users but the overall performance improvement was only marginal.",
                "An obvious question is whether good scouts can be formed by merely rating popular movies as suggested by Rashid et al. [9].",
                "They show that a mix of popularity and rating entropy identifies the best items to suggest to new users when evaluated using MAE.",
                "Following their intuition, we would expect to see a higher correlation between popularityentropy and good scouts.",
                "We measured the Pearson correlation coefficient between aggregated scout values for a movie with the popularity of a movie (number of times it is rated); and with its popularity*variance measure at different snapshots of the system.",
                "Note that the scout values were initially anti-correlated with popularity (Fig. 7), but became moderately correlated as the system evolved.",
                "Both popularity and popularity*variance performed similarly.",
                "A possible explanation is that there has been insufficient time for the popular movies to accumulate ratings. 255 -10 0 10 20 30 40 50 60 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 Scout Value Frequency Figure 6: Distribution of scout values for a sample user. -0.4 -0.2 0 0.2 0.4 0.6 0.8 30000 60000 90000 120000 150000 180000 Popularity Pop*Var Figure 7: Correlation between aggregated scout value and item popularity (computed at different intervals). -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 30000 60000 90000 120000 150000 180000 Figure 8: Correlation between aggregated promoter value and user prolificity (computed at different intervals).",
                "Table 1: Movies forming the best scouts.",
                "Best Scouts Conf.",
                "Pop.",
                "Being John Malkovich (1999) 1.00 445 Star Wars: Episode IV - A New Hope (1977) 0.92 623 Princess Bride, The (1987) 0.85 477 Sixth Sense, The (1999) 0.85 617 Matrix, The (1999) 0.77 522 Ghostbusters (1984) 0.77 441 Casablanca (1942) 0.77 384 Insider, The (1999) 0.77 235 American Beauty (1999) 0.69 624 Terminator 2: Judgment Day (1991) 0.69 503 Fight Club (1999) 0.69 235 Shawshank Redemption, The (1994) 0.69 445 Run Lola Run (Lola rennt) (1998) 0.69 220 Terminator, The (1984) 0.62 450 Usual Suspects, The (1995) 0.62 326 Aliens (1986) 0.62 385 North by Northwest (1959) 0.62 245 Fugitive, The (1993) 0.62 402 End of Days (1999) 0.62 132 Raiders of the Lost Ark (1981) 0.54 540 Schindlers List (1993) 0.54 453 Back to the Future (1985) 0.54 543 Toy Story (1995) 0.54 419 Alien (1979) 0.54 415 Abyss, The (1989) 0.54 345 2001: A Space Odyssey (1968) 0.54 358 Dogma (1999) 0.54 228 Little Mermaid, The (1989) 0.54 203 Table 2: Movies forming the worst scouts.",
                "Worst scouts Conf.",
                "Pop.",
                "Harold and Maude (1971) 0.46 141 Grifters, The (1990) 0.46 180 Sting, The (1973) 0.38 244 Godfather: Part III, The (1990) 0.38 154 Lawrence of Arabia (1962) 0.38 167 High Noon (1952) 0.38 84 Women on the Verge of a... (1988) 0.38 113 Grapes of Wrath, The (1940) 0.38 115 Duck Soup (1933) 0.38 131 Arsenic and Old Lace (1944) 0.38 138 Midnight Cowboy (1969) 0.38 137 To Kill a Mockingbird (1962) 0.31 195 Four Weddings and a Funeral (1994) 0.31 271 Good, The Bad and The Ugly, The (1966) 0.31 156 Its a Wonderful Life (1946) 0.31 146 Player, The (1992) 0.31 220 Jackie Brown (1997) 0.31 118 Boat, The (Das Boot) (1981) 0.31 210 Manhattan (1979) 0.31 158 Truth About Cats & Dogs, The (1996) 0.31 143 Ghost (1990) 0.31 227 Lone Star (1996) 0.31 125 Big Chill, The (1983) 0.31 184 256 By studying the evolution of scout values, we can identify movies that consistently feature in good scouts over time.",
                "We claim these movies will make viable scouts for other users.",
                "We found the aggregated scout values for all movies in intervals of 10,000 ratings each.",
                "A movie is said to induce a good scout if the movie was in the top 100 of the sorted list, and to induce a bad scout if it was in bottom 100 of the same list.",
                "Movies appearing consistently high over time are expected to remain up there in the future.",
                "The effective confidence in a movie m is Cm = Tm − Bm N (8) where Tm is the number of times it appeared in the top 100, Bm the number of times it appeared in the bottom 100, and N is the number of intervals considered.",
                "Using this measure, the top few movies expected to induce the best scouts are shown in Table 1.",
                "Movies that would be bad scout choices are shown in Table 2 with their associated confidences.",
                "The popularities of the movies are also displayed.",
                "Although more popular movies appear in the list of good scouts, these tables show that a blind choice of scout based on popularity alone can be potentially dangerous.",
                "Interestingly, the best scout-Being John Malkovich-is about a puppeteer who discovers a portal into a movie star, a movie that has been described variously on amazon.com as makes you feel giddy, seriously weird, comedy with depth, silly, strange, and inventive.",
                "Indicating whether someone likes this movie or not goes a long way toward situating the user in a suitable neighborhood, with similar preferences.",
                "On the other hand, several factors may have made a movie a bad scout, like the sharp variance in user preferences in the neighborhood of a movie.",
                "Two users may have the same opinion about Lawrence of Arabia, but they may differ sharply about how they felt about the other movies they saw.",
                "Bad scouts ensue when there is deviation in behavior around a common synchronization point. 5.3 Inducing good promoters Ratings that serve to promote items in a collaborative filtering system are critical to allowing a new item be recommended to users.",
                "So, inducing good promoters is important for cold-start recommendation.",
                "We note that the frequency distribution of promoter values for a sample movies ratings is also Gaussian (similar to Fig. 6).",
                "This indicates that the promotion of a movie is benefited most by the ratings of a few users, and are unaffected by the ratings of most users.",
                "We find a strong correlation between a users number of ratings and his aggregated promoter value.",
                "Fig. 8 depicts the evolution of the Pearson correlation co-efficient between the prolificity of a user (number of ratings) versus his aggregated promoter value.",
                "We expect that conspicuous shills, by recommending wrong movies to users, will be discredited with negative aggregate promoter values and should be identifiable easily.",
                "Given this observation, the obvious rule to follow when introducing a new movie is to have it rated directly by prolific users who posses high aggregated promoter values.",
                "A new movie is thus cast into the neighborhood of many other movies improving its visibility.",
                "Note, though, that a user may have long stopped using the system.",
                "Tracking promoter values consistently allows only the most active recent users to be considered. 5.4 Inducing good connectors Given the way scouts, connectors, and promoters are characterized, it follows that the movies that are part of the best scouts are also part of the best connectors.",
                "Similarly, the users that constitute the best promoters are also part of the best connectors.",
                "Good connectors are induced by ensuring a user with a high promoter value rates a movie with a high scout value.",
                "In our experiments, we find that a ratings longest standing role is often as a connector.",
                "A rating with a poor connector value is often seen due to its user being a bad promoter, or its movie being a bad scout.",
                "Such ratings can be removed from the prediction process to bring marginal improvements to recommendations.",
                "In some selected experiments, we observed that removing a set of badly behaving connectors helped improve the systems overall performance by 1.5%.",
                "The effect was even higher on a few select users who observed an improvement of above 10% in precision without much loss in recall. 5.5 Monitoring the evolution of rating roles One of the more significant contributions of our work is the ability to model the evolution of recommender systems, by studying the changing roles of ratings over time.",
                "The role and value of a rating can change depending on many factors like user behavior, redundancy, shilling effects or properties of the collaborative filtering algorithm used.",
                "Studying the dynamics of rating roles in terms of transitions between good, bad, and negligible values can provide insights into the functioning of the recommender system.",
                "We believe that a continuous visualization of these transitions will improve the ability to manage a recommender system.",
                "We classify different rating states as good, bad, or negligible.",
                "Consider a user who has rated 100 movies in a particular interval, of which 20 are part of the test set.",
                "If a scout has a value greater than 0.005, it indicates that it is uniquely involved in at least 2 concordant predictions, which we will say is good.",
                "Thus, a threshold of 0.005 is chosen to bin a rating as good, bad or negligible in terms of its scout, connector and promoter value.",
                "For instance, a rating r, at time t with role value triple [0.1, 0.001, −0.01] is classified as [scout +, connector 0, promoter −], where + indicates good, 0 indicates negligible, and − indicates bad.",
                "The positive credit held by a rating is a measure of its contribution to the betterment of the system, and the discredit is a measure of its contribution to the detriment of the system.",
                "Even though the positive roles (and the negative roles) make up a very small percentage of all ratings, their contribution supersedes their size.",
                "For example, even though only 1.7% of all ratings were classified as good scouts, they hold 79% of all positive credit in the system!",
                "Similarly, the bad scouts were just 1.4% of all ratings but hold 82% of all discredit.",
                "Note that good and bad scouts, together, comprise only 1.4% + 1.7% = 3.1% of the ratings, implying that the majority of the ratings are negligible role players as scouts (more on this later).",
                "Likewise, good connectors were 1.2% of the system, and hold 30% of all positive credit.",
                "The bad connectors (0.8% of the system) hold 36% of all discredit.",
                "Good promoters (3% of the system) hold 46% of all credit, while bad promoters (2%) hold 50% of all discredit.",
                "This reiterates that a few ratings influence most of the systems performance.",
                "Hence it is important to track transitions between them regardless of their small numbers. 257 Across different snapshots, a rating can remain in the same state or change.",
                "A good scout can become a bad scout, a good promoter can become a good connector, good and bad scouts can become vestigial, and so on.",
                "It is not practical to expect a recommender system to have no ratings in bad roles.",
                "However, it suffices to see ratings in bad roles either convert to good or vestigial roles.",
                "Similarly, observing a large number of good roles become bad ones is a sign of imminent failure of the system.",
                "We employ the principle of non-overlapping episodes [6] to count such transitions.",
                "A sequence such as: [+, 0, 0] → [+, 0, 0] → [0, +, 0] → [0, 0, 0] is interpreted as the transitions [+, 0, 0] ; [0, +, 0] : 1 [+, 0, 0] ; [0, 0, 0] : 1 [0, +, 0] ; [0, 0, 0] : 1 instead of [+, 0, 0] ; [0, +, 0] : 2 [+, 0, 0] ; [0, 0, 0] : 2 [0, +, 0] ; [0, 0, 0] : 1.",
                "See [6] for further details about this counting procedure.",
                "Thus, a rating can be in one of 27 possible states, and there are about 272 possible transitions.",
                "We make a further simplification and utilize only 9 states, indicating whether the rating is a scout, promoter, or connector, and whether it has a positive, negative, or negligible role.",
                "Ratings that serve multiple purposes are counted using multiple episode instantiations but the states themselves are not duplicated beyond the 9 restricted states.",
                "In this model, a transition such as [+, 0, +] ; [0, +, 0] : 1 is counted as [scout+] ; [scout0] : 1 [scout+] ; [connector+] : 1 [scout+] ; [promoter0] : 1 [connector0] ; [scout0] : 1 [connector0] ; [scout+] : 1 [connector0] ; [promoter0] : 1 [promoter+] ; [scout0] : 1 [promoter+] ; [connector+] : 1 [promoter+] ; [promoter0] : 1 Of these, transitions like [pX] ; [q0] where p = q, X ∈ {+, 0, −} are considered uninteresting, and only the rest are counted.",
                "Fig. 9 depicts the major transitions counted while processing the first 200,000 ratings from the MovieLens dataset.",
                "Only transitions with frequency greater than or equal to 3% are shown.",
                "The percentages for each state indicates the number of ratings that were found to be in those states.",
                "We consider transitions from any state to a good state as healthy, from any state to a bad state as unhealthy, and from any state to a vestigial state as decaying.",
                "From Fig. 9, we can observe: • The bulk of the ratings have negligible values, irrespective of their role.",
                "The majority of the transitions involve both good and bad ratings becoming negligible.",
                "Scout + (2%) Scout(1.5%) Scout 0 (96.5%) Connector + (1.2%) Connector(0.8%) Connector 0 (98%) Promoter + (3%) Promoter(2%) Promoter 0 (95%) 84% 84% 81% 74% 10% 6% 11% 77% 8% 7% 8% 82% 4% 86% 4% 68% 15% 13% 5% 5% 77% 11% 7% 5% 4% 3% 3% 3% Healthy Unhealthy Decaying Figure 9: Transitions among rating roles. • The number of good ratings is comparable to the bad ratings, and ratings are seen to switch states often, except in the case of scouts (see below). • The negative and positive scout states are not reachable through any transition, indicating that these ratings must begin as such, and cannot be coerced into these roles. • Good promoters and good connectors have a much longer survival period than scouts.",
                "Transitions that recur to these states have frequencies of 10% and 15% when compared to just 4% for scouts.",
                "Good connectors are the slowest to decay whereas (good) scouts decay the fastest. • Healthy percentages are seen on transitions between promoters and connectors.",
                "As indicated earlier, there are hardly any transitions from promoters/connectors to scouts.",
                "This indicates that, over the long run, a users rating is more useful to others (movies or other users) than to the user himself. • The percentages of healthy transitions outweigh the unhealthy ones - this hints that the system is healthy, albeit only marginally.",
                "Note that these results are conditioned by the static nature of the dataset, which is a set of ratings over a fixed window of time.",
                "However a diagram such as Fig. 9 is clearly useful for monitoring the health of a recommender system.",
                "For instance, acceptable limits can be imposed on different types of transitions and, if a transition fails to meet the threshold, the recommender system or a part of it can be brought under closer scrutiny.",
                "Furthermore, the role state transition diagram would also be the ideal place to study the effects of shilling, a topic we will consider in future research. 5.6 Characterizing neighborhoods Earlier we saw that we can characterize the neighborhood of ratings involved in creating a recommendation list L for 258 a user.",
                "In our experiment, we consider lists of length 30, and sample the lists of about 5% of users through the evolution of the model (at intervals of 10,000 ratings each) and compute their neighborhood characteristics.",
                "To simplify our presentation, we consider the percentage of the sample that fall into one of the following categories: 1.",
                "Inactive user: (SFN(u) = 0) 2.",
                "Good scouts, Good neighborhood: (SFN(u) > 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 3.",
                "Good scouts, Bad neighborhood: (SFN(u) > 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) 4.",
                "Bad scouts, Good neighborhood: (SFN(u) < 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 5.",
                "Bad scouts, Bad neighborhood: (SFN(u) < 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) From our sample set of 561 users, we found that 476 users were inactive.",
                "Of the remaining 85 users, we found 26 users had good scouts and a good neighborhood, 6 had bad scouts and a good neighborhood, 29 had good scouts and a bad neighborhood, and 24 had bad scouts and a bad neighborhood.",
                "Thus, we conjecture that 59 users (29+24+6) are in danger of leaving the system.",
                "As a remedy, users with bad scouts and a good neighborhood can be asked to reconsider rating of some movies hoping to improve the systems recommendations.",
                "The system can be expected to deliver more if they engineer some good scouts.",
                "Users with good scouts and a bad neighborhood are harder to address; this situation might entail selectively removing some connector-promoter pairs that are causing the damage.",
                "Handling users with bad scouts and bad neighborhoods is a more difficult challenge.",
                "Such a classification allows the use of different strategies to better a users experience with the system depending on his context.",
                "In future work, we intend to conduct field studies and study the improvement in performance of different strategies for different contexts. 6.",
                "CONCLUSIONS To further recommender system acceptance and deployment, we require new tools and methodologies to manage an installed recommender and develop insights into the roles played by ratings.",
                "A fine-grained characterization in terms of rating roles such as scouts, promoters, and connectors, as done here, helps such an endeavor.",
                "Although we have presented results on only the item-based algorithm with list rank accuracy as the metric, the same approach outlined here applies to user-based algorithms and other metrics.",
                "In future research, we plan to systematically study the many algorithmic parameters, tolerances, and cutoff thresholds employed here and reason about their effects on the downstream conclusions.",
                "We also aim to extend our formulation to other collaborative filtering algorithms, study the effect of shilling in altering rating roles, conduct field studies, and evaluate improvements in user experience by tweaking ratings based on their role values.",
                "Finally, we plan to develop the idea of mining the evolution of rating role patterns into a reporting and tracking system for all aspects of recommender system health. 7.",
                "REFERENCES [1] Cosley, D., Lam, S., Albert, I., Konstan, J., and Riedl, J.",
                "Is Seeing Believing? : How Recommender System Interfaces Affect Users Opinions.",
                "In Proc.",
                "CHI (2001), pp. 585-592. [2] Herlocker, J. L., Konstan, J.",
                "A., Borchers, A., and Riedl, J.",
                "An Algorithmic Framework for Performing Collaborative Filtering.",
                "In Proc.",
                "SIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J.",
                "A., Terveen, L. G., and Riedl, J. T. Evaluating Collaborative Filtering Recommender Systems.",
                "ACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J.",
                "A.",
                "Personal communication. 2003. [5] Lam, S. K., and Riedl, J. Shilling Recommender Systems for Fun and Profit.",
                "In Proceedings of the 13th International World Wide Web Conference (2004), ACM Press, pp. 393-402. [6] Laxman, S., Sastry, P. S., and Unnikrishnan, K. P. Discovering Frequent Episodes and Learning Hidden Markov Models: A Formal Connection.",
                "IEEE Transactions on Knowledge and Data Engineering Vol. 17, 11 (2005), 1505-1517. [7] McLaughlin, M. R., and Herlocker, J. L. A Collaborative Filtering Algorithm and Evaluation Metric that Accurately Model the User Experience.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (2004), pp. 329 - 336. [8] OMahony, M., Hurley, N. J., Kushmerick, N., and Silvestre, G. Collaborative Recommendation: A Robustness Analysis.",
                "ACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J.",
                "A., and Riedl, J.",
                "Getting to Know You: Learning New User Preferences in Recommender Systems.",
                "In Proceedings of the 2002 Conference on Intelligent User Interfaces (IUI 2002) (2002), pp. 127-134. [10] Rashid, A. M., Karypis, G., and Riedl, J.",
                "Influence in Ratings-Based Recommender Systems: An Algorithm-Independent Approach.",
                "In Proc. of the SIAM International Conference on Data Mining (2005). [11] Resnick, P., Iacovou, N., Sushak, M., Bergstrom, P., and Riedl, J. GroupLens: An Open Architecture for Collaborative Filtering of Netnews.",
                "In Proceedings of the Conference on Computer Supported Collaborative Work (CSCW94) (1994), ACM Press, pp. 175-186. [12] Sarwar, B., Karypis, G., Konstan, J., and Reidl, J. Item-Based Collaborative Filtering Recommendation Algorithms.",
                "In Proceedings of the Tenth International World Wide Web Conference (WWW10) (2001), pp. 285-295. [13] Schein, A., Popescu, A., Ungar, L., and Pennock, D. Methods and Metrics for Cold-Start Recommendation.",
                "In Proc.",
                "SIGIR (2002), pp. 253-260. 259"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Introducción Los sistemas de recomendación se han vuelto integrales para el comercio electrónico, proporcionando tecnología que sugiere productos a un visitante basado en el historial de calificación de \"compras\" o de calificación.",
                "Un escenario relatable es cuando Amazons Recomender hace sugerencias de libros o artículos basados en otros artículos que fueron \"comprar\" D como regalos."
            ],
            "translated_text": "",
            "candidates": [
                "compra",
                "compras",
                "compra",
                "comprar"
            ],
            "error": []
        },
        "opinion": {
            "translated_key": "opinión",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Scouts, Promoters, and Connectors: The Roles of Ratings in Nearest Neighbor Collaborative Filtering Bharath Kumar Mohan Dept.",
                "of CSA Indian Institute of Science Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. of Computer Science Eastern Michigan University Ypsilanti, MI 48917, USA bkeller@emich.edu Naren Ramakrishnan Dept. of Computer Science Virginia Tech, Blacksburg VA 24061, USA naren@cs.vt.edu ABSTRACT Recommender systems aggregate individual user ratings into predictions of products or services that might interest visitors.",
                "The quality of this aggregation process crucially affects the user experience and hence the effectiveness of recommenders in e-commerce.",
                "We present a novel study that disaggregates global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearestneighbor collaborative filtering.",
                "In particular, we formulate three roles-scouts, promoters, and connectors-that capture how users receive recommendations, how items get recommended, and how ratings of these two types are themselves connected (resp.).",
                "These roles find direct uses in improving recommendations for users, in better targeting of items and, most importantly, in helping monitor the health of the system as a whole.",
                "For instance, they can be used to track the evolution of neighborhoods, to identify rating subspaces that do not contribute (or contribute negatively) to system performance, to enumerate users who are in danger of leaving, and to assess the susceptibility of the system to attacks such as shilling.",
                "We argue that the three rating roles presented here provide broad primitives to manage a recommender system and its community.",
                "Categories and Subject Descriptors H.4.2 [Information Systems Applications]: Types of Systems-Decision support; J.4 [Computer Applications]: Social and Behavioral Sciences General Terms Algorithms, Human Factors 1.",
                "INTRODUCTION Recommender systems have become integral to e-commerce, providing technology that suggests products to a visitor based on previous purchases or rating history.",
                "Collaborative filtering, a common form of recommendation, predicts a users rating for an item by combining (other) ratings of that user with other users ratings.",
                "Significant research has been conducted in implementing fast and accurate collaborative filtering algorithms [2, 7], designing interfaces for presenting recommendations to users [1], and studying the robustness of these algorithms [8].",
                "However, with the exception of a few studies on the influence of users [10], little attention has been paid to unraveling the inner workings of a recommender in terms of the individual ratings and the roles they play in making (good) recommendations.",
                "Such an understanding will give an important handle to monitoring and managing a recommender system, to engineer mechanisms to sustain the recommender, and thereby ensure its continued success.",
                "Our motivation here is to disaggregate global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearest-neighbor collaborative filtering.",
                "We identify three possible roles: (scouts) to connect the user into the system to receive recommendations, (promoters) to connect an item into the system to be recommended, and (connectors) to connect ratings of these two kinds.",
                "Viewing ratings in this way, we can define the contribution of a rating in each role, both in terms of allowing recommendations to occur, and in terms of influence on the quality of recommendations.",
                "In turn, this capability helps support scenarios such as: 1.",
                "Situating users in better neighborhoods: A users ratings may inadvertently connect the user to a neighborhood for which the users tastes may not be a perfect match.",
                "Identifying ratings responsible for such bad recommendations and suggesting new items to rate can help situate the user in a better neighborhood. 2.",
                "Targeting items: Recommender systems suffer from lack of user participation, especially in cold-start scenarios [13] involving newly arrived items.",
                "Identifying users who can be encouraged to rate specific items helps ensure coverage of the recommender system. 3.",
                "Monitoring the evolution of the recommender system and its stakeholders: A recommender system is constantly under change: growing with new users and 250 items, shrinking with users leaving the system, items becoming irrelevant, and parts of the system under attack.",
                "Tracking the roles of a rating and its evolution over time provides many insights into the health of the system, and how it could be managed and improved.",
                "These include being able to identify rating subspaces that do not contribute (or contribute negatively) to system performance, and could be removed; to enumerate users who are in danger of leaving, or have left the system; and to assess the susceptibility of the system to attacks such as shilling [5].",
                "As we show, the characterization of rating roles presented here provides broad primitives to manage a recommender system and its community.",
                "The rest of the paper is organized as follows.",
                "Background on nearest-neighbor collaborative filtering and algorithm evaluation is discussed in Section 2.",
                "Section 3 defines and discusses the roles of a rating, and Section 4 defines measures of the contribution of a rating in each of these roles.",
                "In Section 5, we illustrate the use of these roles to address the goals outlined above. 2.",
                "BACKGROUND 2.1 Algorithms Nearest-neighbor collaborative filtering algorithms either use neighborhoods of users or neighborhoods of items to compute a prediction.",
                "An algorithm of the first kind is called user-based, and one of the second kind is called itembased [12].",
                "In both families of algorithms, neighborhoods are formed by first computing the similarity between all pairs of users (for user-based) or items (for item-based).",
                "Predictions are then computed by aggregating ratings, which in a user-based algorithm involves aggregating the ratings of the target item by the users neighbors and, in an item-based algorithm, involves aggregating the users ratings of items that are neighbors of the target item.",
                "Algorithms within these families differ in the definition of similarity, formation of neighborhoods, and the computation of predictions.",
                "We consider a user-based algorithm based on that defined for GroupLens [11] with variations from Herlocker et al. [2], and an item-based algorithm similar to that of Sarwar et al. [12].",
                "The algorithm used by Resnick et al. [11] defines the similarity of two users u and v as the Pearson correlation of their common ratings: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2 , where Iu is the set of items rated by user u, ru,i is user us rating for item i, and ¯ru is the average rating of user u (similarly for v).",
                "Similarity computed in this manner is typically scaled by a factor proportional to the number of common ratings, to reduce the chance of making a recommendation made on weak connections: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), where γ ≈ 5 is a constant used as a lower limit in scaling [2].",
                "These new similarities are then used to define a static neighborhood Nu for each user u consisting of the top K users most similar to user u.",
                "A prediction for user u and item i is computed by a weighted average of the ratings by the neighbors pu,i = ¯ru + P v∈V sim (u, v)(rv,i − ¯rv) P v∈V sim (u, v) (1) where V = Nu ∩ Ui is the set of users most similar to u who have rated i.",
                "The item-based algorithm we use is the one defined by Sarwar et al. [12].",
                "In this algorithm, similarity is defined as the adjusted cosine measure sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) where Ui is the set of users who have rated item i.",
                "As for the user-based algorithm, the similarity weights are adjusted proportionally to the number of users that have rated the items in common sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). (3) Given the similarities, the neighborhood Ni of an item i is defined as the top K most similar items for that item.",
                "A prediction for user u and item i is computed as the weighted average pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) where J = Ni ∩ Iu is the set of items rated by u that are most similar to i. 2.2 Evaluation Recommender algorithms have typically been evaluated using measures of predictive accuracy and coverage [3].",
                "Studies on recommender algorithms, notably Herlocker et al. [2] and Sarwar et al. [12], typically compute predictive accuracy by dividing a set of ratings into training and test sets, and compute the prediction for an item in the test set using the ratings in the training set.",
                "A standard measure of predictive accuracy is mean absolute error (MAE), which for a test set T = {(u, i)} is defined as, MAE = P (u,i)∈T |pu,i − ru,i| |T | . (5) Coverage has a number of definitions, but generally refers to the proportion of items that can be predicted by the algorithm [3].",
                "A practical issue with predictive accuracy is that users typically are presented with recommendation lists, and not individual numeric predictions.",
                "Recommendation lists are lists of items in decreasing order of prediction (sometimes stated in terms of star-ratings), and so predictive accuracy may not be reflective of the accuracy of the list.",
                "So, instead we can measure recommendation or rank accuracy, which indicates the extent to which the list is in the correct order.",
                "Herlocker et al. [3] discuss a number of rank accuracy measures, which range from Kendalls Tau to measures that consider the fact that users tend to only look at a prefix of the list [5].",
                "Kendalls Tau measures the number of inversions when comparing ordered pairs in the true user ordering of 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figure 1: Ratings in simple movie recommender. items and the recommended order, and is defined as τ = C − D p (C + D + TR)(C + D + TP) (6) where C is the number of pairs that the system predicts in the correct order, D the number of pairs the system predicts in the wrong order, TR the number of pairs in the true ordering that have the same ratings, and TP is the number of pairs in the predicted ordering that have the same ratings [3].",
                "A shortcoming of the Tau metric is that it is oblivious to the position in the ordered list where the inversion occurs [3].",
                "For instance, an inversion toward the end of the list is given the same weight as one in the beginning.",
                "One solution is to consider inversions only in the top few items in the recommended list or to weight inversions based on their position in the list. 3.",
                "ROLES OF A RATING Our basic observation is that each rating plays a different role in each prediction in which it is used.",
                "Consider a simplified movie recommender system with three users Jim, Jeff, and Tom and their ratings for a few movies, as shown in Fig. 1. (For this initial discussion we will not consider the rating values involved.)",
                "The recommender predicts whether Tom will like The Mask using the other already available ratings.",
                "How this is done depends on the algorithm: 1.",
                "An item-based collaborative filtering algorithm constructs a neighborhood of movies around The Mask by using the ratings of users who rated The Mask and other movies similarly (e.g., Jims ratings of The Matrix and The Mask; and Jeffs ratings of Star Wars and The Mask).",
                "Toms ratings of those movies are then used to make a prediction for The Mask. 2.",
                "A user-based collaborative filtering algorithm would construct a neighborhood around Tom by tracking other users whose rating behaviors are similar to Toms (e.g., Tom and Jeff have rated Star Wars; Tom and Jim have rated The Matrix).",
                "The prediction of Toms rating for The Mask is then based on the ratings of Jeff and Tim.",
                "Although the nearest-neighbor algorithms aggregate the ratings to form neighborhoods used to compute predictions, we can disaggregate the similarities to view the computation of a prediction as simultaneously following parallel paths of ratings.",
                "So, irrespective of the collaborative filtering algorithm used, we can visualize the prediction of Toms rating of The Mask as walking through a sequence of ratings.",
                "In Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figure 2: Ratings used to predict The Mask for Tom.",
                "Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figure 3: Prediction of The Mask for Tom in which a rating is used more than once. this example, two paths were used for this prediction as depicted in Fig. 2: (p1, p2, p3) and (q1, q2, q3).",
                "Note that these paths are undirected, and are all of length 3.",
                "Only the order in which the ratings are traversed is different between the item-based algorithm (e.g., (p3, p2, p1), (q3, q2, q1)) and the user-based algorithm (e.g., (p1, p2, p3), (q1, q2, q3).)",
                "A rating can be part of many paths for a single prediction as shown in Fig. 3, where three paths are used for a prediction, two of which follow p1: (p1, p2, p3) and (p1, r2, r3).",
                "Predictions in a collaborative filtering algorithms may involve thousands of such walks in parallel, each playing a part in influencing the predicted value.",
                "Each prediction path consists of three ratings, playing roles that we call scouts, promoters, and connectors.",
                "To illustrate these roles, consider the path (p1, p2, p3) in Fig. 2 used to make a prediction of The Mask for Tom: 1.",
                "The rating p1 (Tom → Star Wars) makes a connection from Tom to other ratings that can be used to predict Toms rating for The Mask.",
                "This rating serves as a scout in the bipartite graph of ratings to find a path that leads to The Mask. 2.",
                "The rating p2 (Jeff → Star Wars) helps the system recommend The Mask to Tom by connecting the scout to the promoter. 3.",
                "The rating p3 (Jeff → The Mask) allows connections to The Mask, and, therefore, promotes this movie to Tom.",
                "Formally, given a prediction pu,a of a target item a for user u, a scout for pu,a is a rating ru,i such that there exists a user v with ratings rv,a and rv,i for some item i; a promoter for pu,a is a rating rv,a for some user v, such that there exist ratings rv,i and ru,i for an item i, and; a connector for pu,a 252 Jim Tom Jeff Jerry My Cousin Vinny The Matrix Star Wars The Mask Jurasic Park Figure 4: Scouts, promoters, and connectors. is a rating rv,i by some user v and rating i, such that there exists ratings ru,i and rv,a.",
                "The scouts, connectors, and promoters for the prediction of Toms rating of The Mask are p1 and q1, p2 and q2, and p3 and q3 (respectively).",
                "Each of these roles has a value in the recommender to the user, the users neighborhood, and the system in terms of allowing recommendations to be made. 3.1 Roles in Detail Ratings that act as scouts tend to help the recommender system suggest more movies to the user, though the extent to which this is true depends on the rating behavior of other users.",
                "For example, in Fig. 4 the rating Tom → Star Wars helps the system recommend only The Mask to him, while Tom → The Matrix helps recommend The Mask, Jurassic Park, and My Cousin Vinny.",
                "Tom makes a connection to Jim who is a prolific user of the system, by rating The Matrix.",
                "However, this does not make The Matrix the best movie to rate for everyone.",
                "For example, Jim is benefited equally by both The Mask and The Matrix, which allow the system to recommend Star Wars to him.",
                "His rating of The Mask is the best scout for Jeff, and Jerrys only scout is his rating of Star Wars.",
                "This suggests that good scouts allow a user to build similarity with prolific users, and thereby ensure they get more from the system.",
                "While scouts represent beneficial ratings from the perspective of a user, promoters are their duals, and are of benefit to items.",
                "In Fig. 4, My Cousin Vinny benefits from Jims rating, since it allows recommendations to Jeff and Tom.",
                "The Mask is not so dependent on just one rating, since the ratings by Jim and Jeff help it.",
                "On the other hand, Jerrys rating of Star Wars does not help promote it to any other user.",
                "We conclude that a good promoter connects an item to a broader neighborhood of other items, and thereby ensures that it is recommended to more users.",
                "Connectors serve a crucial role in a recommender system that is not as obvious.",
                "The movies My Cousin Vinny and Jurassic Park have the highest recommendation potential since they can be recommended to Jeff, Jerry and Tom based on the linkage structure illustrated in Fig. 4.",
                "Beside the fact that Jim rated these movies, these recommendations are possible only because of the ratings Jim → The Matrix and Jim → The Mask, which are the best connectors.",
                "A connector improves the systems ability to make recommendations with no explicit gain for the user.",
                "Note that every rating can be of varied benefit in each of these roles.",
                "The rating Jim → My Cousin Vinny is a poor scout and connector, but is a very good promoter.",
                "The rating Jim → The Mask is a reasonably good scout, a very good connector, and a good promoter.",
                "Finally, the rating Jerry → Star Wars is a very good scout, but is of no value as a connector or promoter.",
                "As illustrated here, a rating can have different value in each of the three roles in terms of whether a recommendation can be made or not.",
                "We could measure this value by simply counting the number of times a rating is used in each role, which alone would be helpful in characterizing the behavior of a system.",
                "But we can also measure the contribution of each rating to the quality of recommendations or health of the system.",
                "Since every prediction is a combined effort of several recommendation paths, we are interested in discerning the influence of each rating (and, hence, each path) in the system towards the systems overall error.",
                "We can understand the dynamics of the system at a finer granularity by tracking the influence of a rating according to the role played.",
                "The next section describes the approach to measuring the values of a rating in each role. 4.",
                "CONTRIBUTIONS OF RATINGS As weve seen, a rating may play different roles in different predictions and, in each prediction, contribute to the quality of a prediction in different ways.",
                "Our approach can use any numeric measure of a property of system health, and assigns credit (or blame) to each rating proportional to its influence in the prediction.",
                "By tracking the role of each rating in a prediction, we can accumulate the credit for a rating in each of the three roles, and also track the evolution of the roles of rating over time in the system.",
                "This section defines the methodology for computing the contribution of ratings by first defining the influence of a rating, and then instantiating the approach for predictive accuracy, and then rank accuracy.",
                "We also demonstrate how these contributions can be aggregated to study the neighborhood of ratings involved in computing a users recommendations.",
                "Note that although our general formulation for rating influence is algorithm independent, due to space considerations, we present the approach for only item-based collaborative filtering.",
                "The definition for user-based algorithms is similar and will be presented in an expanded version of this paper. 4.1 Influence of Ratings Recall that an item-based approach to collaborative filtering relies on building item neighborhoods using the similarity of ratings by the same user.",
                "As described earlier, similarity is defined by the adjusted cosine measure (Equations (2) and (3)).",
                "A set of the top K neighbors is maintained for all items for space and computational efficiency.",
                "A prediction of item i for a user u is computed as the weighted deviation from the items mean rating as shown in Equation (4).",
                "The list of recommendations for a user is then the list of items sorted in descending order of their predicted values.",
                "We first define impact(a, i, j), the impact a user a has in determining the similarity between two items i and j.",
                "This is the change in the similarity between i and j when as rating is removed, and is defined as impact(a, i, j) = |sim (i, j) − sim¯a(i, j)| P w∈Cij |sim (i, j) − sim ¯w(i, j)| where Cij = {u ∈ U | ∃ ru,i, ru,j ∈ R(u)} is the set of coraters 253 of items i and j (users who rate both i and j), R(u) is the set of ratings provided by user u, and sim¯a(i, j) is the similarity of i and j when the ratings of user a are removed sim¯a(i, j) = P v∈U\\{a} (ru,i − ¯ru)(ru,j − ¯ru) qP u∈U\\{a}(ru,i − ¯ru)2 qP u∈U\\{a}(ru,j − ¯ru)2 , and adjusted for the number of raters sim¯a(i, j) = max(|Ui ∩ Uj| − 1, γ) γ · sim(i, j).",
                "If all coraters of i and j rate them identically, we define the impact as impact(a, i, j) = 1 |Cij| since P w∈Cij |sim (i, j) − sim ¯w(i, j)| = 0.",
                "The influence of each path (u, j, v, i) = [ru,j, rv,j, rv,i] in the prediction of pu,i is given by influence(u, j, v, i) = sim (i, j) P l∈Ni∩Iu sim (i, l) · impact(v, i, j) It follows that the sum of influences over all such paths, for a given set of endpoints, is 1. 4.2 Role Values for Predictive Accuracy The value of a rating in each role is computed from the influence depending on the evaluation measure employed.",
                "Here we illustrate the approach using predictive accuracy as the evaluation metric.",
                "In general, the goodness of a prediction decides whether the ratings involved must be credited or discredited for their role.",
                "For predictive accuracy, the error in prediction e = |pu,i − ru,i| is mapped to a comfort level using a mapping function M(e).",
                "Anecdotal evidence suggests that users are unable to discern errors less than 1.0 (for a rating scale of 1 to 5) [4], and so an error less than 1.0 is considered acceptable, but anything larger is not.",
                "We hence define M(e) as (1 − e) binned to an appropriate value in [−1, −0.5, 0.5, 1].",
                "For each prediction pu,i, M(e) is attributed to all the paths that assisted the computation of pu,i, proportional to their influences.",
                "This tribute, M(e)∗influence(u, j, v, i), is in turn inherited by each of the ratings in the path [ru,j, rv,j, rv,i], with the credit/blame accumulating to the respective roles of ru,j as a scout, rv,j as a connector, and rv,i as a promoter.",
                "In other words, the scout value SF(ru,j), the connector value CF(rv,j) and the promoter value PF(rv,i) are all incremented by the tribute amount.",
                "Over a large number of predictions, scouts that have repeatedly resulted in big error rates have a big negative scout value, and vice versa (similarly with the other roles).",
                "Every rating is thus summarized by its triple [SF, CF, PF]. 4.3 Role Values for Rank Accuracy We now define the computation of the contribution of ratings to observed rank accuracy.",
                "For this computation, we must know the users preference order for a set of items for which predictions can be computed.",
                "We assume that we have a test set of the users ratings of the items presented in the recommendation list.",
                "For every pair of items rated by a user in the test data, we check whether the predicted order is concordant with his preference.",
                "We say a pair (i, j) is concordant (with error ) whenever one of the following holds: • if (ru,i < ru,j) then (pu,i − pu,j < ); • if (ru,i > ru,j) then (pu,i − pu,j > ); or • if (ru,i = ru,j) then (|pu,i − pu,j| ≤ ).",
                "Similarly, a pair (i, j) is discordant (with error ) if it is not concordant.",
                "Our experiments described below use an error tolerance of = 0.1.",
                "All paths involved in the prediction of the two items in a concordant pair are credited, and the paths involved in a discordant pair are discredited.",
                "The credit assigned to a pair of items (i, j) in the recommendation list for user u is computed as c(i, j) = ( t T · 1 C+D if (i, j) are concordant − t T · 1 C+D if (i, j) are discordant (7) where t is the number of items in the users test set whose ratings could be predicted, T is the number of items rated by user u in the test set, C is the number of concordances and D is the number of discordances.",
                "The credit c is then divided among all paths responsible for predicting pu,i and pu,j proportional to their influences.",
                "We again add the role values obtained from all the experiments to form a triple [SF, CF, PF] for each rating. 4.4 Aggregating rating roles After calculating the role values for individual ratings, we can also use these values to study neighborhoods and the system.",
                "Here we consider how we can use the role values to characterize the health of a neighborhood.",
                "Consider the list of top recommendations presented to a user at a specific point in time.",
                "The collaborative filtering algorithm traversed many paths in his neighborhood through his scouts and other connectors and promoters to make these recommendations.",
                "We call these ratings the recommender neighborhood of the user.",
                "The user implicitly chooses this neighborhood of ratings through the items he rates.",
                "Apart from the collaborative filtering algorithm, the health of this neighborhood completely influences a users satisfaction with the system.",
                "We can characterize a users recommender neighborhood by aggregating the individual role values of the ratings involved, weighted by the influence of individual ratings in determining his recommended list.",
                "Different sections of the users neighborhood wield varied influence on his recommendation list.",
                "For instance, ratings reachable through highly rated items have a bigger say in the recommended items.",
                "Our aim is to study the system and classify users with respect to their positioning in a healthy or unhealthy neighborhood.",
                "A user can have a good set of scouts, but may be exposed to a neighborhood with bad connectors and promoters.",
                "He can have a good neighborhood, but his bad scouts may ensure the neighborhoods potential is rendered useless.",
                "We expect that users with good scouts and good neighborhoods will be most satisfied with the system in the future.",
                "A users neighborhood is characterized by a triple that represents the weighted sum of the role values of individual ratings involved in making recommendations.",
                "Consider a user u and his ordered list of recommendations L. An item i 254 in the list is weighted inversely, as K(i), depending on its position in the list.",
                "In our studies we use K(i) = p position(i).",
                "Several paths of ratings [ru,j, rv,j, rv,i] are involved in predicting pu,i which ultimately decides its position in L, each with influence(u, j, v, i).",
                "The recommender neighborhood of a user u is characterized by the triple, [SFN(u), CFN(u), PFN(u)] where SFN(u) = X i∈L P [ru,j ,rv,j ,rv,i] SF(ru,j)influence(u, j, v, i) K(i) !",
                "CFN(u) and PFN(u) are defined similarly.",
                "This triple estimates the quality of us recommendations based on the past track record of the ratings involved in their respective roles. 5.",
                "EXPERIMENTATION As we have seen, we can assign role values to each rating when evaluating a collaborative filtering system.",
                "In this section, we demonstrate the use of this approach to our overall goal of defining an approach to monitor and manage the health of a recommender system through experiments done on the MovieLens million rating dataset.",
                "In particular, we discuss results relating to identifying good scouts, promoters, and connectors; the evolution of rating roles; and the characterization of user neighborhoods. 5.1 Methodology Our experiments use the MovieLens million rating dataset, which consists of ratings by 6040 users of 3952 movies.",
                "The ratings are in the range 1 to 5, and are labeled with the time the rating was given.",
                "As discussed before, we consider only the item-based algorithm here (with item neighborhoods of size 30) and, due to space considerations, only present role value results for rank accuracy.",
                "Since we are interested in the evolution of the rating role values over time, the model of the recommender system is built by processing ratings in their arrival order.",
                "The timestamping provided by MovieLens is hence crucial for the analyses presented here.",
                "We make assessments of rating roles at intervals of 10,000 ratings and processed the first 200,000 ratings in the dataset (giving rise to 20 snapshots).",
                "We incrementally update the role values as the time ordered ratings are merged into the model.",
                "To keep the experiment computationally manageable, we define a test dataset for each user.",
                "As the time ordered ratings are merged into the model, we label a small randomly selected percentage (20%) as test data.",
                "At discrete epochs, i.e., after processing every 10,000 ratings, we compute the predictions for the ratings in the test data, and then compute the role values for the ratings used in the predictions.",
                "One potential criticism of this methodology is that the ratings in the test set are never evaluated for their roles.",
                "We overcome this concern by repeating the experiment, using different random seeds.",
                "The probability that every rating is considered for evaluation is then considerably high: 1 − 0.2n , where n is the number of times the experiment is repeated with different random seeds.",
                "The results here are based on n = 4 repetitions.",
                "The item-based collaborative filtering algorithms performance was ordinary with respect to rank accuracy.",
                "Fig. 5 shows a plot of the precision and recall as ratings were merged in time order into the model.",
                "The recall was always high, but the average precision was just about 53%. 0 0.2 0.4 0.6 0.8 1 1.2 10000 30000 50000 70000 90000110000130000150000 Ratings merged into model Value Precision Recall Figure 5: Precision and recall for the item-based collaborative filtering algorithm. 5.2 Inducing good scouts The ratings of a user that serve as scouts are those that allow the user to receive recommendations.",
                "We claim that users with ratings that have respectable scout values will be happier with the system than those with ratings with low scout values.",
                "Note that the item-based algorithm discussed here produces recommendation lists with nearly half of the pairs in the list discordant from the users preference.",
                "Whether all of these discordant pairs are observable by the user is unclear, however, certainly this suggests that there is a need to be able to direct users to items whose ratings would improve the lists.",
                "The distribution of the scout values for most users ratings are Gaussian with mean zero.",
                "Fig. 6 shows the frequency distribution of scout values for a sample user at a given snapshot.",
                "We observe that a large number of ratings never serve as scouts for their users.",
                "A relatable scenario is when Amazons recommender makes suggestions of books or items based on other items that were purchased as gifts.",
                "With simple relevance feedback from the user, such ratings can be isolated as bad scouts and discounted from future predictions.",
                "Removing bad scouts was found to be worthwhile for individual users but the overall performance improvement was only marginal.",
                "An obvious question is whether good scouts can be formed by merely rating popular movies as suggested by Rashid et al. [9].",
                "They show that a mix of popularity and rating entropy identifies the best items to suggest to new users when evaluated using MAE.",
                "Following their intuition, we would expect to see a higher correlation between popularityentropy and good scouts.",
                "We measured the Pearson correlation coefficient between aggregated scout values for a movie with the popularity of a movie (number of times it is rated); and with its popularity*variance measure at different snapshots of the system.",
                "Note that the scout values were initially anti-correlated with popularity (Fig. 7), but became moderately correlated as the system evolved.",
                "Both popularity and popularity*variance performed similarly.",
                "A possible explanation is that there has been insufficient time for the popular movies to accumulate ratings. 255 -10 0 10 20 30 40 50 60 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 Scout Value Frequency Figure 6: Distribution of scout values for a sample user. -0.4 -0.2 0 0.2 0.4 0.6 0.8 30000 60000 90000 120000 150000 180000 Popularity Pop*Var Figure 7: Correlation between aggregated scout value and item popularity (computed at different intervals). -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 30000 60000 90000 120000 150000 180000 Figure 8: Correlation between aggregated promoter value and user prolificity (computed at different intervals).",
                "Table 1: Movies forming the best scouts.",
                "Best Scouts Conf.",
                "Pop.",
                "Being John Malkovich (1999) 1.00 445 Star Wars: Episode IV - A New Hope (1977) 0.92 623 Princess Bride, The (1987) 0.85 477 Sixth Sense, The (1999) 0.85 617 Matrix, The (1999) 0.77 522 Ghostbusters (1984) 0.77 441 Casablanca (1942) 0.77 384 Insider, The (1999) 0.77 235 American Beauty (1999) 0.69 624 Terminator 2: Judgment Day (1991) 0.69 503 Fight Club (1999) 0.69 235 Shawshank Redemption, The (1994) 0.69 445 Run Lola Run (Lola rennt) (1998) 0.69 220 Terminator, The (1984) 0.62 450 Usual Suspects, The (1995) 0.62 326 Aliens (1986) 0.62 385 North by Northwest (1959) 0.62 245 Fugitive, The (1993) 0.62 402 End of Days (1999) 0.62 132 Raiders of the Lost Ark (1981) 0.54 540 Schindlers List (1993) 0.54 453 Back to the Future (1985) 0.54 543 Toy Story (1995) 0.54 419 Alien (1979) 0.54 415 Abyss, The (1989) 0.54 345 2001: A Space Odyssey (1968) 0.54 358 Dogma (1999) 0.54 228 Little Mermaid, The (1989) 0.54 203 Table 2: Movies forming the worst scouts.",
                "Worst scouts Conf.",
                "Pop.",
                "Harold and Maude (1971) 0.46 141 Grifters, The (1990) 0.46 180 Sting, The (1973) 0.38 244 Godfather: Part III, The (1990) 0.38 154 Lawrence of Arabia (1962) 0.38 167 High Noon (1952) 0.38 84 Women on the Verge of a... (1988) 0.38 113 Grapes of Wrath, The (1940) 0.38 115 Duck Soup (1933) 0.38 131 Arsenic and Old Lace (1944) 0.38 138 Midnight Cowboy (1969) 0.38 137 To Kill a Mockingbird (1962) 0.31 195 Four Weddings and a Funeral (1994) 0.31 271 Good, The Bad and The Ugly, The (1966) 0.31 156 Its a Wonderful Life (1946) 0.31 146 Player, The (1992) 0.31 220 Jackie Brown (1997) 0.31 118 Boat, The (Das Boot) (1981) 0.31 210 Manhattan (1979) 0.31 158 Truth About Cats & Dogs, The (1996) 0.31 143 Ghost (1990) 0.31 227 Lone Star (1996) 0.31 125 Big Chill, The (1983) 0.31 184 256 By studying the evolution of scout values, we can identify movies that consistently feature in good scouts over time.",
                "We claim these movies will make viable scouts for other users.",
                "We found the aggregated scout values for all movies in intervals of 10,000 ratings each.",
                "A movie is said to induce a good scout if the movie was in the top 100 of the sorted list, and to induce a bad scout if it was in bottom 100 of the same list.",
                "Movies appearing consistently high over time are expected to remain up there in the future.",
                "The effective confidence in a movie m is Cm = Tm − Bm N (8) where Tm is the number of times it appeared in the top 100, Bm the number of times it appeared in the bottom 100, and N is the number of intervals considered.",
                "Using this measure, the top few movies expected to induce the best scouts are shown in Table 1.",
                "Movies that would be bad scout choices are shown in Table 2 with their associated confidences.",
                "The popularities of the movies are also displayed.",
                "Although more popular movies appear in the list of good scouts, these tables show that a blind choice of scout based on popularity alone can be potentially dangerous.",
                "Interestingly, the best scout-Being John Malkovich-is about a puppeteer who discovers a portal into a movie star, a movie that has been described variously on amazon.com as makes you feel giddy, seriously weird, comedy with depth, silly, strange, and inventive.",
                "Indicating whether someone likes this movie or not goes a long way toward situating the user in a suitable neighborhood, with similar preferences.",
                "On the other hand, several factors may have made a movie a bad scout, like the sharp variance in user preferences in the neighborhood of a movie.",
                "Two users may have the same <br>opinion</br> about Lawrence of Arabia, but they may differ sharply about how they felt about the other movies they saw.",
                "Bad scouts ensue when there is deviation in behavior around a common synchronization point. 5.3 Inducing good promoters Ratings that serve to promote items in a collaborative filtering system are critical to allowing a new item be recommended to users.",
                "So, inducing good promoters is important for cold-start recommendation.",
                "We note that the frequency distribution of promoter values for a sample movies ratings is also Gaussian (similar to Fig. 6).",
                "This indicates that the promotion of a movie is benefited most by the ratings of a few users, and are unaffected by the ratings of most users.",
                "We find a strong correlation between a users number of ratings and his aggregated promoter value.",
                "Fig. 8 depicts the evolution of the Pearson correlation co-efficient between the prolificity of a user (number of ratings) versus his aggregated promoter value.",
                "We expect that conspicuous shills, by recommending wrong movies to users, will be discredited with negative aggregate promoter values and should be identifiable easily.",
                "Given this observation, the obvious rule to follow when introducing a new movie is to have it rated directly by prolific users who posses high aggregated promoter values.",
                "A new movie is thus cast into the neighborhood of many other movies improving its visibility.",
                "Note, though, that a user may have long stopped using the system.",
                "Tracking promoter values consistently allows only the most active recent users to be considered. 5.4 Inducing good connectors Given the way scouts, connectors, and promoters are characterized, it follows that the movies that are part of the best scouts are also part of the best connectors.",
                "Similarly, the users that constitute the best promoters are also part of the best connectors.",
                "Good connectors are induced by ensuring a user with a high promoter value rates a movie with a high scout value.",
                "In our experiments, we find that a ratings longest standing role is often as a connector.",
                "A rating with a poor connector value is often seen due to its user being a bad promoter, or its movie being a bad scout.",
                "Such ratings can be removed from the prediction process to bring marginal improvements to recommendations.",
                "In some selected experiments, we observed that removing a set of badly behaving connectors helped improve the systems overall performance by 1.5%.",
                "The effect was even higher on a few select users who observed an improvement of above 10% in precision without much loss in recall. 5.5 Monitoring the evolution of rating roles One of the more significant contributions of our work is the ability to model the evolution of recommender systems, by studying the changing roles of ratings over time.",
                "The role and value of a rating can change depending on many factors like user behavior, redundancy, shilling effects or properties of the collaborative filtering algorithm used.",
                "Studying the dynamics of rating roles in terms of transitions between good, bad, and negligible values can provide insights into the functioning of the recommender system.",
                "We believe that a continuous visualization of these transitions will improve the ability to manage a recommender system.",
                "We classify different rating states as good, bad, or negligible.",
                "Consider a user who has rated 100 movies in a particular interval, of which 20 are part of the test set.",
                "If a scout has a value greater than 0.005, it indicates that it is uniquely involved in at least 2 concordant predictions, which we will say is good.",
                "Thus, a threshold of 0.005 is chosen to bin a rating as good, bad or negligible in terms of its scout, connector and promoter value.",
                "For instance, a rating r, at time t with role value triple [0.1, 0.001, −0.01] is classified as [scout +, connector 0, promoter −], where + indicates good, 0 indicates negligible, and − indicates bad.",
                "The positive credit held by a rating is a measure of its contribution to the betterment of the system, and the discredit is a measure of its contribution to the detriment of the system.",
                "Even though the positive roles (and the negative roles) make up a very small percentage of all ratings, their contribution supersedes their size.",
                "For example, even though only 1.7% of all ratings were classified as good scouts, they hold 79% of all positive credit in the system!",
                "Similarly, the bad scouts were just 1.4% of all ratings but hold 82% of all discredit.",
                "Note that good and bad scouts, together, comprise only 1.4% + 1.7% = 3.1% of the ratings, implying that the majority of the ratings are negligible role players as scouts (more on this later).",
                "Likewise, good connectors were 1.2% of the system, and hold 30% of all positive credit.",
                "The bad connectors (0.8% of the system) hold 36% of all discredit.",
                "Good promoters (3% of the system) hold 46% of all credit, while bad promoters (2%) hold 50% of all discredit.",
                "This reiterates that a few ratings influence most of the systems performance.",
                "Hence it is important to track transitions between them regardless of their small numbers. 257 Across different snapshots, a rating can remain in the same state or change.",
                "A good scout can become a bad scout, a good promoter can become a good connector, good and bad scouts can become vestigial, and so on.",
                "It is not practical to expect a recommender system to have no ratings in bad roles.",
                "However, it suffices to see ratings in bad roles either convert to good or vestigial roles.",
                "Similarly, observing a large number of good roles become bad ones is a sign of imminent failure of the system.",
                "We employ the principle of non-overlapping episodes [6] to count such transitions.",
                "A sequence such as: [+, 0, 0] → [+, 0, 0] → [0, +, 0] → [0, 0, 0] is interpreted as the transitions [+, 0, 0] ; [0, +, 0] : 1 [+, 0, 0] ; [0, 0, 0] : 1 [0, +, 0] ; [0, 0, 0] : 1 instead of [+, 0, 0] ; [0, +, 0] : 2 [+, 0, 0] ; [0, 0, 0] : 2 [0, +, 0] ; [0, 0, 0] : 1.",
                "See [6] for further details about this counting procedure.",
                "Thus, a rating can be in one of 27 possible states, and there are about 272 possible transitions.",
                "We make a further simplification and utilize only 9 states, indicating whether the rating is a scout, promoter, or connector, and whether it has a positive, negative, or negligible role.",
                "Ratings that serve multiple purposes are counted using multiple episode instantiations but the states themselves are not duplicated beyond the 9 restricted states.",
                "In this model, a transition such as [+, 0, +] ; [0, +, 0] : 1 is counted as [scout+] ; [scout0] : 1 [scout+] ; [connector+] : 1 [scout+] ; [promoter0] : 1 [connector0] ; [scout0] : 1 [connector0] ; [scout+] : 1 [connector0] ; [promoter0] : 1 [promoter+] ; [scout0] : 1 [promoter+] ; [connector+] : 1 [promoter+] ; [promoter0] : 1 Of these, transitions like [pX] ; [q0] where p = q, X ∈ {+, 0, −} are considered uninteresting, and only the rest are counted.",
                "Fig. 9 depicts the major transitions counted while processing the first 200,000 ratings from the MovieLens dataset.",
                "Only transitions with frequency greater than or equal to 3% are shown.",
                "The percentages for each state indicates the number of ratings that were found to be in those states.",
                "We consider transitions from any state to a good state as healthy, from any state to a bad state as unhealthy, and from any state to a vestigial state as decaying.",
                "From Fig. 9, we can observe: • The bulk of the ratings have negligible values, irrespective of their role.",
                "The majority of the transitions involve both good and bad ratings becoming negligible.",
                "Scout + (2%) Scout(1.5%) Scout 0 (96.5%) Connector + (1.2%) Connector(0.8%) Connector 0 (98%) Promoter + (3%) Promoter(2%) Promoter 0 (95%) 84% 84% 81% 74% 10% 6% 11% 77% 8% 7% 8% 82% 4% 86% 4% 68% 15% 13% 5% 5% 77% 11% 7% 5% 4% 3% 3% 3% Healthy Unhealthy Decaying Figure 9: Transitions among rating roles. • The number of good ratings is comparable to the bad ratings, and ratings are seen to switch states often, except in the case of scouts (see below). • The negative and positive scout states are not reachable through any transition, indicating that these ratings must begin as such, and cannot be coerced into these roles. • Good promoters and good connectors have a much longer survival period than scouts.",
                "Transitions that recur to these states have frequencies of 10% and 15% when compared to just 4% for scouts.",
                "Good connectors are the slowest to decay whereas (good) scouts decay the fastest. • Healthy percentages are seen on transitions between promoters and connectors.",
                "As indicated earlier, there are hardly any transitions from promoters/connectors to scouts.",
                "This indicates that, over the long run, a users rating is more useful to others (movies or other users) than to the user himself. • The percentages of healthy transitions outweigh the unhealthy ones - this hints that the system is healthy, albeit only marginally.",
                "Note that these results are conditioned by the static nature of the dataset, which is a set of ratings over a fixed window of time.",
                "However a diagram such as Fig. 9 is clearly useful for monitoring the health of a recommender system.",
                "For instance, acceptable limits can be imposed on different types of transitions and, if a transition fails to meet the threshold, the recommender system or a part of it can be brought under closer scrutiny.",
                "Furthermore, the role state transition diagram would also be the ideal place to study the effects of shilling, a topic we will consider in future research. 5.6 Characterizing neighborhoods Earlier we saw that we can characterize the neighborhood of ratings involved in creating a recommendation list L for 258 a user.",
                "In our experiment, we consider lists of length 30, and sample the lists of about 5% of users through the evolution of the model (at intervals of 10,000 ratings each) and compute their neighborhood characteristics.",
                "To simplify our presentation, we consider the percentage of the sample that fall into one of the following categories: 1.",
                "Inactive user: (SFN(u) = 0) 2.",
                "Good scouts, Good neighborhood: (SFN(u) > 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 3.",
                "Good scouts, Bad neighborhood: (SFN(u) > 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) 4.",
                "Bad scouts, Good neighborhood: (SFN(u) < 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 5.",
                "Bad scouts, Bad neighborhood: (SFN(u) < 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) From our sample set of 561 users, we found that 476 users were inactive.",
                "Of the remaining 85 users, we found 26 users had good scouts and a good neighborhood, 6 had bad scouts and a good neighborhood, 29 had good scouts and a bad neighborhood, and 24 had bad scouts and a bad neighborhood.",
                "Thus, we conjecture that 59 users (29+24+6) are in danger of leaving the system.",
                "As a remedy, users with bad scouts and a good neighborhood can be asked to reconsider rating of some movies hoping to improve the systems recommendations.",
                "The system can be expected to deliver more if they engineer some good scouts.",
                "Users with good scouts and a bad neighborhood are harder to address; this situation might entail selectively removing some connector-promoter pairs that are causing the damage.",
                "Handling users with bad scouts and bad neighborhoods is a more difficult challenge.",
                "Such a classification allows the use of different strategies to better a users experience with the system depending on his context.",
                "In future work, we intend to conduct field studies and study the improvement in performance of different strategies for different contexts. 6.",
                "CONCLUSIONS To further recommender system acceptance and deployment, we require new tools and methodologies to manage an installed recommender and develop insights into the roles played by ratings.",
                "A fine-grained characterization in terms of rating roles such as scouts, promoters, and connectors, as done here, helps such an endeavor.",
                "Although we have presented results on only the item-based algorithm with list rank accuracy as the metric, the same approach outlined here applies to user-based algorithms and other metrics.",
                "In future research, we plan to systematically study the many algorithmic parameters, tolerances, and cutoff thresholds employed here and reason about their effects on the downstream conclusions.",
                "We also aim to extend our formulation to other collaborative filtering algorithms, study the effect of shilling in altering rating roles, conduct field studies, and evaluate improvements in user experience by tweaking ratings based on their role values.",
                "Finally, we plan to develop the idea of mining the evolution of rating role patterns into a reporting and tracking system for all aspects of recommender system health. 7.",
                "REFERENCES [1] Cosley, D., Lam, S., Albert, I., Konstan, J., and Riedl, J.",
                "Is Seeing Believing? : How Recommender System Interfaces Affect Users Opinions.",
                "In Proc.",
                "CHI (2001), pp. 585-592. [2] Herlocker, J. L., Konstan, J.",
                "A., Borchers, A., and Riedl, J.",
                "An Algorithmic Framework for Performing Collaborative Filtering.",
                "In Proc.",
                "SIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J.",
                "A., Terveen, L. G., and Riedl, J. T. Evaluating Collaborative Filtering Recommender Systems.",
                "ACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J.",
                "A.",
                "Personal communication. 2003. [5] Lam, S. K., and Riedl, J. Shilling Recommender Systems for Fun and Profit.",
                "In Proceedings of the 13th International World Wide Web Conference (2004), ACM Press, pp. 393-402. [6] Laxman, S., Sastry, P. S., and Unnikrishnan, K. P. Discovering Frequent Episodes and Learning Hidden Markov Models: A Formal Connection.",
                "IEEE Transactions on Knowledge and Data Engineering Vol. 17, 11 (2005), 1505-1517. [7] McLaughlin, M. R., and Herlocker, J. L. A Collaborative Filtering Algorithm and Evaluation Metric that Accurately Model the User Experience.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (2004), pp. 329 - 336. [8] OMahony, M., Hurley, N. J., Kushmerick, N., and Silvestre, G. Collaborative Recommendation: A Robustness Analysis.",
                "ACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J.",
                "A., and Riedl, J.",
                "Getting to Know You: Learning New User Preferences in Recommender Systems.",
                "In Proceedings of the 2002 Conference on Intelligent User Interfaces (IUI 2002) (2002), pp. 127-134. [10] Rashid, A. M., Karypis, G., and Riedl, J.",
                "Influence in Ratings-Based Recommender Systems: An Algorithm-Independent Approach.",
                "In Proc. of the SIAM International Conference on Data Mining (2005). [11] Resnick, P., Iacovou, N., Sushak, M., Bergstrom, P., and Riedl, J. GroupLens: An Open Architecture for Collaborative Filtering of Netnews.",
                "In Proceedings of the Conference on Computer Supported Collaborative Work (CSCW94) (1994), ACM Press, pp. 175-186. [12] Sarwar, B., Karypis, G., Konstan, J., and Reidl, J. Item-Based Collaborative Filtering Recommendation Algorithms.",
                "In Proceedings of the Tenth International World Wide Web Conference (WWW10) (2001), pp. 285-295. [13] Schein, A., Popescu, A., Ungar, L., and Pennock, D. Methods and Metrics for Cold-Start Recommendation.",
                "In Proc.",
                "SIGIR (2002), pp. 253-260. 259"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Dos usuarios pueden tener la misma \"opinión\" sobre Lawrence of Arabia, pero pueden diferir considerablemente sobre cómo se sintieron sobre las otras películas que vieron."
            ],
            "translated_text": "",
            "candidates": [
                "opinión",
                "opinión"
            ],
            "error": []
        },
        "scout": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Scouts, Promoters, and Connectors: The Roles of Ratings in Nearest Neighbor Collaborative Filtering Bharath Kumar Mohan Dept.",
                "of CSA Indian Institute of Science Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. of Computer Science Eastern Michigan University Ypsilanti, MI 48917, USA bkeller@emich.edu Naren Ramakrishnan Dept. of Computer Science Virginia Tech, Blacksburg VA 24061, USA naren@cs.vt.edu ABSTRACT Recommender systems aggregate individual user ratings into predictions of products or services that might interest visitors.",
                "The quality of this aggregation process crucially affects the user experience and hence the effectiveness of recommenders in e-commerce.",
                "We present a novel study that disaggregates global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearestneighbor collaborative filtering.",
                "In particular, we formulate three roles-scouts, promoters, and connectors-that capture how users receive recommendations, how items get recommended, and how ratings of these two types are themselves connected (resp.).",
                "These roles find direct uses in improving recommendations for users, in better targeting of items and, most importantly, in helping monitor the health of the system as a whole.",
                "For instance, they can be used to track the evolution of neighborhoods, to identify rating subspaces that do not contribute (or contribute negatively) to system performance, to enumerate users who are in danger of leaving, and to assess the susceptibility of the system to attacks such as shilling.",
                "We argue that the three rating roles presented here provide broad primitives to manage a recommender system and its community.",
                "Categories and Subject Descriptors H.4.2 [Information Systems Applications]: Types of Systems-Decision support; J.4 [Computer Applications]: Social and Behavioral Sciences General Terms Algorithms, Human Factors 1.",
                "INTRODUCTION Recommender systems have become integral to e-commerce, providing technology that suggests products to a visitor based on previous purchases or rating history.",
                "Collaborative filtering, a common form of recommendation, predicts a users rating for an item by combining (other) ratings of that user with other users ratings.",
                "Significant research has been conducted in implementing fast and accurate collaborative filtering algorithms [2, 7], designing interfaces for presenting recommendations to users [1], and studying the robustness of these algorithms [8].",
                "However, with the exception of a few studies on the influence of users [10], little attention has been paid to unraveling the inner workings of a recommender in terms of the individual ratings and the roles they play in making (good) recommendations.",
                "Such an understanding will give an important handle to monitoring and managing a recommender system, to engineer mechanisms to sustain the recommender, and thereby ensure its continued success.",
                "Our motivation here is to disaggregate global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearest-neighbor collaborative filtering.",
                "We identify three possible roles: (scouts) to connect the user into the system to receive recommendations, (promoters) to connect an item into the system to be recommended, and (connectors) to connect ratings of these two kinds.",
                "Viewing ratings in this way, we can define the contribution of a rating in each role, both in terms of allowing recommendations to occur, and in terms of influence on the quality of recommendations.",
                "In turn, this capability helps support scenarios such as: 1.",
                "Situating users in better neighborhoods: A users ratings may inadvertently connect the user to a neighborhood for which the users tastes may not be a perfect match.",
                "Identifying ratings responsible for such bad recommendations and suggesting new items to rate can help situate the user in a better neighborhood. 2.",
                "Targeting items: Recommender systems suffer from lack of user participation, especially in cold-start scenarios [13] involving newly arrived items.",
                "Identifying users who can be encouraged to rate specific items helps ensure coverage of the recommender system. 3.",
                "Monitoring the evolution of the recommender system and its stakeholders: A recommender system is constantly under change: growing with new users and 250 items, shrinking with users leaving the system, items becoming irrelevant, and parts of the system under attack.",
                "Tracking the roles of a rating and its evolution over time provides many insights into the health of the system, and how it could be managed and improved.",
                "These include being able to identify rating subspaces that do not contribute (or contribute negatively) to system performance, and could be removed; to enumerate users who are in danger of leaving, or have left the system; and to assess the susceptibility of the system to attacks such as shilling [5].",
                "As we show, the characterization of rating roles presented here provides broad primitives to manage a recommender system and its community.",
                "The rest of the paper is organized as follows.",
                "Background on nearest-neighbor collaborative filtering and algorithm evaluation is discussed in Section 2.",
                "Section 3 defines and discusses the roles of a rating, and Section 4 defines measures of the contribution of a rating in each of these roles.",
                "In Section 5, we illustrate the use of these roles to address the goals outlined above. 2.",
                "BACKGROUND 2.1 Algorithms Nearest-neighbor collaborative filtering algorithms either use neighborhoods of users or neighborhoods of items to compute a prediction.",
                "An algorithm of the first kind is called user-based, and one of the second kind is called itembased [12].",
                "In both families of algorithms, neighborhoods are formed by first computing the similarity between all pairs of users (for user-based) or items (for item-based).",
                "Predictions are then computed by aggregating ratings, which in a user-based algorithm involves aggregating the ratings of the target item by the users neighbors and, in an item-based algorithm, involves aggregating the users ratings of items that are neighbors of the target item.",
                "Algorithms within these families differ in the definition of similarity, formation of neighborhoods, and the computation of predictions.",
                "We consider a user-based algorithm based on that defined for GroupLens [11] with variations from Herlocker et al. [2], and an item-based algorithm similar to that of Sarwar et al. [12].",
                "The algorithm used by Resnick et al. [11] defines the similarity of two users u and v as the Pearson correlation of their common ratings: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2 , where Iu is the set of items rated by user u, ru,i is user us rating for item i, and ¯ru is the average rating of user u (similarly for v).",
                "Similarity computed in this manner is typically scaled by a factor proportional to the number of common ratings, to reduce the chance of making a recommendation made on weak connections: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), where γ ≈ 5 is a constant used as a lower limit in scaling [2].",
                "These new similarities are then used to define a static neighborhood Nu for each user u consisting of the top K users most similar to user u.",
                "A prediction for user u and item i is computed by a weighted average of the ratings by the neighbors pu,i = ¯ru + P v∈V sim (u, v)(rv,i − ¯rv) P v∈V sim (u, v) (1) where V = Nu ∩ Ui is the set of users most similar to u who have rated i.",
                "The item-based algorithm we use is the one defined by Sarwar et al. [12].",
                "In this algorithm, similarity is defined as the adjusted cosine measure sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) where Ui is the set of users who have rated item i.",
                "As for the user-based algorithm, the similarity weights are adjusted proportionally to the number of users that have rated the items in common sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). (3) Given the similarities, the neighborhood Ni of an item i is defined as the top K most similar items for that item.",
                "A prediction for user u and item i is computed as the weighted average pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) where J = Ni ∩ Iu is the set of items rated by u that are most similar to i. 2.2 Evaluation Recommender algorithms have typically been evaluated using measures of predictive accuracy and coverage [3].",
                "Studies on recommender algorithms, notably Herlocker et al. [2] and Sarwar et al. [12], typically compute predictive accuracy by dividing a set of ratings into training and test sets, and compute the prediction for an item in the test set using the ratings in the training set.",
                "A standard measure of predictive accuracy is mean absolute error (MAE), which for a test set T = {(u, i)} is defined as, MAE = P (u,i)∈T |pu,i − ru,i| |T | . (5) Coverage has a number of definitions, but generally refers to the proportion of items that can be predicted by the algorithm [3].",
                "A practical issue with predictive accuracy is that users typically are presented with recommendation lists, and not individual numeric predictions.",
                "Recommendation lists are lists of items in decreasing order of prediction (sometimes stated in terms of star-ratings), and so predictive accuracy may not be reflective of the accuracy of the list.",
                "So, instead we can measure recommendation or rank accuracy, which indicates the extent to which the list is in the correct order.",
                "Herlocker et al. [3] discuss a number of rank accuracy measures, which range from Kendalls Tau to measures that consider the fact that users tend to only look at a prefix of the list [5].",
                "Kendalls Tau measures the number of inversions when comparing ordered pairs in the true user ordering of 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figure 1: Ratings in simple movie recommender. items and the recommended order, and is defined as τ = C − D p (C + D + TR)(C + D + TP) (6) where C is the number of pairs that the system predicts in the correct order, D the number of pairs the system predicts in the wrong order, TR the number of pairs in the true ordering that have the same ratings, and TP is the number of pairs in the predicted ordering that have the same ratings [3].",
                "A shortcoming of the Tau metric is that it is oblivious to the position in the ordered list where the inversion occurs [3].",
                "For instance, an inversion toward the end of the list is given the same weight as one in the beginning.",
                "One solution is to consider inversions only in the top few items in the recommended list or to weight inversions based on their position in the list. 3.",
                "ROLES OF A RATING Our basic observation is that each rating plays a different role in each prediction in which it is used.",
                "Consider a simplified movie recommender system with three users Jim, Jeff, and Tom and their ratings for a few movies, as shown in Fig. 1. (For this initial discussion we will not consider the rating values involved.)",
                "The recommender predicts whether Tom will like The Mask using the other already available ratings.",
                "How this is done depends on the algorithm: 1.",
                "An item-based collaborative filtering algorithm constructs a neighborhood of movies around The Mask by using the ratings of users who rated The Mask and other movies similarly (e.g., Jims ratings of The Matrix and The Mask; and Jeffs ratings of Star Wars and The Mask).",
                "Toms ratings of those movies are then used to make a prediction for The Mask. 2.",
                "A user-based collaborative filtering algorithm would construct a neighborhood around Tom by tracking other users whose rating behaviors are similar to Toms (e.g., Tom and Jeff have rated Star Wars; Tom and Jim have rated The Matrix).",
                "The prediction of Toms rating for The Mask is then based on the ratings of Jeff and Tim.",
                "Although the nearest-neighbor algorithms aggregate the ratings to form neighborhoods used to compute predictions, we can disaggregate the similarities to view the computation of a prediction as simultaneously following parallel paths of ratings.",
                "So, irrespective of the collaborative filtering algorithm used, we can visualize the prediction of Toms rating of The Mask as walking through a sequence of ratings.",
                "In Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figure 2: Ratings used to predict The Mask for Tom.",
                "Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figure 3: Prediction of The Mask for Tom in which a rating is used more than once. this example, two paths were used for this prediction as depicted in Fig. 2: (p1, p2, p3) and (q1, q2, q3).",
                "Note that these paths are undirected, and are all of length 3.",
                "Only the order in which the ratings are traversed is different between the item-based algorithm (e.g., (p3, p2, p1), (q3, q2, q1)) and the user-based algorithm (e.g., (p1, p2, p3), (q1, q2, q3).)",
                "A rating can be part of many paths for a single prediction as shown in Fig. 3, where three paths are used for a prediction, two of which follow p1: (p1, p2, p3) and (p1, r2, r3).",
                "Predictions in a collaborative filtering algorithms may involve thousands of such walks in parallel, each playing a part in influencing the predicted value.",
                "Each prediction path consists of three ratings, playing roles that we call scouts, promoters, and connectors.",
                "To illustrate these roles, consider the path (p1, p2, p3) in Fig. 2 used to make a prediction of The Mask for Tom: 1.",
                "The rating p1 (Tom → Star Wars) makes a connection from Tom to other ratings that can be used to predict Toms rating for The Mask.",
                "This rating serves as a <br>scout</br> in the bipartite graph of ratings to find a path that leads to The Mask. 2.",
                "The rating p2 (Jeff → Star Wars) helps the system recommend The Mask to Tom by connecting the <br>scout</br> to the promoter. 3.",
                "The rating p3 (Jeff → The Mask) allows connections to The Mask, and, therefore, promotes this movie to Tom.",
                "Formally, given a prediction pu,a of a target item a for user u, a <br>scout</br> for pu,a is a rating ru,i such that there exists a user v with ratings rv,a and rv,i for some item i; a promoter for pu,a is a rating rv,a for some user v, such that there exist ratings rv,i and ru,i for an item i, and; a connector for pu,a 252 Jim Tom Jeff Jerry My Cousin Vinny The Matrix Star Wars The Mask Jurasic Park Figure 4: Scouts, promoters, and connectors. is a rating rv,i by some user v and rating i, such that there exists ratings ru,i and rv,a.",
                "The scouts, connectors, and promoters for the prediction of Toms rating of The Mask are p1 and q1, p2 and q2, and p3 and q3 (respectively).",
                "Each of these roles has a value in the recommender to the user, the users neighborhood, and the system in terms of allowing recommendations to be made. 3.1 Roles in Detail Ratings that act as scouts tend to help the recommender system suggest more movies to the user, though the extent to which this is true depends on the rating behavior of other users.",
                "For example, in Fig. 4 the rating Tom → Star Wars helps the system recommend only The Mask to him, while Tom → The Matrix helps recommend The Mask, Jurassic Park, and My Cousin Vinny.",
                "Tom makes a connection to Jim who is a prolific user of the system, by rating The Matrix.",
                "However, this does not make The Matrix the best movie to rate for everyone.",
                "For example, Jim is benefited equally by both The Mask and The Matrix, which allow the system to recommend Star Wars to him.",
                "His rating of The Mask is the best <br>scout</br> for Jeff, and Jerrys only <br>scout</br> is his rating of Star Wars.",
                "This suggests that good scouts allow a user to build similarity with prolific users, and thereby ensure they get more from the system.",
                "While scouts represent beneficial ratings from the perspective of a user, promoters are their duals, and are of benefit to items.",
                "In Fig. 4, My Cousin Vinny benefits from Jims rating, since it allows recommendations to Jeff and Tom.",
                "The Mask is not so dependent on just one rating, since the ratings by Jim and Jeff help it.",
                "On the other hand, Jerrys rating of Star Wars does not help promote it to any other user.",
                "We conclude that a good promoter connects an item to a broader neighborhood of other items, and thereby ensures that it is recommended to more users.",
                "Connectors serve a crucial role in a recommender system that is not as obvious.",
                "The movies My Cousin Vinny and Jurassic Park have the highest recommendation potential since they can be recommended to Jeff, Jerry and Tom based on the linkage structure illustrated in Fig. 4.",
                "Beside the fact that Jim rated these movies, these recommendations are possible only because of the ratings Jim → The Matrix and Jim → The Mask, which are the best connectors.",
                "A connector improves the systems ability to make recommendations with no explicit gain for the user.",
                "Note that every rating can be of varied benefit in each of these roles.",
                "The rating Jim → My Cousin Vinny is a poor <br>scout</br> and connector, but is a very good promoter.",
                "The rating Jim → The Mask is a reasonably good <br>scout</br>, a very good connector, and a good promoter.",
                "Finally, the rating Jerry → Star Wars is a very good <br>scout</br>, but is of no value as a connector or promoter.",
                "As illustrated here, a rating can have different value in each of the three roles in terms of whether a recommendation can be made or not.",
                "We could measure this value by simply counting the number of times a rating is used in each role, which alone would be helpful in characterizing the behavior of a system.",
                "But we can also measure the contribution of each rating to the quality of recommendations or health of the system.",
                "Since every prediction is a combined effort of several recommendation paths, we are interested in discerning the influence of each rating (and, hence, each path) in the system towards the systems overall error.",
                "We can understand the dynamics of the system at a finer granularity by tracking the influence of a rating according to the role played.",
                "The next section describes the approach to measuring the values of a rating in each role. 4.",
                "CONTRIBUTIONS OF RATINGS As weve seen, a rating may play different roles in different predictions and, in each prediction, contribute to the quality of a prediction in different ways.",
                "Our approach can use any numeric measure of a property of system health, and assigns credit (or blame) to each rating proportional to its influence in the prediction.",
                "By tracking the role of each rating in a prediction, we can accumulate the credit for a rating in each of the three roles, and also track the evolution of the roles of rating over time in the system.",
                "This section defines the methodology for computing the contribution of ratings by first defining the influence of a rating, and then instantiating the approach for predictive accuracy, and then rank accuracy.",
                "We also demonstrate how these contributions can be aggregated to study the neighborhood of ratings involved in computing a users recommendations.",
                "Note that although our general formulation for rating influence is algorithm independent, due to space considerations, we present the approach for only item-based collaborative filtering.",
                "The definition for user-based algorithms is similar and will be presented in an expanded version of this paper. 4.1 Influence of Ratings Recall that an item-based approach to collaborative filtering relies on building item neighborhoods using the similarity of ratings by the same user.",
                "As described earlier, similarity is defined by the adjusted cosine measure (Equations (2) and (3)).",
                "A set of the top K neighbors is maintained for all items for space and computational efficiency.",
                "A prediction of item i for a user u is computed as the weighted deviation from the items mean rating as shown in Equation (4).",
                "The list of recommendations for a user is then the list of items sorted in descending order of their predicted values.",
                "We first define impact(a, i, j), the impact a user a has in determining the similarity between two items i and j.",
                "This is the change in the similarity between i and j when as rating is removed, and is defined as impact(a, i, j) = |sim (i, j) − sim¯a(i, j)| P w∈Cij |sim (i, j) − sim ¯w(i, j)| where Cij = {u ∈ U | ∃ ru,i, ru,j ∈ R(u)} is the set of coraters 253 of items i and j (users who rate both i and j), R(u) is the set of ratings provided by user u, and sim¯a(i, j) is the similarity of i and j when the ratings of user a are removed sim¯a(i, j) = P v∈U\\{a} (ru,i − ¯ru)(ru,j − ¯ru) qP u∈U\\{a}(ru,i − ¯ru)2 qP u∈U\\{a}(ru,j − ¯ru)2 , and adjusted for the number of raters sim¯a(i, j) = max(|Ui ∩ Uj| − 1, γ) γ · sim(i, j).",
                "If all coraters of i and j rate them identically, we define the impact as impact(a, i, j) = 1 |Cij| since P w∈Cij |sim (i, j) − sim ¯w(i, j)| = 0.",
                "The influence of each path (u, j, v, i) = [ru,j, rv,j, rv,i] in the prediction of pu,i is given by influence(u, j, v, i) = sim (i, j) P l∈Ni∩Iu sim (i, l) · impact(v, i, j) It follows that the sum of influences over all such paths, for a given set of endpoints, is 1. 4.2 Role Values for Predictive Accuracy The value of a rating in each role is computed from the influence depending on the evaluation measure employed.",
                "Here we illustrate the approach using predictive accuracy as the evaluation metric.",
                "In general, the goodness of a prediction decides whether the ratings involved must be credited or discredited for their role.",
                "For predictive accuracy, the error in prediction e = |pu,i − ru,i| is mapped to a comfort level using a mapping function M(e).",
                "Anecdotal evidence suggests that users are unable to discern errors less than 1.0 (for a rating scale of 1 to 5) [4], and so an error less than 1.0 is considered acceptable, but anything larger is not.",
                "We hence define M(e) as (1 − e) binned to an appropriate value in [−1, −0.5, 0.5, 1].",
                "For each prediction pu,i, M(e) is attributed to all the paths that assisted the computation of pu,i, proportional to their influences.",
                "This tribute, M(e)∗influence(u, j, v, i), is in turn inherited by each of the ratings in the path [ru,j, rv,j, rv,i], with the credit/blame accumulating to the respective roles of ru,j as a <br>scout</br>, rv,j as a connector, and rv,i as a promoter.",
                "In other words, the <br>scout</br> value SF(ru,j), the connector value CF(rv,j) and the promoter value PF(rv,i) are all incremented by the tribute amount.",
                "Over a large number of predictions, scouts that have repeatedly resulted in big error rates have a big negative <br>scout</br> value, and vice versa (similarly with the other roles).",
                "Every rating is thus summarized by its triple [SF, CF, PF]. 4.3 Role Values for Rank Accuracy We now define the computation of the contribution of ratings to observed rank accuracy.",
                "For this computation, we must know the users preference order for a set of items for which predictions can be computed.",
                "We assume that we have a test set of the users ratings of the items presented in the recommendation list.",
                "For every pair of items rated by a user in the test data, we check whether the predicted order is concordant with his preference.",
                "We say a pair (i, j) is concordant (with error ) whenever one of the following holds: • if (ru,i < ru,j) then (pu,i − pu,j < ); • if (ru,i > ru,j) then (pu,i − pu,j > ); or • if (ru,i = ru,j) then (|pu,i − pu,j| ≤ ).",
                "Similarly, a pair (i, j) is discordant (with error ) if it is not concordant.",
                "Our experiments described below use an error tolerance of = 0.1.",
                "All paths involved in the prediction of the two items in a concordant pair are credited, and the paths involved in a discordant pair are discredited.",
                "The credit assigned to a pair of items (i, j) in the recommendation list for user u is computed as c(i, j) = ( t T · 1 C+D if (i, j) are concordant − t T · 1 C+D if (i, j) are discordant (7) where t is the number of items in the users test set whose ratings could be predicted, T is the number of items rated by user u in the test set, C is the number of concordances and D is the number of discordances.",
                "The credit c is then divided among all paths responsible for predicting pu,i and pu,j proportional to their influences.",
                "We again add the role values obtained from all the experiments to form a triple [SF, CF, PF] for each rating. 4.4 Aggregating rating roles After calculating the role values for individual ratings, we can also use these values to study neighborhoods and the system.",
                "Here we consider how we can use the role values to characterize the health of a neighborhood.",
                "Consider the list of top recommendations presented to a user at a specific point in time.",
                "The collaborative filtering algorithm traversed many paths in his neighborhood through his scouts and other connectors and promoters to make these recommendations.",
                "We call these ratings the recommender neighborhood of the user.",
                "The user implicitly chooses this neighborhood of ratings through the items he rates.",
                "Apart from the collaborative filtering algorithm, the health of this neighborhood completely influences a users satisfaction with the system.",
                "We can characterize a users recommender neighborhood by aggregating the individual role values of the ratings involved, weighted by the influence of individual ratings in determining his recommended list.",
                "Different sections of the users neighborhood wield varied influence on his recommendation list.",
                "For instance, ratings reachable through highly rated items have a bigger say in the recommended items.",
                "Our aim is to study the system and classify users with respect to their positioning in a healthy or unhealthy neighborhood.",
                "A user can have a good set of scouts, but may be exposed to a neighborhood with bad connectors and promoters.",
                "He can have a good neighborhood, but his bad scouts may ensure the neighborhoods potential is rendered useless.",
                "We expect that users with good scouts and good neighborhoods will be most satisfied with the system in the future.",
                "A users neighborhood is characterized by a triple that represents the weighted sum of the role values of individual ratings involved in making recommendations.",
                "Consider a user u and his ordered list of recommendations L. An item i 254 in the list is weighted inversely, as K(i), depending on its position in the list.",
                "In our studies we use K(i) = p position(i).",
                "Several paths of ratings [ru,j, rv,j, rv,i] are involved in predicting pu,i which ultimately decides its position in L, each with influence(u, j, v, i).",
                "The recommender neighborhood of a user u is characterized by the triple, [SFN(u), CFN(u), PFN(u)] where SFN(u) = X i∈L P [ru,j ,rv,j ,rv,i] SF(ru,j)influence(u, j, v, i) K(i) !",
                "CFN(u) and PFN(u) are defined similarly.",
                "This triple estimates the quality of us recommendations based on the past track record of the ratings involved in their respective roles. 5.",
                "EXPERIMENTATION As we have seen, we can assign role values to each rating when evaluating a collaborative filtering system.",
                "In this section, we demonstrate the use of this approach to our overall goal of defining an approach to monitor and manage the health of a recommender system through experiments done on the MovieLens million rating dataset.",
                "In particular, we discuss results relating to identifying good scouts, promoters, and connectors; the evolution of rating roles; and the characterization of user neighborhoods. 5.1 Methodology Our experiments use the MovieLens million rating dataset, which consists of ratings by 6040 users of 3952 movies.",
                "The ratings are in the range 1 to 5, and are labeled with the time the rating was given.",
                "As discussed before, we consider only the item-based algorithm here (with item neighborhoods of size 30) and, due to space considerations, only present role value results for rank accuracy.",
                "Since we are interested in the evolution of the rating role values over time, the model of the recommender system is built by processing ratings in their arrival order.",
                "The timestamping provided by MovieLens is hence crucial for the analyses presented here.",
                "We make assessments of rating roles at intervals of 10,000 ratings and processed the first 200,000 ratings in the dataset (giving rise to 20 snapshots).",
                "We incrementally update the role values as the time ordered ratings are merged into the model.",
                "To keep the experiment computationally manageable, we define a test dataset for each user.",
                "As the time ordered ratings are merged into the model, we label a small randomly selected percentage (20%) as test data.",
                "At discrete epochs, i.e., after processing every 10,000 ratings, we compute the predictions for the ratings in the test data, and then compute the role values for the ratings used in the predictions.",
                "One potential criticism of this methodology is that the ratings in the test set are never evaluated for their roles.",
                "We overcome this concern by repeating the experiment, using different random seeds.",
                "The probability that every rating is considered for evaluation is then considerably high: 1 − 0.2n , where n is the number of times the experiment is repeated with different random seeds.",
                "The results here are based on n = 4 repetitions.",
                "The item-based collaborative filtering algorithms performance was ordinary with respect to rank accuracy.",
                "Fig. 5 shows a plot of the precision and recall as ratings were merged in time order into the model.",
                "The recall was always high, but the average precision was just about 53%. 0 0.2 0.4 0.6 0.8 1 1.2 10000 30000 50000 70000 90000110000130000150000 Ratings merged into model Value Precision Recall Figure 5: Precision and recall for the item-based collaborative filtering algorithm. 5.2 Inducing good scouts The ratings of a user that serve as scouts are those that allow the user to receive recommendations.",
                "We claim that users with ratings that have respectable <br>scout</br> values will be happier with the system than those with ratings with low <br>scout</br> values.",
                "Note that the item-based algorithm discussed here produces recommendation lists with nearly half of the pairs in the list discordant from the users preference.",
                "Whether all of these discordant pairs are observable by the user is unclear, however, certainly this suggests that there is a need to be able to direct users to items whose ratings would improve the lists.",
                "The distribution of the <br>scout</br> values for most users ratings are Gaussian with mean zero.",
                "Fig. 6 shows the frequency distribution of <br>scout</br> values for a sample user at a given snapshot.",
                "We observe that a large number of ratings never serve as scouts for their users.",
                "A relatable scenario is when Amazons recommender makes suggestions of books or items based on other items that were purchased as gifts.",
                "With simple relevance feedback from the user, such ratings can be isolated as bad scouts and discounted from future predictions.",
                "Removing bad scouts was found to be worthwhile for individual users but the overall performance improvement was only marginal.",
                "An obvious question is whether good scouts can be formed by merely rating popular movies as suggested by Rashid et al. [9].",
                "They show that a mix of popularity and rating entropy identifies the best items to suggest to new users when evaluated using MAE.",
                "Following their intuition, we would expect to see a higher correlation between popularityentropy and good scouts.",
                "We measured the Pearson correlation coefficient between aggregated <br>scout</br> values for a movie with the popularity of a movie (number of times it is rated); and with its popularity*variance measure at different snapshots of the system.",
                "Note that the <br>scout</br> values were initially anti-correlated with popularity (Fig. 7), but became moderately correlated as the system evolved.",
                "Both popularity and popularity*variance performed similarly.",
                "A possible explanation is that there has been insufficient time for the popular movies to accumulate ratings. 255 -10 0 10 20 30 40 50 60 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 <br>scout</br> Value Frequency Figure 6: Distribution of <br>scout</br> values for a sample user. -0.4 -0.2 0 0.2 0.4 0.6 0.8 30000 60000 90000 120000 150000 180000 Popularity Pop*Var Figure 7: Correlation between aggregated scout value and item popularity (computed at different intervals). -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 30000 60000 90000 120000 150000 180000 Figure 8: Correlation between aggregated promoter value and user prolificity (computed at different intervals).",
                "Table 1: Movies forming the best scouts.",
                "Best Scouts Conf.",
                "Pop.",
                "Being John Malkovich (1999) 1.00 445 Star Wars: Episode IV - A New Hope (1977) 0.92 623 Princess Bride, The (1987) 0.85 477 Sixth Sense, The (1999) 0.85 617 Matrix, The (1999) 0.77 522 Ghostbusters (1984) 0.77 441 Casablanca (1942) 0.77 384 Insider, The (1999) 0.77 235 American Beauty (1999) 0.69 624 Terminator 2: Judgment Day (1991) 0.69 503 Fight Club (1999) 0.69 235 Shawshank Redemption, The (1994) 0.69 445 Run Lola Run (Lola rennt) (1998) 0.69 220 Terminator, The (1984) 0.62 450 Usual Suspects, The (1995) 0.62 326 Aliens (1986) 0.62 385 North by Northwest (1959) 0.62 245 Fugitive, The (1993) 0.62 402 End of Days (1999) 0.62 132 Raiders of the Lost Ark (1981) 0.54 540 Schindlers List (1993) 0.54 453 Back to the Future (1985) 0.54 543 Toy Story (1995) 0.54 419 Alien (1979) 0.54 415 Abyss, The (1989) 0.54 345 2001: A Space Odyssey (1968) 0.54 358 Dogma (1999) 0.54 228 Little Mermaid, The (1989) 0.54 203 Table 2: Movies forming the worst scouts.",
                "Worst scouts Conf.",
                "Pop.",
                "Harold and Maude (1971) 0.46 141 Grifters, The (1990) 0.46 180 Sting, The (1973) 0.38 244 Godfather: Part III, The (1990) 0.38 154 Lawrence of Arabia (1962) 0.38 167 High Noon (1952) 0.38 84 Women on the Verge of a... (1988) 0.38 113 Grapes of Wrath, The (1940) 0.38 115 Duck Soup (1933) 0.38 131 Arsenic and Old Lace (1944) 0.38 138 Midnight Cowboy (1969) 0.38 137 To Kill a Mockingbird (1962) 0.31 195 Four Weddings and a Funeral (1994) 0.31 271 Good, The Bad and The Ugly, The (1966) 0.31 156 Its a Wonderful Life (1946) 0.31 146 Player, The (1992) 0.31 220 Jackie Brown (1997) 0.31 118 Boat, The (Das Boot) (1981) 0.31 210 Manhattan (1979) 0.31 158 Truth About Cats & Dogs, The (1996) 0.31 143 Ghost (1990) 0.31 227 Lone Star (1996) 0.31 125 Big Chill, The (1983) 0.31 184 256 By studying the evolution of <br>scout</br> values, we can identify movies that consistently feature in good scouts over time.",
                "We claim these movies will make viable scouts for other users.",
                "We found the aggregated <br>scout</br> values for all movies in intervals of 10,000 ratings each.",
                "A movie is said to induce a good <br>scout</br> if the movie was in the top 100 of the sorted list, and to induce a bad <br>scout</br> if it was in bottom 100 of the same list.",
                "Movies appearing consistently high over time are expected to remain up there in the future.",
                "The effective confidence in a movie m is Cm = Tm − Bm N (8) where Tm is the number of times it appeared in the top 100, Bm the number of times it appeared in the bottom 100, and N is the number of intervals considered.",
                "Using this measure, the top few movies expected to induce the best scouts are shown in Table 1.",
                "Movies that would be bad <br>scout</br> choices are shown in Table 2 with their associated confidences.",
                "The popularities of the movies are also displayed.",
                "Although more popular movies appear in the list of good scouts, these tables show that a blind choice of <br>scout</br> based on popularity alone can be potentially dangerous.",
                "Interestingly, the best <br>scout</br>-Being John Malkovich-is about a puppeteer who discovers a portal into a movie star, a movie that has been described variously on amazon.com as makes you feel giddy, seriously weird, comedy with depth, silly, strange, and inventive.",
                "Indicating whether someone likes this movie or not goes a long way toward situating the user in a suitable neighborhood, with similar preferences.",
                "On the other hand, several factors may have made a movie a bad <br>scout</br>, like the sharp variance in user preferences in the neighborhood of a movie.",
                "Two users may have the same opinion about Lawrence of Arabia, but they may differ sharply about how they felt about the other movies they saw.",
                "Bad scouts ensue when there is deviation in behavior around a common synchronization point. 5.3 Inducing good promoters Ratings that serve to promote items in a collaborative filtering system are critical to allowing a new item be recommended to users.",
                "So, inducing good promoters is important for cold-start recommendation.",
                "We note that the frequency distribution of promoter values for a sample movies ratings is also Gaussian (similar to Fig. 6).",
                "This indicates that the promotion of a movie is benefited most by the ratings of a few users, and are unaffected by the ratings of most users.",
                "We find a strong correlation between a users number of ratings and his aggregated promoter value.",
                "Fig. 8 depicts the evolution of the Pearson correlation co-efficient between the prolificity of a user (number of ratings) versus his aggregated promoter value.",
                "We expect that conspicuous shills, by recommending wrong movies to users, will be discredited with negative aggregate promoter values and should be identifiable easily.",
                "Given this observation, the obvious rule to follow when introducing a new movie is to have it rated directly by prolific users who posses high aggregated promoter values.",
                "A new movie is thus cast into the neighborhood of many other movies improving its visibility.",
                "Note, though, that a user may have long stopped using the system.",
                "Tracking promoter values consistently allows only the most active recent users to be considered. 5.4 Inducing good connectors Given the way scouts, connectors, and promoters are characterized, it follows that the movies that are part of the best scouts are also part of the best connectors.",
                "Similarly, the users that constitute the best promoters are also part of the best connectors.",
                "Good connectors are induced by ensuring a user with a high promoter value rates a movie with a high <br>scout</br> value.",
                "In our experiments, we find that a ratings longest standing role is often as a connector.",
                "A rating with a poor connector value is often seen due to its user being a bad promoter, or its movie being a bad <br>scout</br>.",
                "Such ratings can be removed from the prediction process to bring marginal improvements to recommendations.",
                "In some selected experiments, we observed that removing a set of badly behaving connectors helped improve the systems overall performance by 1.5%.",
                "The effect was even higher on a few select users who observed an improvement of above 10% in precision without much loss in recall. 5.5 Monitoring the evolution of rating roles One of the more significant contributions of our work is the ability to model the evolution of recommender systems, by studying the changing roles of ratings over time.",
                "The role and value of a rating can change depending on many factors like user behavior, redundancy, shilling effects or properties of the collaborative filtering algorithm used.",
                "Studying the dynamics of rating roles in terms of transitions between good, bad, and negligible values can provide insights into the functioning of the recommender system.",
                "We believe that a continuous visualization of these transitions will improve the ability to manage a recommender system.",
                "We classify different rating states as good, bad, or negligible.",
                "Consider a user who has rated 100 movies in a particular interval, of which 20 are part of the test set.",
                "If a <br>scout</br> has a value greater than 0.005, it indicates that it is uniquely involved in at least 2 concordant predictions, which we will say is good.",
                "Thus, a threshold of 0.005 is chosen to bin a rating as good, bad or negligible in terms of its <br>scout</br>, connector and promoter value.",
                "For instance, a rating r, at time t with role value triple [0.1, 0.001, −0.01] is classified as [<br>scout</br> +, connector 0, promoter −], where + indicates good, 0 indicates negligible, and − indicates bad.",
                "The positive credit held by a rating is a measure of its contribution to the betterment of the system, and the discredit is a measure of its contribution to the detriment of the system.",
                "Even though the positive roles (and the negative roles) make up a very small percentage of all ratings, their contribution supersedes their size.",
                "For example, even though only 1.7% of all ratings were classified as good scouts, they hold 79% of all positive credit in the system!",
                "Similarly, the bad scouts were just 1.4% of all ratings but hold 82% of all discredit.",
                "Note that good and bad scouts, together, comprise only 1.4% + 1.7% = 3.1% of the ratings, implying that the majority of the ratings are negligible role players as scouts (more on this later).",
                "Likewise, good connectors were 1.2% of the system, and hold 30% of all positive credit.",
                "The bad connectors (0.8% of the system) hold 36% of all discredit.",
                "Good promoters (3% of the system) hold 46% of all credit, while bad promoters (2%) hold 50% of all discredit.",
                "This reiterates that a few ratings influence most of the systems performance.",
                "Hence it is important to track transitions between them regardless of their small numbers. 257 Across different snapshots, a rating can remain in the same state or change.",
                "A good <br>scout</br> can become a bad <br>scout</br>, a good promoter can become a good connector, good and bad scouts can become vestigial, and so on.",
                "It is not practical to expect a recommender system to have no ratings in bad roles.",
                "However, it suffices to see ratings in bad roles either convert to good or vestigial roles.",
                "Similarly, observing a large number of good roles become bad ones is a sign of imminent failure of the system.",
                "We employ the principle of non-overlapping episodes [6] to count such transitions.",
                "A sequence such as: [+, 0, 0] → [+, 0, 0] → [0, +, 0] → [0, 0, 0] is interpreted as the transitions [+, 0, 0] ; [0, +, 0] : 1 [+, 0, 0] ; [0, 0, 0] : 1 [0, +, 0] ; [0, 0, 0] : 1 instead of [+, 0, 0] ; [0, +, 0] : 2 [+, 0, 0] ; [0, 0, 0] : 2 [0, +, 0] ; [0, 0, 0] : 1.",
                "See [6] for further details about this counting procedure.",
                "Thus, a rating can be in one of 27 possible states, and there are about 272 possible transitions.",
                "We make a further simplification and utilize only 9 states, indicating whether the rating is a <br>scout</br>, promoter, or connector, and whether it has a positive, negative, or negligible role.",
                "Ratings that serve multiple purposes are counted using multiple episode instantiations but the states themselves are not duplicated beyond the 9 restricted states.",
                "In this model, a transition such as [+, 0, +] ; [0, +, 0] : 1 is counted as [<br>scout</br>+] ; [scout0] : 1 [<br>scout</br>+] ; [connector+] : 1 [scout+] ; [promoter0] : 1 [connector0] ; [scout0] : 1 [connector0] ; [scout+] : 1 [connector0] ; [promoter0] : 1 [promoter+] ; [scout0] : 1 [promoter+] ; [connector+] : 1 [promoter+] ; [promoter0] : 1 Of these, transitions like [pX] ; [q0] where p = q, X ∈ {+, 0, −} are considered uninteresting, and only the rest are counted.",
                "Fig. 9 depicts the major transitions counted while processing the first 200,000 ratings from the MovieLens dataset.",
                "Only transitions with frequency greater than or equal to 3% are shown.",
                "The percentages for each state indicates the number of ratings that were found to be in those states.",
                "We consider transitions from any state to a good state as healthy, from any state to a bad state as unhealthy, and from any state to a vestigial state as decaying.",
                "From Fig. 9, we can observe: • The bulk of the ratings have negligible values, irrespective of their role.",
                "The majority of the transitions involve both good and bad ratings becoming negligible.",
                "<br>scout</br> + (2%) <br>scout</br>(1.5%) Scout 0 (96.5%) Connector + (1.2%) Connector(0.8%) Connector 0 (98%) Promoter + (3%) Promoter(2%) Promoter 0 (95%) 84% 84% 81% 74% 10% 6% 11% 77% 8% 7% 8% 82% 4% 86% 4% 68% 15% 13% 5% 5% 77% 11% 7% 5% 4% 3% 3% 3% Healthy Unhealthy Decaying Figure 9: Transitions among rating roles. • The number of good ratings is comparable to the bad ratings, and ratings are seen to switch states often, except in the case of scouts (see below). • The negative and positive scout states are not reachable through any transition, indicating that these ratings must begin as such, and cannot be coerced into these roles. • Good promoters and good connectors have a much longer survival period than scouts.",
                "Transitions that recur to these states have frequencies of 10% and 15% when compared to just 4% for scouts.",
                "Good connectors are the slowest to decay whereas (good) scouts decay the fastest. • Healthy percentages are seen on transitions between promoters and connectors.",
                "As indicated earlier, there are hardly any transitions from promoters/connectors to scouts.",
                "This indicates that, over the long run, a users rating is more useful to others (movies or other users) than to the user himself. • The percentages of healthy transitions outweigh the unhealthy ones - this hints that the system is healthy, albeit only marginally.",
                "Note that these results are conditioned by the static nature of the dataset, which is a set of ratings over a fixed window of time.",
                "However a diagram such as Fig. 9 is clearly useful for monitoring the health of a recommender system.",
                "For instance, acceptable limits can be imposed on different types of transitions and, if a transition fails to meet the threshold, the recommender system or a part of it can be brought under closer scrutiny.",
                "Furthermore, the role state transition diagram would also be the ideal place to study the effects of shilling, a topic we will consider in future research. 5.6 Characterizing neighborhoods Earlier we saw that we can characterize the neighborhood of ratings involved in creating a recommendation list L for 258 a user.",
                "In our experiment, we consider lists of length 30, and sample the lists of about 5% of users through the evolution of the model (at intervals of 10,000 ratings each) and compute their neighborhood characteristics.",
                "To simplify our presentation, we consider the percentage of the sample that fall into one of the following categories: 1.",
                "Inactive user: (SFN(u) = 0) 2.",
                "Good scouts, Good neighborhood: (SFN(u) > 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 3.",
                "Good scouts, Bad neighborhood: (SFN(u) > 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) 4.",
                "Bad scouts, Good neighborhood: (SFN(u) < 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 5.",
                "Bad scouts, Bad neighborhood: (SFN(u) < 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) From our sample set of 561 users, we found that 476 users were inactive.",
                "Of the remaining 85 users, we found 26 users had good scouts and a good neighborhood, 6 had bad scouts and a good neighborhood, 29 had good scouts and a bad neighborhood, and 24 had bad scouts and a bad neighborhood.",
                "Thus, we conjecture that 59 users (29+24+6) are in danger of leaving the system.",
                "As a remedy, users with bad scouts and a good neighborhood can be asked to reconsider rating of some movies hoping to improve the systems recommendations.",
                "The system can be expected to deliver more if they engineer some good scouts.",
                "Users with good scouts and a bad neighborhood are harder to address; this situation might entail selectively removing some connector-promoter pairs that are causing the damage.",
                "Handling users with bad scouts and bad neighborhoods is a more difficult challenge.",
                "Such a classification allows the use of different strategies to better a users experience with the system depending on his context.",
                "In future work, we intend to conduct field studies and study the improvement in performance of different strategies for different contexts. 6.",
                "CONCLUSIONS To further recommender system acceptance and deployment, we require new tools and methodologies to manage an installed recommender and develop insights into the roles played by ratings.",
                "A fine-grained characterization in terms of rating roles such as scouts, promoters, and connectors, as done here, helps such an endeavor.",
                "Although we have presented results on only the item-based algorithm with list rank accuracy as the metric, the same approach outlined here applies to user-based algorithms and other metrics.",
                "In future research, we plan to systematically study the many algorithmic parameters, tolerances, and cutoff thresholds employed here and reason about their effects on the downstream conclusions.",
                "We also aim to extend our formulation to other collaborative filtering algorithms, study the effect of shilling in altering rating roles, conduct field studies, and evaluate improvements in user experience by tweaking ratings based on their role values.",
                "Finally, we plan to develop the idea of mining the evolution of rating role patterns into a reporting and tracking system for all aspects of recommender system health. 7.",
                "REFERENCES [1] Cosley, D., Lam, S., Albert, I., Konstan, J., and Riedl, J.",
                "Is Seeing Believing? : How Recommender System Interfaces Affect Users Opinions.",
                "In Proc.",
                "CHI (2001), pp. 585-592. [2] Herlocker, J. L., Konstan, J.",
                "A., Borchers, A., and Riedl, J.",
                "An Algorithmic Framework for Performing Collaborative Filtering.",
                "In Proc.",
                "SIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J.",
                "A., Terveen, L. G., and Riedl, J. T. Evaluating Collaborative Filtering Recommender Systems.",
                "ACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J.",
                "A.",
                "Personal communication. 2003. [5] Lam, S. K., and Riedl, J. Shilling Recommender Systems for Fun and Profit.",
                "In Proceedings of the 13th International World Wide Web Conference (2004), ACM Press, pp. 393-402. [6] Laxman, S., Sastry, P. S., and Unnikrishnan, K. P. Discovering Frequent Episodes and Learning Hidden Markov Models: A Formal Connection.",
                "IEEE Transactions on Knowledge and Data Engineering Vol. 17, 11 (2005), 1505-1517. [7] McLaughlin, M. R., and Herlocker, J. L. A Collaborative Filtering Algorithm and Evaluation Metric that Accurately Model the User Experience.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (2004), pp. 329 - 336. [8] OMahony, M., Hurley, N. J., Kushmerick, N., and Silvestre, G. Collaborative Recommendation: A Robustness Analysis.",
                "ACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J.",
                "A., and Riedl, J.",
                "Getting to Know You: Learning New User Preferences in Recommender Systems.",
                "In Proceedings of the 2002 Conference on Intelligent User Interfaces (IUI 2002) (2002), pp. 127-134. [10] Rashid, A. M., Karypis, G., and Riedl, J.",
                "Influence in Ratings-Based Recommender Systems: An Algorithm-Independent Approach.",
                "In Proc. of the SIAM International Conference on Data Mining (2005). [11] Resnick, P., Iacovou, N., Sushak, M., Bergstrom, P., and Riedl, J. GroupLens: An Open Architecture for Collaborative Filtering of Netnews.",
                "In Proceedings of the Conference on Computer Supported Collaborative Work (CSCW94) (1994), ACM Press, pp. 175-186. [12] Sarwar, B., Karypis, G., Konstan, J., and Reidl, J. Item-Based Collaborative Filtering Recommendation Algorithms.",
                "In Proceedings of the Tenth International World Wide Web Conference (WWW10) (2001), pp. 285-295. [13] Schein, A., Popescu, A., Ungar, L., and Pennock, D. Methods and Metrics for Cold-Start Recommendation.",
                "In Proc.",
                "SIGIR (2002), pp. 253-260. 259"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Esta calificación sirve como un \"explorador\" en el gráfico bipartito de las clasificaciones para encontrar una ruta que conduzca a la máscara.2.",
                "La calificación P2 (Jeff → Star Wars) ayuda al sistema a recomendar la máscara a Tom conectando el \"Scout\" al promotor.3.",
                "Formalmente, dada una predicción PU, un elemento objetivo A para el usuario U, un \"Scout\" para PU, A es una calificación RU, yo tal que existe un usuario V con calificaciones RV, A y RV, para algunos elementosi;Un promotor para PU, A es un RV de calificación, A para algún usuario V, de modo que existan calificaciones RV, I y Ru, I para un artículo I y;Un conector para PU, un 252 Jim Tom Jeff Jerry, mi primo Vinny, el Matrix Star Wars The Mask Jurasic Park Figura 4: Scouts, promotores y conectores.es una calificación RV, I por algún usuario V y calificación I, de modo que exista las calificaciones Ru, I y RV, a.",
                "Su calificación de la máscara es el mejor \"explorador\" para Jeff, y solo \"Scout\" de Jerrys es su calificación de Star Wars.",
                "La calificación Jim → mi primo Vinny es un pobre \"explorador\" y conector, pero es un muy buen promotor.",
                "La calificación Jim → la máscara es un \"explorador\" razonablemente bueno, un muy buen conector y un buen promotor.",
                "Finalmente, la calificación Jerry → Star Wars es un muy buen \"Scout\", pero no tiene valor como conector o promotor.",
                "Este tributo, m (e) ∗ influencia (u, j, v, i), es a su vez heredado por cada una de las calificaciones en el camino [ru, j, rv, j, rv, i], con el crédito/culpaacumulando a los roles respectivos de Ru, j como un \"explorador\", RV, j como conector y RV, yo como promotor.",
                "En otras palabras, el valor \"Scout\" SF (Ru, J), el valor del conector CF (RV, J) y el valor del promotor PF (RV, I) están incrementados por la cantidad de tributo.",
                "En una gran cantidad de predicciones, los exploradores que han resultado repetidamente en grandes tasas de error tienen un gran valor \"explorador\" negativo, y viceversa (de manera similar con los otros roles)."
            ],
            "translated_text": "",
            "candidates": [
                "Scout",
                "explorador",
                "Scout",
                "Scout",
                "explorar",
                "Scout",
                "explorar",
                "explorador",
                "Scout",
                "explorar",
                "explorador",
                "explorar",
                "explorador",
                "explorar",
                "Scout",
                "explorar",
                "explorador",
                "explorar",
                "Scout",
                "explorar",
                "explorador"
            ],
            "error": []
        },
        "promoter": {
            "translated_key": "Promotor",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Scouts, Promoters, and Connectors: The Roles of Ratings in Nearest Neighbor Collaborative Filtering Bharath Kumar Mohan Dept.",
                "of CSA Indian Institute of Science Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. of Computer Science Eastern Michigan University Ypsilanti, MI 48917, USA bkeller@emich.edu Naren Ramakrishnan Dept. of Computer Science Virginia Tech, Blacksburg VA 24061, USA naren@cs.vt.edu ABSTRACT Recommender systems aggregate individual user ratings into predictions of products or services that might interest visitors.",
                "The quality of this aggregation process crucially affects the user experience and hence the effectiveness of recommenders in e-commerce.",
                "We present a novel study that disaggregates global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearestneighbor collaborative filtering.",
                "In particular, we formulate three roles-scouts, promoters, and connectors-that capture how users receive recommendations, how items get recommended, and how ratings of these two types are themselves connected (resp.).",
                "These roles find direct uses in improving recommendations for users, in better targeting of items and, most importantly, in helping monitor the health of the system as a whole.",
                "For instance, they can be used to track the evolution of neighborhoods, to identify rating subspaces that do not contribute (or contribute negatively) to system performance, to enumerate users who are in danger of leaving, and to assess the susceptibility of the system to attacks such as shilling.",
                "We argue that the three rating roles presented here provide broad primitives to manage a recommender system and its community.",
                "Categories and Subject Descriptors H.4.2 [Information Systems Applications]: Types of Systems-Decision support; J.4 [Computer Applications]: Social and Behavioral Sciences General Terms Algorithms, Human Factors 1.",
                "INTRODUCTION Recommender systems have become integral to e-commerce, providing technology that suggests products to a visitor based on previous purchases or rating history.",
                "Collaborative filtering, a common form of recommendation, predicts a users rating for an item by combining (other) ratings of that user with other users ratings.",
                "Significant research has been conducted in implementing fast and accurate collaborative filtering algorithms [2, 7], designing interfaces for presenting recommendations to users [1], and studying the robustness of these algorithms [8].",
                "However, with the exception of a few studies on the influence of users [10], little attention has been paid to unraveling the inner workings of a recommender in terms of the individual ratings and the roles they play in making (good) recommendations.",
                "Such an understanding will give an important handle to monitoring and managing a recommender system, to engineer mechanisms to sustain the recommender, and thereby ensure its continued success.",
                "Our motivation here is to disaggregate global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearest-neighbor collaborative filtering.",
                "We identify three possible roles: (scouts) to connect the user into the system to receive recommendations, (promoters) to connect an item into the system to be recommended, and (connectors) to connect ratings of these two kinds.",
                "Viewing ratings in this way, we can define the contribution of a rating in each role, both in terms of allowing recommendations to occur, and in terms of influence on the quality of recommendations.",
                "In turn, this capability helps support scenarios such as: 1.",
                "Situating users in better neighborhoods: A users ratings may inadvertently connect the user to a neighborhood for which the users tastes may not be a perfect match.",
                "Identifying ratings responsible for such bad recommendations and suggesting new items to rate can help situate the user in a better neighborhood. 2.",
                "Targeting items: Recommender systems suffer from lack of user participation, especially in cold-start scenarios [13] involving newly arrived items.",
                "Identifying users who can be encouraged to rate specific items helps ensure coverage of the recommender system. 3.",
                "Monitoring the evolution of the recommender system and its stakeholders: A recommender system is constantly under change: growing with new users and 250 items, shrinking with users leaving the system, items becoming irrelevant, and parts of the system under attack.",
                "Tracking the roles of a rating and its evolution over time provides many insights into the health of the system, and how it could be managed and improved.",
                "These include being able to identify rating subspaces that do not contribute (or contribute negatively) to system performance, and could be removed; to enumerate users who are in danger of leaving, or have left the system; and to assess the susceptibility of the system to attacks such as shilling [5].",
                "As we show, the characterization of rating roles presented here provides broad primitives to manage a recommender system and its community.",
                "The rest of the paper is organized as follows.",
                "Background on nearest-neighbor collaborative filtering and algorithm evaluation is discussed in Section 2.",
                "Section 3 defines and discusses the roles of a rating, and Section 4 defines measures of the contribution of a rating in each of these roles.",
                "In Section 5, we illustrate the use of these roles to address the goals outlined above. 2.",
                "BACKGROUND 2.1 Algorithms Nearest-neighbor collaborative filtering algorithms either use neighborhoods of users or neighborhoods of items to compute a prediction.",
                "An algorithm of the first kind is called user-based, and one of the second kind is called itembased [12].",
                "In both families of algorithms, neighborhoods are formed by first computing the similarity between all pairs of users (for user-based) or items (for item-based).",
                "Predictions are then computed by aggregating ratings, which in a user-based algorithm involves aggregating the ratings of the target item by the users neighbors and, in an item-based algorithm, involves aggregating the users ratings of items that are neighbors of the target item.",
                "Algorithms within these families differ in the definition of similarity, formation of neighborhoods, and the computation of predictions.",
                "We consider a user-based algorithm based on that defined for GroupLens [11] with variations from Herlocker et al. [2], and an item-based algorithm similar to that of Sarwar et al. [12].",
                "The algorithm used by Resnick et al. [11] defines the similarity of two users u and v as the Pearson correlation of their common ratings: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2 , where Iu is the set of items rated by user u, ru,i is user us rating for item i, and ¯ru is the average rating of user u (similarly for v).",
                "Similarity computed in this manner is typically scaled by a factor proportional to the number of common ratings, to reduce the chance of making a recommendation made on weak connections: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), where γ ≈ 5 is a constant used as a lower limit in scaling [2].",
                "These new similarities are then used to define a static neighborhood Nu for each user u consisting of the top K users most similar to user u.",
                "A prediction for user u and item i is computed by a weighted average of the ratings by the neighbors pu,i = ¯ru + P v∈V sim (u, v)(rv,i − ¯rv) P v∈V sim (u, v) (1) where V = Nu ∩ Ui is the set of users most similar to u who have rated i.",
                "The item-based algorithm we use is the one defined by Sarwar et al. [12].",
                "In this algorithm, similarity is defined as the adjusted cosine measure sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) where Ui is the set of users who have rated item i.",
                "As for the user-based algorithm, the similarity weights are adjusted proportionally to the number of users that have rated the items in common sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). (3) Given the similarities, the neighborhood Ni of an item i is defined as the top K most similar items for that item.",
                "A prediction for user u and item i is computed as the weighted average pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) where J = Ni ∩ Iu is the set of items rated by u that are most similar to i. 2.2 Evaluation Recommender algorithms have typically been evaluated using measures of predictive accuracy and coverage [3].",
                "Studies on recommender algorithms, notably Herlocker et al. [2] and Sarwar et al. [12], typically compute predictive accuracy by dividing a set of ratings into training and test sets, and compute the prediction for an item in the test set using the ratings in the training set.",
                "A standard measure of predictive accuracy is mean absolute error (MAE), which for a test set T = {(u, i)} is defined as, MAE = P (u,i)∈T |pu,i − ru,i| |T | . (5) Coverage has a number of definitions, but generally refers to the proportion of items that can be predicted by the algorithm [3].",
                "A practical issue with predictive accuracy is that users typically are presented with recommendation lists, and not individual numeric predictions.",
                "Recommendation lists are lists of items in decreasing order of prediction (sometimes stated in terms of star-ratings), and so predictive accuracy may not be reflective of the accuracy of the list.",
                "So, instead we can measure recommendation or rank accuracy, which indicates the extent to which the list is in the correct order.",
                "Herlocker et al. [3] discuss a number of rank accuracy measures, which range from Kendalls Tau to measures that consider the fact that users tend to only look at a prefix of the list [5].",
                "Kendalls Tau measures the number of inversions when comparing ordered pairs in the true user ordering of 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figure 1: Ratings in simple movie recommender. items and the recommended order, and is defined as τ = C − D p (C + D + TR)(C + D + TP) (6) where C is the number of pairs that the system predicts in the correct order, D the number of pairs the system predicts in the wrong order, TR the number of pairs in the true ordering that have the same ratings, and TP is the number of pairs in the predicted ordering that have the same ratings [3].",
                "A shortcoming of the Tau metric is that it is oblivious to the position in the ordered list where the inversion occurs [3].",
                "For instance, an inversion toward the end of the list is given the same weight as one in the beginning.",
                "One solution is to consider inversions only in the top few items in the recommended list or to weight inversions based on their position in the list. 3.",
                "ROLES OF A RATING Our basic observation is that each rating plays a different role in each prediction in which it is used.",
                "Consider a simplified movie recommender system with three users Jim, Jeff, and Tom and their ratings for a few movies, as shown in Fig. 1. (For this initial discussion we will not consider the rating values involved.)",
                "The recommender predicts whether Tom will like The Mask using the other already available ratings.",
                "How this is done depends on the algorithm: 1.",
                "An item-based collaborative filtering algorithm constructs a neighborhood of movies around The Mask by using the ratings of users who rated The Mask and other movies similarly (e.g., Jims ratings of The Matrix and The Mask; and Jeffs ratings of Star Wars and The Mask).",
                "Toms ratings of those movies are then used to make a prediction for The Mask. 2.",
                "A user-based collaborative filtering algorithm would construct a neighborhood around Tom by tracking other users whose rating behaviors are similar to Toms (e.g., Tom and Jeff have rated Star Wars; Tom and Jim have rated The Matrix).",
                "The prediction of Toms rating for The Mask is then based on the ratings of Jeff and Tim.",
                "Although the nearest-neighbor algorithms aggregate the ratings to form neighborhoods used to compute predictions, we can disaggregate the similarities to view the computation of a prediction as simultaneously following parallel paths of ratings.",
                "So, irrespective of the collaborative filtering algorithm used, we can visualize the prediction of Toms rating of The Mask as walking through a sequence of ratings.",
                "In Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figure 2: Ratings used to predict The Mask for Tom.",
                "Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figure 3: Prediction of The Mask for Tom in which a rating is used more than once. this example, two paths were used for this prediction as depicted in Fig. 2: (p1, p2, p3) and (q1, q2, q3).",
                "Note that these paths are undirected, and are all of length 3.",
                "Only the order in which the ratings are traversed is different between the item-based algorithm (e.g., (p3, p2, p1), (q3, q2, q1)) and the user-based algorithm (e.g., (p1, p2, p3), (q1, q2, q3).)",
                "A rating can be part of many paths for a single prediction as shown in Fig. 3, where three paths are used for a prediction, two of which follow p1: (p1, p2, p3) and (p1, r2, r3).",
                "Predictions in a collaborative filtering algorithms may involve thousands of such walks in parallel, each playing a part in influencing the predicted value.",
                "Each prediction path consists of three ratings, playing roles that we call scouts, promoters, and connectors.",
                "To illustrate these roles, consider the path (p1, p2, p3) in Fig. 2 used to make a prediction of The Mask for Tom: 1.",
                "The rating p1 (Tom → Star Wars) makes a connection from Tom to other ratings that can be used to predict Toms rating for The Mask.",
                "This rating serves as a scout in the bipartite graph of ratings to find a path that leads to The Mask. 2.",
                "The rating p2 (Jeff → Star Wars) helps the system recommend The Mask to Tom by connecting the scout to the <br>promoter</br>. 3.",
                "The rating p3 (Jeff → The Mask) allows connections to The Mask, and, therefore, promotes this movie to Tom.",
                "Formally, given a prediction pu,a of a target item a for user u, a scout for pu,a is a rating ru,i such that there exists a user v with ratings rv,a and rv,i for some item i; a <br>promoter</br> for pu,a is a rating rv,a for some user v, such that there exist ratings rv,i and ru,i for an item i, and; a connector for pu,a 252 Jim Tom Jeff Jerry My Cousin Vinny The Matrix Star Wars The Mask Jurasic Park Figure 4: Scouts, promoters, and connectors. is a rating rv,i by some user v and rating i, such that there exists ratings ru,i and rv,a.",
                "The scouts, connectors, and promoters for the prediction of Toms rating of The Mask are p1 and q1, p2 and q2, and p3 and q3 (respectively).",
                "Each of these roles has a value in the recommender to the user, the users neighborhood, and the system in terms of allowing recommendations to be made. 3.1 Roles in Detail Ratings that act as scouts tend to help the recommender system suggest more movies to the user, though the extent to which this is true depends on the rating behavior of other users.",
                "For example, in Fig. 4 the rating Tom → Star Wars helps the system recommend only The Mask to him, while Tom → The Matrix helps recommend The Mask, Jurassic Park, and My Cousin Vinny.",
                "Tom makes a connection to Jim who is a prolific user of the system, by rating The Matrix.",
                "However, this does not make The Matrix the best movie to rate for everyone.",
                "For example, Jim is benefited equally by both The Mask and The Matrix, which allow the system to recommend Star Wars to him.",
                "His rating of The Mask is the best scout for Jeff, and Jerrys only scout is his rating of Star Wars.",
                "This suggests that good scouts allow a user to build similarity with prolific users, and thereby ensure they get more from the system.",
                "While scouts represent beneficial ratings from the perspective of a user, promoters are their duals, and are of benefit to items.",
                "In Fig. 4, My Cousin Vinny benefits from Jims rating, since it allows recommendations to Jeff and Tom.",
                "The Mask is not so dependent on just one rating, since the ratings by Jim and Jeff help it.",
                "On the other hand, Jerrys rating of Star Wars does not help promote it to any other user.",
                "We conclude that a good <br>promoter</br> connects an item to a broader neighborhood of other items, and thereby ensures that it is recommended to more users.",
                "Connectors serve a crucial role in a recommender system that is not as obvious.",
                "The movies My Cousin Vinny and Jurassic Park have the highest recommendation potential since they can be recommended to Jeff, Jerry and Tom based on the linkage structure illustrated in Fig. 4.",
                "Beside the fact that Jim rated these movies, these recommendations are possible only because of the ratings Jim → The Matrix and Jim → The Mask, which are the best connectors.",
                "A connector improves the systems ability to make recommendations with no explicit gain for the user.",
                "Note that every rating can be of varied benefit in each of these roles.",
                "The rating Jim → My Cousin Vinny is a poor scout and connector, but is a very good <br>promoter</br>.",
                "The rating Jim → The Mask is a reasonably good scout, a very good connector, and a good <br>promoter</br>.",
                "Finally, the rating Jerry → Star Wars is a very good scout, but is of no value as a connector or <br>promoter</br>.",
                "As illustrated here, a rating can have different value in each of the three roles in terms of whether a recommendation can be made or not.",
                "We could measure this value by simply counting the number of times a rating is used in each role, which alone would be helpful in characterizing the behavior of a system.",
                "But we can also measure the contribution of each rating to the quality of recommendations or health of the system.",
                "Since every prediction is a combined effort of several recommendation paths, we are interested in discerning the influence of each rating (and, hence, each path) in the system towards the systems overall error.",
                "We can understand the dynamics of the system at a finer granularity by tracking the influence of a rating according to the role played.",
                "The next section describes the approach to measuring the values of a rating in each role. 4.",
                "CONTRIBUTIONS OF RATINGS As weve seen, a rating may play different roles in different predictions and, in each prediction, contribute to the quality of a prediction in different ways.",
                "Our approach can use any numeric measure of a property of system health, and assigns credit (or blame) to each rating proportional to its influence in the prediction.",
                "By tracking the role of each rating in a prediction, we can accumulate the credit for a rating in each of the three roles, and also track the evolution of the roles of rating over time in the system.",
                "This section defines the methodology for computing the contribution of ratings by first defining the influence of a rating, and then instantiating the approach for predictive accuracy, and then rank accuracy.",
                "We also demonstrate how these contributions can be aggregated to study the neighborhood of ratings involved in computing a users recommendations.",
                "Note that although our general formulation for rating influence is algorithm independent, due to space considerations, we present the approach for only item-based collaborative filtering.",
                "The definition for user-based algorithms is similar and will be presented in an expanded version of this paper. 4.1 Influence of Ratings Recall that an item-based approach to collaborative filtering relies on building item neighborhoods using the similarity of ratings by the same user.",
                "As described earlier, similarity is defined by the adjusted cosine measure (Equations (2) and (3)).",
                "A set of the top K neighbors is maintained for all items for space and computational efficiency.",
                "A prediction of item i for a user u is computed as the weighted deviation from the items mean rating as shown in Equation (4).",
                "The list of recommendations for a user is then the list of items sorted in descending order of their predicted values.",
                "We first define impact(a, i, j), the impact a user a has in determining the similarity between two items i and j.",
                "This is the change in the similarity between i and j when as rating is removed, and is defined as impact(a, i, j) = |sim (i, j) − sim¯a(i, j)| P w∈Cij |sim (i, j) − sim ¯w(i, j)| where Cij = {u ∈ U | ∃ ru,i, ru,j ∈ R(u)} is the set of coraters 253 of items i and j (users who rate both i and j), R(u) is the set of ratings provided by user u, and sim¯a(i, j) is the similarity of i and j when the ratings of user a are removed sim¯a(i, j) = P v∈U\\{a} (ru,i − ¯ru)(ru,j − ¯ru) qP u∈U\\{a}(ru,i − ¯ru)2 qP u∈U\\{a}(ru,j − ¯ru)2 , and adjusted for the number of raters sim¯a(i, j) = max(|Ui ∩ Uj| − 1, γ) γ · sim(i, j).",
                "If all coraters of i and j rate them identically, we define the impact as impact(a, i, j) = 1 |Cij| since P w∈Cij |sim (i, j) − sim ¯w(i, j)| = 0.",
                "The influence of each path (u, j, v, i) = [ru,j, rv,j, rv,i] in the prediction of pu,i is given by influence(u, j, v, i) = sim (i, j) P l∈Ni∩Iu sim (i, l) · impact(v, i, j) It follows that the sum of influences over all such paths, for a given set of endpoints, is 1. 4.2 Role Values for Predictive Accuracy The value of a rating in each role is computed from the influence depending on the evaluation measure employed.",
                "Here we illustrate the approach using predictive accuracy as the evaluation metric.",
                "In general, the goodness of a prediction decides whether the ratings involved must be credited or discredited for their role.",
                "For predictive accuracy, the error in prediction e = |pu,i − ru,i| is mapped to a comfort level using a mapping function M(e).",
                "Anecdotal evidence suggests that users are unable to discern errors less than 1.0 (for a rating scale of 1 to 5) [4], and so an error less than 1.0 is considered acceptable, but anything larger is not.",
                "We hence define M(e) as (1 − e) binned to an appropriate value in [−1, −0.5, 0.5, 1].",
                "For each prediction pu,i, M(e) is attributed to all the paths that assisted the computation of pu,i, proportional to their influences.",
                "This tribute, M(e)∗influence(u, j, v, i), is in turn inherited by each of the ratings in the path [ru,j, rv,j, rv,i], with the credit/blame accumulating to the respective roles of ru,j as a scout, rv,j as a connector, and rv,i as a <br>promoter</br>.",
                "In other words, the scout value SF(ru,j), the connector value CF(rv,j) and the <br>promoter</br> value PF(rv,i) are all incremented by the tribute amount.",
                "Over a large number of predictions, scouts that have repeatedly resulted in big error rates have a big negative scout value, and vice versa (similarly with the other roles).",
                "Every rating is thus summarized by its triple [SF, CF, PF]. 4.3 Role Values for Rank Accuracy We now define the computation of the contribution of ratings to observed rank accuracy.",
                "For this computation, we must know the users preference order for a set of items for which predictions can be computed.",
                "We assume that we have a test set of the users ratings of the items presented in the recommendation list.",
                "For every pair of items rated by a user in the test data, we check whether the predicted order is concordant with his preference.",
                "We say a pair (i, j) is concordant (with error ) whenever one of the following holds: • if (ru,i < ru,j) then (pu,i − pu,j < ); • if (ru,i > ru,j) then (pu,i − pu,j > ); or • if (ru,i = ru,j) then (|pu,i − pu,j| ≤ ).",
                "Similarly, a pair (i, j) is discordant (with error ) if it is not concordant.",
                "Our experiments described below use an error tolerance of = 0.1.",
                "All paths involved in the prediction of the two items in a concordant pair are credited, and the paths involved in a discordant pair are discredited.",
                "The credit assigned to a pair of items (i, j) in the recommendation list for user u is computed as c(i, j) = ( t T · 1 C+D if (i, j) are concordant − t T · 1 C+D if (i, j) are discordant (7) where t is the number of items in the users test set whose ratings could be predicted, T is the number of items rated by user u in the test set, C is the number of concordances and D is the number of discordances.",
                "The credit c is then divided among all paths responsible for predicting pu,i and pu,j proportional to their influences.",
                "We again add the role values obtained from all the experiments to form a triple [SF, CF, PF] for each rating. 4.4 Aggregating rating roles After calculating the role values for individual ratings, we can also use these values to study neighborhoods and the system.",
                "Here we consider how we can use the role values to characterize the health of a neighborhood.",
                "Consider the list of top recommendations presented to a user at a specific point in time.",
                "The collaborative filtering algorithm traversed many paths in his neighborhood through his scouts and other connectors and promoters to make these recommendations.",
                "We call these ratings the recommender neighborhood of the user.",
                "The user implicitly chooses this neighborhood of ratings through the items he rates.",
                "Apart from the collaborative filtering algorithm, the health of this neighborhood completely influences a users satisfaction with the system.",
                "We can characterize a users recommender neighborhood by aggregating the individual role values of the ratings involved, weighted by the influence of individual ratings in determining his recommended list.",
                "Different sections of the users neighborhood wield varied influence on his recommendation list.",
                "For instance, ratings reachable through highly rated items have a bigger say in the recommended items.",
                "Our aim is to study the system and classify users with respect to their positioning in a healthy or unhealthy neighborhood.",
                "A user can have a good set of scouts, but may be exposed to a neighborhood with bad connectors and promoters.",
                "He can have a good neighborhood, but his bad scouts may ensure the neighborhoods potential is rendered useless.",
                "We expect that users with good scouts and good neighborhoods will be most satisfied with the system in the future.",
                "A users neighborhood is characterized by a triple that represents the weighted sum of the role values of individual ratings involved in making recommendations.",
                "Consider a user u and his ordered list of recommendations L. An item i 254 in the list is weighted inversely, as K(i), depending on its position in the list.",
                "In our studies we use K(i) = p position(i).",
                "Several paths of ratings [ru,j, rv,j, rv,i] are involved in predicting pu,i which ultimately decides its position in L, each with influence(u, j, v, i).",
                "The recommender neighborhood of a user u is characterized by the triple, [SFN(u), CFN(u), PFN(u)] where SFN(u) = X i∈L P [ru,j ,rv,j ,rv,i] SF(ru,j)influence(u, j, v, i) K(i) !",
                "CFN(u) and PFN(u) are defined similarly.",
                "This triple estimates the quality of us recommendations based on the past track record of the ratings involved in their respective roles. 5.",
                "EXPERIMENTATION As we have seen, we can assign role values to each rating when evaluating a collaborative filtering system.",
                "In this section, we demonstrate the use of this approach to our overall goal of defining an approach to monitor and manage the health of a recommender system through experiments done on the MovieLens million rating dataset.",
                "In particular, we discuss results relating to identifying good scouts, promoters, and connectors; the evolution of rating roles; and the characterization of user neighborhoods. 5.1 Methodology Our experiments use the MovieLens million rating dataset, which consists of ratings by 6040 users of 3952 movies.",
                "The ratings are in the range 1 to 5, and are labeled with the time the rating was given.",
                "As discussed before, we consider only the item-based algorithm here (with item neighborhoods of size 30) and, due to space considerations, only present role value results for rank accuracy.",
                "Since we are interested in the evolution of the rating role values over time, the model of the recommender system is built by processing ratings in their arrival order.",
                "The timestamping provided by MovieLens is hence crucial for the analyses presented here.",
                "We make assessments of rating roles at intervals of 10,000 ratings and processed the first 200,000 ratings in the dataset (giving rise to 20 snapshots).",
                "We incrementally update the role values as the time ordered ratings are merged into the model.",
                "To keep the experiment computationally manageable, we define a test dataset for each user.",
                "As the time ordered ratings are merged into the model, we label a small randomly selected percentage (20%) as test data.",
                "At discrete epochs, i.e., after processing every 10,000 ratings, we compute the predictions for the ratings in the test data, and then compute the role values for the ratings used in the predictions.",
                "One potential criticism of this methodology is that the ratings in the test set are never evaluated for their roles.",
                "We overcome this concern by repeating the experiment, using different random seeds.",
                "The probability that every rating is considered for evaluation is then considerably high: 1 − 0.2n , where n is the number of times the experiment is repeated with different random seeds.",
                "The results here are based on n = 4 repetitions.",
                "The item-based collaborative filtering algorithms performance was ordinary with respect to rank accuracy.",
                "Fig. 5 shows a plot of the precision and recall as ratings were merged in time order into the model.",
                "The recall was always high, but the average precision was just about 53%. 0 0.2 0.4 0.6 0.8 1 1.2 10000 30000 50000 70000 90000110000130000150000 Ratings merged into model Value Precision Recall Figure 5: Precision and recall for the item-based collaborative filtering algorithm. 5.2 Inducing good scouts The ratings of a user that serve as scouts are those that allow the user to receive recommendations.",
                "We claim that users with ratings that have respectable scout values will be happier with the system than those with ratings with low scout values.",
                "Note that the item-based algorithm discussed here produces recommendation lists with nearly half of the pairs in the list discordant from the users preference.",
                "Whether all of these discordant pairs are observable by the user is unclear, however, certainly this suggests that there is a need to be able to direct users to items whose ratings would improve the lists.",
                "The distribution of the scout values for most users ratings are Gaussian with mean zero.",
                "Fig. 6 shows the frequency distribution of scout values for a sample user at a given snapshot.",
                "We observe that a large number of ratings never serve as scouts for their users.",
                "A relatable scenario is when Amazons recommender makes suggestions of books or items based on other items that were purchased as gifts.",
                "With simple relevance feedback from the user, such ratings can be isolated as bad scouts and discounted from future predictions.",
                "Removing bad scouts was found to be worthwhile for individual users but the overall performance improvement was only marginal.",
                "An obvious question is whether good scouts can be formed by merely rating popular movies as suggested by Rashid et al. [9].",
                "They show that a mix of popularity and rating entropy identifies the best items to suggest to new users when evaluated using MAE.",
                "Following their intuition, we would expect to see a higher correlation between popularityentropy and good scouts.",
                "We measured the Pearson correlation coefficient between aggregated scout values for a movie with the popularity of a movie (number of times it is rated); and with its popularity*variance measure at different snapshots of the system.",
                "Note that the scout values were initially anti-correlated with popularity (Fig. 7), but became moderately correlated as the system evolved.",
                "Both popularity and popularity*variance performed similarly.",
                "A possible explanation is that there has been insufficient time for the popular movies to accumulate ratings. 255 -10 0 10 20 30 40 50 60 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 Scout Value Frequency Figure 6: Distribution of scout values for a sample user. -0.4 -0.2 0 0.2 0.4 0.6 0.8 30000 60000 90000 120000 150000 180000 Popularity Pop*Var Figure 7: Correlation between aggregated scout value and item popularity (computed at different intervals). -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 30000 60000 90000 120000 150000 180000 Figure 8: Correlation between aggregated <br>promoter</br> value and user prolificity (computed at different intervals).",
                "Table 1: Movies forming the best scouts.",
                "Best Scouts Conf.",
                "Pop.",
                "Being John Malkovich (1999) 1.00 445 Star Wars: Episode IV - A New Hope (1977) 0.92 623 Princess Bride, The (1987) 0.85 477 Sixth Sense, The (1999) 0.85 617 Matrix, The (1999) 0.77 522 Ghostbusters (1984) 0.77 441 Casablanca (1942) 0.77 384 Insider, The (1999) 0.77 235 American Beauty (1999) 0.69 624 Terminator 2: Judgment Day (1991) 0.69 503 Fight Club (1999) 0.69 235 Shawshank Redemption, The (1994) 0.69 445 Run Lola Run (Lola rennt) (1998) 0.69 220 Terminator, The (1984) 0.62 450 Usual Suspects, The (1995) 0.62 326 Aliens (1986) 0.62 385 North by Northwest (1959) 0.62 245 Fugitive, The (1993) 0.62 402 End of Days (1999) 0.62 132 Raiders of the Lost Ark (1981) 0.54 540 Schindlers List (1993) 0.54 453 Back to the Future (1985) 0.54 543 Toy Story (1995) 0.54 419 Alien (1979) 0.54 415 Abyss, The (1989) 0.54 345 2001: A Space Odyssey (1968) 0.54 358 Dogma (1999) 0.54 228 Little Mermaid, The (1989) 0.54 203 Table 2: Movies forming the worst scouts.",
                "Worst scouts Conf.",
                "Pop.",
                "Harold and Maude (1971) 0.46 141 Grifters, The (1990) 0.46 180 Sting, The (1973) 0.38 244 Godfather: Part III, The (1990) 0.38 154 Lawrence of Arabia (1962) 0.38 167 High Noon (1952) 0.38 84 Women on the Verge of a... (1988) 0.38 113 Grapes of Wrath, The (1940) 0.38 115 Duck Soup (1933) 0.38 131 Arsenic and Old Lace (1944) 0.38 138 Midnight Cowboy (1969) 0.38 137 To Kill a Mockingbird (1962) 0.31 195 Four Weddings and a Funeral (1994) 0.31 271 Good, The Bad and The Ugly, The (1966) 0.31 156 Its a Wonderful Life (1946) 0.31 146 Player, The (1992) 0.31 220 Jackie Brown (1997) 0.31 118 Boat, The (Das Boot) (1981) 0.31 210 Manhattan (1979) 0.31 158 Truth About Cats & Dogs, The (1996) 0.31 143 Ghost (1990) 0.31 227 Lone Star (1996) 0.31 125 Big Chill, The (1983) 0.31 184 256 By studying the evolution of scout values, we can identify movies that consistently feature in good scouts over time.",
                "We claim these movies will make viable scouts for other users.",
                "We found the aggregated scout values for all movies in intervals of 10,000 ratings each.",
                "A movie is said to induce a good scout if the movie was in the top 100 of the sorted list, and to induce a bad scout if it was in bottom 100 of the same list.",
                "Movies appearing consistently high over time are expected to remain up there in the future.",
                "The effective confidence in a movie m is Cm = Tm − Bm N (8) where Tm is the number of times it appeared in the top 100, Bm the number of times it appeared in the bottom 100, and N is the number of intervals considered.",
                "Using this measure, the top few movies expected to induce the best scouts are shown in Table 1.",
                "Movies that would be bad scout choices are shown in Table 2 with their associated confidences.",
                "The popularities of the movies are also displayed.",
                "Although more popular movies appear in the list of good scouts, these tables show that a blind choice of scout based on popularity alone can be potentially dangerous.",
                "Interestingly, the best scout-Being John Malkovich-is about a puppeteer who discovers a portal into a movie star, a movie that has been described variously on amazon.com as makes you feel giddy, seriously weird, comedy with depth, silly, strange, and inventive.",
                "Indicating whether someone likes this movie or not goes a long way toward situating the user in a suitable neighborhood, with similar preferences.",
                "On the other hand, several factors may have made a movie a bad scout, like the sharp variance in user preferences in the neighborhood of a movie.",
                "Two users may have the same opinion about Lawrence of Arabia, but they may differ sharply about how they felt about the other movies they saw.",
                "Bad scouts ensue when there is deviation in behavior around a common synchronization point. 5.3 Inducing good promoters Ratings that serve to promote items in a collaborative filtering system are critical to allowing a new item be recommended to users.",
                "So, inducing good promoters is important for cold-start recommendation.",
                "We note that the frequency distribution of <br>promoter</br> values for a sample movies ratings is also Gaussian (similar to Fig. 6).",
                "This indicates that the promotion of a movie is benefited most by the ratings of a few users, and are unaffected by the ratings of most users.",
                "We find a strong correlation between a users number of ratings and his aggregated <br>promoter</br> value.",
                "Fig. 8 depicts the evolution of the Pearson correlation co-efficient between the prolificity of a user (number of ratings) versus his aggregated <br>promoter</br> value.",
                "We expect that conspicuous shills, by recommending wrong movies to users, will be discredited with negative aggregate <br>promoter</br> values and should be identifiable easily.",
                "Given this observation, the obvious rule to follow when introducing a new movie is to have it rated directly by prolific users who posses high aggregated <br>promoter</br> values.",
                "A new movie is thus cast into the neighborhood of many other movies improving its visibility.",
                "Note, though, that a user may have long stopped using the system.",
                "Tracking <br>promoter</br> values consistently allows only the most active recent users to be considered. 5.4 Inducing good connectors Given the way scouts, connectors, and promoters are characterized, it follows that the movies that are part of the best scouts are also part of the best connectors.",
                "Similarly, the users that constitute the best promoters are also part of the best connectors.",
                "Good connectors are induced by ensuring a user with a high <br>promoter</br> value rates a movie with a high scout value.",
                "In our experiments, we find that a ratings longest standing role is often as a connector.",
                "A rating with a poor connector value is often seen due to its user being a bad <br>promoter</br>, or its movie being a bad scout.",
                "Such ratings can be removed from the prediction process to bring marginal improvements to recommendations.",
                "In some selected experiments, we observed that removing a set of badly behaving connectors helped improve the systems overall performance by 1.5%.",
                "The effect was even higher on a few select users who observed an improvement of above 10% in precision without much loss in recall. 5.5 Monitoring the evolution of rating roles One of the more significant contributions of our work is the ability to model the evolution of recommender systems, by studying the changing roles of ratings over time.",
                "The role and value of a rating can change depending on many factors like user behavior, redundancy, shilling effects or properties of the collaborative filtering algorithm used.",
                "Studying the dynamics of rating roles in terms of transitions between good, bad, and negligible values can provide insights into the functioning of the recommender system.",
                "We believe that a continuous visualization of these transitions will improve the ability to manage a recommender system.",
                "We classify different rating states as good, bad, or negligible.",
                "Consider a user who has rated 100 movies in a particular interval, of which 20 are part of the test set.",
                "If a scout has a value greater than 0.005, it indicates that it is uniquely involved in at least 2 concordant predictions, which we will say is good.",
                "Thus, a threshold of 0.005 is chosen to bin a rating as good, bad or negligible in terms of its scout, connector and <br>promoter</br> value.",
                "For instance, a rating r, at time t with role value triple [0.1, 0.001, −0.01] is classified as [scout +, connector 0, <br>promoter</br> −], where + indicates good, 0 indicates negligible, and − indicates bad.",
                "The positive credit held by a rating is a measure of its contribution to the betterment of the system, and the discredit is a measure of its contribution to the detriment of the system.",
                "Even though the positive roles (and the negative roles) make up a very small percentage of all ratings, their contribution supersedes their size.",
                "For example, even though only 1.7% of all ratings were classified as good scouts, they hold 79% of all positive credit in the system!",
                "Similarly, the bad scouts were just 1.4% of all ratings but hold 82% of all discredit.",
                "Note that good and bad scouts, together, comprise only 1.4% + 1.7% = 3.1% of the ratings, implying that the majority of the ratings are negligible role players as scouts (more on this later).",
                "Likewise, good connectors were 1.2% of the system, and hold 30% of all positive credit.",
                "The bad connectors (0.8% of the system) hold 36% of all discredit.",
                "Good promoters (3% of the system) hold 46% of all credit, while bad promoters (2%) hold 50% of all discredit.",
                "This reiterates that a few ratings influence most of the systems performance.",
                "Hence it is important to track transitions between them regardless of their small numbers. 257 Across different snapshots, a rating can remain in the same state or change.",
                "A good scout can become a bad scout, a good <br>promoter</br> can become a good connector, good and bad scouts can become vestigial, and so on.",
                "It is not practical to expect a recommender system to have no ratings in bad roles.",
                "However, it suffices to see ratings in bad roles either convert to good or vestigial roles.",
                "Similarly, observing a large number of good roles become bad ones is a sign of imminent failure of the system.",
                "We employ the principle of non-overlapping episodes [6] to count such transitions.",
                "A sequence such as: [+, 0, 0] → [+, 0, 0] → [0, +, 0] → [0, 0, 0] is interpreted as the transitions [+, 0, 0] ; [0, +, 0] : 1 [+, 0, 0] ; [0, 0, 0] : 1 [0, +, 0] ; [0, 0, 0] : 1 instead of [+, 0, 0] ; [0, +, 0] : 2 [+, 0, 0] ; [0, 0, 0] : 2 [0, +, 0] ; [0, 0, 0] : 1.",
                "See [6] for further details about this counting procedure.",
                "Thus, a rating can be in one of 27 possible states, and there are about 272 possible transitions.",
                "We make a further simplification and utilize only 9 states, indicating whether the rating is a scout, <br>promoter</br>, or connector, and whether it has a positive, negative, or negligible role.",
                "Ratings that serve multiple purposes are counted using multiple episode instantiations but the states themselves are not duplicated beyond the 9 restricted states.",
                "In this model, a transition such as [+, 0, +] ; [0, +, 0] : 1 is counted as [scout+] ; [scout0] : 1 [scout+] ; [connector+] : 1 [scout+] ; [promoter0] : 1 [connector0] ; [scout0] : 1 [connector0] ; [scout+] : 1 [connector0] ; [promoter0] : 1 [<br>promoter</br>+] ; [scout0] : 1 [<br>promoter</br>+] ; [connector+] : 1 [promoter+] ; [promoter0] : 1 Of these, transitions like [pX] ; [q0] where p = q, X ∈ {+, 0, −} are considered uninteresting, and only the rest are counted.",
                "Fig. 9 depicts the major transitions counted while processing the first 200,000 ratings from the MovieLens dataset.",
                "Only transitions with frequency greater than or equal to 3% are shown.",
                "The percentages for each state indicates the number of ratings that were found to be in those states.",
                "We consider transitions from any state to a good state as healthy, from any state to a bad state as unhealthy, and from any state to a vestigial state as decaying.",
                "From Fig. 9, we can observe: • The bulk of the ratings have negligible values, irrespective of their role.",
                "The majority of the transitions involve both good and bad ratings becoming negligible.",
                "Scout + (2%) Scout(1.5%) Scout 0 (96.5%) Connector + (1.2%) Connector(0.8%) Connector 0 (98%) <br>promoter</br> + (3%) <br>promoter</br>(2%) Promoter 0 (95%) 84% 84% 81% 74% 10% 6% 11% 77% 8% 7% 8% 82% 4% 86% 4% 68% 15% 13% 5% 5% 77% 11% 7% 5% 4% 3% 3% 3% Healthy Unhealthy Decaying Figure 9: Transitions among rating roles. • The number of good ratings is comparable to the bad ratings, and ratings are seen to switch states often, except in the case of scouts (see below). • The negative and positive scout states are not reachable through any transition, indicating that these ratings must begin as such, and cannot be coerced into these roles. • Good promoters and good connectors have a much longer survival period than scouts.",
                "Transitions that recur to these states have frequencies of 10% and 15% when compared to just 4% for scouts.",
                "Good connectors are the slowest to decay whereas (good) scouts decay the fastest. • Healthy percentages are seen on transitions between promoters and connectors.",
                "As indicated earlier, there are hardly any transitions from promoters/connectors to scouts.",
                "This indicates that, over the long run, a users rating is more useful to others (movies or other users) than to the user himself. • The percentages of healthy transitions outweigh the unhealthy ones - this hints that the system is healthy, albeit only marginally.",
                "Note that these results are conditioned by the static nature of the dataset, which is a set of ratings over a fixed window of time.",
                "However a diagram such as Fig. 9 is clearly useful for monitoring the health of a recommender system.",
                "For instance, acceptable limits can be imposed on different types of transitions and, if a transition fails to meet the threshold, the recommender system or a part of it can be brought under closer scrutiny.",
                "Furthermore, the role state transition diagram would also be the ideal place to study the effects of shilling, a topic we will consider in future research. 5.6 Characterizing neighborhoods Earlier we saw that we can characterize the neighborhood of ratings involved in creating a recommendation list L for 258 a user.",
                "In our experiment, we consider lists of length 30, and sample the lists of about 5% of users through the evolution of the model (at intervals of 10,000 ratings each) and compute their neighborhood characteristics.",
                "To simplify our presentation, we consider the percentage of the sample that fall into one of the following categories: 1.",
                "Inactive user: (SFN(u) = 0) 2.",
                "Good scouts, Good neighborhood: (SFN(u) > 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 3.",
                "Good scouts, Bad neighborhood: (SFN(u) > 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) 4.",
                "Bad scouts, Good neighborhood: (SFN(u) < 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 5.",
                "Bad scouts, Bad neighborhood: (SFN(u) < 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) From our sample set of 561 users, we found that 476 users were inactive.",
                "Of the remaining 85 users, we found 26 users had good scouts and a good neighborhood, 6 had bad scouts and a good neighborhood, 29 had good scouts and a bad neighborhood, and 24 had bad scouts and a bad neighborhood.",
                "Thus, we conjecture that 59 users (29+24+6) are in danger of leaving the system.",
                "As a remedy, users with bad scouts and a good neighborhood can be asked to reconsider rating of some movies hoping to improve the systems recommendations.",
                "The system can be expected to deliver more if they engineer some good scouts.",
                "Users with good scouts and a bad neighborhood are harder to address; this situation might entail selectively removing some connector-<br>promoter</br> pairs that are causing the damage.",
                "Handling users with bad scouts and bad neighborhoods is a more difficult challenge.",
                "Such a classification allows the use of different strategies to better a users experience with the system depending on his context.",
                "In future work, we intend to conduct field studies and study the improvement in performance of different strategies for different contexts. 6.",
                "CONCLUSIONS To further recommender system acceptance and deployment, we require new tools and methodologies to manage an installed recommender and develop insights into the roles played by ratings.",
                "A fine-grained characterization in terms of rating roles such as scouts, promoters, and connectors, as done here, helps such an endeavor.",
                "Although we have presented results on only the item-based algorithm with list rank accuracy as the metric, the same approach outlined here applies to user-based algorithms and other metrics.",
                "In future research, we plan to systematically study the many algorithmic parameters, tolerances, and cutoff thresholds employed here and reason about their effects on the downstream conclusions.",
                "We also aim to extend our formulation to other collaborative filtering algorithms, study the effect of shilling in altering rating roles, conduct field studies, and evaluate improvements in user experience by tweaking ratings based on their role values.",
                "Finally, we plan to develop the idea of mining the evolution of rating role patterns into a reporting and tracking system for all aspects of recommender system health. 7.",
                "REFERENCES [1] Cosley, D., Lam, S., Albert, I., Konstan, J., and Riedl, J.",
                "Is Seeing Believing? : How Recommender System Interfaces Affect Users Opinions.",
                "In Proc.",
                "CHI (2001), pp. 585-592. [2] Herlocker, J. L., Konstan, J.",
                "A., Borchers, A., and Riedl, J.",
                "An Algorithmic Framework for Performing Collaborative Filtering.",
                "In Proc.",
                "SIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J.",
                "A., Terveen, L. G., and Riedl, J. T. Evaluating Collaborative Filtering Recommender Systems.",
                "ACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J.",
                "A.",
                "Personal communication. 2003. [5] Lam, S. K., and Riedl, J. Shilling Recommender Systems for Fun and Profit.",
                "In Proceedings of the 13th International World Wide Web Conference (2004), ACM Press, pp. 393-402. [6] Laxman, S., Sastry, P. S., and Unnikrishnan, K. P. Discovering Frequent Episodes and Learning Hidden Markov Models: A Formal Connection.",
                "IEEE Transactions on Knowledge and Data Engineering Vol. 17, 11 (2005), 1505-1517. [7] McLaughlin, M. R., and Herlocker, J. L. A Collaborative Filtering Algorithm and Evaluation Metric that Accurately Model the User Experience.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (2004), pp. 329 - 336. [8] OMahony, M., Hurley, N. J., Kushmerick, N., and Silvestre, G. Collaborative Recommendation: A Robustness Analysis.",
                "ACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J.",
                "A., and Riedl, J.",
                "Getting to Know You: Learning New User Preferences in Recommender Systems.",
                "In Proceedings of the 2002 Conference on Intelligent User Interfaces (IUI 2002) (2002), pp. 127-134. [10] Rashid, A. M., Karypis, G., and Riedl, J.",
                "Influence in Ratings-Based Recommender Systems: An Algorithm-Independent Approach.",
                "In Proc. of the SIAM International Conference on Data Mining (2005). [11] Resnick, P., Iacovou, N., Sushak, M., Bergstrom, P., and Riedl, J. GroupLens: An Open Architecture for Collaborative Filtering of Netnews.",
                "In Proceedings of the Conference on Computer Supported Collaborative Work (CSCW94) (1994), ACM Press, pp. 175-186. [12] Sarwar, B., Karypis, G., Konstan, J., and Reidl, J. Item-Based Collaborative Filtering Recommendation Algorithms.",
                "In Proceedings of the Tenth International World Wide Web Conference (WWW10) (2001), pp. 285-295. [13] Schein, A., Popescu, A., Ungar, L., and Pennock, D. Methods and Metrics for Cold-Start Recommendation.",
                "In Proc.",
                "SIGIR (2002), pp. 253-260. 259"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "La calificación P2 (Jeff → Star Wars) ayuda al sistema a recomendar la máscara a Tom conectando el Scout al \"Promotor\".3.",
                "Formalmente, dada una predicción PU, un elemento objetivo A para el usuario U, un explorador para PU, A es una calificación RU, yo tal que existe un usuario V con calificaciones RV, A y RV, para algunos elementos I;Un \"promotor\" para PU, A es un RV de calificación, A para algún usuario V, de modo que existan calificaciones RV, I y Ru, I para un artículo I y;Un conector para PU, un 252 Jim Tom Jeff Jerry, mi primo Vinny, el Matrix Star Wars The Mask Jurasic Park Figura 4: Scouts, promotores y conectores.es una calificación RV, I por algún usuario V y calificación I, de modo que exista las calificaciones Ru, I y RV, a.",
                "Concluimos que un buen \"promotor\" conecta un elemento con un vecindario más amplio de otros artículos y, por lo tanto, asegura que se recomiende a más usuarios.",
                "La calificación Jim → Mi primo Vinny es un pobre explorador y conector, pero es un muy buen \"promotor\".",
                "La calificación Jim → La máscara es un explorador razonablemente bueno, un muy buen conector y un buen \"promotor\".",
                "Finalmente, la calificación Jerry → Star Wars es un muy buen explorador, pero no tiene ningún valor como conector o \"promotor\".",
                "Este tributo, m (e) ∗ influencia (u, j, v, i), es a su vez heredado por cada una de las calificaciones en el camino [ru, j, rv, j, rv, i], con el crédito/culpaacumulando a los roles respectivos de Ru, J como Scout, RV, J como conector y RV, yo como \"promotor\".",
                "En otras palabras, el valor Scout SF (Ru, J), el valor del conector CF (RV, J) y el valor \"Promotor\" PF (RV, I) están incrementados por la cantidad de tributo.",
                "Una posible explicación es que no ha habido tiempo suficiente para que las películas populares acumulen calificaciones.255-10 0 10 20 20 30 40 50 60 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 FRIDUENCIA DE VALOR DE SCOUT Figura 6: Distribución de valores de exploración para un usuario de muestra.-0.4 -0.2 0 0.2 0.4 0.6 0.8 30000 60000 90000 120000 150000 180000 Popularidad Pop*VAR Figura 7: Correlación entre el valor de exploración agregado y la popularidad del artículo (calculado a diferentes intervalos).-0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 30000 60000 90000 120000 150000 180000 Figura 8: Correlación entre el valor agregado del \"promotor\" y la prolificidad del usuario (calculado a diferentes intervalos).",
                "Observamos que la distribución de frecuencia de los valores de \"promotor\" para las clasificaciones de películas de muestra también es gaussiana (similar a la Fig. 6)."
            ],
            "translated_text": "",
            "candidates": [
                "Promotor",
                "Promotor",
                "promotor",
                "promotor",
                "promotor",
                "promotor",
                "promotor",
                "promotor",
                "promotor",
                "promotor",
                "promotor",
                "promotor",
                "promotor",
                "promotor",
                "promotor",
                "Promotor",
                "promotor",
                "promotor",
                "promotor",
                "promotor"
            ],
            "error": []
        },
        "connector": {
            "translated_key": "conector",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Scouts, Promoters, and Connectors: The Roles of Ratings in Nearest Neighbor Collaborative Filtering Bharath Kumar Mohan Dept.",
                "of CSA Indian Institute of Science Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. of Computer Science Eastern Michigan University Ypsilanti, MI 48917, USA bkeller@emich.edu Naren Ramakrishnan Dept. of Computer Science Virginia Tech, Blacksburg VA 24061, USA naren@cs.vt.edu ABSTRACT Recommender systems aggregate individual user ratings into predictions of products or services that might interest visitors.",
                "The quality of this aggregation process crucially affects the user experience and hence the effectiveness of recommenders in e-commerce.",
                "We present a novel study that disaggregates global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearestneighbor collaborative filtering.",
                "In particular, we formulate three roles-scouts, promoters, and connectors-that capture how users receive recommendations, how items get recommended, and how ratings of these two types are themselves connected (resp.).",
                "These roles find direct uses in improving recommendations for users, in better targeting of items and, most importantly, in helping monitor the health of the system as a whole.",
                "For instance, they can be used to track the evolution of neighborhoods, to identify rating subspaces that do not contribute (or contribute negatively) to system performance, to enumerate users who are in danger of leaving, and to assess the susceptibility of the system to attacks such as shilling.",
                "We argue that the three rating roles presented here provide broad primitives to manage a recommender system and its community.",
                "Categories and Subject Descriptors H.4.2 [Information Systems Applications]: Types of Systems-Decision support; J.4 [Computer Applications]: Social and Behavioral Sciences General Terms Algorithms, Human Factors 1.",
                "INTRODUCTION Recommender systems have become integral to e-commerce, providing technology that suggests products to a visitor based on previous purchases or rating history.",
                "Collaborative filtering, a common form of recommendation, predicts a users rating for an item by combining (other) ratings of that user with other users ratings.",
                "Significant research has been conducted in implementing fast and accurate collaborative filtering algorithms [2, 7], designing interfaces for presenting recommendations to users [1], and studying the robustness of these algorithms [8].",
                "However, with the exception of a few studies on the influence of users [10], little attention has been paid to unraveling the inner workings of a recommender in terms of the individual ratings and the roles they play in making (good) recommendations.",
                "Such an understanding will give an important handle to monitoring and managing a recommender system, to engineer mechanisms to sustain the recommender, and thereby ensure its continued success.",
                "Our motivation here is to disaggregate global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearest-neighbor collaborative filtering.",
                "We identify three possible roles: (scouts) to connect the user into the system to receive recommendations, (promoters) to connect an item into the system to be recommended, and (connectors) to connect ratings of these two kinds.",
                "Viewing ratings in this way, we can define the contribution of a rating in each role, both in terms of allowing recommendations to occur, and in terms of influence on the quality of recommendations.",
                "In turn, this capability helps support scenarios such as: 1.",
                "Situating users in better neighborhoods: A users ratings may inadvertently connect the user to a neighborhood for which the users tastes may not be a perfect match.",
                "Identifying ratings responsible for such bad recommendations and suggesting new items to rate can help situate the user in a better neighborhood. 2.",
                "Targeting items: Recommender systems suffer from lack of user participation, especially in cold-start scenarios [13] involving newly arrived items.",
                "Identifying users who can be encouraged to rate specific items helps ensure coverage of the recommender system. 3.",
                "Monitoring the evolution of the recommender system and its stakeholders: A recommender system is constantly under change: growing with new users and 250 items, shrinking with users leaving the system, items becoming irrelevant, and parts of the system under attack.",
                "Tracking the roles of a rating and its evolution over time provides many insights into the health of the system, and how it could be managed and improved.",
                "These include being able to identify rating subspaces that do not contribute (or contribute negatively) to system performance, and could be removed; to enumerate users who are in danger of leaving, or have left the system; and to assess the susceptibility of the system to attacks such as shilling [5].",
                "As we show, the characterization of rating roles presented here provides broad primitives to manage a recommender system and its community.",
                "The rest of the paper is organized as follows.",
                "Background on nearest-neighbor collaborative filtering and algorithm evaluation is discussed in Section 2.",
                "Section 3 defines and discusses the roles of a rating, and Section 4 defines measures of the contribution of a rating in each of these roles.",
                "In Section 5, we illustrate the use of these roles to address the goals outlined above. 2.",
                "BACKGROUND 2.1 Algorithms Nearest-neighbor collaborative filtering algorithms either use neighborhoods of users or neighborhoods of items to compute a prediction.",
                "An algorithm of the first kind is called user-based, and one of the second kind is called itembased [12].",
                "In both families of algorithms, neighborhoods are formed by first computing the similarity between all pairs of users (for user-based) or items (for item-based).",
                "Predictions are then computed by aggregating ratings, which in a user-based algorithm involves aggregating the ratings of the target item by the users neighbors and, in an item-based algorithm, involves aggregating the users ratings of items that are neighbors of the target item.",
                "Algorithms within these families differ in the definition of similarity, formation of neighborhoods, and the computation of predictions.",
                "We consider a user-based algorithm based on that defined for GroupLens [11] with variations from Herlocker et al. [2], and an item-based algorithm similar to that of Sarwar et al. [12].",
                "The algorithm used by Resnick et al. [11] defines the similarity of two users u and v as the Pearson correlation of their common ratings: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2 , where Iu is the set of items rated by user u, ru,i is user us rating for item i, and ¯ru is the average rating of user u (similarly for v).",
                "Similarity computed in this manner is typically scaled by a factor proportional to the number of common ratings, to reduce the chance of making a recommendation made on weak connections: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), where γ ≈ 5 is a constant used as a lower limit in scaling [2].",
                "These new similarities are then used to define a static neighborhood Nu for each user u consisting of the top K users most similar to user u.",
                "A prediction for user u and item i is computed by a weighted average of the ratings by the neighbors pu,i = ¯ru + P v∈V sim (u, v)(rv,i − ¯rv) P v∈V sim (u, v) (1) where V = Nu ∩ Ui is the set of users most similar to u who have rated i.",
                "The item-based algorithm we use is the one defined by Sarwar et al. [12].",
                "In this algorithm, similarity is defined as the adjusted cosine measure sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) where Ui is the set of users who have rated item i.",
                "As for the user-based algorithm, the similarity weights are adjusted proportionally to the number of users that have rated the items in common sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). (3) Given the similarities, the neighborhood Ni of an item i is defined as the top K most similar items for that item.",
                "A prediction for user u and item i is computed as the weighted average pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) where J = Ni ∩ Iu is the set of items rated by u that are most similar to i. 2.2 Evaluation Recommender algorithms have typically been evaluated using measures of predictive accuracy and coverage [3].",
                "Studies on recommender algorithms, notably Herlocker et al. [2] and Sarwar et al. [12], typically compute predictive accuracy by dividing a set of ratings into training and test sets, and compute the prediction for an item in the test set using the ratings in the training set.",
                "A standard measure of predictive accuracy is mean absolute error (MAE), which for a test set T = {(u, i)} is defined as, MAE = P (u,i)∈T |pu,i − ru,i| |T | . (5) Coverage has a number of definitions, but generally refers to the proportion of items that can be predicted by the algorithm [3].",
                "A practical issue with predictive accuracy is that users typically are presented with recommendation lists, and not individual numeric predictions.",
                "Recommendation lists are lists of items in decreasing order of prediction (sometimes stated in terms of star-ratings), and so predictive accuracy may not be reflective of the accuracy of the list.",
                "So, instead we can measure recommendation or rank accuracy, which indicates the extent to which the list is in the correct order.",
                "Herlocker et al. [3] discuss a number of rank accuracy measures, which range from Kendalls Tau to measures that consider the fact that users tend to only look at a prefix of the list [5].",
                "Kendalls Tau measures the number of inversions when comparing ordered pairs in the true user ordering of 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figure 1: Ratings in simple movie recommender. items and the recommended order, and is defined as τ = C − D p (C + D + TR)(C + D + TP) (6) where C is the number of pairs that the system predicts in the correct order, D the number of pairs the system predicts in the wrong order, TR the number of pairs in the true ordering that have the same ratings, and TP is the number of pairs in the predicted ordering that have the same ratings [3].",
                "A shortcoming of the Tau metric is that it is oblivious to the position in the ordered list where the inversion occurs [3].",
                "For instance, an inversion toward the end of the list is given the same weight as one in the beginning.",
                "One solution is to consider inversions only in the top few items in the recommended list or to weight inversions based on their position in the list. 3.",
                "ROLES OF A RATING Our basic observation is that each rating plays a different role in each prediction in which it is used.",
                "Consider a simplified movie recommender system with three users Jim, Jeff, and Tom and their ratings for a few movies, as shown in Fig. 1. (For this initial discussion we will not consider the rating values involved.)",
                "The recommender predicts whether Tom will like The Mask using the other already available ratings.",
                "How this is done depends on the algorithm: 1.",
                "An item-based collaborative filtering algorithm constructs a neighborhood of movies around The Mask by using the ratings of users who rated The Mask and other movies similarly (e.g., Jims ratings of The Matrix and The Mask; and Jeffs ratings of Star Wars and The Mask).",
                "Toms ratings of those movies are then used to make a prediction for The Mask. 2.",
                "A user-based collaborative filtering algorithm would construct a neighborhood around Tom by tracking other users whose rating behaviors are similar to Toms (e.g., Tom and Jeff have rated Star Wars; Tom and Jim have rated The Matrix).",
                "The prediction of Toms rating for The Mask is then based on the ratings of Jeff and Tim.",
                "Although the nearest-neighbor algorithms aggregate the ratings to form neighborhoods used to compute predictions, we can disaggregate the similarities to view the computation of a prediction as simultaneously following parallel paths of ratings.",
                "So, irrespective of the collaborative filtering algorithm used, we can visualize the prediction of Toms rating of The Mask as walking through a sequence of ratings.",
                "In Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figure 2: Ratings used to predict The Mask for Tom.",
                "Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figure 3: Prediction of The Mask for Tom in which a rating is used more than once. this example, two paths were used for this prediction as depicted in Fig. 2: (p1, p2, p3) and (q1, q2, q3).",
                "Note that these paths are undirected, and are all of length 3.",
                "Only the order in which the ratings are traversed is different between the item-based algorithm (e.g., (p3, p2, p1), (q3, q2, q1)) and the user-based algorithm (e.g., (p1, p2, p3), (q1, q2, q3).)",
                "A rating can be part of many paths for a single prediction as shown in Fig. 3, where three paths are used for a prediction, two of which follow p1: (p1, p2, p3) and (p1, r2, r3).",
                "Predictions in a collaborative filtering algorithms may involve thousands of such walks in parallel, each playing a part in influencing the predicted value.",
                "Each prediction path consists of three ratings, playing roles that we call scouts, promoters, and connectors.",
                "To illustrate these roles, consider the path (p1, p2, p3) in Fig. 2 used to make a prediction of The Mask for Tom: 1.",
                "The rating p1 (Tom → Star Wars) makes a connection from Tom to other ratings that can be used to predict Toms rating for The Mask.",
                "This rating serves as a scout in the bipartite graph of ratings to find a path that leads to The Mask. 2.",
                "The rating p2 (Jeff → Star Wars) helps the system recommend The Mask to Tom by connecting the scout to the promoter. 3.",
                "The rating p3 (Jeff → The Mask) allows connections to The Mask, and, therefore, promotes this movie to Tom.",
                "Formally, given a prediction pu,a of a target item a for user u, a scout for pu,a is a rating ru,i such that there exists a user v with ratings rv,a and rv,i for some item i; a promoter for pu,a is a rating rv,a for some user v, such that there exist ratings rv,i and ru,i for an item i, and; a <br>connector</br> for pu,a 252 Jim Tom Jeff Jerry My Cousin Vinny The Matrix Star Wars The Mask Jurasic Park Figure 4: Scouts, promoters, and connectors. is a rating rv,i by some user v and rating i, such that there exists ratings ru,i and rv,a.",
                "The scouts, connectors, and promoters for the prediction of Toms rating of The Mask are p1 and q1, p2 and q2, and p3 and q3 (respectively).",
                "Each of these roles has a value in the recommender to the user, the users neighborhood, and the system in terms of allowing recommendations to be made. 3.1 Roles in Detail Ratings that act as scouts tend to help the recommender system suggest more movies to the user, though the extent to which this is true depends on the rating behavior of other users.",
                "For example, in Fig. 4 the rating Tom → Star Wars helps the system recommend only The Mask to him, while Tom → The Matrix helps recommend The Mask, Jurassic Park, and My Cousin Vinny.",
                "Tom makes a connection to Jim who is a prolific user of the system, by rating The Matrix.",
                "However, this does not make The Matrix the best movie to rate for everyone.",
                "For example, Jim is benefited equally by both The Mask and The Matrix, which allow the system to recommend Star Wars to him.",
                "His rating of The Mask is the best scout for Jeff, and Jerrys only scout is his rating of Star Wars.",
                "This suggests that good scouts allow a user to build similarity with prolific users, and thereby ensure they get more from the system.",
                "While scouts represent beneficial ratings from the perspective of a user, promoters are their duals, and are of benefit to items.",
                "In Fig. 4, My Cousin Vinny benefits from Jims rating, since it allows recommendations to Jeff and Tom.",
                "The Mask is not so dependent on just one rating, since the ratings by Jim and Jeff help it.",
                "On the other hand, Jerrys rating of Star Wars does not help promote it to any other user.",
                "We conclude that a good promoter connects an item to a broader neighborhood of other items, and thereby ensures that it is recommended to more users.",
                "Connectors serve a crucial role in a recommender system that is not as obvious.",
                "The movies My Cousin Vinny and Jurassic Park have the highest recommendation potential since they can be recommended to Jeff, Jerry and Tom based on the linkage structure illustrated in Fig. 4.",
                "Beside the fact that Jim rated these movies, these recommendations are possible only because of the ratings Jim → The Matrix and Jim → The Mask, which are the best connectors.",
                "A <br>connector</br> improves the systems ability to make recommendations with no explicit gain for the user.",
                "Note that every rating can be of varied benefit in each of these roles.",
                "The rating Jim → My Cousin Vinny is a poor scout and <br>connector</br>, but is a very good promoter.",
                "The rating Jim → The Mask is a reasonably good scout, a very good <br>connector</br>, and a good promoter.",
                "Finally, the rating Jerry → Star Wars is a very good scout, but is of no value as a <br>connector</br> or promoter.",
                "As illustrated here, a rating can have different value in each of the three roles in terms of whether a recommendation can be made or not.",
                "We could measure this value by simply counting the number of times a rating is used in each role, which alone would be helpful in characterizing the behavior of a system.",
                "But we can also measure the contribution of each rating to the quality of recommendations or health of the system.",
                "Since every prediction is a combined effort of several recommendation paths, we are interested in discerning the influence of each rating (and, hence, each path) in the system towards the systems overall error.",
                "We can understand the dynamics of the system at a finer granularity by tracking the influence of a rating according to the role played.",
                "The next section describes the approach to measuring the values of a rating in each role. 4.",
                "CONTRIBUTIONS OF RATINGS As weve seen, a rating may play different roles in different predictions and, in each prediction, contribute to the quality of a prediction in different ways.",
                "Our approach can use any numeric measure of a property of system health, and assigns credit (or blame) to each rating proportional to its influence in the prediction.",
                "By tracking the role of each rating in a prediction, we can accumulate the credit for a rating in each of the three roles, and also track the evolution of the roles of rating over time in the system.",
                "This section defines the methodology for computing the contribution of ratings by first defining the influence of a rating, and then instantiating the approach for predictive accuracy, and then rank accuracy.",
                "We also demonstrate how these contributions can be aggregated to study the neighborhood of ratings involved in computing a users recommendations.",
                "Note that although our general formulation for rating influence is algorithm independent, due to space considerations, we present the approach for only item-based collaborative filtering.",
                "The definition for user-based algorithms is similar and will be presented in an expanded version of this paper. 4.1 Influence of Ratings Recall that an item-based approach to collaborative filtering relies on building item neighborhoods using the similarity of ratings by the same user.",
                "As described earlier, similarity is defined by the adjusted cosine measure (Equations (2) and (3)).",
                "A set of the top K neighbors is maintained for all items for space and computational efficiency.",
                "A prediction of item i for a user u is computed as the weighted deviation from the items mean rating as shown in Equation (4).",
                "The list of recommendations for a user is then the list of items sorted in descending order of their predicted values.",
                "We first define impact(a, i, j), the impact a user a has in determining the similarity between two items i and j.",
                "This is the change in the similarity between i and j when as rating is removed, and is defined as impact(a, i, j) = |sim (i, j) − sim¯a(i, j)| P w∈Cij |sim (i, j) − sim ¯w(i, j)| where Cij = {u ∈ U | ∃ ru,i, ru,j ∈ R(u)} is the set of coraters 253 of items i and j (users who rate both i and j), R(u) is the set of ratings provided by user u, and sim¯a(i, j) is the similarity of i and j when the ratings of user a are removed sim¯a(i, j) = P v∈U\\{a} (ru,i − ¯ru)(ru,j − ¯ru) qP u∈U\\{a}(ru,i − ¯ru)2 qP u∈U\\{a}(ru,j − ¯ru)2 , and adjusted for the number of raters sim¯a(i, j) = max(|Ui ∩ Uj| − 1, γ) γ · sim(i, j).",
                "If all coraters of i and j rate them identically, we define the impact as impact(a, i, j) = 1 |Cij| since P w∈Cij |sim (i, j) − sim ¯w(i, j)| = 0.",
                "The influence of each path (u, j, v, i) = [ru,j, rv,j, rv,i] in the prediction of pu,i is given by influence(u, j, v, i) = sim (i, j) P l∈Ni∩Iu sim (i, l) · impact(v, i, j) It follows that the sum of influences over all such paths, for a given set of endpoints, is 1. 4.2 Role Values for Predictive Accuracy The value of a rating in each role is computed from the influence depending on the evaluation measure employed.",
                "Here we illustrate the approach using predictive accuracy as the evaluation metric.",
                "In general, the goodness of a prediction decides whether the ratings involved must be credited or discredited for their role.",
                "For predictive accuracy, the error in prediction e = |pu,i − ru,i| is mapped to a comfort level using a mapping function M(e).",
                "Anecdotal evidence suggests that users are unable to discern errors less than 1.0 (for a rating scale of 1 to 5) [4], and so an error less than 1.0 is considered acceptable, but anything larger is not.",
                "We hence define M(e) as (1 − e) binned to an appropriate value in [−1, −0.5, 0.5, 1].",
                "For each prediction pu,i, M(e) is attributed to all the paths that assisted the computation of pu,i, proportional to their influences.",
                "This tribute, M(e)∗influence(u, j, v, i), is in turn inherited by each of the ratings in the path [ru,j, rv,j, rv,i], with the credit/blame accumulating to the respective roles of ru,j as a scout, rv,j as a <br>connector</br>, and rv,i as a promoter.",
                "In other words, the scout value SF(ru,j), the <br>connector</br> value CF(rv,j) and the promoter value PF(rv,i) are all incremented by the tribute amount.",
                "Over a large number of predictions, scouts that have repeatedly resulted in big error rates have a big negative scout value, and vice versa (similarly with the other roles).",
                "Every rating is thus summarized by its triple [SF, CF, PF]. 4.3 Role Values for Rank Accuracy We now define the computation of the contribution of ratings to observed rank accuracy.",
                "For this computation, we must know the users preference order for a set of items for which predictions can be computed.",
                "We assume that we have a test set of the users ratings of the items presented in the recommendation list.",
                "For every pair of items rated by a user in the test data, we check whether the predicted order is concordant with his preference.",
                "We say a pair (i, j) is concordant (with error ) whenever one of the following holds: • if (ru,i < ru,j) then (pu,i − pu,j < ); • if (ru,i > ru,j) then (pu,i − pu,j > ); or • if (ru,i = ru,j) then (|pu,i − pu,j| ≤ ).",
                "Similarly, a pair (i, j) is discordant (with error ) if it is not concordant.",
                "Our experiments described below use an error tolerance of = 0.1.",
                "All paths involved in the prediction of the two items in a concordant pair are credited, and the paths involved in a discordant pair are discredited.",
                "The credit assigned to a pair of items (i, j) in the recommendation list for user u is computed as c(i, j) = ( t T · 1 C+D if (i, j) are concordant − t T · 1 C+D if (i, j) are discordant (7) where t is the number of items in the users test set whose ratings could be predicted, T is the number of items rated by user u in the test set, C is the number of concordances and D is the number of discordances.",
                "The credit c is then divided among all paths responsible for predicting pu,i and pu,j proportional to their influences.",
                "We again add the role values obtained from all the experiments to form a triple [SF, CF, PF] for each rating. 4.4 Aggregating rating roles After calculating the role values for individual ratings, we can also use these values to study neighborhoods and the system.",
                "Here we consider how we can use the role values to characterize the health of a neighborhood.",
                "Consider the list of top recommendations presented to a user at a specific point in time.",
                "The collaborative filtering algorithm traversed many paths in his neighborhood through his scouts and other connectors and promoters to make these recommendations.",
                "We call these ratings the recommender neighborhood of the user.",
                "The user implicitly chooses this neighborhood of ratings through the items he rates.",
                "Apart from the collaborative filtering algorithm, the health of this neighborhood completely influences a users satisfaction with the system.",
                "We can characterize a users recommender neighborhood by aggregating the individual role values of the ratings involved, weighted by the influence of individual ratings in determining his recommended list.",
                "Different sections of the users neighborhood wield varied influence on his recommendation list.",
                "For instance, ratings reachable through highly rated items have a bigger say in the recommended items.",
                "Our aim is to study the system and classify users with respect to their positioning in a healthy or unhealthy neighborhood.",
                "A user can have a good set of scouts, but may be exposed to a neighborhood with bad connectors and promoters.",
                "He can have a good neighborhood, but his bad scouts may ensure the neighborhoods potential is rendered useless.",
                "We expect that users with good scouts and good neighborhoods will be most satisfied with the system in the future.",
                "A users neighborhood is characterized by a triple that represents the weighted sum of the role values of individual ratings involved in making recommendations.",
                "Consider a user u and his ordered list of recommendations L. An item i 254 in the list is weighted inversely, as K(i), depending on its position in the list.",
                "In our studies we use K(i) = p position(i).",
                "Several paths of ratings [ru,j, rv,j, rv,i] are involved in predicting pu,i which ultimately decides its position in L, each with influence(u, j, v, i).",
                "The recommender neighborhood of a user u is characterized by the triple, [SFN(u), CFN(u), PFN(u)] where SFN(u) = X i∈L P [ru,j ,rv,j ,rv,i] SF(ru,j)influence(u, j, v, i) K(i) !",
                "CFN(u) and PFN(u) are defined similarly.",
                "This triple estimates the quality of us recommendations based on the past track record of the ratings involved in their respective roles. 5.",
                "EXPERIMENTATION As we have seen, we can assign role values to each rating when evaluating a collaborative filtering system.",
                "In this section, we demonstrate the use of this approach to our overall goal of defining an approach to monitor and manage the health of a recommender system through experiments done on the MovieLens million rating dataset.",
                "In particular, we discuss results relating to identifying good scouts, promoters, and connectors; the evolution of rating roles; and the characterization of user neighborhoods. 5.1 Methodology Our experiments use the MovieLens million rating dataset, which consists of ratings by 6040 users of 3952 movies.",
                "The ratings are in the range 1 to 5, and are labeled with the time the rating was given.",
                "As discussed before, we consider only the item-based algorithm here (with item neighborhoods of size 30) and, due to space considerations, only present role value results for rank accuracy.",
                "Since we are interested in the evolution of the rating role values over time, the model of the recommender system is built by processing ratings in their arrival order.",
                "The timestamping provided by MovieLens is hence crucial for the analyses presented here.",
                "We make assessments of rating roles at intervals of 10,000 ratings and processed the first 200,000 ratings in the dataset (giving rise to 20 snapshots).",
                "We incrementally update the role values as the time ordered ratings are merged into the model.",
                "To keep the experiment computationally manageable, we define a test dataset for each user.",
                "As the time ordered ratings are merged into the model, we label a small randomly selected percentage (20%) as test data.",
                "At discrete epochs, i.e., after processing every 10,000 ratings, we compute the predictions for the ratings in the test data, and then compute the role values for the ratings used in the predictions.",
                "One potential criticism of this methodology is that the ratings in the test set are never evaluated for their roles.",
                "We overcome this concern by repeating the experiment, using different random seeds.",
                "The probability that every rating is considered for evaluation is then considerably high: 1 − 0.2n , where n is the number of times the experiment is repeated with different random seeds.",
                "The results here are based on n = 4 repetitions.",
                "The item-based collaborative filtering algorithms performance was ordinary with respect to rank accuracy.",
                "Fig. 5 shows a plot of the precision and recall as ratings were merged in time order into the model.",
                "The recall was always high, but the average precision was just about 53%. 0 0.2 0.4 0.6 0.8 1 1.2 10000 30000 50000 70000 90000110000130000150000 Ratings merged into model Value Precision Recall Figure 5: Precision and recall for the item-based collaborative filtering algorithm. 5.2 Inducing good scouts The ratings of a user that serve as scouts are those that allow the user to receive recommendations.",
                "We claim that users with ratings that have respectable scout values will be happier with the system than those with ratings with low scout values.",
                "Note that the item-based algorithm discussed here produces recommendation lists with nearly half of the pairs in the list discordant from the users preference.",
                "Whether all of these discordant pairs are observable by the user is unclear, however, certainly this suggests that there is a need to be able to direct users to items whose ratings would improve the lists.",
                "The distribution of the scout values for most users ratings are Gaussian with mean zero.",
                "Fig. 6 shows the frequency distribution of scout values for a sample user at a given snapshot.",
                "We observe that a large number of ratings never serve as scouts for their users.",
                "A relatable scenario is when Amazons recommender makes suggestions of books or items based on other items that were purchased as gifts.",
                "With simple relevance feedback from the user, such ratings can be isolated as bad scouts and discounted from future predictions.",
                "Removing bad scouts was found to be worthwhile for individual users but the overall performance improvement was only marginal.",
                "An obvious question is whether good scouts can be formed by merely rating popular movies as suggested by Rashid et al. [9].",
                "They show that a mix of popularity and rating entropy identifies the best items to suggest to new users when evaluated using MAE.",
                "Following their intuition, we would expect to see a higher correlation between popularityentropy and good scouts.",
                "We measured the Pearson correlation coefficient between aggregated scout values for a movie with the popularity of a movie (number of times it is rated); and with its popularity*variance measure at different snapshots of the system.",
                "Note that the scout values were initially anti-correlated with popularity (Fig. 7), but became moderately correlated as the system evolved.",
                "Both popularity and popularity*variance performed similarly.",
                "A possible explanation is that there has been insufficient time for the popular movies to accumulate ratings. 255 -10 0 10 20 30 40 50 60 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 Scout Value Frequency Figure 6: Distribution of scout values for a sample user. -0.4 -0.2 0 0.2 0.4 0.6 0.8 30000 60000 90000 120000 150000 180000 Popularity Pop*Var Figure 7: Correlation between aggregated scout value and item popularity (computed at different intervals). -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 30000 60000 90000 120000 150000 180000 Figure 8: Correlation between aggregated promoter value and user prolificity (computed at different intervals).",
                "Table 1: Movies forming the best scouts.",
                "Best Scouts Conf.",
                "Pop.",
                "Being John Malkovich (1999) 1.00 445 Star Wars: Episode IV - A New Hope (1977) 0.92 623 Princess Bride, The (1987) 0.85 477 Sixth Sense, The (1999) 0.85 617 Matrix, The (1999) 0.77 522 Ghostbusters (1984) 0.77 441 Casablanca (1942) 0.77 384 Insider, The (1999) 0.77 235 American Beauty (1999) 0.69 624 Terminator 2: Judgment Day (1991) 0.69 503 Fight Club (1999) 0.69 235 Shawshank Redemption, The (1994) 0.69 445 Run Lola Run (Lola rennt) (1998) 0.69 220 Terminator, The (1984) 0.62 450 Usual Suspects, The (1995) 0.62 326 Aliens (1986) 0.62 385 North by Northwest (1959) 0.62 245 Fugitive, The (1993) 0.62 402 End of Days (1999) 0.62 132 Raiders of the Lost Ark (1981) 0.54 540 Schindlers List (1993) 0.54 453 Back to the Future (1985) 0.54 543 Toy Story (1995) 0.54 419 Alien (1979) 0.54 415 Abyss, The (1989) 0.54 345 2001: A Space Odyssey (1968) 0.54 358 Dogma (1999) 0.54 228 Little Mermaid, The (1989) 0.54 203 Table 2: Movies forming the worst scouts.",
                "Worst scouts Conf.",
                "Pop.",
                "Harold and Maude (1971) 0.46 141 Grifters, The (1990) 0.46 180 Sting, The (1973) 0.38 244 Godfather: Part III, The (1990) 0.38 154 Lawrence of Arabia (1962) 0.38 167 High Noon (1952) 0.38 84 Women on the Verge of a... (1988) 0.38 113 Grapes of Wrath, The (1940) 0.38 115 Duck Soup (1933) 0.38 131 Arsenic and Old Lace (1944) 0.38 138 Midnight Cowboy (1969) 0.38 137 To Kill a Mockingbird (1962) 0.31 195 Four Weddings and a Funeral (1994) 0.31 271 Good, The Bad and The Ugly, The (1966) 0.31 156 Its a Wonderful Life (1946) 0.31 146 Player, The (1992) 0.31 220 Jackie Brown (1997) 0.31 118 Boat, The (Das Boot) (1981) 0.31 210 Manhattan (1979) 0.31 158 Truth About Cats & Dogs, The (1996) 0.31 143 Ghost (1990) 0.31 227 Lone Star (1996) 0.31 125 Big Chill, The (1983) 0.31 184 256 By studying the evolution of scout values, we can identify movies that consistently feature in good scouts over time.",
                "We claim these movies will make viable scouts for other users.",
                "We found the aggregated scout values for all movies in intervals of 10,000 ratings each.",
                "A movie is said to induce a good scout if the movie was in the top 100 of the sorted list, and to induce a bad scout if it was in bottom 100 of the same list.",
                "Movies appearing consistently high over time are expected to remain up there in the future.",
                "The effective confidence in a movie m is Cm = Tm − Bm N (8) where Tm is the number of times it appeared in the top 100, Bm the number of times it appeared in the bottom 100, and N is the number of intervals considered.",
                "Using this measure, the top few movies expected to induce the best scouts are shown in Table 1.",
                "Movies that would be bad scout choices are shown in Table 2 with their associated confidences.",
                "The popularities of the movies are also displayed.",
                "Although more popular movies appear in the list of good scouts, these tables show that a blind choice of scout based on popularity alone can be potentially dangerous.",
                "Interestingly, the best scout-Being John Malkovich-is about a puppeteer who discovers a portal into a movie star, a movie that has been described variously on amazon.com as makes you feel giddy, seriously weird, comedy with depth, silly, strange, and inventive.",
                "Indicating whether someone likes this movie or not goes a long way toward situating the user in a suitable neighborhood, with similar preferences.",
                "On the other hand, several factors may have made a movie a bad scout, like the sharp variance in user preferences in the neighborhood of a movie.",
                "Two users may have the same opinion about Lawrence of Arabia, but they may differ sharply about how they felt about the other movies they saw.",
                "Bad scouts ensue when there is deviation in behavior around a common synchronization point. 5.3 Inducing good promoters Ratings that serve to promote items in a collaborative filtering system are critical to allowing a new item be recommended to users.",
                "So, inducing good promoters is important for cold-start recommendation.",
                "We note that the frequency distribution of promoter values for a sample movies ratings is also Gaussian (similar to Fig. 6).",
                "This indicates that the promotion of a movie is benefited most by the ratings of a few users, and are unaffected by the ratings of most users.",
                "We find a strong correlation between a users number of ratings and his aggregated promoter value.",
                "Fig. 8 depicts the evolution of the Pearson correlation co-efficient between the prolificity of a user (number of ratings) versus his aggregated promoter value.",
                "We expect that conspicuous shills, by recommending wrong movies to users, will be discredited with negative aggregate promoter values and should be identifiable easily.",
                "Given this observation, the obvious rule to follow when introducing a new movie is to have it rated directly by prolific users who posses high aggregated promoter values.",
                "A new movie is thus cast into the neighborhood of many other movies improving its visibility.",
                "Note, though, that a user may have long stopped using the system.",
                "Tracking promoter values consistently allows only the most active recent users to be considered. 5.4 Inducing good connectors Given the way scouts, connectors, and promoters are characterized, it follows that the movies that are part of the best scouts are also part of the best connectors.",
                "Similarly, the users that constitute the best promoters are also part of the best connectors.",
                "Good connectors are induced by ensuring a user with a high promoter value rates a movie with a high scout value.",
                "In our experiments, we find that a ratings longest standing role is often as a <br>connector</br>.",
                "A rating with a poor <br>connector</br> value is often seen due to its user being a bad promoter, or its movie being a bad scout.",
                "Such ratings can be removed from the prediction process to bring marginal improvements to recommendations.",
                "In some selected experiments, we observed that removing a set of badly behaving connectors helped improve the systems overall performance by 1.5%.",
                "The effect was even higher on a few select users who observed an improvement of above 10% in precision without much loss in recall. 5.5 Monitoring the evolution of rating roles One of the more significant contributions of our work is the ability to model the evolution of recommender systems, by studying the changing roles of ratings over time.",
                "The role and value of a rating can change depending on many factors like user behavior, redundancy, shilling effects or properties of the collaborative filtering algorithm used.",
                "Studying the dynamics of rating roles in terms of transitions between good, bad, and negligible values can provide insights into the functioning of the recommender system.",
                "We believe that a continuous visualization of these transitions will improve the ability to manage a recommender system.",
                "We classify different rating states as good, bad, or negligible.",
                "Consider a user who has rated 100 movies in a particular interval, of which 20 are part of the test set.",
                "If a scout has a value greater than 0.005, it indicates that it is uniquely involved in at least 2 concordant predictions, which we will say is good.",
                "Thus, a threshold of 0.005 is chosen to bin a rating as good, bad or negligible in terms of its scout, <br>connector</br> and promoter value.",
                "For instance, a rating r, at time t with role value triple [0.1, 0.001, −0.01] is classified as [scout +, <br>connector</br> 0, promoter −], where + indicates good, 0 indicates negligible, and − indicates bad.",
                "The positive credit held by a rating is a measure of its contribution to the betterment of the system, and the discredit is a measure of its contribution to the detriment of the system.",
                "Even though the positive roles (and the negative roles) make up a very small percentage of all ratings, their contribution supersedes their size.",
                "For example, even though only 1.7% of all ratings were classified as good scouts, they hold 79% of all positive credit in the system!",
                "Similarly, the bad scouts were just 1.4% of all ratings but hold 82% of all discredit.",
                "Note that good and bad scouts, together, comprise only 1.4% + 1.7% = 3.1% of the ratings, implying that the majority of the ratings are negligible role players as scouts (more on this later).",
                "Likewise, good connectors were 1.2% of the system, and hold 30% of all positive credit.",
                "The bad connectors (0.8% of the system) hold 36% of all discredit.",
                "Good promoters (3% of the system) hold 46% of all credit, while bad promoters (2%) hold 50% of all discredit.",
                "This reiterates that a few ratings influence most of the systems performance.",
                "Hence it is important to track transitions between them regardless of their small numbers. 257 Across different snapshots, a rating can remain in the same state or change.",
                "A good scout can become a bad scout, a good promoter can become a good <br>connector</br>, good and bad scouts can become vestigial, and so on.",
                "It is not practical to expect a recommender system to have no ratings in bad roles.",
                "However, it suffices to see ratings in bad roles either convert to good or vestigial roles.",
                "Similarly, observing a large number of good roles become bad ones is a sign of imminent failure of the system.",
                "We employ the principle of non-overlapping episodes [6] to count such transitions.",
                "A sequence such as: [+, 0, 0] → [+, 0, 0] → [0, +, 0] → [0, 0, 0] is interpreted as the transitions [+, 0, 0] ; [0, +, 0] : 1 [+, 0, 0] ; [0, 0, 0] : 1 [0, +, 0] ; [0, 0, 0] : 1 instead of [+, 0, 0] ; [0, +, 0] : 2 [+, 0, 0] ; [0, 0, 0] : 2 [0, +, 0] ; [0, 0, 0] : 1.",
                "See [6] for further details about this counting procedure.",
                "Thus, a rating can be in one of 27 possible states, and there are about 272 possible transitions.",
                "We make a further simplification and utilize only 9 states, indicating whether the rating is a scout, promoter, or <br>connector</br>, and whether it has a positive, negative, or negligible role.",
                "Ratings that serve multiple purposes are counted using multiple episode instantiations but the states themselves are not duplicated beyond the 9 restricted states.",
                "In this model, a transition such as [+, 0, +] ; [0, +, 0] : 1 is counted as [scout+] ; [scout0] : 1 [scout+] ; [<br>connector</br>+] : 1 [scout+] ; [promoter0] : 1 [connector0] ; [scout0] : 1 [connector0] ; [scout+] : 1 [connector0] ; [promoter0] : 1 [promoter+] ; [scout0] : 1 [promoter+] ; [<br>connector</br>+] : 1 [promoter+] ; [promoter0] : 1 Of these, transitions like [pX] ; [q0] where p = q, X ∈ {+, 0, −} are considered uninteresting, and only the rest are counted.",
                "Fig. 9 depicts the major transitions counted while processing the first 200,000 ratings from the MovieLens dataset.",
                "Only transitions with frequency greater than or equal to 3% are shown.",
                "The percentages for each state indicates the number of ratings that were found to be in those states.",
                "We consider transitions from any state to a good state as healthy, from any state to a bad state as unhealthy, and from any state to a vestigial state as decaying.",
                "From Fig. 9, we can observe: • The bulk of the ratings have negligible values, irrespective of their role.",
                "The majority of the transitions involve both good and bad ratings becoming negligible.",
                "Scout + (2%) Scout(1.5%) Scout 0 (96.5%) <br>connector</br> + (1.2%) <br>connector</br>(0.8%) Connector 0 (98%) Promoter + (3%) Promoter(2%) Promoter 0 (95%) 84% 84% 81% 74% 10% 6% 11% 77% 8% 7% 8% 82% 4% 86% 4% 68% 15% 13% 5% 5% 77% 11% 7% 5% 4% 3% 3% 3% Healthy Unhealthy Decaying Figure 9: Transitions among rating roles. • The number of good ratings is comparable to the bad ratings, and ratings are seen to switch states often, except in the case of scouts (see below). • The negative and positive scout states are not reachable through any transition, indicating that these ratings must begin as such, and cannot be coerced into these roles. • Good promoters and good connectors have a much longer survival period than scouts.",
                "Transitions that recur to these states have frequencies of 10% and 15% when compared to just 4% for scouts.",
                "Good connectors are the slowest to decay whereas (good) scouts decay the fastest. • Healthy percentages are seen on transitions between promoters and connectors.",
                "As indicated earlier, there are hardly any transitions from promoters/connectors to scouts.",
                "This indicates that, over the long run, a users rating is more useful to others (movies or other users) than to the user himself. • The percentages of healthy transitions outweigh the unhealthy ones - this hints that the system is healthy, albeit only marginally.",
                "Note that these results are conditioned by the static nature of the dataset, which is a set of ratings over a fixed window of time.",
                "However a diagram such as Fig. 9 is clearly useful for monitoring the health of a recommender system.",
                "For instance, acceptable limits can be imposed on different types of transitions and, if a transition fails to meet the threshold, the recommender system or a part of it can be brought under closer scrutiny.",
                "Furthermore, the role state transition diagram would also be the ideal place to study the effects of shilling, a topic we will consider in future research. 5.6 Characterizing neighborhoods Earlier we saw that we can characterize the neighborhood of ratings involved in creating a recommendation list L for 258 a user.",
                "In our experiment, we consider lists of length 30, and sample the lists of about 5% of users through the evolution of the model (at intervals of 10,000 ratings each) and compute their neighborhood characteristics.",
                "To simplify our presentation, we consider the percentage of the sample that fall into one of the following categories: 1.",
                "Inactive user: (SFN(u) = 0) 2.",
                "Good scouts, Good neighborhood: (SFN(u) > 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 3.",
                "Good scouts, Bad neighborhood: (SFN(u) > 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) 4.",
                "Bad scouts, Good neighborhood: (SFN(u) < 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 5.",
                "Bad scouts, Bad neighborhood: (SFN(u) < 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) From our sample set of 561 users, we found that 476 users were inactive.",
                "Of the remaining 85 users, we found 26 users had good scouts and a good neighborhood, 6 had bad scouts and a good neighborhood, 29 had good scouts and a bad neighborhood, and 24 had bad scouts and a bad neighborhood.",
                "Thus, we conjecture that 59 users (29+24+6) are in danger of leaving the system.",
                "As a remedy, users with bad scouts and a good neighborhood can be asked to reconsider rating of some movies hoping to improve the systems recommendations.",
                "The system can be expected to deliver more if they engineer some good scouts.",
                "Users with good scouts and a bad neighborhood are harder to address; this situation might entail selectively removing some <br>connector</br>-promoter pairs that are causing the damage.",
                "Handling users with bad scouts and bad neighborhoods is a more difficult challenge.",
                "Such a classification allows the use of different strategies to better a users experience with the system depending on his context.",
                "In future work, we intend to conduct field studies and study the improvement in performance of different strategies for different contexts. 6.",
                "CONCLUSIONS To further recommender system acceptance and deployment, we require new tools and methodologies to manage an installed recommender and develop insights into the roles played by ratings.",
                "A fine-grained characterization in terms of rating roles such as scouts, promoters, and connectors, as done here, helps such an endeavor.",
                "Although we have presented results on only the item-based algorithm with list rank accuracy as the metric, the same approach outlined here applies to user-based algorithms and other metrics.",
                "In future research, we plan to systematically study the many algorithmic parameters, tolerances, and cutoff thresholds employed here and reason about their effects on the downstream conclusions.",
                "We also aim to extend our formulation to other collaborative filtering algorithms, study the effect of shilling in altering rating roles, conduct field studies, and evaluate improvements in user experience by tweaking ratings based on their role values.",
                "Finally, we plan to develop the idea of mining the evolution of rating role patterns into a reporting and tracking system for all aspects of recommender system health. 7.",
                "REFERENCES [1] Cosley, D., Lam, S., Albert, I., Konstan, J., and Riedl, J.",
                "Is Seeing Believing? : How Recommender System Interfaces Affect Users Opinions.",
                "In Proc.",
                "CHI (2001), pp. 585-592. [2] Herlocker, J. L., Konstan, J.",
                "A., Borchers, A., and Riedl, J.",
                "An Algorithmic Framework for Performing Collaborative Filtering.",
                "In Proc.",
                "SIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J.",
                "A., Terveen, L. G., and Riedl, J. T. Evaluating Collaborative Filtering Recommender Systems.",
                "ACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J.",
                "A.",
                "Personal communication. 2003. [5] Lam, S. K., and Riedl, J. Shilling Recommender Systems for Fun and Profit.",
                "In Proceedings of the 13th International World Wide Web Conference (2004), ACM Press, pp. 393-402. [6] Laxman, S., Sastry, P. S., and Unnikrishnan, K. P. Discovering Frequent Episodes and Learning Hidden Markov Models: A Formal Connection.",
                "IEEE Transactions on Knowledge and Data Engineering Vol. 17, 11 (2005), 1505-1517. [7] McLaughlin, M. R., and Herlocker, J. L. A Collaborative Filtering Algorithm and Evaluation Metric that Accurately Model the User Experience.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (2004), pp. 329 - 336. [8] OMahony, M., Hurley, N. J., Kushmerick, N., and Silvestre, G. Collaborative Recommendation: A Robustness Analysis.",
                "ACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J.",
                "A., and Riedl, J.",
                "Getting to Know You: Learning New User Preferences in Recommender Systems.",
                "In Proceedings of the 2002 Conference on Intelligent User Interfaces (IUI 2002) (2002), pp. 127-134. [10] Rashid, A. M., Karypis, G., and Riedl, J.",
                "Influence in Ratings-Based Recommender Systems: An Algorithm-Independent Approach.",
                "In Proc. of the SIAM International Conference on Data Mining (2005). [11] Resnick, P., Iacovou, N., Sushak, M., Bergstrom, P., and Riedl, J. GroupLens: An Open Architecture for Collaborative Filtering of Netnews.",
                "In Proceedings of the Conference on Computer Supported Collaborative Work (CSCW94) (1994), ACM Press, pp. 175-186. [12] Sarwar, B., Karypis, G., Konstan, J., and Reidl, J. Item-Based Collaborative Filtering Recommendation Algorithms.",
                "In Proceedings of the Tenth International World Wide Web Conference (WWW10) (2001), pp. 285-295. [13] Schein, A., Popescu, A., Ungar, L., and Pennock, D. Methods and Metrics for Cold-Start Recommendation.",
                "In Proc.",
                "SIGIR (2002), pp. 253-260. 259"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Formalmente, dada una predicción PU, un elemento objetivo A para el usuario U, un explorador para PU, A es una calificación RU, yo tal que existe un usuario V con calificaciones RV, A y RV, para algunos elementos I;Un promotor para PU, A es un RV de calificación, A para algún usuario V, de modo que existan calificaciones RV, I y Ru, I para un artículo I y;Un \"conector\" para PU, un 252 Jim Tom Jeff Jerry, mi primo Vinny, el Matrix Star Wars The Mask Jurasic Park Figura 4: Scouts, promotores y conectores.es una calificación RV, I por algún usuario V y calificación I, de modo que exista las calificaciones Ru, I y RV, a.",
                "Un \"conector\" mejora la capacidad de los sistemas para hacer recomendaciones sin ganancia explícita para el usuario.",
                "La calificación Jim → Mi primo Vinny es un pobre explorador y \"conector\", pero es un muy buen promotor.",
                "La calificación Jim → La máscara es un explorador razonablemente bueno, un muy buen \"conector\" y un buen promotor.",
                "Finalmente, la calificación Jerry → Star Wars es un muy buen explorador, pero no tiene ningún valor como un \"conector\" o promotor.",
                "Este tributo, m (e) ∗ influencia (u, j, v, i), es a su vez heredado por cada una de las calificaciones en el camino [ru, j, rv, j, rv, i], con el crédito/culpaacumulando a los roles respectivos de Ru, J como Scout, RV, J como \"conector\" y RV, yo como promotor.",
                "En otras palabras, el valor Scout SF (Ru, J), el valor del \"conector\" CF (RV, J) y el valor del promotor PF (RV, I) están incrementados por la cantidad de tributo.",
                "En nuestros experimentos, encontramos que un rol más largo de calificación es a menudo como un \"conector\".",
                "A menudo se ve una calificación con un valor de mal \"conector\" debido a que su usuario es un mal promotor, o su película es un mal explorador.",
                "Por lo tanto, se elige un umbral de 0.005 para acumular una calificación como buena, mala o insignificante en términos de su explorador, \"conector\" y valor promotor."
            ],
            "translated_text": "",
            "candidates": [
                "conector",
                "conector",
                "conector",
                "conector",
                "conector",
                "conector",
                "conector",
                "conector",
                "conector",
                "conector",
                "conector",
                "conector",
                "conector",
                "conector",
                "conector",
                "conector",
                "conector",
                "conector",
                "conector",
                "conector"
            ],
            "error": []
        },
        "list rank accuracy": {
            "translated_key": "precisión de rango de lista",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Scouts, Promoters, and Connectors: The Roles of Ratings in Nearest Neighbor Collaborative Filtering Bharath Kumar Mohan Dept.",
                "of CSA Indian Institute of Science Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. of Computer Science Eastern Michigan University Ypsilanti, MI 48917, USA bkeller@emich.edu Naren Ramakrishnan Dept. of Computer Science Virginia Tech, Blacksburg VA 24061, USA naren@cs.vt.edu ABSTRACT Recommender systems aggregate individual user ratings into predictions of products or services that might interest visitors.",
                "The quality of this aggregation process crucially affects the user experience and hence the effectiveness of recommenders in e-commerce.",
                "We present a novel study that disaggregates global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearestneighbor collaborative filtering.",
                "In particular, we formulate three roles-scouts, promoters, and connectors-that capture how users receive recommendations, how items get recommended, and how ratings of these two types are themselves connected (resp.).",
                "These roles find direct uses in improving recommendations for users, in better targeting of items and, most importantly, in helping monitor the health of the system as a whole.",
                "For instance, they can be used to track the evolution of neighborhoods, to identify rating subspaces that do not contribute (or contribute negatively) to system performance, to enumerate users who are in danger of leaving, and to assess the susceptibility of the system to attacks such as shilling.",
                "We argue that the three rating roles presented here provide broad primitives to manage a recommender system and its community.",
                "Categories and Subject Descriptors H.4.2 [Information Systems Applications]: Types of Systems-Decision support; J.4 [Computer Applications]: Social and Behavioral Sciences General Terms Algorithms, Human Factors 1.",
                "INTRODUCTION Recommender systems have become integral to e-commerce, providing technology that suggests products to a visitor based on previous purchases or rating history.",
                "Collaborative filtering, a common form of recommendation, predicts a users rating for an item by combining (other) ratings of that user with other users ratings.",
                "Significant research has been conducted in implementing fast and accurate collaborative filtering algorithms [2, 7], designing interfaces for presenting recommendations to users [1], and studying the robustness of these algorithms [8].",
                "However, with the exception of a few studies on the influence of users [10], little attention has been paid to unraveling the inner workings of a recommender in terms of the individual ratings and the roles they play in making (good) recommendations.",
                "Such an understanding will give an important handle to monitoring and managing a recommender system, to engineer mechanisms to sustain the recommender, and thereby ensure its continued success.",
                "Our motivation here is to disaggregate global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearest-neighbor collaborative filtering.",
                "We identify three possible roles: (scouts) to connect the user into the system to receive recommendations, (promoters) to connect an item into the system to be recommended, and (connectors) to connect ratings of these two kinds.",
                "Viewing ratings in this way, we can define the contribution of a rating in each role, both in terms of allowing recommendations to occur, and in terms of influence on the quality of recommendations.",
                "In turn, this capability helps support scenarios such as: 1.",
                "Situating users in better neighborhoods: A users ratings may inadvertently connect the user to a neighborhood for which the users tastes may not be a perfect match.",
                "Identifying ratings responsible for such bad recommendations and suggesting new items to rate can help situate the user in a better neighborhood. 2.",
                "Targeting items: Recommender systems suffer from lack of user participation, especially in cold-start scenarios [13] involving newly arrived items.",
                "Identifying users who can be encouraged to rate specific items helps ensure coverage of the recommender system. 3.",
                "Monitoring the evolution of the recommender system and its stakeholders: A recommender system is constantly under change: growing with new users and 250 items, shrinking with users leaving the system, items becoming irrelevant, and parts of the system under attack.",
                "Tracking the roles of a rating and its evolution over time provides many insights into the health of the system, and how it could be managed and improved.",
                "These include being able to identify rating subspaces that do not contribute (or contribute negatively) to system performance, and could be removed; to enumerate users who are in danger of leaving, or have left the system; and to assess the susceptibility of the system to attacks such as shilling [5].",
                "As we show, the characterization of rating roles presented here provides broad primitives to manage a recommender system and its community.",
                "The rest of the paper is organized as follows.",
                "Background on nearest-neighbor collaborative filtering and algorithm evaluation is discussed in Section 2.",
                "Section 3 defines and discusses the roles of a rating, and Section 4 defines measures of the contribution of a rating in each of these roles.",
                "In Section 5, we illustrate the use of these roles to address the goals outlined above. 2.",
                "BACKGROUND 2.1 Algorithms Nearest-neighbor collaborative filtering algorithms either use neighborhoods of users or neighborhoods of items to compute a prediction.",
                "An algorithm of the first kind is called user-based, and one of the second kind is called itembased [12].",
                "In both families of algorithms, neighborhoods are formed by first computing the similarity between all pairs of users (for user-based) or items (for item-based).",
                "Predictions are then computed by aggregating ratings, which in a user-based algorithm involves aggregating the ratings of the target item by the users neighbors and, in an item-based algorithm, involves aggregating the users ratings of items that are neighbors of the target item.",
                "Algorithms within these families differ in the definition of similarity, formation of neighborhoods, and the computation of predictions.",
                "We consider a user-based algorithm based on that defined for GroupLens [11] with variations from Herlocker et al. [2], and an item-based algorithm similar to that of Sarwar et al. [12].",
                "The algorithm used by Resnick et al. [11] defines the similarity of two users u and v as the Pearson correlation of their common ratings: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2 , where Iu is the set of items rated by user u, ru,i is user us rating for item i, and ¯ru is the average rating of user u (similarly for v).",
                "Similarity computed in this manner is typically scaled by a factor proportional to the number of common ratings, to reduce the chance of making a recommendation made on weak connections: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), where γ ≈ 5 is a constant used as a lower limit in scaling [2].",
                "These new similarities are then used to define a static neighborhood Nu for each user u consisting of the top K users most similar to user u.",
                "A prediction for user u and item i is computed by a weighted average of the ratings by the neighbors pu,i = ¯ru + P v∈V sim (u, v)(rv,i − ¯rv) P v∈V sim (u, v) (1) where V = Nu ∩ Ui is the set of users most similar to u who have rated i.",
                "The item-based algorithm we use is the one defined by Sarwar et al. [12].",
                "In this algorithm, similarity is defined as the adjusted cosine measure sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) where Ui is the set of users who have rated item i.",
                "As for the user-based algorithm, the similarity weights are adjusted proportionally to the number of users that have rated the items in common sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). (3) Given the similarities, the neighborhood Ni of an item i is defined as the top K most similar items for that item.",
                "A prediction for user u and item i is computed as the weighted average pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) where J = Ni ∩ Iu is the set of items rated by u that are most similar to i. 2.2 Evaluation Recommender algorithms have typically been evaluated using measures of predictive accuracy and coverage [3].",
                "Studies on recommender algorithms, notably Herlocker et al. [2] and Sarwar et al. [12], typically compute predictive accuracy by dividing a set of ratings into training and test sets, and compute the prediction for an item in the test set using the ratings in the training set.",
                "A standard measure of predictive accuracy is mean absolute error (MAE), which for a test set T = {(u, i)} is defined as, MAE = P (u,i)∈T |pu,i − ru,i| |T | . (5) Coverage has a number of definitions, but generally refers to the proportion of items that can be predicted by the algorithm [3].",
                "A practical issue with predictive accuracy is that users typically are presented with recommendation lists, and not individual numeric predictions.",
                "Recommendation lists are lists of items in decreasing order of prediction (sometimes stated in terms of star-ratings), and so predictive accuracy may not be reflective of the accuracy of the list.",
                "So, instead we can measure recommendation or rank accuracy, which indicates the extent to which the list is in the correct order.",
                "Herlocker et al. [3] discuss a number of rank accuracy measures, which range from Kendalls Tau to measures that consider the fact that users tend to only look at a prefix of the list [5].",
                "Kendalls Tau measures the number of inversions when comparing ordered pairs in the true user ordering of 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figure 1: Ratings in simple movie recommender. items and the recommended order, and is defined as τ = C − D p (C + D + TR)(C + D + TP) (6) where C is the number of pairs that the system predicts in the correct order, D the number of pairs the system predicts in the wrong order, TR the number of pairs in the true ordering that have the same ratings, and TP is the number of pairs in the predicted ordering that have the same ratings [3].",
                "A shortcoming of the Tau metric is that it is oblivious to the position in the ordered list where the inversion occurs [3].",
                "For instance, an inversion toward the end of the list is given the same weight as one in the beginning.",
                "One solution is to consider inversions only in the top few items in the recommended list or to weight inversions based on their position in the list. 3.",
                "ROLES OF A RATING Our basic observation is that each rating plays a different role in each prediction in which it is used.",
                "Consider a simplified movie recommender system with three users Jim, Jeff, and Tom and their ratings for a few movies, as shown in Fig. 1. (For this initial discussion we will not consider the rating values involved.)",
                "The recommender predicts whether Tom will like The Mask using the other already available ratings.",
                "How this is done depends on the algorithm: 1.",
                "An item-based collaborative filtering algorithm constructs a neighborhood of movies around The Mask by using the ratings of users who rated The Mask and other movies similarly (e.g., Jims ratings of The Matrix and The Mask; and Jeffs ratings of Star Wars and The Mask).",
                "Toms ratings of those movies are then used to make a prediction for The Mask. 2.",
                "A user-based collaborative filtering algorithm would construct a neighborhood around Tom by tracking other users whose rating behaviors are similar to Toms (e.g., Tom and Jeff have rated Star Wars; Tom and Jim have rated The Matrix).",
                "The prediction of Toms rating for The Mask is then based on the ratings of Jeff and Tim.",
                "Although the nearest-neighbor algorithms aggregate the ratings to form neighborhoods used to compute predictions, we can disaggregate the similarities to view the computation of a prediction as simultaneously following parallel paths of ratings.",
                "So, irrespective of the collaborative filtering algorithm used, we can visualize the prediction of Toms rating of The Mask as walking through a sequence of ratings.",
                "In Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figure 2: Ratings used to predict The Mask for Tom.",
                "Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figure 3: Prediction of The Mask for Tom in which a rating is used more than once. this example, two paths were used for this prediction as depicted in Fig. 2: (p1, p2, p3) and (q1, q2, q3).",
                "Note that these paths are undirected, and are all of length 3.",
                "Only the order in which the ratings are traversed is different between the item-based algorithm (e.g., (p3, p2, p1), (q3, q2, q1)) and the user-based algorithm (e.g., (p1, p2, p3), (q1, q2, q3).)",
                "A rating can be part of many paths for a single prediction as shown in Fig. 3, where three paths are used for a prediction, two of which follow p1: (p1, p2, p3) and (p1, r2, r3).",
                "Predictions in a collaborative filtering algorithms may involve thousands of such walks in parallel, each playing a part in influencing the predicted value.",
                "Each prediction path consists of three ratings, playing roles that we call scouts, promoters, and connectors.",
                "To illustrate these roles, consider the path (p1, p2, p3) in Fig. 2 used to make a prediction of The Mask for Tom: 1.",
                "The rating p1 (Tom → Star Wars) makes a connection from Tom to other ratings that can be used to predict Toms rating for The Mask.",
                "This rating serves as a scout in the bipartite graph of ratings to find a path that leads to The Mask. 2.",
                "The rating p2 (Jeff → Star Wars) helps the system recommend The Mask to Tom by connecting the scout to the promoter. 3.",
                "The rating p3 (Jeff → The Mask) allows connections to The Mask, and, therefore, promotes this movie to Tom.",
                "Formally, given a prediction pu,a of a target item a for user u, a scout for pu,a is a rating ru,i such that there exists a user v with ratings rv,a and rv,i for some item i; a promoter for pu,a is a rating rv,a for some user v, such that there exist ratings rv,i and ru,i for an item i, and; a connector for pu,a 252 Jim Tom Jeff Jerry My Cousin Vinny The Matrix Star Wars The Mask Jurasic Park Figure 4: Scouts, promoters, and connectors. is a rating rv,i by some user v and rating i, such that there exists ratings ru,i and rv,a.",
                "The scouts, connectors, and promoters for the prediction of Toms rating of The Mask are p1 and q1, p2 and q2, and p3 and q3 (respectively).",
                "Each of these roles has a value in the recommender to the user, the users neighborhood, and the system in terms of allowing recommendations to be made. 3.1 Roles in Detail Ratings that act as scouts tend to help the recommender system suggest more movies to the user, though the extent to which this is true depends on the rating behavior of other users.",
                "For example, in Fig. 4 the rating Tom → Star Wars helps the system recommend only The Mask to him, while Tom → The Matrix helps recommend The Mask, Jurassic Park, and My Cousin Vinny.",
                "Tom makes a connection to Jim who is a prolific user of the system, by rating The Matrix.",
                "However, this does not make The Matrix the best movie to rate for everyone.",
                "For example, Jim is benefited equally by both The Mask and The Matrix, which allow the system to recommend Star Wars to him.",
                "His rating of The Mask is the best scout for Jeff, and Jerrys only scout is his rating of Star Wars.",
                "This suggests that good scouts allow a user to build similarity with prolific users, and thereby ensure they get more from the system.",
                "While scouts represent beneficial ratings from the perspective of a user, promoters are their duals, and are of benefit to items.",
                "In Fig. 4, My Cousin Vinny benefits from Jims rating, since it allows recommendations to Jeff and Tom.",
                "The Mask is not so dependent on just one rating, since the ratings by Jim and Jeff help it.",
                "On the other hand, Jerrys rating of Star Wars does not help promote it to any other user.",
                "We conclude that a good promoter connects an item to a broader neighborhood of other items, and thereby ensures that it is recommended to more users.",
                "Connectors serve a crucial role in a recommender system that is not as obvious.",
                "The movies My Cousin Vinny and Jurassic Park have the highest recommendation potential since they can be recommended to Jeff, Jerry and Tom based on the linkage structure illustrated in Fig. 4.",
                "Beside the fact that Jim rated these movies, these recommendations are possible only because of the ratings Jim → The Matrix and Jim → The Mask, which are the best connectors.",
                "A connector improves the systems ability to make recommendations with no explicit gain for the user.",
                "Note that every rating can be of varied benefit in each of these roles.",
                "The rating Jim → My Cousin Vinny is a poor scout and connector, but is a very good promoter.",
                "The rating Jim → The Mask is a reasonably good scout, a very good connector, and a good promoter.",
                "Finally, the rating Jerry → Star Wars is a very good scout, but is of no value as a connector or promoter.",
                "As illustrated here, a rating can have different value in each of the three roles in terms of whether a recommendation can be made or not.",
                "We could measure this value by simply counting the number of times a rating is used in each role, which alone would be helpful in characterizing the behavior of a system.",
                "But we can also measure the contribution of each rating to the quality of recommendations or health of the system.",
                "Since every prediction is a combined effort of several recommendation paths, we are interested in discerning the influence of each rating (and, hence, each path) in the system towards the systems overall error.",
                "We can understand the dynamics of the system at a finer granularity by tracking the influence of a rating according to the role played.",
                "The next section describes the approach to measuring the values of a rating in each role. 4.",
                "CONTRIBUTIONS OF RATINGS As weve seen, a rating may play different roles in different predictions and, in each prediction, contribute to the quality of a prediction in different ways.",
                "Our approach can use any numeric measure of a property of system health, and assigns credit (or blame) to each rating proportional to its influence in the prediction.",
                "By tracking the role of each rating in a prediction, we can accumulate the credit for a rating in each of the three roles, and also track the evolution of the roles of rating over time in the system.",
                "This section defines the methodology for computing the contribution of ratings by first defining the influence of a rating, and then instantiating the approach for predictive accuracy, and then rank accuracy.",
                "We also demonstrate how these contributions can be aggregated to study the neighborhood of ratings involved in computing a users recommendations.",
                "Note that although our general formulation for rating influence is algorithm independent, due to space considerations, we present the approach for only item-based collaborative filtering.",
                "The definition for user-based algorithms is similar and will be presented in an expanded version of this paper. 4.1 Influence of Ratings Recall that an item-based approach to collaborative filtering relies on building item neighborhoods using the similarity of ratings by the same user.",
                "As described earlier, similarity is defined by the adjusted cosine measure (Equations (2) and (3)).",
                "A set of the top K neighbors is maintained for all items for space and computational efficiency.",
                "A prediction of item i for a user u is computed as the weighted deviation from the items mean rating as shown in Equation (4).",
                "The list of recommendations for a user is then the list of items sorted in descending order of their predicted values.",
                "We first define impact(a, i, j), the impact a user a has in determining the similarity between two items i and j.",
                "This is the change in the similarity between i and j when as rating is removed, and is defined as impact(a, i, j) = |sim (i, j) − sim¯a(i, j)| P w∈Cij |sim (i, j) − sim ¯w(i, j)| where Cij = {u ∈ U | ∃ ru,i, ru,j ∈ R(u)} is the set of coraters 253 of items i and j (users who rate both i and j), R(u) is the set of ratings provided by user u, and sim¯a(i, j) is the similarity of i and j when the ratings of user a are removed sim¯a(i, j) = P v∈U\\{a} (ru,i − ¯ru)(ru,j − ¯ru) qP u∈U\\{a}(ru,i − ¯ru)2 qP u∈U\\{a}(ru,j − ¯ru)2 , and adjusted for the number of raters sim¯a(i, j) = max(|Ui ∩ Uj| − 1, γ) γ · sim(i, j).",
                "If all coraters of i and j rate them identically, we define the impact as impact(a, i, j) = 1 |Cij| since P w∈Cij |sim (i, j) − sim ¯w(i, j)| = 0.",
                "The influence of each path (u, j, v, i) = [ru,j, rv,j, rv,i] in the prediction of pu,i is given by influence(u, j, v, i) = sim (i, j) P l∈Ni∩Iu sim (i, l) · impact(v, i, j) It follows that the sum of influences over all such paths, for a given set of endpoints, is 1. 4.2 Role Values for Predictive Accuracy The value of a rating in each role is computed from the influence depending on the evaluation measure employed.",
                "Here we illustrate the approach using predictive accuracy as the evaluation metric.",
                "In general, the goodness of a prediction decides whether the ratings involved must be credited or discredited for their role.",
                "For predictive accuracy, the error in prediction e = |pu,i − ru,i| is mapped to a comfort level using a mapping function M(e).",
                "Anecdotal evidence suggests that users are unable to discern errors less than 1.0 (for a rating scale of 1 to 5) [4], and so an error less than 1.0 is considered acceptable, but anything larger is not.",
                "We hence define M(e) as (1 − e) binned to an appropriate value in [−1, −0.5, 0.5, 1].",
                "For each prediction pu,i, M(e) is attributed to all the paths that assisted the computation of pu,i, proportional to their influences.",
                "This tribute, M(e)∗influence(u, j, v, i), is in turn inherited by each of the ratings in the path [ru,j, rv,j, rv,i], with the credit/blame accumulating to the respective roles of ru,j as a scout, rv,j as a connector, and rv,i as a promoter.",
                "In other words, the scout value SF(ru,j), the connector value CF(rv,j) and the promoter value PF(rv,i) are all incremented by the tribute amount.",
                "Over a large number of predictions, scouts that have repeatedly resulted in big error rates have a big negative scout value, and vice versa (similarly with the other roles).",
                "Every rating is thus summarized by its triple [SF, CF, PF]. 4.3 Role Values for Rank Accuracy We now define the computation of the contribution of ratings to observed rank accuracy.",
                "For this computation, we must know the users preference order for a set of items for which predictions can be computed.",
                "We assume that we have a test set of the users ratings of the items presented in the recommendation list.",
                "For every pair of items rated by a user in the test data, we check whether the predicted order is concordant with his preference.",
                "We say a pair (i, j) is concordant (with error ) whenever one of the following holds: • if (ru,i < ru,j) then (pu,i − pu,j < ); • if (ru,i > ru,j) then (pu,i − pu,j > ); or • if (ru,i = ru,j) then (|pu,i − pu,j| ≤ ).",
                "Similarly, a pair (i, j) is discordant (with error ) if it is not concordant.",
                "Our experiments described below use an error tolerance of = 0.1.",
                "All paths involved in the prediction of the two items in a concordant pair are credited, and the paths involved in a discordant pair are discredited.",
                "The credit assigned to a pair of items (i, j) in the recommendation list for user u is computed as c(i, j) = ( t T · 1 C+D if (i, j) are concordant − t T · 1 C+D if (i, j) are discordant (7) where t is the number of items in the users test set whose ratings could be predicted, T is the number of items rated by user u in the test set, C is the number of concordances and D is the number of discordances.",
                "The credit c is then divided among all paths responsible for predicting pu,i and pu,j proportional to their influences.",
                "We again add the role values obtained from all the experiments to form a triple [SF, CF, PF] for each rating. 4.4 Aggregating rating roles After calculating the role values for individual ratings, we can also use these values to study neighborhoods and the system.",
                "Here we consider how we can use the role values to characterize the health of a neighborhood.",
                "Consider the list of top recommendations presented to a user at a specific point in time.",
                "The collaborative filtering algorithm traversed many paths in his neighborhood through his scouts and other connectors and promoters to make these recommendations.",
                "We call these ratings the recommender neighborhood of the user.",
                "The user implicitly chooses this neighborhood of ratings through the items he rates.",
                "Apart from the collaborative filtering algorithm, the health of this neighborhood completely influences a users satisfaction with the system.",
                "We can characterize a users recommender neighborhood by aggregating the individual role values of the ratings involved, weighted by the influence of individual ratings in determining his recommended list.",
                "Different sections of the users neighborhood wield varied influence on his recommendation list.",
                "For instance, ratings reachable through highly rated items have a bigger say in the recommended items.",
                "Our aim is to study the system and classify users with respect to their positioning in a healthy or unhealthy neighborhood.",
                "A user can have a good set of scouts, but may be exposed to a neighborhood with bad connectors and promoters.",
                "He can have a good neighborhood, but his bad scouts may ensure the neighborhoods potential is rendered useless.",
                "We expect that users with good scouts and good neighborhoods will be most satisfied with the system in the future.",
                "A users neighborhood is characterized by a triple that represents the weighted sum of the role values of individual ratings involved in making recommendations.",
                "Consider a user u and his ordered list of recommendations L. An item i 254 in the list is weighted inversely, as K(i), depending on its position in the list.",
                "In our studies we use K(i) = p position(i).",
                "Several paths of ratings [ru,j, rv,j, rv,i] are involved in predicting pu,i which ultimately decides its position in L, each with influence(u, j, v, i).",
                "The recommender neighborhood of a user u is characterized by the triple, [SFN(u), CFN(u), PFN(u)] where SFN(u) = X i∈L P [ru,j ,rv,j ,rv,i] SF(ru,j)influence(u, j, v, i) K(i) !",
                "CFN(u) and PFN(u) are defined similarly.",
                "This triple estimates the quality of us recommendations based on the past track record of the ratings involved in their respective roles. 5.",
                "EXPERIMENTATION As we have seen, we can assign role values to each rating when evaluating a collaborative filtering system.",
                "In this section, we demonstrate the use of this approach to our overall goal of defining an approach to monitor and manage the health of a recommender system through experiments done on the MovieLens million rating dataset.",
                "In particular, we discuss results relating to identifying good scouts, promoters, and connectors; the evolution of rating roles; and the characterization of user neighborhoods. 5.1 Methodology Our experiments use the MovieLens million rating dataset, which consists of ratings by 6040 users of 3952 movies.",
                "The ratings are in the range 1 to 5, and are labeled with the time the rating was given.",
                "As discussed before, we consider only the item-based algorithm here (with item neighborhoods of size 30) and, due to space considerations, only present role value results for rank accuracy.",
                "Since we are interested in the evolution of the rating role values over time, the model of the recommender system is built by processing ratings in their arrival order.",
                "The timestamping provided by MovieLens is hence crucial for the analyses presented here.",
                "We make assessments of rating roles at intervals of 10,000 ratings and processed the first 200,000 ratings in the dataset (giving rise to 20 snapshots).",
                "We incrementally update the role values as the time ordered ratings are merged into the model.",
                "To keep the experiment computationally manageable, we define a test dataset for each user.",
                "As the time ordered ratings are merged into the model, we label a small randomly selected percentage (20%) as test data.",
                "At discrete epochs, i.e., after processing every 10,000 ratings, we compute the predictions for the ratings in the test data, and then compute the role values for the ratings used in the predictions.",
                "One potential criticism of this methodology is that the ratings in the test set are never evaluated for their roles.",
                "We overcome this concern by repeating the experiment, using different random seeds.",
                "The probability that every rating is considered for evaluation is then considerably high: 1 − 0.2n , where n is the number of times the experiment is repeated with different random seeds.",
                "The results here are based on n = 4 repetitions.",
                "The item-based collaborative filtering algorithms performance was ordinary with respect to rank accuracy.",
                "Fig. 5 shows a plot of the precision and recall as ratings were merged in time order into the model.",
                "The recall was always high, but the average precision was just about 53%. 0 0.2 0.4 0.6 0.8 1 1.2 10000 30000 50000 70000 90000110000130000150000 Ratings merged into model Value Precision Recall Figure 5: Precision and recall for the item-based collaborative filtering algorithm. 5.2 Inducing good scouts The ratings of a user that serve as scouts are those that allow the user to receive recommendations.",
                "We claim that users with ratings that have respectable scout values will be happier with the system than those with ratings with low scout values.",
                "Note that the item-based algorithm discussed here produces recommendation lists with nearly half of the pairs in the list discordant from the users preference.",
                "Whether all of these discordant pairs are observable by the user is unclear, however, certainly this suggests that there is a need to be able to direct users to items whose ratings would improve the lists.",
                "The distribution of the scout values for most users ratings are Gaussian with mean zero.",
                "Fig. 6 shows the frequency distribution of scout values for a sample user at a given snapshot.",
                "We observe that a large number of ratings never serve as scouts for their users.",
                "A relatable scenario is when Amazons recommender makes suggestions of books or items based on other items that were purchased as gifts.",
                "With simple relevance feedback from the user, such ratings can be isolated as bad scouts and discounted from future predictions.",
                "Removing bad scouts was found to be worthwhile for individual users but the overall performance improvement was only marginal.",
                "An obvious question is whether good scouts can be formed by merely rating popular movies as suggested by Rashid et al. [9].",
                "They show that a mix of popularity and rating entropy identifies the best items to suggest to new users when evaluated using MAE.",
                "Following their intuition, we would expect to see a higher correlation between popularityentropy and good scouts.",
                "We measured the Pearson correlation coefficient between aggregated scout values for a movie with the popularity of a movie (number of times it is rated); and with its popularity*variance measure at different snapshots of the system.",
                "Note that the scout values were initially anti-correlated with popularity (Fig. 7), but became moderately correlated as the system evolved.",
                "Both popularity and popularity*variance performed similarly.",
                "A possible explanation is that there has been insufficient time for the popular movies to accumulate ratings. 255 -10 0 10 20 30 40 50 60 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 Scout Value Frequency Figure 6: Distribution of scout values for a sample user. -0.4 -0.2 0 0.2 0.4 0.6 0.8 30000 60000 90000 120000 150000 180000 Popularity Pop*Var Figure 7: Correlation between aggregated scout value and item popularity (computed at different intervals). -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 30000 60000 90000 120000 150000 180000 Figure 8: Correlation between aggregated promoter value and user prolificity (computed at different intervals).",
                "Table 1: Movies forming the best scouts.",
                "Best Scouts Conf.",
                "Pop.",
                "Being John Malkovich (1999) 1.00 445 Star Wars: Episode IV - A New Hope (1977) 0.92 623 Princess Bride, The (1987) 0.85 477 Sixth Sense, The (1999) 0.85 617 Matrix, The (1999) 0.77 522 Ghostbusters (1984) 0.77 441 Casablanca (1942) 0.77 384 Insider, The (1999) 0.77 235 American Beauty (1999) 0.69 624 Terminator 2: Judgment Day (1991) 0.69 503 Fight Club (1999) 0.69 235 Shawshank Redemption, The (1994) 0.69 445 Run Lola Run (Lola rennt) (1998) 0.69 220 Terminator, The (1984) 0.62 450 Usual Suspects, The (1995) 0.62 326 Aliens (1986) 0.62 385 North by Northwest (1959) 0.62 245 Fugitive, The (1993) 0.62 402 End of Days (1999) 0.62 132 Raiders of the Lost Ark (1981) 0.54 540 Schindlers List (1993) 0.54 453 Back to the Future (1985) 0.54 543 Toy Story (1995) 0.54 419 Alien (1979) 0.54 415 Abyss, The (1989) 0.54 345 2001: A Space Odyssey (1968) 0.54 358 Dogma (1999) 0.54 228 Little Mermaid, The (1989) 0.54 203 Table 2: Movies forming the worst scouts.",
                "Worst scouts Conf.",
                "Pop.",
                "Harold and Maude (1971) 0.46 141 Grifters, The (1990) 0.46 180 Sting, The (1973) 0.38 244 Godfather: Part III, The (1990) 0.38 154 Lawrence of Arabia (1962) 0.38 167 High Noon (1952) 0.38 84 Women on the Verge of a... (1988) 0.38 113 Grapes of Wrath, The (1940) 0.38 115 Duck Soup (1933) 0.38 131 Arsenic and Old Lace (1944) 0.38 138 Midnight Cowboy (1969) 0.38 137 To Kill a Mockingbird (1962) 0.31 195 Four Weddings and a Funeral (1994) 0.31 271 Good, The Bad and The Ugly, The (1966) 0.31 156 Its a Wonderful Life (1946) 0.31 146 Player, The (1992) 0.31 220 Jackie Brown (1997) 0.31 118 Boat, The (Das Boot) (1981) 0.31 210 Manhattan (1979) 0.31 158 Truth About Cats & Dogs, The (1996) 0.31 143 Ghost (1990) 0.31 227 Lone Star (1996) 0.31 125 Big Chill, The (1983) 0.31 184 256 By studying the evolution of scout values, we can identify movies that consistently feature in good scouts over time.",
                "We claim these movies will make viable scouts for other users.",
                "We found the aggregated scout values for all movies in intervals of 10,000 ratings each.",
                "A movie is said to induce a good scout if the movie was in the top 100 of the sorted list, and to induce a bad scout if it was in bottom 100 of the same list.",
                "Movies appearing consistently high over time are expected to remain up there in the future.",
                "The effective confidence in a movie m is Cm = Tm − Bm N (8) where Tm is the number of times it appeared in the top 100, Bm the number of times it appeared in the bottom 100, and N is the number of intervals considered.",
                "Using this measure, the top few movies expected to induce the best scouts are shown in Table 1.",
                "Movies that would be bad scout choices are shown in Table 2 with their associated confidences.",
                "The popularities of the movies are also displayed.",
                "Although more popular movies appear in the list of good scouts, these tables show that a blind choice of scout based on popularity alone can be potentially dangerous.",
                "Interestingly, the best scout-Being John Malkovich-is about a puppeteer who discovers a portal into a movie star, a movie that has been described variously on amazon.com as makes you feel giddy, seriously weird, comedy with depth, silly, strange, and inventive.",
                "Indicating whether someone likes this movie or not goes a long way toward situating the user in a suitable neighborhood, with similar preferences.",
                "On the other hand, several factors may have made a movie a bad scout, like the sharp variance in user preferences in the neighborhood of a movie.",
                "Two users may have the same opinion about Lawrence of Arabia, but they may differ sharply about how they felt about the other movies they saw.",
                "Bad scouts ensue when there is deviation in behavior around a common synchronization point. 5.3 Inducing good promoters Ratings that serve to promote items in a collaborative filtering system are critical to allowing a new item be recommended to users.",
                "So, inducing good promoters is important for cold-start recommendation.",
                "We note that the frequency distribution of promoter values for a sample movies ratings is also Gaussian (similar to Fig. 6).",
                "This indicates that the promotion of a movie is benefited most by the ratings of a few users, and are unaffected by the ratings of most users.",
                "We find a strong correlation between a users number of ratings and his aggregated promoter value.",
                "Fig. 8 depicts the evolution of the Pearson correlation co-efficient between the prolificity of a user (number of ratings) versus his aggregated promoter value.",
                "We expect that conspicuous shills, by recommending wrong movies to users, will be discredited with negative aggregate promoter values and should be identifiable easily.",
                "Given this observation, the obvious rule to follow when introducing a new movie is to have it rated directly by prolific users who posses high aggregated promoter values.",
                "A new movie is thus cast into the neighborhood of many other movies improving its visibility.",
                "Note, though, that a user may have long stopped using the system.",
                "Tracking promoter values consistently allows only the most active recent users to be considered. 5.4 Inducing good connectors Given the way scouts, connectors, and promoters are characterized, it follows that the movies that are part of the best scouts are also part of the best connectors.",
                "Similarly, the users that constitute the best promoters are also part of the best connectors.",
                "Good connectors are induced by ensuring a user with a high promoter value rates a movie with a high scout value.",
                "In our experiments, we find that a ratings longest standing role is often as a connector.",
                "A rating with a poor connector value is often seen due to its user being a bad promoter, or its movie being a bad scout.",
                "Such ratings can be removed from the prediction process to bring marginal improvements to recommendations.",
                "In some selected experiments, we observed that removing a set of badly behaving connectors helped improve the systems overall performance by 1.5%.",
                "The effect was even higher on a few select users who observed an improvement of above 10% in precision without much loss in recall. 5.5 Monitoring the evolution of rating roles One of the more significant contributions of our work is the ability to model the evolution of recommender systems, by studying the changing roles of ratings over time.",
                "The role and value of a rating can change depending on many factors like user behavior, redundancy, shilling effects or properties of the collaborative filtering algorithm used.",
                "Studying the dynamics of rating roles in terms of transitions between good, bad, and negligible values can provide insights into the functioning of the recommender system.",
                "We believe that a continuous visualization of these transitions will improve the ability to manage a recommender system.",
                "We classify different rating states as good, bad, or negligible.",
                "Consider a user who has rated 100 movies in a particular interval, of which 20 are part of the test set.",
                "If a scout has a value greater than 0.005, it indicates that it is uniquely involved in at least 2 concordant predictions, which we will say is good.",
                "Thus, a threshold of 0.005 is chosen to bin a rating as good, bad or negligible in terms of its scout, connector and promoter value.",
                "For instance, a rating r, at time t with role value triple [0.1, 0.001, −0.01] is classified as [scout +, connector 0, promoter −], where + indicates good, 0 indicates negligible, and − indicates bad.",
                "The positive credit held by a rating is a measure of its contribution to the betterment of the system, and the discredit is a measure of its contribution to the detriment of the system.",
                "Even though the positive roles (and the negative roles) make up a very small percentage of all ratings, their contribution supersedes their size.",
                "For example, even though only 1.7% of all ratings were classified as good scouts, they hold 79% of all positive credit in the system!",
                "Similarly, the bad scouts were just 1.4% of all ratings but hold 82% of all discredit.",
                "Note that good and bad scouts, together, comprise only 1.4% + 1.7% = 3.1% of the ratings, implying that the majority of the ratings are negligible role players as scouts (more on this later).",
                "Likewise, good connectors were 1.2% of the system, and hold 30% of all positive credit.",
                "The bad connectors (0.8% of the system) hold 36% of all discredit.",
                "Good promoters (3% of the system) hold 46% of all credit, while bad promoters (2%) hold 50% of all discredit.",
                "This reiterates that a few ratings influence most of the systems performance.",
                "Hence it is important to track transitions between them regardless of their small numbers. 257 Across different snapshots, a rating can remain in the same state or change.",
                "A good scout can become a bad scout, a good promoter can become a good connector, good and bad scouts can become vestigial, and so on.",
                "It is not practical to expect a recommender system to have no ratings in bad roles.",
                "However, it suffices to see ratings in bad roles either convert to good or vestigial roles.",
                "Similarly, observing a large number of good roles become bad ones is a sign of imminent failure of the system.",
                "We employ the principle of non-overlapping episodes [6] to count such transitions.",
                "A sequence such as: [+, 0, 0] → [+, 0, 0] → [0, +, 0] → [0, 0, 0] is interpreted as the transitions [+, 0, 0] ; [0, +, 0] : 1 [+, 0, 0] ; [0, 0, 0] : 1 [0, +, 0] ; [0, 0, 0] : 1 instead of [+, 0, 0] ; [0, +, 0] : 2 [+, 0, 0] ; [0, 0, 0] : 2 [0, +, 0] ; [0, 0, 0] : 1.",
                "See [6] for further details about this counting procedure.",
                "Thus, a rating can be in one of 27 possible states, and there are about 272 possible transitions.",
                "We make a further simplification and utilize only 9 states, indicating whether the rating is a scout, promoter, or connector, and whether it has a positive, negative, or negligible role.",
                "Ratings that serve multiple purposes are counted using multiple episode instantiations but the states themselves are not duplicated beyond the 9 restricted states.",
                "In this model, a transition such as [+, 0, +] ; [0, +, 0] : 1 is counted as [scout+] ; [scout0] : 1 [scout+] ; [connector+] : 1 [scout+] ; [promoter0] : 1 [connector0] ; [scout0] : 1 [connector0] ; [scout+] : 1 [connector0] ; [promoter0] : 1 [promoter+] ; [scout0] : 1 [promoter+] ; [connector+] : 1 [promoter+] ; [promoter0] : 1 Of these, transitions like [pX] ; [q0] where p = q, X ∈ {+, 0, −} are considered uninteresting, and only the rest are counted.",
                "Fig. 9 depicts the major transitions counted while processing the first 200,000 ratings from the MovieLens dataset.",
                "Only transitions with frequency greater than or equal to 3% are shown.",
                "The percentages for each state indicates the number of ratings that were found to be in those states.",
                "We consider transitions from any state to a good state as healthy, from any state to a bad state as unhealthy, and from any state to a vestigial state as decaying.",
                "From Fig. 9, we can observe: • The bulk of the ratings have negligible values, irrespective of their role.",
                "The majority of the transitions involve both good and bad ratings becoming negligible.",
                "Scout + (2%) Scout(1.5%) Scout 0 (96.5%) Connector + (1.2%) Connector(0.8%) Connector 0 (98%) Promoter + (3%) Promoter(2%) Promoter 0 (95%) 84% 84% 81% 74% 10% 6% 11% 77% 8% 7% 8% 82% 4% 86% 4% 68% 15% 13% 5% 5% 77% 11% 7% 5% 4% 3% 3% 3% Healthy Unhealthy Decaying Figure 9: Transitions among rating roles. • The number of good ratings is comparable to the bad ratings, and ratings are seen to switch states often, except in the case of scouts (see below). • The negative and positive scout states are not reachable through any transition, indicating that these ratings must begin as such, and cannot be coerced into these roles. • Good promoters and good connectors have a much longer survival period than scouts.",
                "Transitions that recur to these states have frequencies of 10% and 15% when compared to just 4% for scouts.",
                "Good connectors are the slowest to decay whereas (good) scouts decay the fastest. • Healthy percentages are seen on transitions between promoters and connectors.",
                "As indicated earlier, there are hardly any transitions from promoters/connectors to scouts.",
                "This indicates that, over the long run, a users rating is more useful to others (movies or other users) than to the user himself. • The percentages of healthy transitions outweigh the unhealthy ones - this hints that the system is healthy, albeit only marginally.",
                "Note that these results are conditioned by the static nature of the dataset, which is a set of ratings over a fixed window of time.",
                "However a diagram such as Fig. 9 is clearly useful for monitoring the health of a recommender system.",
                "For instance, acceptable limits can be imposed on different types of transitions and, if a transition fails to meet the threshold, the recommender system or a part of it can be brought under closer scrutiny.",
                "Furthermore, the role state transition diagram would also be the ideal place to study the effects of shilling, a topic we will consider in future research. 5.6 Characterizing neighborhoods Earlier we saw that we can characterize the neighborhood of ratings involved in creating a recommendation list L for 258 a user.",
                "In our experiment, we consider lists of length 30, and sample the lists of about 5% of users through the evolution of the model (at intervals of 10,000 ratings each) and compute their neighborhood characteristics.",
                "To simplify our presentation, we consider the percentage of the sample that fall into one of the following categories: 1.",
                "Inactive user: (SFN(u) = 0) 2.",
                "Good scouts, Good neighborhood: (SFN(u) > 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 3.",
                "Good scouts, Bad neighborhood: (SFN(u) > 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) 4.",
                "Bad scouts, Good neighborhood: (SFN(u) < 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 5.",
                "Bad scouts, Bad neighborhood: (SFN(u) < 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) From our sample set of 561 users, we found that 476 users were inactive.",
                "Of the remaining 85 users, we found 26 users had good scouts and a good neighborhood, 6 had bad scouts and a good neighborhood, 29 had good scouts and a bad neighborhood, and 24 had bad scouts and a bad neighborhood.",
                "Thus, we conjecture that 59 users (29+24+6) are in danger of leaving the system.",
                "As a remedy, users with bad scouts and a good neighborhood can be asked to reconsider rating of some movies hoping to improve the systems recommendations.",
                "The system can be expected to deliver more if they engineer some good scouts.",
                "Users with good scouts and a bad neighborhood are harder to address; this situation might entail selectively removing some connector-promoter pairs that are causing the damage.",
                "Handling users with bad scouts and bad neighborhoods is a more difficult challenge.",
                "Such a classification allows the use of different strategies to better a users experience with the system depending on his context.",
                "In future work, we intend to conduct field studies and study the improvement in performance of different strategies for different contexts. 6.",
                "CONCLUSIONS To further recommender system acceptance and deployment, we require new tools and methodologies to manage an installed recommender and develop insights into the roles played by ratings.",
                "A fine-grained characterization in terms of rating roles such as scouts, promoters, and connectors, as done here, helps such an endeavor.",
                "Although we have presented results on only the item-based algorithm with <br>list rank accuracy</br> as the metric, the same approach outlined here applies to user-based algorithms and other metrics.",
                "In future research, we plan to systematically study the many algorithmic parameters, tolerances, and cutoff thresholds employed here and reason about their effects on the downstream conclusions.",
                "We also aim to extend our formulation to other collaborative filtering algorithms, study the effect of shilling in altering rating roles, conduct field studies, and evaluate improvements in user experience by tweaking ratings based on their role values.",
                "Finally, we plan to develop the idea of mining the evolution of rating role patterns into a reporting and tracking system for all aspects of recommender system health. 7.",
                "REFERENCES [1] Cosley, D., Lam, S., Albert, I., Konstan, J., and Riedl, J.",
                "Is Seeing Believing? : How Recommender System Interfaces Affect Users Opinions.",
                "In Proc.",
                "CHI (2001), pp. 585-592. [2] Herlocker, J. L., Konstan, J.",
                "A., Borchers, A., and Riedl, J.",
                "An Algorithmic Framework for Performing Collaborative Filtering.",
                "In Proc.",
                "SIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J.",
                "A., Terveen, L. G., and Riedl, J. T. Evaluating Collaborative Filtering Recommender Systems.",
                "ACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J.",
                "A.",
                "Personal communication. 2003. [5] Lam, S. K., and Riedl, J. Shilling Recommender Systems for Fun and Profit.",
                "In Proceedings of the 13th International World Wide Web Conference (2004), ACM Press, pp. 393-402. [6] Laxman, S., Sastry, P. S., and Unnikrishnan, K. P. Discovering Frequent Episodes and Learning Hidden Markov Models: A Formal Connection.",
                "IEEE Transactions on Knowledge and Data Engineering Vol. 17, 11 (2005), 1505-1517. [7] McLaughlin, M. R., and Herlocker, J. L. A Collaborative Filtering Algorithm and Evaluation Metric that Accurately Model the User Experience.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (2004), pp. 329 - 336. [8] OMahony, M., Hurley, N. J., Kushmerick, N., and Silvestre, G. Collaborative Recommendation: A Robustness Analysis.",
                "ACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J.",
                "A., and Riedl, J.",
                "Getting to Know You: Learning New User Preferences in Recommender Systems.",
                "In Proceedings of the 2002 Conference on Intelligent User Interfaces (IUI 2002) (2002), pp. 127-134. [10] Rashid, A. M., Karypis, G., and Riedl, J.",
                "Influence in Ratings-Based Recommender Systems: An Algorithm-Independent Approach.",
                "In Proc. of the SIAM International Conference on Data Mining (2005). [11] Resnick, P., Iacovou, N., Sushak, M., Bergstrom, P., and Riedl, J. GroupLens: An Open Architecture for Collaborative Filtering of Netnews.",
                "In Proceedings of the Conference on Computer Supported Collaborative Work (CSCW94) (1994), ACM Press, pp. 175-186. [12] Sarwar, B., Karypis, G., Konstan, J., and Reidl, J. Item-Based Collaborative Filtering Recommendation Algorithms.",
                "In Proceedings of the Tenth International World Wide Web Conference (WWW10) (2001), pp. 285-295. [13] Schein, A., Popescu, A., Ungar, L., and Pennock, D. Methods and Metrics for Cold-Start Recommendation.",
                "In Proc.",
                "SIGIR (2002), pp. 253-260. 259"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Aunque hemos presentado resultados solo en el algoritmo basado en elementos con \"precisión de rango de lista\" como la métrica, el mismo enfoque descrito aquí se aplica aquí a los algoritmos basados en el usuario y otras métricas."
            ],
            "translated_text": "",
            "candidates": [
                "precisión de rango de lista",
                "precisión de rango de lista"
            ],
            "error": []
        },
        "neighborhood": {
            "translated_key": "vecindario",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Scouts, Promoters, and Connectors: The Roles of Ratings in Nearest Neighbor Collaborative Filtering Bharath Kumar Mohan Dept.",
                "of CSA Indian Institute of Science Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. of Computer Science Eastern Michigan University Ypsilanti, MI 48917, USA bkeller@emich.edu Naren Ramakrishnan Dept. of Computer Science Virginia Tech, Blacksburg VA 24061, USA naren@cs.vt.edu ABSTRACT Recommender systems aggregate individual user ratings into predictions of products or services that might interest visitors.",
                "The quality of this aggregation process crucially affects the user experience and hence the effectiveness of recommenders in e-commerce.",
                "We present a novel study that disaggregates global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearestneighbor collaborative filtering.",
                "In particular, we formulate three roles-scouts, promoters, and connectors-that capture how users receive recommendations, how items get recommended, and how ratings of these two types are themselves connected (resp.).",
                "These roles find direct uses in improving recommendations for users, in better targeting of items and, most importantly, in helping monitor the health of the system as a whole.",
                "For instance, they can be used to track the evolution of neighborhoods, to identify rating subspaces that do not contribute (or contribute negatively) to system performance, to enumerate users who are in danger of leaving, and to assess the susceptibility of the system to attacks such as shilling.",
                "We argue that the three rating roles presented here provide broad primitives to manage a recommender system and its community.",
                "Categories and Subject Descriptors H.4.2 [Information Systems Applications]: Types of Systems-Decision support; J.4 [Computer Applications]: Social and Behavioral Sciences General Terms Algorithms, Human Factors 1.",
                "INTRODUCTION Recommender systems have become integral to e-commerce, providing technology that suggests products to a visitor based on previous purchases or rating history.",
                "Collaborative filtering, a common form of recommendation, predicts a users rating for an item by combining (other) ratings of that user with other users ratings.",
                "Significant research has been conducted in implementing fast and accurate collaborative filtering algorithms [2, 7], designing interfaces for presenting recommendations to users [1], and studying the robustness of these algorithms [8].",
                "However, with the exception of a few studies on the influence of users [10], little attention has been paid to unraveling the inner workings of a recommender in terms of the individual ratings and the roles they play in making (good) recommendations.",
                "Such an understanding will give an important handle to monitoring and managing a recommender system, to engineer mechanisms to sustain the recommender, and thereby ensure its continued success.",
                "Our motivation here is to disaggregate global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearest-neighbor collaborative filtering.",
                "We identify three possible roles: (scouts) to connect the user into the system to receive recommendations, (promoters) to connect an item into the system to be recommended, and (connectors) to connect ratings of these two kinds.",
                "Viewing ratings in this way, we can define the contribution of a rating in each role, both in terms of allowing recommendations to occur, and in terms of influence on the quality of recommendations.",
                "In turn, this capability helps support scenarios such as: 1.",
                "Situating users in better neighborhoods: A users ratings may inadvertently connect the user to a <br>neighborhood</br> for which the users tastes may not be a perfect match.",
                "Identifying ratings responsible for such bad recommendations and suggesting new items to rate can help situate the user in a better <br>neighborhood</br>. 2.",
                "Targeting items: Recommender systems suffer from lack of user participation, especially in cold-start scenarios [13] involving newly arrived items.",
                "Identifying users who can be encouraged to rate specific items helps ensure coverage of the recommender system. 3.",
                "Monitoring the evolution of the recommender system and its stakeholders: A recommender system is constantly under change: growing with new users and 250 items, shrinking with users leaving the system, items becoming irrelevant, and parts of the system under attack.",
                "Tracking the roles of a rating and its evolution over time provides many insights into the health of the system, and how it could be managed and improved.",
                "These include being able to identify rating subspaces that do not contribute (or contribute negatively) to system performance, and could be removed; to enumerate users who are in danger of leaving, or have left the system; and to assess the susceptibility of the system to attacks such as shilling [5].",
                "As we show, the characterization of rating roles presented here provides broad primitives to manage a recommender system and its community.",
                "The rest of the paper is organized as follows.",
                "Background on nearest-neighbor collaborative filtering and algorithm evaluation is discussed in Section 2.",
                "Section 3 defines and discusses the roles of a rating, and Section 4 defines measures of the contribution of a rating in each of these roles.",
                "In Section 5, we illustrate the use of these roles to address the goals outlined above. 2.",
                "BACKGROUND 2.1 Algorithms Nearest-neighbor collaborative filtering algorithms either use neighborhoods of users or neighborhoods of items to compute a prediction.",
                "An algorithm of the first kind is called user-based, and one of the second kind is called itembased [12].",
                "In both families of algorithms, neighborhoods are formed by first computing the similarity between all pairs of users (for user-based) or items (for item-based).",
                "Predictions are then computed by aggregating ratings, which in a user-based algorithm involves aggregating the ratings of the target item by the users neighbors and, in an item-based algorithm, involves aggregating the users ratings of items that are neighbors of the target item.",
                "Algorithms within these families differ in the definition of similarity, formation of neighborhoods, and the computation of predictions.",
                "We consider a user-based algorithm based on that defined for GroupLens [11] with variations from Herlocker et al. [2], and an item-based algorithm similar to that of Sarwar et al. [12].",
                "The algorithm used by Resnick et al. [11] defines the similarity of two users u and v as the Pearson correlation of their common ratings: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2 , where Iu is the set of items rated by user u, ru,i is user us rating for item i, and ¯ru is the average rating of user u (similarly for v).",
                "Similarity computed in this manner is typically scaled by a factor proportional to the number of common ratings, to reduce the chance of making a recommendation made on weak connections: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), where γ ≈ 5 is a constant used as a lower limit in scaling [2].",
                "These new similarities are then used to define a static <br>neighborhood</br> Nu for each user u consisting of the top K users most similar to user u.",
                "A prediction for user u and item i is computed by a weighted average of the ratings by the neighbors pu,i = ¯ru + P v∈V sim (u, v)(rv,i − ¯rv) P v∈V sim (u, v) (1) where V = Nu ∩ Ui is the set of users most similar to u who have rated i.",
                "The item-based algorithm we use is the one defined by Sarwar et al. [12].",
                "In this algorithm, similarity is defined as the adjusted cosine measure sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) where Ui is the set of users who have rated item i.",
                "As for the user-based algorithm, the similarity weights are adjusted proportionally to the number of users that have rated the items in common sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). (3) Given the similarities, the <br>neighborhood</br> Ni of an item i is defined as the top K most similar items for that item.",
                "A prediction for user u and item i is computed as the weighted average pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) where J = Ni ∩ Iu is the set of items rated by u that are most similar to i. 2.2 Evaluation Recommender algorithms have typically been evaluated using measures of predictive accuracy and coverage [3].",
                "Studies on recommender algorithms, notably Herlocker et al. [2] and Sarwar et al. [12], typically compute predictive accuracy by dividing a set of ratings into training and test sets, and compute the prediction for an item in the test set using the ratings in the training set.",
                "A standard measure of predictive accuracy is mean absolute error (MAE), which for a test set T = {(u, i)} is defined as, MAE = P (u,i)∈T |pu,i − ru,i| |T | . (5) Coverage has a number of definitions, but generally refers to the proportion of items that can be predicted by the algorithm [3].",
                "A practical issue with predictive accuracy is that users typically are presented with recommendation lists, and not individual numeric predictions.",
                "Recommendation lists are lists of items in decreasing order of prediction (sometimes stated in terms of star-ratings), and so predictive accuracy may not be reflective of the accuracy of the list.",
                "So, instead we can measure recommendation or rank accuracy, which indicates the extent to which the list is in the correct order.",
                "Herlocker et al. [3] discuss a number of rank accuracy measures, which range from Kendalls Tau to measures that consider the fact that users tend to only look at a prefix of the list [5].",
                "Kendalls Tau measures the number of inversions when comparing ordered pairs in the true user ordering of 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figure 1: Ratings in simple movie recommender. items and the recommended order, and is defined as τ = C − D p (C + D + TR)(C + D + TP) (6) where C is the number of pairs that the system predicts in the correct order, D the number of pairs the system predicts in the wrong order, TR the number of pairs in the true ordering that have the same ratings, and TP is the number of pairs in the predicted ordering that have the same ratings [3].",
                "A shortcoming of the Tau metric is that it is oblivious to the position in the ordered list where the inversion occurs [3].",
                "For instance, an inversion toward the end of the list is given the same weight as one in the beginning.",
                "One solution is to consider inversions only in the top few items in the recommended list or to weight inversions based on their position in the list. 3.",
                "ROLES OF A RATING Our basic observation is that each rating plays a different role in each prediction in which it is used.",
                "Consider a simplified movie recommender system with three users Jim, Jeff, and Tom and their ratings for a few movies, as shown in Fig. 1. (For this initial discussion we will not consider the rating values involved.)",
                "The recommender predicts whether Tom will like The Mask using the other already available ratings.",
                "How this is done depends on the algorithm: 1.",
                "An item-based collaborative filtering algorithm constructs a <br>neighborhood</br> of movies around The Mask by using the ratings of users who rated The Mask and other movies similarly (e.g., Jims ratings of The Matrix and The Mask; and Jeffs ratings of Star Wars and The Mask).",
                "Toms ratings of those movies are then used to make a prediction for The Mask. 2.",
                "A user-based collaborative filtering algorithm would construct a <br>neighborhood</br> around Tom by tracking other users whose rating behaviors are similar to Toms (e.g., Tom and Jeff have rated Star Wars; Tom and Jim have rated The Matrix).",
                "The prediction of Toms rating for The Mask is then based on the ratings of Jeff and Tim.",
                "Although the nearest-neighbor algorithms aggregate the ratings to form neighborhoods used to compute predictions, we can disaggregate the similarities to view the computation of a prediction as simultaneously following parallel paths of ratings.",
                "So, irrespective of the collaborative filtering algorithm used, we can visualize the prediction of Toms rating of The Mask as walking through a sequence of ratings.",
                "In Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figure 2: Ratings used to predict The Mask for Tom.",
                "Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figure 3: Prediction of The Mask for Tom in which a rating is used more than once. this example, two paths were used for this prediction as depicted in Fig. 2: (p1, p2, p3) and (q1, q2, q3).",
                "Note that these paths are undirected, and are all of length 3.",
                "Only the order in which the ratings are traversed is different between the item-based algorithm (e.g., (p3, p2, p1), (q3, q2, q1)) and the user-based algorithm (e.g., (p1, p2, p3), (q1, q2, q3).)",
                "A rating can be part of many paths for a single prediction as shown in Fig. 3, where three paths are used for a prediction, two of which follow p1: (p1, p2, p3) and (p1, r2, r3).",
                "Predictions in a collaborative filtering algorithms may involve thousands of such walks in parallel, each playing a part in influencing the predicted value.",
                "Each prediction path consists of three ratings, playing roles that we call scouts, promoters, and connectors.",
                "To illustrate these roles, consider the path (p1, p2, p3) in Fig. 2 used to make a prediction of The Mask for Tom: 1.",
                "The rating p1 (Tom → Star Wars) makes a connection from Tom to other ratings that can be used to predict Toms rating for The Mask.",
                "This rating serves as a scout in the bipartite graph of ratings to find a path that leads to The Mask. 2.",
                "The rating p2 (Jeff → Star Wars) helps the system recommend The Mask to Tom by connecting the scout to the promoter. 3.",
                "The rating p3 (Jeff → The Mask) allows connections to The Mask, and, therefore, promotes this movie to Tom.",
                "Formally, given a prediction pu,a of a target item a for user u, a scout for pu,a is a rating ru,i such that there exists a user v with ratings rv,a and rv,i for some item i; a promoter for pu,a is a rating rv,a for some user v, such that there exist ratings rv,i and ru,i for an item i, and; a connector for pu,a 252 Jim Tom Jeff Jerry My Cousin Vinny The Matrix Star Wars The Mask Jurasic Park Figure 4: Scouts, promoters, and connectors. is a rating rv,i by some user v and rating i, such that there exists ratings ru,i and rv,a.",
                "The scouts, connectors, and promoters for the prediction of Toms rating of The Mask are p1 and q1, p2 and q2, and p3 and q3 (respectively).",
                "Each of these roles has a value in the recommender to the user, the users <br>neighborhood</br>, and the system in terms of allowing recommendations to be made. 3.1 Roles in Detail Ratings that act as scouts tend to help the recommender system suggest more movies to the user, though the extent to which this is true depends on the rating behavior of other users.",
                "For example, in Fig. 4 the rating Tom → Star Wars helps the system recommend only The Mask to him, while Tom → The Matrix helps recommend The Mask, Jurassic Park, and My Cousin Vinny.",
                "Tom makes a connection to Jim who is a prolific user of the system, by rating The Matrix.",
                "However, this does not make The Matrix the best movie to rate for everyone.",
                "For example, Jim is benefited equally by both The Mask and The Matrix, which allow the system to recommend Star Wars to him.",
                "His rating of The Mask is the best scout for Jeff, and Jerrys only scout is his rating of Star Wars.",
                "This suggests that good scouts allow a user to build similarity with prolific users, and thereby ensure they get more from the system.",
                "While scouts represent beneficial ratings from the perspective of a user, promoters are their duals, and are of benefit to items.",
                "In Fig. 4, My Cousin Vinny benefits from Jims rating, since it allows recommendations to Jeff and Tom.",
                "The Mask is not so dependent on just one rating, since the ratings by Jim and Jeff help it.",
                "On the other hand, Jerrys rating of Star Wars does not help promote it to any other user.",
                "We conclude that a good promoter connects an item to a broader <br>neighborhood</br> of other items, and thereby ensures that it is recommended to more users.",
                "Connectors serve a crucial role in a recommender system that is not as obvious.",
                "The movies My Cousin Vinny and Jurassic Park have the highest recommendation potential since they can be recommended to Jeff, Jerry and Tom based on the linkage structure illustrated in Fig. 4.",
                "Beside the fact that Jim rated these movies, these recommendations are possible only because of the ratings Jim → The Matrix and Jim → The Mask, which are the best connectors.",
                "A connector improves the systems ability to make recommendations with no explicit gain for the user.",
                "Note that every rating can be of varied benefit in each of these roles.",
                "The rating Jim → My Cousin Vinny is a poor scout and connector, but is a very good promoter.",
                "The rating Jim → The Mask is a reasonably good scout, a very good connector, and a good promoter.",
                "Finally, the rating Jerry → Star Wars is a very good scout, but is of no value as a connector or promoter.",
                "As illustrated here, a rating can have different value in each of the three roles in terms of whether a recommendation can be made or not.",
                "We could measure this value by simply counting the number of times a rating is used in each role, which alone would be helpful in characterizing the behavior of a system.",
                "But we can also measure the contribution of each rating to the quality of recommendations or health of the system.",
                "Since every prediction is a combined effort of several recommendation paths, we are interested in discerning the influence of each rating (and, hence, each path) in the system towards the systems overall error.",
                "We can understand the dynamics of the system at a finer granularity by tracking the influence of a rating according to the role played.",
                "The next section describes the approach to measuring the values of a rating in each role. 4.",
                "CONTRIBUTIONS OF RATINGS As weve seen, a rating may play different roles in different predictions and, in each prediction, contribute to the quality of a prediction in different ways.",
                "Our approach can use any numeric measure of a property of system health, and assigns credit (or blame) to each rating proportional to its influence in the prediction.",
                "By tracking the role of each rating in a prediction, we can accumulate the credit for a rating in each of the three roles, and also track the evolution of the roles of rating over time in the system.",
                "This section defines the methodology for computing the contribution of ratings by first defining the influence of a rating, and then instantiating the approach for predictive accuracy, and then rank accuracy.",
                "We also demonstrate how these contributions can be aggregated to study the <br>neighborhood</br> of ratings involved in computing a users recommendations.",
                "Note that although our general formulation for rating influence is algorithm independent, due to space considerations, we present the approach for only item-based collaborative filtering.",
                "The definition for user-based algorithms is similar and will be presented in an expanded version of this paper. 4.1 Influence of Ratings Recall that an item-based approach to collaborative filtering relies on building item neighborhoods using the similarity of ratings by the same user.",
                "As described earlier, similarity is defined by the adjusted cosine measure (Equations (2) and (3)).",
                "A set of the top K neighbors is maintained for all items for space and computational efficiency.",
                "A prediction of item i for a user u is computed as the weighted deviation from the items mean rating as shown in Equation (4).",
                "The list of recommendations for a user is then the list of items sorted in descending order of their predicted values.",
                "We first define impact(a, i, j), the impact a user a has in determining the similarity between two items i and j.",
                "This is the change in the similarity between i and j when as rating is removed, and is defined as impact(a, i, j) = |sim (i, j) − sim¯a(i, j)| P w∈Cij |sim (i, j) − sim ¯w(i, j)| where Cij = {u ∈ U | ∃ ru,i, ru,j ∈ R(u)} is the set of coraters 253 of items i and j (users who rate both i and j), R(u) is the set of ratings provided by user u, and sim¯a(i, j) is the similarity of i and j when the ratings of user a are removed sim¯a(i, j) = P v∈U\\{a} (ru,i − ¯ru)(ru,j − ¯ru) qP u∈U\\{a}(ru,i − ¯ru)2 qP u∈U\\{a}(ru,j − ¯ru)2 , and adjusted for the number of raters sim¯a(i, j) = max(|Ui ∩ Uj| − 1, γ) γ · sim(i, j).",
                "If all coraters of i and j rate them identically, we define the impact as impact(a, i, j) = 1 |Cij| since P w∈Cij |sim (i, j) − sim ¯w(i, j)| = 0.",
                "The influence of each path (u, j, v, i) = [ru,j, rv,j, rv,i] in the prediction of pu,i is given by influence(u, j, v, i) = sim (i, j) P l∈Ni∩Iu sim (i, l) · impact(v, i, j) It follows that the sum of influences over all such paths, for a given set of endpoints, is 1. 4.2 Role Values for Predictive Accuracy The value of a rating in each role is computed from the influence depending on the evaluation measure employed.",
                "Here we illustrate the approach using predictive accuracy as the evaluation metric.",
                "In general, the goodness of a prediction decides whether the ratings involved must be credited or discredited for their role.",
                "For predictive accuracy, the error in prediction e = |pu,i − ru,i| is mapped to a comfort level using a mapping function M(e).",
                "Anecdotal evidence suggests that users are unable to discern errors less than 1.0 (for a rating scale of 1 to 5) [4], and so an error less than 1.0 is considered acceptable, but anything larger is not.",
                "We hence define M(e) as (1 − e) binned to an appropriate value in [−1, −0.5, 0.5, 1].",
                "For each prediction pu,i, M(e) is attributed to all the paths that assisted the computation of pu,i, proportional to their influences.",
                "This tribute, M(e)∗influence(u, j, v, i), is in turn inherited by each of the ratings in the path [ru,j, rv,j, rv,i], with the credit/blame accumulating to the respective roles of ru,j as a scout, rv,j as a connector, and rv,i as a promoter.",
                "In other words, the scout value SF(ru,j), the connector value CF(rv,j) and the promoter value PF(rv,i) are all incremented by the tribute amount.",
                "Over a large number of predictions, scouts that have repeatedly resulted in big error rates have a big negative scout value, and vice versa (similarly with the other roles).",
                "Every rating is thus summarized by its triple [SF, CF, PF]. 4.3 Role Values for Rank Accuracy We now define the computation of the contribution of ratings to observed rank accuracy.",
                "For this computation, we must know the users preference order for a set of items for which predictions can be computed.",
                "We assume that we have a test set of the users ratings of the items presented in the recommendation list.",
                "For every pair of items rated by a user in the test data, we check whether the predicted order is concordant with his preference.",
                "We say a pair (i, j) is concordant (with error ) whenever one of the following holds: • if (ru,i < ru,j) then (pu,i − pu,j < ); • if (ru,i > ru,j) then (pu,i − pu,j > ); or • if (ru,i = ru,j) then (|pu,i − pu,j| ≤ ).",
                "Similarly, a pair (i, j) is discordant (with error ) if it is not concordant.",
                "Our experiments described below use an error tolerance of = 0.1.",
                "All paths involved in the prediction of the two items in a concordant pair are credited, and the paths involved in a discordant pair are discredited.",
                "The credit assigned to a pair of items (i, j) in the recommendation list for user u is computed as c(i, j) = ( t T · 1 C+D if (i, j) are concordant − t T · 1 C+D if (i, j) are discordant (7) where t is the number of items in the users test set whose ratings could be predicted, T is the number of items rated by user u in the test set, C is the number of concordances and D is the number of discordances.",
                "The credit c is then divided among all paths responsible for predicting pu,i and pu,j proportional to their influences.",
                "We again add the role values obtained from all the experiments to form a triple [SF, CF, PF] for each rating. 4.4 Aggregating rating roles After calculating the role values for individual ratings, we can also use these values to study neighborhoods and the system.",
                "Here we consider how we can use the role values to characterize the health of a <br>neighborhood</br>.",
                "Consider the list of top recommendations presented to a user at a specific point in time.",
                "The collaborative filtering algorithm traversed many paths in his <br>neighborhood</br> through his scouts and other connectors and promoters to make these recommendations.",
                "We call these ratings the recommender <br>neighborhood</br> of the user.",
                "The user implicitly chooses this <br>neighborhood</br> of ratings through the items he rates.",
                "Apart from the collaborative filtering algorithm, the health of this <br>neighborhood</br> completely influences a users satisfaction with the system.",
                "We can characterize a users recommender <br>neighborhood</br> by aggregating the individual role values of the ratings involved, weighted by the influence of individual ratings in determining his recommended list.",
                "Different sections of the users <br>neighborhood</br> wield varied influence on his recommendation list.",
                "For instance, ratings reachable through highly rated items have a bigger say in the recommended items.",
                "Our aim is to study the system and classify users with respect to their positioning in a healthy or unhealthy <br>neighborhood</br>.",
                "A user can have a good set of scouts, but may be exposed to a <br>neighborhood</br> with bad connectors and promoters.",
                "He can have a good <br>neighborhood</br>, but his bad scouts may ensure the neighborhoods potential is rendered useless.",
                "We expect that users with good scouts and good neighborhoods will be most satisfied with the system in the future.",
                "A users <br>neighborhood</br> is characterized by a triple that represents the weighted sum of the role values of individual ratings involved in making recommendations.",
                "Consider a user u and his ordered list of recommendations L. An item i 254 in the list is weighted inversely, as K(i), depending on its position in the list.",
                "In our studies we use K(i) = p position(i).",
                "Several paths of ratings [ru,j, rv,j, rv,i] are involved in predicting pu,i which ultimately decides its position in L, each with influence(u, j, v, i).",
                "The recommender <br>neighborhood</br> of a user u is characterized by the triple, [SFN(u), CFN(u), PFN(u)] where SFN(u) = X i∈L P [ru,j ,rv,j ,rv,i] SF(ru,j)influence(u, j, v, i) K(i) !",
                "CFN(u) and PFN(u) are defined similarly.",
                "This triple estimates the quality of us recommendations based on the past track record of the ratings involved in their respective roles. 5.",
                "EXPERIMENTATION As we have seen, we can assign role values to each rating when evaluating a collaborative filtering system.",
                "In this section, we demonstrate the use of this approach to our overall goal of defining an approach to monitor and manage the health of a recommender system through experiments done on the MovieLens million rating dataset.",
                "In particular, we discuss results relating to identifying good scouts, promoters, and connectors; the evolution of rating roles; and the characterization of user neighborhoods. 5.1 Methodology Our experiments use the MovieLens million rating dataset, which consists of ratings by 6040 users of 3952 movies.",
                "The ratings are in the range 1 to 5, and are labeled with the time the rating was given.",
                "As discussed before, we consider only the item-based algorithm here (with item neighborhoods of size 30) and, due to space considerations, only present role value results for rank accuracy.",
                "Since we are interested in the evolution of the rating role values over time, the model of the recommender system is built by processing ratings in their arrival order.",
                "The timestamping provided by MovieLens is hence crucial for the analyses presented here.",
                "We make assessments of rating roles at intervals of 10,000 ratings and processed the first 200,000 ratings in the dataset (giving rise to 20 snapshots).",
                "We incrementally update the role values as the time ordered ratings are merged into the model.",
                "To keep the experiment computationally manageable, we define a test dataset for each user.",
                "As the time ordered ratings are merged into the model, we label a small randomly selected percentage (20%) as test data.",
                "At discrete epochs, i.e., after processing every 10,000 ratings, we compute the predictions for the ratings in the test data, and then compute the role values for the ratings used in the predictions.",
                "One potential criticism of this methodology is that the ratings in the test set are never evaluated for their roles.",
                "We overcome this concern by repeating the experiment, using different random seeds.",
                "The probability that every rating is considered for evaluation is then considerably high: 1 − 0.2n , where n is the number of times the experiment is repeated with different random seeds.",
                "The results here are based on n = 4 repetitions.",
                "The item-based collaborative filtering algorithms performance was ordinary with respect to rank accuracy.",
                "Fig. 5 shows a plot of the precision and recall as ratings were merged in time order into the model.",
                "The recall was always high, but the average precision was just about 53%. 0 0.2 0.4 0.6 0.8 1 1.2 10000 30000 50000 70000 90000110000130000150000 Ratings merged into model Value Precision Recall Figure 5: Precision and recall for the item-based collaborative filtering algorithm. 5.2 Inducing good scouts The ratings of a user that serve as scouts are those that allow the user to receive recommendations.",
                "We claim that users with ratings that have respectable scout values will be happier with the system than those with ratings with low scout values.",
                "Note that the item-based algorithm discussed here produces recommendation lists with nearly half of the pairs in the list discordant from the users preference.",
                "Whether all of these discordant pairs are observable by the user is unclear, however, certainly this suggests that there is a need to be able to direct users to items whose ratings would improve the lists.",
                "The distribution of the scout values for most users ratings are Gaussian with mean zero.",
                "Fig. 6 shows the frequency distribution of scout values for a sample user at a given snapshot.",
                "We observe that a large number of ratings never serve as scouts for their users.",
                "A relatable scenario is when Amazons recommender makes suggestions of books or items based on other items that were purchased as gifts.",
                "With simple relevance feedback from the user, such ratings can be isolated as bad scouts and discounted from future predictions.",
                "Removing bad scouts was found to be worthwhile for individual users but the overall performance improvement was only marginal.",
                "An obvious question is whether good scouts can be formed by merely rating popular movies as suggested by Rashid et al. [9].",
                "They show that a mix of popularity and rating entropy identifies the best items to suggest to new users when evaluated using MAE.",
                "Following their intuition, we would expect to see a higher correlation between popularityentropy and good scouts.",
                "We measured the Pearson correlation coefficient between aggregated scout values for a movie with the popularity of a movie (number of times it is rated); and with its popularity*variance measure at different snapshots of the system.",
                "Note that the scout values were initially anti-correlated with popularity (Fig. 7), but became moderately correlated as the system evolved.",
                "Both popularity and popularity*variance performed similarly.",
                "A possible explanation is that there has been insufficient time for the popular movies to accumulate ratings. 255 -10 0 10 20 30 40 50 60 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 Scout Value Frequency Figure 6: Distribution of scout values for a sample user. -0.4 -0.2 0 0.2 0.4 0.6 0.8 30000 60000 90000 120000 150000 180000 Popularity Pop*Var Figure 7: Correlation between aggregated scout value and item popularity (computed at different intervals). -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 30000 60000 90000 120000 150000 180000 Figure 8: Correlation between aggregated promoter value and user prolificity (computed at different intervals).",
                "Table 1: Movies forming the best scouts.",
                "Best Scouts Conf.",
                "Pop.",
                "Being John Malkovich (1999) 1.00 445 Star Wars: Episode IV - A New Hope (1977) 0.92 623 Princess Bride, The (1987) 0.85 477 Sixth Sense, The (1999) 0.85 617 Matrix, The (1999) 0.77 522 Ghostbusters (1984) 0.77 441 Casablanca (1942) 0.77 384 Insider, The (1999) 0.77 235 American Beauty (1999) 0.69 624 Terminator 2: Judgment Day (1991) 0.69 503 Fight Club (1999) 0.69 235 Shawshank Redemption, The (1994) 0.69 445 Run Lola Run (Lola rennt) (1998) 0.69 220 Terminator, The (1984) 0.62 450 Usual Suspects, The (1995) 0.62 326 Aliens (1986) 0.62 385 North by Northwest (1959) 0.62 245 Fugitive, The (1993) 0.62 402 End of Days (1999) 0.62 132 Raiders of the Lost Ark (1981) 0.54 540 Schindlers List (1993) 0.54 453 Back to the Future (1985) 0.54 543 Toy Story (1995) 0.54 419 Alien (1979) 0.54 415 Abyss, The (1989) 0.54 345 2001: A Space Odyssey (1968) 0.54 358 Dogma (1999) 0.54 228 Little Mermaid, The (1989) 0.54 203 Table 2: Movies forming the worst scouts.",
                "Worst scouts Conf.",
                "Pop.",
                "Harold and Maude (1971) 0.46 141 Grifters, The (1990) 0.46 180 Sting, The (1973) 0.38 244 Godfather: Part III, The (1990) 0.38 154 Lawrence of Arabia (1962) 0.38 167 High Noon (1952) 0.38 84 Women on the Verge of a... (1988) 0.38 113 Grapes of Wrath, The (1940) 0.38 115 Duck Soup (1933) 0.38 131 Arsenic and Old Lace (1944) 0.38 138 Midnight Cowboy (1969) 0.38 137 To Kill a Mockingbird (1962) 0.31 195 Four Weddings and a Funeral (1994) 0.31 271 Good, The Bad and The Ugly, The (1966) 0.31 156 Its a Wonderful Life (1946) 0.31 146 Player, The (1992) 0.31 220 Jackie Brown (1997) 0.31 118 Boat, The (Das Boot) (1981) 0.31 210 Manhattan (1979) 0.31 158 Truth About Cats & Dogs, The (1996) 0.31 143 Ghost (1990) 0.31 227 Lone Star (1996) 0.31 125 Big Chill, The (1983) 0.31 184 256 By studying the evolution of scout values, we can identify movies that consistently feature in good scouts over time.",
                "We claim these movies will make viable scouts for other users.",
                "We found the aggregated scout values for all movies in intervals of 10,000 ratings each.",
                "A movie is said to induce a good scout if the movie was in the top 100 of the sorted list, and to induce a bad scout if it was in bottom 100 of the same list.",
                "Movies appearing consistently high over time are expected to remain up there in the future.",
                "The effective confidence in a movie m is Cm = Tm − Bm N (8) where Tm is the number of times it appeared in the top 100, Bm the number of times it appeared in the bottom 100, and N is the number of intervals considered.",
                "Using this measure, the top few movies expected to induce the best scouts are shown in Table 1.",
                "Movies that would be bad scout choices are shown in Table 2 with their associated confidences.",
                "The popularities of the movies are also displayed.",
                "Although more popular movies appear in the list of good scouts, these tables show that a blind choice of scout based on popularity alone can be potentially dangerous.",
                "Interestingly, the best scout-Being John Malkovich-is about a puppeteer who discovers a portal into a movie star, a movie that has been described variously on amazon.com as makes you feel giddy, seriously weird, comedy with depth, silly, strange, and inventive.",
                "Indicating whether someone likes this movie or not goes a long way toward situating the user in a suitable <br>neighborhood</br>, with similar preferences.",
                "On the other hand, several factors may have made a movie a bad scout, like the sharp variance in user preferences in the <br>neighborhood</br> of a movie.",
                "Two users may have the same opinion about Lawrence of Arabia, but they may differ sharply about how they felt about the other movies they saw.",
                "Bad scouts ensue when there is deviation in behavior around a common synchronization point. 5.3 Inducing good promoters Ratings that serve to promote items in a collaborative filtering system are critical to allowing a new item be recommended to users.",
                "So, inducing good promoters is important for cold-start recommendation.",
                "We note that the frequency distribution of promoter values for a sample movies ratings is also Gaussian (similar to Fig. 6).",
                "This indicates that the promotion of a movie is benefited most by the ratings of a few users, and are unaffected by the ratings of most users.",
                "We find a strong correlation between a users number of ratings and his aggregated promoter value.",
                "Fig. 8 depicts the evolution of the Pearson correlation co-efficient between the prolificity of a user (number of ratings) versus his aggregated promoter value.",
                "We expect that conspicuous shills, by recommending wrong movies to users, will be discredited with negative aggregate promoter values and should be identifiable easily.",
                "Given this observation, the obvious rule to follow when introducing a new movie is to have it rated directly by prolific users who posses high aggregated promoter values.",
                "A new movie is thus cast into the <br>neighborhood</br> of many other movies improving its visibility.",
                "Note, though, that a user may have long stopped using the system.",
                "Tracking promoter values consistently allows only the most active recent users to be considered. 5.4 Inducing good connectors Given the way scouts, connectors, and promoters are characterized, it follows that the movies that are part of the best scouts are also part of the best connectors.",
                "Similarly, the users that constitute the best promoters are also part of the best connectors.",
                "Good connectors are induced by ensuring a user with a high promoter value rates a movie with a high scout value.",
                "In our experiments, we find that a ratings longest standing role is often as a connector.",
                "A rating with a poor connector value is often seen due to its user being a bad promoter, or its movie being a bad scout.",
                "Such ratings can be removed from the prediction process to bring marginal improvements to recommendations.",
                "In some selected experiments, we observed that removing a set of badly behaving connectors helped improve the systems overall performance by 1.5%.",
                "The effect was even higher on a few select users who observed an improvement of above 10% in precision without much loss in recall. 5.5 Monitoring the evolution of rating roles One of the more significant contributions of our work is the ability to model the evolution of recommender systems, by studying the changing roles of ratings over time.",
                "The role and value of a rating can change depending on many factors like user behavior, redundancy, shilling effects or properties of the collaborative filtering algorithm used.",
                "Studying the dynamics of rating roles in terms of transitions between good, bad, and negligible values can provide insights into the functioning of the recommender system.",
                "We believe that a continuous visualization of these transitions will improve the ability to manage a recommender system.",
                "We classify different rating states as good, bad, or negligible.",
                "Consider a user who has rated 100 movies in a particular interval, of which 20 are part of the test set.",
                "If a scout has a value greater than 0.005, it indicates that it is uniquely involved in at least 2 concordant predictions, which we will say is good.",
                "Thus, a threshold of 0.005 is chosen to bin a rating as good, bad or negligible in terms of its scout, connector and promoter value.",
                "For instance, a rating r, at time t with role value triple [0.1, 0.001, −0.01] is classified as [scout +, connector 0, promoter −], where + indicates good, 0 indicates negligible, and − indicates bad.",
                "The positive credit held by a rating is a measure of its contribution to the betterment of the system, and the discredit is a measure of its contribution to the detriment of the system.",
                "Even though the positive roles (and the negative roles) make up a very small percentage of all ratings, their contribution supersedes their size.",
                "For example, even though only 1.7% of all ratings were classified as good scouts, they hold 79% of all positive credit in the system!",
                "Similarly, the bad scouts were just 1.4% of all ratings but hold 82% of all discredit.",
                "Note that good and bad scouts, together, comprise only 1.4% + 1.7% = 3.1% of the ratings, implying that the majority of the ratings are negligible role players as scouts (more on this later).",
                "Likewise, good connectors were 1.2% of the system, and hold 30% of all positive credit.",
                "The bad connectors (0.8% of the system) hold 36% of all discredit.",
                "Good promoters (3% of the system) hold 46% of all credit, while bad promoters (2%) hold 50% of all discredit.",
                "This reiterates that a few ratings influence most of the systems performance.",
                "Hence it is important to track transitions between them regardless of their small numbers. 257 Across different snapshots, a rating can remain in the same state or change.",
                "A good scout can become a bad scout, a good promoter can become a good connector, good and bad scouts can become vestigial, and so on.",
                "It is not practical to expect a recommender system to have no ratings in bad roles.",
                "However, it suffices to see ratings in bad roles either convert to good or vestigial roles.",
                "Similarly, observing a large number of good roles become bad ones is a sign of imminent failure of the system.",
                "We employ the principle of non-overlapping episodes [6] to count such transitions.",
                "A sequence such as: [+, 0, 0] → [+, 0, 0] → [0, +, 0] → [0, 0, 0] is interpreted as the transitions [+, 0, 0] ; [0, +, 0] : 1 [+, 0, 0] ; [0, 0, 0] : 1 [0, +, 0] ; [0, 0, 0] : 1 instead of [+, 0, 0] ; [0, +, 0] : 2 [+, 0, 0] ; [0, 0, 0] : 2 [0, +, 0] ; [0, 0, 0] : 1.",
                "See [6] for further details about this counting procedure.",
                "Thus, a rating can be in one of 27 possible states, and there are about 272 possible transitions.",
                "We make a further simplification and utilize only 9 states, indicating whether the rating is a scout, promoter, or connector, and whether it has a positive, negative, or negligible role.",
                "Ratings that serve multiple purposes are counted using multiple episode instantiations but the states themselves are not duplicated beyond the 9 restricted states.",
                "In this model, a transition such as [+, 0, +] ; [0, +, 0] : 1 is counted as [scout+] ; [scout0] : 1 [scout+] ; [connector+] : 1 [scout+] ; [promoter0] : 1 [connector0] ; [scout0] : 1 [connector0] ; [scout+] : 1 [connector0] ; [promoter0] : 1 [promoter+] ; [scout0] : 1 [promoter+] ; [connector+] : 1 [promoter+] ; [promoter0] : 1 Of these, transitions like [pX] ; [q0] where p = q, X ∈ {+, 0, −} are considered uninteresting, and only the rest are counted.",
                "Fig. 9 depicts the major transitions counted while processing the first 200,000 ratings from the MovieLens dataset.",
                "Only transitions with frequency greater than or equal to 3% are shown.",
                "The percentages for each state indicates the number of ratings that were found to be in those states.",
                "We consider transitions from any state to a good state as healthy, from any state to a bad state as unhealthy, and from any state to a vestigial state as decaying.",
                "From Fig. 9, we can observe: • The bulk of the ratings have negligible values, irrespective of their role.",
                "The majority of the transitions involve both good and bad ratings becoming negligible.",
                "Scout + (2%) Scout(1.5%) Scout 0 (96.5%) Connector + (1.2%) Connector(0.8%) Connector 0 (98%) Promoter + (3%) Promoter(2%) Promoter 0 (95%) 84% 84% 81% 74% 10% 6% 11% 77% 8% 7% 8% 82% 4% 86% 4% 68% 15% 13% 5% 5% 77% 11% 7% 5% 4% 3% 3% 3% Healthy Unhealthy Decaying Figure 9: Transitions among rating roles. • The number of good ratings is comparable to the bad ratings, and ratings are seen to switch states often, except in the case of scouts (see below). • The negative and positive scout states are not reachable through any transition, indicating that these ratings must begin as such, and cannot be coerced into these roles. • Good promoters and good connectors have a much longer survival period than scouts.",
                "Transitions that recur to these states have frequencies of 10% and 15% when compared to just 4% for scouts.",
                "Good connectors are the slowest to decay whereas (good) scouts decay the fastest. • Healthy percentages are seen on transitions between promoters and connectors.",
                "As indicated earlier, there are hardly any transitions from promoters/connectors to scouts.",
                "This indicates that, over the long run, a users rating is more useful to others (movies or other users) than to the user himself. • The percentages of healthy transitions outweigh the unhealthy ones - this hints that the system is healthy, albeit only marginally.",
                "Note that these results are conditioned by the static nature of the dataset, which is a set of ratings over a fixed window of time.",
                "However a diagram such as Fig. 9 is clearly useful for monitoring the health of a recommender system.",
                "For instance, acceptable limits can be imposed on different types of transitions and, if a transition fails to meet the threshold, the recommender system or a part of it can be brought under closer scrutiny.",
                "Furthermore, the role state transition diagram would also be the ideal place to study the effects of shilling, a topic we will consider in future research. 5.6 Characterizing neighborhoods Earlier we saw that we can characterize the <br>neighborhood</br> of ratings involved in creating a recommendation list L for 258 a user.",
                "In our experiment, we consider lists of length 30, and sample the lists of about 5% of users through the evolution of the model (at intervals of 10,000 ratings each) and compute their <br>neighborhood</br> characteristics.",
                "To simplify our presentation, we consider the percentage of the sample that fall into one of the following categories: 1.",
                "Inactive user: (SFN(u) = 0) 2.",
                "Good scouts, Good <br>neighborhood</br>: (SFN(u) > 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 3.",
                "Good scouts, Bad <br>neighborhood</br>: (SFN(u) > 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) 4.",
                "Bad scouts, Good <br>neighborhood</br>: (SFN(u) < 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 5.",
                "Bad scouts, Bad <br>neighborhood</br>: (SFN(u) < 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) From our sample set of 561 users, we found that 476 users were inactive.",
                "Of the remaining 85 users, we found 26 users had good scouts and a good <br>neighborhood</br>, 6 had bad scouts and a good <br>neighborhood</br>, 29 had good scouts and a bad neighborhood, and 24 had bad scouts and a bad neighborhood.",
                "Thus, we conjecture that 59 users (29+24+6) are in danger of leaving the system.",
                "As a remedy, users with bad scouts and a good <br>neighborhood</br> can be asked to reconsider rating of some movies hoping to improve the systems recommendations.",
                "The system can be expected to deliver more if they engineer some good scouts.",
                "Users with good scouts and a bad <br>neighborhood</br> are harder to address; this situation might entail selectively removing some connector-promoter pairs that are causing the damage.",
                "Handling users with bad scouts and bad neighborhoods is a more difficult challenge.",
                "Such a classification allows the use of different strategies to better a users experience with the system depending on his context.",
                "In future work, we intend to conduct field studies and study the improvement in performance of different strategies for different contexts. 6.",
                "CONCLUSIONS To further recommender system acceptance and deployment, we require new tools and methodologies to manage an installed recommender and develop insights into the roles played by ratings.",
                "A fine-grained characterization in terms of rating roles such as scouts, promoters, and connectors, as done here, helps such an endeavor.",
                "Although we have presented results on only the item-based algorithm with list rank accuracy as the metric, the same approach outlined here applies to user-based algorithms and other metrics.",
                "In future research, we plan to systematically study the many algorithmic parameters, tolerances, and cutoff thresholds employed here and reason about their effects on the downstream conclusions.",
                "We also aim to extend our formulation to other collaborative filtering algorithms, study the effect of shilling in altering rating roles, conduct field studies, and evaluate improvements in user experience by tweaking ratings based on their role values.",
                "Finally, we plan to develop the idea of mining the evolution of rating role patterns into a reporting and tracking system for all aspects of recommender system health. 7.",
                "REFERENCES [1] Cosley, D., Lam, S., Albert, I., Konstan, J., and Riedl, J.",
                "Is Seeing Believing? : How Recommender System Interfaces Affect Users Opinions.",
                "In Proc.",
                "CHI (2001), pp. 585-592. [2] Herlocker, J. L., Konstan, J.",
                "A., Borchers, A., and Riedl, J.",
                "An Algorithmic Framework for Performing Collaborative Filtering.",
                "In Proc.",
                "SIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J.",
                "A., Terveen, L. G., and Riedl, J. T. Evaluating Collaborative Filtering Recommender Systems.",
                "ACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J.",
                "A.",
                "Personal communication. 2003. [5] Lam, S. K., and Riedl, J. Shilling Recommender Systems for Fun and Profit.",
                "In Proceedings of the 13th International World Wide Web Conference (2004), ACM Press, pp. 393-402. [6] Laxman, S., Sastry, P. S., and Unnikrishnan, K. P. Discovering Frequent Episodes and Learning Hidden Markov Models: A Formal Connection.",
                "IEEE Transactions on Knowledge and Data Engineering Vol. 17, 11 (2005), 1505-1517. [7] McLaughlin, M. R., and Herlocker, J. L. A Collaborative Filtering Algorithm and Evaluation Metric that Accurately Model the User Experience.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (2004), pp. 329 - 336. [8] OMahony, M., Hurley, N. J., Kushmerick, N., and Silvestre, G. Collaborative Recommendation: A Robustness Analysis.",
                "ACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J.",
                "A., and Riedl, J.",
                "Getting to Know You: Learning New User Preferences in Recommender Systems.",
                "In Proceedings of the 2002 Conference on Intelligent User Interfaces (IUI 2002) (2002), pp. 127-134. [10] Rashid, A. M., Karypis, G., and Riedl, J.",
                "Influence in Ratings-Based Recommender Systems: An Algorithm-Independent Approach.",
                "In Proc. of the SIAM International Conference on Data Mining (2005). [11] Resnick, P., Iacovou, N., Sushak, M., Bergstrom, P., and Riedl, J. GroupLens: An Open Architecture for Collaborative Filtering of Netnews.",
                "In Proceedings of the Conference on Computer Supported Collaborative Work (CSCW94) (1994), ACM Press, pp. 175-186. [12] Sarwar, B., Karypis, G., Konstan, J., and Reidl, J. Item-Based Collaborative Filtering Recommendation Algorithms.",
                "In Proceedings of the Tenth International World Wide Web Conference (WWW10) (2001), pp. 285-295. [13] Schein, A., Popescu, A., Ungar, L., and Pennock, D. Methods and Metrics for Cold-Start Recommendation.",
                "In Proc.",
                "SIGIR (2002), pp. 253-260. 259"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Usuarios de situación en mejores vecindarios: las calificaciones de los usuarios pueden conectar inadvertidamente al usuario a un \"vecindario\" para el cual los gustos de los usuarios pueden no ser una coincidencia perfecta.",
                "Identificar las calificaciones responsables de tales malas recomendaciones y sugerir nuevos elementos para calificar puede ayudar a ubicar al usuario en un mejor \"vecindario\".2.",
                "Estas nuevas similitudes se utilizan para definir un \"vecindario\" estático NU para cada usuario U que consiste en los principales usuarios de K más similares al usuario u.",
                "En cuanto al algoritmo basado en el usuario, los pesos de similitud se ajustan proporcionalmente al número de usuarios que han calificado los elementos en SIM (i, j) = max (| ui ∩ uj |, γ) γ · SIM (I, J).(3) Dadas las similitudes, el Ni \"vecindario\" de un artículo I se define como los mejores elementos K más similares para ese artículo.",
                "Un algoritmo de filtrado colaborativo basado en elementos construye un \"vecindario\" de películas alrededor de la máscara utilizando las clasificaciones de usuarios que calificaron la máscara y otras películas de manera similar (por ejemplo, las clasificaciones JIMS de la matriz y la máscara; y las clasificaciones de Jeffs de Star Wars yLa máscara).",
                "Un algoritmo de filtrado colaborativo basado en el usuario construiría un \"vecindario\" alrededor de Tom al rastrear a otros usuarios cuyos comportamientos de calificación son similares a Toms (por ejemplo, Tom y Jeff han calificado Star Wars; Tom y Jim han calificado la matriz).",
                "Cada uno de estos roles tiene un valor en el recomendador para el usuario, los usuarios \"vecindario\" y el sistema en términos de permitir que se hagan recomendaciones.3.1 Roles en las clasificaciones detalladas que actúan como Scouts tienden a ayudar al sistema de recomendación sugerir más películas para el usuario, aunque la medida en que esto es cierto depende del comportamiento de calificación de otros usuarios.",
                "Concluimos que un buen promotor conecta un elemento con un \"vecindario\" más amplio de otros artículos y, por lo tanto, asegura que se recomiende a más usuarios.",
                "También demostramos cómo estas contribuciones pueden agregarse para estudiar el \"vecindario\" de las calificaciones involucradas en la calculación de las recomendaciones de los usuarios.",
                "Aquí consideramos cómo podemos usar los valores de roles para caracterizar la salud de un \"vecindario\"."
            ],
            "translated_text": "",
            "candidates": [
                "vecindario",
                "vecindario",
                "Vecindario",
                "vecindario",
                "vecindario",
                "vecindario",
                "vecindario",
                "vecindario",
                "vecindario",
                "vecindario",
                "vecindario",
                "vecindario",
                "vecindario",
                "vecindario",
                "vecindario",
                "vecindario",
                "vecindario",
                "vecindario",
                "vecindario",
                "vecindario"
            ],
            "error": []
        },
        "recommender system": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Scouts, Promoters, and Connectors: The Roles of Ratings in Nearest Neighbor Collaborative Filtering Bharath Kumar Mohan Dept.",
                "of CSA Indian Institute of Science Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. of Computer Science Eastern Michigan University Ypsilanti, MI 48917, USA bkeller@emich.edu Naren Ramakrishnan Dept. of Computer Science Virginia Tech, Blacksburg VA 24061, USA naren@cs.vt.edu ABSTRACT Recommender systems aggregate individual user ratings into predictions of products or services that might interest visitors.",
                "The quality of this aggregation process crucially affects the user experience and hence the effectiveness of recommenders in e-commerce.",
                "We present a novel study that disaggregates global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearestneighbor collaborative filtering.",
                "In particular, we formulate three roles-scouts, promoters, and connectors-that capture how users receive recommendations, how items get recommended, and how ratings of these two types are themselves connected (resp.).",
                "These roles find direct uses in improving recommendations for users, in better targeting of items and, most importantly, in helping monitor the health of the system as a whole.",
                "For instance, they can be used to track the evolution of neighborhoods, to identify rating subspaces that do not contribute (or contribute negatively) to system performance, to enumerate users who are in danger of leaving, and to assess the susceptibility of the system to attacks such as shilling.",
                "We argue that the three rating roles presented here provide broad primitives to manage a <br>recommender system</br> and its community.",
                "Categories and Subject Descriptors H.4.2 [Information Systems Applications]: Types of Systems-Decision support; J.4 [Computer Applications]: Social and Behavioral Sciences General Terms Algorithms, Human Factors 1.",
                "INTRODUCTION Recommender systems have become integral to e-commerce, providing technology that suggests products to a visitor based on previous purchases or rating history.",
                "Collaborative filtering, a common form of recommendation, predicts a users rating for an item by combining (other) ratings of that user with other users ratings.",
                "Significant research has been conducted in implementing fast and accurate collaborative filtering algorithms [2, 7], designing interfaces for presenting recommendations to users [1], and studying the robustness of these algorithms [8].",
                "However, with the exception of a few studies on the influence of users [10], little attention has been paid to unraveling the inner workings of a recommender in terms of the individual ratings and the roles they play in making (good) recommendations.",
                "Such an understanding will give an important handle to monitoring and managing a <br>recommender system</br>, to engineer mechanisms to sustain the recommender, and thereby ensure its continued success.",
                "Our motivation here is to disaggregate global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearest-neighbor collaborative filtering.",
                "We identify three possible roles: (scouts) to connect the user into the system to receive recommendations, (promoters) to connect an item into the system to be recommended, and (connectors) to connect ratings of these two kinds.",
                "Viewing ratings in this way, we can define the contribution of a rating in each role, both in terms of allowing recommendations to occur, and in terms of influence on the quality of recommendations.",
                "In turn, this capability helps support scenarios such as: 1.",
                "Situating users in better neighborhoods: A users ratings may inadvertently connect the user to a neighborhood for which the users tastes may not be a perfect match.",
                "Identifying ratings responsible for such bad recommendations and suggesting new items to rate can help situate the user in a better neighborhood. 2.",
                "Targeting items: Recommender systems suffer from lack of user participation, especially in cold-start scenarios [13] involving newly arrived items.",
                "Identifying users who can be encouraged to rate specific items helps ensure coverage of the <br>recommender system</br>. 3.",
                "Monitoring the evolution of the <br>recommender system</br> and its stakeholders: A <br>recommender system</br> is constantly under change: growing with new users and 250 items, shrinking with users leaving the system, items becoming irrelevant, and parts of the system under attack.",
                "Tracking the roles of a rating and its evolution over time provides many insights into the health of the system, and how it could be managed and improved.",
                "These include being able to identify rating subspaces that do not contribute (or contribute negatively) to system performance, and could be removed; to enumerate users who are in danger of leaving, or have left the system; and to assess the susceptibility of the system to attacks such as shilling [5].",
                "As we show, the characterization of rating roles presented here provides broad primitives to manage a <br>recommender system</br> and its community.",
                "The rest of the paper is organized as follows.",
                "Background on nearest-neighbor collaborative filtering and algorithm evaluation is discussed in Section 2.",
                "Section 3 defines and discusses the roles of a rating, and Section 4 defines measures of the contribution of a rating in each of these roles.",
                "In Section 5, we illustrate the use of these roles to address the goals outlined above. 2.",
                "BACKGROUND 2.1 Algorithms Nearest-neighbor collaborative filtering algorithms either use neighborhoods of users or neighborhoods of items to compute a prediction.",
                "An algorithm of the first kind is called user-based, and one of the second kind is called itembased [12].",
                "In both families of algorithms, neighborhoods are formed by first computing the similarity between all pairs of users (for user-based) or items (for item-based).",
                "Predictions are then computed by aggregating ratings, which in a user-based algorithm involves aggregating the ratings of the target item by the users neighbors and, in an item-based algorithm, involves aggregating the users ratings of items that are neighbors of the target item.",
                "Algorithms within these families differ in the definition of similarity, formation of neighborhoods, and the computation of predictions.",
                "We consider a user-based algorithm based on that defined for GroupLens [11] with variations from Herlocker et al. [2], and an item-based algorithm similar to that of Sarwar et al. [12].",
                "The algorithm used by Resnick et al. [11] defines the similarity of two users u and v as the Pearson correlation of their common ratings: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2 , where Iu is the set of items rated by user u, ru,i is user us rating for item i, and ¯ru is the average rating of user u (similarly for v).",
                "Similarity computed in this manner is typically scaled by a factor proportional to the number of common ratings, to reduce the chance of making a recommendation made on weak connections: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), where γ ≈ 5 is a constant used as a lower limit in scaling [2].",
                "These new similarities are then used to define a static neighborhood Nu for each user u consisting of the top K users most similar to user u.",
                "A prediction for user u and item i is computed by a weighted average of the ratings by the neighbors pu,i = ¯ru + P v∈V sim (u, v)(rv,i − ¯rv) P v∈V sim (u, v) (1) where V = Nu ∩ Ui is the set of users most similar to u who have rated i.",
                "The item-based algorithm we use is the one defined by Sarwar et al. [12].",
                "In this algorithm, similarity is defined as the adjusted cosine measure sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) where Ui is the set of users who have rated item i.",
                "As for the user-based algorithm, the similarity weights are adjusted proportionally to the number of users that have rated the items in common sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). (3) Given the similarities, the neighborhood Ni of an item i is defined as the top K most similar items for that item.",
                "A prediction for user u and item i is computed as the weighted average pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) where J = Ni ∩ Iu is the set of items rated by u that are most similar to i. 2.2 Evaluation Recommender algorithms have typically been evaluated using measures of predictive accuracy and coverage [3].",
                "Studies on recommender algorithms, notably Herlocker et al. [2] and Sarwar et al. [12], typically compute predictive accuracy by dividing a set of ratings into training and test sets, and compute the prediction for an item in the test set using the ratings in the training set.",
                "A standard measure of predictive accuracy is mean absolute error (MAE), which for a test set T = {(u, i)} is defined as, MAE = P (u,i)∈T |pu,i − ru,i| |T | . (5) Coverage has a number of definitions, but generally refers to the proportion of items that can be predicted by the algorithm [3].",
                "A practical issue with predictive accuracy is that users typically are presented with recommendation lists, and not individual numeric predictions.",
                "Recommendation lists are lists of items in decreasing order of prediction (sometimes stated in terms of star-ratings), and so predictive accuracy may not be reflective of the accuracy of the list.",
                "So, instead we can measure recommendation or rank accuracy, which indicates the extent to which the list is in the correct order.",
                "Herlocker et al. [3] discuss a number of rank accuracy measures, which range from Kendalls Tau to measures that consider the fact that users tend to only look at a prefix of the list [5].",
                "Kendalls Tau measures the number of inversions when comparing ordered pairs in the true user ordering of 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figure 1: Ratings in simple movie recommender. items and the recommended order, and is defined as τ = C − D p (C + D + TR)(C + D + TP) (6) where C is the number of pairs that the system predicts in the correct order, D the number of pairs the system predicts in the wrong order, TR the number of pairs in the true ordering that have the same ratings, and TP is the number of pairs in the predicted ordering that have the same ratings [3].",
                "A shortcoming of the Tau metric is that it is oblivious to the position in the ordered list where the inversion occurs [3].",
                "For instance, an inversion toward the end of the list is given the same weight as one in the beginning.",
                "One solution is to consider inversions only in the top few items in the recommended list or to weight inversions based on their position in the list. 3.",
                "ROLES OF A RATING Our basic observation is that each rating plays a different role in each prediction in which it is used.",
                "Consider a simplified movie <br>recommender system</br> with three users Jim, Jeff, and Tom and their ratings for a few movies, as shown in Fig. 1. (For this initial discussion we will not consider the rating values involved.)",
                "The recommender predicts whether Tom will like The Mask using the other already available ratings.",
                "How this is done depends on the algorithm: 1.",
                "An item-based collaborative filtering algorithm constructs a neighborhood of movies around The Mask by using the ratings of users who rated The Mask and other movies similarly (e.g., Jims ratings of The Matrix and The Mask; and Jeffs ratings of Star Wars and The Mask).",
                "Toms ratings of those movies are then used to make a prediction for The Mask. 2.",
                "A user-based collaborative filtering algorithm would construct a neighborhood around Tom by tracking other users whose rating behaviors are similar to Toms (e.g., Tom and Jeff have rated Star Wars; Tom and Jim have rated The Matrix).",
                "The prediction of Toms rating for The Mask is then based on the ratings of Jeff and Tim.",
                "Although the nearest-neighbor algorithms aggregate the ratings to form neighborhoods used to compute predictions, we can disaggregate the similarities to view the computation of a prediction as simultaneously following parallel paths of ratings.",
                "So, irrespective of the collaborative filtering algorithm used, we can visualize the prediction of Toms rating of The Mask as walking through a sequence of ratings.",
                "In Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figure 2: Ratings used to predict The Mask for Tom.",
                "Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figure 3: Prediction of The Mask for Tom in which a rating is used more than once. this example, two paths were used for this prediction as depicted in Fig. 2: (p1, p2, p3) and (q1, q2, q3).",
                "Note that these paths are undirected, and are all of length 3.",
                "Only the order in which the ratings are traversed is different between the item-based algorithm (e.g., (p3, p2, p1), (q3, q2, q1)) and the user-based algorithm (e.g., (p1, p2, p3), (q1, q2, q3).)",
                "A rating can be part of many paths for a single prediction as shown in Fig. 3, where three paths are used for a prediction, two of which follow p1: (p1, p2, p3) and (p1, r2, r3).",
                "Predictions in a collaborative filtering algorithms may involve thousands of such walks in parallel, each playing a part in influencing the predicted value.",
                "Each prediction path consists of three ratings, playing roles that we call scouts, promoters, and connectors.",
                "To illustrate these roles, consider the path (p1, p2, p3) in Fig. 2 used to make a prediction of The Mask for Tom: 1.",
                "The rating p1 (Tom → Star Wars) makes a connection from Tom to other ratings that can be used to predict Toms rating for The Mask.",
                "This rating serves as a scout in the bipartite graph of ratings to find a path that leads to The Mask. 2.",
                "The rating p2 (Jeff → Star Wars) helps the system recommend The Mask to Tom by connecting the scout to the promoter. 3.",
                "The rating p3 (Jeff → The Mask) allows connections to The Mask, and, therefore, promotes this movie to Tom.",
                "Formally, given a prediction pu,a of a target item a for user u, a scout for pu,a is a rating ru,i such that there exists a user v with ratings rv,a and rv,i for some item i; a promoter for pu,a is a rating rv,a for some user v, such that there exist ratings rv,i and ru,i for an item i, and; a connector for pu,a 252 Jim Tom Jeff Jerry My Cousin Vinny The Matrix Star Wars The Mask Jurasic Park Figure 4: Scouts, promoters, and connectors. is a rating rv,i by some user v and rating i, such that there exists ratings ru,i and rv,a.",
                "The scouts, connectors, and promoters for the prediction of Toms rating of The Mask are p1 and q1, p2 and q2, and p3 and q3 (respectively).",
                "Each of these roles has a value in the recommender to the user, the users neighborhood, and the system in terms of allowing recommendations to be made. 3.1 Roles in Detail Ratings that act as scouts tend to help the <br>recommender system</br> suggest more movies to the user, though the extent to which this is true depends on the rating behavior of other users.",
                "For example, in Fig. 4 the rating Tom → Star Wars helps the system recommend only The Mask to him, while Tom → The Matrix helps recommend The Mask, Jurassic Park, and My Cousin Vinny.",
                "Tom makes a connection to Jim who is a prolific user of the system, by rating The Matrix.",
                "However, this does not make The Matrix the best movie to rate for everyone.",
                "For example, Jim is benefited equally by both The Mask and The Matrix, which allow the system to recommend Star Wars to him.",
                "His rating of The Mask is the best scout for Jeff, and Jerrys only scout is his rating of Star Wars.",
                "This suggests that good scouts allow a user to build similarity with prolific users, and thereby ensure they get more from the system.",
                "While scouts represent beneficial ratings from the perspective of a user, promoters are their duals, and are of benefit to items.",
                "In Fig. 4, My Cousin Vinny benefits from Jims rating, since it allows recommendations to Jeff and Tom.",
                "The Mask is not so dependent on just one rating, since the ratings by Jim and Jeff help it.",
                "On the other hand, Jerrys rating of Star Wars does not help promote it to any other user.",
                "We conclude that a good promoter connects an item to a broader neighborhood of other items, and thereby ensures that it is recommended to more users.",
                "Connectors serve a crucial role in a <br>recommender system</br> that is not as obvious.",
                "The movies My Cousin Vinny and Jurassic Park have the highest recommendation potential since they can be recommended to Jeff, Jerry and Tom based on the linkage structure illustrated in Fig. 4.",
                "Beside the fact that Jim rated these movies, these recommendations are possible only because of the ratings Jim → The Matrix and Jim → The Mask, which are the best connectors.",
                "A connector improves the systems ability to make recommendations with no explicit gain for the user.",
                "Note that every rating can be of varied benefit in each of these roles.",
                "The rating Jim → My Cousin Vinny is a poor scout and connector, but is a very good promoter.",
                "The rating Jim → The Mask is a reasonably good scout, a very good connector, and a good promoter.",
                "Finally, the rating Jerry → Star Wars is a very good scout, but is of no value as a connector or promoter.",
                "As illustrated here, a rating can have different value in each of the three roles in terms of whether a recommendation can be made or not.",
                "We could measure this value by simply counting the number of times a rating is used in each role, which alone would be helpful in characterizing the behavior of a system.",
                "But we can also measure the contribution of each rating to the quality of recommendations or health of the system.",
                "Since every prediction is a combined effort of several recommendation paths, we are interested in discerning the influence of each rating (and, hence, each path) in the system towards the systems overall error.",
                "We can understand the dynamics of the system at a finer granularity by tracking the influence of a rating according to the role played.",
                "The next section describes the approach to measuring the values of a rating in each role. 4.",
                "CONTRIBUTIONS OF RATINGS As weve seen, a rating may play different roles in different predictions and, in each prediction, contribute to the quality of a prediction in different ways.",
                "Our approach can use any numeric measure of a property of system health, and assigns credit (or blame) to each rating proportional to its influence in the prediction.",
                "By tracking the role of each rating in a prediction, we can accumulate the credit for a rating in each of the three roles, and also track the evolution of the roles of rating over time in the system.",
                "This section defines the methodology for computing the contribution of ratings by first defining the influence of a rating, and then instantiating the approach for predictive accuracy, and then rank accuracy.",
                "We also demonstrate how these contributions can be aggregated to study the neighborhood of ratings involved in computing a users recommendations.",
                "Note that although our general formulation for rating influence is algorithm independent, due to space considerations, we present the approach for only item-based collaborative filtering.",
                "The definition for user-based algorithms is similar and will be presented in an expanded version of this paper. 4.1 Influence of Ratings Recall that an item-based approach to collaborative filtering relies on building item neighborhoods using the similarity of ratings by the same user.",
                "As described earlier, similarity is defined by the adjusted cosine measure (Equations (2) and (3)).",
                "A set of the top K neighbors is maintained for all items for space and computational efficiency.",
                "A prediction of item i for a user u is computed as the weighted deviation from the items mean rating as shown in Equation (4).",
                "The list of recommendations for a user is then the list of items sorted in descending order of their predicted values.",
                "We first define impact(a, i, j), the impact a user a has in determining the similarity between two items i and j.",
                "This is the change in the similarity between i and j when as rating is removed, and is defined as impact(a, i, j) = |sim (i, j) − sim¯a(i, j)| P w∈Cij |sim (i, j) − sim ¯w(i, j)| where Cij = {u ∈ U | ∃ ru,i, ru,j ∈ R(u)} is the set of coraters 253 of items i and j (users who rate both i and j), R(u) is the set of ratings provided by user u, and sim¯a(i, j) is the similarity of i and j when the ratings of user a are removed sim¯a(i, j) = P v∈U\\{a} (ru,i − ¯ru)(ru,j − ¯ru) qP u∈U\\{a}(ru,i − ¯ru)2 qP u∈U\\{a}(ru,j − ¯ru)2 , and adjusted for the number of raters sim¯a(i, j) = max(|Ui ∩ Uj| − 1, γ) γ · sim(i, j).",
                "If all coraters of i and j rate them identically, we define the impact as impact(a, i, j) = 1 |Cij| since P w∈Cij |sim (i, j) − sim ¯w(i, j)| = 0.",
                "The influence of each path (u, j, v, i) = [ru,j, rv,j, rv,i] in the prediction of pu,i is given by influence(u, j, v, i) = sim (i, j) P l∈Ni∩Iu sim (i, l) · impact(v, i, j) It follows that the sum of influences over all such paths, for a given set of endpoints, is 1. 4.2 Role Values for Predictive Accuracy The value of a rating in each role is computed from the influence depending on the evaluation measure employed.",
                "Here we illustrate the approach using predictive accuracy as the evaluation metric.",
                "In general, the goodness of a prediction decides whether the ratings involved must be credited or discredited for their role.",
                "For predictive accuracy, the error in prediction e = |pu,i − ru,i| is mapped to a comfort level using a mapping function M(e).",
                "Anecdotal evidence suggests that users are unable to discern errors less than 1.0 (for a rating scale of 1 to 5) [4], and so an error less than 1.0 is considered acceptable, but anything larger is not.",
                "We hence define M(e) as (1 − e) binned to an appropriate value in [−1, −0.5, 0.5, 1].",
                "For each prediction pu,i, M(e) is attributed to all the paths that assisted the computation of pu,i, proportional to their influences.",
                "This tribute, M(e)∗influence(u, j, v, i), is in turn inherited by each of the ratings in the path [ru,j, rv,j, rv,i], with the credit/blame accumulating to the respective roles of ru,j as a scout, rv,j as a connector, and rv,i as a promoter.",
                "In other words, the scout value SF(ru,j), the connector value CF(rv,j) and the promoter value PF(rv,i) are all incremented by the tribute amount.",
                "Over a large number of predictions, scouts that have repeatedly resulted in big error rates have a big negative scout value, and vice versa (similarly with the other roles).",
                "Every rating is thus summarized by its triple [SF, CF, PF]. 4.3 Role Values for Rank Accuracy We now define the computation of the contribution of ratings to observed rank accuracy.",
                "For this computation, we must know the users preference order for a set of items for which predictions can be computed.",
                "We assume that we have a test set of the users ratings of the items presented in the recommendation list.",
                "For every pair of items rated by a user in the test data, we check whether the predicted order is concordant with his preference.",
                "We say a pair (i, j) is concordant (with error ) whenever one of the following holds: • if (ru,i < ru,j) then (pu,i − pu,j < ); • if (ru,i > ru,j) then (pu,i − pu,j > ); or • if (ru,i = ru,j) then (|pu,i − pu,j| ≤ ).",
                "Similarly, a pair (i, j) is discordant (with error ) if it is not concordant.",
                "Our experiments described below use an error tolerance of = 0.1.",
                "All paths involved in the prediction of the two items in a concordant pair are credited, and the paths involved in a discordant pair are discredited.",
                "The credit assigned to a pair of items (i, j) in the recommendation list for user u is computed as c(i, j) = ( t T · 1 C+D if (i, j) are concordant − t T · 1 C+D if (i, j) are discordant (7) where t is the number of items in the users test set whose ratings could be predicted, T is the number of items rated by user u in the test set, C is the number of concordances and D is the number of discordances.",
                "The credit c is then divided among all paths responsible for predicting pu,i and pu,j proportional to their influences.",
                "We again add the role values obtained from all the experiments to form a triple [SF, CF, PF] for each rating. 4.4 Aggregating rating roles After calculating the role values for individual ratings, we can also use these values to study neighborhoods and the system.",
                "Here we consider how we can use the role values to characterize the health of a neighborhood.",
                "Consider the list of top recommendations presented to a user at a specific point in time.",
                "The collaborative filtering algorithm traversed many paths in his neighborhood through his scouts and other connectors and promoters to make these recommendations.",
                "We call these ratings the recommender neighborhood of the user.",
                "The user implicitly chooses this neighborhood of ratings through the items he rates.",
                "Apart from the collaborative filtering algorithm, the health of this neighborhood completely influences a users satisfaction with the system.",
                "We can characterize a users recommender neighborhood by aggregating the individual role values of the ratings involved, weighted by the influence of individual ratings in determining his recommended list.",
                "Different sections of the users neighborhood wield varied influence on his recommendation list.",
                "For instance, ratings reachable through highly rated items have a bigger say in the recommended items.",
                "Our aim is to study the system and classify users with respect to their positioning in a healthy or unhealthy neighborhood.",
                "A user can have a good set of scouts, but may be exposed to a neighborhood with bad connectors and promoters.",
                "He can have a good neighborhood, but his bad scouts may ensure the neighborhoods potential is rendered useless.",
                "We expect that users with good scouts and good neighborhoods will be most satisfied with the system in the future.",
                "A users neighborhood is characterized by a triple that represents the weighted sum of the role values of individual ratings involved in making recommendations.",
                "Consider a user u and his ordered list of recommendations L. An item i 254 in the list is weighted inversely, as K(i), depending on its position in the list.",
                "In our studies we use K(i) = p position(i).",
                "Several paths of ratings [ru,j, rv,j, rv,i] are involved in predicting pu,i which ultimately decides its position in L, each with influence(u, j, v, i).",
                "The recommender neighborhood of a user u is characterized by the triple, [SFN(u), CFN(u), PFN(u)] where SFN(u) = X i∈L P [ru,j ,rv,j ,rv,i] SF(ru,j)influence(u, j, v, i) K(i) !",
                "CFN(u) and PFN(u) are defined similarly.",
                "This triple estimates the quality of us recommendations based on the past track record of the ratings involved in their respective roles. 5.",
                "EXPERIMENTATION As we have seen, we can assign role values to each rating when evaluating a collaborative filtering system.",
                "In this section, we demonstrate the use of this approach to our overall goal of defining an approach to monitor and manage the health of a <br>recommender system</br> through experiments done on the MovieLens million rating dataset.",
                "In particular, we discuss results relating to identifying good scouts, promoters, and connectors; the evolution of rating roles; and the characterization of user neighborhoods. 5.1 Methodology Our experiments use the MovieLens million rating dataset, which consists of ratings by 6040 users of 3952 movies.",
                "The ratings are in the range 1 to 5, and are labeled with the time the rating was given.",
                "As discussed before, we consider only the item-based algorithm here (with item neighborhoods of size 30) and, due to space considerations, only present role value results for rank accuracy.",
                "Since we are interested in the evolution of the rating role values over time, the model of the <br>recommender system</br> is built by processing ratings in their arrival order.",
                "The timestamping provided by MovieLens is hence crucial for the analyses presented here.",
                "We make assessments of rating roles at intervals of 10,000 ratings and processed the first 200,000 ratings in the dataset (giving rise to 20 snapshots).",
                "We incrementally update the role values as the time ordered ratings are merged into the model.",
                "To keep the experiment computationally manageable, we define a test dataset for each user.",
                "As the time ordered ratings are merged into the model, we label a small randomly selected percentage (20%) as test data.",
                "At discrete epochs, i.e., after processing every 10,000 ratings, we compute the predictions for the ratings in the test data, and then compute the role values for the ratings used in the predictions.",
                "One potential criticism of this methodology is that the ratings in the test set are never evaluated for their roles.",
                "We overcome this concern by repeating the experiment, using different random seeds.",
                "The probability that every rating is considered for evaluation is then considerably high: 1 − 0.2n , where n is the number of times the experiment is repeated with different random seeds.",
                "The results here are based on n = 4 repetitions.",
                "The item-based collaborative filtering algorithms performance was ordinary with respect to rank accuracy.",
                "Fig. 5 shows a plot of the precision and recall as ratings were merged in time order into the model.",
                "The recall was always high, but the average precision was just about 53%. 0 0.2 0.4 0.6 0.8 1 1.2 10000 30000 50000 70000 90000110000130000150000 Ratings merged into model Value Precision Recall Figure 5: Precision and recall for the item-based collaborative filtering algorithm. 5.2 Inducing good scouts The ratings of a user that serve as scouts are those that allow the user to receive recommendations.",
                "We claim that users with ratings that have respectable scout values will be happier with the system than those with ratings with low scout values.",
                "Note that the item-based algorithm discussed here produces recommendation lists with nearly half of the pairs in the list discordant from the users preference.",
                "Whether all of these discordant pairs are observable by the user is unclear, however, certainly this suggests that there is a need to be able to direct users to items whose ratings would improve the lists.",
                "The distribution of the scout values for most users ratings are Gaussian with mean zero.",
                "Fig. 6 shows the frequency distribution of scout values for a sample user at a given snapshot.",
                "We observe that a large number of ratings never serve as scouts for their users.",
                "A relatable scenario is when Amazons recommender makes suggestions of books or items based on other items that were purchased as gifts.",
                "With simple relevance feedback from the user, such ratings can be isolated as bad scouts and discounted from future predictions.",
                "Removing bad scouts was found to be worthwhile for individual users but the overall performance improvement was only marginal.",
                "An obvious question is whether good scouts can be formed by merely rating popular movies as suggested by Rashid et al. [9].",
                "They show that a mix of popularity and rating entropy identifies the best items to suggest to new users when evaluated using MAE.",
                "Following their intuition, we would expect to see a higher correlation between popularityentropy and good scouts.",
                "We measured the Pearson correlation coefficient between aggregated scout values for a movie with the popularity of a movie (number of times it is rated); and with its popularity*variance measure at different snapshots of the system.",
                "Note that the scout values were initially anti-correlated with popularity (Fig. 7), but became moderately correlated as the system evolved.",
                "Both popularity and popularity*variance performed similarly.",
                "A possible explanation is that there has been insufficient time for the popular movies to accumulate ratings. 255 -10 0 10 20 30 40 50 60 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 Scout Value Frequency Figure 6: Distribution of scout values for a sample user. -0.4 -0.2 0 0.2 0.4 0.6 0.8 30000 60000 90000 120000 150000 180000 Popularity Pop*Var Figure 7: Correlation between aggregated scout value and item popularity (computed at different intervals). -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 30000 60000 90000 120000 150000 180000 Figure 8: Correlation between aggregated promoter value and user prolificity (computed at different intervals).",
                "Table 1: Movies forming the best scouts.",
                "Best Scouts Conf.",
                "Pop.",
                "Being John Malkovich (1999) 1.00 445 Star Wars: Episode IV - A New Hope (1977) 0.92 623 Princess Bride, The (1987) 0.85 477 Sixth Sense, The (1999) 0.85 617 Matrix, The (1999) 0.77 522 Ghostbusters (1984) 0.77 441 Casablanca (1942) 0.77 384 Insider, The (1999) 0.77 235 American Beauty (1999) 0.69 624 Terminator 2: Judgment Day (1991) 0.69 503 Fight Club (1999) 0.69 235 Shawshank Redemption, The (1994) 0.69 445 Run Lola Run (Lola rennt) (1998) 0.69 220 Terminator, The (1984) 0.62 450 Usual Suspects, The (1995) 0.62 326 Aliens (1986) 0.62 385 North by Northwest (1959) 0.62 245 Fugitive, The (1993) 0.62 402 End of Days (1999) 0.62 132 Raiders of the Lost Ark (1981) 0.54 540 Schindlers List (1993) 0.54 453 Back to the Future (1985) 0.54 543 Toy Story (1995) 0.54 419 Alien (1979) 0.54 415 Abyss, The (1989) 0.54 345 2001: A Space Odyssey (1968) 0.54 358 Dogma (1999) 0.54 228 Little Mermaid, The (1989) 0.54 203 Table 2: Movies forming the worst scouts.",
                "Worst scouts Conf.",
                "Pop.",
                "Harold and Maude (1971) 0.46 141 Grifters, The (1990) 0.46 180 Sting, The (1973) 0.38 244 Godfather: Part III, The (1990) 0.38 154 Lawrence of Arabia (1962) 0.38 167 High Noon (1952) 0.38 84 Women on the Verge of a... (1988) 0.38 113 Grapes of Wrath, The (1940) 0.38 115 Duck Soup (1933) 0.38 131 Arsenic and Old Lace (1944) 0.38 138 Midnight Cowboy (1969) 0.38 137 To Kill a Mockingbird (1962) 0.31 195 Four Weddings and a Funeral (1994) 0.31 271 Good, The Bad and The Ugly, The (1966) 0.31 156 Its a Wonderful Life (1946) 0.31 146 Player, The (1992) 0.31 220 Jackie Brown (1997) 0.31 118 Boat, The (Das Boot) (1981) 0.31 210 Manhattan (1979) 0.31 158 Truth About Cats & Dogs, The (1996) 0.31 143 Ghost (1990) 0.31 227 Lone Star (1996) 0.31 125 Big Chill, The (1983) 0.31 184 256 By studying the evolution of scout values, we can identify movies that consistently feature in good scouts over time.",
                "We claim these movies will make viable scouts for other users.",
                "We found the aggregated scout values for all movies in intervals of 10,000 ratings each.",
                "A movie is said to induce a good scout if the movie was in the top 100 of the sorted list, and to induce a bad scout if it was in bottom 100 of the same list.",
                "Movies appearing consistently high over time are expected to remain up there in the future.",
                "The effective confidence in a movie m is Cm = Tm − Bm N (8) where Tm is the number of times it appeared in the top 100, Bm the number of times it appeared in the bottom 100, and N is the number of intervals considered.",
                "Using this measure, the top few movies expected to induce the best scouts are shown in Table 1.",
                "Movies that would be bad scout choices are shown in Table 2 with their associated confidences.",
                "The popularities of the movies are also displayed.",
                "Although more popular movies appear in the list of good scouts, these tables show that a blind choice of scout based on popularity alone can be potentially dangerous.",
                "Interestingly, the best scout-Being John Malkovich-is about a puppeteer who discovers a portal into a movie star, a movie that has been described variously on amazon.com as makes you feel giddy, seriously weird, comedy with depth, silly, strange, and inventive.",
                "Indicating whether someone likes this movie or not goes a long way toward situating the user in a suitable neighborhood, with similar preferences.",
                "On the other hand, several factors may have made a movie a bad scout, like the sharp variance in user preferences in the neighborhood of a movie.",
                "Two users may have the same opinion about Lawrence of Arabia, but they may differ sharply about how they felt about the other movies they saw.",
                "Bad scouts ensue when there is deviation in behavior around a common synchronization point. 5.3 Inducing good promoters Ratings that serve to promote items in a collaborative filtering system are critical to allowing a new item be recommended to users.",
                "So, inducing good promoters is important for cold-start recommendation.",
                "We note that the frequency distribution of promoter values for a sample movies ratings is also Gaussian (similar to Fig. 6).",
                "This indicates that the promotion of a movie is benefited most by the ratings of a few users, and are unaffected by the ratings of most users.",
                "We find a strong correlation between a users number of ratings and his aggregated promoter value.",
                "Fig. 8 depicts the evolution of the Pearson correlation co-efficient between the prolificity of a user (number of ratings) versus his aggregated promoter value.",
                "We expect that conspicuous shills, by recommending wrong movies to users, will be discredited with negative aggregate promoter values and should be identifiable easily.",
                "Given this observation, the obvious rule to follow when introducing a new movie is to have it rated directly by prolific users who posses high aggregated promoter values.",
                "A new movie is thus cast into the neighborhood of many other movies improving its visibility.",
                "Note, though, that a user may have long stopped using the system.",
                "Tracking promoter values consistently allows only the most active recent users to be considered. 5.4 Inducing good connectors Given the way scouts, connectors, and promoters are characterized, it follows that the movies that are part of the best scouts are also part of the best connectors.",
                "Similarly, the users that constitute the best promoters are also part of the best connectors.",
                "Good connectors are induced by ensuring a user with a high promoter value rates a movie with a high scout value.",
                "In our experiments, we find that a ratings longest standing role is often as a connector.",
                "A rating with a poor connector value is often seen due to its user being a bad promoter, or its movie being a bad scout.",
                "Such ratings can be removed from the prediction process to bring marginal improvements to recommendations.",
                "In some selected experiments, we observed that removing a set of badly behaving connectors helped improve the systems overall performance by 1.5%.",
                "The effect was even higher on a few select users who observed an improvement of above 10% in precision without much loss in recall. 5.5 Monitoring the evolution of rating roles One of the more significant contributions of our work is the ability to model the evolution of recommender systems, by studying the changing roles of ratings over time.",
                "The role and value of a rating can change depending on many factors like user behavior, redundancy, shilling effects or properties of the collaborative filtering algorithm used.",
                "Studying the dynamics of rating roles in terms of transitions between good, bad, and negligible values can provide insights into the functioning of the <br>recommender system</br>.",
                "We believe that a continuous visualization of these transitions will improve the ability to manage a <br>recommender system</br>.",
                "We classify different rating states as good, bad, or negligible.",
                "Consider a user who has rated 100 movies in a particular interval, of which 20 are part of the test set.",
                "If a scout has a value greater than 0.005, it indicates that it is uniquely involved in at least 2 concordant predictions, which we will say is good.",
                "Thus, a threshold of 0.005 is chosen to bin a rating as good, bad or negligible in terms of its scout, connector and promoter value.",
                "For instance, a rating r, at time t with role value triple [0.1, 0.001, −0.01] is classified as [scout +, connector 0, promoter −], where + indicates good, 0 indicates negligible, and − indicates bad.",
                "The positive credit held by a rating is a measure of its contribution to the betterment of the system, and the discredit is a measure of its contribution to the detriment of the system.",
                "Even though the positive roles (and the negative roles) make up a very small percentage of all ratings, their contribution supersedes their size.",
                "For example, even though only 1.7% of all ratings were classified as good scouts, they hold 79% of all positive credit in the system!",
                "Similarly, the bad scouts were just 1.4% of all ratings but hold 82% of all discredit.",
                "Note that good and bad scouts, together, comprise only 1.4% + 1.7% = 3.1% of the ratings, implying that the majority of the ratings are negligible role players as scouts (more on this later).",
                "Likewise, good connectors were 1.2% of the system, and hold 30% of all positive credit.",
                "The bad connectors (0.8% of the system) hold 36% of all discredit.",
                "Good promoters (3% of the system) hold 46% of all credit, while bad promoters (2%) hold 50% of all discredit.",
                "This reiterates that a few ratings influence most of the systems performance.",
                "Hence it is important to track transitions between them regardless of their small numbers. 257 Across different snapshots, a rating can remain in the same state or change.",
                "A good scout can become a bad scout, a good promoter can become a good connector, good and bad scouts can become vestigial, and so on.",
                "It is not practical to expect a <br>recommender system</br> to have no ratings in bad roles.",
                "However, it suffices to see ratings in bad roles either convert to good or vestigial roles.",
                "Similarly, observing a large number of good roles become bad ones is a sign of imminent failure of the system.",
                "We employ the principle of non-overlapping episodes [6] to count such transitions.",
                "A sequence such as: [+, 0, 0] → [+, 0, 0] → [0, +, 0] → [0, 0, 0] is interpreted as the transitions [+, 0, 0] ; [0, +, 0] : 1 [+, 0, 0] ; [0, 0, 0] : 1 [0, +, 0] ; [0, 0, 0] : 1 instead of [+, 0, 0] ; [0, +, 0] : 2 [+, 0, 0] ; [0, 0, 0] : 2 [0, +, 0] ; [0, 0, 0] : 1.",
                "See [6] for further details about this counting procedure.",
                "Thus, a rating can be in one of 27 possible states, and there are about 272 possible transitions.",
                "We make a further simplification and utilize only 9 states, indicating whether the rating is a scout, promoter, or connector, and whether it has a positive, negative, or negligible role.",
                "Ratings that serve multiple purposes are counted using multiple episode instantiations but the states themselves are not duplicated beyond the 9 restricted states.",
                "In this model, a transition such as [+, 0, +] ; [0, +, 0] : 1 is counted as [scout+] ; [scout0] : 1 [scout+] ; [connector+] : 1 [scout+] ; [promoter0] : 1 [connector0] ; [scout0] : 1 [connector0] ; [scout+] : 1 [connector0] ; [promoter0] : 1 [promoter+] ; [scout0] : 1 [promoter+] ; [connector+] : 1 [promoter+] ; [promoter0] : 1 Of these, transitions like [pX] ; [q0] where p = q, X ∈ {+, 0, −} are considered uninteresting, and only the rest are counted.",
                "Fig. 9 depicts the major transitions counted while processing the first 200,000 ratings from the MovieLens dataset.",
                "Only transitions with frequency greater than or equal to 3% are shown.",
                "The percentages for each state indicates the number of ratings that were found to be in those states.",
                "We consider transitions from any state to a good state as healthy, from any state to a bad state as unhealthy, and from any state to a vestigial state as decaying.",
                "From Fig. 9, we can observe: • The bulk of the ratings have negligible values, irrespective of their role.",
                "The majority of the transitions involve both good and bad ratings becoming negligible.",
                "Scout + (2%) Scout(1.5%) Scout 0 (96.5%) Connector + (1.2%) Connector(0.8%) Connector 0 (98%) Promoter + (3%) Promoter(2%) Promoter 0 (95%) 84% 84% 81% 74% 10% 6% 11% 77% 8% 7% 8% 82% 4% 86% 4% 68% 15% 13% 5% 5% 77% 11% 7% 5% 4% 3% 3% 3% Healthy Unhealthy Decaying Figure 9: Transitions among rating roles. • The number of good ratings is comparable to the bad ratings, and ratings are seen to switch states often, except in the case of scouts (see below). • The negative and positive scout states are not reachable through any transition, indicating that these ratings must begin as such, and cannot be coerced into these roles. • Good promoters and good connectors have a much longer survival period than scouts.",
                "Transitions that recur to these states have frequencies of 10% and 15% when compared to just 4% for scouts.",
                "Good connectors are the slowest to decay whereas (good) scouts decay the fastest. • Healthy percentages are seen on transitions between promoters and connectors.",
                "As indicated earlier, there are hardly any transitions from promoters/connectors to scouts.",
                "This indicates that, over the long run, a users rating is more useful to others (movies or other users) than to the user himself. • The percentages of healthy transitions outweigh the unhealthy ones - this hints that the system is healthy, albeit only marginally.",
                "Note that these results are conditioned by the static nature of the dataset, which is a set of ratings over a fixed window of time.",
                "However a diagram such as Fig. 9 is clearly useful for monitoring the health of a <br>recommender system</br>.",
                "For instance, acceptable limits can be imposed on different types of transitions and, if a transition fails to meet the threshold, the <br>recommender system</br> or a part of it can be brought under closer scrutiny.",
                "Furthermore, the role state transition diagram would also be the ideal place to study the effects of shilling, a topic we will consider in future research. 5.6 Characterizing neighborhoods Earlier we saw that we can characterize the neighborhood of ratings involved in creating a recommendation list L for 258 a user.",
                "In our experiment, we consider lists of length 30, and sample the lists of about 5% of users through the evolution of the model (at intervals of 10,000 ratings each) and compute their neighborhood characteristics.",
                "To simplify our presentation, we consider the percentage of the sample that fall into one of the following categories: 1.",
                "Inactive user: (SFN(u) = 0) 2.",
                "Good scouts, Good neighborhood: (SFN(u) > 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 3.",
                "Good scouts, Bad neighborhood: (SFN(u) > 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) 4.",
                "Bad scouts, Good neighborhood: (SFN(u) < 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 5.",
                "Bad scouts, Bad neighborhood: (SFN(u) < 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) From our sample set of 561 users, we found that 476 users were inactive.",
                "Of the remaining 85 users, we found 26 users had good scouts and a good neighborhood, 6 had bad scouts and a good neighborhood, 29 had good scouts and a bad neighborhood, and 24 had bad scouts and a bad neighborhood.",
                "Thus, we conjecture that 59 users (29+24+6) are in danger of leaving the system.",
                "As a remedy, users with bad scouts and a good neighborhood can be asked to reconsider rating of some movies hoping to improve the systems recommendations.",
                "The system can be expected to deliver more if they engineer some good scouts.",
                "Users with good scouts and a bad neighborhood are harder to address; this situation might entail selectively removing some connector-promoter pairs that are causing the damage.",
                "Handling users with bad scouts and bad neighborhoods is a more difficult challenge.",
                "Such a classification allows the use of different strategies to better a users experience with the system depending on his context.",
                "In future work, we intend to conduct field studies and study the improvement in performance of different strategies for different contexts. 6.",
                "CONCLUSIONS To further <br>recommender system</br> acceptance and deployment, we require new tools and methodologies to manage an installed recommender and develop insights into the roles played by ratings.",
                "A fine-grained characterization in terms of rating roles such as scouts, promoters, and connectors, as done here, helps such an endeavor.",
                "Although we have presented results on only the item-based algorithm with list rank accuracy as the metric, the same approach outlined here applies to user-based algorithms and other metrics.",
                "In future research, we plan to systematically study the many algorithmic parameters, tolerances, and cutoff thresholds employed here and reason about their effects on the downstream conclusions.",
                "We also aim to extend our formulation to other collaborative filtering algorithms, study the effect of shilling in altering rating roles, conduct field studies, and evaluate improvements in user experience by tweaking ratings based on their role values.",
                "Finally, we plan to develop the idea of mining the evolution of rating role patterns into a reporting and tracking system for all aspects of <br>recommender system</br> health. 7.",
                "REFERENCES [1] Cosley, D., Lam, S., Albert, I., Konstan, J., and Riedl, J.",
                "Is Seeing Believing? : How <br>recommender system</br> Interfaces Affect Users Opinions.",
                "In Proc.",
                "CHI (2001), pp. 585-592. [2] Herlocker, J. L., Konstan, J.",
                "A., Borchers, A., and Riedl, J.",
                "An Algorithmic Framework for Performing Collaborative Filtering.",
                "In Proc.",
                "SIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J.",
                "A., Terveen, L. G., and Riedl, J. T. Evaluating Collaborative Filtering Recommender Systems.",
                "ACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J.",
                "A.",
                "Personal communication. 2003. [5] Lam, S. K., and Riedl, J. Shilling Recommender Systems for Fun and Profit.",
                "In Proceedings of the 13th International World Wide Web Conference (2004), ACM Press, pp. 393-402. [6] Laxman, S., Sastry, P. S., and Unnikrishnan, K. P. Discovering Frequent Episodes and Learning Hidden Markov Models: A Formal Connection.",
                "IEEE Transactions on Knowledge and Data Engineering Vol. 17, 11 (2005), 1505-1517. [7] McLaughlin, M. R., and Herlocker, J. L. A Collaborative Filtering Algorithm and Evaluation Metric that Accurately Model the User Experience.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (2004), pp. 329 - 336. [8] OMahony, M., Hurley, N. J., Kushmerick, N., and Silvestre, G. Collaborative Recommendation: A Robustness Analysis.",
                "ACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J.",
                "A., and Riedl, J.",
                "Getting to Know You: Learning New User Preferences in Recommender Systems.",
                "In Proceedings of the 2002 Conference on Intelligent User Interfaces (IUI 2002) (2002), pp. 127-134. [10] Rashid, A. M., Karypis, G., and Riedl, J.",
                "Influence in Ratings-Based Recommender Systems: An Algorithm-Independent Approach.",
                "In Proc. of the SIAM International Conference on Data Mining (2005). [11] Resnick, P., Iacovou, N., Sushak, M., Bergstrom, P., and Riedl, J. GroupLens: An Open Architecture for Collaborative Filtering of Netnews.",
                "In Proceedings of the Conference on Computer Supported Collaborative Work (CSCW94) (1994), ACM Press, pp. 175-186. [12] Sarwar, B., Karypis, G., Konstan, J., and Reidl, J. Item-Based Collaborative Filtering Recommendation Algorithms.",
                "In Proceedings of the Tenth International World Wide Web Conference (WWW10) (2001), pp. 285-295. [13] Schein, A., Popescu, A., Ungar, L., and Pennock, D. Methods and Metrics for Cold-Start Recommendation.",
                "In Proc.",
                "SIGIR (2002), pp. 253-260. 259"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Argumentamos que los tres roles de calificación presentados aquí proporcionan primitivas amplias para administrar un \"sistema de recomendación\" y su comunidad.",
                "Tal comprensión dará un mango importante para el monitoreo y la gestión de un \"sistema de recomendación\", para diseñar mecanismos para mantener el recomendador y, por lo tanto, garantizar su éxito continuo.",
                "Identificar a los usuarios que pueden ser alentados a calificar elementos específicos ayuda a garantizar la cobertura del \"sistema de recomendación\".3.",
                "Monitoreo de la evolución del \"sistema de recomendación\" y sus partes interesadas: un \"sistema de recomendación\" está constantemente bajo cambio: crecer con nuevos usuarios y 250 elementos, reducirse con los usuarios que abandonan el sistema, los elementos se vuelven irrelevantes y partes del sistema bajo ataque.",
                "Como mostramos, la caracterización de los roles de calificación presentados aquí proporciona primitivas amplias para administrar un \"sistema de recomendación\" y su comunidad.",
                "",
                "Cada uno de estos roles tiene un valor en el recomendador para el usuario, el vecindario de los usuarios y el sistema en términos de permitir que se hagan recomendaciones.3.1 Roles en las calificaciones detalladas que actúan como exploradores tienden a ayudar al \"sistema de recomendación\" sugerir más películas para el usuario, aunque la medida en que esto es cierto depende del comportamiento de calificación de otros usuarios.",
                "Los conectores cumplen un papel crucial en un \"sistema de recomendación\" que no es tan obvio.",
                "En esta sección, demostramos el uso de este enfoque de nuestro objetivo general de definir un enfoque para monitorear y administrar la salud de un \"sistema de recomendación\" a través de experimentos realizados en el conjunto de datos de calificación de Mobielens Million.",
                "Dado que estamos interesados en la evolución de los valores de rol de calificación a lo largo del tiempo, el modelo del \"sistema de recomendación\" se construye mediante las calificaciones de procesamiento en su orden de llegada."
            ],
            "translated_text": "",
            "candidates": [
                "sistema de recomendación",
                "sistema de recomendación",
                "sistema de recomendación",
                "sistema de recomendación",
                "Sistema de recomendación",
                "sistema de recomendación",
                "sistema de recomendación",
                "sistema de recomendación",
                "sistema de recomendación",
                "sistema de recomendación",
                "sistema de recomendación",
                "",
                "sistema de recomendación",
                "sistema de recomendación",
                "sistema de recomendación",
                "sistema de recomendación",
                "sistema de recomendación",
                "sistema de recomendación",
                "sistema de recomendación",
                "sistema de recomendación",
                "sistema de recomendación"
            ],
            "error": []
        },
        "collaborative filter": {
            "translated_key": "filtro colaborativo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Scouts, Promoters, and Connectors: The Roles of Ratings in Nearest Neighbor Collaborative Filtering Bharath Kumar Mohan Dept.",
                "of CSA Indian Institute of Science Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. of Computer Science Eastern Michigan University Ypsilanti, MI 48917, USA bkeller@emich.edu Naren Ramakrishnan Dept. of Computer Science Virginia Tech, Blacksburg VA 24061, USA naren@cs.vt.edu ABSTRACT Recommender systems aggregate individual user ratings into predictions of products or services that might interest visitors.",
                "The quality of this aggregation process crucially affects the user experience and hence the effectiveness of recommenders in e-commerce.",
                "We present a novel study that disaggregates global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearestneighbor <br>collaborative filter</br>ing.",
                "In particular, we formulate three roles-scouts, promoters, and connectors-that capture how users receive recommendations, how items get recommended, and how ratings of these two types are themselves connected (resp.).",
                "These roles find direct uses in improving recommendations for users, in better targeting of items and, most importantly, in helping monitor the health of the system as a whole.",
                "For instance, they can be used to track the evolution of neighborhoods, to identify rating subspaces that do not contribute (or contribute negatively) to system performance, to enumerate users who are in danger of leaving, and to assess the susceptibility of the system to attacks such as shilling.",
                "We argue that the three rating roles presented here provide broad primitives to manage a recommender system and its community.",
                "Categories and Subject Descriptors H.4.2 [Information Systems Applications]: Types of Systems-Decision support; J.4 [Computer Applications]: Social and Behavioral Sciences General Terms Algorithms, Human Factors 1.",
                "INTRODUCTION Recommender systems have become integral to e-commerce, providing technology that suggests products to a visitor based on previous purchases or rating history.",
                "Collaborative filtering, a common form of recommendation, predicts a users rating for an item by combining (other) ratings of that user with other users ratings.",
                "Significant research has been conducted in implementing fast and accurate <br>collaborative filter</br>ing algorithms [2, 7], designing interfaces for presenting recommendations to users [1], and studying the robustness of these algorithms [8].",
                "However, with the exception of a few studies on the influence of users [10], little attention has been paid to unraveling the inner workings of a recommender in terms of the individual ratings and the roles they play in making (good) recommendations.",
                "Such an understanding will give an important handle to monitoring and managing a recommender system, to engineer mechanisms to sustain the recommender, and thereby ensure its continued success.",
                "Our motivation here is to disaggregate global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearest-neighbor <br>collaborative filter</br>ing.",
                "We identify three possible roles: (scouts) to connect the user into the system to receive recommendations, (promoters) to connect an item into the system to be recommended, and (connectors) to connect ratings of these two kinds.",
                "Viewing ratings in this way, we can define the contribution of a rating in each role, both in terms of allowing recommendations to occur, and in terms of influence on the quality of recommendations.",
                "In turn, this capability helps support scenarios such as: 1.",
                "Situating users in better neighborhoods: A users ratings may inadvertently connect the user to a neighborhood for which the users tastes may not be a perfect match.",
                "Identifying ratings responsible for such bad recommendations and suggesting new items to rate can help situate the user in a better neighborhood. 2.",
                "Targeting items: Recommender systems suffer from lack of user participation, especially in cold-start scenarios [13] involving newly arrived items.",
                "Identifying users who can be encouraged to rate specific items helps ensure coverage of the recommender system. 3.",
                "Monitoring the evolution of the recommender system and its stakeholders: A recommender system is constantly under change: growing with new users and 250 items, shrinking with users leaving the system, items becoming irrelevant, and parts of the system under attack.",
                "Tracking the roles of a rating and its evolution over time provides many insights into the health of the system, and how it could be managed and improved.",
                "These include being able to identify rating subspaces that do not contribute (or contribute negatively) to system performance, and could be removed; to enumerate users who are in danger of leaving, or have left the system; and to assess the susceptibility of the system to attacks such as shilling [5].",
                "As we show, the characterization of rating roles presented here provides broad primitives to manage a recommender system and its community.",
                "The rest of the paper is organized as follows.",
                "Background on nearest-neighbor <br>collaborative filter</br>ing and algorithm evaluation is discussed in Section 2.",
                "Section 3 defines and discusses the roles of a rating, and Section 4 defines measures of the contribution of a rating in each of these roles.",
                "In Section 5, we illustrate the use of these roles to address the goals outlined above. 2.",
                "BACKGROUND 2.1 Algorithms Nearest-neighbor <br>collaborative filter</br>ing algorithms either use neighborhoods of users or neighborhoods of items to compute a prediction.",
                "An algorithm of the first kind is called user-based, and one of the second kind is called itembased [12].",
                "In both families of algorithms, neighborhoods are formed by first computing the similarity between all pairs of users (for user-based) or items (for item-based).",
                "Predictions are then computed by aggregating ratings, which in a user-based algorithm involves aggregating the ratings of the target item by the users neighbors and, in an item-based algorithm, involves aggregating the users ratings of items that are neighbors of the target item.",
                "Algorithms within these families differ in the definition of similarity, formation of neighborhoods, and the computation of predictions.",
                "We consider a user-based algorithm based on that defined for GroupLens [11] with variations from Herlocker et al. [2], and an item-based algorithm similar to that of Sarwar et al. [12].",
                "The algorithm used by Resnick et al. [11] defines the similarity of two users u and v as the Pearson correlation of their common ratings: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2 , where Iu is the set of items rated by user u, ru,i is user us rating for item i, and ¯ru is the average rating of user u (similarly for v).",
                "Similarity computed in this manner is typically scaled by a factor proportional to the number of common ratings, to reduce the chance of making a recommendation made on weak connections: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), where γ ≈ 5 is a constant used as a lower limit in scaling [2].",
                "These new similarities are then used to define a static neighborhood Nu for each user u consisting of the top K users most similar to user u.",
                "A prediction for user u and item i is computed by a weighted average of the ratings by the neighbors pu,i = ¯ru + P v∈V sim (u, v)(rv,i − ¯rv) P v∈V sim (u, v) (1) where V = Nu ∩ Ui is the set of users most similar to u who have rated i.",
                "The item-based algorithm we use is the one defined by Sarwar et al. [12].",
                "In this algorithm, similarity is defined as the adjusted cosine measure sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) where Ui is the set of users who have rated item i.",
                "As for the user-based algorithm, the similarity weights are adjusted proportionally to the number of users that have rated the items in common sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). (3) Given the similarities, the neighborhood Ni of an item i is defined as the top K most similar items for that item.",
                "A prediction for user u and item i is computed as the weighted average pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) where J = Ni ∩ Iu is the set of items rated by u that are most similar to i. 2.2 Evaluation Recommender algorithms have typically been evaluated using measures of predictive accuracy and coverage [3].",
                "Studies on recommender algorithms, notably Herlocker et al. [2] and Sarwar et al. [12], typically compute predictive accuracy by dividing a set of ratings into training and test sets, and compute the prediction for an item in the test set using the ratings in the training set.",
                "A standard measure of predictive accuracy is mean absolute error (MAE), which for a test set T = {(u, i)} is defined as, MAE = P (u,i)∈T |pu,i − ru,i| |T | . (5) Coverage has a number of definitions, but generally refers to the proportion of items that can be predicted by the algorithm [3].",
                "A practical issue with predictive accuracy is that users typically are presented with recommendation lists, and not individual numeric predictions.",
                "Recommendation lists are lists of items in decreasing order of prediction (sometimes stated in terms of star-ratings), and so predictive accuracy may not be reflective of the accuracy of the list.",
                "So, instead we can measure recommendation or rank accuracy, which indicates the extent to which the list is in the correct order.",
                "Herlocker et al. [3] discuss a number of rank accuracy measures, which range from Kendalls Tau to measures that consider the fact that users tend to only look at a prefix of the list [5].",
                "Kendalls Tau measures the number of inversions when comparing ordered pairs in the true user ordering of 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figure 1: Ratings in simple movie recommender. items and the recommended order, and is defined as τ = C − D p (C + D + TR)(C + D + TP) (6) where C is the number of pairs that the system predicts in the correct order, D the number of pairs the system predicts in the wrong order, TR the number of pairs in the true ordering that have the same ratings, and TP is the number of pairs in the predicted ordering that have the same ratings [3].",
                "A shortcoming of the Tau metric is that it is oblivious to the position in the ordered list where the inversion occurs [3].",
                "For instance, an inversion toward the end of the list is given the same weight as one in the beginning.",
                "One solution is to consider inversions only in the top few items in the recommended list or to weight inversions based on their position in the list. 3.",
                "ROLES OF A RATING Our basic observation is that each rating plays a different role in each prediction in which it is used.",
                "Consider a simplified movie recommender system with three users Jim, Jeff, and Tom and their ratings for a few movies, as shown in Fig. 1. (For this initial discussion we will not consider the rating values involved.)",
                "The recommender predicts whether Tom will like The Mask using the other already available ratings.",
                "How this is done depends on the algorithm: 1.",
                "An item-based <br>collaborative filter</br>ing algorithm constructs a neighborhood of movies around The Mask by using the ratings of users who rated The Mask and other movies similarly (e.g., Jims ratings of The Matrix and The Mask; and Jeffs ratings of Star Wars and The Mask).",
                "Toms ratings of those movies are then used to make a prediction for The Mask. 2.",
                "A user-based <br>collaborative filter</br>ing algorithm would construct a neighborhood around Tom by tracking other users whose rating behaviors are similar to Toms (e.g., Tom and Jeff have rated Star Wars; Tom and Jim have rated The Matrix).",
                "The prediction of Toms rating for The Mask is then based on the ratings of Jeff and Tim.",
                "Although the nearest-neighbor algorithms aggregate the ratings to form neighborhoods used to compute predictions, we can disaggregate the similarities to view the computation of a prediction as simultaneously following parallel paths of ratings.",
                "So, irrespective of the <br>collaborative filter</br>ing algorithm used, we can visualize the prediction of Toms rating of The Mask as walking through a sequence of ratings.",
                "In Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figure 2: Ratings used to predict The Mask for Tom.",
                "Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figure 3: Prediction of The Mask for Tom in which a rating is used more than once. this example, two paths were used for this prediction as depicted in Fig. 2: (p1, p2, p3) and (q1, q2, q3).",
                "Note that these paths are undirected, and are all of length 3.",
                "Only the order in which the ratings are traversed is different between the item-based algorithm (e.g., (p3, p2, p1), (q3, q2, q1)) and the user-based algorithm (e.g., (p1, p2, p3), (q1, q2, q3).)",
                "A rating can be part of many paths for a single prediction as shown in Fig. 3, where three paths are used for a prediction, two of which follow p1: (p1, p2, p3) and (p1, r2, r3).",
                "Predictions in a <br>collaborative filter</br>ing algorithms may involve thousands of such walks in parallel, each playing a part in influencing the predicted value.",
                "Each prediction path consists of three ratings, playing roles that we call scouts, promoters, and connectors.",
                "To illustrate these roles, consider the path (p1, p2, p3) in Fig. 2 used to make a prediction of The Mask for Tom: 1.",
                "The rating p1 (Tom → Star Wars) makes a connection from Tom to other ratings that can be used to predict Toms rating for The Mask.",
                "This rating serves as a scout in the bipartite graph of ratings to find a path that leads to The Mask. 2.",
                "The rating p2 (Jeff → Star Wars) helps the system recommend The Mask to Tom by connecting the scout to the promoter. 3.",
                "The rating p3 (Jeff → The Mask) allows connections to The Mask, and, therefore, promotes this movie to Tom.",
                "Formally, given a prediction pu,a of a target item a for user u, a scout for pu,a is a rating ru,i such that there exists a user v with ratings rv,a and rv,i for some item i; a promoter for pu,a is a rating rv,a for some user v, such that there exist ratings rv,i and ru,i for an item i, and; a connector for pu,a 252 Jim Tom Jeff Jerry My Cousin Vinny The Matrix Star Wars The Mask Jurasic Park Figure 4: Scouts, promoters, and connectors. is a rating rv,i by some user v and rating i, such that there exists ratings ru,i and rv,a.",
                "The scouts, connectors, and promoters for the prediction of Toms rating of The Mask are p1 and q1, p2 and q2, and p3 and q3 (respectively).",
                "Each of these roles has a value in the recommender to the user, the users neighborhood, and the system in terms of allowing recommendations to be made. 3.1 Roles in Detail Ratings that act as scouts tend to help the recommender system suggest more movies to the user, though the extent to which this is true depends on the rating behavior of other users.",
                "For example, in Fig. 4 the rating Tom → Star Wars helps the system recommend only The Mask to him, while Tom → The Matrix helps recommend The Mask, Jurassic Park, and My Cousin Vinny.",
                "Tom makes a connection to Jim who is a prolific user of the system, by rating The Matrix.",
                "However, this does not make The Matrix the best movie to rate for everyone.",
                "For example, Jim is benefited equally by both The Mask and The Matrix, which allow the system to recommend Star Wars to him.",
                "His rating of The Mask is the best scout for Jeff, and Jerrys only scout is his rating of Star Wars.",
                "This suggests that good scouts allow a user to build similarity with prolific users, and thereby ensure they get more from the system.",
                "While scouts represent beneficial ratings from the perspective of a user, promoters are their duals, and are of benefit to items.",
                "In Fig. 4, My Cousin Vinny benefits from Jims rating, since it allows recommendations to Jeff and Tom.",
                "The Mask is not so dependent on just one rating, since the ratings by Jim and Jeff help it.",
                "On the other hand, Jerrys rating of Star Wars does not help promote it to any other user.",
                "We conclude that a good promoter connects an item to a broader neighborhood of other items, and thereby ensures that it is recommended to more users.",
                "Connectors serve a crucial role in a recommender system that is not as obvious.",
                "The movies My Cousin Vinny and Jurassic Park have the highest recommendation potential since they can be recommended to Jeff, Jerry and Tom based on the linkage structure illustrated in Fig. 4.",
                "Beside the fact that Jim rated these movies, these recommendations are possible only because of the ratings Jim → The Matrix and Jim → The Mask, which are the best connectors.",
                "A connector improves the systems ability to make recommendations with no explicit gain for the user.",
                "Note that every rating can be of varied benefit in each of these roles.",
                "The rating Jim → My Cousin Vinny is a poor scout and connector, but is a very good promoter.",
                "The rating Jim → The Mask is a reasonably good scout, a very good connector, and a good promoter.",
                "Finally, the rating Jerry → Star Wars is a very good scout, but is of no value as a connector or promoter.",
                "As illustrated here, a rating can have different value in each of the three roles in terms of whether a recommendation can be made or not.",
                "We could measure this value by simply counting the number of times a rating is used in each role, which alone would be helpful in characterizing the behavior of a system.",
                "But we can also measure the contribution of each rating to the quality of recommendations or health of the system.",
                "Since every prediction is a combined effort of several recommendation paths, we are interested in discerning the influence of each rating (and, hence, each path) in the system towards the systems overall error.",
                "We can understand the dynamics of the system at a finer granularity by tracking the influence of a rating according to the role played.",
                "The next section describes the approach to measuring the values of a rating in each role. 4.",
                "CONTRIBUTIONS OF RATINGS As weve seen, a rating may play different roles in different predictions and, in each prediction, contribute to the quality of a prediction in different ways.",
                "Our approach can use any numeric measure of a property of system health, and assigns credit (or blame) to each rating proportional to its influence in the prediction.",
                "By tracking the role of each rating in a prediction, we can accumulate the credit for a rating in each of the three roles, and also track the evolution of the roles of rating over time in the system.",
                "This section defines the methodology for computing the contribution of ratings by first defining the influence of a rating, and then instantiating the approach for predictive accuracy, and then rank accuracy.",
                "We also demonstrate how these contributions can be aggregated to study the neighborhood of ratings involved in computing a users recommendations.",
                "Note that although our general formulation for rating influence is algorithm independent, due to space considerations, we present the approach for only item-based <br>collaborative filter</br>ing.",
                "The definition for user-based algorithms is similar and will be presented in an expanded version of this paper. 4.1 Influence of Ratings Recall that an item-based approach to <br>collaborative filter</br>ing relies on building item neighborhoods using the similarity of ratings by the same user.",
                "As described earlier, similarity is defined by the adjusted cosine measure (Equations (2) and (3)).",
                "A set of the top K neighbors is maintained for all items for space and computational efficiency.",
                "A prediction of item i for a user u is computed as the weighted deviation from the items mean rating as shown in Equation (4).",
                "The list of recommendations for a user is then the list of items sorted in descending order of their predicted values.",
                "We first define impact(a, i, j), the impact a user a has in determining the similarity between two items i and j.",
                "This is the change in the similarity between i and j when as rating is removed, and is defined as impact(a, i, j) = |sim (i, j) − sim¯a(i, j)| P w∈Cij |sim (i, j) − sim ¯w(i, j)| where Cij = {u ∈ U | ∃ ru,i, ru,j ∈ R(u)} is the set of coraters 253 of items i and j (users who rate both i and j), R(u) is the set of ratings provided by user u, and sim¯a(i, j) is the similarity of i and j when the ratings of user a are removed sim¯a(i, j) = P v∈U\\{a} (ru,i − ¯ru)(ru,j − ¯ru) qP u∈U\\{a}(ru,i − ¯ru)2 qP u∈U\\{a}(ru,j − ¯ru)2 , and adjusted for the number of raters sim¯a(i, j) = max(|Ui ∩ Uj| − 1, γ) γ · sim(i, j).",
                "If all coraters of i and j rate them identically, we define the impact as impact(a, i, j) = 1 |Cij| since P w∈Cij |sim (i, j) − sim ¯w(i, j)| = 0.",
                "The influence of each path (u, j, v, i) = [ru,j, rv,j, rv,i] in the prediction of pu,i is given by influence(u, j, v, i) = sim (i, j) P l∈Ni∩Iu sim (i, l) · impact(v, i, j) It follows that the sum of influences over all such paths, for a given set of endpoints, is 1. 4.2 Role Values for Predictive Accuracy The value of a rating in each role is computed from the influence depending on the evaluation measure employed.",
                "Here we illustrate the approach using predictive accuracy as the evaluation metric.",
                "In general, the goodness of a prediction decides whether the ratings involved must be credited or discredited for their role.",
                "For predictive accuracy, the error in prediction e = |pu,i − ru,i| is mapped to a comfort level using a mapping function M(e).",
                "Anecdotal evidence suggests that users are unable to discern errors less than 1.0 (for a rating scale of 1 to 5) [4], and so an error less than 1.0 is considered acceptable, but anything larger is not.",
                "We hence define M(e) as (1 − e) binned to an appropriate value in [−1, −0.5, 0.5, 1].",
                "For each prediction pu,i, M(e) is attributed to all the paths that assisted the computation of pu,i, proportional to their influences.",
                "This tribute, M(e)∗influence(u, j, v, i), is in turn inherited by each of the ratings in the path [ru,j, rv,j, rv,i], with the credit/blame accumulating to the respective roles of ru,j as a scout, rv,j as a connector, and rv,i as a promoter.",
                "In other words, the scout value SF(ru,j), the connector value CF(rv,j) and the promoter value PF(rv,i) are all incremented by the tribute amount.",
                "Over a large number of predictions, scouts that have repeatedly resulted in big error rates have a big negative scout value, and vice versa (similarly with the other roles).",
                "Every rating is thus summarized by its triple [SF, CF, PF]. 4.3 Role Values for Rank Accuracy We now define the computation of the contribution of ratings to observed rank accuracy.",
                "For this computation, we must know the users preference order for a set of items for which predictions can be computed.",
                "We assume that we have a test set of the users ratings of the items presented in the recommendation list.",
                "For every pair of items rated by a user in the test data, we check whether the predicted order is concordant with his preference.",
                "We say a pair (i, j) is concordant (with error ) whenever one of the following holds: • if (ru,i < ru,j) then (pu,i − pu,j < ); • if (ru,i > ru,j) then (pu,i − pu,j > ); or • if (ru,i = ru,j) then (|pu,i − pu,j| ≤ ).",
                "Similarly, a pair (i, j) is discordant (with error ) if it is not concordant.",
                "Our experiments described below use an error tolerance of = 0.1.",
                "All paths involved in the prediction of the two items in a concordant pair are credited, and the paths involved in a discordant pair are discredited.",
                "The credit assigned to a pair of items (i, j) in the recommendation list for user u is computed as c(i, j) = ( t T · 1 C+D if (i, j) are concordant − t T · 1 C+D if (i, j) are discordant (7) where t is the number of items in the users test set whose ratings could be predicted, T is the number of items rated by user u in the test set, C is the number of concordances and D is the number of discordances.",
                "The credit c is then divided among all paths responsible for predicting pu,i and pu,j proportional to their influences.",
                "We again add the role values obtained from all the experiments to form a triple [SF, CF, PF] for each rating. 4.4 Aggregating rating roles After calculating the role values for individual ratings, we can also use these values to study neighborhoods and the system.",
                "Here we consider how we can use the role values to characterize the health of a neighborhood.",
                "Consider the list of top recommendations presented to a user at a specific point in time.",
                "The <br>collaborative filter</br>ing algorithm traversed many paths in his neighborhood through his scouts and other connectors and promoters to make these recommendations.",
                "We call these ratings the recommender neighborhood of the user.",
                "The user implicitly chooses this neighborhood of ratings through the items he rates.",
                "Apart from the <br>collaborative filter</br>ing algorithm, the health of this neighborhood completely influences a users satisfaction with the system.",
                "We can characterize a users recommender neighborhood by aggregating the individual role values of the ratings involved, weighted by the influence of individual ratings in determining his recommended list.",
                "Different sections of the users neighborhood wield varied influence on his recommendation list.",
                "For instance, ratings reachable through highly rated items have a bigger say in the recommended items.",
                "Our aim is to study the system and classify users with respect to their positioning in a healthy or unhealthy neighborhood.",
                "A user can have a good set of scouts, but may be exposed to a neighborhood with bad connectors and promoters.",
                "He can have a good neighborhood, but his bad scouts may ensure the neighborhoods potential is rendered useless.",
                "We expect that users with good scouts and good neighborhoods will be most satisfied with the system in the future.",
                "A users neighborhood is characterized by a triple that represents the weighted sum of the role values of individual ratings involved in making recommendations.",
                "Consider a user u and his ordered list of recommendations L. An item i 254 in the list is weighted inversely, as K(i), depending on its position in the list.",
                "In our studies we use K(i) = p position(i).",
                "Several paths of ratings [ru,j, rv,j, rv,i] are involved in predicting pu,i which ultimately decides its position in L, each with influence(u, j, v, i).",
                "The recommender neighborhood of a user u is characterized by the triple, [SFN(u), CFN(u), PFN(u)] where SFN(u) = X i∈L P [ru,j ,rv,j ,rv,i] SF(ru,j)influence(u, j, v, i) K(i) !",
                "CFN(u) and PFN(u) are defined similarly.",
                "This triple estimates the quality of us recommendations based on the past track record of the ratings involved in their respective roles. 5.",
                "EXPERIMENTATION As we have seen, we can assign role values to each rating when evaluating a <br>collaborative filter</br>ing system.",
                "In this section, we demonstrate the use of this approach to our overall goal of defining an approach to monitor and manage the health of a recommender system through experiments done on the MovieLens million rating dataset.",
                "In particular, we discuss results relating to identifying good scouts, promoters, and connectors; the evolution of rating roles; and the characterization of user neighborhoods. 5.1 Methodology Our experiments use the MovieLens million rating dataset, which consists of ratings by 6040 users of 3952 movies.",
                "The ratings are in the range 1 to 5, and are labeled with the time the rating was given.",
                "As discussed before, we consider only the item-based algorithm here (with item neighborhoods of size 30) and, due to space considerations, only present role value results for rank accuracy.",
                "Since we are interested in the evolution of the rating role values over time, the model of the recommender system is built by processing ratings in their arrival order.",
                "The timestamping provided by MovieLens is hence crucial for the analyses presented here.",
                "We make assessments of rating roles at intervals of 10,000 ratings and processed the first 200,000 ratings in the dataset (giving rise to 20 snapshots).",
                "We incrementally update the role values as the time ordered ratings are merged into the model.",
                "To keep the experiment computationally manageable, we define a test dataset for each user.",
                "As the time ordered ratings are merged into the model, we label a small randomly selected percentage (20%) as test data.",
                "At discrete epochs, i.e., after processing every 10,000 ratings, we compute the predictions for the ratings in the test data, and then compute the role values for the ratings used in the predictions.",
                "One potential criticism of this methodology is that the ratings in the test set are never evaluated for their roles.",
                "We overcome this concern by repeating the experiment, using different random seeds.",
                "The probability that every rating is considered for evaluation is then considerably high: 1 − 0.2n , where n is the number of times the experiment is repeated with different random seeds.",
                "The results here are based on n = 4 repetitions.",
                "The item-based <br>collaborative filter</br>ing algorithms performance was ordinary with respect to rank accuracy.",
                "Fig. 5 shows a plot of the precision and recall as ratings were merged in time order into the model.",
                "The recall was always high, but the average precision was just about 53%. 0 0.2 0.4 0.6 0.8 1 1.2 10000 30000 50000 70000 90000110000130000150000 Ratings merged into model Value Precision Recall Figure 5: Precision and recall for the item-based <br>collaborative filter</br>ing algorithm. 5.2 Inducing good scouts The ratings of a user that serve as scouts are those that allow the user to receive recommendations.",
                "We claim that users with ratings that have respectable scout values will be happier with the system than those with ratings with low scout values.",
                "Note that the item-based algorithm discussed here produces recommendation lists with nearly half of the pairs in the list discordant from the users preference.",
                "Whether all of these discordant pairs are observable by the user is unclear, however, certainly this suggests that there is a need to be able to direct users to items whose ratings would improve the lists.",
                "The distribution of the scout values for most users ratings are Gaussian with mean zero.",
                "Fig. 6 shows the frequency distribution of scout values for a sample user at a given snapshot.",
                "We observe that a large number of ratings never serve as scouts for their users.",
                "A relatable scenario is when Amazons recommender makes suggestions of books or items based on other items that were purchased as gifts.",
                "With simple relevance feedback from the user, such ratings can be isolated as bad scouts and discounted from future predictions.",
                "Removing bad scouts was found to be worthwhile for individual users but the overall performance improvement was only marginal.",
                "An obvious question is whether good scouts can be formed by merely rating popular movies as suggested by Rashid et al. [9].",
                "They show that a mix of popularity and rating entropy identifies the best items to suggest to new users when evaluated using MAE.",
                "Following their intuition, we would expect to see a higher correlation between popularityentropy and good scouts.",
                "We measured the Pearson correlation coefficient between aggregated scout values for a movie with the popularity of a movie (number of times it is rated); and with its popularity*variance measure at different snapshots of the system.",
                "Note that the scout values were initially anti-correlated with popularity (Fig. 7), but became moderately correlated as the system evolved.",
                "Both popularity and popularity*variance performed similarly.",
                "A possible explanation is that there has been insufficient time for the popular movies to accumulate ratings. 255 -10 0 10 20 30 40 50 60 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 Scout Value Frequency Figure 6: Distribution of scout values for a sample user. -0.4 -0.2 0 0.2 0.4 0.6 0.8 30000 60000 90000 120000 150000 180000 Popularity Pop*Var Figure 7: Correlation between aggregated scout value and item popularity (computed at different intervals). -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 30000 60000 90000 120000 150000 180000 Figure 8: Correlation between aggregated promoter value and user prolificity (computed at different intervals).",
                "Table 1: Movies forming the best scouts.",
                "Best Scouts Conf.",
                "Pop.",
                "Being John Malkovich (1999) 1.00 445 Star Wars: Episode IV - A New Hope (1977) 0.92 623 Princess Bride, The (1987) 0.85 477 Sixth Sense, The (1999) 0.85 617 Matrix, The (1999) 0.77 522 Ghostbusters (1984) 0.77 441 Casablanca (1942) 0.77 384 Insider, The (1999) 0.77 235 American Beauty (1999) 0.69 624 Terminator 2: Judgment Day (1991) 0.69 503 Fight Club (1999) 0.69 235 Shawshank Redemption, The (1994) 0.69 445 Run Lola Run (Lola rennt) (1998) 0.69 220 Terminator, The (1984) 0.62 450 Usual Suspects, The (1995) 0.62 326 Aliens (1986) 0.62 385 North by Northwest (1959) 0.62 245 Fugitive, The (1993) 0.62 402 End of Days (1999) 0.62 132 Raiders of the Lost Ark (1981) 0.54 540 Schindlers List (1993) 0.54 453 Back to the Future (1985) 0.54 543 Toy Story (1995) 0.54 419 Alien (1979) 0.54 415 Abyss, The (1989) 0.54 345 2001: A Space Odyssey (1968) 0.54 358 Dogma (1999) 0.54 228 Little Mermaid, The (1989) 0.54 203 Table 2: Movies forming the worst scouts.",
                "Worst scouts Conf.",
                "Pop.",
                "Harold and Maude (1971) 0.46 141 Grifters, The (1990) 0.46 180 Sting, The (1973) 0.38 244 Godfather: Part III, The (1990) 0.38 154 Lawrence of Arabia (1962) 0.38 167 High Noon (1952) 0.38 84 Women on the Verge of a... (1988) 0.38 113 Grapes of Wrath, The (1940) 0.38 115 Duck Soup (1933) 0.38 131 Arsenic and Old Lace (1944) 0.38 138 Midnight Cowboy (1969) 0.38 137 To Kill a Mockingbird (1962) 0.31 195 Four Weddings and a Funeral (1994) 0.31 271 Good, The Bad and The Ugly, The (1966) 0.31 156 Its a Wonderful Life (1946) 0.31 146 Player, The (1992) 0.31 220 Jackie Brown (1997) 0.31 118 Boat, The (Das Boot) (1981) 0.31 210 Manhattan (1979) 0.31 158 Truth About Cats & Dogs, The (1996) 0.31 143 Ghost (1990) 0.31 227 Lone Star (1996) 0.31 125 Big Chill, The (1983) 0.31 184 256 By studying the evolution of scout values, we can identify movies that consistently feature in good scouts over time.",
                "We claim these movies will make viable scouts for other users.",
                "We found the aggregated scout values for all movies in intervals of 10,000 ratings each.",
                "A movie is said to induce a good scout if the movie was in the top 100 of the sorted list, and to induce a bad scout if it was in bottom 100 of the same list.",
                "Movies appearing consistently high over time are expected to remain up there in the future.",
                "The effective confidence in a movie m is Cm = Tm − Bm N (8) where Tm is the number of times it appeared in the top 100, Bm the number of times it appeared in the bottom 100, and N is the number of intervals considered.",
                "Using this measure, the top few movies expected to induce the best scouts are shown in Table 1.",
                "Movies that would be bad scout choices are shown in Table 2 with their associated confidences.",
                "The popularities of the movies are also displayed.",
                "Although more popular movies appear in the list of good scouts, these tables show that a blind choice of scout based on popularity alone can be potentially dangerous.",
                "Interestingly, the best scout-Being John Malkovich-is about a puppeteer who discovers a portal into a movie star, a movie that has been described variously on amazon.com as makes you feel giddy, seriously weird, comedy with depth, silly, strange, and inventive.",
                "Indicating whether someone likes this movie or not goes a long way toward situating the user in a suitable neighborhood, with similar preferences.",
                "On the other hand, several factors may have made a movie a bad scout, like the sharp variance in user preferences in the neighborhood of a movie.",
                "Two users may have the same opinion about Lawrence of Arabia, but they may differ sharply about how they felt about the other movies they saw.",
                "Bad scouts ensue when there is deviation in behavior around a common synchronization point. 5.3 Inducing good promoters Ratings that serve to promote items in a <br>collaborative filter</br>ing system are critical to allowing a new item be recommended to users.",
                "So, inducing good promoters is important for cold-start recommendation.",
                "We note that the frequency distribution of promoter values for a sample movies ratings is also Gaussian (similar to Fig. 6).",
                "This indicates that the promotion of a movie is benefited most by the ratings of a few users, and are unaffected by the ratings of most users.",
                "We find a strong correlation between a users number of ratings and his aggregated promoter value.",
                "Fig. 8 depicts the evolution of the Pearson correlation co-efficient between the prolificity of a user (number of ratings) versus his aggregated promoter value.",
                "We expect that conspicuous shills, by recommending wrong movies to users, will be discredited with negative aggregate promoter values and should be identifiable easily.",
                "Given this observation, the obvious rule to follow when introducing a new movie is to have it rated directly by prolific users who posses high aggregated promoter values.",
                "A new movie is thus cast into the neighborhood of many other movies improving its visibility.",
                "Note, though, that a user may have long stopped using the system.",
                "Tracking promoter values consistently allows only the most active recent users to be considered. 5.4 Inducing good connectors Given the way scouts, connectors, and promoters are characterized, it follows that the movies that are part of the best scouts are also part of the best connectors.",
                "Similarly, the users that constitute the best promoters are also part of the best connectors.",
                "Good connectors are induced by ensuring a user with a high promoter value rates a movie with a high scout value.",
                "In our experiments, we find that a ratings longest standing role is often as a connector.",
                "A rating with a poor connector value is often seen due to its user being a bad promoter, or its movie being a bad scout.",
                "Such ratings can be removed from the prediction process to bring marginal improvements to recommendations.",
                "In some selected experiments, we observed that removing a set of badly behaving connectors helped improve the systems overall performance by 1.5%.",
                "The effect was even higher on a few select users who observed an improvement of above 10% in precision without much loss in recall. 5.5 Monitoring the evolution of rating roles One of the more significant contributions of our work is the ability to model the evolution of recommender systems, by studying the changing roles of ratings over time.",
                "The role and value of a rating can change depending on many factors like user behavior, redundancy, shilling effects or properties of the <br>collaborative filter</br>ing algorithm used.",
                "Studying the dynamics of rating roles in terms of transitions between good, bad, and negligible values can provide insights into the functioning of the recommender system.",
                "We believe that a continuous visualization of these transitions will improve the ability to manage a recommender system.",
                "We classify different rating states as good, bad, or negligible.",
                "Consider a user who has rated 100 movies in a particular interval, of which 20 are part of the test set.",
                "If a scout has a value greater than 0.005, it indicates that it is uniquely involved in at least 2 concordant predictions, which we will say is good.",
                "Thus, a threshold of 0.005 is chosen to bin a rating as good, bad or negligible in terms of its scout, connector and promoter value.",
                "For instance, a rating r, at time t with role value triple [0.1, 0.001, −0.01] is classified as [scout +, connector 0, promoter −], where + indicates good, 0 indicates negligible, and − indicates bad.",
                "The positive credit held by a rating is a measure of its contribution to the betterment of the system, and the discredit is a measure of its contribution to the detriment of the system.",
                "Even though the positive roles (and the negative roles) make up a very small percentage of all ratings, their contribution supersedes their size.",
                "For example, even though only 1.7% of all ratings were classified as good scouts, they hold 79% of all positive credit in the system!",
                "Similarly, the bad scouts were just 1.4% of all ratings but hold 82% of all discredit.",
                "Note that good and bad scouts, together, comprise only 1.4% + 1.7% = 3.1% of the ratings, implying that the majority of the ratings are negligible role players as scouts (more on this later).",
                "Likewise, good connectors were 1.2% of the system, and hold 30% of all positive credit.",
                "The bad connectors (0.8% of the system) hold 36% of all discredit.",
                "Good promoters (3% of the system) hold 46% of all credit, while bad promoters (2%) hold 50% of all discredit.",
                "This reiterates that a few ratings influence most of the systems performance.",
                "Hence it is important to track transitions between them regardless of their small numbers. 257 Across different snapshots, a rating can remain in the same state or change.",
                "A good scout can become a bad scout, a good promoter can become a good connector, good and bad scouts can become vestigial, and so on.",
                "It is not practical to expect a recommender system to have no ratings in bad roles.",
                "However, it suffices to see ratings in bad roles either convert to good or vestigial roles.",
                "Similarly, observing a large number of good roles become bad ones is a sign of imminent failure of the system.",
                "We employ the principle of non-overlapping episodes [6] to count such transitions.",
                "A sequence such as: [+, 0, 0] → [+, 0, 0] → [0, +, 0] → [0, 0, 0] is interpreted as the transitions [+, 0, 0] ; [0, +, 0] : 1 [+, 0, 0] ; [0, 0, 0] : 1 [0, +, 0] ; [0, 0, 0] : 1 instead of [+, 0, 0] ; [0, +, 0] : 2 [+, 0, 0] ; [0, 0, 0] : 2 [0, +, 0] ; [0, 0, 0] : 1.",
                "See [6] for further details about this counting procedure.",
                "Thus, a rating can be in one of 27 possible states, and there are about 272 possible transitions.",
                "We make a further simplification and utilize only 9 states, indicating whether the rating is a scout, promoter, or connector, and whether it has a positive, negative, or negligible role.",
                "Ratings that serve multiple purposes are counted using multiple episode instantiations but the states themselves are not duplicated beyond the 9 restricted states.",
                "In this model, a transition such as [+, 0, +] ; [0, +, 0] : 1 is counted as [scout+] ; [scout0] : 1 [scout+] ; [connector+] : 1 [scout+] ; [promoter0] : 1 [connector0] ; [scout0] : 1 [connector0] ; [scout+] : 1 [connector0] ; [promoter0] : 1 [promoter+] ; [scout0] : 1 [promoter+] ; [connector+] : 1 [promoter+] ; [promoter0] : 1 Of these, transitions like [pX] ; [q0] where p = q, X ∈ {+, 0, −} are considered uninteresting, and only the rest are counted.",
                "Fig. 9 depicts the major transitions counted while processing the first 200,000 ratings from the MovieLens dataset.",
                "Only transitions with frequency greater than or equal to 3% are shown.",
                "The percentages for each state indicates the number of ratings that were found to be in those states.",
                "We consider transitions from any state to a good state as healthy, from any state to a bad state as unhealthy, and from any state to a vestigial state as decaying.",
                "From Fig. 9, we can observe: • The bulk of the ratings have negligible values, irrespective of their role.",
                "The majority of the transitions involve both good and bad ratings becoming negligible.",
                "Scout + (2%) Scout(1.5%) Scout 0 (96.5%) Connector + (1.2%) Connector(0.8%) Connector 0 (98%) Promoter + (3%) Promoter(2%) Promoter 0 (95%) 84% 84% 81% 74% 10% 6% 11% 77% 8% 7% 8% 82% 4% 86% 4% 68% 15% 13% 5% 5% 77% 11% 7% 5% 4% 3% 3% 3% Healthy Unhealthy Decaying Figure 9: Transitions among rating roles. • The number of good ratings is comparable to the bad ratings, and ratings are seen to switch states often, except in the case of scouts (see below). • The negative and positive scout states are not reachable through any transition, indicating that these ratings must begin as such, and cannot be coerced into these roles. • Good promoters and good connectors have a much longer survival period than scouts.",
                "Transitions that recur to these states have frequencies of 10% and 15% when compared to just 4% for scouts.",
                "Good connectors are the slowest to decay whereas (good) scouts decay the fastest. • Healthy percentages are seen on transitions between promoters and connectors.",
                "As indicated earlier, there are hardly any transitions from promoters/connectors to scouts.",
                "This indicates that, over the long run, a users rating is more useful to others (movies or other users) than to the user himself. • The percentages of healthy transitions outweigh the unhealthy ones - this hints that the system is healthy, albeit only marginally.",
                "Note that these results are conditioned by the static nature of the dataset, which is a set of ratings over a fixed window of time.",
                "However a diagram such as Fig. 9 is clearly useful for monitoring the health of a recommender system.",
                "For instance, acceptable limits can be imposed on different types of transitions and, if a transition fails to meet the threshold, the recommender system or a part of it can be brought under closer scrutiny.",
                "Furthermore, the role state transition diagram would also be the ideal place to study the effects of shilling, a topic we will consider in future research. 5.6 Characterizing neighborhoods Earlier we saw that we can characterize the neighborhood of ratings involved in creating a recommendation list L for 258 a user.",
                "In our experiment, we consider lists of length 30, and sample the lists of about 5% of users through the evolution of the model (at intervals of 10,000 ratings each) and compute their neighborhood characteristics.",
                "To simplify our presentation, we consider the percentage of the sample that fall into one of the following categories: 1.",
                "Inactive user: (SFN(u) = 0) 2.",
                "Good scouts, Good neighborhood: (SFN(u) > 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 3.",
                "Good scouts, Bad neighborhood: (SFN(u) > 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) 4.",
                "Bad scouts, Good neighborhood: (SFN(u) < 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 5.",
                "Bad scouts, Bad neighborhood: (SFN(u) < 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) From our sample set of 561 users, we found that 476 users were inactive.",
                "Of the remaining 85 users, we found 26 users had good scouts and a good neighborhood, 6 had bad scouts and a good neighborhood, 29 had good scouts and a bad neighborhood, and 24 had bad scouts and a bad neighborhood.",
                "Thus, we conjecture that 59 users (29+24+6) are in danger of leaving the system.",
                "As a remedy, users with bad scouts and a good neighborhood can be asked to reconsider rating of some movies hoping to improve the systems recommendations.",
                "The system can be expected to deliver more if they engineer some good scouts.",
                "Users with good scouts and a bad neighborhood are harder to address; this situation might entail selectively removing some connector-promoter pairs that are causing the damage.",
                "Handling users with bad scouts and bad neighborhoods is a more difficult challenge.",
                "Such a classification allows the use of different strategies to better a users experience with the system depending on his context.",
                "In future work, we intend to conduct field studies and study the improvement in performance of different strategies for different contexts. 6.",
                "CONCLUSIONS To further recommender system acceptance and deployment, we require new tools and methodologies to manage an installed recommender and develop insights into the roles played by ratings.",
                "A fine-grained characterization in terms of rating roles such as scouts, promoters, and connectors, as done here, helps such an endeavor.",
                "Although we have presented results on only the item-based algorithm with list rank accuracy as the metric, the same approach outlined here applies to user-based algorithms and other metrics.",
                "In future research, we plan to systematically study the many algorithmic parameters, tolerances, and cutoff thresholds employed here and reason about their effects on the downstream conclusions.",
                "We also aim to extend our formulation to other <br>collaborative filter</br>ing algorithms, study the effect of shilling in altering rating roles, conduct field studies, and evaluate improvements in user experience by tweaking ratings based on their role values.",
                "Finally, we plan to develop the idea of mining the evolution of rating role patterns into a reporting and tracking system for all aspects of recommender system health. 7.",
                "REFERENCES [1] Cosley, D., Lam, S., Albert, I., Konstan, J., and Riedl, J.",
                "Is Seeing Believing? : How Recommender System Interfaces Affect Users Opinions.",
                "In Proc.",
                "CHI (2001), pp. 585-592. [2] Herlocker, J. L., Konstan, J.",
                "A., Borchers, A., and Riedl, J.",
                "An Algorithmic Framework for Performing Collaborative Filtering.",
                "In Proc.",
                "SIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J.",
                "A., Terveen, L. G., and Riedl, J. T. Evaluating Collaborative Filtering Recommender Systems.",
                "ACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J.",
                "A.",
                "Personal communication. 2003. [5] Lam, S. K., and Riedl, J. Shilling Recommender Systems for Fun and Profit.",
                "In Proceedings of the 13th International World Wide Web Conference (2004), ACM Press, pp. 393-402. [6] Laxman, S., Sastry, P. S., and Unnikrishnan, K. P. Discovering Frequent Episodes and Learning Hidden Markov Models: A Formal Connection.",
                "IEEE Transactions on Knowledge and Data Engineering Vol. 17, 11 (2005), 1505-1517. [7] McLaughlin, M. R., and Herlocker, J. L. A Collaborative Filtering Algorithm and Evaluation Metric that Accurately Model the User Experience.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (2004), pp. 329 - 336. [8] OMahony, M., Hurley, N. J., Kushmerick, N., and Silvestre, G. Collaborative Recommendation: A Robustness Analysis.",
                "ACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J.",
                "A., and Riedl, J.",
                "Getting to Know You: Learning New User Preferences in Recommender Systems.",
                "In Proceedings of the 2002 Conference on Intelligent User Interfaces (IUI 2002) (2002), pp. 127-134. [10] Rashid, A. M., Karypis, G., and Riedl, J.",
                "Influence in Ratings-Based Recommender Systems: An Algorithm-Independent Approach.",
                "In Proc. of the SIAM International Conference on Data Mining (2005). [11] Resnick, P., Iacovou, N., Sushak, M., Bergstrom, P., and Riedl, J. GroupLens: An Open Architecture for Collaborative Filtering of Netnews.",
                "In Proceedings of the Conference on Computer Supported Collaborative Work (CSCW94) (1994), ACM Press, pp. 175-186. [12] Sarwar, B., Karypis, G., Konstan, J., and Reidl, J. Item-Based Collaborative Filtering Recommendation Algorithms.",
                "In Proceedings of the Tenth International World Wide Web Conference (WWW10) (2001), pp. 285-295. [13] Schein, A., Popescu, A., Ungar, L., and Pennock, D. Methods and Metrics for Cold-Start Recommendation.",
                "In Proc.",
                "SIGIR (2002), pp. 253-260. 259"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Presentamos un estudio novedoso que desagrega las métricas de rendimiento de recomendaciones globales en las contribuciones hechas por cada calificación individual, lo que nos permite caracterizar los muchos roles desempeñados por las calificaciones en el \"filtro colaborativo\" más cercano \"Filtro colaborativo\".",
                "Se han realizado una investigación significativa en la implementación de algoritmos de \"filtro colaborativo\" rápido y preciso [2, 7], diseñando interfaces para presentar recomendaciones a los usuarios [1] y estudiar la robustez de estos algoritmos [8].",
                "Nuestra motivación aquí es desglosar las métricas de rendimiento de recomendaciones globales en las contribuciones hechas por cada calificación individual, lo que nos permite caracterizar los muchos roles desempeñados por las calificaciones en el \"filtro colaborativo\" de vecino más cercano.",
                "Antecedentes sobre el \"filtro colaborativo\" de vecino más cercano y la evaluación de algoritmo se discute en la Sección 2.",
                "Antecedentes 2.1 Algoritmos El \"filtro colaborativo\" de vecino más cercano \"Filtro colaborativo\" de los algoritmos usan vecindarios de usuarios o vecindarios de elementos para calcular una predicción.",
                "Un algoritmo de \"filtro colaborativo\" basado en elementos construye un vecindario de películas alrededor de la máscara utilizando las clasificaciones de usuarios que calificaron la máscara y otras películas de manera similar (por ejemplo, las clasificaciones de Jims de la matriz y la máscara; y las clasificaciones de Jeffs de Star Warsy la máscara).",
                "Un algoritmo de \"filtro colaborativo\" basado en el usuario construiría un vecindario alrededor de Tom al rastrear a otros usuarios cuyos comportamientos de calificación son similares a Toms (por ejemplo, Tom y Jeff han calificado Star Wars; Tom y Jim han calificado la matriz).",
                "Por lo tanto, independientemente del algoritmo de \"filtro colaborativo\" utilizado, podemos visualizar la predicción de la clasificación TOMS de la máscara como caminar a través de una secuencia de clasificaciones.",
                "Las predicciones en un \"filtro colaborativo\" de algoritmos de ing \"pueden involucrar miles de tales caminatas en paralelo, cada una jugando un papel en la influencia del valor predicho.",
                "Tenga en cuenta que aunque nuestra formulación general para la influencia de calificación es independiente del algoritmo, debido a consideraciones de espacio, presentamos el enfoque solo para el \"filtro colaborativo\" basado en elementos."
            ],
            "translated_text": "",
            "candidates": [
                "filtro colaborativo",
                "filtro colaborativo",
                "Filtro colaborativo",
                "filtro colaborativo",
                "filtro colaborativo",
                "filtro colaborativo",
                "filtro colaborativo",
                "Filtro colaborativo",
                "filtro colaborativo",
                "filtro colaborativo",
                "filtro colaborativo",
                "Filtro colaborativo",
                "filtro colaborativo",
                "filtro colaborativo",
                "filtro colaborativo",
                "filtro colaborativo",
                "filtro colaborativo",
                "filtro colaborativo",
                "filtro colaborativo",
                "filtro colaborativo",
                "filtro colaborativo",
                "filtro colaborativo"
            ],
            "error": []
        },
        "user-base and item-base algorithm": {
            "translated_key": "Algoritmo de base de usuarios y elementos",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Scouts, Promoters, and Connectors: The Roles of Ratings in Nearest Neighbor Collaborative Filtering Bharath Kumar Mohan Dept.",
                "of CSA Indian Institute of Science Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. of Computer Science Eastern Michigan University Ypsilanti, MI 48917, USA bkeller@emich.edu Naren Ramakrishnan Dept. of Computer Science Virginia Tech, Blacksburg VA 24061, USA naren@cs.vt.edu ABSTRACT Recommender systems aggregate individual user ratings into predictions of products or services that might interest visitors.",
                "The quality of this aggregation process crucially affects the user experience and hence the effectiveness of recommenders in e-commerce.",
                "We present a novel study that disaggregates global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearestneighbor collaborative filtering.",
                "In particular, we formulate three roles-scouts, promoters, and connectors-that capture how users receive recommendations, how items get recommended, and how ratings of these two types are themselves connected (resp.).",
                "These roles find direct uses in improving recommendations for users, in better targeting of items and, most importantly, in helping monitor the health of the system as a whole.",
                "For instance, they can be used to track the evolution of neighborhoods, to identify rating subspaces that do not contribute (or contribute negatively) to system performance, to enumerate users who are in danger of leaving, and to assess the susceptibility of the system to attacks such as shilling.",
                "We argue that the three rating roles presented here provide broad primitives to manage a recommender system and its community.",
                "Categories and Subject Descriptors H.4.2 [Information Systems Applications]: Types of Systems-Decision support; J.4 [Computer Applications]: Social and Behavioral Sciences General Terms Algorithms, Human Factors 1.",
                "INTRODUCTION Recommender systems have become integral to e-commerce, providing technology that suggests products to a visitor based on previous purchases or rating history.",
                "Collaborative filtering, a common form of recommendation, predicts a users rating for an item by combining (other) ratings of that user with other users ratings.",
                "Significant research has been conducted in implementing fast and accurate collaborative filtering algorithms [2, 7], designing interfaces for presenting recommendations to users [1], and studying the robustness of these algorithms [8].",
                "However, with the exception of a few studies on the influence of users [10], little attention has been paid to unraveling the inner workings of a recommender in terms of the individual ratings and the roles they play in making (good) recommendations.",
                "Such an understanding will give an important handle to monitoring and managing a recommender system, to engineer mechanisms to sustain the recommender, and thereby ensure its continued success.",
                "Our motivation here is to disaggregate global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearest-neighbor collaborative filtering.",
                "We identify three possible roles: (scouts) to connect the user into the system to receive recommendations, (promoters) to connect an item into the system to be recommended, and (connectors) to connect ratings of these two kinds.",
                "Viewing ratings in this way, we can define the contribution of a rating in each role, both in terms of allowing recommendations to occur, and in terms of influence on the quality of recommendations.",
                "In turn, this capability helps support scenarios such as: 1.",
                "Situating users in better neighborhoods: A users ratings may inadvertently connect the user to a neighborhood for which the users tastes may not be a perfect match.",
                "Identifying ratings responsible for such bad recommendations and suggesting new items to rate can help situate the user in a better neighborhood. 2.",
                "Targeting items: Recommender systems suffer from lack of user participation, especially in cold-start scenarios [13] involving newly arrived items.",
                "Identifying users who can be encouraged to rate specific items helps ensure coverage of the recommender system. 3.",
                "Monitoring the evolution of the recommender system and its stakeholders: A recommender system is constantly under change: growing with new users and 250 items, shrinking with users leaving the system, items becoming irrelevant, and parts of the system under attack.",
                "Tracking the roles of a rating and its evolution over time provides many insights into the health of the system, and how it could be managed and improved.",
                "These include being able to identify rating subspaces that do not contribute (or contribute negatively) to system performance, and could be removed; to enumerate users who are in danger of leaving, or have left the system; and to assess the susceptibility of the system to attacks such as shilling [5].",
                "As we show, the characterization of rating roles presented here provides broad primitives to manage a recommender system and its community.",
                "The rest of the paper is organized as follows.",
                "Background on nearest-neighbor collaborative filtering and algorithm evaluation is discussed in Section 2.",
                "Section 3 defines and discusses the roles of a rating, and Section 4 defines measures of the contribution of a rating in each of these roles.",
                "In Section 5, we illustrate the use of these roles to address the goals outlined above. 2.",
                "BACKGROUND 2.1 Algorithms Nearest-neighbor collaborative filtering algorithms either use neighborhoods of users or neighborhoods of items to compute a prediction.",
                "An algorithm of the first kind is called user-based, and one of the second kind is called itembased [12].",
                "In both families of algorithms, neighborhoods are formed by first computing the similarity between all pairs of users (for user-based) or items (for item-based).",
                "Predictions are then computed by aggregating ratings, which in a user-based algorithm involves aggregating the ratings of the target item by the users neighbors and, in an item-based algorithm, involves aggregating the users ratings of items that are neighbors of the target item.",
                "Algorithms within these families differ in the definition of similarity, formation of neighborhoods, and the computation of predictions.",
                "We consider a user-based algorithm based on that defined for GroupLens [11] with variations from Herlocker et al. [2], and an item-based algorithm similar to that of Sarwar et al. [12].",
                "The algorithm used by Resnick et al. [11] defines the similarity of two users u and v as the Pearson correlation of their common ratings: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2 , where Iu is the set of items rated by user u, ru,i is user us rating for item i, and ¯ru is the average rating of user u (similarly for v).",
                "Similarity computed in this manner is typically scaled by a factor proportional to the number of common ratings, to reduce the chance of making a recommendation made on weak connections: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), where γ ≈ 5 is a constant used as a lower limit in scaling [2].",
                "These new similarities are then used to define a static neighborhood Nu for each user u consisting of the top K users most similar to user u.",
                "A prediction for user u and item i is computed by a weighted average of the ratings by the neighbors pu,i = ¯ru + P v∈V sim (u, v)(rv,i − ¯rv) P v∈V sim (u, v) (1) where V = Nu ∩ Ui is the set of users most similar to u who have rated i.",
                "The item-based algorithm we use is the one defined by Sarwar et al. [12].",
                "In this algorithm, similarity is defined as the adjusted cosine measure sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) where Ui is the set of users who have rated item i.",
                "As for the user-based algorithm, the similarity weights are adjusted proportionally to the number of users that have rated the items in common sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). (3) Given the similarities, the neighborhood Ni of an item i is defined as the top K most similar items for that item.",
                "A prediction for user u and item i is computed as the weighted average pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) where J = Ni ∩ Iu is the set of items rated by u that are most similar to i. 2.2 Evaluation Recommender algorithms have typically been evaluated using measures of predictive accuracy and coverage [3].",
                "Studies on recommender algorithms, notably Herlocker et al. [2] and Sarwar et al. [12], typically compute predictive accuracy by dividing a set of ratings into training and test sets, and compute the prediction for an item in the test set using the ratings in the training set.",
                "A standard measure of predictive accuracy is mean absolute error (MAE), which for a test set T = {(u, i)} is defined as, MAE = P (u,i)∈T |pu,i − ru,i| |T | . (5) Coverage has a number of definitions, but generally refers to the proportion of items that can be predicted by the algorithm [3].",
                "A practical issue with predictive accuracy is that users typically are presented with recommendation lists, and not individual numeric predictions.",
                "Recommendation lists are lists of items in decreasing order of prediction (sometimes stated in terms of star-ratings), and so predictive accuracy may not be reflective of the accuracy of the list.",
                "So, instead we can measure recommendation or rank accuracy, which indicates the extent to which the list is in the correct order.",
                "Herlocker et al. [3] discuss a number of rank accuracy measures, which range from Kendalls Tau to measures that consider the fact that users tend to only look at a prefix of the list [5].",
                "Kendalls Tau measures the number of inversions when comparing ordered pairs in the true user ordering of 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figure 1: Ratings in simple movie recommender. items and the recommended order, and is defined as τ = C − D p (C + D + TR)(C + D + TP) (6) where C is the number of pairs that the system predicts in the correct order, D the number of pairs the system predicts in the wrong order, TR the number of pairs in the true ordering that have the same ratings, and TP is the number of pairs in the predicted ordering that have the same ratings [3].",
                "A shortcoming of the Tau metric is that it is oblivious to the position in the ordered list where the inversion occurs [3].",
                "For instance, an inversion toward the end of the list is given the same weight as one in the beginning.",
                "One solution is to consider inversions only in the top few items in the recommended list or to weight inversions based on their position in the list. 3.",
                "ROLES OF A RATING Our basic observation is that each rating plays a different role in each prediction in which it is used.",
                "Consider a simplified movie recommender system with three users Jim, Jeff, and Tom and their ratings for a few movies, as shown in Fig. 1. (For this initial discussion we will not consider the rating values involved.)",
                "The recommender predicts whether Tom will like The Mask using the other already available ratings.",
                "How this is done depends on the algorithm: 1.",
                "An item-based collaborative filtering algorithm constructs a neighborhood of movies around The Mask by using the ratings of users who rated The Mask and other movies similarly (e.g., Jims ratings of The Matrix and The Mask; and Jeffs ratings of Star Wars and The Mask).",
                "Toms ratings of those movies are then used to make a prediction for The Mask. 2.",
                "A user-based collaborative filtering algorithm would construct a neighborhood around Tom by tracking other users whose rating behaviors are similar to Toms (e.g., Tom and Jeff have rated Star Wars; Tom and Jim have rated The Matrix).",
                "The prediction of Toms rating for The Mask is then based on the ratings of Jeff and Tim.",
                "Although the nearest-neighbor algorithms aggregate the ratings to form neighborhoods used to compute predictions, we can disaggregate the similarities to view the computation of a prediction as simultaneously following parallel paths of ratings.",
                "So, irrespective of the collaborative filtering algorithm used, we can visualize the prediction of Toms rating of The Mask as walking through a sequence of ratings.",
                "In Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figure 2: Ratings used to predict The Mask for Tom.",
                "Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figure 3: Prediction of The Mask for Tom in which a rating is used more than once. this example, two paths were used for this prediction as depicted in Fig. 2: (p1, p2, p3) and (q1, q2, q3).",
                "Note that these paths are undirected, and are all of length 3.",
                "Only the order in which the ratings are traversed is different between the item-based algorithm (e.g., (p3, p2, p1), (q3, q2, q1)) and the user-based algorithm (e.g., (p1, p2, p3), (q1, q2, q3).)",
                "A rating can be part of many paths for a single prediction as shown in Fig. 3, where three paths are used for a prediction, two of which follow p1: (p1, p2, p3) and (p1, r2, r3).",
                "Predictions in a collaborative filtering algorithms may involve thousands of such walks in parallel, each playing a part in influencing the predicted value.",
                "Each prediction path consists of three ratings, playing roles that we call scouts, promoters, and connectors.",
                "To illustrate these roles, consider the path (p1, p2, p3) in Fig. 2 used to make a prediction of The Mask for Tom: 1.",
                "The rating p1 (Tom → Star Wars) makes a connection from Tom to other ratings that can be used to predict Toms rating for The Mask.",
                "This rating serves as a scout in the bipartite graph of ratings to find a path that leads to The Mask. 2.",
                "The rating p2 (Jeff → Star Wars) helps the system recommend The Mask to Tom by connecting the scout to the promoter. 3.",
                "The rating p3 (Jeff → The Mask) allows connections to The Mask, and, therefore, promotes this movie to Tom.",
                "Formally, given a prediction pu,a of a target item a for user u, a scout for pu,a is a rating ru,i such that there exists a user v with ratings rv,a and rv,i for some item i; a promoter for pu,a is a rating rv,a for some user v, such that there exist ratings rv,i and ru,i for an item i, and; a connector for pu,a 252 Jim Tom Jeff Jerry My Cousin Vinny The Matrix Star Wars The Mask Jurasic Park Figure 4: Scouts, promoters, and connectors. is a rating rv,i by some user v and rating i, such that there exists ratings ru,i and rv,a.",
                "The scouts, connectors, and promoters for the prediction of Toms rating of The Mask are p1 and q1, p2 and q2, and p3 and q3 (respectively).",
                "Each of these roles has a value in the recommender to the user, the users neighborhood, and the system in terms of allowing recommendations to be made. 3.1 Roles in Detail Ratings that act as scouts tend to help the recommender system suggest more movies to the user, though the extent to which this is true depends on the rating behavior of other users.",
                "For example, in Fig. 4 the rating Tom → Star Wars helps the system recommend only The Mask to him, while Tom → The Matrix helps recommend The Mask, Jurassic Park, and My Cousin Vinny.",
                "Tom makes a connection to Jim who is a prolific user of the system, by rating The Matrix.",
                "However, this does not make The Matrix the best movie to rate for everyone.",
                "For example, Jim is benefited equally by both The Mask and The Matrix, which allow the system to recommend Star Wars to him.",
                "His rating of The Mask is the best scout for Jeff, and Jerrys only scout is his rating of Star Wars.",
                "This suggests that good scouts allow a user to build similarity with prolific users, and thereby ensure they get more from the system.",
                "While scouts represent beneficial ratings from the perspective of a user, promoters are their duals, and are of benefit to items.",
                "In Fig. 4, My Cousin Vinny benefits from Jims rating, since it allows recommendations to Jeff and Tom.",
                "The Mask is not so dependent on just one rating, since the ratings by Jim and Jeff help it.",
                "On the other hand, Jerrys rating of Star Wars does not help promote it to any other user.",
                "We conclude that a good promoter connects an item to a broader neighborhood of other items, and thereby ensures that it is recommended to more users.",
                "Connectors serve a crucial role in a recommender system that is not as obvious.",
                "The movies My Cousin Vinny and Jurassic Park have the highest recommendation potential since they can be recommended to Jeff, Jerry and Tom based on the linkage structure illustrated in Fig. 4.",
                "Beside the fact that Jim rated these movies, these recommendations are possible only because of the ratings Jim → The Matrix and Jim → The Mask, which are the best connectors.",
                "A connector improves the systems ability to make recommendations with no explicit gain for the user.",
                "Note that every rating can be of varied benefit in each of these roles.",
                "The rating Jim → My Cousin Vinny is a poor scout and connector, but is a very good promoter.",
                "The rating Jim → The Mask is a reasonably good scout, a very good connector, and a good promoter.",
                "Finally, the rating Jerry → Star Wars is a very good scout, but is of no value as a connector or promoter.",
                "As illustrated here, a rating can have different value in each of the three roles in terms of whether a recommendation can be made or not.",
                "We could measure this value by simply counting the number of times a rating is used in each role, which alone would be helpful in characterizing the behavior of a system.",
                "But we can also measure the contribution of each rating to the quality of recommendations or health of the system.",
                "Since every prediction is a combined effort of several recommendation paths, we are interested in discerning the influence of each rating (and, hence, each path) in the system towards the systems overall error.",
                "We can understand the dynamics of the system at a finer granularity by tracking the influence of a rating according to the role played.",
                "The next section describes the approach to measuring the values of a rating in each role. 4.",
                "CONTRIBUTIONS OF RATINGS As weve seen, a rating may play different roles in different predictions and, in each prediction, contribute to the quality of a prediction in different ways.",
                "Our approach can use any numeric measure of a property of system health, and assigns credit (or blame) to each rating proportional to its influence in the prediction.",
                "By tracking the role of each rating in a prediction, we can accumulate the credit for a rating in each of the three roles, and also track the evolution of the roles of rating over time in the system.",
                "This section defines the methodology for computing the contribution of ratings by first defining the influence of a rating, and then instantiating the approach for predictive accuracy, and then rank accuracy.",
                "We also demonstrate how these contributions can be aggregated to study the neighborhood of ratings involved in computing a users recommendations.",
                "Note that although our general formulation for rating influence is algorithm independent, due to space considerations, we present the approach for only item-based collaborative filtering.",
                "The definition for user-based algorithms is similar and will be presented in an expanded version of this paper. 4.1 Influence of Ratings Recall that an item-based approach to collaborative filtering relies on building item neighborhoods using the similarity of ratings by the same user.",
                "As described earlier, similarity is defined by the adjusted cosine measure (Equations (2) and (3)).",
                "A set of the top K neighbors is maintained for all items for space and computational efficiency.",
                "A prediction of item i for a user u is computed as the weighted deviation from the items mean rating as shown in Equation (4).",
                "The list of recommendations for a user is then the list of items sorted in descending order of their predicted values.",
                "We first define impact(a, i, j), the impact a user a has in determining the similarity between two items i and j.",
                "This is the change in the similarity between i and j when as rating is removed, and is defined as impact(a, i, j) = |sim (i, j) − sim¯a(i, j)| P w∈Cij |sim (i, j) − sim ¯w(i, j)| where Cij = {u ∈ U | ∃ ru,i, ru,j ∈ R(u)} is the set of coraters 253 of items i and j (users who rate both i and j), R(u) is the set of ratings provided by user u, and sim¯a(i, j) is the similarity of i and j when the ratings of user a are removed sim¯a(i, j) = P v∈U\\{a} (ru,i − ¯ru)(ru,j − ¯ru) qP u∈U\\{a}(ru,i − ¯ru)2 qP u∈U\\{a}(ru,j − ¯ru)2 , and adjusted for the number of raters sim¯a(i, j) = max(|Ui ∩ Uj| − 1, γ) γ · sim(i, j).",
                "If all coraters of i and j rate them identically, we define the impact as impact(a, i, j) = 1 |Cij| since P w∈Cij |sim (i, j) − sim ¯w(i, j)| = 0.",
                "The influence of each path (u, j, v, i) = [ru,j, rv,j, rv,i] in the prediction of pu,i is given by influence(u, j, v, i) = sim (i, j) P l∈Ni∩Iu sim (i, l) · impact(v, i, j) It follows that the sum of influences over all such paths, for a given set of endpoints, is 1. 4.2 Role Values for Predictive Accuracy The value of a rating in each role is computed from the influence depending on the evaluation measure employed.",
                "Here we illustrate the approach using predictive accuracy as the evaluation metric.",
                "In general, the goodness of a prediction decides whether the ratings involved must be credited or discredited for their role.",
                "For predictive accuracy, the error in prediction e = |pu,i − ru,i| is mapped to a comfort level using a mapping function M(e).",
                "Anecdotal evidence suggests that users are unable to discern errors less than 1.0 (for a rating scale of 1 to 5) [4], and so an error less than 1.0 is considered acceptable, but anything larger is not.",
                "We hence define M(e) as (1 − e) binned to an appropriate value in [−1, −0.5, 0.5, 1].",
                "For each prediction pu,i, M(e) is attributed to all the paths that assisted the computation of pu,i, proportional to their influences.",
                "This tribute, M(e)∗influence(u, j, v, i), is in turn inherited by each of the ratings in the path [ru,j, rv,j, rv,i], with the credit/blame accumulating to the respective roles of ru,j as a scout, rv,j as a connector, and rv,i as a promoter.",
                "In other words, the scout value SF(ru,j), the connector value CF(rv,j) and the promoter value PF(rv,i) are all incremented by the tribute amount.",
                "Over a large number of predictions, scouts that have repeatedly resulted in big error rates have a big negative scout value, and vice versa (similarly with the other roles).",
                "Every rating is thus summarized by its triple [SF, CF, PF]. 4.3 Role Values for Rank Accuracy We now define the computation of the contribution of ratings to observed rank accuracy.",
                "For this computation, we must know the users preference order for a set of items for which predictions can be computed.",
                "We assume that we have a test set of the users ratings of the items presented in the recommendation list.",
                "For every pair of items rated by a user in the test data, we check whether the predicted order is concordant with his preference.",
                "We say a pair (i, j) is concordant (with error ) whenever one of the following holds: • if (ru,i < ru,j) then (pu,i − pu,j < ); • if (ru,i > ru,j) then (pu,i − pu,j > ); or • if (ru,i = ru,j) then (|pu,i − pu,j| ≤ ).",
                "Similarly, a pair (i, j) is discordant (with error ) if it is not concordant.",
                "Our experiments described below use an error tolerance of = 0.1.",
                "All paths involved in the prediction of the two items in a concordant pair are credited, and the paths involved in a discordant pair are discredited.",
                "The credit assigned to a pair of items (i, j) in the recommendation list for user u is computed as c(i, j) = ( t T · 1 C+D if (i, j) are concordant − t T · 1 C+D if (i, j) are discordant (7) where t is the number of items in the users test set whose ratings could be predicted, T is the number of items rated by user u in the test set, C is the number of concordances and D is the number of discordances.",
                "The credit c is then divided among all paths responsible for predicting pu,i and pu,j proportional to their influences.",
                "We again add the role values obtained from all the experiments to form a triple [SF, CF, PF] for each rating. 4.4 Aggregating rating roles After calculating the role values for individual ratings, we can also use these values to study neighborhoods and the system.",
                "Here we consider how we can use the role values to characterize the health of a neighborhood.",
                "Consider the list of top recommendations presented to a user at a specific point in time.",
                "The collaborative filtering algorithm traversed many paths in his neighborhood through his scouts and other connectors and promoters to make these recommendations.",
                "We call these ratings the recommender neighborhood of the user.",
                "The user implicitly chooses this neighborhood of ratings through the items he rates.",
                "Apart from the collaborative filtering algorithm, the health of this neighborhood completely influences a users satisfaction with the system.",
                "We can characterize a users recommender neighborhood by aggregating the individual role values of the ratings involved, weighted by the influence of individual ratings in determining his recommended list.",
                "Different sections of the users neighborhood wield varied influence on his recommendation list.",
                "For instance, ratings reachable through highly rated items have a bigger say in the recommended items.",
                "Our aim is to study the system and classify users with respect to their positioning in a healthy or unhealthy neighborhood.",
                "A user can have a good set of scouts, but may be exposed to a neighborhood with bad connectors and promoters.",
                "He can have a good neighborhood, but his bad scouts may ensure the neighborhoods potential is rendered useless.",
                "We expect that users with good scouts and good neighborhoods will be most satisfied with the system in the future.",
                "A users neighborhood is characterized by a triple that represents the weighted sum of the role values of individual ratings involved in making recommendations.",
                "Consider a user u and his ordered list of recommendations L. An item i 254 in the list is weighted inversely, as K(i), depending on its position in the list.",
                "In our studies we use K(i) = p position(i).",
                "Several paths of ratings [ru,j, rv,j, rv,i] are involved in predicting pu,i which ultimately decides its position in L, each with influence(u, j, v, i).",
                "The recommender neighborhood of a user u is characterized by the triple, [SFN(u), CFN(u), PFN(u)] where SFN(u) = X i∈L P [ru,j ,rv,j ,rv,i] SF(ru,j)influence(u, j, v, i) K(i) !",
                "CFN(u) and PFN(u) are defined similarly.",
                "This triple estimates the quality of us recommendations based on the past track record of the ratings involved in their respective roles. 5.",
                "EXPERIMENTATION As we have seen, we can assign role values to each rating when evaluating a collaborative filtering system.",
                "In this section, we demonstrate the use of this approach to our overall goal of defining an approach to monitor and manage the health of a recommender system through experiments done on the MovieLens million rating dataset.",
                "In particular, we discuss results relating to identifying good scouts, promoters, and connectors; the evolution of rating roles; and the characterization of user neighborhoods. 5.1 Methodology Our experiments use the MovieLens million rating dataset, which consists of ratings by 6040 users of 3952 movies.",
                "The ratings are in the range 1 to 5, and are labeled with the time the rating was given.",
                "As discussed before, we consider only the item-based algorithm here (with item neighborhoods of size 30) and, due to space considerations, only present role value results for rank accuracy.",
                "Since we are interested in the evolution of the rating role values over time, the model of the recommender system is built by processing ratings in their arrival order.",
                "The timestamping provided by MovieLens is hence crucial for the analyses presented here.",
                "We make assessments of rating roles at intervals of 10,000 ratings and processed the first 200,000 ratings in the dataset (giving rise to 20 snapshots).",
                "We incrementally update the role values as the time ordered ratings are merged into the model.",
                "To keep the experiment computationally manageable, we define a test dataset for each user.",
                "As the time ordered ratings are merged into the model, we label a small randomly selected percentage (20%) as test data.",
                "At discrete epochs, i.e., after processing every 10,000 ratings, we compute the predictions for the ratings in the test data, and then compute the role values for the ratings used in the predictions.",
                "One potential criticism of this methodology is that the ratings in the test set are never evaluated for their roles.",
                "We overcome this concern by repeating the experiment, using different random seeds.",
                "The probability that every rating is considered for evaluation is then considerably high: 1 − 0.2n , where n is the number of times the experiment is repeated with different random seeds.",
                "The results here are based on n = 4 repetitions.",
                "The item-based collaborative filtering algorithms performance was ordinary with respect to rank accuracy.",
                "Fig. 5 shows a plot of the precision and recall as ratings were merged in time order into the model.",
                "The recall was always high, but the average precision was just about 53%. 0 0.2 0.4 0.6 0.8 1 1.2 10000 30000 50000 70000 90000110000130000150000 Ratings merged into model Value Precision Recall Figure 5: Precision and recall for the item-based collaborative filtering algorithm. 5.2 Inducing good scouts The ratings of a user that serve as scouts are those that allow the user to receive recommendations.",
                "We claim that users with ratings that have respectable scout values will be happier with the system than those with ratings with low scout values.",
                "Note that the item-based algorithm discussed here produces recommendation lists with nearly half of the pairs in the list discordant from the users preference.",
                "Whether all of these discordant pairs are observable by the user is unclear, however, certainly this suggests that there is a need to be able to direct users to items whose ratings would improve the lists.",
                "The distribution of the scout values for most users ratings are Gaussian with mean zero.",
                "Fig. 6 shows the frequency distribution of scout values for a sample user at a given snapshot.",
                "We observe that a large number of ratings never serve as scouts for their users.",
                "A relatable scenario is when Amazons recommender makes suggestions of books or items based on other items that were purchased as gifts.",
                "With simple relevance feedback from the user, such ratings can be isolated as bad scouts and discounted from future predictions.",
                "Removing bad scouts was found to be worthwhile for individual users but the overall performance improvement was only marginal.",
                "An obvious question is whether good scouts can be formed by merely rating popular movies as suggested by Rashid et al. [9].",
                "They show that a mix of popularity and rating entropy identifies the best items to suggest to new users when evaluated using MAE.",
                "Following their intuition, we would expect to see a higher correlation between popularityentropy and good scouts.",
                "We measured the Pearson correlation coefficient between aggregated scout values for a movie with the popularity of a movie (number of times it is rated); and with its popularity*variance measure at different snapshots of the system.",
                "Note that the scout values were initially anti-correlated with popularity (Fig. 7), but became moderately correlated as the system evolved.",
                "Both popularity and popularity*variance performed similarly.",
                "A possible explanation is that there has been insufficient time for the popular movies to accumulate ratings. 255 -10 0 10 20 30 40 50 60 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 Scout Value Frequency Figure 6: Distribution of scout values for a sample user. -0.4 -0.2 0 0.2 0.4 0.6 0.8 30000 60000 90000 120000 150000 180000 Popularity Pop*Var Figure 7: Correlation between aggregated scout value and item popularity (computed at different intervals). -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 30000 60000 90000 120000 150000 180000 Figure 8: Correlation between aggregated promoter value and user prolificity (computed at different intervals).",
                "Table 1: Movies forming the best scouts.",
                "Best Scouts Conf.",
                "Pop.",
                "Being John Malkovich (1999) 1.00 445 Star Wars: Episode IV - A New Hope (1977) 0.92 623 Princess Bride, The (1987) 0.85 477 Sixth Sense, The (1999) 0.85 617 Matrix, The (1999) 0.77 522 Ghostbusters (1984) 0.77 441 Casablanca (1942) 0.77 384 Insider, The (1999) 0.77 235 American Beauty (1999) 0.69 624 Terminator 2: Judgment Day (1991) 0.69 503 Fight Club (1999) 0.69 235 Shawshank Redemption, The (1994) 0.69 445 Run Lola Run (Lola rennt) (1998) 0.69 220 Terminator, The (1984) 0.62 450 Usual Suspects, The (1995) 0.62 326 Aliens (1986) 0.62 385 North by Northwest (1959) 0.62 245 Fugitive, The (1993) 0.62 402 End of Days (1999) 0.62 132 Raiders of the Lost Ark (1981) 0.54 540 Schindlers List (1993) 0.54 453 Back to the Future (1985) 0.54 543 Toy Story (1995) 0.54 419 Alien (1979) 0.54 415 Abyss, The (1989) 0.54 345 2001: A Space Odyssey (1968) 0.54 358 Dogma (1999) 0.54 228 Little Mermaid, The (1989) 0.54 203 Table 2: Movies forming the worst scouts.",
                "Worst scouts Conf.",
                "Pop.",
                "Harold and Maude (1971) 0.46 141 Grifters, The (1990) 0.46 180 Sting, The (1973) 0.38 244 Godfather: Part III, The (1990) 0.38 154 Lawrence of Arabia (1962) 0.38 167 High Noon (1952) 0.38 84 Women on the Verge of a... (1988) 0.38 113 Grapes of Wrath, The (1940) 0.38 115 Duck Soup (1933) 0.38 131 Arsenic and Old Lace (1944) 0.38 138 Midnight Cowboy (1969) 0.38 137 To Kill a Mockingbird (1962) 0.31 195 Four Weddings and a Funeral (1994) 0.31 271 Good, The Bad and The Ugly, The (1966) 0.31 156 Its a Wonderful Life (1946) 0.31 146 Player, The (1992) 0.31 220 Jackie Brown (1997) 0.31 118 Boat, The (Das Boot) (1981) 0.31 210 Manhattan (1979) 0.31 158 Truth About Cats & Dogs, The (1996) 0.31 143 Ghost (1990) 0.31 227 Lone Star (1996) 0.31 125 Big Chill, The (1983) 0.31 184 256 By studying the evolution of scout values, we can identify movies that consistently feature in good scouts over time.",
                "We claim these movies will make viable scouts for other users.",
                "We found the aggregated scout values for all movies in intervals of 10,000 ratings each.",
                "A movie is said to induce a good scout if the movie was in the top 100 of the sorted list, and to induce a bad scout if it was in bottom 100 of the same list.",
                "Movies appearing consistently high over time are expected to remain up there in the future.",
                "The effective confidence in a movie m is Cm = Tm − Bm N (8) where Tm is the number of times it appeared in the top 100, Bm the number of times it appeared in the bottom 100, and N is the number of intervals considered.",
                "Using this measure, the top few movies expected to induce the best scouts are shown in Table 1.",
                "Movies that would be bad scout choices are shown in Table 2 with their associated confidences.",
                "The popularities of the movies are also displayed.",
                "Although more popular movies appear in the list of good scouts, these tables show that a blind choice of scout based on popularity alone can be potentially dangerous.",
                "Interestingly, the best scout-Being John Malkovich-is about a puppeteer who discovers a portal into a movie star, a movie that has been described variously on amazon.com as makes you feel giddy, seriously weird, comedy with depth, silly, strange, and inventive.",
                "Indicating whether someone likes this movie or not goes a long way toward situating the user in a suitable neighborhood, with similar preferences.",
                "On the other hand, several factors may have made a movie a bad scout, like the sharp variance in user preferences in the neighborhood of a movie.",
                "Two users may have the same opinion about Lawrence of Arabia, but they may differ sharply about how they felt about the other movies they saw.",
                "Bad scouts ensue when there is deviation in behavior around a common synchronization point. 5.3 Inducing good promoters Ratings that serve to promote items in a collaborative filtering system are critical to allowing a new item be recommended to users.",
                "So, inducing good promoters is important for cold-start recommendation.",
                "We note that the frequency distribution of promoter values for a sample movies ratings is also Gaussian (similar to Fig. 6).",
                "This indicates that the promotion of a movie is benefited most by the ratings of a few users, and are unaffected by the ratings of most users.",
                "We find a strong correlation between a users number of ratings and his aggregated promoter value.",
                "Fig. 8 depicts the evolution of the Pearson correlation co-efficient between the prolificity of a user (number of ratings) versus his aggregated promoter value.",
                "We expect that conspicuous shills, by recommending wrong movies to users, will be discredited with negative aggregate promoter values and should be identifiable easily.",
                "Given this observation, the obvious rule to follow when introducing a new movie is to have it rated directly by prolific users who posses high aggregated promoter values.",
                "A new movie is thus cast into the neighborhood of many other movies improving its visibility.",
                "Note, though, that a user may have long stopped using the system.",
                "Tracking promoter values consistently allows only the most active recent users to be considered. 5.4 Inducing good connectors Given the way scouts, connectors, and promoters are characterized, it follows that the movies that are part of the best scouts are also part of the best connectors.",
                "Similarly, the users that constitute the best promoters are also part of the best connectors.",
                "Good connectors are induced by ensuring a user with a high promoter value rates a movie with a high scout value.",
                "In our experiments, we find that a ratings longest standing role is often as a connector.",
                "A rating with a poor connector value is often seen due to its user being a bad promoter, or its movie being a bad scout.",
                "Such ratings can be removed from the prediction process to bring marginal improvements to recommendations.",
                "In some selected experiments, we observed that removing a set of badly behaving connectors helped improve the systems overall performance by 1.5%.",
                "The effect was even higher on a few select users who observed an improvement of above 10% in precision without much loss in recall. 5.5 Monitoring the evolution of rating roles One of the more significant contributions of our work is the ability to model the evolution of recommender systems, by studying the changing roles of ratings over time.",
                "The role and value of a rating can change depending on many factors like user behavior, redundancy, shilling effects or properties of the collaborative filtering algorithm used.",
                "Studying the dynamics of rating roles in terms of transitions between good, bad, and negligible values can provide insights into the functioning of the recommender system.",
                "We believe that a continuous visualization of these transitions will improve the ability to manage a recommender system.",
                "We classify different rating states as good, bad, or negligible.",
                "Consider a user who has rated 100 movies in a particular interval, of which 20 are part of the test set.",
                "If a scout has a value greater than 0.005, it indicates that it is uniquely involved in at least 2 concordant predictions, which we will say is good.",
                "Thus, a threshold of 0.005 is chosen to bin a rating as good, bad or negligible in terms of its scout, connector and promoter value.",
                "For instance, a rating r, at time t with role value triple [0.1, 0.001, −0.01] is classified as [scout +, connector 0, promoter −], where + indicates good, 0 indicates negligible, and − indicates bad.",
                "The positive credit held by a rating is a measure of its contribution to the betterment of the system, and the discredit is a measure of its contribution to the detriment of the system.",
                "Even though the positive roles (and the negative roles) make up a very small percentage of all ratings, their contribution supersedes their size.",
                "For example, even though only 1.7% of all ratings were classified as good scouts, they hold 79% of all positive credit in the system!",
                "Similarly, the bad scouts were just 1.4% of all ratings but hold 82% of all discredit.",
                "Note that good and bad scouts, together, comprise only 1.4% + 1.7% = 3.1% of the ratings, implying that the majority of the ratings are negligible role players as scouts (more on this later).",
                "Likewise, good connectors were 1.2% of the system, and hold 30% of all positive credit.",
                "The bad connectors (0.8% of the system) hold 36% of all discredit.",
                "Good promoters (3% of the system) hold 46% of all credit, while bad promoters (2%) hold 50% of all discredit.",
                "This reiterates that a few ratings influence most of the systems performance.",
                "Hence it is important to track transitions between them regardless of their small numbers. 257 Across different snapshots, a rating can remain in the same state or change.",
                "A good scout can become a bad scout, a good promoter can become a good connector, good and bad scouts can become vestigial, and so on.",
                "It is not practical to expect a recommender system to have no ratings in bad roles.",
                "However, it suffices to see ratings in bad roles either convert to good or vestigial roles.",
                "Similarly, observing a large number of good roles become bad ones is a sign of imminent failure of the system.",
                "We employ the principle of non-overlapping episodes [6] to count such transitions.",
                "A sequence such as: [+, 0, 0] → [+, 0, 0] → [0, +, 0] → [0, 0, 0] is interpreted as the transitions [+, 0, 0] ; [0, +, 0] : 1 [+, 0, 0] ; [0, 0, 0] : 1 [0, +, 0] ; [0, 0, 0] : 1 instead of [+, 0, 0] ; [0, +, 0] : 2 [+, 0, 0] ; [0, 0, 0] : 2 [0, +, 0] ; [0, 0, 0] : 1.",
                "See [6] for further details about this counting procedure.",
                "Thus, a rating can be in one of 27 possible states, and there are about 272 possible transitions.",
                "We make a further simplification and utilize only 9 states, indicating whether the rating is a scout, promoter, or connector, and whether it has a positive, negative, or negligible role.",
                "Ratings that serve multiple purposes are counted using multiple episode instantiations but the states themselves are not duplicated beyond the 9 restricted states.",
                "In this model, a transition such as [+, 0, +] ; [0, +, 0] : 1 is counted as [scout+] ; [scout0] : 1 [scout+] ; [connector+] : 1 [scout+] ; [promoter0] : 1 [connector0] ; [scout0] : 1 [connector0] ; [scout+] : 1 [connector0] ; [promoter0] : 1 [promoter+] ; [scout0] : 1 [promoter+] ; [connector+] : 1 [promoter+] ; [promoter0] : 1 Of these, transitions like [pX] ; [q0] where p = q, X ∈ {+, 0, −} are considered uninteresting, and only the rest are counted.",
                "Fig. 9 depicts the major transitions counted while processing the first 200,000 ratings from the MovieLens dataset.",
                "Only transitions with frequency greater than or equal to 3% are shown.",
                "The percentages for each state indicates the number of ratings that were found to be in those states.",
                "We consider transitions from any state to a good state as healthy, from any state to a bad state as unhealthy, and from any state to a vestigial state as decaying.",
                "From Fig. 9, we can observe: • The bulk of the ratings have negligible values, irrespective of their role.",
                "The majority of the transitions involve both good and bad ratings becoming negligible.",
                "Scout + (2%) Scout(1.5%) Scout 0 (96.5%) Connector + (1.2%) Connector(0.8%) Connector 0 (98%) Promoter + (3%) Promoter(2%) Promoter 0 (95%) 84% 84% 81% 74% 10% 6% 11% 77% 8% 7% 8% 82% 4% 86% 4% 68% 15% 13% 5% 5% 77% 11% 7% 5% 4% 3% 3% 3% Healthy Unhealthy Decaying Figure 9: Transitions among rating roles. • The number of good ratings is comparable to the bad ratings, and ratings are seen to switch states often, except in the case of scouts (see below). • The negative and positive scout states are not reachable through any transition, indicating that these ratings must begin as such, and cannot be coerced into these roles. • Good promoters and good connectors have a much longer survival period than scouts.",
                "Transitions that recur to these states have frequencies of 10% and 15% when compared to just 4% for scouts.",
                "Good connectors are the slowest to decay whereas (good) scouts decay the fastest. • Healthy percentages are seen on transitions between promoters and connectors.",
                "As indicated earlier, there are hardly any transitions from promoters/connectors to scouts.",
                "This indicates that, over the long run, a users rating is more useful to others (movies or other users) than to the user himself. • The percentages of healthy transitions outweigh the unhealthy ones - this hints that the system is healthy, albeit only marginally.",
                "Note that these results are conditioned by the static nature of the dataset, which is a set of ratings over a fixed window of time.",
                "However a diagram such as Fig. 9 is clearly useful for monitoring the health of a recommender system.",
                "For instance, acceptable limits can be imposed on different types of transitions and, if a transition fails to meet the threshold, the recommender system or a part of it can be brought under closer scrutiny.",
                "Furthermore, the role state transition diagram would also be the ideal place to study the effects of shilling, a topic we will consider in future research. 5.6 Characterizing neighborhoods Earlier we saw that we can characterize the neighborhood of ratings involved in creating a recommendation list L for 258 a user.",
                "In our experiment, we consider lists of length 30, and sample the lists of about 5% of users through the evolution of the model (at intervals of 10,000 ratings each) and compute their neighborhood characteristics.",
                "To simplify our presentation, we consider the percentage of the sample that fall into one of the following categories: 1.",
                "Inactive user: (SFN(u) = 0) 2.",
                "Good scouts, Good neighborhood: (SFN(u) > 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 3.",
                "Good scouts, Bad neighborhood: (SFN(u) > 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) 4.",
                "Bad scouts, Good neighborhood: (SFN(u) < 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 5.",
                "Bad scouts, Bad neighborhood: (SFN(u) < 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) From our sample set of 561 users, we found that 476 users were inactive.",
                "Of the remaining 85 users, we found 26 users had good scouts and a good neighborhood, 6 had bad scouts and a good neighborhood, 29 had good scouts and a bad neighborhood, and 24 had bad scouts and a bad neighborhood.",
                "Thus, we conjecture that 59 users (29+24+6) are in danger of leaving the system.",
                "As a remedy, users with bad scouts and a good neighborhood can be asked to reconsider rating of some movies hoping to improve the systems recommendations.",
                "The system can be expected to deliver more if they engineer some good scouts.",
                "Users with good scouts and a bad neighborhood are harder to address; this situation might entail selectively removing some connector-promoter pairs that are causing the damage.",
                "Handling users with bad scouts and bad neighborhoods is a more difficult challenge.",
                "Such a classification allows the use of different strategies to better a users experience with the system depending on his context.",
                "In future work, we intend to conduct field studies and study the improvement in performance of different strategies for different contexts. 6.",
                "CONCLUSIONS To further recommender system acceptance and deployment, we require new tools and methodologies to manage an installed recommender and develop insights into the roles played by ratings.",
                "A fine-grained characterization in terms of rating roles such as scouts, promoters, and connectors, as done here, helps such an endeavor.",
                "Although we have presented results on only the item-based algorithm with list rank accuracy as the metric, the same approach outlined here applies to user-based algorithms and other metrics.",
                "In future research, we plan to systematically study the many algorithmic parameters, tolerances, and cutoff thresholds employed here and reason about their effects on the downstream conclusions.",
                "We also aim to extend our formulation to other collaborative filtering algorithms, study the effect of shilling in altering rating roles, conduct field studies, and evaluate improvements in user experience by tweaking ratings based on their role values.",
                "Finally, we plan to develop the idea of mining the evolution of rating role patterns into a reporting and tracking system for all aspects of recommender system health. 7.",
                "REFERENCES [1] Cosley, D., Lam, S., Albert, I., Konstan, J., and Riedl, J.",
                "Is Seeing Believing? : How Recommender System Interfaces Affect Users Opinions.",
                "In Proc.",
                "CHI (2001), pp. 585-592. [2] Herlocker, J. L., Konstan, J.",
                "A., Borchers, A., and Riedl, J.",
                "An Algorithmic Framework for Performing Collaborative Filtering.",
                "In Proc.",
                "SIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J.",
                "A., Terveen, L. G., and Riedl, J. T. Evaluating Collaborative Filtering Recommender Systems.",
                "ACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J.",
                "A.",
                "Personal communication. 2003. [5] Lam, S. K., and Riedl, J. Shilling Recommender Systems for Fun and Profit.",
                "In Proceedings of the 13th International World Wide Web Conference (2004), ACM Press, pp. 393-402. [6] Laxman, S., Sastry, P. S., and Unnikrishnan, K. P. Discovering Frequent Episodes and Learning Hidden Markov Models: A Formal Connection.",
                "IEEE Transactions on Knowledge and Data Engineering Vol. 17, 11 (2005), 1505-1517. [7] McLaughlin, M. R., and Herlocker, J. L. A Collaborative Filtering Algorithm and Evaluation Metric that Accurately Model the User Experience.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (2004), pp. 329 - 336. [8] OMahony, M., Hurley, N. J., Kushmerick, N., and Silvestre, G. Collaborative Recommendation: A Robustness Analysis.",
                "ACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J.",
                "A., and Riedl, J.",
                "Getting to Know You: Learning New User Preferences in Recommender Systems.",
                "In Proceedings of the 2002 Conference on Intelligent User Interfaces (IUI 2002) (2002), pp. 127-134. [10] Rashid, A. M., Karypis, G., and Riedl, J.",
                "Influence in Ratings-Based Recommender Systems: An Algorithm-Independent Approach.",
                "In Proc. of the SIAM International Conference on Data Mining (2005). [11] Resnick, P., Iacovou, N., Sushak, M., Bergstrom, P., and Riedl, J. GroupLens: An Open Architecture for Collaborative Filtering of Netnews.",
                "In Proceedings of the Conference on Computer Supported Collaborative Work (CSCW94) (1994), ACM Press, pp. 175-186. [12] Sarwar, B., Karypis, G., Konstan, J., and Reidl, J. Item-Based Collaborative Filtering Recommendation Algorithms.",
                "In Proceedings of the Tenth International World Wide Web Conference (WWW10) (2001), pp. 285-295. [13] Schein, A., Popescu, A., Ungar, L., and Pennock, D. Methods and Metrics for Cold-Start Recommendation.",
                "In Proc.",
                "SIGIR (2002), pp. 253-260. 259"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        }
    }
}