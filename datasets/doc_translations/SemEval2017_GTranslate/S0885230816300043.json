{
    "original_text": "The final set of experiments involved an adaptive retraining of the GMM–HMM parameters following the aNAT procedure. This new model only provided an improvement of 0.3%, similar to using the aCMLLR transforms on the baseline GMM–HMM model. However, training show-based aCMLLR transforms on top of the adaptively trained model boosted the improvement to 0.8% absolute. This showed how adaptive training provided a better flexibility of the model to adapt to specific background conditions existing in each show. Finally, the factorisation approach using MLLR speaker transforms on top of the aNAT model and show-based aCMLLR transforms was tested. This only increased the improvement to 0.9% absolute (2.9% relative), which reflects the difficulty of performing accurate speaker clustering in this task and how this actually hampers speaker adaptation.",
    "original_translation": "El conjunto final de experimentos implicó un reentrenamiento adaptativo de los parámetros GMM -HMM después del procedimiento ANAT.Este nuevo modelo solo proporcionó una mejora de 0.3%, similar a el uso de las transformaciones ACMLLR en el modelo de base GMM -HMM.Sin embargo, el entrenamiento de ACMLLR basado en Show se transforma además del modelo entrenado adaptativamente aumentó la mejora al 0.8% absoluto.Esto mostró cómo la capacitación adaptativa proporcionó una mejor flexibilidad del modelo para adaptarse a las condiciones de fondo específicas existentes en cada programa.Finalmente, se probó el enfoque de factorización utilizando transformaciones de altavoz MLLR en la parte superior del modelo ANAT y las transformaciones de ACMLLR basadas en Show.Esto solo aumentó la mejora al 0.9% absoluto (2.9% relativo), lo que refleja la dificultad de realizar una agrupación precisa de los altavoces en esta tarea y cómo esto realmente obstaculiza la adaptación del altavoz.",
    "error_count": 1,
    "keys": {
        "aCMLLR transforms": {
            "translated_key": [
                "transformaciones ACMLLR",
                "ACMLLR Transforms",
                "además del modelo entrenado adaptativamente aumentó la mejora al 0.8% absoluto.Esto mostró cómo la capacitación adaptativa proporcionó una mejor flexibilidad del modelo para adaptarse a las condiciones de fondo específicas existentes en cada programa.Finalmente, se probó el enfoque de factorización utilizando transformaciones de altavoz MLLR en la parte superior del modelo ANAT y las "
            ],
            "translated_annotated_text": "El conjunto final de experimentos implicó un reentrenamiento adaptativo de los parámetros GMM -HMM después del procedimiento ANAT.Este nuevo modelo solo proporcionó una mejora de 0.3%, similar al uso de las \"transformaciones ACMLLR\" en el modelo de base GMM -HMM.Sin embargo, la capacitación de \"ACMLLR Transforms\" basadas en Show \"además del modelo entrenado adaptativamente aumentó la mejora al 0.8% absoluto.Esto mostró cómo la capacitación adaptativa proporcionó una mejor flexibilidad del modelo para adaptarse a las condiciones de fondo específicas existentes en cada programa.Finalmente, se probó el enfoque de factorización utilizando transformaciones de altavoz MLLR en la parte superior del modelo ANAT y las \"transformaciones ACMLLR\" basadas en el espectáculo.Esto solo aumentó la mejora al 0.9% absoluto (2.9% relativo), lo que refleja la dificultad de realizar una agrupación precisa de los altavoces en esta tarea y cómo esto realmente obstaculiza la adaptación del altavoz.",
            "error": [
                "transformaciones ACMLLR",
                "ACMLLR Transforms",
                "además del modelo entrenado adaptativamente aumentó la mejora al 0.8% absoluto.Esto mostró cómo la capacitación adaptativa proporcionó una mejor flexibilidad del modelo para adaptarse a las condiciones de fondo específicas existentes en cada programa.Finalmente, se probó el enfoque de factorización utilizando transformaciones de altavoz MLLR en la parte superior del modelo ANAT y las "
            ]
        },
        "adaptive retraining of the GMM–HMM parameters": {
            "translated_key": "reentrenamiento adaptativo de los parámetros GMM -HMM",
            "translated_annotated_text": "El conjunto final de experimentos implicó un \"reentrenamiento adaptativo de los parámetros GMM -HMM\" siguiendo el procedimiento ANAT.Este nuevo modelo solo proporcionó una mejora de 0.3%, similar a el uso de las transformaciones ACMLLR en el modelo de base GMM -HMM.Sin embargo, el entrenamiento de ACMLLR basado en Show se transforma además del modelo entrenado adaptativamente aumentó la mejora al 0.8% absoluto.Esto mostró cómo la capacitación adaptativa proporcionó una mejor flexibilidad del modelo para adaptarse a las condiciones de fondo específicas existentes en cada programa.Finalmente, se probó el enfoque de factorización utilizando transformaciones de altavoz MLLR en la parte superior del modelo ANAT y las transformaciones de ACMLLR basadas en Show.Esto solo aumentó la mejora al 0.9% absoluto (2.9% relativo), lo que refleja la dificultad de realizar una agrupación precisa de los altavoces en esta tarea y cómo esto realmente obstaculiza la adaptación del altavoz.",
            "error": [
                ""
            ]
        },
        "adaptive training": {
            "translated_key": "capacitación adaptativa",
            "translated_annotated_text": "El conjunto final de experimentos implicó un reentrenamiento adaptativo de los parámetros GMM -HMM después del procedimiento ANAT.Este nuevo modelo solo proporcionó una mejora de 0.3%, similar a el uso de las transformaciones ACMLLR en el modelo de base GMM -HMM.Sin embargo, el entrenamiento de ACMLLR basado en Show se transforma además del modelo entrenado adaptativamente aumentó la mejora al 0.8% absoluto.Esto mostró cómo la \"capacitación adaptativa\" proporcionó una mejor flexibilidad del modelo para adaptarse a las condiciones de fondo específicas existentes en cada programa.Finalmente, se probó el enfoque de factorización utilizando transformaciones de altavoz MLLR en la parte superior del modelo ANAT y las transformaciones de ACMLLR basadas en Show.Esto solo aumentó la mejora al 0.9% absoluto (2.9% relativo), lo que refleja la dificultad de realizar una agrupación precisa de los altavoces en esta tarea y cómo esto realmente obstaculiza la adaptación del altavoz.",
            "error": [
                ""
            ]
        },
        "aNAT model": {
            "translated_key": "modelo ANAT",
            "translated_annotated_text": "El conjunto final de experimentos implicó un reentrenamiento adaptativo de los parámetros GMM -HMM después del procedimiento ANAT.Este nuevo modelo solo proporcionó una mejora de 0.3%, similar a el uso de las transformaciones ACMLLR en el modelo de base GMM -HMM.Sin embargo, el entrenamiento de ACMLLR basado en Show se transforma además del modelo entrenado adaptativamente aumentó la mejora al 0.8% absoluto.Esto mostró cómo la capacitación adaptativa proporcionó una mejor flexibilidad del modelo para adaptarse a las condiciones de fondo específicas existentes en cada programa.Finalmente, se probó el enfoque de factorización utilizando transformaciones de altavoz MLLR sobre el \"modelo ANAT\" y las transformaciones de ACMLLR basadas en el espectáculo.Esto solo aumentó la mejora al 0.9% absoluto (2.9% relativo), lo que refleja la dificultad de realizar una agrupación precisa de los altavoces en esta tarea y cómo esto realmente obstaculiza la adaptación del altavoz.",
            "error": [
                ""
            ]
        },
        "aNAT procedure": {
            "translated_key": "procedimiento ANAT",
            "translated_annotated_text": "El conjunto final de experimentos implicó un reentrenamiento adaptativo de los parámetros GMM -HMM siguiendo el \"procedimiento ANAT\".Este nuevo modelo solo proporcionó una mejora de 0.3%, similar a el uso de las transformaciones ACMLLR en el modelo de base GMM -HMM.Sin embargo, el entrenamiento de ACMLLR basado en Show se transforma además del modelo entrenado adaptativamente aumentó la mejora al 0.8% absoluto.Esto mostró cómo la capacitación adaptativa proporcionó una mejor flexibilidad del modelo para adaptarse a las condiciones de fondo específicas existentes en cada programa.Finalmente, se probó el enfoque de factorización utilizando transformaciones de altavoz MLLR en la parte superior del modelo ANAT y las transformaciones de ACMLLR basadas en Show.Esto solo aumentó la mejora al 0.9% absoluto (2.9% relativo), lo que refleja la dificultad de realizar una agrupación precisa de los altavoces en esta tarea y cómo esto realmente obstaculiza la adaptación del altavoz.",
            "error": [
                ""
            ]
        },
        "factorisation approach using MLLR speaker transforms": {
            "translated_key": "enfoque de factorización utilizando transformaciones de altavoz MLLR",
            "translated_annotated_text": "El conjunto final de experimentos implicó un reentrenamiento adaptativo de los parámetros GMM -HMM después del procedimiento ANAT.Este nuevo modelo solo proporcionó una mejora de 0.3%, similar a el uso de las transformaciones ACMLLR en el modelo de base GMM -HMM.Sin embargo, el entrenamiento de ACMLLR basado en Show se transforma además del modelo entrenado adaptativamente aumentó la mejora al 0.8% absoluto.Esto mostró cómo la capacitación adaptativa proporcionó una mejor flexibilidad del modelo para adaptarse a las condiciones de fondo específicas existentes en cada programa.Finalmente, se probó el \"enfoque de factorización utilizando transformaciones de altavoz MLLR\" en la parte superior del modelo ANAT y las transformaciones de ACMLLR basadas en el espectáculo.Esto solo aumentó la mejora al 0.9% absoluto (2.9% relativo), lo que refleja la dificultad de realizar una agrupación precisa de los altavoces en esta tarea y cómo esto realmente obstaculiza la adaptación del altavoz.",
            "error": [
                ""
            ]
        },
        "GMM–HMM model": {
            "translated_key": "modelo GMM -HMM",
            "translated_annotated_text": "El conjunto final de experimentos implicó un reentrenamiento adaptativo de los parámetros GMM -HMM después del procedimiento ANAT.Este nuevo modelo solo proporcionó una mejora de 0.3%, similar a el uso de las transformaciones de ACMLLR en la línea de base \"modelo GMM -HMM\".Sin embargo, el entrenamiento de ACMLLR basado en Show se transforma además del modelo entrenado adaptativamente aumentó la mejora al 0.8% absoluto.Esto mostró cómo la capacitación adaptativa proporcionó una mejor flexibilidad del modelo para adaptarse a las condiciones de fondo específicas existentes en cada programa.Finalmente, se probó el enfoque de factorización utilizando transformaciones de altavoz MLLR en la parte superior del modelo ANAT y las transformaciones de ACMLLR basadas en Show.Esto solo aumentó la mejora al 0.9% absoluto (2.9% relativo), lo que refleja la dificultad de realizar una agrupación precisa de los altavoces en esta tarea y cómo esto realmente obstaculiza la adaptación del altavoz.",
            "error": [
                ""
            ]
        },
        "speaker adaptation": {
            "translated_key": "adaptación al altavoz",
            "translated_annotated_text": "El conjunto final de experimentos implicó un reentrenamiento adaptativo de los parámetros GMM -HMM después del procedimiento ANAT.Este nuevo modelo solo proporcionó una mejora de 0.3%, similar a el uso de las transformaciones ACMLLR en el modelo de base GMM -HMM.Sin embargo, el entrenamiento de ACMLLR basado en Show se transforma además del modelo entrenado adaptativamente aumentó la mejora al 0.8% absoluto.Esto mostró cómo la capacitación adaptativa proporcionó una mejor flexibilidad del modelo para adaptarse a las condiciones de fondo específicas existentes en cada programa.Finalmente, se probó el enfoque de factorización utilizando transformaciones de altavoz MLLR en la parte superior del modelo ANAT y las transformaciones de ACMLLR basadas en Show.Esto solo aumentó la mejora a 0.9% absoluto (2.9% relativo), lo que refleja la dificultad de realizar una agrupación precisa de los altavoces en esta tarea y cómo esto realmente obstaculiza la \"adaptación al altavoz\".",
            "error": [
                ""
            ]
        },
        "speaker clustering": {
            "translated_key": "agrupación de altavoces",
            "translated_annotated_text": "El conjunto final de experimentos implicó un reentrenamiento adaptativo de los parámetros GMM -HMM después del procedimiento ANAT.Este nuevo modelo solo proporcionó una mejora de 0.3%, similar a el uso de las transformaciones ACMLLR en el modelo de base GMM -HMM.Sin embargo, el entrenamiento de ACMLLR basado en Show se transforma además del modelo entrenado adaptativamente aumentó la mejora al 0.8% absoluto.Esto mostró cómo la capacitación adaptativa proporcionó una mejor flexibilidad del modelo para adaptarse a las condiciones de fondo específicas existentes en cada programa.Finalmente, se probó el enfoque de factorización utilizando transformaciones de altavoz MLLR en la parte superior del modelo ANAT y las transformaciones de ACMLLR basadas en Show.Esto solo aumentó la mejora al 0.9% absoluto (2.9% relativo), lo que refleja la dificultad de realizar una \"agrupación de altavoces\" precisa en esta tarea y cómo esto realmente obstaculiza la adaptación del altavoz.",
            "error": [
                ""
            ]
        },
        "training show-based aCMLLR transforms": {
            "translated_key": "capacitación de ACMLLR basadas en el programa se transforma",
            "translated_annotated_text": "El conjunto final de experimentos implicó un reentrenamiento adaptativo de los parámetros GMM -HMM después del procedimiento ANAT.Este nuevo modelo solo proporcionó una mejora de 0.3%, similar a el uso de las transformaciones ACMLLR en el modelo de base GMM -HMM.Sin embargo, la \"capacitación de ACMLLR basadas en el programa se transforma\" además del modelo entrenado adaptativamente aumentó la mejora al 0.8% absoluto.Esto mostró cómo la capacitación adaptativa proporcionó una mejor flexibilidad del modelo para adaptarse a las condiciones de fondo específicas existentes en cada programa.Finalmente, se probó el enfoque de factorización utilizando transformaciones de altavoz MLLR en la parte superior del modelo ANAT y las transformaciones de ACMLLR basadas en Show.Esto solo aumentó la mejora al 0.9% absoluto (2.9% relativo), lo que refleja la dificultad de realizar una agrupación precisa de los altavoces en esta tarea y cómo esto realmente obstaculiza la adaptación del altavoz.",
            "error": [
                ""
            ]
        }
    }
}