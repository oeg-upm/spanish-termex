{
    "id": "H-5",
    "original_text": "Utility-based Information Distillation Over Temporally Sequenced Documents Yiming Yang Language Technologies Inst. Carnegie Mellon University Pittsburgh, USA yiming@cs.cmu.edu Abhimanyu Lad Language Technologies Inst. Carnegie Mellon University Pittsburgh, USA alad@cs.cmu.edu Ni Lao Language Technologies Inst. Carnegie Mellon University Pittsburgh, USA nlao@cs.cmu.edu Abhay Harpale Language Technologies Inst. Carnegie Mellon University Pittsburgh, USA aharpale@cs.cmu.edu Bryan Kisiel Language Technologies Inst. Carnegie Mellon University Pittsburgh, USA bkisiel@cs.cmu.edu Monica Rogati Language Technologies Inst. Carnegie Mellon University Pittsburgh, USA mrogati@cs.cmu.edu ABSTRACT This paper examines a new approach to information distillation over temporally ordered documents, and proposes a novel evaluation scheme for such a framework. It combines the strengths of and extends beyond conventional adaptive filtering, novelty detection and non-redundant passage ranking with respect to long-lasting information needs (tasks with multiple queries). Our approach supports fine-grained user feedback via highlighting of arbitrary spans of text, and leverages such information for utility optimization in adaptive settings. For our experiments, we defined hypothetical tasks based on news events in the TDT4 corpus, with multiple queries per task. Answer keys (nuggets) were generated for each query and a semiautomatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses. We also propose an extension of the NDCG metric for assessing the utility of ranked passages as a combination of relevance and novelty. Our results show encouraging utility enhancements using the new approach, compared to the baseline systems without incremental learning or the novelty detection components. Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 General Terms Design, Measurement, Performance, Experimentation. 1. INTRODUCTION Tracking new and relevant information from temporal data streams for users with long-lasting needs has been a challenging research topic in information retrieval. Adaptive filtering (AF) is one such task of online prediction of the relevance of each new document with respect to pre-defined topics. Based on the initial query and a few positive examples (if available), an AF system maintains a profile for each such topic of interest, and constantly updates it based on feedback from the user. The incremental learning nature of AF systems makes them more powerful than standard search engines that support ad-hoc retrieval (e.g. Google and Yahoo) in terms of finding relevant information with respect to long-lasting topics of interest, and more attractive for users who are willing to provide feedback to adapt the system towards their specific information needs, without having to modify their queries manually. A variety of supervised learning algorithms (Rocchio-style classifiers, Exponential-Gaussian models, local regression and logistic regression approaches) have been studied for adaptive settings, examined with explicit and implicit relevance feedback, and evaluated with respect to utility optimization on large benchmark data collections in TREC (Text Retrieval Conferences) and TDT (Topic Detection and Tracking) forums [1, 4, 7, 15, 16, 20, 24, 23]. Regularized logistic regression [21] has been found representative for the state-of-the-art approaches, and highly efficient for frequent model adaptations over large document collections such as the TREC-10 corpus (over 800,000 documents and 84 topics). Despite substantial achievements in recent adaptive filtering research, significant problems remain unsolved regarding how to leverage user feedback effectively and efficiently. Specifically, the following issues may seriously limit the true utility of AF systems in real-world applications: 1. User has a rather passive role in the conventional adaptive filtering setup - he or she reacts to the system only when the system makes a yes decision on a document, by confirming or rejecting that decision. A more active alternative would be to allow the user to issue multiple queries for a topic, review a ranked list of candidate documents (or passages) per query, and provide feedback on the ranked list, thus refining their information need and requesting updated ranked lists. The latter form of user interaction has been highly effective in standard retrieval for ad-hoc queries. How to deploy such a strategy for long-lasting information needs in AF settings is an open question for research. 2. The unit for receiving a relevance judgment (yes or no) is restricted to the document level in conventional AF. However, a real user may be willing to provide more informative, fine-grained feedback via highlighting some pieces of text in a retrieved document as relevant, instead of labeling the entire document as relevant. Effectively leveraging such fine-grained feedback could substantially enhance the quality of an AF system. For this, we need to enable supervised learning from labeled pieces of text of arbitrary span instead of just allowing labeled documents. 3. System-selected documents are often highly redundant. A major news event, for example, would be reported by multiple sources repeatedly for a while, making most of the information content in those articles redundant with each other. A conventional AF system would select all these redundant news stories for user feedback, wasting the users time while offering little gain. Clearly, techniques for novelty detection can help in principle [25, 2, 22] for improving the utility of the AF systems. However, the effectiveness of such techniques at passage level to detect novelty with respect to users (fine-grained) feedback and to detect redundancy in ranked lists remains to be evaluated using a measure of utility that mimics the needs of a real user. To address the above limitations of current AF systems, we propose and examine a new approach in this paper, combining the strengths of conventional AF (incremental learning of topic models), multi-pass passage retrieval for long-lasting queries conditioned on topic, and novelty detection for removal of redundancy from user interactions with the system. We call the new process utility-based information distillation. Note that conventional benchmark corpora for AF evaluations, which have relevance judgments at the document level and do not define tasks with multiple queries, are insufficient for evaluating the new approach. Therefore, we extended a benchmark corpus - the TDT4 collection of news stories and TV broadcasts - with task definitions, multiple queries per task, and answer keys per query. We have conducted our experiments on this extended TDT4 corpus and have made the additionally generated data publicly available for future comparative evaluations 1 . To automatically evaluate the system-returned arbitrary spans of text using our answer keys, we further developed an evaluation scheme with semi-automatic procedure for 1 URL: http://nyc.lti.cs.cmu.edu/downloads acquiring rules that can match nuggets against system responses. Moreover, we propose an extension of NDCG (Normalized Discounted Cumulated Gain) [9] for assessing the utility of ranked passages as a function of both relevance and novelty. The rest of this paper is organized as follows. Section 2 outlines the information distillation process with a concrete example. Section 3 describes the technical cores of our system called CAF´E - CMU Adaptive Filtering Engine. Section 4 discusses issues with respect to evaluation methodology and proposes a new scheme. Section 5 describes the extended TDT4 corpus. Section 6 presents our experiments and results. Section 7 concludes the study and gives future perspectives. 2. A SAMPLE TASK Consider a news event - the escape of seven convicts from a Texas prison in December 2000 and their capture a month later. Assuming a user were interested in this event since its early stage, the information need could be: Find information about the escape of convicts from Texas prison, and information related to their recapture. The associated lower-level questions could be: 1. How many prisoners escaped? 2. Where and when were they sighted? 3. Who are their known contacts inside and outside the prison? 4. How are they armed? 5. Do they have any vehicles? 6. What steps have been taken so far? We call such an information need a task, and the associated questions as the queries in this task. A distillation system is supposed to monitor the incoming documents, process them chunk by chunk in a temporal order, select potentially relevant and novel passages from each chunk with respect to each query, and present a ranked list of passages to the user. Passage ranking here is based on how relevant a passage is with respect to the current query, how novel it is with respect to the current user history (of his or her interactions with the system), and how redundant it is compared to other passages with a higher rank in the list. When presented with a list of passages, the user may provide feedback by highlighting arbitrary spans of text that he or she found relevant. These spans of text are taken as positive examples in the adaptation of the query profile, and also added to the users history. Passages not marked by the user are taken as negative examples. As soon as the query profile is updated, the system re-issues a search and returns another ranked list of passages where the previously seen passages are either removed or ranked low, based on user preference. For example, if the user highlights ...officials have posted a $100,000 reward for their capture... as relevant answer to the query What steps have been taken so far?, then the highlighted piece is used as an additional positive training example in the adaptation of the query profile. This piece of feedback is also added to the user history as a seen example, so that in future, the system will not place another passage mentioning $100,000 reward at the top of the ranked list. However, an article mentioning ...officials have doubled the reward money to $200,000... might be ranked high since it is both relevant to the (updated) query profile and novel with respect to the (updated) user history. The user may modify the original queries or add a new query during the process; the query profiles will be changed accordingly. Clearly, novelty detection is very important for the utility of such a system because of the iterative search. Without novelty detection, the old relevant passages would be shown to the user repeatedly in each ranked list. Through the above example, we can see the main properties of our new framework for utility-based information distillation over temporally ordered documents. Our framework combines and extends the power of adaptive filtering (AF), ad-hoc retrieval (IR) and novelty detection (ND). Compared to standard IR, our approach has the power of incrementally learning long-term information needs and modeling a sequence of queries within a task. Compared to conventional AF, it enables a more active role of the user in refining his or her information needs and requesting new results by allowing relevance and novelty feedback via highlighting of arbitrary spans of text in passages returned by the system. Compared to past work, this is the first evaluation of novelty detection integrated with adaptive filtering for sequenced queries that allows flexible user feedback over ranked passages. The combination of AF, IR and ND with the new extensions raises an important research question regarding evaluation methodology: how can we measure the utility of such an information distillation system? Existing metrics in standard IR, AF and ND are insufficient, and new solutions must be explored, as we will discuss in Section 4, after describing the technical cores of our system in the next section. 3. TECHNICAL CORES The core components of CAF´E are - 1) AF for incremental learning of query profiles, 2) IR for estimating relevance of passages with respect to query profiles, 3) ND for assessing novelty of passages with respect to users history, and 4) anti-redundancy component to remove redundancy from ranked lists. 3.1 Adaptive Filtering Component We use a state-of-the-art algorithm in the field - the regularized logistic regression method which had the best results on several benchmark evaluation corpora for AF [21]. Logistic regression (LR) is a supervised learning algorithm for statistical classification. Based on a training set of labeled instances, it learns a class model which can then by used to predict the labels of unseen instances. Its performance as well as efficiency in terms of training time makes it a good candidate when frequent updates of the class model are required, as is the case in adaptive filtering, where the system must learn from each new feedback provided by the user. (See [21] and [23] for computational complexity and implementation issues). In adaptive filtering, each query is considered as a class and the probability of a passage belonging to this class corresponds to the degree of relevance of the passage with respect to the query. For training the model, we use the query itself as the initial positive training example of the class, and the user-highlighted pieces of text (marked as Relevant or Not-relevant) during feedback as additional training examples. To address the cold start issue in the early stage before any user feedback is obtained, the system uses a small sample from a retrospective corpus as the initial negative examples in the training set. The details of using logistic regression for adaptive filtering (assigning different weights to positive and negative training instances, and regularizing the objective function to prevent over-fitting on training data) are presented in [21]. The class model w∗ learned by Logistic Regression, or the query profile, is a vector whose dimensions are individual terms and whose elements are the regression coefficients, indicating how influential each term is in the query profile. The query profile is updated whenever a new piece of user feedback is received. A temporally decaying weight can be applied to each training example, as an option, to emphasize the most recent user feedback. 3.2 Passage Retrieval Component We use standard IR techniques in this part of our system. Incoming documents are processed in chunks, where each chunk can be defined as a fixed span of time or as a fixed number of documents, as preferred by the user. For each incoming document, corpus statistics like the IDF (Inverted Document Frequency) of each term are updated. We use a state-of-the-art named entity identifier and tracker [8, 12] to identify person and location names, and merge them with co-referent named entities seen in the past. Then the documents are segmented into passages, which can be a whole document, a paragraph, a sentence, or any other continuous span of text, as preferred. Each passage is represented using a vector of TF-IDF (Term FrequencyInverse Document Frequency) weights, where term can be a word or a named entity. Given a query profile, i.e. the logistic regression solution w∗ as described in Section 3.1, the system computes the posterior probability of relevance for each passage x as fRL(x) ≡ P(y = 1|x, w∗ ) = 1 (1 + e−w∗·x) (1) Passages are ordered by their relevance scores, and the ones with scores above a threshold (tuned on a training set) comprise the relevance list that is passed on to the novelty detection step. 3.3 Novelty Detection Component CAF´E maintains a user history H(t), which contains all the spans of text hi that the user highlighted (as feedback) during his or her past interactions with the system, up to the current time t. Denoting the history as H(t) = n h1, h2, ..., ht o , (2) the novelty score of a new candidate passage x is computed as: fND(x) = 1 − max i∈1..t {cos(x, hi)} (3) where both candidate passage x and highlighted spans of text hi are represented as TF-IDF vectors. The novelty score of each passage is compared to a prespecified threshold (also tuned on a training set), and any passage with a score below this threshold is removed from the relevance list. 3.4 Anti-redundant Ranking Component Although the novelty detection component ensures that only novel (previously unseen) information remains in the relevance list, this list might still contain the same novel information at multiple positions in the ranked list. Suppose, for example, that the user has already read about a $100,000 reward for information about the escaped convicts. A new piece of news that the award has been increased to $200,000 is novel since the user hasnt read about it yet. However, multiple news sources would report this news and we might end up showing (redundant) articles from all these sources in a ranked list. Hence, a ranked list should also be made non-redundant with respect to its own contents. We use a simplified version of the Maximal Marginal Relevance method [5], originally developed for combining relevance and novelty in text retrieval and summarization. Our procedure starts with the current list of passages sorted by relevance (section 3.2), filtered by Novelty Detection component (section 3.3), and generates a new non-redundant list as follows: 1. Take the top passage in the current list as the top one in the new list. 2. Add the next passage x in the current list to the new list only if fAR(x) > t where fAR(x) = 1 − max pi∈Lnew {cos(x, pi)} and Lnew is the set of passages already selected in the new list. 3. Repeat step 2 until all the passages in the current list have been examined. After applying the above-mentioned algorithm, each passage in the new list is sufficiently dissimilar to others, thus favoring diversity rather than redundancy in the new ranked list. The anti-redundancy threshold t is tuned on a training set. 4. EVALUATION METHODOLOGY The approach we proposed above for information distillation raises important issues regarding evaluation methodology. Firstly, since our framework allows the output to be passages at different levels of granularity (e.g. k-sentence windows where k may vary) instead of a fixed length, it is not possible to have pre-annotated relevance judgments at all such granularity levels. Secondly, since we wish to measure the utility of the system output as a combination of both relevance and novelty, traditional relevance-only based measures must be replaced by measures that penalize the repetition of the same information in the system output across time. Thirdly, since the output of the system is ranked lists, we must reward those systems that present useful information (both relevant and previously unseen) using shorter ranked lists, and penalize those that present the same information using longer ranked lists. None of the existing measures in ad-hoc retrieval, adaptive filtering, novelty detection or other related areas (text summarization and question answering) have desirable properties in all the three aspects. Therefore, we must develop a new evaluation methodology. 4.1 Answer Keys To enable the evaluation of a system whose output consists of passages of arbitrary length, we borrow the concept of answer keys from the Question Answering (QA) community, where systems are allowed to return arbitrary spans of text as answers. Answer keys define what should be present in a system response to receive credit, and are comprised of a collection of information nuggets, i.e. factoid units about which human assessors can make binary decisions of whether or not a system response contains them. Defining answer keys and making the associated binary decisions are conceptual tasks that require semantic mapping [19], since system-returned passages can contain the same information expressed in many different ways. Hence, QA evaluations have relied on human assessors for the mapping between various expressions, making the process costly, time consuming, and not scalable to large query and document collections, and extensive system evaluations with various parameter settings. 4.1.1 Automating Evaluation based on Answer Keys Automatic evaluation methods would allow for faster system building and tuning, as well as provide an objective and affordable way of comparing various systems. Recently, such methods have been proposed, more or less, based on the idea of n-gram co-occurrences. Pourpre [10] assigns a fractional recall score to a system response based on its unigram overlap with a given nuggets description. For example, a system response A B C has recall 3/4 with respect to a nugget with description A B C D. However, such an approach is unfair to systems that present the same information but using words other than A, B, C, and D. Another open issue is how to weight individual words in measuring the closeness of a match. For example, consider the question How many prisoners escaped?. In the nugget Seven prisoners escaped from a Texas prison, there is no indication that seven is the keyword, and that it must be matched to get any relevance credit. Using IDF values does not help, since seven will generally not have a higher IDF than words like texas and prison. Also, redefining the nugget as just seven does not solve the problem since now it might spuriously match any mention of seven out of context. Nuggeteer [13] works on similar principles but makes binary decisions about whether a nugget is present in a given system response by tuning a threshold. However, it is also plagued by spurious relevance since not all words contained in the nugget description (or known correct responses) are central to the nugget. 4.1.2 Nugget-Matching Rules We propose a reliable automatic method for determining whether a snippet of text contains a given nugget, based on nugget-matching rules, which are generated using a semiautomatic procedure explained below. These rules are essentially Boolean queries that will only match against snippets that contain the nugget. For instance, a candidate rule for matching answers to How many prisoners escaped? is (Texas AND seven AND escape AND (convicts OR prisoners)), possibly with other synonyms and variants in the rule. For a corpus of news articles, which usually follow a typical formal prose, it is fairly easy to write such simple rules to match expected answers using a bootstrap approach, as described below. We propose a two-stage approach, inspired by Autoslog [14], that combines the strength of humans in identifying semantically equivalent expressions and the strength of the system in gathering statistical evidence from a humanannotated corpus of documents. In the first stage, human subjects annotated (using a highlighting tool) portions of ontopic documents that contained answers to each nugget 2 . In the second stage, subjects used our rule generation tool to create rules that would match the annotations for each nugget. The tool allows users to enter a Boolean rule as a disjunction of conjunctions (e.g. ((a AND b) OR (a AND c AND d) OR (e))). Given a candidate rule, our tool uses it as a Boolean query over the entire set of on-topic documents and calculates its recall and precision with respect to the annotations that it is expected to match. Hence, the subjects can start with a simple rule and iteratively refine it until they are satisfied with its recall and precision. We observed that it was very easy for humans to improve the precision of a rule by tweaking its existing conjunctions (adding more ANDs), and improving the recall by adding more conjunctions to the disjunction (adding more ORs). As an example, lets try to create a rule for the nugget which says that seven prisoners escaped from the Texas prison. We start with a simple rule - (seven). When we input this into the rule generation tool, we realize that this rule matches many spurious occurrences of seven (e.g. ...seven states...) and thus gets a low precision score. We can further qualify our rule - Texas AND seven AND convicts. Next, by looking at the missed annotations, we realize that some news articles mentioned ...seven prisoners escaped.... We then replace convicts with the disjunction (convicts OR prisoners). We continue tweaking the rule in this manner until we achieve a sufficiently high recall and precision - i.e. the (small number of) misses and false alarms can be safely ignored. Thus we can create nugget-matching rules that succinctly capture various ways of expressing a nugget, while avoiding matching incorrect (or out of context) responses. Human involvement in the rule creation process ensures high quality generic rules which can then be used to evaluate arbitrary system responses reliably. 4.2 Evaluating the Utility of a Sequence of Ranked Lists The utility of a retrieval system can be defined as the difference between how much the user gained in terms of useful information, and how much the user lost in terms of time and energy. We calculate this utility from the utilities of individual passages as follows. After reading each passage returned by the system, the user derives some gain depending on the presence of relevant and novel information, and incurs a loss in terms of the time and energy spent in going through the passage. However, the likelihood that the user would actually read a passage depends on its position in the ranked list. Hence, for a query q, the expected utility 2 LDC [18] already provides relevance judgments for 100 topics on the TDT4 corpus. We further ensured that these judgments are exhaustive on the entire corpus using pooling. of a passage pi at rank i can be defined as U(pi, q) = P(i) ∗ (Gain(pi, q) − Loss(pi, q)) (4) where P(i) is the probability that the user would go through a passage at rank i. The expected utility for an entire ranked list of length n can be calculated simply by adding the expected utility of each passage: U(q) = nX i=1 P(i) ∗ (Gain(pi, q) − Loss(pi, q)) (5) Note that if we ignore the loss term and define P(i) as P(i) ∝ 1/ logb(b + i − 1) (6) then we get the recently popularized metric called Discounted Cumulated Gain (DCG) [9], where Gain(pi, q) is defined as the graded relevance of passage pi. However, without the loss term, DCG is a purely recall-oriented metric and not suitable for an adaptive filtering setting, where the systems utility depends in part on its ability to limit the number of items shown to the user. Although P(i) could be defined based on empirical studies of user behavior, for simplicity, we use P(i) exactly as defined in equation 6. The gain G(pi, q) of passage pi with respect to the query q is a function of - 1) the number of relevant nuggets present in pi, and 2) the novelty of each of these nuggets. We combine these two factors as follows. For each nugget Nj, we assign an initial weight wj, and also keep a count nj of the number of times this nugget has been seen by the user in the past. The gain derived from each subsequent occurrence of the same nugget is assumed to reduce by a dampening factor γ. Thus, G(pi, q) is defined as G(pi, q) = X Nj ∈C(pi,q) wj ∗ γnj (7) where C(pi, q) is the set of all nuggets that appear in passage pi and also belong to the answer key of query q. The initial weights wj are all set of be 1.0 in our experiments, but can also be set based on a pyramid approach [11]. The choice of dampening factor γ determines the users tolerance for redundancy. When γ = 0, a nugget will only receive credit for its first occurrence i.e. when nj is zero3 . For 0 < γ < 1, a nugget receives smaller credit for each successive occurrence. When γ = 1, no dampening occurs and repeated occurrences of a nugget receive the same credit. Note that the nugget occurrence counts are preserved between evaluation of successive ranked lists returned by the system, since the users are expected to remember what the system showed them in the past. We define the loss L(pi, q) as a constant cost c (we use 0.1) incurred when reading a system-returned passage. Thus, our metric can be re-written as U(q) = nX i=1 Gain(pi, q) logb(b + i − 1) − L(n) (8) where L(n) is the loss associated with a ranked list of length n: L(n) = c · nX i=1 1 logb(b + i − 1) (9) 3 Note that 00 = 1 Due to the similarity with Discounted Cumulated Gain (DCG), we call our metric Discounted Cumulated Utility (DCU). The DCU score obtained by the system is converted to a Normalized DCU (NDCU) score by dividing it by the DCU score of the ideal ranked list, which is created by ordering passages by their decreasing utility scores U(pi, q) and stopping when U(pi, q) ≤ 0 i.e. when the gain is less than or equal to the cost of reading the passage. 5. DATA TDT4 was the benchmark corpus used in TDT2002 and TDT2003 evaluations. The corpus consists of over 90, 000 news articles from multiple sources (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) published between October 2000 and January 2001, in the languages of Arabic, English, and Mandarin. Speech-recognized and machine-translated versions of the non-English articles were provided as well. LDC [18] has annotated the corpus with 100 topics, that correspond to various news events in this time period. Out of these, we selected a subset of 12 actionable events, and defined corresponding tasks for them4 . For each task, we manually defined a profile consisting of an initial set of (5 to 10) queries, a free-text description of the user history, i.e., what the user already knows about the event, and a list of known on-topic and off-topic documents (if available) as training examples. For each query, we generated answer keys and corresponding nugget matching rules using the procedure described in section 4.1.2, and produced a total of 120 queries, with an average of 7 nuggets per query. 6. EXPERIMENTS AND RESULTS 6.1 Baselines We used Indri [17], a popular language-model based retrieval engine, as a baseline for comparison with CAF´E. Indri supports standard search engine functionality, including pseudo-relevance feedback (PRF) [3, 6], and is representative of a typical query-based retrieval system. Indri does not support any kind of novelty detection. We compare Indri with PRF turned on and off, against CAF´E with user feedback, novelty detection and antiredundant ranking turned on and off. 6.2 Experimental Setup We divided the TDT4 corpus spanning 4 months into 10 chunks, each defined as a period of 12 consecutive days. At any given point of time in the distillation process, each system accessed the past data up to the current point, and returned a ranked list of up 50 passages per query. The 12 tasks defined on the corpus were divided into a training and test set with 6 tasks each. Each system was allowed to use the training set to tune its parameters for optimizing NDCU (equation 8), including the relevance threshold for both Indri and CAF´E, and the novelty and antiredundancy thresholds for CAF´E. The NDCU for each system run is calculated automatically. User feedback was also simulated - relevance judgments for each system-returned passage (as determined by the nugget matching rules described in section 4.1.2) were 4 URL: http://nyc.lti.cs.cmu.edu/downloads Figure 1: Performance of Indri across chunks Figure 2: Performance of CAF´E across chunks used as user feedback in the adaptation of query profiles and user histories. 6.3 Results In Table 1, we show the NDCU scores of the two systems under various settings. These scores are averaged over the six tasks in the test set, and are calculated with two dampening factors (see section 4.2): γ = 0 and 0.1, to simulate no tolerance and small tolerance for redundancy, respectively. Using γ = 0 creates a much more strict metric since it does not give any credit to a passage that contains relevant but redundant information. Hence, the improvement obtained from enabling user feedback is smaller with γ = 0 than the improvement obtained from feedback with γ = 0.1. This reveals a shortcoming of contemporary retrieval systemswhen the user gives positive feedback on a passage, the systems gives higher weights to the terms present in that passage and tends to retrieve other passages containing the same terms - and thus - usually the same information. However, the user does not benefit from seeing such redundant passages, and is usually interested in other passages containing related information. It is informative to evaluate retrieval systems using our utility measure (with γ = 0) which accounts for novelty and thus gives a more realistic picture of how well a system can generalize from user feedback, rather than using traditional IR measures like recall and precision which give an incomplete picture of improvement obtained from user feedback. Sometimes, however, users might indeed be interested in seeing the same information from multiple sources, as an Table 1: NDCU Scores of Indri and CAF´E for two dampening factors (γ), and various settings (F: Feedback, N: Novelty Detection, A: Anti-Redundant Ranking) Indri CAF´E γ Base +PRF Base +F +F+N +F+A +F+N+A 0 0.19 0.19 0.22 0.23 0.24 0.24 0.24 0.1 0.28 0.29 0.24 0.35 0.35 0.36 0.36 indicator of its importance or reliability. In such a case, we can simply choose a higher value for γ which corresponds to a higher tolerance for redundancy, and hence let the system tune its parameters accordingly. Since documents were processed chunk by chunk, it would be interesting to see how the performance of systems improves over time. Figures 1 and 2 show the performance trends for both the systems across chunks. While the performance with and without feedback on the first few chunks is expected to be close, for subsequent chunks, the performance curve with feedback enabled rises above the one with the no-feedback setting. The performance trends are not consistent across all chunks because on-topic documents are not uniformly distributed over all the chunks, making some queries easier than others in certain chunks. Moreover, since Indri uses pseudo-relevance feedback while CAF´E uses feedback based on actual relevance judgments, the improvement in case of Indri is less dramatic than that of CAF´E. 7. CONCLUDING REMARKS This paper presents the first investigation on utility-based information distillation with a system that learns the longlasting information needs from fine-grained user feedback over a sequence of ranked passages. Our system, called CAF´E, combines adaptive filtering, novelty detection and antiredundant passage ranking in a unified framework for utility optimization. We developed a new scheme for automated evaluation and feedback based on a semi-automatic procedure for acquiring rules that allow automatically matching nuggets against system responses. We also proposed an extension of the NDCG metric for assessing the utility of ranked passages as a weighted combination of relevance and novelty. Our experiments on the newly annotated TDT4 benchmark corpus show encouraging utility enhancement over Indri, and also over our own system with incremental learning and novelty detection turned off. 8. ACKNOWLEDGMENTS We would like to thank Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng, and the Qualitative Data Analysis Program at the University of Pittsburgh lead by Dr. Stuart Shulman for their help with collecting and processing the extended TDT4 annotations used in our experiments. This work is supported in parts by the National Science Foundation (NSF) under grant IIS0434035, and the Defense Advanced Research Project Agency (DARPA) under contracts NBCHD030010 and W0550432. Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors. 9. ADDITIONAL AUTHORS Jian Zhang (jianzhan@stat.purdue.edu)∗ , Jaime Carbonell (jgc@cs.cmu.edu)† , Peter Brusilovsky (peterb+@pitt.edu)‡ , Daqing He(dah44@pitt.edu)‡ 10. REFERENCES [1] J. Allan. Incremental Relevance Feedback for Information Filtering. Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, pages 270-278, 1996. [2] J. Allan, C. Wade, and A. Bolivar. Retrieval and Novelty Detection at the Sentence Level. Proceedings of the ACM SIGIR conference on research and development in information retrieval, 2003. [3] C. Buckley, G. Salton, and J. Allan. Automatic Retrieval with Locality Information using SMART. NIST special publication, (500207):59-72, 1993. [4] J. Callan. Learning While Filtering Documents. Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 224-231, 1998. [5] J. Carbonell and J. Goldstein. The use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries. Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 335-336, 1998. [6] E. Efthimiadis. Query Expansion. Annual Review of Information Science and Technology (ARIST), 31:p121-87, 1996. [7] J. Fiscus and G. Duddington. Topic Detection and Tracking Overview. Topic Detection and Tracking: Event-based Information Organization, pages 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos. A Statistical Model for Multilingual Entity Detection and Tracking. NAACL/HLT, 2004. [9] K. J¨arvelin and J. Kek¨al¨ainen. Cumulated Gain-based Evaluation of IR Techniques. ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin and D. Demner-Fushman. Automatically Evaluating Answers to Definition Questions. Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), 2005. ∗ Statistics Dept., Purdue University, West Lafayette, USA † Language Technologies Inst., Carnegie Mellon University, Pittsburgh, USA ‡ School of Information Sciences, Univ. of Pittsburgh, Pittsburgh, USA [11] J. Lin and D. Demner-Fushman. Will Pyramids Built of nUggets Topple Over. Proceedings of HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos. A Mention-synchronous Coreference Resolution Algorithm based on the Bell Tree. Proc. of ACL, 4:136-143, 2004. [13] G. Marton. Nuggeteer: Automatic Nugget-Based Evaluation Using Descriptions and Judgments. HLT/NAACL, 2006. [14] E. Riloff. Automatically Constructing a Dictionary for Information Extraction Tasks. Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 811-816, 1993. [15] S. Robertson and S. Walker. Microsoft Cambridge at TREC-9: Filtering track. The Ninth Text REtrieval Conference (TREC-9), pages 361-368. [16] R. Schapire, Y. Singer, and A. Singhal. Boosting and Rocchio Applied to Text Filtering. Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle, and W. Croft. Indri: A Language Model-based Search Engine for Complex Queries. Proceedings of the International Conference on Intelligence Analysis, 2004. [18] The Linguistic Data Consortium. http://www.ldc.upenn.edu/. [19] E. Voorhees. Overview of the TREC 2003 Question Answering Track. Proceedings of the Twelfth Text REtrieval Conference (TREC 2003), 2003. [20] Y. Yang and B. Kisiel. Margin-based Local Regression for Adaptive Filtering. Proceedings of the twelfth international conference on Information and knowledge management, pages 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel. Robustness of Adaptive Filtering Methods in a Cross-benchmark Evaluation. Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 98-105, 2005. [22] C. Zhai, W. Cohen, and J. Lafferty. Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval. Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 10-17, 2003. [23] J. Zhang and Y. Yang. Robustness of Regularized Linear Classification Methods in Text Categorization. Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 190-197, 2003. [24] Y. Zhang. Using Bayesian Priors to Combine Classifiers for Adaptive Filtering. Proceedings of the 27th annual international conference on Research and development in information retrieval, pages 345-352, 2004. [25] Y. Zhang, J. Callan, and T. Minka. Novelty and Redundancy Detection in Adaptive Filtering. Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002.",
    "original_translation": "Destilación de información basada en servicios públicos sobre documentos secuenciados temporalmente Yiming Yang Language Technologies Inst. Universidad de Carnegie Mellon Pittsburgh, EE. UU. Yiming@cs.cmu.edu Abhimanyu Lad Language Technologies Inst. Universidad de Carnegie Mellon Pittsburgh, EE. UU. Alad@cs.cmu.edu ni Lao Language Technologies Inst. Universidad de Carnegie Mellon Pittsburgh, EE. UU. Nlao@cs.cmu.edu abhay Harpale Language Technologies Inst. Universidad de Carnegie Mellon Pittsburgh, EE. UU. Aharpale@cs.cmu.edu Bryan Kisiel Language Technologies Inst. Universidad de Carnegie Mellon Pittsburgh, EE. UU. Bkisiel@cs.cmu.edu Monica Rogati Language Technologies Inst. Carnegie Mellon University Pittsburgh, EE. UU. Mrogati@cs.cmu.edu Resumen Este documento examina un nuevo enfoque para la destilación de información sobre documentos ordenados temporalmente y propone un nuevo esquema de evaluación para dicho marco. Combina las fortalezas y se extiende más allá del filtrado adaptativo convencional, la detección de novedad y la clasificación de pasaje no redundante con respecto a las necesidades de información duradera (tareas con múltiples consultas). Nuestro enfoque admite la retroalimentación de los usuarios de grano fino mediante la resaltación de tramos arbitrarios de texto y aprovecha dicha información para la optimización de servicios públicos en entornos adaptativos. Para nuestros experimentos, definimos tareas hipotéticas basadas en eventos de noticias en el corpus TDT4, con múltiples consultas por tarea. Se generaron claves de respuesta (pepitas) para cada consulta y se utilizó un procedimiento semiautomático para adquirir reglas que permiten que las pepitas de coincidencia automáticamente coincidan con las respuestas del sistema. También proponemos una extensión de la métrica NDCG para evaluar la utilidad de los pasajes clasificados como una combinación de relevancia y novedad. Nuestros resultados muestran mejoras alentadoras de utilidad utilizando el nuevo enfoque, en comparación con los sistemas de referencia sin aprendizaje incremental o los componentes de detección de novedades. Categorías y descriptores de sujetos H.3.3 [Búsqueda y recuperación de información]: filtrado de información, retroalimentación de relevancia, modelos de recuperación, proceso de selección;I.5.2 Términos generales Diseño, medición, rendimiento, experimentación.1. Introducción El seguimiento de la información nueva y relevante de los flujos de datos temporales para usuarios con necesidades duraderas ha sido un tema de investigación desafiante en la recuperación de la información. El filtrado adaptativo (AF) es una tarea de predicción en línea de la relevancia de cada nuevo documento con respecto a los temas predefinidos. Según la consulta inicial y algunos ejemplos positivos (si están disponibles), un sistema AF mantiene un perfil para cada tema de interés, y lo actualiza constantemente en función de los comentarios del usuario. La naturaleza de aprendizaje incremental de los sistemas AF los hace más potentes que los motores de búsqueda estándar que respaldan la recuperación ad-hoc (p. Ej. Google y Yahoo) en términos de encontrar información relevante con respecto a los temas de interés duraderos y más atractivos para los usuarios que están dispuestos a proporcionar comentarios para adaptar el sistema a sus necesidades de información específicas, sin tener que modificar sus consultas manualmente. Se han estudiado una variedad de algoritmos de aprendizaje supervisados (clasificadores de estilo Rocchio, modelos exponenciales-gaussianos, regresión local y enfoques de regresión logística) para obtener entornos adaptativos, examinados con retroalimentación de relevancia explícita e implícita, y se evalúa con respecto a la optimización de utilidad en los datos de contenido de referencia grandes en grandesColecciones en TREC (conferencias de recuperación de texto) y foros TDT (detección y seguimiento de temas) [1, 4, 7, 15, 16, 20, 24, 23]. La regresión logística regularizada [21] se ha encontrado representativa para los enfoques de última generación y altamente eficiente para adaptaciones de modelos frecuentes sobre grandes colecciones de documentos, como el Corpus TREC-10 (más de 800,000 documentos y 84 temas). A pesar de los logros sustanciales en la investigación de filtrado adaptativo reciente, los problemas significativos siguen sin resolverse con respecto a cómo aprovechar la retroalimentación de los usuarios de manera efectiva y eficiente. Específicamente, los siguientes problemas pueden limitar seriamente la verdadera utilidad de los sistemas AF en aplicaciones del mundo real: 1. El usuario tiene un papel bastante pasivo en la configuración de filtrado adaptativo convencional: él o ella reacciona al sistema solo cuando el sistema toma una decisión Sí en un documento, al confirmar o rechazar esa decisión. Una alternativa más activa sería permitir al usuario emitir múltiples consultas para un tema, revisar una lista clasificada de documentos candidatos (o pasajes) por consulta y proporcionar comentarios sobre la lista clasificada, refinando así su necesidad de información y solicitando listas clasificadas actualizadas. La última forma de interacción del usuario ha sido altamente efectiva en la recuperación estándar para consultas ad-hoc. Cómo implementar una estrategia de este tipo para las necesidades de información duradera en los entornos de AF es una pregunta abierta para la investigación.2. La unidad para recibir un juicio de relevancia (sí o no) está restringido al nivel de documento en la FA convencional. Sin embargo, un usuario real puede estar dispuesto a proporcionar comentarios más informativos y de grano fino al resaltar algunas piezas de texto en un documento recuperado como relevante, en lugar de etiquetar todo el documento como relevante. Aprovechar efectivamente dichos comentarios de grano fino podría mejorar sustancialmente la calidad de un sistema AF. Para esto, necesitamos habilitar el aprendizaje supervisado de las piezas de texto etiquetadas de lapso arbitrario en lugar de solo permitir documentos etiquetados.3. Los documentos seleccionados por el sistema a menudo son altamente redundantes. Un evento de noticias importante, por ejemplo, sería informado por múltiples fuentes repetidamente durante un tiempo, lo que hace que la mayor parte del contenido de información en esos artículos sea redundante entre sí. Un sistema AF convencional seleccionaría todas estas noticias redundantes para los comentarios de los usuarios, perdiendo el tiempo de los usuarios mientras ofrece poca ganancia. Claramente, las técnicas para la detección de novedad pueden ayudar en principio [25, 2, 22] para mejorar la utilidad de los sistemas AF. Sin embargo, la efectividad de tales técnicas a nivel de pasaje para detectar la novedad con respecto a la retroalimentación de los usuarios (de grano fino) y detectar la redundancia en las listas clasificadas aún no se ha evaluado utilizando una medida de utilidad que imita las necesidades de un usuario real. Para abordar las limitaciones anteriores de los sistemas AF actuales, proponemos y examinamos un nuevo enfoque en este documento, combinando las fortalezas de la FA convencional (aprendizaje incremental de los modelos de temas), recuperación de pasaje de múltiples pasos para consultas duraderas condicionadas sobre el tema, yDetección de novedad para la eliminación de la redundancia de las interacciones del usuario con el sistema. Llamamos al nuevo proceso de destilación de información basada en servicios públicos. Tenga en cuenta que los cuerpos de referencia convencionales para las evaluaciones de AF, que tienen juicios de relevancia a nivel de documento y no definen tareas con múltiples consultas, son insuficientes para evaluar el nuevo enfoque. Por lo tanto, ampliamos un corpus de referencia, la colección TDT4 de noticias y transmisiones de televisión, con definiciones de tareas, múltiples consultas por tarea y claves de respuesta por consulta. Hemos realizado nuestros experimentos en este corpus TDT4 extendido y hemos puesto los datos generados adicionalmente disponibles públicamente para futuras evaluaciones comparativas 1. Para evaluar automáticamente los tramos arbitrarios de texto arbitrarios retirados del sistema utilizando nuestras claves de respuesta, desarrollamos un esquema de evaluación con procedimiento semiautomático para 1 URL: http://nyc.lti.cs.cmu.edu/downloads que pueden adquirir reglas que puedencoincidir con las pepitas con respuestas del sistema. Además, proponemos una extensión de NDCG (ganancia acumulada con descuento normalizada) [9] para evaluar la utilidad de los pasajes clasificados en función de la relevancia y la novedad. El resto de este documento está organizado de la siguiente manera. La Sección 2 describe el proceso de destilación de información con un ejemplo concreto. La Sección 3 describe los núcleos técnicos de nuestro sistema llamado Motor de filtrado adaptativo CAF´E - CMU. La Sección 4 analiza temas con respecto a la metodología de evaluación y propone un nuevo esquema. La Sección 5 describe el corpus TDT4 extendido. La Sección 6 presenta nuestros experimentos y resultados. La Sección 7 concluye el estudio y ofrece perspectivas futuras.2. Una tarea de muestra considere un evento de noticias: el escape de siete convictos de una prisión de Texas en diciembre de 2000 y su captura un mes después. Suponiendo que un usuario estuviera interesado en este evento desde su etapa inicial, la necesidad de información podría ser: encontrar información sobre el escape de los convictos de la prisión de Texas e información relacionada con su recuperación. Las preguntas asociadas de nivel inferior podrían ser: 1. ¿Cuántos prisioneros escaparon?2. ¿Dónde y cuándo fueron vistos?3. ¿Quiénes son sus contactos conocidos dentro y fuera de la prisión?4. ¿Cómo están armados?5. ¿Tienen algún vehículo?6. ¿Qué pasos se han tomado hasta ahora? Llamamos a tal información que necesita una tarea y las preguntas asociadas como consultas en esta tarea. Se supone que un sistema de destilación monitorea los documentos entrantes, los procesa por fragmento en un orden temporal, seleccione pasajes potencialmente relevantes y novedosos de cada fragmento con respecto a cada consulta y presente una lista clasificada de pasajes al usuario. La clasificación de pasaje aquí se basa en cuán relevante es un pasaje con respecto a la consulta actual, cuán novedosa es con respecto al historial actual del usuario (de sus interacciones con el sistema) y cuán redundante es en comparación con otros pasajes conun rango más alto en la lista. Cuando se le presenta una lista de pasajes, el usuario puede proporcionar comentarios al resaltar los tramos arbitrarios de texto que él o ella encontró relevante. Estos tramos de texto se toman como ejemplos positivos en la adaptación del perfil de consulta, y también se agregan al historial de usuarios. Los pasajes no marcados por el usuario se toman como ejemplos negativos. Tan pronto como se actualiza el perfil de consulta, el sistema vuelve a publicar una búsqueda y devuelve otra lista clasificada de pasajes donde los pasajes previamente vistos se eliminan o se clasifican bajo, según la preferencia del usuario. Por ejemplo, si el usuario destaca ... Los funcionarios han publicado una recompensa de $ 100,000 por su captura ... como respuesta relevante a la consulta ¿Qué pasos se han tomado hasta ahora?, Entonces la pieza resaltada se usa como un ejemplo de entrenamiento positivo adicional enLa adaptación del perfil de consulta. Esta retroalimentación también se agrega al historial del usuario como un ejemplo visto, de modo que en el futuro, el sistema no colocará otro pasaje que mencione la recompensa de $ 100,000 en la parte superior de la lista clasificada. Sin embargo, un artículo que menciona ... los funcionarios han duplicado el dinero de la recompensa a $ 200,000 ... podría clasificarse alto, ya que es relevante para el perfil de consulta (actualizado) y novedoso con respecto al historial de usuarios (actualizado). El usuario puede modificar las consultas originales o agregar una nueva consulta durante el proceso;Los perfiles de consulta se cambiarán en consecuencia. Claramente, la detección de novedad es muy importante para la utilidad de dicho sistema debido a la búsqueda iterativa. Sin detección de novedad, los viejos pasajes relevantes se mostrarían al usuario repetidamente en cada lista clasificada. A través del ejemplo anterior, podemos ver las principales propiedades de nuestro nuevo marco para la destilación de información basada en servicios públicos sobre documentos ordenados temporalmente. Nuestro marco combina y extiende el poder del filtrado adaptativo (AF), la recuperación ad-hoc (IR) y la detección de novedad (ND). En comparación con el estándar IR, nuestro enfoque tiene el poder de aprender incrementalmente las necesidades de información a largo plazo y modelar una secuencia de consultas dentro de una tarea. En comparación con la FA convencional, permite un papel más activo del usuario en la refinación de sus necesidades de información y solicitar nuevos resultados al permitir la relevancia y la retroalimentación de novedad mediante el resaltado de los tramos arbitrarios de texto en los pasajes devueltos por el sistema. En comparación con el trabajo pasado, esta es la primera evaluación de la detección de novedad integrada con el filtrado adaptativo para consultas secuenciadas que permite la retroalimentación flexible de los usuarios sobre los pasajes clasificados. La combinación de AF, IR y ND con las nuevas extensiones plantea una importante pregunta de investigación sobre la metodología de evaluación: ¿Cómo podemos medir la utilidad de dicho sistema de destilación de información? Las métricas existentes en IR, AF y ND estándar son insuficientes, y se deben explorar nuevas soluciones, como discutiremos en la Sección 4, después de describir los núcleos técnicos de nuestro sistema en la siguiente sección.3. Núcleos técnicos Los componentes centrales de CAF´e son - 1) AF para el aprendizaje incremental de los perfiles de consulta, 2) IR para estimar la relevancia de los pasajes con respecto a los perfiles de consulta, 3) para evaluar la novedad de los pasajes con respecto a la historia de los usuarios, y4) Componente anti-redundancia para eliminar la redundancia de las listas clasificadas.3.1 Componente de filtrado adaptativo Utilizamos un algoritmo de última generación en el campo: el método de regresión logística regularizada que tuvo los mejores resultados en varios corpus de evaluación de referencia para AF [21]. La regresión logística (LR) es un algoritmo de aprendizaje supervisado para la clasificación estadística. Basado en un conjunto de capacitación de instancias etiquetadas, aprende un modelo de clase que puede usarse para predecir las etiquetas de instancias invisibles. Su rendimiento, así como la eficiencia en términos de tiempo de capacitación, lo convierten en un buen candidato cuando se requieren actualizaciones frecuentes del modelo de clase, como es el caso en el filtrado adaptativo, donde el sistema debe aprender de cada nuevo retroalimentación proporcionado por el usuario.(Ver [21] y [23] para los problemas de complejidad y implementación computacional). En el filtrado adaptativo, cada consulta se considera como una clase y la probabilidad de un pasaje que pertenece a esta clase corresponde al grado de relevancia del pasaje con respecto a la consulta. Para capacitar al modelo, utilizamos la consulta en sí como el ejemplo de entrenamiento positivo inicial de la clase, y las piezas de texto altas del usuario (marcadas como relevantes o no relevantes) durante la retroalimentación como ejemplos de capacitación adicionales. Para abordar el problema de inicio en frío en la etapa inicial antes de obtener cualquier retroalimentación del usuario, el sistema utiliza una pequeña muestra de un corpus retrospectivo como los ejemplos negativos iniciales en el conjunto de capacitación. Los detalles del uso de la regresión logística para el filtrado adaptativo (asignando diferentes pesos a instancias de entrenamiento positivas y negativas, y regularizar la función objetivo para evitar el ajuste en exceso en los datos de entrenamiento) se presentan en [21]. El modelo de clase W ∗ aprendió por regresión logística, o el perfil de consulta, es un vector cuyas dimensiones son términos individuales y cuyos elementos son los coeficientes de regresión, lo que indica cuán influyente es cada término en el perfil de consulta. El perfil de consulta se actualiza cada vez que se recibe una nueva pieza de comentarios de los usuarios. Se puede aplicar un peso en descomposición temporal a cada ejemplo de capacitación, como una opción, para enfatizar los comentarios más recientes del usuario.3.2 Componente de recuperación de pasaje Utilizamos técnicas IR estándar en esta parte de nuestro sistema. Los documentos entrantes se procesan en fragmentos, donde cada fragmento se puede definir como un lapso de tiempo fijo o como un número fijo de documentos, según lo prefiere el usuario. Para cada documento entrante, se actualizan las estadísticas de Corpus como la IDF (frecuencia de documento invertida) de cada término. Utilizamos un identificador y rastreador de la entidad nombrado de última generación [8, 12] para identificar nombres de personas y ubicaciones, y fusionarlos con entidades con nombres referentes vistas en el pasado. Luego, los documentos se segmentan en pasajes, que pueden ser un documento completo, un párrafo, una oración o cualquier otro tramo continuo de texto, como se prefiere. Cada pasaje se representa utilizando un vector de pesos TF-IDF (frecuencia de documento de frecuencia de término), donde el término puede ser una palabra o una entidad con nombre. Dado un perfil de consulta, es decir, la solución de regresión logística w ∗ como se describe en la Sección 3.1, el sistema calcula la probabilidad posterior de relevancia para cada pasaje x como frl (x) ≡ p (y = 1 | x, w ∗) = 1 (1 + E - W ∗ · x) (1) Los pasajes se ordenan por sus puntajes de relevancia, y los que tienen puntajes por encima de un umbral (sintonizado en un conjunto de entrenamiento) comprenden la lista de relevancia que se transmite al paso de detección de novedad.3.3 El componente de detección de novedades Caf´e mantiene un historial de usuario H (t), que contiene todos los tramos de texto HI que el usuario destacó (como retroalimentación) durante sus interacciones pasadas con el sistema, hasta el tiempo actual t.Denotando la historia como h (t) = n h1, h2, ..., ht o, (2) El puntaje de novedad de un nuevo pasaje candidato X se calcula como: fnd (x) = 1 - max i∈1 ..t {cos (x, hi)} (3) donde ambos pasos candidatos X y los tramos resaltados de texto HI se representan como vectores TF-IDF. El puntaje de novedad de cada pasaje se compara con un umbral presespecificado (también sintonizado en un conjunto de entrenamiento), y cualquier pasaje con un puntaje por debajo de este umbral se elimina de la lista de relevancia.3.4 Componente de clasificación anti-redundante Aunque el componente de detección de novedad asegura que solo la información novedosa (previamente no vista) permanezca en la lista de relevancia, esta lista aún podría contener la misma información novedosa en múltiples posiciones en la lista clasificada. Supongamos, por ejemplo, que el usuario ya ha leído una recompensa de $ 100,000 por información sobre los convictos escapados. Una nueva noticia de que el premio se ha incrementado a $ 200,000 es novedoso ya que el usuario aún no ha leído al respecto. Sin embargo, múltiples fuentes de noticias informarían esta noticia y podríamos terminar mostrando artículos (redundantes) de todas estas fuentes en una lista clasificada. Por lo tanto, una lista clasificada también debe hacerse no redundante con respecto a su propio contenido. Utilizamos una versión simplificada del método de relevancia marginal máxima [5], desarrollada originalmente para combinar relevancia y novedad en la recuperación y resumen de texto. Nuestro procedimiento comienza con la lista actual de pasajes ordenados por relevancia (Sección 3.2), filtrado por el componente de detección de novedad (Sección 3.3), y genera una nueva lista no redundante de la siguiente manera: 1. Tome el mejor pasaje en la lista actual como el mejor en la nueva lista.2. Agregue el siguiente pasaje X en la lista actual a la nueva lista solo si le FAR (x)> t donde FAR (x) = 1 - max pi borrado {cos (x, pi)} y lnew es el conjunto de pasajes ya seleccionadosen la nueva lista.3. Repita el paso 2 hasta que se hayan examinado todos los pasajes en la lista actual. Después de aplicar el algoritmo mencionado anteriormente, cada pasaje en la nueva lista es suficientemente diferente a los demás, favoreciendo así la diversidad en lugar de la redundancia en la nueva lista clasificada. El umbral anti-redundancia T se ajusta en un conjunto de entrenamiento.4. Metodología de evaluación El enfoque que propusimos anteriormente para la destilación de información plantea problemas importantes con respecto a la metodología de evaluación. En primer lugar, dado que nuestro marco permite que la salida sea pasajes a diferentes niveles de granularidad (por ejemplo, ventanas de oración K donde K puede variar) en lugar de una longitud fija, no es posible tener juicios de relevancia pre-anotados en todos estos niveles de granularidad. En segundo lugar, dado que deseamos medir la utilidad de la salida del sistema como una combinación de relevancia y novedad, las medidas tradicionales basadas en relevancia deben ser reemplazadas por medidas que penalizan la repetición de la misma información en la salida del sistema a lo largo del tiempo. En tercer lugar, dado que la salida del sistema se clasifica en listas, debemos recompensar aquellos sistemas que presentan información útil (tanto relevante como previamente invisible) utilizando listas de clasificación más cortas, y penalizar aquellos que presentan la misma información utilizando listas de clasificación más largas. Ninguna de las medidas existentes en recuperación ad-hoc, filtrado adaptativo, detección de novedad u otras áreas relacionadas (resumen de texto y respuesta de preguntas) tienen propiedades deseables en los tres aspectos. Por lo tanto, debemos desarrollar una nueva metodología de evaluación.4.1 Claves de respuesta Para habilitar la evaluación de un sistema cuya salida consiste en pasajes de longitud arbitraria, tomamos prestado el concepto de claves de respuesta de la comunidad de respuesta (QA), donde los sistemas pueden devolver los tramos arbitrarios de texto como respuestas. Las claves de respuesta definen lo que deberían estar presentes en una respuesta del sistema para recibir crédito, y están compuestos por una recopilación de pepitas de información, es decir, unidades de factoides sobre las cuales los asesores humanos pueden tomar decisiones binarias de si una respuesta del sistema las contiene o no. Definir claves de respuesta y tomar las decisiones binarias asociadas son tareas conceptuales que requieren mapeo semántico [19], ya que los pasajes retornados del sistema pueden contener la misma información expresada de muchas maneras diferentes. Por lo tanto, las evaluaciones de control de calidad se han basado en los evaluadores humanos para el mapeo entre varias expresiones, lo que hace que el proceso consuma, lento y no sea escalable para grandes consultas y colecciones de documentos, y amplias evaluaciones del sistema con diversas configuraciones de parámetros.4.1.1 Automatización de la evaluación basada en las claves de respuestas Los métodos de evaluación automática permitirían una construcción y ajuste de sistemas más rápida, así como proporcionar una forma objetiva y asequible de comparar varios sistemas. Recientemente, tales métodos se han propuesto, más o menos, basados en la idea de las concurrencias de N-Gram. Pourpre [10] asigna una puntuación de recuperación fraccional a una respuesta del sistema basada en su superposición unigram con una descripción de Nuggets dada. Por ejemplo, una respuesta al sistema A B C tiene el recuerdo 3/4 con respecto a una pepita con la descripción A B C D. Sin embargo, dicho enfoque es injusto para los sistemas que presentan la misma información pero que usan palabras distintas a A, B, C y D.Otro problema abierto es cómo soportar las palabras individuales para medir la cercanía de una coincidencia. Por ejemplo, considere la pregunta cuántos prisioneros escaparon? En la pepita, siete prisioneros escaparon de una prisión de Texas, no hay indicios de que Seven sea la palabra clave, y que debe igualarse para obtener algún crédito de relevancia. El uso de valores de IDF no ayuda, ya que siete generalmente no tendrá una IDF más alta que las palabras como Texas y la prisión. Además, redefinir la pepita como solo siete no resuelve el problema, ya que ahora podría igualar espuradamente cualquier mención de siete fuera de contexto. Nuggeteer [13] trabaja en principios similares, pero toma decisiones binarias sobre si una pepita está presente en una respuesta del sistema dada al ajustar un umbral. Sin embargo, también está plagado de relevancia espuria, ya que no todas las palabras contenidas en la descripción de la pepita (o respuestas correctas conocidas) son fundamentales para la pepita.4.1. Estas reglas son esencialmente consultas booleanas que solo coincidirán con los fragmentos que contienen la pepita. Por ejemplo, ¿una regla candidata para que coincidan con las respuestas a cuántos prisioneros escaparon?es (Texas y siete y Escape y (convictos o prisioneros)), posiblemente con otros sinónimos y variantes en la regla. Para un corpus de artículos de noticias, que generalmente siguen una prosa formal típica, es bastante fácil escribir reglas tan simples para que coincida con las respuestas esperadas usando un enfoque de arranque, como se describe a continuación. Proponemos un enfoque de dos etapas, inspirado en AutoSlog [14], que combina la fuerza de los humanos en la identificación de expresiones semánticamente equivalentes y la fuerza del sistema en la recopilación de evidencia estadística de un corpus de documentos humannotados. En la primera etapa, los sujetos humanos anotaron (utilizando una herramienta de resaltado) porciones de documentos ottopic que contenían respuestas a cada pepita 2. En la segunda etapa, los sujetos utilizaron nuestra herramienta de generación de reglas para crear reglas que coincidan con las anotaciones para cada pepita. La herramienta permite a los usuarios ingresar una regla booleana como disyunción de conjunciones (por ejemplo, ((A y B) o (A y C y D) o (E))). Dada una regla candidata, nuestra herramienta la usa como una consulta booleana sobre todo el conjunto de documentos sobre el tema y calcula su retiro y precisión con respecto a las anotaciones que se espera que coincida. Por lo tanto, los sujetos pueden comenzar con una regla simple y refinarla iterativamente hasta que estén satisfechos con su retiro y precisión. Observamos que era muy fácil para los humanos mejorar la precisión de una regla ajustando sus conjunciones existentes (agregando más y) y mejorando el recuerdo agregando más conjunciones a la disyunción (agregando más OR). Como ejemplo, intentemos crear una regla para la pepita que dice que siete prisioneros escaparon de la prisión de Texas. Comenzamos con una regla simple - (siete). Cuando ingresamos esto en la herramienta de generación de reglas, nos damos cuenta de que esta regla coincide con muchos sucesos espurios de siete (por ejemplo ... siete estados ...) y, por lo tanto, obtiene una puntuación de baja precisión. Podemos calificar aún más nuestra regla: Texas y siete y convictos. Luego, al observar las anotaciones perdidas, nos damos cuenta de que algunos artículos de noticias mencionados ... Siete prisioneros escaparon ... Luego reemplazamos a los convictos con la disyunción (convictos o prisioneros). Continuamos ajustando la regla de esta manera hasta que logremos un recuerdo y precisión suficientemente altos, es decir, el (pequeño número de) fallas y falsas alarmas se pueden ignorar de manera segura. Por lo tanto, podemos crear reglas de coincidencia de pepitas que capturen sucintamente varias formas de expresar una pepita, mientras evitan las respuestas incorrectas (o fuera de contexto) coincidentes. La participación humana en el proceso de creación de reglas garantiza reglas genéricas de alta calidad que luego pueden usarse para evaluar las respuestas arbitrarias del sistema de manera confiable.4.2 Evaluación de la utilidad de una secuencia de listas clasificadas La utilidad de un sistema de recuperación puede definirse como la diferencia entre cuánto ganó el usuario en términos de información útil y cuánto perdió el usuario en términos de tiempo y energía. Calculamos esta utilidad a partir de los servicios públicos de pasajes individuales de la siguiente manera. Después de leer cada pasaje devuelto por el sistema, el usuario obtiene alguna ganancia dependiendo de la presencia de información relevante y novedosa, e incurre en una pérdida en términos del tiempo y la energía gastada en el pasaje. Sin embargo, la probabilidad de que el usuario realmente lea un pasaje depende de su posición en la lista clasificada. Por lo tanto, para una consulta Q, la utilidad esperada 2 LDC [18] ya proporciona juicios de relevancia para 100 temas en el corpus TDT4. Además, nos aseguramos de que estos juicios sean exhaustivos en todo el corpus utilizando la agrupación.de un pasaje Pi en rango se puede definir como u (pi, q) = p (i) ∗ (ganancia (pi, q) - pérdida (pi, q)) (4) donde p (i) es la probabilidad de queEl usuario pasaría por un pasaje en el rango I. La utilidad esperada para una lista completa de longitud n se puede calcular simplemente agregando la utilidad esperada de cada pasaje: u (q) = nx i = 1 p (i) ∗ (ganancia (pi, q) - pérdida (pi,q)) (5) Tenga en cuenta que si ignoramos el término de pérdida y definimos p (i) como p (i) ∝ 1/ logb (b + i - 1) (6), entonces obtenemos la métrica recientemente popularizada llamada ganancia acumulada con descuento(DCG) [9], donde la ganancia (Pi, Q) se define como la relevancia graduada del paso PI. Sin embargo, sin el término de pérdida, DCG es una métrica puramente de retiro y no es adecuado para una configuración de filtrado adaptativo, donde la utilidad de los sistemas depende en parte de su capacidad para limitar el número de elementos que se muestran al usuario. Aunque P (i) podría definirse en función de los estudios empíricos del comportamiento del usuario, por simplicidad, usamos P (i) exactamente como se define en la Ecuación 6. La ganancia G (PI, Q) del paso PI con respecto a la consulta Q es una función de - 1) el número de pepitas relevantes presentes en Pi, y 2) la novedad de cada una de estas pepitas. Combinamos estos dos factores de la siguiente manera. Para cada Nugget NJ, asignamos un peso inicial WJ, y también mantenemos un recuento de NJ del número de veces que el usuario ha visto esta pepita en el pasado. Se supone que la ganancia derivada de cada ocurrencia posterior de la misma pepita se reduce mediante un factor de amortiguación γ. Por lo tanto, G (Pi, Q) se define como G (Pi, Q) = x nj ∈C (PI, Q) WJ ∗ γnj (7) donde C (Pi, Q) es el conjunto de todas las pepitas que aparecen en el pasoPi y también pertenecen a la clave de respuesta de la consulta q. Los pesos iniciales de WJ están establecidos de BE 1.0 en nuestros experimentos, pero también se pueden establecer en base a un enfoque piramidal [11]. La elección del factor de amortiguación γ determina la tolerancia de los usuarios para la redundancia. Cuando γ = 0, una pepita solo recibirá crédito por su primera ocurrencia, es decir, cuando NJ es cero3. Para 0 <γ <1, una pepita recibe un crédito más pequeño por cada ocurrencia sucesiva. Cuando γ = 1, no se produce una amortiguación y las ocurrencias repetidas de una pepita reciben el mismo crédito. Tenga en cuenta que los recuentos de ocurrencia de la pepita se conservan entre la evaluación de listas clasificadas sucesivas devueltas por el sistema, ya que se espera que los usuarios recuerden lo que el sistema les mostró en el pasado. Definimos la pérdida L (PI, Q) como un costo constante C (usamos 0.1) incurrido al leer un pasaje retirado del sistema. Por lo tanto, nuestra métrica puede reescribirse como u (q) = nx i = 1 ganancia (pi, q) logb (b + i-1)-l (n) (8) donde l (n) es la pérdida asociadacon una lista clasificada de longitud n: l (n) = c · nx i = 1 1 logb (b + i - 1) (9) 3 Tenga en cuenta que 00 = 1 debido a la similitud con ganancia acumulada con descuento (DCG), nosotrosLlame a nuestra utilidad acumulada con descuento métrica (DCU). El puntaje DCU obtenido por el sistema se convierte en una puntuación de DCU normalizada (NDCU) dividiéndola mediante el puntaje DCU de la lista clasificada ideal, que se crea ordenando pasajes por sus puntajes de utilidad decrecientes u (Pi, Q) cuando se detiene cuandoU (pi, q) ≤ 0, es decir, cuando la ganancia es menor o igual al costo de leer el pasaje.5. Los datos TDT4 fueron el corpus de referencia utilizado en evaluaciones TDT2002 y TDT2003. El corpus consta de más de 90, 000 artículos de noticias de múltiples fuentes (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) publicado entre octubre de 2000 y enero de 2001, INLos idiomas del árabe, el inglés y el mandarín. También se proporcionaron versiones reconocidas por voz y traducidas a máquina de los artículos que no son ingleses. LDC [18] ha anotado el corpus con 100 temas, que corresponden a varios eventos de noticias en este período de tiempo. De estos, seleccionamos un subconjunto de 12 eventos procesables y definimos las tareas correspondientes para ellos4. Para cada tarea, definimos manualmente un perfil que consiste en un conjunto inicial de (5 a 10) consultas, una descripción de texto libre del historial del usuario, es decir, lo que el usuario ya sabe sobre el evento y una lista de información conocidaTema y documentos fuera del tema (si están disponibles) como ejemplos de capacitación. Para cada consulta, generamos claves de respuesta y reglas correspondientes de coincidencia de pepitas utilizando el procedimiento descrito en la Sección 4.1.2, y produjimos un total de 120 consultas, con un promedio de 7 pepitas por consulta.6. Experimentos y resultados 6.1 Basas de base Utilizamos Indri [17], un motor de recuperación basado en el modelo de lenguaje popular, como línea de base para comparar con CAF'E. Indri admite la funcionalidad de los motores de búsqueda estándar, incluida la retroalimentación de pseudo-relevancia (PRF) [3, 6], y es representativo de un sistema de recuperación típico basado en la consulta. Indri no admite ningún tipo de detección de novedad. Comparamos Indri con PRF activado y apagado, con Cafí con retroalimentación de usuarios, detección de novedad y clasificación antiredenunda encendida y apagada.6.2 Configuración experimental Dividimos el corpus TDT4 que abarca 4 meses en 10 trozos, cada uno definido como un período de 12 días consecutivos. En cualquier momento dado en el proceso de destilación, cada sistema accedió a los datos anteriores hasta el punto actual y devolvió una lista clasificada de 50 pasajes por consulta. Las 12 tareas definidas en el corpus se dividieron en un entrenamiento y prueba con 6 tareas cada una. Se permitió a cada sistema usar el conjunto de capacitación para ajustar sus parámetros para optimizar la NDCU (Ecuación 8), incluido el umbral de relevancia tanto para Indri como para CAF´e, y los umbrales de novedad y antironanciosos para CAF´E. El NDCU para cada ejecución del sistema se calcula automáticamente. También se simularon la retroalimentación del usuario: los juicios de relevancia para cada pasaje retornado del sistema (según lo determinado por las reglas de coincidencia de Nugget descritas en la Sección 4.1.2) fueron 4 URL: http://nyc.lti.cs.cmu.edu/downloads Figura 1: Rendimiento de Indri a través de fragmentos Figura 2: Rendimiento de CAF'E en fragmentos utilizados como retroalimentación de usuarios en la adaptación de los perfiles de consulta e historias de usuarios.6.3 Resultados en la Tabla 1, mostramos las puntuaciones de NDCU de los dos sistemas en diversos configuraciones. Estos puntajes se promedian sobre las seis tareas en el conjunto de pruebas, y se calculan con dos factores de amortiguación (ver Sección 4.2): γ = 0 y 0.1, para simular sin tolerancia y pequeña tolerancia para la redundancia, respectivamente. Usar γ = 0 crea una métrica mucho más estricta, ya que no da ningún crédito a un pasaje que contenga información relevante pero redundante. Por lo tanto, la mejora obtenida de habilitar la retroalimentación del usuario es menor con γ = 0 que la mejora obtenida de la retroalimentación con γ = 0.1. Esto revela una deficiencia de los sistemas de recuperación contemporáneos que el usuario da retroalimentación positiva sobre un pasaje, los sistemas ofrecen mayores pesos a los términos presentes en ese pasaje y tienden a recuperar otros pasajes que contienen los mismos términos, y por lo tanto, generalmente la misma información. Sin embargo, el usuario no se beneficia al ver tales pasajes redundantes, y generalmente está interesado en otros pasajes que contienen información relacionada. Es informativo evaluar los sistemas de recuperación utilizando nuestra medida de utilidad (con γ = 0) que explica la novedad y, por lo tanto, ofrece una imagen más realista de qué tan bien un sistema puede generalizarse a partir de la retroalimentación del usuario, en lugar de usar medidas IR tradicionales como el retiro y la precisión queDé una imagen incompleta de mejora obtenida de los comentarios de los usuarios. A veces, sin embargo, los usuarios pueden estar interesados en ver la misma información de múltiples fuentes, como una Tabla 1: puntajes NDCU de Indri y Caf´e para dos factores de amortiguación (γ) y diversos configuraciones (F: retroalimentación, N: novedadDetección, A: Ranking Anti-Redundante) Indri Caf´e γ Base +Base PRF +F +F +N +F +A +F +N +A 0 0.19 0.19 0.22 0.23 0.24 0.24 0.24 0.1 0.28 0.29 0.24 0.35 0.35 0.36 0.36indicador de su importancia o confiabilidad. En tal caso, simplemente podemos elegir un valor más alto para γ que corresponde a una mayor tolerancia a la redundancia y, por lo tanto, dejar que el sistema sintonice sus parámetros en consecuencia. Dado que los documentos fueron procesados por un fragmento, sería interesante ver cómo el rendimiento de los sistemas mejora con el tiempo. Las Figuras 1 y 2 muestran las tendencias de rendimiento para ambos sistemas a través de fragmentos. Si bien se espera que el rendimiento con y sin retroalimentación sobre los primeros fragmentos esté cerca, para los fragmentos posteriores, la curva de rendimiento con retroalimentación habilitada se eleva por encima de la que tiene la configuración sin retroalimentación. Las tendencias de rendimiento no son consistentes en todos los trozos porque los documentos sobre el tema no se distribuyen de manera uniforme sobre todos los trozos, lo que hace que algunas consultas sean más fáciles que otras en ciertos fragmentos. Además, dado que Indri usa la retroalimentación de pseudo-relevancia, mientras que CAF'E utiliza la retroalimentación basada en juicios de relevancia reales, la mejora en el caso de Indri es menos dramática que la de CAF'E.7. Observaciones finales Este documento presenta la primera investigación sobre la destilación de información basada en servicios públicos con un sistema que aprende las necesidades de información de larga duración de los comentarios de los usuarios de grano fino sobre una secuencia de pasajes clasificados. Nuestro sistema, llamado CAF´E, combina filtrado adaptativo, detección de novedad y clasificación de pasaje antiredado en un marco unificado para la optimización de servicios públicos. Desarrollamos un nuevo esquema para la evaluación automatizada y la retroalimentación basada en un procedimiento semiautomático para adquirir reglas que permiten que coincidan automáticamente las pepitas con las respuestas del sistema. También propusimos una extensión de la métrica NDCG para evaluar la utilidad de los pasajes clasificados como una combinación ponderada de relevancia y novedad. Nuestros experimentos en el Corpus de referencia TDT4 recientemente anotado muestran una mejora de servicios públicos alentador sobre Indri, y también sobre nuestro propio sistema con aprendizaje incremental y detección de novedad desactivadas.8. Agradecimientos Nos gustaría agradecer a Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng y el Programa de Análisis de Datos Qualitativos en la Universidad de Pittsburgh, dirigido por el Dr. Stuart Shulman por su ayuda con la recopilación y el procesamiento de las anotaciones TDT4 extendidas utilizadas en nuestros experimentos. Este trabajo es apoyado en piezas por la National Science Foundation (NSF) bajo la subvención IIS0434035, y la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo contratos NBCHD030010 y W0550432. Cualquier opinión, hallazgos, conclusiones o recomendaciones expresadas en este material son las de los autores y no reflejan necesariamente las opiniones de los patrocinadores.9. Autores adicionales Jian Zhang (jianzhan@stat.purdue.edu) ∗, Jaime Carbonell (jgc@cs.cmu.edu) †, Peter Brusilovsky (peterb+@pitt.edu) ‡, Daqing He (dah44@pitt.edu) ‡ 10. Referencias [1] J. Allan. Comentarios de relevancia incremental para el filtrado de información. Actas de la 19a Conferencia Anual de ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 270-278, 1996. [2] J. Allan, C. Wade y A. Bolítar. Recuperación y detección de novedad a nivel de oración. Actas de la Conferencia ACM Sigir sobre investigación y desarrollo en recuperación de información, 2003. [3] C. Buckley, G. Salton y J. Allan. Recuperación automática con información de localidad utilizando SMART. Publicación especial de NIST, (500207): 59-72, 1993. [4] J. Callan. Aprendiendo mientras filtra documentos. Actas de la 21a Conferencia Anual de ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 224-231, 1998. [5] J. Carbonell y J. Goldstein. El uso de MMR, Reranking basado en la diversidad para reordenar documentos y producir resúmenes. Actas de la 21a Conferencia Anual de ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 335-336, 1998. [6] E. Efthimiadis. Expansión de consulta. Revisión anual de la Ciencia y Tecnología de la Información (ARIST), 31: P121-87, 1996. [7] J. Fiscus y G. Duddington. Detección de temas y descripción general de seguimiento. Detección y seguimiento de temas: Organización de información basada en eventos, páginas 17-31.[8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov y S. Roukos. Un modelo estadístico para la detección y seguimiento de entidades multilingües. NAACL/HLT, 2004. [9] K. J¨arvelin y J. Kek¨al¨ainen. Evaluación basada en ganancias acumuladas de técnicas IR. Transacciones ACM en Sistemas de Información (TOI), 20 (4): 422-446, 2002. [10] J. Lin y D. Demner-Fushman. Evaluar automáticamente las respuestas a las preguntas de definición. Actas de la Conferencia de Tecnología del Lenguaje Humano de 2005 y Conferencia sobre Métodos Empíricos en Procesamiento del Lenguaje Natural (HLT/EMNLP 2005), 2005. ∗ Estadística Departamento, Universidad de Purdue, West Lafayette, EE. UU. † Idiomas Technologies Inst., Universidad Carnegie Mellon, Pittsburgh, Pittsburgh,USA ‡ Escuela de Ciencias de la Información, Univ.de Pittsburgh, Pittsburgh, EE. UU. [11] J. Lin y D. Demner-Fushman. Will Pyramids construidas de pepitas se derrumba. Actas de HLT-Naacl, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla y S. Roukos. Un algoritmo de resolución de coreferencia sincrónica de mención basado en el campanario. Proc.de ACL, 4: 136-143, 2004. [13] G. Marton. Nuggeteer: evaluación automática basada en pepitas utilizando descripciones y juicios. HLT/NAACL, 2006. [14] E. Riloff. Construyendo automáticamente un diccionario para tareas de extracción de información. Actas de la undécima Conferencia Nacional sobre Inteligencia Artificial, páginas 811-816, 1993. [15] S. Robertson y S. Walker. Microsoft Cambridge en TREC-9: pista de filtrado. La Novena Conferencia de Recuperación de Textos (TREC-9), páginas 361-368.[16] R. Schapire, Y. Cantante y A. Singhal. Aumento y rocchio aplicado al filtrado de texto. Actas de la 21a Conferencia Anual de ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle y W. Croft. Indri: un motor de búsqueda basado en modelos de idiomas para consultas complejas. Actas de la Conferencia Internacional sobre Análisis de Inteligencia, 2004. [18] El consorcio de datos lingüísticos.http://www.ldc.upenn.edu/.[19] E. Voorhees. Descripción general de la pista de respuesta a la pregunta TREC 2003. Actas de la duodécima Conferencia de Recuperación de Textos (TREC 2003), 2003. [20] Y. Yang y B. Kisiel. Regresión local basada en el margen para el filtrado adaptativo. Actas de la Duodécima Conferencia Internacional sobre Gestión de Información y Conocimiento, páginas 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang y B. Kisiel. Robustez de los métodos de filtrado adaptativo en una evaluación de bencillo. Actas de la 28ª Conferencia Anual de ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 98-105, 2005. [22] C. Zhai, W. Cohen y J. Lafferty. Más allá de la relevancia independiente: métodos y métricas de evaluación para la recuperación subtópica. Actas de la 26ª Conferencia Internacional de ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 10-17, 2003. [23] J. Zhang e Y. Yang. Robustez de los métodos de clasificación lineal regularizados en la categorización de texto. Actas de la 26ª Conferencia Anual de ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 190-197, 2003. [24] Y. Zhang. Uso de antecedentes bayesianos para combinar clasificadores para el filtrado adaptativo. Actas de la 27ª Conferencia Internacional Anual de Investigación y Desarrollo en Recuperación de Información, páginas 345-352, 2004. [25] Y. Zhang, J. Callan y T. Minka. Detección de novedad y redundancia en filtrado adaptativo. Actas de la 25ª Conferencia Anual de ACM Sigir sobre investigación y desarrollo en recuperación de información, 2002.",
    "original_sentences": [
        "Utility-based Information Distillation Over Temporally Sequenced Documents Yiming Yang Language Technologies Inst.",
        "Carnegie Mellon University Pittsburgh, USA yiming@cs.cmu.edu Abhimanyu Lad Language Technologies Inst.",
        "Carnegie Mellon University Pittsburgh, USA alad@cs.cmu.edu Ni Lao Language Technologies Inst.",
        "Carnegie Mellon University Pittsburgh, USA nlao@cs.cmu.edu Abhay Harpale Language Technologies Inst.",
        "Carnegie Mellon University Pittsburgh, USA aharpale@cs.cmu.edu Bryan Kisiel Language Technologies Inst.",
        "Carnegie Mellon University Pittsburgh, USA bkisiel@cs.cmu.edu Monica Rogati Language Technologies Inst.",
        "Carnegie Mellon University Pittsburgh, USA mrogati@cs.cmu.edu ABSTRACT This paper examines a new approach to information distillation over temporally ordered documents, and proposes a novel evaluation scheme for such a framework.",
        "It combines the strengths of and extends beyond conventional adaptive filtering, novelty detection and non-redundant passage ranking with respect to long-lasting information needs (tasks with multiple queries).",
        "Our approach supports fine-grained user feedback via highlighting of arbitrary spans of text, and leverages such information for utility optimization in adaptive settings.",
        "For our experiments, we defined hypothetical tasks based on news events in the TDT4 corpus, with multiple queries per task.",
        "Answer keys (nuggets) were generated for each query and a semiautomatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses.",
        "We also propose an extension of the NDCG metric for assessing the utility of ranked passages as a combination of relevance and novelty.",
        "Our results show encouraging utility enhancements using the new approach, compared to the baseline systems without incremental learning or the novelty detection components.",
        "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 General Terms Design, Measurement, Performance, Experimentation. 1.",
        "INTRODUCTION Tracking new and relevant information from temporal data streams for users with long-lasting needs has been a challenging research topic in information retrieval.",
        "Adaptive filtering (AF) is one such task of online prediction of the relevance of each new document with respect to pre-defined topics.",
        "Based on the initial query and a few positive examples (if available), an AF system maintains a profile for each such topic of interest, and constantly updates it based on feedback from the user.",
        "The incremental learning nature of AF systems makes them more powerful than standard search engines that support ad-hoc retrieval (e.g.",
        "Google and Yahoo) in terms of finding relevant information with respect to long-lasting topics of interest, and more attractive for users who are willing to provide feedback to adapt the system towards their specific information needs, without having to modify their queries manually.",
        "A variety of supervised learning algorithms (Rocchio-style classifiers, Exponential-Gaussian models, local regression and logistic regression approaches) have been studied for adaptive settings, examined with explicit and implicit relevance feedback, and evaluated with respect to utility optimization on large benchmark data collections in TREC (Text Retrieval Conferences) and TDT (Topic Detection and Tracking) forums [1, 4, 7, 15, 16, 20, 24, 23].",
        "Regularized logistic regression [21] has been found representative for the state-of-the-art approaches, and highly efficient for frequent model adaptations over large document collections such as the TREC-10 corpus (over 800,000 documents and 84 topics).",
        "Despite substantial achievements in recent adaptive filtering research, significant problems remain unsolved regarding how to leverage user feedback effectively and efficiently.",
        "Specifically, the following issues may seriously limit the true utility of AF systems in real-world applications: 1.",
        "User has a rather passive role in the conventional adaptive filtering setup - he or she reacts to the system only when the system makes a yes decision on a document, by confirming or rejecting that decision.",
        "A more active alternative would be to allow the user to issue multiple queries for a topic, review a ranked list of candidate documents (or passages) per query, and provide feedback on the ranked list, thus refining their information need and requesting updated ranked lists.",
        "The latter form of user interaction has been highly effective in standard retrieval for ad-hoc queries.",
        "How to deploy such a strategy for long-lasting information needs in AF settings is an open question for research. 2.",
        "The unit for receiving a relevance judgment (yes or no) is restricted to the document level in conventional AF.",
        "However, a real user may be willing to provide more informative, fine-grained feedback via highlighting some pieces of text in a retrieved document as relevant, instead of labeling the entire document as relevant.",
        "Effectively leveraging such fine-grained feedback could substantially enhance the quality of an AF system.",
        "For this, we need to enable supervised learning from labeled pieces of text of arbitrary span instead of just allowing labeled documents. 3.",
        "System-selected documents are often highly redundant.",
        "A major news event, for example, would be reported by multiple sources repeatedly for a while, making most of the information content in those articles redundant with each other.",
        "A conventional AF system would select all these redundant news stories for user feedback, wasting the users time while offering little gain.",
        "Clearly, techniques for novelty detection can help in principle [25, 2, 22] for improving the utility of the AF systems.",
        "However, the effectiveness of such techniques at passage level to detect novelty with respect to users (fine-grained) feedback and to detect redundancy in ranked lists remains to be evaluated using a measure of utility that mimics the needs of a real user.",
        "To address the above limitations of current AF systems, we propose and examine a new approach in this paper, combining the strengths of conventional AF (incremental learning of topic models), multi-pass passage retrieval for long-lasting queries conditioned on topic, and novelty detection for removal of redundancy from user interactions with the system.",
        "We call the new process utility-based information distillation.",
        "Note that conventional benchmark corpora for AF evaluations, which have relevance judgments at the document level and do not define tasks with multiple queries, are insufficient for evaluating the new approach.",
        "Therefore, we extended a benchmark corpus - the TDT4 collection of news stories and TV broadcasts - with task definitions, multiple queries per task, and answer keys per query.",
        "We have conducted our experiments on this extended TDT4 corpus and have made the additionally generated data publicly available for future comparative evaluations 1 .",
        "To automatically evaluate the system-returned arbitrary spans of text using our answer keys, we further developed an evaluation scheme with semi-automatic procedure for 1 URL: http://nyc.lti.cs.cmu.edu/downloads acquiring rules that can match nuggets against system responses.",
        "Moreover, we propose an extension of NDCG (Normalized Discounted Cumulated Gain) [9] for assessing the utility of ranked passages as a function of both relevance and novelty.",
        "The rest of this paper is organized as follows.",
        "Section 2 outlines the information distillation process with a concrete example.",
        "Section 3 describes the technical cores of our system called CAF´E - CMU Adaptive Filtering Engine.",
        "Section 4 discusses issues with respect to evaluation methodology and proposes a new scheme.",
        "Section 5 describes the extended TDT4 corpus.",
        "Section 6 presents our experiments and results.",
        "Section 7 concludes the study and gives future perspectives. 2.",
        "A SAMPLE TASK Consider a news event - the escape of seven convicts from a Texas prison in December 2000 and their capture a month later.",
        "Assuming a user were interested in this event since its early stage, the information need could be: Find information about the escape of convicts from Texas prison, and information related to their recapture.",
        "The associated lower-level questions could be: 1.",
        "How many prisoners escaped? 2.",
        "Where and when were they sighted? 3. Who are their known contacts inside and outside the prison? 4.",
        "How are they armed? 5.",
        "Do they have any vehicles? 6.",
        "What steps have been taken so far?",
        "We call such an information need a task, and the associated questions as the queries in this task.",
        "A distillation system is supposed to monitor the incoming documents, process them chunk by chunk in a temporal order, select potentially relevant and novel passages from each chunk with respect to each query, and present a ranked list of passages to the user.",
        "Passage ranking here is based on how relevant a passage is with respect to the current query, how novel it is with respect to the current user history (of his or her interactions with the system), and how redundant it is compared to other passages with a higher rank in the list.",
        "When presented with a list of passages, the user may provide feedback by highlighting arbitrary spans of text that he or she found relevant.",
        "These spans of text are taken as positive examples in the adaptation of the query profile, and also added to the users history.",
        "Passages not marked by the user are taken as negative examples.",
        "As soon as the query profile is updated, the system re-issues a search and returns another ranked list of passages where the previously seen passages are either removed or ranked low, based on user preference.",
        "For example, if the user highlights ...officials have posted a $100,000 reward for their capture... as relevant answer to the query What steps have been taken so far?, then the highlighted piece is used as an additional positive training example in the adaptation of the query profile.",
        "This piece of feedback is also added to the user history as a seen example, so that in future, the system will not place another passage mentioning $100,000 reward at the top of the ranked list.",
        "However, an article mentioning ...officials have doubled the reward money to $200,000... might be ranked high since it is both relevant to the (updated) query profile and novel with respect to the (updated) user history.",
        "The user may modify the original queries or add a new query during the process; the query profiles will be changed accordingly.",
        "Clearly, novelty detection is very important for the utility of such a system because of the iterative search.",
        "Without novelty detection, the old relevant passages would be shown to the user repeatedly in each ranked list.",
        "Through the above example, we can see the main properties of our new framework for utility-based information distillation over temporally ordered documents.",
        "Our framework combines and extends the power of adaptive filtering (AF), ad-hoc retrieval (IR) and novelty detection (ND).",
        "Compared to standard IR, our approach has the power of incrementally learning long-term information needs and modeling a sequence of queries within a task.",
        "Compared to conventional AF, it enables a more active role of the user in refining his or her information needs and requesting new results by allowing relevance and novelty feedback via highlighting of arbitrary spans of text in passages returned by the system.",
        "Compared to past work, this is the first evaluation of novelty detection integrated with adaptive filtering for sequenced queries that allows flexible user feedback over ranked passages.",
        "The combination of AF, IR and ND with the new extensions raises an important research question regarding evaluation methodology: how can we measure the utility of such an information distillation system?",
        "Existing metrics in standard IR, AF and ND are insufficient, and new solutions must be explored, as we will discuss in Section 4, after describing the technical cores of our system in the next section. 3.",
        "TECHNICAL CORES The core components of CAF´E are - 1) AF for incremental learning of query profiles, 2) IR for estimating relevance of passages with respect to query profiles, 3) ND for assessing novelty of passages with respect to users history, and 4) anti-redundancy component to remove redundancy from ranked lists. 3.1 Adaptive Filtering Component We use a state-of-the-art algorithm in the field - the regularized logistic regression method which had the best results on several benchmark evaluation corpora for AF [21].",
        "Logistic regression (LR) is a supervised learning algorithm for statistical classification.",
        "Based on a training set of labeled instances, it learns a class model which can then by used to predict the labels of unseen instances.",
        "Its performance as well as efficiency in terms of training time makes it a good candidate when frequent updates of the class model are required, as is the case in adaptive filtering, where the system must learn from each new feedback provided by the user. (See [21] and [23] for computational complexity and implementation issues).",
        "In adaptive filtering, each query is considered as a class and the probability of a passage belonging to this class corresponds to the degree of relevance of the passage with respect to the query.",
        "For training the model, we use the query itself as the initial positive training example of the class, and the user-highlighted pieces of text (marked as Relevant or Not-relevant) during feedback as additional training examples.",
        "To address the cold start issue in the early stage before any user feedback is obtained, the system uses a small sample from a retrospective corpus as the initial negative examples in the training set.",
        "The details of using logistic regression for adaptive filtering (assigning different weights to positive and negative training instances, and regularizing the objective function to prevent over-fitting on training data) are presented in [21].",
        "The class model w∗ learned by Logistic Regression, or the query profile, is a vector whose dimensions are individual terms and whose elements are the regression coefficients, indicating how influential each term is in the query profile.",
        "The query profile is updated whenever a new piece of user feedback is received.",
        "A temporally decaying weight can be applied to each training example, as an option, to emphasize the most recent user feedback. 3.2 Passage Retrieval Component We use standard IR techniques in this part of our system.",
        "Incoming documents are processed in chunks, where each chunk can be defined as a fixed span of time or as a fixed number of documents, as preferred by the user.",
        "For each incoming document, corpus statistics like the IDF (Inverted Document Frequency) of each term are updated.",
        "We use a state-of-the-art named entity identifier and tracker [8, 12] to identify person and location names, and merge them with co-referent named entities seen in the past.",
        "Then the documents are segmented into passages, which can be a whole document, a paragraph, a sentence, or any other continuous span of text, as preferred.",
        "Each passage is represented using a vector of TF-IDF (Term FrequencyInverse Document Frequency) weights, where term can be a word or a named entity.",
        "Given a query profile, i.e. the logistic regression solution w∗ as described in Section 3.1, the system computes the posterior probability of relevance for each passage x as fRL(x) ≡ P(y = 1|x, w∗ ) = 1 (1 + e−w∗·x) (1) Passages are ordered by their relevance scores, and the ones with scores above a threshold (tuned on a training set) comprise the relevance list that is passed on to the novelty detection step. 3.3 Novelty Detection Component CAF´E maintains a user history H(t), which contains all the spans of text hi that the user highlighted (as feedback) during his or her past interactions with the system, up to the current time t. Denoting the history as H(t) = n h1, h2, ..., ht o , (2) the novelty score of a new candidate passage x is computed as: fND(x) = 1 − max i∈1..t {cos(x, hi)} (3) where both candidate passage x and highlighted spans of text hi are represented as TF-IDF vectors.",
        "The novelty score of each passage is compared to a prespecified threshold (also tuned on a training set), and any passage with a score below this threshold is removed from the relevance list. 3.4 Anti-redundant Ranking Component Although the novelty detection component ensures that only novel (previously unseen) information remains in the relevance list, this list might still contain the same novel information at multiple positions in the ranked list.",
        "Suppose, for example, that the user has already read about a $100,000 reward for information about the escaped convicts.",
        "A new piece of news that the award has been increased to $200,000 is novel since the user hasnt read about it yet.",
        "However, multiple news sources would report this news and we might end up showing (redundant) articles from all these sources in a ranked list.",
        "Hence, a ranked list should also be made non-redundant with respect to its own contents.",
        "We use a simplified version of the Maximal Marginal Relevance method [5], originally developed for combining relevance and novelty in text retrieval and summarization.",
        "Our procedure starts with the current list of passages sorted by relevance (section 3.2), filtered by Novelty Detection component (section 3.3), and generates a new non-redundant list as follows: 1.",
        "Take the top passage in the current list as the top one in the new list. 2.",
        "Add the next passage x in the current list to the new list only if fAR(x) > t where fAR(x) = 1 − max pi∈Lnew {cos(x, pi)} and Lnew is the set of passages already selected in the new list. 3.",
        "Repeat step 2 until all the passages in the current list have been examined.",
        "After applying the above-mentioned algorithm, each passage in the new list is sufficiently dissimilar to others, thus favoring diversity rather than redundancy in the new ranked list.",
        "The anti-redundancy threshold t is tuned on a training set. 4.",
        "EVALUATION METHODOLOGY The approach we proposed above for information distillation raises important issues regarding evaluation methodology.",
        "Firstly, since our framework allows the output to be passages at different levels of granularity (e.g. k-sentence windows where k may vary) instead of a fixed length, it is not possible to have pre-annotated relevance judgments at all such granularity levels.",
        "Secondly, since we wish to measure the utility of the system output as a combination of both relevance and novelty, traditional relevance-only based measures must be replaced by measures that penalize the repetition of the same information in the system output across time.",
        "Thirdly, since the output of the system is ranked lists, we must reward those systems that present useful information (both relevant and previously unseen) using shorter ranked lists, and penalize those that present the same information using longer ranked lists.",
        "None of the existing measures in ad-hoc retrieval, adaptive filtering, novelty detection or other related areas (text summarization and question answering) have desirable properties in all the three aspects.",
        "Therefore, we must develop a new evaluation methodology. 4.1 Answer Keys To enable the evaluation of a system whose output consists of passages of arbitrary length, we borrow the concept of answer keys from the Question Answering (QA) community, where systems are allowed to return arbitrary spans of text as answers.",
        "Answer keys define what should be present in a system response to receive credit, and are comprised of a collection of information nuggets, i.e. factoid units about which human assessors can make binary decisions of whether or not a system response contains them.",
        "Defining answer keys and making the associated binary decisions are conceptual tasks that require semantic mapping [19], since system-returned passages can contain the same information expressed in many different ways.",
        "Hence, QA evaluations have relied on human assessors for the mapping between various expressions, making the process costly, time consuming, and not scalable to large query and document collections, and extensive system evaluations with various parameter settings. 4.1.1 Automating Evaluation based on Answer Keys Automatic evaluation methods would allow for faster system building and tuning, as well as provide an objective and affordable way of comparing various systems.",
        "Recently, such methods have been proposed, more or less, based on the idea of n-gram co-occurrences.",
        "Pourpre [10] assigns a fractional recall score to a system response based on its unigram overlap with a given nuggets description.",
        "For example, a system response A B C has recall 3/4 with respect to a nugget with description A B C D. However, such an approach is unfair to systems that present the same information but using words other than A, B, C, and D. Another open issue is how to weight individual words in measuring the closeness of a match.",
        "For example, consider the question How many prisoners escaped?.",
        "In the nugget Seven prisoners escaped from a Texas prison, there is no indication that seven is the keyword, and that it must be matched to get any relevance credit.",
        "Using IDF values does not help, since seven will generally not have a higher IDF than words like texas and prison.",
        "Also, redefining the nugget as just seven does not solve the problem since now it might spuriously match any mention of seven out of context.",
        "Nuggeteer [13] works on similar principles but makes binary decisions about whether a nugget is present in a given system response by tuning a threshold.",
        "However, it is also plagued by spurious relevance since not all words contained in the nugget description (or known correct responses) are central to the nugget. 4.1.2 Nugget-Matching Rules We propose a reliable automatic method for determining whether a snippet of text contains a given nugget, based on nugget-matching rules, which are generated using a semiautomatic procedure explained below.",
        "These rules are essentially Boolean queries that will only match against snippets that contain the nugget.",
        "For instance, a candidate rule for matching answers to How many prisoners escaped? is (Texas AND seven AND escape AND (convicts OR prisoners)), possibly with other synonyms and variants in the rule.",
        "For a corpus of news articles, which usually follow a typical formal prose, it is fairly easy to write such simple rules to match expected answers using a bootstrap approach, as described below.",
        "We propose a two-stage approach, inspired by Autoslog [14], that combines the strength of humans in identifying semantically equivalent expressions and the strength of the system in gathering statistical evidence from a humanannotated corpus of documents.",
        "In the first stage, human subjects annotated (using a highlighting tool) portions of ontopic documents that contained answers to each nugget 2 .",
        "In the second stage, subjects used our rule generation tool to create rules that would match the annotations for each nugget.",
        "The tool allows users to enter a Boolean rule as a disjunction of conjunctions (e.g. ((a AND b) OR (a AND c AND d) OR (e))).",
        "Given a candidate rule, our tool uses it as a Boolean query over the entire set of on-topic documents and calculates its recall and precision with respect to the annotations that it is expected to match.",
        "Hence, the subjects can start with a simple rule and iteratively refine it until they are satisfied with its recall and precision.",
        "We observed that it was very easy for humans to improve the precision of a rule by tweaking its existing conjunctions (adding more ANDs), and improving the recall by adding more conjunctions to the disjunction (adding more ORs).",
        "As an example, lets try to create a rule for the nugget which says that seven prisoners escaped from the Texas prison.",
        "We start with a simple rule - (seven).",
        "When we input this into the rule generation tool, we realize that this rule matches many spurious occurrences of seven (e.g. ...seven states...) and thus gets a low precision score.",
        "We can further qualify our rule - Texas AND seven AND convicts.",
        "Next, by looking at the missed annotations, we realize that some news articles mentioned ...seven prisoners escaped.... We then replace convicts with the disjunction (convicts OR prisoners).",
        "We continue tweaking the rule in this manner until we achieve a sufficiently high recall and precision - i.e. the (small number of) misses and false alarms can be safely ignored.",
        "Thus we can create nugget-matching rules that succinctly capture various ways of expressing a nugget, while avoiding matching incorrect (or out of context) responses.",
        "Human involvement in the rule creation process ensures high quality generic rules which can then be used to evaluate arbitrary system responses reliably. 4.2 Evaluating the Utility of a Sequence of Ranked Lists The utility of a retrieval system can be defined as the difference between how much the user gained in terms of useful information, and how much the user lost in terms of time and energy.",
        "We calculate this utility from the utilities of individual passages as follows.",
        "After reading each passage returned by the system, the user derives some gain depending on the presence of relevant and novel information, and incurs a loss in terms of the time and energy spent in going through the passage.",
        "However, the likelihood that the user would actually read a passage depends on its position in the ranked list.",
        "Hence, for a query q, the expected utility 2 LDC [18] already provides relevance judgments for 100 topics on the TDT4 corpus.",
        "We further ensured that these judgments are exhaustive on the entire corpus using pooling. of a passage pi at rank i can be defined as U(pi, q) = P(i) ∗ (Gain(pi, q) − Loss(pi, q)) (4) where P(i) is the probability that the user would go through a passage at rank i.",
        "The expected utility for an entire ranked list of length n can be calculated simply by adding the expected utility of each passage: U(q) = nX i=1 P(i) ∗ (Gain(pi, q) − Loss(pi, q)) (5) Note that if we ignore the loss term and define P(i) as P(i) ∝ 1/ logb(b + i − 1) (6) then we get the recently popularized metric called Discounted Cumulated Gain (DCG) [9], where Gain(pi, q) is defined as the graded relevance of passage pi.",
        "However, without the loss term, DCG is a purely recall-oriented metric and not suitable for an adaptive filtering setting, where the systems utility depends in part on its ability to limit the number of items shown to the user.",
        "Although P(i) could be defined based on empirical studies of user behavior, for simplicity, we use P(i) exactly as defined in equation 6.",
        "The gain G(pi, q) of passage pi with respect to the query q is a function of - 1) the number of relevant nuggets present in pi, and 2) the novelty of each of these nuggets.",
        "We combine these two factors as follows.",
        "For each nugget Nj, we assign an initial weight wj, and also keep a count nj of the number of times this nugget has been seen by the user in the past.",
        "The gain derived from each subsequent occurrence of the same nugget is assumed to reduce by a dampening factor γ.",
        "Thus, G(pi, q) is defined as G(pi, q) = X Nj ∈C(pi,q) wj ∗ γnj (7) where C(pi, q) is the set of all nuggets that appear in passage pi and also belong to the answer key of query q.",
        "The initial weights wj are all set of be 1.0 in our experiments, but can also be set based on a pyramid approach [11].",
        "The choice of dampening factor γ determines the users tolerance for redundancy.",
        "When γ = 0, a nugget will only receive credit for its first occurrence i.e. when nj is zero3 .",
        "For 0 < γ < 1, a nugget receives smaller credit for each successive occurrence.",
        "When γ = 1, no dampening occurs and repeated occurrences of a nugget receive the same credit.",
        "Note that the nugget occurrence counts are preserved between evaluation of successive ranked lists returned by the system, since the users are expected to remember what the system showed them in the past.",
        "We define the loss L(pi, q) as a constant cost c (we use 0.1) incurred when reading a system-returned passage.",
        "Thus, our metric can be re-written as U(q) = nX i=1 Gain(pi, q) logb(b + i − 1) − L(n) (8) where L(n) is the loss associated with a ranked list of length n: L(n) = c · nX i=1 1 logb(b + i − 1) (9) 3 Note that 00 = 1 Due to the similarity with Discounted Cumulated Gain (DCG), we call our metric Discounted Cumulated Utility (DCU).",
        "The DCU score obtained by the system is converted to a Normalized DCU (NDCU) score by dividing it by the DCU score of the ideal ranked list, which is created by ordering passages by their decreasing utility scores U(pi, q) and stopping when U(pi, q) ≤ 0 i.e. when the gain is less than or equal to the cost of reading the passage. 5.",
        "DATA TDT4 was the benchmark corpus used in TDT2002 and TDT2003 evaluations.",
        "The corpus consists of over 90, 000 news articles from multiple sources (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) published between October 2000 and January 2001, in the languages of Arabic, English, and Mandarin.",
        "Speech-recognized and machine-translated versions of the non-English articles were provided as well.",
        "LDC [18] has annotated the corpus with 100 topics, that correspond to various news events in this time period.",
        "Out of these, we selected a subset of 12 actionable events, and defined corresponding tasks for them4 .",
        "For each task, we manually defined a profile consisting of an initial set of (5 to 10) queries, a free-text description of the user history, i.e., what the user already knows about the event, and a list of known on-topic and off-topic documents (if available) as training examples.",
        "For each query, we generated answer keys and corresponding nugget matching rules using the procedure described in section 4.1.2, and produced a total of 120 queries, with an average of 7 nuggets per query. 6.",
        "EXPERIMENTS AND RESULTS 6.1 Baselines We used Indri [17], a popular language-model based retrieval engine, as a baseline for comparison with CAF´E.",
        "Indri supports standard search engine functionality, including pseudo-relevance feedback (PRF) [3, 6], and is representative of a typical query-based retrieval system.",
        "Indri does not support any kind of novelty detection.",
        "We compare Indri with PRF turned on and off, against CAF´E with user feedback, novelty detection and antiredundant ranking turned on and off. 6.2 Experimental Setup We divided the TDT4 corpus spanning 4 months into 10 chunks, each defined as a period of 12 consecutive days.",
        "At any given point of time in the distillation process, each system accessed the past data up to the current point, and returned a ranked list of up 50 passages per query.",
        "The 12 tasks defined on the corpus were divided into a training and test set with 6 tasks each.",
        "Each system was allowed to use the training set to tune its parameters for optimizing NDCU (equation 8), including the relevance threshold for both Indri and CAF´E, and the novelty and antiredundancy thresholds for CAF´E.",
        "The NDCU for each system run is calculated automatically.",
        "User feedback was also simulated - relevance judgments for each system-returned passage (as determined by the nugget matching rules described in section 4.1.2) were 4 URL: http://nyc.lti.cs.cmu.edu/downloads Figure 1: Performance of Indri across chunks Figure 2: Performance of CAF´E across chunks used as user feedback in the adaptation of query profiles and user histories. 6.3 Results In Table 1, we show the NDCU scores of the two systems under various settings.",
        "These scores are averaged over the six tasks in the test set, and are calculated with two dampening factors (see section 4.2): γ = 0 and 0.1, to simulate no tolerance and small tolerance for redundancy, respectively.",
        "Using γ = 0 creates a much more strict metric since it does not give any credit to a passage that contains relevant but redundant information.",
        "Hence, the improvement obtained from enabling user feedback is smaller with γ = 0 than the improvement obtained from feedback with γ = 0.1.",
        "This reveals a shortcoming of contemporary retrieval systemswhen the user gives positive feedback on a passage, the systems gives higher weights to the terms present in that passage and tends to retrieve other passages containing the same terms - and thus - usually the same information.",
        "However, the user does not benefit from seeing such redundant passages, and is usually interested in other passages containing related information.",
        "It is informative to evaluate retrieval systems using our utility measure (with γ = 0) which accounts for novelty and thus gives a more realistic picture of how well a system can generalize from user feedback, rather than using traditional IR measures like recall and precision which give an incomplete picture of improvement obtained from user feedback.",
        "Sometimes, however, users might indeed be interested in seeing the same information from multiple sources, as an Table 1: NDCU Scores of Indri and CAF´E for two dampening factors (γ), and various settings (F: Feedback, N: Novelty Detection, A: Anti-Redundant Ranking) Indri CAF´E γ Base +PRF Base +F +F+N +F+A +F+N+A 0 0.19 0.19 0.22 0.23 0.24 0.24 0.24 0.1 0.28 0.29 0.24 0.35 0.35 0.36 0.36 indicator of its importance or reliability.",
        "In such a case, we can simply choose a higher value for γ which corresponds to a higher tolerance for redundancy, and hence let the system tune its parameters accordingly.",
        "Since documents were processed chunk by chunk, it would be interesting to see how the performance of systems improves over time.",
        "Figures 1 and 2 show the performance trends for both the systems across chunks.",
        "While the performance with and without feedback on the first few chunks is expected to be close, for subsequent chunks, the performance curve with feedback enabled rises above the one with the no-feedback setting.",
        "The performance trends are not consistent across all chunks because on-topic documents are not uniformly distributed over all the chunks, making some queries easier than others in certain chunks.",
        "Moreover, since Indri uses pseudo-relevance feedback while CAF´E uses feedback based on actual relevance judgments, the improvement in case of Indri is less dramatic than that of CAF´E. 7.",
        "CONCLUDING REMARKS This paper presents the first investigation on utility-based information distillation with a system that learns the longlasting information needs from fine-grained user feedback over a sequence of ranked passages.",
        "Our system, called CAF´E, combines adaptive filtering, novelty detection and antiredundant passage ranking in a unified framework for utility optimization.",
        "We developed a new scheme for automated evaluation and feedback based on a semi-automatic procedure for acquiring rules that allow automatically matching nuggets against system responses.",
        "We also proposed an extension of the NDCG metric for assessing the utility of ranked passages as a weighted combination of relevance and novelty.",
        "Our experiments on the newly annotated TDT4 benchmark corpus show encouraging utility enhancement over Indri, and also over our own system with incremental learning and novelty detection turned off. 8.",
        "ACKNOWLEDGMENTS We would like to thank Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng, and the Qualitative Data Analysis Program at the University of Pittsburgh lead by Dr. Stuart Shulman for their help with collecting and processing the extended TDT4 annotations used in our experiments.",
        "This work is supported in parts by the National Science Foundation (NSF) under grant IIS0434035, and the Defense Advanced Research Project Agency (DARPA) under contracts NBCHD030010 and W0550432.",
        "Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors. 9.",
        "ADDITIONAL AUTHORS Jian Zhang (jianzhan@stat.purdue.edu)∗ , Jaime Carbonell (jgc@cs.cmu.edu)† , Peter Brusilovsky (peterb+@pitt.edu)‡ , Daqing He(dah44@pitt.edu)‡ 10.",
        "REFERENCES [1] J. Allan.",
        "Incremental Relevance Feedback for Information Filtering.",
        "Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, pages 270-278, 1996. [2] J. Allan, C. Wade, and A. Bolivar.",
        "Retrieval and Novelty Detection at the Sentence Level.",
        "Proceedings of the ACM SIGIR conference on research and development in information retrieval, 2003. [3] C. Buckley, G. Salton, and J. Allan.",
        "Automatic Retrieval with Locality Information using SMART.",
        "NIST special publication, (500207):59-72, 1993. [4] J. Callan.",
        "Learning While Filtering Documents.",
        "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 224-231, 1998. [5] J. Carbonell and J. Goldstein.",
        "The use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries.",
        "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 335-336, 1998. [6] E. Efthimiadis.",
        "Query Expansion.",
        "Annual Review of Information Science and Technology (ARIST), 31:p121-87, 1996. [7] J. Fiscus and G. Duddington.",
        "Topic Detection and Tracking Overview.",
        "Topic Detection and Tracking: Event-based Information Organization, pages 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos.",
        "A Statistical Model for Multilingual Entity Detection and Tracking.",
        "NAACL/HLT, 2004. [9] K. J¨arvelin and J. Kek¨al¨ainen.",
        "Cumulated Gain-based Evaluation of IR Techniques.",
        "ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin and D. Demner-Fushman.",
        "Automatically Evaluating Answers to Definition Questions.",
        "Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), 2005. ∗ Statistics Dept., Purdue University, West Lafayette, USA † Language Technologies Inst., Carnegie Mellon University, Pittsburgh, USA ‡ School of Information Sciences, Univ. of Pittsburgh, Pittsburgh, USA [11] J. Lin and D. Demner-Fushman.",
        "Will Pyramids Built of nUggets Topple Over.",
        "Proceedings of HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos.",
        "A Mention-synchronous Coreference Resolution Algorithm based on the Bell Tree.",
        "Proc. of ACL, 4:136-143, 2004. [13] G. Marton.",
        "Nuggeteer: Automatic Nugget-Based Evaluation Using Descriptions and Judgments.",
        "HLT/NAACL, 2006. [14] E. Riloff.",
        "Automatically Constructing a Dictionary for Information Extraction Tasks.",
        "Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 811-816, 1993. [15] S. Robertson and S. Walker.",
        "Microsoft Cambridge at TREC-9: Filtering track.",
        "The Ninth Text REtrieval Conference (TREC-9), pages 361-368. [16] R. Schapire, Y.",
        "Singer, and A. Singhal.",
        "Boosting and Rocchio Applied to Text Filtering.",
        "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle, and W. Croft.",
        "Indri: A Language Model-based Search Engine for Complex Queries.",
        "Proceedings of the International Conference on Intelligence Analysis, 2004. [18] The Linguistic Data Consortium. http://www.ldc.upenn.edu/. [19] E. Voorhees.",
        "Overview of the TREC 2003 Question Answering Track.",
        "Proceedings of the Twelfth Text REtrieval Conference (TREC 2003), 2003. [20] Y. Yang and B. Kisiel.",
        "Margin-based Local Regression for Adaptive Filtering.",
        "Proceedings of the twelfth international conference on Information and knowledge management, pages 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
        "Robustness of Adaptive Filtering Methods in a Cross-benchmark Evaluation.",
        "Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 98-105, 2005. [22] C. Zhai, W. Cohen, and J. Lafferty.",
        "Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval.",
        "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 10-17, 2003. [23] J. Zhang and Y. Yang.",
        "Robustness of Regularized Linear Classification Methods in Text Categorization.",
        "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 190-197, 2003. [24] Y. Zhang.",
        "Using Bayesian Priors to Combine Classifiers for Adaptive Filtering.",
        "Proceedings of the 27th annual international conference on Research and development in information retrieval, pages 345-352, 2004. [25] Y. Zhang, J. Callan, and T. Minka.",
        "Novelty and Redundancy Detection in Adaptive Filtering.",
        "Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002."
    ],
    "error_count": 0,
    "keys": {
        "utility-based information distillation": {
            "translated_key": "Destilación de información basada en servicios públicos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "<br>utility-based information distillation</br> Over Temporally Sequenced Documents Yiming Yang Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA yiming@cs.cmu.edu Abhimanyu Lad Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA alad@cs.cmu.edu Ni Lao Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA nlao@cs.cmu.edu Abhay Harpale Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA aharpale@cs.cmu.edu Bryan Kisiel Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA bkisiel@cs.cmu.edu Monica Rogati Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA mrogati@cs.cmu.edu ABSTRACT This paper examines a new approach to information distillation over temporally ordered documents, and proposes a novel evaluation scheme for such a framework.",
                "It combines the strengths of and extends beyond conventional adaptive filtering, novelty detection and non-redundant passage ranking with respect to long-lasting information needs (tasks with multiple queries).",
                "Our approach supports fine-grained user feedback via highlighting of arbitrary spans of text, and leverages such information for utility optimization in adaptive settings.",
                "For our experiments, we defined hypothetical tasks based on news events in the TDT4 corpus, with multiple queries per task.",
                "Answer keys (nuggets) were generated for each query and a semiautomatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also propose an extension of the NDCG metric for assessing the utility of ranked passages as a combination of relevance and novelty.",
                "Our results show encouraging utility enhancements using the new approach, compared to the baseline systems without incremental learning or the novelty detection components.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 General Terms Design, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Tracking new and relevant information from temporal data streams for users with long-lasting needs has been a challenging research topic in information retrieval.",
                "Adaptive filtering (AF) is one such task of online prediction of the relevance of each new document with respect to pre-defined topics.",
                "Based on the initial query and a few positive examples (if available), an AF system maintains a profile for each such topic of interest, and constantly updates it based on feedback from the user.",
                "The incremental learning nature of AF systems makes them more powerful than standard search engines that support ad-hoc retrieval (e.g.",
                "Google and Yahoo) in terms of finding relevant information with respect to long-lasting topics of interest, and more attractive for users who are willing to provide feedback to adapt the system towards their specific information needs, without having to modify their queries manually.",
                "A variety of supervised learning algorithms (Rocchio-style classifiers, Exponential-Gaussian models, local regression and logistic regression approaches) have been studied for adaptive settings, examined with explicit and implicit relevance feedback, and evaluated with respect to utility optimization on large benchmark data collections in TREC (Text Retrieval Conferences) and TDT (Topic Detection and Tracking) forums [1, 4, 7, 15, 16, 20, 24, 23].",
                "Regularized logistic regression [21] has been found representative for the state-of-the-art approaches, and highly efficient for frequent model adaptations over large document collections such as the TREC-10 corpus (over 800,000 documents and 84 topics).",
                "Despite substantial achievements in recent adaptive filtering research, significant problems remain unsolved regarding how to leverage user feedback effectively and efficiently.",
                "Specifically, the following issues may seriously limit the true utility of AF systems in real-world applications: 1.",
                "User has a rather passive role in the conventional adaptive filtering setup - he or she reacts to the system only when the system makes a yes decision on a document, by confirming or rejecting that decision.",
                "A more active alternative would be to allow the user to issue multiple queries for a topic, review a ranked list of candidate documents (or passages) per query, and provide feedback on the ranked list, thus refining their information need and requesting updated ranked lists.",
                "The latter form of user interaction has been highly effective in standard retrieval for ad-hoc queries.",
                "How to deploy such a strategy for long-lasting information needs in AF settings is an open question for research. 2.",
                "The unit for receiving a relevance judgment (yes or no) is restricted to the document level in conventional AF.",
                "However, a real user may be willing to provide more informative, fine-grained feedback via highlighting some pieces of text in a retrieved document as relevant, instead of labeling the entire document as relevant.",
                "Effectively leveraging such fine-grained feedback could substantially enhance the quality of an AF system.",
                "For this, we need to enable supervised learning from labeled pieces of text of arbitrary span instead of just allowing labeled documents. 3.",
                "System-selected documents are often highly redundant.",
                "A major news event, for example, would be reported by multiple sources repeatedly for a while, making most of the information content in those articles redundant with each other.",
                "A conventional AF system would select all these redundant news stories for user feedback, wasting the users time while offering little gain.",
                "Clearly, techniques for novelty detection can help in principle [25, 2, 22] for improving the utility of the AF systems.",
                "However, the effectiveness of such techniques at passage level to detect novelty with respect to users (fine-grained) feedback and to detect redundancy in ranked lists remains to be evaluated using a measure of utility that mimics the needs of a real user.",
                "To address the above limitations of current AF systems, we propose and examine a new approach in this paper, combining the strengths of conventional AF (incremental learning of topic models), multi-pass passage retrieval for long-lasting queries conditioned on topic, and novelty detection for removal of redundancy from user interactions with the system.",
                "We call the new process <br>utility-based information distillation</br>.",
                "Note that conventional benchmark corpora for AF evaluations, which have relevance judgments at the document level and do not define tasks with multiple queries, are insufficient for evaluating the new approach.",
                "Therefore, we extended a benchmark corpus - the TDT4 collection of news stories and TV broadcasts - with task definitions, multiple queries per task, and answer keys per query.",
                "We have conducted our experiments on this extended TDT4 corpus and have made the additionally generated data publicly available for future comparative evaluations 1 .",
                "To automatically evaluate the system-returned arbitrary spans of text using our answer keys, we further developed an evaluation scheme with semi-automatic procedure for 1 URL: http://nyc.lti.cs.cmu.edu/downloads acquiring rules that can match nuggets against system responses.",
                "Moreover, we propose an extension of NDCG (Normalized Discounted Cumulated Gain) [9] for assessing the utility of ranked passages as a function of both relevance and novelty.",
                "The rest of this paper is organized as follows.",
                "Section 2 outlines the information distillation process with a concrete example.",
                "Section 3 describes the technical cores of our system called CAF´E - CMU Adaptive Filtering Engine.",
                "Section 4 discusses issues with respect to evaluation methodology and proposes a new scheme.",
                "Section 5 describes the extended TDT4 corpus.",
                "Section 6 presents our experiments and results.",
                "Section 7 concludes the study and gives future perspectives. 2.",
                "A SAMPLE TASK Consider a news event - the escape of seven convicts from a Texas prison in December 2000 and their capture a month later.",
                "Assuming a user were interested in this event since its early stage, the information need could be: Find information about the escape of convicts from Texas prison, and information related to their recapture.",
                "The associated lower-level questions could be: 1.",
                "How many prisoners escaped? 2.",
                "Where and when were they sighted? 3. Who are their known contacts inside and outside the prison? 4.",
                "How are they armed? 5.",
                "Do they have any vehicles? 6.",
                "What steps have been taken so far?",
                "We call such an information need a task, and the associated questions as the queries in this task.",
                "A distillation system is supposed to monitor the incoming documents, process them chunk by chunk in a temporal order, select potentially relevant and novel passages from each chunk with respect to each query, and present a ranked list of passages to the user.",
                "Passage ranking here is based on how relevant a passage is with respect to the current query, how novel it is with respect to the current user history (of his or her interactions with the system), and how redundant it is compared to other passages with a higher rank in the list.",
                "When presented with a list of passages, the user may provide feedback by highlighting arbitrary spans of text that he or she found relevant.",
                "These spans of text are taken as positive examples in the adaptation of the query profile, and also added to the users history.",
                "Passages not marked by the user are taken as negative examples.",
                "As soon as the query profile is updated, the system re-issues a search and returns another ranked list of passages where the previously seen passages are either removed or ranked low, based on user preference.",
                "For example, if the user highlights ...officials have posted a $100,000 reward for their capture... as relevant answer to the query What steps have been taken so far?, then the highlighted piece is used as an additional positive training example in the adaptation of the query profile.",
                "This piece of feedback is also added to the user history as a seen example, so that in future, the system will not place another passage mentioning $100,000 reward at the top of the ranked list.",
                "However, an article mentioning ...officials have doubled the reward money to $200,000... might be ranked high since it is both relevant to the (updated) query profile and novel with respect to the (updated) user history.",
                "The user may modify the original queries or add a new query during the process; the query profiles will be changed accordingly.",
                "Clearly, novelty detection is very important for the utility of such a system because of the iterative search.",
                "Without novelty detection, the old relevant passages would be shown to the user repeatedly in each ranked list.",
                "Through the above example, we can see the main properties of our new framework for <br>utility-based information distillation</br> over temporally ordered documents.",
                "Our framework combines and extends the power of adaptive filtering (AF), ad-hoc retrieval (IR) and novelty detection (ND).",
                "Compared to standard IR, our approach has the power of incrementally learning long-term information needs and modeling a sequence of queries within a task.",
                "Compared to conventional AF, it enables a more active role of the user in refining his or her information needs and requesting new results by allowing relevance and novelty feedback via highlighting of arbitrary spans of text in passages returned by the system.",
                "Compared to past work, this is the first evaluation of novelty detection integrated with adaptive filtering for sequenced queries that allows flexible user feedback over ranked passages.",
                "The combination of AF, IR and ND with the new extensions raises an important research question regarding evaluation methodology: how can we measure the utility of such an information distillation system?",
                "Existing metrics in standard IR, AF and ND are insufficient, and new solutions must be explored, as we will discuss in Section 4, after describing the technical cores of our system in the next section. 3.",
                "TECHNICAL CORES The core components of CAF´E are - 1) AF for incremental learning of query profiles, 2) IR for estimating relevance of passages with respect to query profiles, 3) ND for assessing novelty of passages with respect to users history, and 4) anti-redundancy component to remove redundancy from ranked lists. 3.1 Adaptive Filtering Component We use a state-of-the-art algorithm in the field - the regularized logistic regression method which had the best results on several benchmark evaluation corpora for AF [21].",
                "Logistic regression (LR) is a supervised learning algorithm for statistical classification.",
                "Based on a training set of labeled instances, it learns a class model which can then by used to predict the labels of unseen instances.",
                "Its performance as well as efficiency in terms of training time makes it a good candidate when frequent updates of the class model are required, as is the case in adaptive filtering, where the system must learn from each new feedback provided by the user. (See [21] and [23] for computational complexity and implementation issues).",
                "In adaptive filtering, each query is considered as a class and the probability of a passage belonging to this class corresponds to the degree of relevance of the passage with respect to the query.",
                "For training the model, we use the query itself as the initial positive training example of the class, and the user-highlighted pieces of text (marked as Relevant or Not-relevant) during feedback as additional training examples.",
                "To address the cold start issue in the early stage before any user feedback is obtained, the system uses a small sample from a retrospective corpus as the initial negative examples in the training set.",
                "The details of using logistic regression for adaptive filtering (assigning different weights to positive and negative training instances, and regularizing the objective function to prevent over-fitting on training data) are presented in [21].",
                "The class model w∗ learned by Logistic Regression, or the query profile, is a vector whose dimensions are individual terms and whose elements are the regression coefficients, indicating how influential each term is in the query profile.",
                "The query profile is updated whenever a new piece of user feedback is received.",
                "A temporally decaying weight can be applied to each training example, as an option, to emphasize the most recent user feedback. 3.2 Passage Retrieval Component We use standard IR techniques in this part of our system.",
                "Incoming documents are processed in chunks, where each chunk can be defined as a fixed span of time or as a fixed number of documents, as preferred by the user.",
                "For each incoming document, corpus statistics like the IDF (Inverted Document Frequency) of each term are updated.",
                "We use a state-of-the-art named entity identifier and tracker [8, 12] to identify person and location names, and merge them with co-referent named entities seen in the past.",
                "Then the documents are segmented into passages, which can be a whole document, a paragraph, a sentence, or any other continuous span of text, as preferred.",
                "Each passage is represented using a vector of TF-IDF (Term FrequencyInverse Document Frequency) weights, where term can be a word or a named entity.",
                "Given a query profile, i.e. the logistic regression solution w∗ as described in Section 3.1, the system computes the posterior probability of relevance for each passage x as fRL(x) ≡ P(y = 1|x, w∗ ) = 1 (1 + e−w∗·x) (1) Passages are ordered by their relevance scores, and the ones with scores above a threshold (tuned on a training set) comprise the relevance list that is passed on to the novelty detection step. 3.3 Novelty Detection Component CAF´E maintains a user history H(t), which contains all the spans of text hi that the user highlighted (as feedback) during his or her past interactions with the system, up to the current time t. Denoting the history as H(t) = n h1, h2, ..., ht o , (2) the novelty score of a new candidate passage x is computed as: fND(x) = 1 − max i∈1..t {cos(x, hi)} (3) where both candidate passage x and highlighted spans of text hi are represented as TF-IDF vectors.",
                "The novelty score of each passage is compared to a prespecified threshold (also tuned on a training set), and any passage with a score below this threshold is removed from the relevance list. 3.4 Anti-redundant Ranking Component Although the novelty detection component ensures that only novel (previously unseen) information remains in the relevance list, this list might still contain the same novel information at multiple positions in the ranked list.",
                "Suppose, for example, that the user has already read about a $100,000 reward for information about the escaped convicts.",
                "A new piece of news that the award has been increased to $200,000 is novel since the user hasnt read about it yet.",
                "However, multiple news sources would report this news and we might end up showing (redundant) articles from all these sources in a ranked list.",
                "Hence, a ranked list should also be made non-redundant with respect to its own contents.",
                "We use a simplified version of the Maximal Marginal Relevance method [5], originally developed for combining relevance and novelty in text retrieval and summarization.",
                "Our procedure starts with the current list of passages sorted by relevance (section 3.2), filtered by Novelty Detection component (section 3.3), and generates a new non-redundant list as follows: 1.",
                "Take the top passage in the current list as the top one in the new list. 2.",
                "Add the next passage x in the current list to the new list only if fAR(x) > t where fAR(x) = 1 − max pi∈Lnew {cos(x, pi)} and Lnew is the set of passages already selected in the new list. 3.",
                "Repeat step 2 until all the passages in the current list have been examined.",
                "After applying the above-mentioned algorithm, each passage in the new list is sufficiently dissimilar to others, thus favoring diversity rather than redundancy in the new ranked list.",
                "The anti-redundancy threshold t is tuned on a training set. 4.",
                "EVALUATION METHODOLOGY The approach we proposed above for information distillation raises important issues regarding evaluation methodology.",
                "Firstly, since our framework allows the output to be passages at different levels of granularity (e.g. k-sentence windows where k may vary) instead of a fixed length, it is not possible to have pre-annotated relevance judgments at all such granularity levels.",
                "Secondly, since we wish to measure the utility of the system output as a combination of both relevance and novelty, traditional relevance-only based measures must be replaced by measures that penalize the repetition of the same information in the system output across time.",
                "Thirdly, since the output of the system is ranked lists, we must reward those systems that present useful information (both relevant and previously unseen) using shorter ranked lists, and penalize those that present the same information using longer ranked lists.",
                "None of the existing measures in ad-hoc retrieval, adaptive filtering, novelty detection or other related areas (text summarization and question answering) have desirable properties in all the three aspects.",
                "Therefore, we must develop a new evaluation methodology. 4.1 Answer Keys To enable the evaluation of a system whose output consists of passages of arbitrary length, we borrow the concept of answer keys from the Question Answering (QA) community, where systems are allowed to return arbitrary spans of text as answers.",
                "Answer keys define what should be present in a system response to receive credit, and are comprised of a collection of information nuggets, i.e. factoid units about which human assessors can make binary decisions of whether or not a system response contains them.",
                "Defining answer keys and making the associated binary decisions are conceptual tasks that require semantic mapping [19], since system-returned passages can contain the same information expressed in many different ways.",
                "Hence, QA evaluations have relied on human assessors for the mapping between various expressions, making the process costly, time consuming, and not scalable to large query and document collections, and extensive system evaluations with various parameter settings. 4.1.1 Automating Evaluation based on Answer Keys Automatic evaluation methods would allow for faster system building and tuning, as well as provide an objective and affordable way of comparing various systems.",
                "Recently, such methods have been proposed, more or less, based on the idea of n-gram co-occurrences.",
                "Pourpre [10] assigns a fractional recall score to a system response based on its unigram overlap with a given nuggets description.",
                "For example, a system response A B C has recall 3/4 with respect to a nugget with description A B C D. However, such an approach is unfair to systems that present the same information but using words other than A, B, C, and D. Another open issue is how to weight individual words in measuring the closeness of a match.",
                "For example, consider the question How many prisoners escaped?.",
                "In the nugget Seven prisoners escaped from a Texas prison, there is no indication that seven is the keyword, and that it must be matched to get any relevance credit.",
                "Using IDF values does not help, since seven will generally not have a higher IDF than words like texas and prison.",
                "Also, redefining the nugget as just seven does not solve the problem since now it might spuriously match any mention of seven out of context.",
                "Nuggeteer [13] works on similar principles but makes binary decisions about whether a nugget is present in a given system response by tuning a threshold.",
                "However, it is also plagued by spurious relevance since not all words contained in the nugget description (or known correct responses) are central to the nugget. 4.1.2 Nugget-Matching Rules We propose a reliable automatic method for determining whether a snippet of text contains a given nugget, based on nugget-matching rules, which are generated using a semiautomatic procedure explained below.",
                "These rules are essentially Boolean queries that will only match against snippets that contain the nugget.",
                "For instance, a candidate rule for matching answers to How many prisoners escaped? is (Texas AND seven AND escape AND (convicts OR prisoners)), possibly with other synonyms and variants in the rule.",
                "For a corpus of news articles, which usually follow a typical formal prose, it is fairly easy to write such simple rules to match expected answers using a bootstrap approach, as described below.",
                "We propose a two-stage approach, inspired by Autoslog [14], that combines the strength of humans in identifying semantically equivalent expressions and the strength of the system in gathering statistical evidence from a humanannotated corpus of documents.",
                "In the first stage, human subjects annotated (using a highlighting tool) portions of ontopic documents that contained answers to each nugget 2 .",
                "In the second stage, subjects used our rule generation tool to create rules that would match the annotations for each nugget.",
                "The tool allows users to enter a Boolean rule as a disjunction of conjunctions (e.g. ((a AND b) OR (a AND c AND d) OR (e))).",
                "Given a candidate rule, our tool uses it as a Boolean query over the entire set of on-topic documents and calculates its recall and precision with respect to the annotations that it is expected to match.",
                "Hence, the subjects can start with a simple rule and iteratively refine it until they are satisfied with its recall and precision.",
                "We observed that it was very easy for humans to improve the precision of a rule by tweaking its existing conjunctions (adding more ANDs), and improving the recall by adding more conjunctions to the disjunction (adding more ORs).",
                "As an example, lets try to create a rule for the nugget which says that seven prisoners escaped from the Texas prison.",
                "We start with a simple rule - (seven).",
                "When we input this into the rule generation tool, we realize that this rule matches many spurious occurrences of seven (e.g. ...seven states...) and thus gets a low precision score.",
                "We can further qualify our rule - Texas AND seven AND convicts.",
                "Next, by looking at the missed annotations, we realize that some news articles mentioned ...seven prisoners escaped.... We then replace convicts with the disjunction (convicts OR prisoners).",
                "We continue tweaking the rule in this manner until we achieve a sufficiently high recall and precision - i.e. the (small number of) misses and false alarms can be safely ignored.",
                "Thus we can create nugget-matching rules that succinctly capture various ways of expressing a nugget, while avoiding matching incorrect (or out of context) responses.",
                "Human involvement in the rule creation process ensures high quality generic rules which can then be used to evaluate arbitrary system responses reliably. 4.2 Evaluating the Utility of a Sequence of Ranked Lists The utility of a retrieval system can be defined as the difference between how much the user gained in terms of useful information, and how much the user lost in terms of time and energy.",
                "We calculate this utility from the utilities of individual passages as follows.",
                "After reading each passage returned by the system, the user derives some gain depending on the presence of relevant and novel information, and incurs a loss in terms of the time and energy spent in going through the passage.",
                "However, the likelihood that the user would actually read a passage depends on its position in the ranked list.",
                "Hence, for a query q, the expected utility 2 LDC [18] already provides relevance judgments for 100 topics on the TDT4 corpus.",
                "We further ensured that these judgments are exhaustive on the entire corpus using pooling. of a passage pi at rank i can be defined as U(pi, q) = P(i) ∗ (Gain(pi, q) − Loss(pi, q)) (4) where P(i) is the probability that the user would go through a passage at rank i.",
                "The expected utility for an entire ranked list of length n can be calculated simply by adding the expected utility of each passage: U(q) = nX i=1 P(i) ∗ (Gain(pi, q) − Loss(pi, q)) (5) Note that if we ignore the loss term and define P(i) as P(i) ∝ 1/ logb(b + i − 1) (6) then we get the recently popularized metric called Discounted Cumulated Gain (DCG) [9], where Gain(pi, q) is defined as the graded relevance of passage pi.",
                "However, without the loss term, DCG is a purely recall-oriented metric and not suitable for an adaptive filtering setting, where the systems utility depends in part on its ability to limit the number of items shown to the user.",
                "Although P(i) could be defined based on empirical studies of user behavior, for simplicity, we use P(i) exactly as defined in equation 6.",
                "The gain G(pi, q) of passage pi with respect to the query q is a function of - 1) the number of relevant nuggets present in pi, and 2) the novelty of each of these nuggets.",
                "We combine these two factors as follows.",
                "For each nugget Nj, we assign an initial weight wj, and also keep a count nj of the number of times this nugget has been seen by the user in the past.",
                "The gain derived from each subsequent occurrence of the same nugget is assumed to reduce by a dampening factor γ.",
                "Thus, G(pi, q) is defined as G(pi, q) = X Nj ∈C(pi,q) wj ∗ γnj (7) where C(pi, q) is the set of all nuggets that appear in passage pi and also belong to the answer key of query q.",
                "The initial weights wj are all set of be 1.0 in our experiments, but can also be set based on a pyramid approach [11].",
                "The choice of dampening factor γ determines the users tolerance for redundancy.",
                "When γ = 0, a nugget will only receive credit for its first occurrence i.e. when nj is zero3 .",
                "For 0 < γ < 1, a nugget receives smaller credit for each successive occurrence.",
                "When γ = 1, no dampening occurs and repeated occurrences of a nugget receive the same credit.",
                "Note that the nugget occurrence counts are preserved between evaluation of successive ranked lists returned by the system, since the users are expected to remember what the system showed them in the past.",
                "We define the loss L(pi, q) as a constant cost c (we use 0.1) incurred when reading a system-returned passage.",
                "Thus, our metric can be re-written as U(q) = nX i=1 Gain(pi, q) logb(b + i − 1) − L(n) (8) where L(n) is the loss associated with a ranked list of length n: L(n) = c · nX i=1 1 logb(b + i − 1) (9) 3 Note that 00 = 1 Due to the similarity with Discounted Cumulated Gain (DCG), we call our metric Discounted Cumulated Utility (DCU).",
                "The DCU score obtained by the system is converted to a Normalized DCU (NDCU) score by dividing it by the DCU score of the ideal ranked list, which is created by ordering passages by their decreasing utility scores U(pi, q) and stopping when U(pi, q) ≤ 0 i.e. when the gain is less than or equal to the cost of reading the passage. 5.",
                "DATA TDT4 was the benchmark corpus used in TDT2002 and TDT2003 evaluations.",
                "The corpus consists of over 90, 000 news articles from multiple sources (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) published between October 2000 and January 2001, in the languages of Arabic, English, and Mandarin.",
                "Speech-recognized and machine-translated versions of the non-English articles were provided as well.",
                "LDC [18] has annotated the corpus with 100 topics, that correspond to various news events in this time period.",
                "Out of these, we selected a subset of 12 actionable events, and defined corresponding tasks for them4 .",
                "For each task, we manually defined a profile consisting of an initial set of (5 to 10) queries, a free-text description of the user history, i.e., what the user already knows about the event, and a list of known on-topic and off-topic documents (if available) as training examples.",
                "For each query, we generated answer keys and corresponding nugget matching rules using the procedure described in section 4.1.2, and produced a total of 120 queries, with an average of 7 nuggets per query. 6.",
                "EXPERIMENTS AND RESULTS 6.1 Baselines We used Indri [17], a popular language-model based retrieval engine, as a baseline for comparison with CAF´E.",
                "Indri supports standard search engine functionality, including pseudo-relevance feedback (PRF) [3, 6], and is representative of a typical query-based retrieval system.",
                "Indri does not support any kind of novelty detection.",
                "We compare Indri with PRF turned on and off, against CAF´E with user feedback, novelty detection and antiredundant ranking turned on and off. 6.2 Experimental Setup We divided the TDT4 corpus spanning 4 months into 10 chunks, each defined as a period of 12 consecutive days.",
                "At any given point of time in the distillation process, each system accessed the past data up to the current point, and returned a ranked list of up 50 passages per query.",
                "The 12 tasks defined on the corpus were divided into a training and test set with 6 tasks each.",
                "Each system was allowed to use the training set to tune its parameters for optimizing NDCU (equation 8), including the relevance threshold for both Indri and CAF´E, and the novelty and antiredundancy thresholds for CAF´E.",
                "The NDCU for each system run is calculated automatically.",
                "User feedback was also simulated - relevance judgments for each system-returned passage (as determined by the nugget matching rules described in section 4.1.2) were 4 URL: http://nyc.lti.cs.cmu.edu/downloads Figure 1: Performance of Indri across chunks Figure 2: Performance of CAF´E across chunks used as user feedback in the adaptation of query profiles and user histories. 6.3 Results In Table 1, we show the NDCU scores of the two systems under various settings.",
                "These scores are averaged over the six tasks in the test set, and are calculated with two dampening factors (see section 4.2): γ = 0 and 0.1, to simulate no tolerance and small tolerance for redundancy, respectively.",
                "Using γ = 0 creates a much more strict metric since it does not give any credit to a passage that contains relevant but redundant information.",
                "Hence, the improvement obtained from enabling user feedback is smaller with γ = 0 than the improvement obtained from feedback with γ = 0.1.",
                "This reveals a shortcoming of contemporary retrieval systemswhen the user gives positive feedback on a passage, the systems gives higher weights to the terms present in that passage and tends to retrieve other passages containing the same terms - and thus - usually the same information.",
                "However, the user does not benefit from seeing such redundant passages, and is usually interested in other passages containing related information.",
                "It is informative to evaluate retrieval systems using our utility measure (with γ = 0) which accounts for novelty and thus gives a more realistic picture of how well a system can generalize from user feedback, rather than using traditional IR measures like recall and precision which give an incomplete picture of improvement obtained from user feedback.",
                "Sometimes, however, users might indeed be interested in seeing the same information from multiple sources, as an Table 1: NDCU Scores of Indri and CAF´E for two dampening factors (γ), and various settings (F: Feedback, N: Novelty Detection, A: Anti-Redundant Ranking) Indri CAF´E γ Base +PRF Base +F +F+N +F+A +F+N+A 0 0.19 0.19 0.22 0.23 0.24 0.24 0.24 0.1 0.28 0.29 0.24 0.35 0.35 0.36 0.36 indicator of its importance or reliability.",
                "In such a case, we can simply choose a higher value for γ which corresponds to a higher tolerance for redundancy, and hence let the system tune its parameters accordingly.",
                "Since documents were processed chunk by chunk, it would be interesting to see how the performance of systems improves over time.",
                "Figures 1 and 2 show the performance trends for both the systems across chunks.",
                "While the performance with and without feedback on the first few chunks is expected to be close, for subsequent chunks, the performance curve with feedback enabled rises above the one with the no-feedback setting.",
                "The performance trends are not consistent across all chunks because on-topic documents are not uniformly distributed over all the chunks, making some queries easier than others in certain chunks.",
                "Moreover, since Indri uses pseudo-relevance feedback while CAF´E uses feedback based on actual relevance judgments, the improvement in case of Indri is less dramatic than that of CAF´E. 7.",
                "CONCLUDING REMARKS This paper presents the first investigation on <br>utility-based information distillation</br> with a system that learns the longlasting information needs from fine-grained user feedback over a sequence of ranked passages.",
                "Our system, called CAF´E, combines adaptive filtering, novelty detection and antiredundant passage ranking in a unified framework for utility optimization.",
                "We developed a new scheme for automated evaluation and feedback based on a semi-automatic procedure for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also proposed an extension of the NDCG metric for assessing the utility of ranked passages as a weighted combination of relevance and novelty.",
                "Our experiments on the newly annotated TDT4 benchmark corpus show encouraging utility enhancement over Indri, and also over our own system with incremental learning and novelty detection turned off. 8.",
                "ACKNOWLEDGMENTS We would like to thank Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng, and the Qualitative Data Analysis Program at the University of Pittsburgh lead by Dr. Stuart Shulman for their help with collecting and processing the extended TDT4 annotations used in our experiments.",
                "This work is supported in parts by the National Science Foundation (NSF) under grant IIS0434035, and the Defense Advanced Research Project Agency (DARPA) under contracts NBCHD030010 and W0550432.",
                "Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors. 9.",
                "ADDITIONAL AUTHORS Jian Zhang (jianzhan@stat.purdue.edu)∗ , Jaime Carbonell (jgc@cs.cmu.edu)† , Peter Brusilovsky (peterb+@pitt.edu)‡ , Daqing He(dah44@pitt.edu)‡ 10.",
                "REFERENCES [1] J. Allan.",
                "Incremental Relevance Feedback for Information Filtering.",
                "Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, pages 270-278, 1996. [2] J. Allan, C. Wade, and A. Bolivar.",
                "Retrieval and Novelty Detection at the Sentence Level.",
                "Proceedings of the ACM SIGIR conference on research and development in information retrieval, 2003. [3] C. Buckley, G. Salton, and J. Allan.",
                "Automatic Retrieval with Locality Information using SMART.",
                "NIST special publication, (500207):59-72, 1993. [4] J. Callan.",
                "Learning While Filtering Documents.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 224-231, 1998. [5] J. Carbonell and J. Goldstein.",
                "The use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 335-336, 1998. [6] E. Efthimiadis.",
                "Query Expansion.",
                "Annual Review of Information Science and Technology (ARIST), 31:p121-87, 1996. [7] J. Fiscus and G. Duddington.",
                "Topic Detection and Tracking Overview.",
                "Topic Detection and Tracking: Event-based Information Organization, pages 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos.",
                "A Statistical Model for Multilingual Entity Detection and Tracking.",
                "NAACL/HLT, 2004. [9] K. J¨arvelin and J. Kek¨al¨ainen.",
                "Cumulated Gain-based Evaluation of IR Techniques.",
                "ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin and D. Demner-Fushman.",
                "Automatically Evaluating Answers to Definition Questions.",
                "Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), 2005. ∗ Statistics Dept., Purdue University, West Lafayette, USA † Language Technologies Inst., Carnegie Mellon University, Pittsburgh, USA ‡ School of Information Sciences, Univ. of Pittsburgh, Pittsburgh, USA [11] J. Lin and D. Demner-Fushman.",
                "Will Pyramids Built of nUggets Topple Over.",
                "Proceedings of HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos.",
                "A Mention-synchronous Coreference Resolution Algorithm based on the Bell Tree.",
                "Proc. of ACL, 4:136-143, 2004. [13] G. Marton.",
                "Nuggeteer: Automatic Nugget-Based Evaluation Using Descriptions and Judgments.",
                "HLT/NAACL, 2006. [14] E. Riloff.",
                "Automatically Constructing a Dictionary for Information Extraction Tasks.",
                "Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 811-816, 1993. [15] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9: Filtering track.",
                "The Ninth Text REtrieval Conference (TREC-9), pages 361-368. [16] R. Schapire, Y.",
                "Singer, and A. Singhal.",
                "Boosting and Rocchio Applied to Text Filtering.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle, and W. Croft.",
                "Indri: A Language Model-based Search Engine for Complex Queries.",
                "Proceedings of the International Conference on Intelligence Analysis, 2004. [18] The Linguistic Data Consortium. http://www.ldc.upenn.edu/. [19] E. Voorhees.",
                "Overview of the TREC 2003 Question Answering Track.",
                "Proceedings of the Twelfth Text REtrieval Conference (TREC 2003), 2003. [20] Y. Yang and B. Kisiel.",
                "Margin-based Local Regression for Adaptive Filtering.",
                "Proceedings of the twelfth international conference on Information and knowledge management, pages 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of Adaptive Filtering Methods in a Cross-benchmark Evaluation.",
                "Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 98-105, 2005. [22] C. Zhai, W. Cohen, and J. Lafferty.",
                "Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 10-17, 2003. [23] J. Zhang and Y. Yang.",
                "Robustness of Regularized Linear Classification Methods in Text Categorization.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 190-197, 2003. [24] Y. Zhang.",
                "Using Bayesian Priors to Combine Classifiers for Adaptive Filtering.",
                "Proceedings of the 27th annual international conference on Research and development in information retrieval, pages 345-352, 2004. [25] Y. Zhang, J. Callan, and T. Minka.",
                "Novelty and Redundancy Detection in Adaptive Filtering.",
                "Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "\"Destilación de información basada en servicios públicos\" sobre documentos secuenciados temporalmente Yiming Yang Language Technologies Inst.",
                "Llamamos al nuevo proceso \"destilación de información basada en servicios públicos\".",
                "A través del ejemplo anterior, podemos ver las propiedades principales de nuestro nuevo marco para la \"destilación de información basada en servicios públicos\" sobre los documentos ordenados temporalmente.",
                "Observaciones finales Este documento presenta la primera investigación sobre la \"destilación de información basada en servicios públicos\" con un sistema que aprende las necesidades de información de larga duración de los comentarios de los usuarios de grano fino sobre una secuencia de pasajes clasificados."
            ],
            "translated_text": "",
            "candidates": [
                "Destilación de información basada en servicios públicos",
                "Destilación de información basada en servicios públicos",
                "Destilación de información basada en servicios públicos",
                "destilación de información basada en servicios públicos",
                "Destilación de información basada en servicios públicos",
                "destilación de información basada en servicios públicos",
                "Destilación de información basada en servicios públicos",
                "destilación de información basada en servicios públicos"
            ],
            "error": []
        },
        "temporally ordered document": {
            "translated_key": "documento ordenado temporalmente",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Utility-based Information Distillation Over Temporally Sequenced Documents Yiming Yang Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA yiming@cs.cmu.edu Abhimanyu Lad Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA alad@cs.cmu.edu Ni Lao Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA nlao@cs.cmu.edu Abhay Harpale Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA aharpale@cs.cmu.edu Bryan Kisiel Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA bkisiel@cs.cmu.edu Monica Rogati Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA mrogati@cs.cmu.edu ABSTRACT This paper examines a new approach to information distillation over <br>temporally ordered document</br>s, and proposes a novel evaluation scheme for such a framework.",
                "It combines the strengths of and extends beyond conventional adaptive filtering, novelty detection and non-redundant passage ranking with respect to long-lasting information needs (tasks with multiple queries).",
                "Our approach supports fine-grained user feedback via highlighting of arbitrary spans of text, and leverages such information for utility optimization in adaptive settings.",
                "For our experiments, we defined hypothetical tasks based on news events in the TDT4 corpus, with multiple queries per task.",
                "Answer keys (nuggets) were generated for each query and a semiautomatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also propose an extension of the NDCG metric for assessing the utility of ranked passages as a combination of relevance and novelty.",
                "Our results show encouraging utility enhancements using the new approach, compared to the baseline systems without incremental learning or the novelty detection components.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 General Terms Design, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Tracking new and relevant information from temporal data streams for users with long-lasting needs has been a challenging research topic in information retrieval.",
                "Adaptive filtering (AF) is one such task of online prediction of the relevance of each new document with respect to pre-defined topics.",
                "Based on the initial query and a few positive examples (if available), an AF system maintains a profile for each such topic of interest, and constantly updates it based on feedback from the user.",
                "The incremental learning nature of AF systems makes them more powerful than standard search engines that support ad-hoc retrieval (e.g.",
                "Google and Yahoo) in terms of finding relevant information with respect to long-lasting topics of interest, and more attractive for users who are willing to provide feedback to adapt the system towards their specific information needs, without having to modify their queries manually.",
                "A variety of supervised learning algorithms (Rocchio-style classifiers, Exponential-Gaussian models, local regression and logistic regression approaches) have been studied for adaptive settings, examined with explicit and implicit relevance feedback, and evaluated with respect to utility optimization on large benchmark data collections in TREC (Text Retrieval Conferences) and TDT (Topic Detection and Tracking) forums [1, 4, 7, 15, 16, 20, 24, 23].",
                "Regularized logistic regression [21] has been found representative for the state-of-the-art approaches, and highly efficient for frequent model adaptations over large document collections such as the TREC-10 corpus (over 800,000 documents and 84 topics).",
                "Despite substantial achievements in recent adaptive filtering research, significant problems remain unsolved regarding how to leverage user feedback effectively and efficiently.",
                "Specifically, the following issues may seriously limit the true utility of AF systems in real-world applications: 1.",
                "User has a rather passive role in the conventional adaptive filtering setup - he or she reacts to the system only when the system makes a yes decision on a document, by confirming or rejecting that decision.",
                "A more active alternative would be to allow the user to issue multiple queries for a topic, review a ranked list of candidate documents (or passages) per query, and provide feedback on the ranked list, thus refining their information need and requesting updated ranked lists.",
                "The latter form of user interaction has been highly effective in standard retrieval for ad-hoc queries.",
                "How to deploy such a strategy for long-lasting information needs in AF settings is an open question for research. 2.",
                "The unit for receiving a relevance judgment (yes or no) is restricted to the document level in conventional AF.",
                "However, a real user may be willing to provide more informative, fine-grained feedback via highlighting some pieces of text in a retrieved document as relevant, instead of labeling the entire document as relevant.",
                "Effectively leveraging such fine-grained feedback could substantially enhance the quality of an AF system.",
                "For this, we need to enable supervised learning from labeled pieces of text of arbitrary span instead of just allowing labeled documents. 3.",
                "System-selected documents are often highly redundant.",
                "A major news event, for example, would be reported by multiple sources repeatedly for a while, making most of the information content in those articles redundant with each other.",
                "A conventional AF system would select all these redundant news stories for user feedback, wasting the users time while offering little gain.",
                "Clearly, techniques for novelty detection can help in principle [25, 2, 22] for improving the utility of the AF systems.",
                "However, the effectiveness of such techniques at passage level to detect novelty with respect to users (fine-grained) feedback and to detect redundancy in ranked lists remains to be evaluated using a measure of utility that mimics the needs of a real user.",
                "To address the above limitations of current AF systems, we propose and examine a new approach in this paper, combining the strengths of conventional AF (incremental learning of topic models), multi-pass passage retrieval for long-lasting queries conditioned on topic, and novelty detection for removal of redundancy from user interactions with the system.",
                "We call the new process utility-based information distillation.",
                "Note that conventional benchmark corpora for AF evaluations, which have relevance judgments at the document level and do not define tasks with multiple queries, are insufficient for evaluating the new approach.",
                "Therefore, we extended a benchmark corpus - the TDT4 collection of news stories and TV broadcasts - with task definitions, multiple queries per task, and answer keys per query.",
                "We have conducted our experiments on this extended TDT4 corpus and have made the additionally generated data publicly available for future comparative evaluations 1 .",
                "To automatically evaluate the system-returned arbitrary spans of text using our answer keys, we further developed an evaluation scheme with semi-automatic procedure for 1 URL: http://nyc.lti.cs.cmu.edu/downloads acquiring rules that can match nuggets against system responses.",
                "Moreover, we propose an extension of NDCG (Normalized Discounted Cumulated Gain) [9] for assessing the utility of ranked passages as a function of both relevance and novelty.",
                "The rest of this paper is organized as follows.",
                "Section 2 outlines the information distillation process with a concrete example.",
                "Section 3 describes the technical cores of our system called CAF´E - CMU Adaptive Filtering Engine.",
                "Section 4 discusses issues with respect to evaluation methodology and proposes a new scheme.",
                "Section 5 describes the extended TDT4 corpus.",
                "Section 6 presents our experiments and results.",
                "Section 7 concludes the study and gives future perspectives. 2.",
                "A SAMPLE TASK Consider a news event - the escape of seven convicts from a Texas prison in December 2000 and their capture a month later.",
                "Assuming a user were interested in this event since its early stage, the information need could be: Find information about the escape of convicts from Texas prison, and information related to their recapture.",
                "The associated lower-level questions could be: 1.",
                "How many prisoners escaped? 2.",
                "Where and when were they sighted? 3. Who are their known contacts inside and outside the prison? 4.",
                "How are they armed? 5.",
                "Do they have any vehicles? 6.",
                "What steps have been taken so far?",
                "We call such an information need a task, and the associated questions as the queries in this task.",
                "A distillation system is supposed to monitor the incoming documents, process them chunk by chunk in a temporal order, select potentially relevant and novel passages from each chunk with respect to each query, and present a ranked list of passages to the user.",
                "Passage ranking here is based on how relevant a passage is with respect to the current query, how novel it is with respect to the current user history (of his or her interactions with the system), and how redundant it is compared to other passages with a higher rank in the list.",
                "When presented with a list of passages, the user may provide feedback by highlighting arbitrary spans of text that he or she found relevant.",
                "These spans of text are taken as positive examples in the adaptation of the query profile, and also added to the users history.",
                "Passages not marked by the user are taken as negative examples.",
                "As soon as the query profile is updated, the system re-issues a search and returns another ranked list of passages where the previously seen passages are either removed or ranked low, based on user preference.",
                "For example, if the user highlights ...officials have posted a $100,000 reward for their capture... as relevant answer to the query What steps have been taken so far?, then the highlighted piece is used as an additional positive training example in the adaptation of the query profile.",
                "This piece of feedback is also added to the user history as a seen example, so that in future, the system will not place another passage mentioning $100,000 reward at the top of the ranked list.",
                "However, an article mentioning ...officials have doubled the reward money to $200,000... might be ranked high since it is both relevant to the (updated) query profile and novel with respect to the (updated) user history.",
                "The user may modify the original queries or add a new query during the process; the query profiles will be changed accordingly.",
                "Clearly, novelty detection is very important for the utility of such a system because of the iterative search.",
                "Without novelty detection, the old relevant passages would be shown to the user repeatedly in each ranked list.",
                "Through the above example, we can see the main properties of our new framework for utility-based information distillation over <br>temporally ordered document</br>s.",
                "Our framework combines and extends the power of adaptive filtering (AF), ad-hoc retrieval (IR) and novelty detection (ND).",
                "Compared to standard IR, our approach has the power of incrementally learning long-term information needs and modeling a sequence of queries within a task.",
                "Compared to conventional AF, it enables a more active role of the user in refining his or her information needs and requesting new results by allowing relevance and novelty feedback via highlighting of arbitrary spans of text in passages returned by the system.",
                "Compared to past work, this is the first evaluation of novelty detection integrated with adaptive filtering for sequenced queries that allows flexible user feedback over ranked passages.",
                "The combination of AF, IR and ND with the new extensions raises an important research question regarding evaluation methodology: how can we measure the utility of such an information distillation system?",
                "Existing metrics in standard IR, AF and ND are insufficient, and new solutions must be explored, as we will discuss in Section 4, after describing the technical cores of our system in the next section. 3.",
                "TECHNICAL CORES The core components of CAF´E are - 1) AF for incremental learning of query profiles, 2) IR for estimating relevance of passages with respect to query profiles, 3) ND for assessing novelty of passages with respect to users history, and 4) anti-redundancy component to remove redundancy from ranked lists. 3.1 Adaptive Filtering Component We use a state-of-the-art algorithm in the field - the regularized logistic regression method which had the best results on several benchmark evaluation corpora for AF [21].",
                "Logistic regression (LR) is a supervised learning algorithm for statistical classification.",
                "Based on a training set of labeled instances, it learns a class model which can then by used to predict the labels of unseen instances.",
                "Its performance as well as efficiency in terms of training time makes it a good candidate when frequent updates of the class model are required, as is the case in adaptive filtering, where the system must learn from each new feedback provided by the user. (See [21] and [23] for computational complexity and implementation issues).",
                "In adaptive filtering, each query is considered as a class and the probability of a passage belonging to this class corresponds to the degree of relevance of the passage with respect to the query.",
                "For training the model, we use the query itself as the initial positive training example of the class, and the user-highlighted pieces of text (marked as Relevant or Not-relevant) during feedback as additional training examples.",
                "To address the cold start issue in the early stage before any user feedback is obtained, the system uses a small sample from a retrospective corpus as the initial negative examples in the training set.",
                "The details of using logistic regression for adaptive filtering (assigning different weights to positive and negative training instances, and regularizing the objective function to prevent over-fitting on training data) are presented in [21].",
                "The class model w∗ learned by Logistic Regression, or the query profile, is a vector whose dimensions are individual terms and whose elements are the regression coefficients, indicating how influential each term is in the query profile.",
                "The query profile is updated whenever a new piece of user feedback is received.",
                "A temporally decaying weight can be applied to each training example, as an option, to emphasize the most recent user feedback. 3.2 Passage Retrieval Component We use standard IR techniques in this part of our system.",
                "Incoming documents are processed in chunks, where each chunk can be defined as a fixed span of time or as a fixed number of documents, as preferred by the user.",
                "For each incoming document, corpus statistics like the IDF (Inverted Document Frequency) of each term are updated.",
                "We use a state-of-the-art named entity identifier and tracker [8, 12] to identify person and location names, and merge them with co-referent named entities seen in the past.",
                "Then the documents are segmented into passages, which can be a whole document, a paragraph, a sentence, or any other continuous span of text, as preferred.",
                "Each passage is represented using a vector of TF-IDF (Term FrequencyInverse Document Frequency) weights, where term can be a word or a named entity.",
                "Given a query profile, i.e. the logistic regression solution w∗ as described in Section 3.1, the system computes the posterior probability of relevance for each passage x as fRL(x) ≡ P(y = 1|x, w∗ ) = 1 (1 + e−w∗·x) (1) Passages are ordered by their relevance scores, and the ones with scores above a threshold (tuned on a training set) comprise the relevance list that is passed on to the novelty detection step. 3.3 Novelty Detection Component CAF´E maintains a user history H(t), which contains all the spans of text hi that the user highlighted (as feedback) during his or her past interactions with the system, up to the current time t. Denoting the history as H(t) = n h1, h2, ..., ht o , (2) the novelty score of a new candidate passage x is computed as: fND(x) = 1 − max i∈1..t {cos(x, hi)} (3) where both candidate passage x and highlighted spans of text hi are represented as TF-IDF vectors.",
                "The novelty score of each passage is compared to a prespecified threshold (also tuned on a training set), and any passage with a score below this threshold is removed from the relevance list. 3.4 Anti-redundant Ranking Component Although the novelty detection component ensures that only novel (previously unseen) information remains in the relevance list, this list might still contain the same novel information at multiple positions in the ranked list.",
                "Suppose, for example, that the user has already read about a $100,000 reward for information about the escaped convicts.",
                "A new piece of news that the award has been increased to $200,000 is novel since the user hasnt read about it yet.",
                "However, multiple news sources would report this news and we might end up showing (redundant) articles from all these sources in a ranked list.",
                "Hence, a ranked list should also be made non-redundant with respect to its own contents.",
                "We use a simplified version of the Maximal Marginal Relevance method [5], originally developed for combining relevance and novelty in text retrieval and summarization.",
                "Our procedure starts with the current list of passages sorted by relevance (section 3.2), filtered by Novelty Detection component (section 3.3), and generates a new non-redundant list as follows: 1.",
                "Take the top passage in the current list as the top one in the new list. 2.",
                "Add the next passage x in the current list to the new list only if fAR(x) > t where fAR(x) = 1 − max pi∈Lnew {cos(x, pi)} and Lnew is the set of passages already selected in the new list. 3.",
                "Repeat step 2 until all the passages in the current list have been examined.",
                "After applying the above-mentioned algorithm, each passage in the new list is sufficiently dissimilar to others, thus favoring diversity rather than redundancy in the new ranked list.",
                "The anti-redundancy threshold t is tuned on a training set. 4.",
                "EVALUATION METHODOLOGY The approach we proposed above for information distillation raises important issues regarding evaluation methodology.",
                "Firstly, since our framework allows the output to be passages at different levels of granularity (e.g. k-sentence windows where k may vary) instead of a fixed length, it is not possible to have pre-annotated relevance judgments at all such granularity levels.",
                "Secondly, since we wish to measure the utility of the system output as a combination of both relevance and novelty, traditional relevance-only based measures must be replaced by measures that penalize the repetition of the same information in the system output across time.",
                "Thirdly, since the output of the system is ranked lists, we must reward those systems that present useful information (both relevant and previously unseen) using shorter ranked lists, and penalize those that present the same information using longer ranked lists.",
                "None of the existing measures in ad-hoc retrieval, adaptive filtering, novelty detection or other related areas (text summarization and question answering) have desirable properties in all the three aspects.",
                "Therefore, we must develop a new evaluation methodology. 4.1 Answer Keys To enable the evaluation of a system whose output consists of passages of arbitrary length, we borrow the concept of answer keys from the Question Answering (QA) community, where systems are allowed to return arbitrary spans of text as answers.",
                "Answer keys define what should be present in a system response to receive credit, and are comprised of a collection of information nuggets, i.e. factoid units about which human assessors can make binary decisions of whether or not a system response contains them.",
                "Defining answer keys and making the associated binary decisions are conceptual tasks that require semantic mapping [19], since system-returned passages can contain the same information expressed in many different ways.",
                "Hence, QA evaluations have relied on human assessors for the mapping between various expressions, making the process costly, time consuming, and not scalable to large query and document collections, and extensive system evaluations with various parameter settings. 4.1.1 Automating Evaluation based on Answer Keys Automatic evaluation methods would allow for faster system building and tuning, as well as provide an objective and affordable way of comparing various systems.",
                "Recently, such methods have been proposed, more or less, based on the idea of n-gram co-occurrences.",
                "Pourpre [10] assigns a fractional recall score to a system response based on its unigram overlap with a given nuggets description.",
                "For example, a system response A B C has recall 3/4 with respect to a nugget with description A B C D. However, such an approach is unfair to systems that present the same information but using words other than A, B, C, and D. Another open issue is how to weight individual words in measuring the closeness of a match.",
                "For example, consider the question How many prisoners escaped?.",
                "In the nugget Seven prisoners escaped from a Texas prison, there is no indication that seven is the keyword, and that it must be matched to get any relevance credit.",
                "Using IDF values does not help, since seven will generally not have a higher IDF than words like texas and prison.",
                "Also, redefining the nugget as just seven does not solve the problem since now it might spuriously match any mention of seven out of context.",
                "Nuggeteer [13] works on similar principles but makes binary decisions about whether a nugget is present in a given system response by tuning a threshold.",
                "However, it is also plagued by spurious relevance since not all words contained in the nugget description (or known correct responses) are central to the nugget. 4.1.2 Nugget-Matching Rules We propose a reliable automatic method for determining whether a snippet of text contains a given nugget, based on nugget-matching rules, which are generated using a semiautomatic procedure explained below.",
                "These rules are essentially Boolean queries that will only match against snippets that contain the nugget.",
                "For instance, a candidate rule for matching answers to How many prisoners escaped? is (Texas AND seven AND escape AND (convicts OR prisoners)), possibly with other synonyms and variants in the rule.",
                "For a corpus of news articles, which usually follow a typical formal prose, it is fairly easy to write such simple rules to match expected answers using a bootstrap approach, as described below.",
                "We propose a two-stage approach, inspired by Autoslog [14], that combines the strength of humans in identifying semantically equivalent expressions and the strength of the system in gathering statistical evidence from a humanannotated corpus of documents.",
                "In the first stage, human subjects annotated (using a highlighting tool) portions of ontopic documents that contained answers to each nugget 2 .",
                "In the second stage, subjects used our rule generation tool to create rules that would match the annotations for each nugget.",
                "The tool allows users to enter a Boolean rule as a disjunction of conjunctions (e.g. ((a AND b) OR (a AND c AND d) OR (e))).",
                "Given a candidate rule, our tool uses it as a Boolean query over the entire set of on-topic documents and calculates its recall and precision with respect to the annotations that it is expected to match.",
                "Hence, the subjects can start with a simple rule and iteratively refine it until they are satisfied with its recall and precision.",
                "We observed that it was very easy for humans to improve the precision of a rule by tweaking its existing conjunctions (adding more ANDs), and improving the recall by adding more conjunctions to the disjunction (adding more ORs).",
                "As an example, lets try to create a rule for the nugget which says that seven prisoners escaped from the Texas prison.",
                "We start with a simple rule - (seven).",
                "When we input this into the rule generation tool, we realize that this rule matches many spurious occurrences of seven (e.g. ...seven states...) and thus gets a low precision score.",
                "We can further qualify our rule - Texas AND seven AND convicts.",
                "Next, by looking at the missed annotations, we realize that some news articles mentioned ...seven prisoners escaped.... We then replace convicts with the disjunction (convicts OR prisoners).",
                "We continue tweaking the rule in this manner until we achieve a sufficiently high recall and precision - i.e. the (small number of) misses and false alarms can be safely ignored.",
                "Thus we can create nugget-matching rules that succinctly capture various ways of expressing a nugget, while avoiding matching incorrect (or out of context) responses.",
                "Human involvement in the rule creation process ensures high quality generic rules which can then be used to evaluate arbitrary system responses reliably. 4.2 Evaluating the Utility of a Sequence of Ranked Lists The utility of a retrieval system can be defined as the difference between how much the user gained in terms of useful information, and how much the user lost in terms of time and energy.",
                "We calculate this utility from the utilities of individual passages as follows.",
                "After reading each passage returned by the system, the user derives some gain depending on the presence of relevant and novel information, and incurs a loss in terms of the time and energy spent in going through the passage.",
                "However, the likelihood that the user would actually read a passage depends on its position in the ranked list.",
                "Hence, for a query q, the expected utility 2 LDC [18] already provides relevance judgments for 100 topics on the TDT4 corpus.",
                "We further ensured that these judgments are exhaustive on the entire corpus using pooling. of a passage pi at rank i can be defined as U(pi, q) = P(i) ∗ (Gain(pi, q) − Loss(pi, q)) (4) where P(i) is the probability that the user would go through a passage at rank i.",
                "The expected utility for an entire ranked list of length n can be calculated simply by adding the expected utility of each passage: U(q) = nX i=1 P(i) ∗ (Gain(pi, q) − Loss(pi, q)) (5) Note that if we ignore the loss term and define P(i) as P(i) ∝ 1/ logb(b + i − 1) (6) then we get the recently popularized metric called Discounted Cumulated Gain (DCG) [9], where Gain(pi, q) is defined as the graded relevance of passage pi.",
                "However, without the loss term, DCG is a purely recall-oriented metric and not suitable for an adaptive filtering setting, where the systems utility depends in part on its ability to limit the number of items shown to the user.",
                "Although P(i) could be defined based on empirical studies of user behavior, for simplicity, we use P(i) exactly as defined in equation 6.",
                "The gain G(pi, q) of passage pi with respect to the query q is a function of - 1) the number of relevant nuggets present in pi, and 2) the novelty of each of these nuggets.",
                "We combine these two factors as follows.",
                "For each nugget Nj, we assign an initial weight wj, and also keep a count nj of the number of times this nugget has been seen by the user in the past.",
                "The gain derived from each subsequent occurrence of the same nugget is assumed to reduce by a dampening factor γ.",
                "Thus, G(pi, q) is defined as G(pi, q) = X Nj ∈C(pi,q) wj ∗ γnj (7) where C(pi, q) is the set of all nuggets that appear in passage pi and also belong to the answer key of query q.",
                "The initial weights wj are all set of be 1.0 in our experiments, but can also be set based on a pyramid approach [11].",
                "The choice of dampening factor γ determines the users tolerance for redundancy.",
                "When γ = 0, a nugget will only receive credit for its first occurrence i.e. when nj is zero3 .",
                "For 0 < γ < 1, a nugget receives smaller credit for each successive occurrence.",
                "When γ = 1, no dampening occurs and repeated occurrences of a nugget receive the same credit.",
                "Note that the nugget occurrence counts are preserved between evaluation of successive ranked lists returned by the system, since the users are expected to remember what the system showed them in the past.",
                "We define the loss L(pi, q) as a constant cost c (we use 0.1) incurred when reading a system-returned passage.",
                "Thus, our metric can be re-written as U(q) = nX i=1 Gain(pi, q) logb(b + i − 1) − L(n) (8) where L(n) is the loss associated with a ranked list of length n: L(n) = c · nX i=1 1 logb(b + i − 1) (9) 3 Note that 00 = 1 Due to the similarity with Discounted Cumulated Gain (DCG), we call our metric Discounted Cumulated Utility (DCU).",
                "The DCU score obtained by the system is converted to a Normalized DCU (NDCU) score by dividing it by the DCU score of the ideal ranked list, which is created by ordering passages by their decreasing utility scores U(pi, q) and stopping when U(pi, q) ≤ 0 i.e. when the gain is less than or equal to the cost of reading the passage. 5.",
                "DATA TDT4 was the benchmark corpus used in TDT2002 and TDT2003 evaluations.",
                "The corpus consists of over 90, 000 news articles from multiple sources (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) published between October 2000 and January 2001, in the languages of Arabic, English, and Mandarin.",
                "Speech-recognized and machine-translated versions of the non-English articles were provided as well.",
                "LDC [18] has annotated the corpus with 100 topics, that correspond to various news events in this time period.",
                "Out of these, we selected a subset of 12 actionable events, and defined corresponding tasks for them4 .",
                "For each task, we manually defined a profile consisting of an initial set of (5 to 10) queries, a free-text description of the user history, i.e., what the user already knows about the event, and a list of known on-topic and off-topic documents (if available) as training examples.",
                "For each query, we generated answer keys and corresponding nugget matching rules using the procedure described in section 4.1.2, and produced a total of 120 queries, with an average of 7 nuggets per query. 6.",
                "EXPERIMENTS AND RESULTS 6.1 Baselines We used Indri [17], a popular language-model based retrieval engine, as a baseline for comparison with CAF´E.",
                "Indri supports standard search engine functionality, including pseudo-relevance feedback (PRF) [3, 6], and is representative of a typical query-based retrieval system.",
                "Indri does not support any kind of novelty detection.",
                "We compare Indri with PRF turned on and off, against CAF´E with user feedback, novelty detection and antiredundant ranking turned on and off. 6.2 Experimental Setup We divided the TDT4 corpus spanning 4 months into 10 chunks, each defined as a period of 12 consecutive days.",
                "At any given point of time in the distillation process, each system accessed the past data up to the current point, and returned a ranked list of up 50 passages per query.",
                "The 12 tasks defined on the corpus were divided into a training and test set with 6 tasks each.",
                "Each system was allowed to use the training set to tune its parameters for optimizing NDCU (equation 8), including the relevance threshold for both Indri and CAF´E, and the novelty and antiredundancy thresholds for CAF´E.",
                "The NDCU for each system run is calculated automatically.",
                "User feedback was also simulated - relevance judgments for each system-returned passage (as determined by the nugget matching rules described in section 4.1.2) were 4 URL: http://nyc.lti.cs.cmu.edu/downloads Figure 1: Performance of Indri across chunks Figure 2: Performance of CAF´E across chunks used as user feedback in the adaptation of query profiles and user histories. 6.3 Results In Table 1, we show the NDCU scores of the two systems under various settings.",
                "These scores are averaged over the six tasks in the test set, and are calculated with two dampening factors (see section 4.2): γ = 0 and 0.1, to simulate no tolerance and small tolerance for redundancy, respectively.",
                "Using γ = 0 creates a much more strict metric since it does not give any credit to a passage that contains relevant but redundant information.",
                "Hence, the improvement obtained from enabling user feedback is smaller with γ = 0 than the improvement obtained from feedback with γ = 0.1.",
                "This reveals a shortcoming of contemporary retrieval systemswhen the user gives positive feedback on a passage, the systems gives higher weights to the terms present in that passage and tends to retrieve other passages containing the same terms - and thus - usually the same information.",
                "However, the user does not benefit from seeing such redundant passages, and is usually interested in other passages containing related information.",
                "It is informative to evaluate retrieval systems using our utility measure (with γ = 0) which accounts for novelty and thus gives a more realistic picture of how well a system can generalize from user feedback, rather than using traditional IR measures like recall and precision which give an incomplete picture of improvement obtained from user feedback.",
                "Sometimes, however, users might indeed be interested in seeing the same information from multiple sources, as an Table 1: NDCU Scores of Indri and CAF´E for two dampening factors (γ), and various settings (F: Feedback, N: Novelty Detection, A: Anti-Redundant Ranking) Indri CAF´E γ Base +PRF Base +F +F+N +F+A +F+N+A 0 0.19 0.19 0.22 0.23 0.24 0.24 0.24 0.1 0.28 0.29 0.24 0.35 0.35 0.36 0.36 indicator of its importance or reliability.",
                "In such a case, we can simply choose a higher value for γ which corresponds to a higher tolerance for redundancy, and hence let the system tune its parameters accordingly.",
                "Since documents were processed chunk by chunk, it would be interesting to see how the performance of systems improves over time.",
                "Figures 1 and 2 show the performance trends for both the systems across chunks.",
                "While the performance with and without feedback on the first few chunks is expected to be close, for subsequent chunks, the performance curve with feedback enabled rises above the one with the no-feedback setting.",
                "The performance trends are not consistent across all chunks because on-topic documents are not uniformly distributed over all the chunks, making some queries easier than others in certain chunks.",
                "Moreover, since Indri uses pseudo-relevance feedback while CAF´E uses feedback based on actual relevance judgments, the improvement in case of Indri is less dramatic than that of CAF´E. 7.",
                "CONCLUDING REMARKS This paper presents the first investigation on utility-based information distillation with a system that learns the longlasting information needs from fine-grained user feedback over a sequence of ranked passages.",
                "Our system, called CAF´E, combines adaptive filtering, novelty detection and antiredundant passage ranking in a unified framework for utility optimization.",
                "We developed a new scheme for automated evaluation and feedback based on a semi-automatic procedure for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also proposed an extension of the NDCG metric for assessing the utility of ranked passages as a weighted combination of relevance and novelty.",
                "Our experiments on the newly annotated TDT4 benchmark corpus show encouraging utility enhancement over Indri, and also over our own system with incremental learning and novelty detection turned off. 8.",
                "ACKNOWLEDGMENTS We would like to thank Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng, and the Qualitative Data Analysis Program at the University of Pittsburgh lead by Dr. Stuart Shulman for their help with collecting and processing the extended TDT4 annotations used in our experiments.",
                "This work is supported in parts by the National Science Foundation (NSF) under grant IIS0434035, and the Defense Advanced Research Project Agency (DARPA) under contracts NBCHD030010 and W0550432.",
                "Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors. 9.",
                "ADDITIONAL AUTHORS Jian Zhang (jianzhan@stat.purdue.edu)∗ , Jaime Carbonell (jgc@cs.cmu.edu)† , Peter Brusilovsky (peterb+@pitt.edu)‡ , Daqing He(dah44@pitt.edu)‡ 10.",
                "REFERENCES [1] J. Allan.",
                "Incremental Relevance Feedback for Information Filtering.",
                "Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, pages 270-278, 1996. [2] J. Allan, C. Wade, and A. Bolivar.",
                "Retrieval and Novelty Detection at the Sentence Level.",
                "Proceedings of the ACM SIGIR conference on research and development in information retrieval, 2003. [3] C. Buckley, G. Salton, and J. Allan.",
                "Automatic Retrieval with Locality Information using SMART.",
                "NIST special publication, (500207):59-72, 1993. [4] J. Callan.",
                "Learning While Filtering Documents.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 224-231, 1998. [5] J. Carbonell and J. Goldstein.",
                "The use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 335-336, 1998. [6] E. Efthimiadis.",
                "Query Expansion.",
                "Annual Review of Information Science and Technology (ARIST), 31:p121-87, 1996. [7] J. Fiscus and G. Duddington.",
                "Topic Detection and Tracking Overview.",
                "Topic Detection and Tracking: Event-based Information Organization, pages 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos.",
                "A Statistical Model for Multilingual Entity Detection and Tracking.",
                "NAACL/HLT, 2004. [9] K. J¨arvelin and J. Kek¨al¨ainen.",
                "Cumulated Gain-based Evaluation of IR Techniques.",
                "ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin and D. Demner-Fushman.",
                "Automatically Evaluating Answers to Definition Questions.",
                "Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), 2005. ∗ Statistics Dept., Purdue University, West Lafayette, USA † Language Technologies Inst., Carnegie Mellon University, Pittsburgh, USA ‡ School of Information Sciences, Univ. of Pittsburgh, Pittsburgh, USA [11] J. Lin and D. Demner-Fushman.",
                "Will Pyramids Built of nUggets Topple Over.",
                "Proceedings of HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos.",
                "A Mention-synchronous Coreference Resolution Algorithm based on the Bell Tree.",
                "Proc. of ACL, 4:136-143, 2004. [13] G. Marton.",
                "Nuggeteer: Automatic Nugget-Based Evaluation Using Descriptions and Judgments.",
                "HLT/NAACL, 2006. [14] E. Riloff.",
                "Automatically Constructing a Dictionary for Information Extraction Tasks.",
                "Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 811-816, 1993. [15] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9: Filtering track.",
                "The Ninth Text REtrieval Conference (TREC-9), pages 361-368. [16] R. Schapire, Y.",
                "Singer, and A. Singhal.",
                "Boosting and Rocchio Applied to Text Filtering.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle, and W. Croft.",
                "Indri: A Language Model-based Search Engine for Complex Queries.",
                "Proceedings of the International Conference on Intelligence Analysis, 2004. [18] The Linguistic Data Consortium. http://www.ldc.upenn.edu/. [19] E. Voorhees.",
                "Overview of the TREC 2003 Question Answering Track.",
                "Proceedings of the Twelfth Text REtrieval Conference (TREC 2003), 2003. [20] Y. Yang and B. Kisiel.",
                "Margin-based Local Regression for Adaptive Filtering.",
                "Proceedings of the twelfth international conference on Information and knowledge management, pages 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of Adaptive Filtering Methods in a Cross-benchmark Evaluation.",
                "Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 98-105, 2005. [22] C. Zhai, W. Cohen, and J. Lafferty.",
                "Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 10-17, 2003. [23] J. Zhang and Y. Yang.",
                "Robustness of Regularized Linear Classification Methods in Text Categorization.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 190-197, 2003. [24] Y. Zhang.",
                "Using Bayesian Priors to Combine Classifiers for Adaptive Filtering.",
                "Proceedings of the 27th annual international conference on Research and development in information retrieval, pages 345-352, 2004. [25] Y. Zhang, J. Callan, and T. Minka.",
                "Novelty and Redundancy Detection in Adaptive Filtering.",
                "Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Carnegie Mellon University Pittsburgh, EE. UU. Mrogati@cs.cmu.edu Resumen Este documento examina un nuevo enfoque para la destilación de información sobre \"documento ordenado temporalmente\", y propone un nuevo esquema de evaluación para dicho marco.",
                "A través del ejemplo anterior, podemos ver las propiedades principales de nuestro nuevo marco para la destilación de información basada en servicios públicos sobre \"documento ordenado temporalmente\" s."
            ],
            "translated_text": "",
            "candidates": [
                "documento ordenado temporalmente",
                "documento ordenado temporalmente",
                "documento ordenado temporalmente",
                "documento ordenado temporalmente"
            ],
            "error": []
        },
        "passage ranking": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Utility-based Information Distillation Over Temporally Sequenced Documents Yiming Yang Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA yiming@cs.cmu.edu Abhimanyu Lad Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA alad@cs.cmu.edu Ni Lao Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA nlao@cs.cmu.edu Abhay Harpale Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA aharpale@cs.cmu.edu Bryan Kisiel Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA bkisiel@cs.cmu.edu Monica Rogati Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA mrogati@cs.cmu.edu ABSTRACT This paper examines a new approach to information distillation over temporally ordered documents, and proposes a novel evaluation scheme for such a framework.",
                "It combines the strengths of and extends beyond conventional adaptive filtering, novelty detection and non-redundant <br>passage ranking</br> with respect to long-lasting information needs (tasks with multiple queries).",
                "Our approach supports fine-grained user feedback via highlighting of arbitrary spans of text, and leverages such information for utility optimization in adaptive settings.",
                "For our experiments, we defined hypothetical tasks based on news events in the TDT4 corpus, with multiple queries per task.",
                "Answer keys (nuggets) were generated for each query and a semiautomatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also propose an extension of the NDCG metric for assessing the utility of ranked passages as a combination of relevance and novelty.",
                "Our results show encouraging utility enhancements using the new approach, compared to the baseline systems without incremental learning or the novelty detection components.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 General Terms Design, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Tracking new and relevant information from temporal data streams for users with long-lasting needs has been a challenging research topic in information retrieval.",
                "Adaptive filtering (AF) is one such task of online prediction of the relevance of each new document with respect to pre-defined topics.",
                "Based on the initial query and a few positive examples (if available), an AF system maintains a profile for each such topic of interest, and constantly updates it based on feedback from the user.",
                "The incremental learning nature of AF systems makes them more powerful than standard search engines that support ad-hoc retrieval (e.g.",
                "Google and Yahoo) in terms of finding relevant information with respect to long-lasting topics of interest, and more attractive for users who are willing to provide feedback to adapt the system towards their specific information needs, without having to modify their queries manually.",
                "A variety of supervised learning algorithms (Rocchio-style classifiers, Exponential-Gaussian models, local regression and logistic regression approaches) have been studied for adaptive settings, examined with explicit and implicit relevance feedback, and evaluated with respect to utility optimization on large benchmark data collections in TREC (Text Retrieval Conferences) and TDT (Topic Detection and Tracking) forums [1, 4, 7, 15, 16, 20, 24, 23].",
                "Regularized logistic regression [21] has been found representative for the state-of-the-art approaches, and highly efficient for frequent model adaptations over large document collections such as the TREC-10 corpus (over 800,000 documents and 84 topics).",
                "Despite substantial achievements in recent adaptive filtering research, significant problems remain unsolved regarding how to leverage user feedback effectively and efficiently.",
                "Specifically, the following issues may seriously limit the true utility of AF systems in real-world applications: 1.",
                "User has a rather passive role in the conventional adaptive filtering setup - he or she reacts to the system only when the system makes a yes decision on a document, by confirming or rejecting that decision.",
                "A more active alternative would be to allow the user to issue multiple queries for a topic, review a ranked list of candidate documents (or passages) per query, and provide feedback on the ranked list, thus refining their information need and requesting updated ranked lists.",
                "The latter form of user interaction has been highly effective in standard retrieval for ad-hoc queries.",
                "How to deploy such a strategy for long-lasting information needs in AF settings is an open question for research. 2.",
                "The unit for receiving a relevance judgment (yes or no) is restricted to the document level in conventional AF.",
                "However, a real user may be willing to provide more informative, fine-grained feedback via highlighting some pieces of text in a retrieved document as relevant, instead of labeling the entire document as relevant.",
                "Effectively leveraging such fine-grained feedback could substantially enhance the quality of an AF system.",
                "For this, we need to enable supervised learning from labeled pieces of text of arbitrary span instead of just allowing labeled documents. 3.",
                "System-selected documents are often highly redundant.",
                "A major news event, for example, would be reported by multiple sources repeatedly for a while, making most of the information content in those articles redundant with each other.",
                "A conventional AF system would select all these redundant news stories for user feedback, wasting the users time while offering little gain.",
                "Clearly, techniques for novelty detection can help in principle [25, 2, 22] for improving the utility of the AF systems.",
                "However, the effectiveness of such techniques at passage level to detect novelty with respect to users (fine-grained) feedback and to detect redundancy in ranked lists remains to be evaluated using a measure of utility that mimics the needs of a real user.",
                "To address the above limitations of current AF systems, we propose and examine a new approach in this paper, combining the strengths of conventional AF (incremental learning of topic models), multi-pass passage retrieval for long-lasting queries conditioned on topic, and novelty detection for removal of redundancy from user interactions with the system.",
                "We call the new process utility-based information distillation.",
                "Note that conventional benchmark corpora for AF evaluations, which have relevance judgments at the document level and do not define tasks with multiple queries, are insufficient for evaluating the new approach.",
                "Therefore, we extended a benchmark corpus - the TDT4 collection of news stories and TV broadcasts - with task definitions, multiple queries per task, and answer keys per query.",
                "We have conducted our experiments on this extended TDT4 corpus and have made the additionally generated data publicly available for future comparative evaluations 1 .",
                "To automatically evaluate the system-returned arbitrary spans of text using our answer keys, we further developed an evaluation scheme with semi-automatic procedure for 1 URL: http://nyc.lti.cs.cmu.edu/downloads acquiring rules that can match nuggets against system responses.",
                "Moreover, we propose an extension of NDCG (Normalized Discounted Cumulated Gain) [9] for assessing the utility of ranked passages as a function of both relevance and novelty.",
                "The rest of this paper is organized as follows.",
                "Section 2 outlines the information distillation process with a concrete example.",
                "Section 3 describes the technical cores of our system called CAF´E - CMU Adaptive Filtering Engine.",
                "Section 4 discusses issues with respect to evaluation methodology and proposes a new scheme.",
                "Section 5 describes the extended TDT4 corpus.",
                "Section 6 presents our experiments and results.",
                "Section 7 concludes the study and gives future perspectives. 2.",
                "A SAMPLE TASK Consider a news event - the escape of seven convicts from a Texas prison in December 2000 and their capture a month later.",
                "Assuming a user were interested in this event since its early stage, the information need could be: Find information about the escape of convicts from Texas prison, and information related to their recapture.",
                "The associated lower-level questions could be: 1.",
                "How many prisoners escaped? 2.",
                "Where and when were they sighted? 3. Who are their known contacts inside and outside the prison? 4.",
                "How are they armed? 5.",
                "Do they have any vehicles? 6.",
                "What steps have been taken so far?",
                "We call such an information need a task, and the associated questions as the queries in this task.",
                "A distillation system is supposed to monitor the incoming documents, process them chunk by chunk in a temporal order, select potentially relevant and novel passages from each chunk with respect to each query, and present a ranked list of passages to the user.",
                "<br>passage ranking</br> here is based on how relevant a passage is with respect to the current query, how novel it is with respect to the current user history (of his or her interactions with the system), and how redundant it is compared to other passages with a higher rank in the list.",
                "When presented with a list of passages, the user may provide feedback by highlighting arbitrary spans of text that he or she found relevant.",
                "These spans of text are taken as positive examples in the adaptation of the query profile, and also added to the users history.",
                "Passages not marked by the user are taken as negative examples.",
                "As soon as the query profile is updated, the system re-issues a search and returns another ranked list of passages where the previously seen passages are either removed or ranked low, based on user preference.",
                "For example, if the user highlights ...officials have posted a $100,000 reward for their capture... as relevant answer to the query What steps have been taken so far?, then the highlighted piece is used as an additional positive training example in the adaptation of the query profile.",
                "This piece of feedback is also added to the user history as a seen example, so that in future, the system will not place another passage mentioning $100,000 reward at the top of the ranked list.",
                "However, an article mentioning ...officials have doubled the reward money to $200,000... might be ranked high since it is both relevant to the (updated) query profile and novel with respect to the (updated) user history.",
                "The user may modify the original queries or add a new query during the process; the query profiles will be changed accordingly.",
                "Clearly, novelty detection is very important for the utility of such a system because of the iterative search.",
                "Without novelty detection, the old relevant passages would be shown to the user repeatedly in each ranked list.",
                "Through the above example, we can see the main properties of our new framework for utility-based information distillation over temporally ordered documents.",
                "Our framework combines and extends the power of adaptive filtering (AF), ad-hoc retrieval (IR) and novelty detection (ND).",
                "Compared to standard IR, our approach has the power of incrementally learning long-term information needs and modeling a sequence of queries within a task.",
                "Compared to conventional AF, it enables a more active role of the user in refining his or her information needs and requesting new results by allowing relevance and novelty feedback via highlighting of arbitrary spans of text in passages returned by the system.",
                "Compared to past work, this is the first evaluation of novelty detection integrated with adaptive filtering for sequenced queries that allows flexible user feedback over ranked passages.",
                "The combination of AF, IR and ND with the new extensions raises an important research question regarding evaluation methodology: how can we measure the utility of such an information distillation system?",
                "Existing metrics in standard IR, AF and ND are insufficient, and new solutions must be explored, as we will discuss in Section 4, after describing the technical cores of our system in the next section. 3.",
                "TECHNICAL CORES The core components of CAF´E are - 1) AF for incremental learning of query profiles, 2) IR for estimating relevance of passages with respect to query profiles, 3) ND for assessing novelty of passages with respect to users history, and 4) anti-redundancy component to remove redundancy from ranked lists. 3.1 Adaptive Filtering Component We use a state-of-the-art algorithm in the field - the regularized logistic regression method which had the best results on several benchmark evaluation corpora for AF [21].",
                "Logistic regression (LR) is a supervised learning algorithm for statistical classification.",
                "Based on a training set of labeled instances, it learns a class model which can then by used to predict the labels of unseen instances.",
                "Its performance as well as efficiency in terms of training time makes it a good candidate when frequent updates of the class model are required, as is the case in adaptive filtering, where the system must learn from each new feedback provided by the user. (See [21] and [23] for computational complexity and implementation issues).",
                "In adaptive filtering, each query is considered as a class and the probability of a passage belonging to this class corresponds to the degree of relevance of the passage with respect to the query.",
                "For training the model, we use the query itself as the initial positive training example of the class, and the user-highlighted pieces of text (marked as Relevant or Not-relevant) during feedback as additional training examples.",
                "To address the cold start issue in the early stage before any user feedback is obtained, the system uses a small sample from a retrospective corpus as the initial negative examples in the training set.",
                "The details of using logistic regression for adaptive filtering (assigning different weights to positive and negative training instances, and regularizing the objective function to prevent over-fitting on training data) are presented in [21].",
                "The class model w∗ learned by Logistic Regression, or the query profile, is a vector whose dimensions are individual terms and whose elements are the regression coefficients, indicating how influential each term is in the query profile.",
                "The query profile is updated whenever a new piece of user feedback is received.",
                "A temporally decaying weight can be applied to each training example, as an option, to emphasize the most recent user feedback. 3.2 Passage Retrieval Component We use standard IR techniques in this part of our system.",
                "Incoming documents are processed in chunks, where each chunk can be defined as a fixed span of time or as a fixed number of documents, as preferred by the user.",
                "For each incoming document, corpus statistics like the IDF (Inverted Document Frequency) of each term are updated.",
                "We use a state-of-the-art named entity identifier and tracker [8, 12] to identify person and location names, and merge them with co-referent named entities seen in the past.",
                "Then the documents are segmented into passages, which can be a whole document, a paragraph, a sentence, or any other continuous span of text, as preferred.",
                "Each passage is represented using a vector of TF-IDF (Term FrequencyInverse Document Frequency) weights, where term can be a word or a named entity.",
                "Given a query profile, i.e. the logistic regression solution w∗ as described in Section 3.1, the system computes the posterior probability of relevance for each passage x as fRL(x) ≡ P(y = 1|x, w∗ ) = 1 (1 + e−w∗·x) (1) Passages are ordered by their relevance scores, and the ones with scores above a threshold (tuned on a training set) comprise the relevance list that is passed on to the novelty detection step. 3.3 Novelty Detection Component CAF´E maintains a user history H(t), which contains all the spans of text hi that the user highlighted (as feedback) during his or her past interactions with the system, up to the current time t. Denoting the history as H(t) = n h1, h2, ..., ht o , (2) the novelty score of a new candidate passage x is computed as: fND(x) = 1 − max i∈1..t {cos(x, hi)} (3) where both candidate passage x and highlighted spans of text hi are represented as TF-IDF vectors.",
                "The novelty score of each passage is compared to a prespecified threshold (also tuned on a training set), and any passage with a score below this threshold is removed from the relevance list. 3.4 Anti-redundant Ranking Component Although the novelty detection component ensures that only novel (previously unseen) information remains in the relevance list, this list might still contain the same novel information at multiple positions in the ranked list.",
                "Suppose, for example, that the user has already read about a $100,000 reward for information about the escaped convicts.",
                "A new piece of news that the award has been increased to $200,000 is novel since the user hasnt read about it yet.",
                "However, multiple news sources would report this news and we might end up showing (redundant) articles from all these sources in a ranked list.",
                "Hence, a ranked list should also be made non-redundant with respect to its own contents.",
                "We use a simplified version of the Maximal Marginal Relevance method [5], originally developed for combining relevance and novelty in text retrieval and summarization.",
                "Our procedure starts with the current list of passages sorted by relevance (section 3.2), filtered by Novelty Detection component (section 3.3), and generates a new non-redundant list as follows: 1.",
                "Take the top passage in the current list as the top one in the new list. 2.",
                "Add the next passage x in the current list to the new list only if fAR(x) > t where fAR(x) = 1 − max pi∈Lnew {cos(x, pi)} and Lnew is the set of passages already selected in the new list. 3.",
                "Repeat step 2 until all the passages in the current list have been examined.",
                "After applying the above-mentioned algorithm, each passage in the new list is sufficiently dissimilar to others, thus favoring diversity rather than redundancy in the new ranked list.",
                "The anti-redundancy threshold t is tuned on a training set. 4.",
                "EVALUATION METHODOLOGY The approach we proposed above for information distillation raises important issues regarding evaluation methodology.",
                "Firstly, since our framework allows the output to be passages at different levels of granularity (e.g. k-sentence windows where k may vary) instead of a fixed length, it is not possible to have pre-annotated relevance judgments at all such granularity levels.",
                "Secondly, since we wish to measure the utility of the system output as a combination of both relevance and novelty, traditional relevance-only based measures must be replaced by measures that penalize the repetition of the same information in the system output across time.",
                "Thirdly, since the output of the system is ranked lists, we must reward those systems that present useful information (both relevant and previously unseen) using shorter ranked lists, and penalize those that present the same information using longer ranked lists.",
                "None of the existing measures in ad-hoc retrieval, adaptive filtering, novelty detection or other related areas (text summarization and question answering) have desirable properties in all the three aspects.",
                "Therefore, we must develop a new evaluation methodology. 4.1 Answer Keys To enable the evaluation of a system whose output consists of passages of arbitrary length, we borrow the concept of answer keys from the Question Answering (QA) community, where systems are allowed to return arbitrary spans of text as answers.",
                "Answer keys define what should be present in a system response to receive credit, and are comprised of a collection of information nuggets, i.e. factoid units about which human assessors can make binary decisions of whether or not a system response contains them.",
                "Defining answer keys and making the associated binary decisions are conceptual tasks that require semantic mapping [19], since system-returned passages can contain the same information expressed in many different ways.",
                "Hence, QA evaluations have relied on human assessors for the mapping between various expressions, making the process costly, time consuming, and not scalable to large query and document collections, and extensive system evaluations with various parameter settings. 4.1.1 Automating Evaluation based on Answer Keys Automatic evaluation methods would allow for faster system building and tuning, as well as provide an objective and affordable way of comparing various systems.",
                "Recently, such methods have been proposed, more or less, based on the idea of n-gram co-occurrences.",
                "Pourpre [10] assigns a fractional recall score to a system response based on its unigram overlap with a given nuggets description.",
                "For example, a system response A B C has recall 3/4 with respect to a nugget with description A B C D. However, such an approach is unfair to systems that present the same information but using words other than A, B, C, and D. Another open issue is how to weight individual words in measuring the closeness of a match.",
                "For example, consider the question How many prisoners escaped?.",
                "In the nugget Seven prisoners escaped from a Texas prison, there is no indication that seven is the keyword, and that it must be matched to get any relevance credit.",
                "Using IDF values does not help, since seven will generally not have a higher IDF than words like texas and prison.",
                "Also, redefining the nugget as just seven does not solve the problem since now it might spuriously match any mention of seven out of context.",
                "Nuggeteer [13] works on similar principles but makes binary decisions about whether a nugget is present in a given system response by tuning a threshold.",
                "However, it is also plagued by spurious relevance since not all words contained in the nugget description (or known correct responses) are central to the nugget. 4.1.2 Nugget-Matching Rules We propose a reliable automatic method for determining whether a snippet of text contains a given nugget, based on nugget-matching rules, which are generated using a semiautomatic procedure explained below.",
                "These rules are essentially Boolean queries that will only match against snippets that contain the nugget.",
                "For instance, a candidate rule for matching answers to How many prisoners escaped? is (Texas AND seven AND escape AND (convicts OR prisoners)), possibly with other synonyms and variants in the rule.",
                "For a corpus of news articles, which usually follow a typical formal prose, it is fairly easy to write such simple rules to match expected answers using a bootstrap approach, as described below.",
                "We propose a two-stage approach, inspired by Autoslog [14], that combines the strength of humans in identifying semantically equivalent expressions and the strength of the system in gathering statistical evidence from a humanannotated corpus of documents.",
                "In the first stage, human subjects annotated (using a highlighting tool) portions of ontopic documents that contained answers to each nugget 2 .",
                "In the second stage, subjects used our rule generation tool to create rules that would match the annotations for each nugget.",
                "The tool allows users to enter a Boolean rule as a disjunction of conjunctions (e.g. ((a AND b) OR (a AND c AND d) OR (e))).",
                "Given a candidate rule, our tool uses it as a Boolean query over the entire set of on-topic documents and calculates its recall and precision with respect to the annotations that it is expected to match.",
                "Hence, the subjects can start with a simple rule and iteratively refine it until they are satisfied with its recall and precision.",
                "We observed that it was very easy for humans to improve the precision of a rule by tweaking its existing conjunctions (adding more ANDs), and improving the recall by adding more conjunctions to the disjunction (adding more ORs).",
                "As an example, lets try to create a rule for the nugget which says that seven prisoners escaped from the Texas prison.",
                "We start with a simple rule - (seven).",
                "When we input this into the rule generation tool, we realize that this rule matches many spurious occurrences of seven (e.g. ...seven states...) and thus gets a low precision score.",
                "We can further qualify our rule - Texas AND seven AND convicts.",
                "Next, by looking at the missed annotations, we realize that some news articles mentioned ...seven prisoners escaped.... We then replace convicts with the disjunction (convicts OR prisoners).",
                "We continue tweaking the rule in this manner until we achieve a sufficiently high recall and precision - i.e. the (small number of) misses and false alarms can be safely ignored.",
                "Thus we can create nugget-matching rules that succinctly capture various ways of expressing a nugget, while avoiding matching incorrect (or out of context) responses.",
                "Human involvement in the rule creation process ensures high quality generic rules which can then be used to evaluate arbitrary system responses reliably. 4.2 Evaluating the Utility of a Sequence of Ranked Lists The utility of a retrieval system can be defined as the difference between how much the user gained in terms of useful information, and how much the user lost in terms of time and energy.",
                "We calculate this utility from the utilities of individual passages as follows.",
                "After reading each passage returned by the system, the user derives some gain depending on the presence of relevant and novel information, and incurs a loss in terms of the time and energy spent in going through the passage.",
                "However, the likelihood that the user would actually read a passage depends on its position in the ranked list.",
                "Hence, for a query q, the expected utility 2 LDC [18] already provides relevance judgments for 100 topics on the TDT4 corpus.",
                "We further ensured that these judgments are exhaustive on the entire corpus using pooling. of a passage pi at rank i can be defined as U(pi, q) = P(i) ∗ (Gain(pi, q) − Loss(pi, q)) (4) where P(i) is the probability that the user would go through a passage at rank i.",
                "The expected utility for an entire ranked list of length n can be calculated simply by adding the expected utility of each passage: U(q) = nX i=1 P(i) ∗ (Gain(pi, q) − Loss(pi, q)) (5) Note that if we ignore the loss term and define P(i) as P(i) ∝ 1/ logb(b + i − 1) (6) then we get the recently popularized metric called Discounted Cumulated Gain (DCG) [9], where Gain(pi, q) is defined as the graded relevance of passage pi.",
                "However, without the loss term, DCG is a purely recall-oriented metric and not suitable for an adaptive filtering setting, where the systems utility depends in part on its ability to limit the number of items shown to the user.",
                "Although P(i) could be defined based on empirical studies of user behavior, for simplicity, we use P(i) exactly as defined in equation 6.",
                "The gain G(pi, q) of passage pi with respect to the query q is a function of - 1) the number of relevant nuggets present in pi, and 2) the novelty of each of these nuggets.",
                "We combine these two factors as follows.",
                "For each nugget Nj, we assign an initial weight wj, and also keep a count nj of the number of times this nugget has been seen by the user in the past.",
                "The gain derived from each subsequent occurrence of the same nugget is assumed to reduce by a dampening factor γ.",
                "Thus, G(pi, q) is defined as G(pi, q) = X Nj ∈C(pi,q) wj ∗ γnj (7) where C(pi, q) is the set of all nuggets that appear in passage pi and also belong to the answer key of query q.",
                "The initial weights wj are all set of be 1.0 in our experiments, but can also be set based on a pyramid approach [11].",
                "The choice of dampening factor γ determines the users tolerance for redundancy.",
                "When γ = 0, a nugget will only receive credit for its first occurrence i.e. when nj is zero3 .",
                "For 0 < γ < 1, a nugget receives smaller credit for each successive occurrence.",
                "When γ = 1, no dampening occurs and repeated occurrences of a nugget receive the same credit.",
                "Note that the nugget occurrence counts are preserved between evaluation of successive ranked lists returned by the system, since the users are expected to remember what the system showed them in the past.",
                "We define the loss L(pi, q) as a constant cost c (we use 0.1) incurred when reading a system-returned passage.",
                "Thus, our metric can be re-written as U(q) = nX i=1 Gain(pi, q) logb(b + i − 1) − L(n) (8) where L(n) is the loss associated with a ranked list of length n: L(n) = c · nX i=1 1 logb(b + i − 1) (9) 3 Note that 00 = 1 Due to the similarity with Discounted Cumulated Gain (DCG), we call our metric Discounted Cumulated Utility (DCU).",
                "The DCU score obtained by the system is converted to a Normalized DCU (NDCU) score by dividing it by the DCU score of the ideal ranked list, which is created by ordering passages by their decreasing utility scores U(pi, q) and stopping when U(pi, q) ≤ 0 i.e. when the gain is less than or equal to the cost of reading the passage. 5.",
                "DATA TDT4 was the benchmark corpus used in TDT2002 and TDT2003 evaluations.",
                "The corpus consists of over 90, 000 news articles from multiple sources (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) published between October 2000 and January 2001, in the languages of Arabic, English, and Mandarin.",
                "Speech-recognized and machine-translated versions of the non-English articles were provided as well.",
                "LDC [18] has annotated the corpus with 100 topics, that correspond to various news events in this time period.",
                "Out of these, we selected a subset of 12 actionable events, and defined corresponding tasks for them4 .",
                "For each task, we manually defined a profile consisting of an initial set of (5 to 10) queries, a free-text description of the user history, i.e., what the user already knows about the event, and a list of known on-topic and off-topic documents (if available) as training examples.",
                "For each query, we generated answer keys and corresponding nugget matching rules using the procedure described in section 4.1.2, and produced a total of 120 queries, with an average of 7 nuggets per query. 6.",
                "EXPERIMENTS AND RESULTS 6.1 Baselines We used Indri [17], a popular language-model based retrieval engine, as a baseline for comparison with CAF´E.",
                "Indri supports standard search engine functionality, including pseudo-relevance feedback (PRF) [3, 6], and is representative of a typical query-based retrieval system.",
                "Indri does not support any kind of novelty detection.",
                "We compare Indri with PRF turned on and off, against CAF´E with user feedback, novelty detection and antiredundant ranking turned on and off. 6.2 Experimental Setup We divided the TDT4 corpus spanning 4 months into 10 chunks, each defined as a period of 12 consecutive days.",
                "At any given point of time in the distillation process, each system accessed the past data up to the current point, and returned a ranked list of up 50 passages per query.",
                "The 12 tasks defined on the corpus were divided into a training and test set with 6 tasks each.",
                "Each system was allowed to use the training set to tune its parameters for optimizing NDCU (equation 8), including the relevance threshold for both Indri and CAF´E, and the novelty and antiredundancy thresholds for CAF´E.",
                "The NDCU for each system run is calculated automatically.",
                "User feedback was also simulated - relevance judgments for each system-returned passage (as determined by the nugget matching rules described in section 4.1.2) were 4 URL: http://nyc.lti.cs.cmu.edu/downloads Figure 1: Performance of Indri across chunks Figure 2: Performance of CAF´E across chunks used as user feedback in the adaptation of query profiles and user histories. 6.3 Results In Table 1, we show the NDCU scores of the two systems under various settings.",
                "These scores are averaged over the six tasks in the test set, and are calculated with two dampening factors (see section 4.2): γ = 0 and 0.1, to simulate no tolerance and small tolerance for redundancy, respectively.",
                "Using γ = 0 creates a much more strict metric since it does not give any credit to a passage that contains relevant but redundant information.",
                "Hence, the improvement obtained from enabling user feedback is smaller with γ = 0 than the improvement obtained from feedback with γ = 0.1.",
                "This reveals a shortcoming of contemporary retrieval systemswhen the user gives positive feedback on a passage, the systems gives higher weights to the terms present in that passage and tends to retrieve other passages containing the same terms - and thus - usually the same information.",
                "However, the user does not benefit from seeing such redundant passages, and is usually interested in other passages containing related information.",
                "It is informative to evaluate retrieval systems using our utility measure (with γ = 0) which accounts for novelty and thus gives a more realistic picture of how well a system can generalize from user feedback, rather than using traditional IR measures like recall and precision which give an incomplete picture of improvement obtained from user feedback.",
                "Sometimes, however, users might indeed be interested in seeing the same information from multiple sources, as an Table 1: NDCU Scores of Indri and CAF´E for two dampening factors (γ), and various settings (F: Feedback, N: Novelty Detection, A: Anti-Redundant Ranking) Indri CAF´E γ Base +PRF Base +F +F+N +F+A +F+N+A 0 0.19 0.19 0.22 0.23 0.24 0.24 0.24 0.1 0.28 0.29 0.24 0.35 0.35 0.36 0.36 indicator of its importance or reliability.",
                "In such a case, we can simply choose a higher value for γ which corresponds to a higher tolerance for redundancy, and hence let the system tune its parameters accordingly.",
                "Since documents were processed chunk by chunk, it would be interesting to see how the performance of systems improves over time.",
                "Figures 1 and 2 show the performance trends for both the systems across chunks.",
                "While the performance with and without feedback on the first few chunks is expected to be close, for subsequent chunks, the performance curve with feedback enabled rises above the one with the no-feedback setting.",
                "The performance trends are not consistent across all chunks because on-topic documents are not uniformly distributed over all the chunks, making some queries easier than others in certain chunks.",
                "Moreover, since Indri uses pseudo-relevance feedback while CAF´E uses feedback based on actual relevance judgments, the improvement in case of Indri is less dramatic than that of CAF´E. 7.",
                "CONCLUDING REMARKS This paper presents the first investigation on utility-based information distillation with a system that learns the longlasting information needs from fine-grained user feedback over a sequence of ranked passages.",
                "Our system, called CAF´E, combines adaptive filtering, novelty detection and antiredundant <br>passage ranking</br> in a unified framework for utility optimization.",
                "We developed a new scheme for automated evaluation and feedback based on a semi-automatic procedure for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also proposed an extension of the NDCG metric for assessing the utility of ranked passages as a weighted combination of relevance and novelty.",
                "Our experiments on the newly annotated TDT4 benchmark corpus show encouraging utility enhancement over Indri, and also over our own system with incremental learning and novelty detection turned off. 8.",
                "ACKNOWLEDGMENTS We would like to thank Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng, and the Qualitative Data Analysis Program at the University of Pittsburgh lead by Dr. Stuart Shulman for their help with collecting and processing the extended TDT4 annotations used in our experiments.",
                "This work is supported in parts by the National Science Foundation (NSF) under grant IIS0434035, and the Defense Advanced Research Project Agency (DARPA) under contracts NBCHD030010 and W0550432.",
                "Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors. 9.",
                "ADDITIONAL AUTHORS Jian Zhang (jianzhan@stat.purdue.edu)∗ , Jaime Carbonell (jgc@cs.cmu.edu)† , Peter Brusilovsky (peterb+@pitt.edu)‡ , Daqing He(dah44@pitt.edu)‡ 10.",
                "REFERENCES [1] J. Allan.",
                "Incremental Relevance Feedback for Information Filtering.",
                "Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, pages 270-278, 1996. [2] J. Allan, C. Wade, and A. Bolivar.",
                "Retrieval and Novelty Detection at the Sentence Level.",
                "Proceedings of the ACM SIGIR conference on research and development in information retrieval, 2003. [3] C. Buckley, G. Salton, and J. Allan.",
                "Automatic Retrieval with Locality Information using SMART.",
                "NIST special publication, (500207):59-72, 1993. [4] J. Callan.",
                "Learning While Filtering Documents.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 224-231, 1998. [5] J. Carbonell and J. Goldstein.",
                "The use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 335-336, 1998. [6] E. Efthimiadis.",
                "Query Expansion.",
                "Annual Review of Information Science and Technology (ARIST), 31:p121-87, 1996. [7] J. Fiscus and G. Duddington.",
                "Topic Detection and Tracking Overview.",
                "Topic Detection and Tracking: Event-based Information Organization, pages 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos.",
                "A Statistical Model for Multilingual Entity Detection and Tracking.",
                "NAACL/HLT, 2004. [9] K. J¨arvelin and J. Kek¨al¨ainen.",
                "Cumulated Gain-based Evaluation of IR Techniques.",
                "ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin and D. Demner-Fushman.",
                "Automatically Evaluating Answers to Definition Questions.",
                "Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), 2005. ∗ Statistics Dept., Purdue University, West Lafayette, USA † Language Technologies Inst., Carnegie Mellon University, Pittsburgh, USA ‡ School of Information Sciences, Univ. of Pittsburgh, Pittsburgh, USA [11] J. Lin and D. Demner-Fushman.",
                "Will Pyramids Built of nUggets Topple Over.",
                "Proceedings of HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos.",
                "A Mention-synchronous Coreference Resolution Algorithm based on the Bell Tree.",
                "Proc. of ACL, 4:136-143, 2004. [13] G. Marton.",
                "Nuggeteer: Automatic Nugget-Based Evaluation Using Descriptions and Judgments.",
                "HLT/NAACL, 2006. [14] E. Riloff.",
                "Automatically Constructing a Dictionary for Information Extraction Tasks.",
                "Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 811-816, 1993. [15] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9: Filtering track.",
                "The Ninth Text REtrieval Conference (TREC-9), pages 361-368. [16] R. Schapire, Y.",
                "Singer, and A. Singhal.",
                "Boosting and Rocchio Applied to Text Filtering.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle, and W. Croft.",
                "Indri: A Language Model-based Search Engine for Complex Queries.",
                "Proceedings of the International Conference on Intelligence Analysis, 2004. [18] The Linguistic Data Consortium. http://www.ldc.upenn.edu/. [19] E. Voorhees.",
                "Overview of the TREC 2003 Question Answering Track.",
                "Proceedings of the Twelfth Text REtrieval Conference (TREC 2003), 2003. [20] Y. Yang and B. Kisiel.",
                "Margin-based Local Regression for Adaptive Filtering.",
                "Proceedings of the twelfth international conference on Information and knowledge management, pages 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of Adaptive Filtering Methods in a Cross-benchmark Evaluation.",
                "Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 98-105, 2005. [22] C. Zhai, W. Cohen, and J. Lafferty.",
                "Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 10-17, 2003. [23] J. Zhang and Y. Yang.",
                "Robustness of Regularized Linear Classification Methods in Text Categorization.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 190-197, 2003. [24] Y. Zhang.",
                "Using Bayesian Priors to Combine Classifiers for Adaptive Filtering.",
                "Proceedings of the 27th annual international conference on Research and development in information retrieval, pages 345-352, 2004. [25] Y. Zhang, J. Callan, and T. Minka.",
                "Novelty and Redundancy Detection in Adaptive Filtering.",
                "Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Combina las fortalezas y se extiende más allá del filtrado adaptativo convencional, la detección de novedad y la \"clasificación de pasaje\" no redundante con respecto a las necesidades de información duradera (tareas con múltiples consultas).",
                "La \"clasificación del pasaje\" aquí se basa en cuán relevante es un pasaje con respecto a la consulta actual, cuán novedoso es con respecto al historial actual del usuario (de sus interacciones con el sistema) y cuán redundante es en comparación con otrospasajes con un rango más alto en la lista.",
                "Nuestro sistema, llamado CAF´E, combina filtrado adaptativo, detección de novedad y \"clasificación de pasaje\" antiredado en un marco unificado para la optimización de servicios públicos."
            ],
            "translated_text": "",
            "candidates": [
                "ranking de pasaje",
                "clasificación de pasaje",
                "ranking de pasaje",
                "clasificación del pasaje",
                "ranking de pasaje",
                "clasificación de pasaje"
            ],
            "error": []
        },
        "adaptive filtering": {
            "translated_key": "filtrado adaptativo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Utility-based Information Distillation Over Temporally Sequenced Documents Yiming Yang Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA yiming@cs.cmu.edu Abhimanyu Lad Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA alad@cs.cmu.edu Ni Lao Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA nlao@cs.cmu.edu Abhay Harpale Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA aharpale@cs.cmu.edu Bryan Kisiel Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA bkisiel@cs.cmu.edu Monica Rogati Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA mrogati@cs.cmu.edu ABSTRACT This paper examines a new approach to information distillation over temporally ordered documents, and proposes a novel evaluation scheme for such a framework.",
                "It combines the strengths of and extends beyond conventional <br>adaptive filtering</br>, novelty detection and non-redundant passage ranking with respect to long-lasting information needs (tasks with multiple queries).",
                "Our approach supports fine-grained user feedback via highlighting of arbitrary spans of text, and leverages such information for utility optimization in adaptive settings.",
                "For our experiments, we defined hypothetical tasks based on news events in the TDT4 corpus, with multiple queries per task.",
                "Answer keys (nuggets) were generated for each query and a semiautomatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also propose an extension of the NDCG metric for assessing the utility of ranked passages as a combination of relevance and novelty.",
                "Our results show encouraging utility enhancements using the new approach, compared to the baseline systems without incremental learning or the novelty detection components.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 General Terms Design, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Tracking new and relevant information from temporal data streams for users with long-lasting needs has been a challenging research topic in information retrieval.",
                "<br>adaptive filtering</br> (AF) is one such task of online prediction of the relevance of each new document with respect to pre-defined topics.",
                "Based on the initial query and a few positive examples (if available), an AF system maintains a profile for each such topic of interest, and constantly updates it based on feedback from the user.",
                "The incremental learning nature of AF systems makes them more powerful than standard search engines that support ad-hoc retrieval (e.g.",
                "Google and Yahoo) in terms of finding relevant information with respect to long-lasting topics of interest, and more attractive for users who are willing to provide feedback to adapt the system towards their specific information needs, without having to modify their queries manually.",
                "A variety of supervised learning algorithms (Rocchio-style classifiers, Exponential-Gaussian models, local regression and logistic regression approaches) have been studied for adaptive settings, examined with explicit and implicit relevance feedback, and evaluated with respect to utility optimization on large benchmark data collections in TREC (Text Retrieval Conferences) and TDT (Topic Detection and Tracking) forums [1, 4, 7, 15, 16, 20, 24, 23].",
                "Regularized logistic regression [21] has been found representative for the state-of-the-art approaches, and highly efficient for frequent model adaptations over large document collections such as the TREC-10 corpus (over 800,000 documents and 84 topics).",
                "Despite substantial achievements in recent <br>adaptive filtering</br> research, significant problems remain unsolved regarding how to leverage user feedback effectively and efficiently.",
                "Specifically, the following issues may seriously limit the true utility of AF systems in real-world applications: 1.",
                "User has a rather passive role in the conventional <br>adaptive filtering</br> setup - he or she reacts to the system only when the system makes a yes decision on a document, by confirming or rejecting that decision.",
                "A more active alternative would be to allow the user to issue multiple queries for a topic, review a ranked list of candidate documents (or passages) per query, and provide feedback on the ranked list, thus refining their information need and requesting updated ranked lists.",
                "The latter form of user interaction has been highly effective in standard retrieval for ad-hoc queries.",
                "How to deploy such a strategy for long-lasting information needs in AF settings is an open question for research. 2.",
                "The unit for receiving a relevance judgment (yes or no) is restricted to the document level in conventional AF.",
                "However, a real user may be willing to provide more informative, fine-grained feedback via highlighting some pieces of text in a retrieved document as relevant, instead of labeling the entire document as relevant.",
                "Effectively leveraging such fine-grained feedback could substantially enhance the quality of an AF system.",
                "For this, we need to enable supervised learning from labeled pieces of text of arbitrary span instead of just allowing labeled documents. 3.",
                "System-selected documents are often highly redundant.",
                "A major news event, for example, would be reported by multiple sources repeatedly for a while, making most of the information content in those articles redundant with each other.",
                "A conventional AF system would select all these redundant news stories for user feedback, wasting the users time while offering little gain.",
                "Clearly, techniques for novelty detection can help in principle [25, 2, 22] for improving the utility of the AF systems.",
                "However, the effectiveness of such techniques at passage level to detect novelty with respect to users (fine-grained) feedback and to detect redundancy in ranked lists remains to be evaluated using a measure of utility that mimics the needs of a real user.",
                "To address the above limitations of current AF systems, we propose and examine a new approach in this paper, combining the strengths of conventional AF (incremental learning of topic models), multi-pass passage retrieval for long-lasting queries conditioned on topic, and novelty detection for removal of redundancy from user interactions with the system.",
                "We call the new process utility-based information distillation.",
                "Note that conventional benchmark corpora for AF evaluations, which have relevance judgments at the document level and do not define tasks with multiple queries, are insufficient for evaluating the new approach.",
                "Therefore, we extended a benchmark corpus - the TDT4 collection of news stories and TV broadcasts - with task definitions, multiple queries per task, and answer keys per query.",
                "We have conducted our experiments on this extended TDT4 corpus and have made the additionally generated data publicly available for future comparative evaluations 1 .",
                "To automatically evaluate the system-returned arbitrary spans of text using our answer keys, we further developed an evaluation scheme with semi-automatic procedure for 1 URL: http://nyc.lti.cs.cmu.edu/downloads acquiring rules that can match nuggets against system responses.",
                "Moreover, we propose an extension of NDCG (Normalized Discounted Cumulated Gain) [9] for assessing the utility of ranked passages as a function of both relevance and novelty.",
                "The rest of this paper is organized as follows.",
                "Section 2 outlines the information distillation process with a concrete example.",
                "Section 3 describes the technical cores of our system called CAF´E - CMU <br>adaptive filtering</br> Engine.",
                "Section 4 discusses issues with respect to evaluation methodology and proposes a new scheme.",
                "Section 5 describes the extended TDT4 corpus.",
                "Section 6 presents our experiments and results.",
                "Section 7 concludes the study and gives future perspectives. 2.",
                "A SAMPLE TASK Consider a news event - the escape of seven convicts from a Texas prison in December 2000 and their capture a month later.",
                "Assuming a user were interested in this event since its early stage, the information need could be: Find information about the escape of convicts from Texas prison, and information related to their recapture.",
                "The associated lower-level questions could be: 1.",
                "How many prisoners escaped? 2.",
                "Where and when were they sighted? 3. Who are their known contacts inside and outside the prison? 4.",
                "How are they armed? 5.",
                "Do they have any vehicles? 6.",
                "What steps have been taken so far?",
                "We call such an information need a task, and the associated questions as the queries in this task.",
                "A distillation system is supposed to monitor the incoming documents, process them chunk by chunk in a temporal order, select potentially relevant and novel passages from each chunk with respect to each query, and present a ranked list of passages to the user.",
                "Passage ranking here is based on how relevant a passage is with respect to the current query, how novel it is with respect to the current user history (of his or her interactions with the system), and how redundant it is compared to other passages with a higher rank in the list.",
                "When presented with a list of passages, the user may provide feedback by highlighting arbitrary spans of text that he or she found relevant.",
                "These spans of text are taken as positive examples in the adaptation of the query profile, and also added to the users history.",
                "Passages not marked by the user are taken as negative examples.",
                "As soon as the query profile is updated, the system re-issues a search and returns another ranked list of passages where the previously seen passages are either removed or ranked low, based on user preference.",
                "For example, if the user highlights ...officials have posted a $100,000 reward for their capture... as relevant answer to the query What steps have been taken so far?, then the highlighted piece is used as an additional positive training example in the adaptation of the query profile.",
                "This piece of feedback is also added to the user history as a seen example, so that in future, the system will not place another passage mentioning $100,000 reward at the top of the ranked list.",
                "However, an article mentioning ...officials have doubled the reward money to $200,000... might be ranked high since it is both relevant to the (updated) query profile and novel with respect to the (updated) user history.",
                "The user may modify the original queries or add a new query during the process; the query profiles will be changed accordingly.",
                "Clearly, novelty detection is very important for the utility of such a system because of the iterative search.",
                "Without novelty detection, the old relevant passages would be shown to the user repeatedly in each ranked list.",
                "Through the above example, we can see the main properties of our new framework for utility-based information distillation over temporally ordered documents.",
                "Our framework combines and extends the power of <br>adaptive filtering</br> (AF), ad-hoc retrieval (IR) and novelty detection (ND).",
                "Compared to standard IR, our approach has the power of incrementally learning long-term information needs and modeling a sequence of queries within a task.",
                "Compared to conventional AF, it enables a more active role of the user in refining his or her information needs and requesting new results by allowing relevance and novelty feedback via highlighting of arbitrary spans of text in passages returned by the system.",
                "Compared to past work, this is the first evaluation of novelty detection integrated with <br>adaptive filtering</br> for sequenced queries that allows flexible user feedback over ranked passages.",
                "The combination of AF, IR and ND with the new extensions raises an important research question regarding evaluation methodology: how can we measure the utility of such an information distillation system?",
                "Existing metrics in standard IR, AF and ND are insufficient, and new solutions must be explored, as we will discuss in Section 4, after describing the technical cores of our system in the next section. 3.",
                "TECHNICAL CORES The core components of CAF´E are - 1) AF for incremental learning of query profiles, 2) IR for estimating relevance of passages with respect to query profiles, 3) ND for assessing novelty of passages with respect to users history, and 4) anti-redundancy component to remove redundancy from ranked lists. 3.1 <br>adaptive filtering</br> Component We use a state-of-the-art algorithm in the field - the regularized logistic regression method which had the best results on several benchmark evaluation corpora for AF [21].",
                "Logistic regression (LR) is a supervised learning algorithm for statistical classification.",
                "Based on a training set of labeled instances, it learns a class model which can then by used to predict the labels of unseen instances.",
                "Its performance as well as efficiency in terms of training time makes it a good candidate when frequent updates of the class model are required, as is the case in <br>adaptive filtering</br>, where the system must learn from each new feedback provided by the user. (See [21] and [23] for computational complexity and implementation issues).",
                "In <br>adaptive filtering</br>, each query is considered as a class and the probability of a passage belonging to this class corresponds to the degree of relevance of the passage with respect to the query.",
                "For training the model, we use the query itself as the initial positive training example of the class, and the user-highlighted pieces of text (marked as Relevant or Not-relevant) during feedback as additional training examples.",
                "To address the cold start issue in the early stage before any user feedback is obtained, the system uses a small sample from a retrospective corpus as the initial negative examples in the training set.",
                "The details of using logistic regression for <br>adaptive filtering</br> (assigning different weights to positive and negative training instances, and regularizing the objective function to prevent over-fitting on training data) are presented in [21].",
                "The class model w∗ learned by Logistic Regression, or the query profile, is a vector whose dimensions are individual terms and whose elements are the regression coefficients, indicating how influential each term is in the query profile.",
                "The query profile is updated whenever a new piece of user feedback is received.",
                "A temporally decaying weight can be applied to each training example, as an option, to emphasize the most recent user feedback. 3.2 Passage Retrieval Component We use standard IR techniques in this part of our system.",
                "Incoming documents are processed in chunks, where each chunk can be defined as a fixed span of time or as a fixed number of documents, as preferred by the user.",
                "For each incoming document, corpus statistics like the IDF (Inverted Document Frequency) of each term are updated.",
                "We use a state-of-the-art named entity identifier and tracker [8, 12] to identify person and location names, and merge them with co-referent named entities seen in the past.",
                "Then the documents are segmented into passages, which can be a whole document, a paragraph, a sentence, or any other continuous span of text, as preferred.",
                "Each passage is represented using a vector of TF-IDF (Term FrequencyInverse Document Frequency) weights, where term can be a word or a named entity.",
                "Given a query profile, i.e. the logistic regression solution w∗ as described in Section 3.1, the system computes the posterior probability of relevance for each passage x as fRL(x) ≡ P(y = 1|x, w∗ ) = 1 (1 + e−w∗·x) (1) Passages are ordered by their relevance scores, and the ones with scores above a threshold (tuned on a training set) comprise the relevance list that is passed on to the novelty detection step. 3.3 Novelty Detection Component CAF´E maintains a user history H(t), which contains all the spans of text hi that the user highlighted (as feedback) during his or her past interactions with the system, up to the current time t. Denoting the history as H(t) = n h1, h2, ..., ht o , (2) the novelty score of a new candidate passage x is computed as: fND(x) = 1 − max i∈1..t {cos(x, hi)} (3) where both candidate passage x and highlighted spans of text hi are represented as TF-IDF vectors.",
                "The novelty score of each passage is compared to a prespecified threshold (also tuned on a training set), and any passage with a score below this threshold is removed from the relevance list. 3.4 Anti-redundant Ranking Component Although the novelty detection component ensures that only novel (previously unseen) information remains in the relevance list, this list might still contain the same novel information at multiple positions in the ranked list.",
                "Suppose, for example, that the user has already read about a $100,000 reward for information about the escaped convicts.",
                "A new piece of news that the award has been increased to $200,000 is novel since the user hasnt read about it yet.",
                "However, multiple news sources would report this news and we might end up showing (redundant) articles from all these sources in a ranked list.",
                "Hence, a ranked list should also be made non-redundant with respect to its own contents.",
                "We use a simplified version of the Maximal Marginal Relevance method [5], originally developed for combining relevance and novelty in text retrieval and summarization.",
                "Our procedure starts with the current list of passages sorted by relevance (section 3.2), filtered by Novelty Detection component (section 3.3), and generates a new non-redundant list as follows: 1.",
                "Take the top passage in the current list as the top one in the new list. 2.",
                "Add the next passage x in the current list to the new list only if fAR(x) > t where fAR(x) = 1 − max pi∈Lnew {cos(x, pi)} and Lnew is the set of passages already selected in the new list. 3.",
                "Repeat step 2 until all the passages in the current list have been examined.",
                "After applying the above-mentioned algorithm, each passage in the new list is sufficiently dissimilar to others, thus favoring diversity rather than redundancy in the new ranked list.",
                "The anti-redundancy threshold t is tuned on a training set. 4.",
                "EVALUATION METHODOLOGY The approach we proposed above for information distillation raises important issues regarding evaluation methodology.",
                "Firstly, since our framework allows the output to be passages at different levels of granularity (e.g. k-sentence windows where k may vary) instead of a fixed length, it is not possible to have pre-annotated relevance judgments at all such granularity levels.",
                "Secondly, since we wish to measure the utility of the system output as a combination of both relevance and novelty, traditional relevance-only based measures must be replaced by measures that penalize the repetition of the same information in the system output across time.",
                "Thirdly, since the output of the system is ranked lists, we must reward those systems that present useful information (both relevant and previously unseen) using shorter ranked lists, and penalize those that present the same information using longer ranked lists.",
                "None of the existing measures in ad-hoc retrieval, <br>adaptive filtering</br>, novelty detection or other related areas (text summarization and question answering) have desirable properties in all the three aspects.",
                "Therefore, we must develop a new evaluation methodology. 4.1 Answer Keys To enable the evaluation of a system whose output consists of passages of arbitrary length, we borrow the concept of answer keys from the Question Answering (QA) community, where systems are allowed to return arbitrary spans of text as answers.",
                "Answer keys define what should be present in a system response to receive credit, and are comprised of a collection of information nuggets, i.e. factoid units about which human assessors can make binary decisions of whether or not a system response contains them.",
                "Defining answer keys and making the associated binary decisions are conceptual tasks that require semantic mapping [19], since system-returned passages can contain the same information expressed in many different ways.",
                "Hence, QA evaluations have relied on human assessors for the mapping between various expressions, making the process costly, time consuming, and not scalable to large query and document collections, and extensive system evaluations with various parameter settings. 4.1.1 Automating Evaluation based on Answer Keys Automatic evaluation methods would allow for faster system building and tuning, as well as provide an objective and affordable way of comparing various systems.",
                "Recently, such methods have been proposed, more or less, based on the idea of n-gram co-occurrences.",
                "Pourpre [10] assigns a fractional recall score to a system response based on its unigram overlap with a given nuggets description.",
                "For example, a system response A B C has recall 3/4 with respect to a nugget with description A B C D. However, such an approach is unfair to systems that present the same information but using words other than A, B, C, and D. Another open issue is how to weight individual words in measuring the closeness of a match.",
                "For example, consider the question How many prisoners escaped?.",
                "In the nugget Seven prisoners escaped from a Texas prison, there is no indication that seven is the keyword, and that it must be matched to get any relevance credit.",
                "Using IDF values does not help, since seven will generally not have a higher IDF than words like texas and prison.",
                "Also, redefining the nugget as just seven does not solve the problem since now it might spuriously match any mention of seven out of context.",
                "Nuggeteer [13] works on similar principles but makes binary decisions about whether a nugget is present in a given system response by tuning a threshold.",
                "However, it is also plagued by spurious relevance since not all words contained in the nugget description (or known correct responses) are central to the nugget. 4.1.2 Nugget-Matching Rules We propose a reliable automatic method for determining whether a snippet of text contains a given nugget, based on nugget-matching rules, which are generated using a semiautomatic procedure explained below.",
                "These rules are essentially Boolean queries that will only match against snippets that contain the nugget.",
                "For instance, a candidate rule for matching answers to How many prisoners escaped? is (Texas AND seven AND escape AND (convicts OR prisoners)), possibly with other synonyms and variants in the rule.",
                "For a corpus of news articles, which usually follow a typical formal prose, it is fairly easy to write such simple rules to match expected answers using a bootstrap approach, as described below.",
                "We propose a two-stage approach, inspired by Autoslog [14], that combines the strength of humans in identifying semantically equivalent expressions and the strength of the system in gathering statistical evidence from a humanannotated corpus of documents.",
                "In the first stage, human subjects annotated (using a highlighting tool) portions of ontopic documents that contained answers to each nugget 2 .",
                "In the second stage, subjects used our rule generation tool to create rules that would match the annotations for each nugget.",
                "The tool allows users to enter a Boolean rule as a disjunction of conjunctions (e.g. ((a AND b) OR (a AND c AND d) OR (e))).",
                "Given a candidate rule, our tool uses it as a Boolean query over the entire set of on-topic documents and calculates its recall and precision with respect to the annotations that it is expected to match.",
                "Hence, the subjects can start with a simple rule and iteratively refine it until they are satisfied with its recall and precision.",
                "We observed that it was very easy for humans to improve the precision of a rule by tweaking its existing conjunctions (adding more ANDs), and improving the recall by adding more conjunctions to the disjunction (adding more ORs).",
                "As an example, lets try to create a rule for the nugget which says that seven prisoners escaped from the Texas prison.",
                "We start with a simple rule - (seven).",
                "When we input this into the rule generation tool, we realize that this rule matches many spurious occurrences of seven (e.g. ...seven states...) and thus gets a low precision score.",
                "We can further qualify our rule - Texas AND seven AND convicts.",
                "Next, by looking at the missed annotations, we realize that some news articles mentioned ...seven prisoners escaped.... We then replace convicts with the disjunction (convicts OR prisoners).",
                "We continue tweaking the rule in this manner until we achieve a sufficiently high recall and precision - i.e. the (small number of) misses and false alarms can be safely ignored.",
                "Thus we can create nugget-matching rules that succinctly capture various ways of expressing a nugget, while avoiding matching incorrect (or out of context) responses.",
                "Human involvement in the rule creation process ensures high quality generic rules which can then be used to evaluate arbitrary system responses reliably. 4.2 Evaluating the Utility of a Sequence of Ranked Lists The utility of a retrieval system can be defined as the difference between how much the user gained in terms of useful information, and how much the user lost in terms of time and energy.",
                "We calculate this utility from the utilities of individual passages as follows.",
                "After reading each passage returned by the system, the user derives some gain depending on the presence of relevant and novel information, and incurs a loss in terms of the time and energy spent in going through the passage.",
                "However, the likelihood that the user would actually read a passage depends on its position in the ranked list.",
                "Hence, for a query q, the expected utility 2 LDC [18] already provides relevance judgments for 100 topics on the TDT4 corpus.",
                "We further ensured that these judgments are exhaustive on the entire corpus using pooling. of a passage pi at rank i can be defined as U(pi, q) = P(i) ∗ (Gain(pi, q) − Loss(pi, q)) (4) where P(i) is the probability that the user would go through a passage at rank i.",
                "The expected utility for an entire ranked list of length n can be calculated simply by adding the expected utility of each passage: U(q) = nX i=1 P(i) ∗ (Gain(pi, q) − Loss(pi, q)) (5) Note that if we ignore the loss term and define P(i) as P(i) ∝ 1/ logb(b + i − 1) (6) then we get the recently popularized metric called Discounted Cumulated Gain (DCG) [9], where Gain(pi, q) is defined as the graded relevance of passage pi.",
                "However, without the loss term, DCG is a purely recall-oriented metric and not suitable for an <br>adaptive filtering</br> setting, where the systems utility depends in part on its ability to limit the number of items shown to the user.",
                "Although P(i) could be defined based on empirical studies of user behavior, for simplicity, we use P(i) exactly as defined in equation 6.",
                "The gain G(pi, q) of passage pi with respect to the query q is a function of - 1) the number of relevant nuggets present in pi, and 2) the novelty of each of these nuggets.",
                "We combine these two factors as follows.",
                "For each nugget Nj, we assign an initial weight wj, and also keep a count nj of the number of times this nugget has been seen by the user in the past.",
                "The gain derived from each subsequent occurrence of the same nugget is assumed to reduce by a dampening factor γ.",
                "Thus, G(pi, q) is defined as G(pi, q) = X Nj ∈C(pi,q) wj ∗ γnj (7) where C(pi, q) is the set of all nuggets that appear in passage pi and also belong to the answer key of query q.",
                "The initial weights wj are all set of be 1.0 in our experiments, but can also be set based on a pyramid approach [11].",
                "The choice of dampening factor γ determines the users tolerance for redundancy.",
                "When γ = 0, a nugget will only receive credit for its first occurrence i.e. when nj is zero3 .",
                "For 0 < γ < 1, a nugget receives smaller credit for each successive occurrence.",
                "When γ = 1, no dampening occurs and repeated occurrences of a nugget receive the same credit.",
                "Note that the nugget occurrence counts are preserved between evaluation of successive ranked lists returned by the system, since the users are expected to remember what the system showed them in the past.",
                "We define the loss L(pi, q) as a constant cost c (we use 0.1) incurred when reading a system-returned passage.",
                "Thus, our metric can be re-written as U(q) = nX i=1 Gain(pi, q) logb(b + i − 1) − L(n) (8) where L(n) is the loss associated with a ranked list of length n: L(n) = c · nX i=1 1 logb(b + i − 1) (9) 3 Note that 00 = 1 Due to the similarity with Discounted Cumulated Gain (DCG), we call our metric Discounted Cumulated Utility (DCU).",
                "The DCU score obtained by the system is converted to a Normalized DCU (NDCU) score by dividing it by the DCU score of the ideal ranked list, which is created by ordering passages by their decreasing utility scores U(pi, q) and stopping when U(pi, q) ≤ 0 i.e. when the gain is less than or equal to the cost of reading the passage. 5.",
                "DATA TDT4 was the benchmark corpus used in TDT2002 and TDT2003 evaluations.",
                "The corpus consists of over 90, 000 news articles from multiple sources (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) published between October 2000 and January 2001, in the languages of Arabic, English, and Mandarin.",
                "Speech-recognized and machine-translated versions of the non-English articles were provided as well.",
                "LDC [18] has annotated the corpus with 100 topics, that correspond to various news events in this time period.",
                "Out of these, we selected a subset of 12 actionable events, and defined corresponding tasks for them4 .",
                "For each task, we manually defined a profile consisting of an initial set of (5 to 10) queries, a free-text description of the user history, i.e., what the user already knows about the event, and a list of known on-topic and off-topic documents (if available) as training examples.",
                "For each query, we generated answer keys and corresponding nugget matching rules using the procedure described in section 4.1.2, and produced a total of 120 queries, with an average of 7 nuggets per query. 6.",
                "EXPERIMENTS AND RESULTS 6.1 Baselines We used Indri [17], a popular language-model based retrieval engine, as a baseline for comparison with CAF´E.",
                "Indri supports standard search engine functionality, including pseudo-relevance feedback (PRF) [3, 6], and is representative of a typical query-based retrieval system.",
                "Indri does not support any kind of novelty detection.",
                "We compare Indri with PRF turned on and off, against CAF´E with user feedback, novelty detection and antiredundant ranking turned on and off. 6.2 Experimental Setup We divided the TDT4 corpus spanning 4 months into 10 chunks, each defined as a period of 12 consecutive days.",
                "At any given point of time in the distillation process, each system accessed the past data up to the current point, and returned a ranked list of up 50 passages per query.",
                "The 12 tasks defined on the corpus were divided into a training and test set with 6 tasks each.",
                "Each system was allowed to use the training set to tune its parameters for optimizing NDCU (equation 8), including the relevance threshold for both Indri and CAF´E, and the novelty and antiredundancy thresholds for CAF´E.",
                "The NDCU for each system run is calculated automatically.",
                "User feedback was also simulated - relevance judgments for each system-returned passage (as determined by the nugget matching rules described in section 4.1.2) were 4 URL: http://nyc.lti.cs.cmu.edu/downloads Figure 1: Performance of Indri across chunks Figure 2: Performance of CAF´E across chunks used as user feedback in the adaptation of query profiles and user histories. 6.3 Results In Table 1, we show the NDCU scores of the two systems under various settings.",
                "These scores are averaged over the six tasks in the test set, and are calculated with two dampening factors (see section 4.2): γ = 0 and 0.1, to simulate no tolerance and small tolerance for redundancy, respectively.",
                "Using γ = 0 creates a much more strict metric since it does not give any credit to a passage that contains relevant but redundant information.",
                "Hence, the improvement obtained from enabling user feedback is smaller with γ = 0 than the improvement obtained from feedback with γ = 0.1.",
                "This reveals a shortcoming of contemporary retrieval systemswhen the user gives positive feedback on a passage, the systems gives higher weights to the terms present in that passage and tends to retrieve other passages containing the same terms - and thus - usually the same information.",
                "However, the user does not benefit from seeing such redundant passages, and is usually interested in other passages containing related information.",
                "It is informative to evaluate retrieval systems using our utility measure (with γ = 0) which accounts for novelty and thus gives a more realistic picture of how well a system can generalize from user feedback, rather than using traditional IR measures like recall and precision which give an incomplete picture of improvement obtained from user feedback.",
                "Sometimes, however, users might indeed be interested in seeing the same information from multiple sources, as an Table 1: NDCU Scores of Indri and CAF´E for two dampening factors (γ), and various settings (F: Feedback, N: Novelty Detection, A: Anti-Redundant Ranking) Indri CAF´E γ Base +PRF Base +F +F+N +F+A +F+N+A 0 0.19 0.19 0.22 0.23 0.24 0.24 0.24 0.1 0.28 0.29 0.24 0.35 0.35 0.36 0.36 indicator of its importance or reliability.",
                "In such a case, we can simply choose a higher value for γ which corresponds to a higher tolerance for redundancy, and hence let the system tune its parameters accordingly.",
                "Since documents were processed chunk by chunk, it would be interesting to see how the performance of systems improves over time.",
                "Figures 1 and 2 show the performance trends for both the systems across chunks.",
                "While the performance with and without feedback on the first few chunks is expected to be close, for subsequent chunks, the performance curve with feedback enabled rises above the one with the no-feedback setting.",
                "The performance trends are not consistent across all chunks because on-topic documents are not uniformly distributed over all the chunks, making some queries easier than others in certain chunks.",
                "Moreover, since Indri uses pseudo-relevance feedback while CAF´E uses feedback based on actual relevance judgments, the improvement in case of Indri is less dramatic than that of CAF´E. 7.",
                "CONCLUDING REMARKS This paper presents the first investigation on utility-based information distillation with a system that learns the longlasting information needs from fine-grained user feedback over a sequence of ranked passages.",
                "Our system, called CAF´E, combines <br>adaptive filtering</br>, novelty detection and antiredundant passage ranking in a unified framework for utility optimization.",
                "We developed a new scheme for automated evaluation and feedback based on a semi-automatic procedure for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also proposed an extension of the NDCG metric for assessing the utility of ranked passages as a weighted combination of relevance and novelty.",
                "Our experiments on the newly annotated TDT4 benchmark corpus show encouraging utility enhancement over Indri, and also over our own system with incremental learning and novelty detection turned off. 8.",
                "ACKNOWLEDGMENTS We would like to thank Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng, and the Qualitative Data Analysis Program at the University of Pittsburgh lead by Dr. Stuart Shulman for their help with collecting and processing the extended TDT4 annotations used in our experiments.",
                "This work is supported in parts by the National Science Foundation (NSF) under grant IIS0434035, and the Defense Advanced Research Project Agency (DARPA) under contracts NBCHD030010 and W0550432.",
                "Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors. 9.",
                "ADDITIONAL AUTHORS Jian Zhang (jianzhan@stat.purdue.edu)∗ , Jaime Carbonell (jgc@cs.cmu.edu)† , Peter Brusilovsky (peterb+@pitt.edu)‡ , Daqing He(dah44@pitt.edu)‡ 10.",
                "REFERENCES [1] J. Allan.",
                "Incremental Relevance Feedback for Information Filtering.",
                "Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, pages 270-278, 1996. [2] J. Allan, C. Wade, and A. Bolivar.",
                "Retrieval and Novelty Detection at the Sentence Level.",
                "Proceedings of the ACM SIGIR conference on research and development in information retrieval, 2003. [3] C. Buckley, G. Salton, and J. Allan.",
                "Automatic Retrieval with Locality Information using SMART.",
                "NIST special publication, (500207):59-72, 1993. [4] J. Callan.",
                "Learning While Filtering Documents.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 224-231, 1998. [5] J. Carbonell and J. Goldstein.",
                "The use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 335-336, 1998. [6] E. Efthimiadis.",
                "Query Expansion.",
                "Annual Review of Information Science and Technology (ARIST), 31:p121-87, 1996. [7] J. Fiscus and G. Duddington.",
                "Topic Detection and Tracking Overview.",
                "Topic Detection and Tracking: Event-based Information Organization, pages 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos.",
                "A Statistical Model for Multilingual Entity Detection and Tracking.",
                "NAACL/HLT, 2004. [9] K. J¨arvelin and J. Kek¨al¨ainen.",
                "Cumulated Gain-based Evaluation of IR Techniques.",
                "ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin and D. Demner-Fushman.",
                "Automatically Evaluating Answers to Definition Questions.",
                "Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), 2005. ∗ Statistics Dept., Purdue University, West Lafayette, USA † Language Technologies Inst., Carnegie Mellon University, Pittsburgh, USA ‡ School of Information Sciences, Univ. of Pittsburgh, Pittsburgh, USA [11] J. Lin and D. Demner-Fushman.",
                "Will Pyramids Built of nUggets Topple Over.",
                "Proceedings of HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos.",
                "A Mention-synchronous Coreference Resolution Algorithm based on the Bell Tree.",
                "Proc. of ACL, 4:136-143, 2004. [13] G. Marton.",
                "Nuggeteer: Automatic Nugget-Based Evaluation Using Descriptions and Judgments.",
                "HLT/NAACL, 2006. [14] E. Riloff.",
                "Automatically Constructing a Dictionary for Information Extraction Tasks.",
                "Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 811-816, 1993. [15] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9: Filtering track.",
                "The Ninth Text REtrieval Conference (TREC-9), pages 361-368. [16] R. Schapire, Y.",
                "Singer, and A. Singhal.",
                "Boosting and Rocchio Applied to Text Filtering.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle, and W. Croft.",
                "Indri: A Language Model-based Search Engine for Complex Queries.",
                "Proceedings of the International Conference on Intelligence Analysis, 2004. [18] The Linguistic Data Consortium. http://www.ldc.upenn.edu/. [19] E. Voorhees.",
                "Overview of the TREC 2003 Question Answering Track.",
                "Proceedings of the Twelfth Text REtrieval Conference (TREC 2003), 2003. [20] Y. Yang and B. Kisiel.",
                "Margin-based Local Regression for <br>adaptive filtering</br>.",
                "Proceedings of the twelfth international conference on Information and knowledge management, pages 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of <br>adaptive filtering</br> Methods in a Cross-benchmark Evaluation.",
                "Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 98-105, 2005. [22] C. Zhai, W. Cohen, and J. Lafferty.",
                "Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 10-17, 2003. [23] J. Zhang and Y. Yang.",
                "Robustness of Regularized Linear Classification Methods in Text Categorization.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 190-197, 2003. [24] Y. Zhang.",
                "Using Bayesian Priors to Combine Classifiers for <br>adaptive filtering</br>.",
                "Proceedings of the 27th annual international conference on Research and development in information retrieval, pages 345-352, 2004. [25] Y. Zhang, J. Callan, and T. Minka.",
                "Novelty and Redundancy Detection in <br>adaptive filtering</br>.",
                "Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Combina las fortalezas y se extiende más allá de \"filtrado adaptativo\" convencional, detección de novedad y clasificación de pasaje no redundante con respecto a las necesidades de información duradera (tareas con múltiples consultas).",
                "\"Filtrado adaptativo\" (AF) es una tarea de predicción en línea de la relevancia de cada nuevo documento con respecto a los temas predefinidos.",
                "A pesar de los logros sustanciales en la investigación reciente de \"filtrado adaptativo\", los problemas significativos siguen sin resolverse con respecto a cómo aprovechar la retroalimentación del usuario de manera efectiva y eficiente.",
                "El usuario tiene un papel bastante pasivo en la configuración convencional de \"filtrado adaptativo\": él o ella reacciona al sistema solo cuando el sistema toma una decisión Sí en un documento, al confirmar o rechazar esa decisión.",
                "La Sección 3 describe los núcleos técnicos de nuestro sistema llamado motor CAF´E - CMU \"Filtrado adaptativo\".",
                "Nuestro marco combina y extiende el poder del \"filtrado adaptativo\" (AF), la recuperación ad-hoc (IR) y la detección de novedad (ND).",
                "En comparación con el trabajo pasado, esta es la primera evaluación de la detección de novedad integrada con \"filtrado adaptativo\" para consultas secuenciadas que permite la retroalimentación flexible de los usuarios sobre los pasajes clasificados.",
                "Núcleos técnicos Los componentes centrales de CAF´e son - 1) AF para el aprendizaje incremental de los perfiles de consulta, 2) IR para estimar la relevancia de los pasajes con respecto a los perfiles de consulta, 3) para evaluar la novedad de los pasajes con respecto a la historia de los usuarios, y4) Componente anti-redundancia para eliminar la redundancia de las listas clasificadas.3.1 Componente de \"filtrado adaptativo\" Utilizamos un algoritmo de última generación en el campo: el método de regresión logística regularizada que tuvo los mejores resultados en varios corpus de evaluación de referencia para AF [21].",
                "Su rendimiento, así como la eficiencia en términos de tiempo de capacitación, lo convierten en un buen candidato cuando se requieren actualizaciones frecuentes del modelo de clase, como es el caso en el \"filtrado adaptativo\", donde el sistema debe aprender de cada nuevo retroalimentación proporcionado por el usuario.(Ver [21] y [23] para los problemas de complejidad y implementación computacional).",
                "En el \"filtrado adaptativo\", cada consulta se considera una clase y la probabilidad de un pasaje que pertenece a esta clase corresponde al grado de relevancia del pasaje con respecto a la consulta.",
                "Los detalles del uso de la regresión logística para el \"filtrado adaptativo\" (asignando diferentes pesos a instancias de entrenamiento positivas y negativas, y regularizando la función objetivo para evitar el ajuste excesivo en los datos de entrenamiento) se presentan en [21].",
                "Ninguna de las medidas existentes en la recuperación ad-hoc, el \"filtrado adaptativo\", la detección de novedades u otras áreas relacionadas (resumen de texto y respuesta de preguntas) tienen propiedades deseables en los tres aspectos.",
                "Sin embargo, sin el término de pérdida, DCG es una métrica puramente orientada al retiro y no es adecuada para una configuración de \"filtrado adaptativo\", donde la utilidad de los sistemas depende en parte de su capacidad para limitar el número de elementos que se muestran al usuario.",
                "Nuestro sistema, llamado CAF´E, combina \"filtrado adaptativo\", detección de novedad y clasificación de pasaje antiredado en un marco unificado para la optimización de servicios públicos.",
                "Regresión local basada en margen para \"filtrado adaptativo\".",
                "La robustez de los métodos de \"filtrado adaptativo\" en una evaluación transversal.",
                "Uso de Priors bayesianos para combinar clasificadores para \"filtrado adaptativo\".",
                "Detección de novedad y redundancia en \"filtrado adaptativo\"."
            ],
            "translated_text": "",
            "candidates": [
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "Filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "Filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo"
            ],
            "error": []
        },
        "ad-hoc retrieval": {
            "translated_key": "recuperación ad-hoc",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Utility-based Information Distillation Over Temporally Sequenced Documents Yiming Yang Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA yiming@cs.cmu.edu Abhimanyu Lad Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA alad@cs.cmu.edu Ni Lao Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA nlao@cs.cmu.edu Abhay Harpale Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA aharpale@cs.cmu.edu Bryan Kisiel Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA bkisiel@cs.cmu.edu Monica Rogati Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA mrogati@cs.cmu.edu ABSTRACT This paper examines a new approach to information distillation over temporally ordered documents, and proposes a novel evaluation scheme for such a framework.",
                "It combines the strengths of and extends beyond conventional adaptive filtering, novelty detection and non-redundant passage ranking with respect to long-lasting information needs (tasks with multiple queries).",
                "Our approach supports fine-grained user feedback via highlighting of arbitrary spans of text, and leverages such information for utility optimization in adaptive settings.",
                "For our experiments, we defined hypothetical tasks based on news events in the TDT4 corpus, with multiple queries per task.",
                "Answer keys (nuggets) were generated for each query and a semiautomatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also propose an extension of the NDCG metric for assessing the utility of ranked passages as a combination of relevance and novelty.",
                "Our results show encouraging utility enhancements using the new approach, compared to the baseline systems without incremental learning or the novelty detection components.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 General Terms Design, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Tracking new and relevant information from temporal data streams for users with long-lasting needs has been a challenging research topic in information retrieval.",
                "Adaptive filtering (AF) is one such task of online prediction of the relevance of each new document with respect to pre-defined topics.",
                "Based on the initial query and a few positive examples (if available), an AF system maintains a profile for each such topic of interest, and constantly updates it based on feedback from the user.",
                "The incremental learning nature of AF systems makes them more powerful than standard search engines that support <br>ad-hoc retrieval</br> (e.g.",
                "Google and Yahoo) in terms of finding relevant information with respect to long-lasting topics of interest, and more attractive for users who are willing to provide feedback to adapt the system towards their specific information needs, without having to modify their queries manually.",
                "A variety of supervised learning algorithms (Rocchio-style classifiers, Exponential-Gaussian models, local regression and logistic regression approaches) have been studied for adaptive settings, examined with explicit and implicit relevance feedback, and evaluated with respect to utility optimization on large benchmark data collections in TREC (Text Retrieval Conferences) and TDT (Topic Detection and Tracking) forums [1, 4, 7, 15, 16, 20, 24, 23].",
                "Regularized logistic regression [21] has been found representative for the state-of-the-art approaches, and highly efficient for frequent model adaptations over large document collections such as the TREC-10 corpus (over 800,000 documents and 84 topics).",
                "Despite substantial achievements in recent adaptive filtering research, significant problems remain unsolved regarding how to leverage user feedback effectively and efficiently.",
                "Specifically, the following issues may seriously limit the true utility of AF systems in real-world applications: 1.",
                "User has a rather passive role in the conventional adaptive filtering setup - he or she reacts to the system only when the system makes a yes decision on a document, by confirming or rejecting that decision.",
                "A more active alternative would be to allow the user to issue multiple queries for a topic, review a ranked list of candidate documents (or passages) per query, and provide feedback on the ranked list, thus refining their information need and requesting updated ranked lists.",
                "The latter form of user interaction has been highly effective in standard retrieval for ad-hoc queries.",
                "How to deploy such a strategy for long-lasting information needs in AF settings is an open question for research. 2.",
                "The unit for receiving a relevance judgment (yes or no) is restricted to the document level in conventional AF.",
                "However, a real user may be willing to provide more informative, fine-grained feedback via highlighting some pieces of text in a retrieved document as relevant, instead of labeling the entire document as relevant.",
                "Effectively leveraging such fine-grained feedback could substantially enhance the quality of an AF system.",
                "For this, we need to enable supervised learning from labeled pieces of text of arbitrary span instead of just allowing labeled documents. 3.",
                "System-selected documents are often highly redundant.",
                "A major news event, for example, would be reported by multiple sources repeatedly for a while, making most of the information content in those articles redundant with each other.",
                "A conventional AF system would select all these redundant news stories for user feedback, wasting the users time while offering little gain.",
                "Clearly, techniques for novelty detection can help in principle [25, 2, 22] for improving the utility of the AF systems.",
                "However, the effectiveness of such techniques at passage level to detect novelty with respect to users (fine-grained) feedback and to detect redundancy in ranked lists remains to be evaluated using a measure of utility that mimics the needs of a real user.",
                "To address the above limitations of current AF systems, we propose and examine a new approach in this paper, combining the strengths of conventional AF (incremental learning of topic models), multi-pass passage retrieval for long-lasting queries conditioned on topic, and novelty detection for removal of redundancy from user interactions with the system.",
                "We call the new process utility-based information distillation.",
                "Note that conventional benchmark corpora for AF evaluations, which have relevance judgments at the document level and do not define tasks with multiple queries, are insufficient for evaluating the new approach.",
                "Therefore, we extended a benchmark corpus - the TDT4 collection of news stories and TV broadcasts - with task definitions, multiple queries per task, and answer keys per query.",
                "We have conducted our experiments on this extended TDT4 corpus and have made the additionally generated data publicly available for future comparative evaluations 1 .",
                "To automatically evaluate the system-returned arbitrary spans of text using our answer keys, we further developed an evaluation scheme with semi-automatic procedure for 1 URL: http://nyc.lti.cs.cmu.edu/downloads acquiring rules that can match nuggets against system responses.",
                "Moreover, we propose an extension of NDCG (Normalized Discounted Cumulated Gain) [9] for assessing the utility of ranked passages as a function of both relevance and novelty.",
                "The rest of this paper is organized as follows.",
                "Section 2 outlines the information distillation process with a concrete example.",
                "Section 3 describes the technical cores of our system called CAF´E - CMU Adaptive Filtering Engine.",
                "Section 4 discusses issues with respect to evaluation methodology and proposes a new scheme.",
                "Section 5 describes the extended TDT4 corpus.",
                "Section 6 presents our experiments and results.",
                "Section 7 concludes the study and gives future perspectives. 2.",
                "A SAMPLE TASK Consider a news event - the escape of seven convicts from a Texas prison in December 2000 and their capture a month later.",
                "Assuming a user were interested in this event since its early stage, the information need could be: Find information about the escape of convicts from Texas prison, and information related to their recapture.",
                "The associated lower-level questions could be: 1.",
                "How many prisoners escaped? 2.",
                "Where and when were they sighted? 3. Who are their known contacts inside and outside the prison? 4.",
                "How are they armed? 5.",
                "Do they have any vehicles? 6.",
                "What steps have been taken so far?",
                "We call such an information need a task, and the associated questions as the queries in this task.",
                "A distillation system is supposed to monitor the incoming documents, process them chunk by chunk in a temporal order, select potentially relevant and novel passages from each chunk with respect to each query, and present a ranked list of passages to the user.",
                "Passage ranking here is based on how relevant a passage is with respect to the current query, how novel it is with respect to the current user history (of his or her interactions with the system), and how redundant it is compared to other passages with a higher rank in the list.",
                "When presented with a list of passages, the user may provide feedback by highlighting arbitrary spans of text that he or she found relevant.",
                "These spans of text are taken as positive examples in the adaptation of the query profile, and also added to the users history.",
                "Passages not marked by the user are taken as negative examples.",
                "As soon as the query profile is updated, the system re-issues a search and returns another ranked list of passages where the previously seen passages are either removed or ranked low, based on user preference.",
                "For example, if the user highlights ...officials have posted a $100,000 reward for their capture... as relevant answer to the query What steps have been taken so far?, then the highlighted piece is used as an additional positive training example in the adaptation of the query profile.",
                "This piece of feedback is also added to the user history as a seen example, so that in future, the system will not place another passage mentioning $100,000 reward at the top of the ranked list.",
                "However, an article mentioning ...officials have doubled the reward money to $200,000... might be ranked high since it is both relevant to the (updated) query profile and novel with respect to the (updated) user history.",
                "The user may modify the original queries or add a new query during the process; the query profiles will be changed accordingly.",
                "Clearly, novelty detection is very important for the utility of such a system because of the iterative search.",
                "Without novelty detection, the old relevant passages would be shown to the user repeatedly in each ranked list.",
                "Through the above example, we can see the main properties of our new framework for utility-based information distillation over temporally ordered documents.",
                "Our framework combines and extends the power of adaptive filtering (AF), <br>ad-hoc retrieval</br> (IR) and novelty detection (ND).",
                "Compared to standard IR, our approach has the power of incrementally learning long-term information needs and modeling a sequence of queries within a task.",
                "Compared to conventional AF, it enables a more active role of the user in refining his or her information needs and requesting new results by allowing relevance and novelty feedback via highlighting of arbitrary spans of text in passages returned by the system.",
                "Compared to past work, this is the first evaluation of novelty detection integrated with adaptive filtering for sequenced queries that allows flexible user feedback over ranked passages.",
                "The combination of AF, IR and ND with the new extensions raises an important research question regarding evaluation methodology: how can we measure the utility of such an information distillation system?",
                "Existing metrics in standard IR, AF and ND are insufficient, and new solutions must be explored, as we will discuss in Section 4, after describing the technical cores of our system in the next section. 3.",
                "TECHNICAL CORES The core components of CAF´E are - 1) AF for incremental learning of query profiles, 2) IR for estimating relevance of passages with respect to query profiles, 3) ND for assessing novelty of passages with respect to users history, and 4) anti-redundancy component to remove redundancy from ranked lists. 3.1 Adaptive Filtering Component We use a state-of-the-art algorithm in the field - the regularized logistic regression method which had the best results on several benchmark evaluation corpora for AF [21].",
                "Logistic regression (LR) is a supervised learning algorithm for statistical classification.",
                "Based on a training set of labeled instances, it learns a class model which can then by used to predict the labels of unseen instances.",
                "Its performance as well as efficiency in terms of training time makes it a good candidate when frequent updates of the class model are required, as is the case in adaptive filtering, where the system must learn from each new feedback provided by the user. (See [21] and [23] for computational complexity and implementation issues).",
                "In adaptive filtering, each query is considered as a class and the probability of a passage belonging to this class corresponds to the degree of relevance of the passage with respect to the query.",
                "For training the model, we use the query itself as the initial positive training example of the class, and the user-highlighted pieces of text (marked as Relevant or Not-relevant) during feedback as additional training examples.",
                "To address the cold start issue in the early stage before any user feedback is obtained, the system uses a small sample from a retrospective corpus as the initial negative examples in the training set.",
                "The details of using logistic regression for adaptive filtering (assigning different weights to positive and negative training instances, and regularizing the objective function to prevent over-fitting on training data) are presented in [21].",
                "The class model w∗ learned by Logistic Regression, or the query profile, is a vector whose dimensions are individual terms and whose elements are the regression coefficients, indicating how influential each term is in the query profile.",
                "The query profile is updated whenever a new piece of user feedback is received.",
                "A temporally decaying weight can be applied to each training example, as an option, to emphasize the most recent user feedback. 3.2 Passage Retrieval Component We use standard IR techniques in this part of our system.",
                "Incoming documents are processed in chunks, where each chunk can be defined as a fixed span of time or as a fixed number of documents, as preferred by the user.",
                "For each incoming document, corpus statistics like the IDF (Inverted Document Frequency) of each term are updated.",
                "We use a state-of-the-art named entity identifier and tracker [8, 12] to identify person and location names, and merge them with co-referent named entities seen in the past.",
                "Then the documents are segmented into passages, which can be a whole document, a paragraph, a sentence, or any other continuous span of text, as preferred.",
                "Each passage is represented using a vector of TF-IDF (Term FrequencyInverse Document Frequency) weights, where term can be a word or a named entity.",
                "Given a query profile, i.e. the logistic regression solution w∗ as described in Section 3.1, the system computes the posterior probability of relevance for each passage x as fRL(x) ≡ P(y = 1|x, w∗ ) = 1 (1 + e−w∗·x) (1) Passages are ordered by their relevance scores, and the ones with scores above a threshold (tuned on a training set) comprise the relevance list that is passed on to the novelty detection step. 3.3 Novelty Detection Component CAF´E maintains a user history H(t), which contains all the spans of text hi that the user highlighted (as feedback) during his or her past interactions with the system, up to the current time t. Denoting the history as H(t) = n h1, h2, ..., ht o , (2) the novelty score of a new candidate passage x is computed as: fND(x) = 1 − max i∈1..t {cos(x, hi)} (3) where both candidate passage x and highlighted spans of text hi are represented as TF-IDF vectors.",
                "The novelty score of each passage is compared to a prespecified threshold (also tuned on a training set), and any passage with a score below this threshold is removed from the relevance list. 3.4 Anti-redundant Ranking Component Although the novelty detection component ensures that only novel (previously unseen) information remains in the relevance list, this list might still contain the same novel information at multiple positions in the ranked list.",
                "Suppose, for example, that the user has already read about a $100,000 reward for information about the escaped convicts.",
                "A new piece of news that the award has been increased to $200,000 is novel since the user hasnt read about it yet.",
                "However, multiple news sources would report this news and we might end up showing (redundant) articles from all these sources in a ranked list.",
                "Hence, a ranked list should also be made non-redundant with respect to its own contents.",
                "We use a simplified version of the Maximal Marginal Relevance method [5], originally developed for combining relevance and novelty in text retrieval and summarization.",
                "Our procedure starts with the current list of passages sorted by relevance (section 3.2), filtered by Novelty Detection component (section 3.3), and generates a new non-redundant list as follows: 1.",
                "Take the top passage in the current list as the top one in the new list. 2.",
                "Add the next passage x in the current list to the new list only if fAR(x) > t where fAR(x) = 1 − max pi∈Lnew {cos(x, pi)} and Lnew is the set of passages already selected in the new list. 3.",
                "Repeat step 2 until all the passages in the current list have been examined.",
                "After applying the above-mentioned algorithm, each passage in the new list is sufficiently dissimilar to others, thus favoring diversity rather than redundancy in the new ranked list.",
                "The anti-redundancy threshold t is tuned on a training set. 4.",
                "EVALUATION METHODOLOGY The approach we proposed above for information distillation raises important issues regarding evaluation methodology.",
                "Firstly, since our framework allows the output to be passages at different levels of granularity (e.g. k-sentence windows where k may vary) instead of a fixed length, it is not possible to have pre-annotated relevance judgments at all such granularity levels.",
                "Secondly, since we wish to measure the utility of the system output as a combination of both relevance and novelty, traditional relevance-only based measures must be replaced by measures that penalize the repetition of the same information in the system output across time.",
                "Thirdly, since the output of the system is ranked lists, we must reward those systems that present useful information (both relevant and previously unseen) using shorter ranked lists, and penalize those that present the same information using longer ranked lists.",
                "None of the existing measures in <br>ad-hoc retrieval</br>, adaptive filtering, novelty detection or other related areas (text summarization and question answering) have desirable properties in all the three aspects.",
                "Therefore, we must develop a new evaluation methodology. 4.1 Answer Keys To enable the evaluation of a system whose output consists of passages of arbitrary length, we borrow the concept of answer keys from the Question Answering (QA) community, where systems are allowed to return arbitrary spans of text as answers.",
                "Answer keys define what should be present in a system response to receive credit, and are comprised of a collection of information nuggets, i.e. factoid units about which human assessors can make binary decisions of whether or not a system response contains them.",
                "Defining answer keys and making the associated binary decisions are conceptual tasks that require semantic mapping [19], since system-returned passages can contain the same information expressed in many different ways.",
                "Hence, QA evaluations have relied on human assessors for the mapping between various expressions, making the process costly, time consuming, and not scalable to large query and document collections, and extensive system evaluations with various parameter settings. 4.1.1 Automating Evaluation based on Answer Keys Automatic evaluation methods would allow for faster system building and tuning, as well as provide an objective and affordable way of comparing various systems.",
                "Recently, such methods have been proposed, more or less, based on the idea of n-gram co-occurrences.",
                "Pourpre [10] assigns a fractional recall score to a system response based on its unigram overlap with a given nuggets description.",
                "For example, a system response A B C has recall 3/4 with respect to a nugget with description A B C D. However, such an approach is unfair to systems that present the same information but using words other than A, B, C, and D. Another open issue is how to weight individual words in measuring the closeness of a match.",
                "For example, consider the question How many prisoners escaped?.",
                "In the nugget Seven prisoners escaped from a Texas prison, there is no indication that seven is the keyword, and that it must be matched to get any relevance credit.",
                "Using IDF values does not help, since seven will generally not have a higher IDF than words like texas and prison.",
                "Also, redefining the nugget as just seven does not solve the problem since now it might spuriously match any mention of seven out of context.",
                "Nuggeteer [13] works on similar principles but makes binary decisions about whether a nugget is present in a given system response by tuning a threshold.",
                "However, it is also plagued by spurious relevance since not all words contained in the nugget description (or known correct responses) are central to the nugget. 4.1.2 Nugget-Matching Rules We propose a reliable automatic method for determining whether a snippet of text contains a given nugget, based on nugget-matching rules, which are generated using a semiautomatic procedure explained below.",
                "These rules are essentially Boolean queries that will only match against snippets that contain the nugget.",
                "For instance, a candidate rule for matching answers to How many prisoners escaped? is (Texas AND seven AND escape AND (convicts OR prisoners)), possibly with other synonyms and variants in the rule.",
                "For a corpus of news articles, which usually follow a typical formal prose, it is fairly easy to write such simple rules to match expected answers using a bootstrap approach, as described below.",
                "We propose a two-stage approach, inspired by Autoslog [14], that combines the strength of humans in identifying semantically equivalent expressions and the strength of the system in gathering statistical evidence from a humanannotated corpus of documents.",
                "In the first stage, human subjects annotated (using a highlighting tool) portions of ontopic documents that contained answers to each nugget 2 .",
                "In the second stage, subjects used our rule generation tool to create rules that would match the annotations for each nugget.",
                "The tool allows users to enter a Boolean rule as a disjunction of conjunctions (e.g. ((a AND b) OR (a AND c AND d) OR (e))).",
                "Given a candidate rule, our tool uses it as a Boolean query over the entire set of on-topic documents and calculates its recall and precision with respect to the annotations that it is expected to match.",
                "Hence, the subjects can start with a simple rule and iteratively refine it until they are satisfied with its recall and precision.",
                "We observed that it was very easy for humans to improve the precision of a rule by tweaking its existing conjunctions (adding more ANDs), and improving the recall by adding more conjunctions to the disjunction (adding more ORs).",
                "As an example, lets try to create a rule for the nugget which says that seven prisoners escaped from the Texas prison.",
                "We start with a simple rule - (seven).",
                "When we input this into the rule generation tool, we realize that this rule matches many spurious occurrences of seven (e.g. ...seven states...) and thus gets a low precision score.",
                "We can further qualify our rule - Texas AND seven AND convicts.",
                "Next, by looking at the missed annotations, we realize that some news articles mentioned ...seven prisoners escaped.... We then replace convicts with the disjunction (convicts OR prisoners).",
                "We continue tweaking the rule in this manner until we achieve a sufficiently high recall and precision - i.e. the (small number of) misses and false alarms can be safely ignored.",
                "Thus we can create nugget-matching rules that succinctly capture various ways of expressing a nugget, while avoiding matching incorrect (or out of context) responses.",
                "Human involvement in the rule creation process ensures high quality generic rules which can then be used to evaluate arbitrary system responses reliably. 4.2 Evaluating the Utility of a Sequence of Ranked Lists The utility of a retrieval system can be defined as the difference between how much the user gained in terms of useful information, and how much the user lost in terms of time and energy.",
                "We calculate this utility from the utilities of individual passages as follows.",
                "After reading each passage returned by the system, the user derives some gain depending on the presence of relevant and novel information, and incurs a loss in terms of the time and energy spent in going through the passage.",
                "However, the likelihood that the user would actually read a passage depends on its position in the ranked list.",
                "Hence, for a query q, the expected utility 2 LDC [18] already provides relevance judgments for 100 topics on the TDT4 corpus.",
                "We further ensured that these judgments are exhaustive on the entire corpus using pooling. of a passage pi at rank i can be defined as U(pi, q) = P(i) ∗ (Gain(pi, q) − Loss(pi, q)) (4) where P(i) is the probability that the user would go through a passage at rank i.",
                "The expected utility for an entire ranked list of length n can be calculated simply by adding the expected utility of each passage: U(q) = nX i=1 P(i) ∗ (Gain(pi, q) − Loss(pi, q)) (5) Note that if we ignore the loss term and define P(i) as P(i) ∝ 1/ logb(b + i − 1) (6) then we get the recently popularized metric called Discounted Cumulated Gain (DCG) [9], where Gain(pi, q) is defined as the graded relevance of passage pi.",
                "However, without the loss term, DCG is a purely recall-oriented metric and not suitable for an adaptive filtering setting, where the systems utility depends in part on its ability to limit the number of items shown to the user.",
                "Although P(i) could be defined based on empirical studies of user behavior, for simplicity, we use P(i) exactly as defined in equation 6.",
                "The gain G(pi, q) of passage pi with respect to the query q is a function of - 1) the number of relevant nuggets present in pi, and 2) the novelty of each of these nuggets.",
                "We combine these two factors as follows.",
                "For each nugget Nj, we assign an initial weight wj, and also keep a count nj of the number of times this nugget has been seen by the user in the past.",
                "The gain derived from each subsequent occurrence of the same nugget is assumed to reduce by a dampening factor γ.",
                "Thus, G(pi, q) is defined as G(pi, q) = X Nj ∈C(pi,q) wj ∗ γnj (7) where C(pi, q) is the set of all nuggets that appear in passage pi and also belong to the answer key of query q.",
                "The initial weights wj are all set of be 1.0 in our experiments, but can also be set based on a pyramid approach [11].",
                "The choice of dampening factor γ determines the users tolerance for redundancy.",
                "When γ = 0, a nugget will only receive credit for its first occurrence i.e. when nj is zero3 .",
                "For 0 < γ < 1, a nugget receives smaller credit for each successive occurrence.",
                "When γ = 1, no dampening occurs and repeated occurrences of a nugget receive the same credit.",
                "Note that the nugget occurrence counts are preserved between evaluation of successive ranked lists returned by the system, since the users are expected to remember what the system showed them in the past.",
                "We define the loss L(pi, q) as a constant cost c (we use 0.1) incurred when reading a system-returned passage.",
                "Thus, our metric can be re-written as U(q) = nX i=1 Gain(pi, q) logb(b + i − 1) − L(n) (8) where L(n) is the loss associated with a ranked list of length n: L(n) = c · nX i=1 1 logb(b + i − 1) (9) 3 Note that 00 = 1 Due to the similarity with Discounted Cumulated Gain (DCG), we call our metric Discounted Cumulated Utility (DCU).",
                "The DCU score obtained by the system is converted to a Normalized DCU (NDCU) score by dividing it by the DCU score of the ideal ranked list, which is created by ordering passages by their decreasing utility scores U(pi, q) and stopping when U(pi, q) ≤ 0 i.e. when the gain is less than or equal to the cost of reading the passage. 5.",
                "DATA TDT4 was the benchmark corpus used in TDT2002 and TDT2003 evaluations.",
                "The corpus consists of over 90, 000 news articles from multiple sources (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) published between October 2000 and January 2001, in the languages of Arabic, English, and Mandarin.",
                "Speech-recognized and machine-translated versions of the non-English articles were provided as well.",
                "LDC [18] has annotated the corpus with 100 topics, that correspond to various news events in this time period.",
                "Out of these, we selected a subset of 12 actionable events, and defined corresponding tasks for them4 .",
                "For each task, we manually defined a profile consisting of an initial set of (5 to 10) queries, a free-text description of the user history, i.e., what the user already knows about the event, and a list of known on-topic and off-topic documents (if available) as training examples.",
                "For each query, we generated answer keys and corresponding nugget matching rules using the procedure described in section 4.1.2, and produced a total of 120 queries, with an average of 7 nuggets per query. 6.",
                "EXPERIMENTS AND RESULTS 6.1 Baselines We used Indri [17], a popular language-model based retrieval engine, as a baseline for comparison with CAF´E.",
                "Indri supports standard search engine functionality, including pseudo-relevance feedback (PRF) [3, 6], and is representative of a typical query-based retrieval system.",
                "Indri does not support any kind of novelty detection.",
                "We compare Indri with PRF turned on and off, against CAF´E with user feedback, novelty detection and antiredundant ranking turned on and off. 6.2 Experimental Setup We divided the TDT4 corpus spanning 4 months into 10 chunks, each defined as a period of 12 consecutive days.",
                "At any given point of time in the distillation process, each system accessed the past data up to the current point, and returned a ranked list of up 50 passages per query.",
                "The 12 tasks defined on the corpus were divided into a training and test set with 6 tasks each.",
                "Each system was allowed to use the training set to tune its parameters for optimizing NDCU (equation 8), including the relevance threshold for both Indri and CAF´E, and the novelty and antiredundancy thresholds for CAF´E.",
                "The NDCU for each system run is calculated automatically.",
                "User feedback was also simulated - relevance judgments for each system-returned passage (as determined by the nugget matching rules described in section 4.1.2) were 4 URL: http://nyc.lti.cs.cmu.edu/downloads Figure 1: Performance of Indri across chunks Figure 2: Performance of CAF´E across chunks used as user feedback in the adaptation of query profiles and user histories. 6.3 Results In Table 1, we show the NDCU scores of the two systems under various settings.",
                "These scores are averaged over the six tasks in the test set, and are calculated with two dampening factors (see section 4.2): γ = 0 and 0.1, to simulate no tolerance and small tolerance for redundancy, respectively.",
                "Using γ = 0 creates a much more strict metric since it does not give any credit to a passage that contains relevant but redundant information.",
                "Hence, the improvement obtained from enabling user feedback is smaller with γ = 0 than the improvement obtained from feedback with γ = 0.1.",
                "This reveals a shortcoming of contemporary retrieval systemswhen the user gives positive feedback on a passage, the systems gives higher weights to the terms present in that passage and tends to retrieve other passages containing the same terms - and thus - usually the same information.",
                "However, the user does not benefit from seeing such redundant passages, and is usually interested in other passages containing related information.",
                "It is informative to evaluate retrieval systems using our utility measure (with γ = 0) which accounts for novelty and thus gives a more realistic picture of how well a system can generalize from user feedback, rather than using traditional IR measures like recall and precision which give an incomplete picture of improvement obtained from user feedback.",
                "Sometimes, however, users might indeed be interested in seeing the same information from multiple sources, as an Table 1: NDCU Scores of Indri and CAF´E for two dampening factors (γ), and various settings (F: Feedback, N: Novelty Detection, A: Anti-Redundant Ranking) Indri CAF´E γ Base +PRF Base +F +F+N +F+A +F+N+A 0 0.19 0.19 0.22 0.23 0.24 0.24 0.24 0.1 0.28 0.29 0.24 0.35 0.35 0.36 0.36 indicator of its importance or reliability.",
                "In such a case, we can simply choose a higher value for γ which corresponds to a higher tolerance for redundancy, and hence let the system tune its parameters accordingly.",
                "Since documents were processed chunk by chunk, it would be interesting to see how the performance of systems improves over time.",
                "Figures 1 and 2 show the performance trends for both the systems across chunks.",
                "While the performance with and without feedback on the first few chunks is expected to be close, for subsequent chunks, the performance curve with feedback enabled rises above the one with the no-feedback setting.",
                "The performance trends are not consistent across all chunks because on-topic documents are not uniformly distributed over all the chunks, making some queries easier than others in certain chunks.",
                "Moreover, since Indri uses pseudo-relevance feedback while CAF´E uses feedback based on actual relevance judgments, the improvement in case of Indri is less dramatic than that of CAF´E. 7.",
                "CONCLUDING REMARKS This paper presents the first investigation on utility-based information distillation with a system that learns the longlasting information needs from fine-grained user feedback over a sequence of ranked passages.",
                "Our system, called CAF´E, combines adaptive filtering, novelty detection and antiredundant passage ranking in a unified framework for utility optimization.",
                "We developed a new scheme for automated evaluation and feedback based on a semi-automatic procedure for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also proposed an extension of the NDCG metric for assessing the utility of ranked passages as a weighted combination of relevance and novelty.",
                "Our experiments on the newly annotated TDT4 benchmark corpus show encouraging utility enhancement over Indri, and also over our own system with incremental learning and novelty detection turned off. 8.",
                "ACKNOWLEDGMENTS We would like to thank Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng, and the Qualitative Data Analysis Program at the University of Pittsburgh lead by Dr. Stuart Shulman for their help with collecting and processing the extended TDT4 annotations used in our experiments.",
                "This work is supported in parts by the National Science Foundation (NSF) under grant IIS0434035, and the Defense Advanced Research Project Agency (DARPA) under contracts NBCHD030010 and W0550432.",
                "Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors. 9.",
                "ADDITIONAL AUTHORS Jian Zhang (jianzhan@stat.purdue.edu)∗ , Jaime Carbonell (jgc@cs.cmu.edu)† , Peter Brusilovsky (peterb+@pitt.edu)‡ , Daqing He(dah44@pitt.edu)‡ 10.",
                "REFERENCES [1] J. Allan.",
                "Incremental Relevance Feedback for Information Filtering.",
                "Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, pages 270-278, 1996. [2] J. Allan, C. Wade, and A. Bolivar.",
                "Retrieval and Novelty Detection at the Sentence Level.",
                "Proceedings of the ACM SIGIR conference on research and development in information retrieval, 2003. [3] C. Buckley, G. Salton, and J. Allan.",
                "Automatic Retrieval with Locality Information using SMART.",
                "NIST special publication, (500207):59-72, 1993. [4] J. Callan.",
                "Learning While Filtering Documents.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 224-231, 1998. [5] J. Carbonell and J. Goldstein.",
                "The use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 335-336, 1998. [6] E. Efthimiadis.",
                "Query Expansion.",
                "Annual Review of Information Science and Technology (ARIST), 31:p121-87, 1996. [7] J. Fiscus and G. Duddington.",
                "Topic Detection and Tracking Overview.",
                "Topic Detection and Tracking: Event-based Information Organization, pages 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos.",
                "A Statistical Model for Multilingual Entity Detection and Tracking.",
                "NAACL/HLT, 2004. [9] K. J¨arvelin and J. Kek¨al¨ainen.",
                "Cumulated Gain-based Evaluation of IR Techniques.",
                "ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin and D. Demner-Fushman.",
                "Automatically Evaluating Answers to Definition Questions.",
                "Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), 2005. ∗ Statistics Dept., Purdue University, West Lafayette, USA † Language Technologies Inst., Carnegie Mellon University, Pittsburgh, USA ‡ School of Information Sciences, Univ. of Pittsburgh, Pittsburgh, USA [11] J. Lin and D. Demner-Fushman.",
                "Will Pyramids Built of nUggets Topple Over.",
                "Proceedings of HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos.",
                "A Mention-synchronous Coreference Resolution Algorithm based on the Bell Tree.",
                "Proc. of ACL, 4:136-143, 2004. [13] G. Marton.",
                "Nuggeteer: Automatic Nugget-Based Evaluation Using Descriptions and Judgments.",
                "HLT/NAACL, 2006. [14] E. Riloff.",
                "Automatically Constructing a Dictionary for Information Extraction Tasks.",
                "Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 811-816, 1993. [15] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9: Filtering track.",
                "The Ninth Text REtrieval Conference (TREC-9), pages 361-368. [16] R. Schapire, Y.",
                "Singer, and A. Singhal.",
                "Boosting and Rocchio Applied to Text Filtering.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle, and W. Croft.",
                "Indri: A Language Model-based Search Engine for Complex Queries.",
                "Proceedings of the International Conference on Intelligence Analysis, 2004. [18] The Linguistic Data Consortium. http://www.ldc.upenn.edu/. [19] E. Voorhees.",
                "Overview of the TREC 2003 Question Answering Track.",
                "Proceedings of the Twelfth Text REtrieval Conference (TREC 2003), 2003. [20] Y. Yang and B. Kisiel.",
                "Margin-based Local Regression for Adaptive Filtering.",
                "Proceedings of the twelfth international conference on Information and knowledge management, pages 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of Adaptive Filtering Methods in a Cross-benchmark Evaluation.",
                "Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 98-105, 2005. [22] C. Zhai, W. Cohen, and J. Lafferty.",
                "Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 10-17, 2003. [23] J. Zhang and Y. Yang.",
                "Robustness of Regularized Linear Classification Methods in Text Categorization.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 190-197, 2003. [24] Y. Zhang.",
                "Using Bayesian Priors to Combine Classifiers for Adaptive Filtering.",
                "Proceedings of the 27th annual international conference on Research and development in information retrieval, pages 345-352, 2004. [25] Y. Zhang, J. Callan, and T. Minka.",
                "Novelty and Redundancy Detection in Adaptive Filtering.",
                "Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "La naturaleza de aprendizaje incremental de los sistemas AF los hace más poderosos que los motores de búsqueda estándar que respaldan la \"recuperación ad-hoc\" (por ejemplo,",
                "Nuestro marco combina y extiende el poder del filtrado adaptativo (AF), la \"recuperación ad-hoc\" (IR) y la detección de novedad (ND).",
                "Ninguna de las medidas existentes en \"recuperación ad-hoc\", filtrado adaptativo, detección de novedades u otras áreas relacionadas (resumen de texto y respuesta de preguntas) tienen propiedades deseables en los tres aspectos."
            ],
            "translated_text": "",
            "candidates": [
                "recuperación ad-hoc",
                "recuperación ad-hoc",
                "recuperación ad-hoc",
                "recuperación ad-hoc",
                "recuperación ad-hoc",
                "recuperación ad-hoc"
            ],
            "error": []
        },
        "novelty detection": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Utility-based Information Distillation Over Temporally Sequenced Documents Yiming Yang Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA yiming@cs.cmu.edu Abhimanyu Lad Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA alad@cs.cmu.edu Ni Lao Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA nlao@cs.cmu.edu Abhay Harpale Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA aharpale@cs.cmu.edu Bryan Kisiel Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA bkisiel@cs.cmu.edu Monica Rogati Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA mrogati@cs.cmu.edu ABSTRACT This paper examines a new approach to information distillation over temporally ordered documents, and proposes a novel evaluation scheme for such a framework.",
                "It combines the strengths of and extends beyond conventional adaptive filtering, <br>novelty detection</br> and non-redundant passage ranking with respect to long-lasting information needs (tasks with multiple queries).",
                "Our approach supports fine-grained user feedback via highlighting of arbitrary spans of text, and leverages such information for utility optimization in adaptive settings.",
                "For our experiments, we defined hypothetical tasks based on news events in the TDT4 corpus, with multiple queries per task.",
                "Answer keys (nuggets) were generated for each query and a semiautomatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also propose an extension of the NDCG metric for assessing the utility of ranked passages as a combination of relevance and novelty.",
                "Our results show encouraging utility enhancements using the new approach, compared to the baseline systems without incremental learning or the <br>novelty detection</br> components.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 General Terms Design, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Tracking new and relevant information from temporal data streams for users with long-lasting needs has been a challenging research topic in information retrieval.",
                "Adaptive filtering (AF) is one such task of online prediction of the relevance of each new document with respect to pre-defined topics.",
                "Based on the initial query and a few positive examples (if available), an AF system maintains a profile for each such topic of interest, and constantly updates it based on feedback from the user.",
                "The incremental learning nature of AF systems makes them more powerful than standard search engines that support ad-hoc retrieval (e.g.",
                "Google and Yahoo) in terms of finding relevant information with respect to long-lasting topics of interest, and more attractive for users who are willing to provide feedback to adapt the system towards their specific information needs, without having to modify their queries manually.",
                "A variety of supervised learning algorithms (Rocchio-style classifiers, Exponential-Gaussian models, local regression and logistic regression approaches) have been studied for adaptive settings, examined with explicit and implicit relevance feedback, and evaluated with respect to utility optimization on large benchmark data collections in TREC (Text Retrieval Conferences) and TDT (Topic Detection and Tracking) forums [1, 4, 7, 15, 16, 20, 24, 23].",
                "Regularized logistic regression [21] has been found representative for the state-of-the-art approaches, and highly efficient for frequent model adaptations over large document collections such as the TREC-10 corpus (over 800,000 documents and 84 topics).",
                "Despite substantial achievements in recent adaptive filtering research, significant problems remain unsolved regarding how to leverage user feedback effectively and efficiently.",
                "Specifically, the following issues may seriously limit the true utility of AF systems in real-world applications: 1.",
                "User has a rather passive role in the conventional adaptive filtering setup - he or she reacts to the system only when the system makes a yes decision on a document, by confirming or rejecting that decision.",
                "A more active alternative would be to allow the user to issue multiple queries for a topic, review a ranked list of candidate documents (or passages) per query, and provide feedback on the ranked list, thus refining their information need and requesting updated ranked lists.",
                "The latter form of user interaction has been highly effective in standard retrieval for ad-hoc queries.",
                "How to deploy such a strategy for long-lasting information needs in AF settings is an open question for research. 2.",
                "The unit for receiving a relevance judgment (yes or no) is restricted to the document level in conventional AF.",
                "However, a real user may be willing to provide more informative, fine-grained feedback via highlighting some pieces of text in a retrieved document as relevant, instead of labeling the entire document as relevant.",
                "Effectively leveraging such fine-grained feedback could substantially enhance the quality of an AF system.",
                "For this, we need to enable supervised learning from labeled pieces of text of arbitrary span instead of just allowing labeled documents. 3.",
                "System-selected documents are often highly redundant.",
                "A major news event, for example, would be reported by multiple sources repeatedly for a while, making most of the information content in those articles redundant with each other.",
                "A conventional AF system would select all these redundant news stories for user feedback, wasting the users time while offering little gain.",
                "Clearly, techniques for <br>novelty detection</br> can help in principle [25, 2, 22] for improving the utility of the AF systems.",
                "However, the effectiveness of such techniques at passage level to detect novelty with respect to users (fine-grained) feedback and to detect redundancy in ranked lists remains to be evaluated using a measure of utility that mimics the needs of a real user.",
                "To address the above limitations of current AF systems, we propose and examine a new approach in this paper, combining the strengths of conventional AF (incremental learning of topic models), multi-pass passage retrieval for long-lasting queries conditioned on topic, and <br>novelty detection</br> for removal of redundancy from user interactions with the system.",
                "We call the new process utility-based information distillation.",
                "Note that conventional benchmark corpora for AF evaluations, which have relevance judgments at the document level and do not define tasks with multiple queries, are insufficient for evaluating the new approach.",
                "Therefore, we extended a benchmark corpus - the TDT4 collection of news stories and TV broadcasts - with task definitions, multiple queries per task, and answer keys per query.",
                "We have conducted our experiments on this extended TDT4 corpus and have made the additionally generated data publicly available for future comparative evaluations 1 .",
                "To automatically evaluate the system-returned arbitrary spans of text using our answer keys, we further developed an evaluation scheme with semi-automatic procedure for 1 URL: http://nyc.lti.cs.cmu.edu/downloads acquiring rules that can match nuggets against system responses.",
                "Moreover, we propose an extension of NDCG (Normalized Discounted Cumulated Gain) [9] for assessing the utility of ranked passages as a function of both relevance and novelty.",
                "The rest of this paper is organized as follows.",
                "Section 2 outlines the information distillation process with a concrete example.",
                "Section 3 describes the technical cores of our system called CAF´E - CMU Adaptive Filtering Engine.",
                "Section 4 discusses issues with respect to evaluation methodology and proposes a new scheme.",
                "Section 5 describes the extended TDT4 corpus.",
                "Section 6 presents our experiments and results.",
                "Section 7 concludes the study and gives future perspectives. 2.",
                "A SAMPLE TASK Consider a news event - the escape of seven convicts from a Texas prison in December 2000 and their capture a month later.",
                "Assuming a user were interested in this event since its early stage, the information need could be: Find information about the escape of convicts from Texas prison, and information related to their recapture.",
                "The associated lower-level questions could be: 1.",
                "How many prisoners escaped? 2.",
                "Where and when were they sighted? 3. Who are their known contacts inside and outside the prison? 4.",
                "How are they armed? 5.",
                "Do they have any vehicles? 6.",
                "What steps have been taken so far?",
                "We call such an information need a task, and the associated questions as the queries in this task.",
                "A distillation system is supposed to monitor the incoming documents, process them chunk by chunk in a temporal order, select potentially relevant and novel passages from each chunk with respect to each query, and present a ranked list of passages to the user.",
                "Passage ranking here is based on how relevant a passage is with respect to the current query, how novel it is with respect to the current user history (of his or her interactions with the system), and how redundant it is compared to other passages with a higher rank in the list.",
                "When presented with a list of passages, the user may provide feedback by highlighting arbitrary spans of text that he or she found relevant.",
                "These spans of text are taken as positive examples in the adaptation of the query profile, and also added to the users history.",
                "Passages not marked by the user are taken as negative examples.",
                "As soon as the query profile is updated, the system re-issues a search and returns another ranked list of passages where the previously seen passages are either removed or ranked low, based on user preference.",
                "For example, if the user highlights ...officials have posted a $100,000 reward for their capture... as relevant answer to the query What steps have been taken so far?, then the highlighted piece is used as an additional positive training example in the adaptation of the query profile.",
                "This piece of feedback is also added to the user history as a seen example, so that in future, the system will not place another passage mentioning $100,000 reward at the top of the ranked list.",
                "However, an article mentioning ...officials have doubled the reward money to $200,000... might be ranked high since it is both relevant to the (updated) query profile and novel with respect to the (updated) user history.",
                "The user may modify the original queries or add a new query during the process; the query profiles will be changed accordingly.",
                "Clearly, <br>novelty detection</br> is very important for the utility of such a system because of the iterative search.",
                "Without <br>novelty detection</br>, the old relevant passages would be shown to the user repeatedly in each ranked list.",
                "Through the above example, we can see the main properties of our new framework for utility-based information distillation over temporally ordered documents.",
                "Our framework combines and extends the power of adaptive filtering (AF), ad-hoc retrieval (IR) and <br>novelty detection</br> (ND).",
                "Compared to standard IR, our approach has the power of incrementally learning long-term information needs and modeling a sequence of queries within a task.",
                "Compared to conventional AF, it enables a more active role of the user in refining his or her information needs and requesting new results by allowing relevance and novelty feedback via highlighting of arbitrary spans of text in passages returned by the system.",
                "Compared to past work, this is the first evaluation of <br>novelty detection</br> integrated with adaptive filtering for sequenced queries that allows flexible user feedback over ranked passages.",
                "The combination of AF, IR and ND with the new extensions raises an important research question regarding evaluation methodology: how can we measure the utility of such an information distillation system?",
                "Existing metrics in standard IR, AF and ND are insufficient, and new solutions must be explored, as we will discuss in Section 4, after describing the technical cores of our system in the next section. 3.",
                "TECHNICAL CORES The core components of CAF´E are - 1) AF for incremental learning of query profiles, 2) IR for estimating relevance of passages with respect to query profiles, 3) ND for assessing novelty of passages with respect to users history, and 4) anti-redundancy component to remove redundancy from ranked lists. 3.1 Adaptive Filtering Component We use a state-of-the-art algorithm in the field - the regularized logistic regression method which had the best results on several benchmark evaluation corpora for AF [21].",
                "Logistic regression (LR) is a supervised learning algorithm for statistical classification.",
                "Based on a training set of labeled instances, it learns a class model which can then by used to predict the labels of unseen instances.",
                "Its performance as well as efficiency in terms of training time makes it a good candidate when frequent updates of the class model are required, as is the case in adaptive filtering, where the system must learn from each new feedback provided by the user. (See [21] and [23] for computational complexity and implementation issues).",
                "In adaptive filtering, each query is considered as a class and the probability of a passage belonging to this class corresponds to the degree of relevance of the passage with respect to the query.",
                "For training the model, we use the query itself as the initial positive training example of the class, and the user-highlighted pieces of text (marked as Relevant or Not-relevant) during feedback as additional training examples.",
                "To address the cold start issue in the early stage before any user feedback is obtained, the system uses a small sample from a retrospective corpus as the initial negative examples in the training set.",
                "The details of using logistic regression for adaptive filtering (assigning different weights to positive and negative training instances, and regularizing the objective function to prevent over-fitting on training data) are presented in [21].",
                "The class model w∗ learned by Logistic Regression, or the query profile, is a vector whose dimensions are individual terms and whose elements are the regression coefficients, indicating how influential each term is in the query profile.",
                "The query profile is updated whenever a new piece of user feedback is received.",
                "A temporally decaying weight can be applied to each training example, as an option, to emphasize the most recent user feedback. 3.2 Passage Retrieval Component We use standard IR techniques in this part of our system.",
                "Incoming documents are processed in chunks, where each chunk can be defined as a fixed span of time or as a fixed number of documents, as preferred by the user.",
                "For each incoming document, corpus statistics like the IDF (Inverted Document Frequency) of each term are updated.",
                "We use a state-of-the-art named entity identifier and tracker [8, 12] to identify person and location names, and merge them with co-referent named entities seen in the past.",
                "Then the documents are segmented into passages, which can be a whole document, a paragraph, a sentence, or any other continuous span of text, as preferred.",
                "Each passage is represented using a vector of TF-IDF (Term FrequencyInverse Document Frequency) weights, where term can be a word or a named entity.",
                "Given a query profile, i.e. the logistic regression solution w∗ as described in Section 3.1, the system computes the posterior probability of relevance for each passage x as fRL(x) ≡ P(y = 1|x, w∗ ) = 1 (1 + e−w∗·x) (1) Passages are ordered by their relevance scores, and the ones with scores above a threshold (tuned on a training set) comprise the relevance list that is passed on to the <br>novelty detection</br> step. 3.3 <br>novelty detection</br> Component CAF´E maintains a user history H(t), which contains all the spans of text hi that the user highlighted (as feedback) during his or her past interactions with the system, up to the current time t. Denoting the history as H(t) = n h1, h2, ..., ht o , (2) the novelty score of a new candidate passage x is computed as: fND(x) = 1 − max i∈1..t {cos(x, hi)} (3) where both candidate passage x and highlighted spans of text hi are represented as TF-IDF vectors.",
                "The novelty score of each passage is compared to a prespecified threshold (also tuned on a training set), and any passage with a score below this threshold is removed from the relevance list. 3.4 Anti-redundant Ranking Component Although the <br>novelty detection</br> component ensures that only novel (previously unseen) information remains in the relevance list, this list might still contain the same novel information at multiple positions in the ranked list.",
                "Suppose, for example, that the user has already read about a $100,000 reward for information about the escaped convicts.",
                "A new piece of news that the award has been increased to $200,000 is novel since the user hasnt read about it yet.",
                "However, multiple news sources would report this news and we might end up showing (redundant) articles from all these sources in a ranked list.",
                "Hence, a ranked list should also be made non-redundant with respect to its own contents.",
                "We use a simplified version of the Maximal Marginal Relevance method [5], originally developed for combining relevance and novelty in text retrieval and summarization.",
                "Our procedure starts with the current list of passages sorted by relevance (section 3.2), filtered by <br>novelty detection</br> component (section 3.3), and generates a new non-redundant list as follows: 1.",
                "Take the top passage in the current list as the top one in the new list. 2.",
                "Add the next passage x in the current list to the new list only if fAR(x) > t where fAR(x) = 1 − max pi∈Lnew {cos(x, pi)} and Lnew is the set of passages already selected in the new list. 3.",
                "Repeat step 2 until all the passages in the current list have been examined.",
                "After applying the above-mentioned algorithm, each passage in the new list is sufficiently dissimilar to others, thus favoring diversity rather than redundancy in the new ranked list.",
                "The anti-redundancy threshold t is tuned on a training set. 4.",
                "EVALUATION METHODOLOGY The approach we proposed above for information distillation raises important issues regarding evaluation methodology.",
                "Firstly, since our framework allows the output to be passages at different levels of granularity (e.g. k-sentence windows where k may vary) instead of a fixed length, it is not possible to have pre-annotated relevance judgments at all such granularity levels.",
                "Secondly, since we wish to measure the utility of the system output as a combination of both relevance and novelty, traditional relevance-only based measures must be replaced by measures that penalize the repetition of the same information in the system output across time.",
                "Thirdly, since the output of the system is ranked lists, we must reward those systems that present useful information (both relevant and previously unseen) using shorter ranked lists, and penalize those that present the same information using longer ranked lists.",
                "None of the existing measures in ad-hoc retrieval, adaptive filtering, <br>novelty detection</br> or other related areas (text summarization and question answering) have desirable properties in all the three aspects.",
                "Therefore, we must develop a new evaluation methodology. 4.1 Answer Keys To enable the evaluation of a system whose output consists of passages of arbitrary length, we borrow the concept of answer keys from the Question Answering (QA) community, where systems are allowed to return arbitrary spans of text as answers.",
                "Answer keys define what should be present in a system response to receive credit, and are comprised of a collection of information nuggets, i.e. factoid units about which human assessors can make binary decisions of whether or not a system response contains them.",
                "Defining answer keys and making the associated binary decisions are conceptual tasks that require semantic mapping [19], since system-returned passages can contain the same information expressed in many different ways.",
                "Hence, QA evaluations have relied on human assessors for the mapping between various expressions, making the process costly, time consuming, and not scalable to large query and document collections, and extensive system evaluations with various parameter settings. 4.1.1 Automating Evaluation based on Answer Keys Automatic evaluation methods would allow for faster system building and tuning, as well as provide an objective and affordable way of comparing various systems.",
                "Recently, such methods have been proposed, more or less, based on the idea of n-gram co-occurrences.",
                "Pourpre [10] assigns a fractional recall score to a system response based on its unigram overlap with a given nuggets description.",
                "For example, a system response A B C has recall 3/4 with respect to a nugget with description A B C D. However, such an approach is unfair to systems that present the same information but using words other than A, B, C, and D. Another open issue is how to weight individual words in measuring the closeness of a match.",
                "For example, consider the question How many prisoners escaped?.",
                "In the nugget Seven prisoners escaped from a Texas prison, there is no indication that seven is the keyword, and that it must be matched to get any relevance credit.",
                "Using IDF values does not help, since seven will generally not have a higher IDF than words like texas and prison.",
                "Also, redefining the nugget as just seven does not solve the problem since now it might spuriously match any mention of seven out of context.",
                "Nuggeteer [13] works on similar principles but makes binary decisions about whether a nugget is present in a given system response by tuning a threshold.",
                "However, it is also plagued by spurious relevance since not all words contained in the nugget description (or known correct responses) are central to the nugget. 4.1.2 Nugget-Matching Rules We propose a reliable automatic method for determining whether a snippet of text contains a given nugget, based on nugget-matching rules, which are generated using a semiautomatic procedure explained below.",
                "These rules are essentially Boolean queries that will only match against snippets that contain the nugget.",
                "For instance, a candidate rule for matching answers to How many prisoners escaped? is (Texas AND seven AND escape AND (convicts OR prisoners)), possibly with other synonyms and variants in the rule.",
                "For a corpus of news articles, which usually follow a typical formal prose, it is fairly easy to write such simple rules to match expected answers using a bootstrap approach, as described below.",
                "We propose a two-stage approach, inspired by Autoslog [14], that combines the strength of humans in identifying semantically equivalent expressions and the strength of the system in gathering statistical evidence from a humanannotated corpus of documents.",
                "In the first stage, human subjects annotated (using a highlighting tool) portions of ontopic documents that contained answers to each nugget 2 .",
                "In the second stage, subjects used our rule generation tool to create rules that would match the annotations for each nugget.",
                "The tool allows users to enter a Boolean rule as a disjunction of conjunctions (e.g. ((a AND b) OR (a AND c AND d) OR (e))).",
                "Given a candidate rule, our tool uses it as a Boolean query over the entire set of on-topic documents and calculates its recall and precision with respect to the annotations that it is expected to match.",
                "Hence, the subjects can start with a simple rule and iteratively refine it until they are satisfied with its recall and precision.",
                "We observed that it was very easy for humans to improve the precision of a rule by tweaking its existing conjunctions (adding more ANDs), and improving the recall by adding more conjunctions to the disjunction (adding more ORs).",
                "As an example, lets try to create a rule for the nugget which says that seven prisoners escaped from the Texas prison.",
                "We start with a simple rule - (seven).",
                "When we input this into the rule generation tool, we realize that this rule matches many spurious occurrences of seven (e.g. ...seven states...) and thus gets a low precision score.",
                "We can further qualify our rule - Texas AND seven AND convicts.",
                "Next, by looking at the missed annotations, we realize that some news articles mentioned ...seven prisoners escaped.... We then replace convicts with the disjunction (convicts OR prisoners).",
                "We continue tweaking the rule in this manner until we achieve a sufficiently high recall and precision - i.e. the (small number of) misses and false alarms can be safely ignored.",
                "Thus we can create nugget-matching rules that succinctly capture various ways of expressing a nugget, while avoiding matching incorrect (or out of context) responses.",
                "Human involvement in the rule creation process ensures high quality generic rules which can then be used to evaluate arbitrary system responses reliably. 4.2 Evaluating the Utility of a Sequence of Ranked Lists The utility of a retrieval system can be defined as the difference between how much the user gained in terms of useful information, and how much the user lost in terms of time and energy.",
                "We calculate this utility from the utilities of individual passages as follows.",
                "After reading each passage returned by the system, the user derives some gain depending on the presence of relevant and novel information, and incurs a loss in terms of the time and energy spent in going through the passage.",
                "However, the likelihood that the user would actually read a passage depends on its position in the ranked list.",
                "Hence, for a query q, the expected utility 2 LDC [18] already provides relevance judgments for 100 topics on the TDT4 corpus.",
                "We further ensured that these judgments are exhaustive on the entire corpus using pooling. of a passage pi at rank i can be defined as U(pi, q) = P(i) ∗ (Gain(pi, q) − Loss(pi, q)) (4) where P(i) is the probability that the user would go through a passage at rank i.",
                "The expected utility for an entire ranked list of length n can be calculated simply by adding the expected utility of each passage: U(q) = nX i=1 P(i) ∗ (Gain(pi, q) − Loss(pi, q)) (5) Note that if we ignore the loss term and define P(i) as P(i) ∝ 1/ logb(b + i − 1) (6) then we get the recently popularized metric called Discounted Cumulated Gain (DCG) [9], where Gain(pi, q) is defined as the graded relevance of passage pi.",
                "However, without the loss term, DCG is a purely recall-oriented metric and not suitable for an adaptive filtering setting, where the systems utility depends in part on its ability to limit the number of items shown to the user.",
                "Although P(i) could be defined based on empirical studies of user behavior, for simplicity, we use P(i) exactly as defined in equation 6.",
                "The gain G(pi, q) of passage pi with respect to the query q is a function of - 1) the number of relevant nuggets present in pi, and 2) the novelty of each of these nuggets.",
                "We combine these two factors as follows.",
                "For each nugget Nj, we assign an initial weight wj, and also keep a count nj of the number of times this nugget has been seen by the user in the past.",
                "The gain derived from each subsequent occurrence of the same nugget is assumed to reduce by a dampening factor γ.",
                "Thus, G(pi, q) is defined as G(pi, q) = X Nj ∈C(pi,q) wj ∗ γnj (7) where C(pi, q) is the set of all nuggets that appear in passage pi and also belong to the answer key of query q.",
                "The initial weights wj are all set of be 1.0 in our experiments, but can also be set based on a pyramid approach [11].",
                "The choice of dampening factor γ determines the users tolerance for redundancy.",
                "When γ = 0, a nugget will only receive credit for its first occurrence i.e. when nj is zero3 .",
                "For 0 < γ < 1, a nugget receives smaller credit for each successive occurrence.",
                "When γ = 1, no dampening occurs and repeated occurrences of a nugget receive the same credit.",
                "Note that the nugget occurrence counts are preserved between evaluation of successive ranked lists returned by the system, since the users are expected to remember what the system showed them in the past.",
                "We define the loss L(pi, q) as a constant cost c (we use 0.1) incurred when reading a system-returned passage.",
                "Thus, our metric can be re-written as U(q) = nX i=1 Gain(pi, q) logb(b + i − 1) − L(n) (8) where L(n) is the loss associated with a ranked list of length n: L(n) = c · nX i=1 1 logb(b + i − 1) (9) 3 Note that 00 = 1 Due to the similarity with Discounted Cumulated Gain (DCG), we call our metric Discounted Cumulated Utility (DCU).",
                "The DCU score obtained by the system is converted to a Normalized DCU (NDCU) score by dividing it by the DCU score of the ideal ranked list, which is created by ordering passages by their decreasing utility scores U(pi, q) and stopping when U(pi, q) ≤ 0 i.e. when the gain is less than or equal to the cost of reading the passage. 5.",
                "DATA TDT4 was the benchmark corpus used in TDT2002 and TDT2003 evaluations.",
                "The corpus consists of over 90, 000 news articles from multiple sources (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) published between October 2000 and January 2001, in the languages of Arabic, English, and Mandarin.",
                "Speech-recognized and machine-translated versions of the non-English articles were provided as well.",
                "LDC [18] has annotated the corpus with 100 topics, that correspond to various news events in this time period.",
                "Out of these, we selected a subset of 12 actionable events, and defined corresponding tasks for them4 .",
                "For each task, we manually defined a profile consisting of an initial set of (5 to 10) queries, a free-text description of the user history, i.e., what the user already knows about the event, and a list of known on-topic and off-topic documents (if available) as training examples.",
                "For each query, we generated answer keys and corresponding nugget matching rules using the procedure described in section 4.1.2, and produced a total of 120 queries, with an average of 7 nuggets per query. 6.",
                "EXPERIMENTS AND RESULTS 6.1 Baselines We used Indri [17], a popular language-model based retrieval engine, as a baseline for comparison with CAF´E.",
                "Indri supports standard search engine functionality, including pseudo-relevance feedback (PRF) [3, 6], and is representative of a typical query-based retrieval system.",
                "Indri does not support any kind of <br>novelty detection</br>.",
                "We compare Indri with PRF turned on and off, against CAF´E with user feedback, <br>novelty detection</br> and antiredundant ranking turned on and off. 6.2 Experimental Setup We divided the TDT4 corpus spanning 4 months into 10 chunks, each defined as a period of 12 consecutive days.",
                "At any given point of time in the distillation process, each system accessed the past data up to the current point, and returned a ranked list of up 50 passages per query.",
                "The 12 tasks defined on the corpus were divided into a training and test set with 6 tasks each.",
                "Each system was allowed to use the training set to tune its parameters for optimizing NDCU (equation 8), including the relevance threshold for both Indri and CAF´E, and the novelty and antiredundancy thresholds for CAF´E.",
                "The NDCU for each system run is calculated automatically.",
                "User feedback was also simulated - relevance judgments for each system-returned passage (as determined by the nugget matching rules described in section 4.1.2) were 4 URL: http://nyc.lti.cs.cmu.edu/downloads Figure 1: Performance of Indri across chunks Figure 2: Performance of CAF´E across chunks used as user feedback in the adaptation of query profiles and user histories. 6.3 Results In Table 1, we show the NDCU scores of the two systems under various settings.",
                "These scores are averaged over the six tasks in the test set, and are calculated with two dampening factors (see section 4.2): γ = 0 and 0.1, to simulate no tolerance and small tolerance for redundancy, respectively.",
                "Using γ = 0 creates a much more strict metric since it does not give any credit to a passage that contains relevant but redundant information.",
                "Hence, the improvement obtained from enabling user feedback is smaller with γ = 0 than the improvement obtained from feedback with γ = 0.1.",
                "This reveals a shortcoming of contemporary retrieval systemswhen the user gives positive feedback on a passage, the systems gives higher weights to the terms present in that passage and tends to retrieve other passages containing the same terms - and thus - usually the same information.",
                "However, the user does not benefit from seeing such redundant passages, and is usually interested in other passages containing related information.",
                "It is informative to evaluate retrieval systems using our utility measure (with γ = 0) which accounts for novelty and thus gives a more realistic picture of how well a system can generalize from user feedback, rather than using traditional IR measures like recall and precision which give an incomplete picture of improvement obtained from user feedback.",
                "Sometimes, however, users might indeed be interested in seeing the same information from multiple sources, as an Table 1: NDCU Scores of Indri and CAF´E for two dampening factors (γ), and various settings (F: Feedback, N: <br>novelty detection</br>, A: Anti-Redundant Ranking) Indri CAF´E γ Base +PRF Base +F +F+N +F+A +F+N+A 0 0.19 0.19 0.22 0.23 0.24 0.24 0.24 0.1 0.28 0.29 0.24 0.35 0.35 0.36 0.36 indicator of its importance or reliability.",
                "In such a case, we can simply choose a higher value for γ which corresponds to a higher tolerance for redundancy, and hence let the system tune its parameters accordingly.",
                "Since documents were processed chunk by chunk, it would be interesting to see how the performance of systems improves over time.",
                "Figures 1 and 2 show the performance trends for both the systems across chunks.",
                "While the performance with and without feedback on the first few chunks is expected to be close, for subsequent chunks, the performance curve with feedback enabled rises above the one with the no-feedback setting.",
                "The performance trends are not consistent across all chunks because on-topic documents are not uniformly distributed over all the chunks, making some queries easier than others in certain chunks.",
                "Moreover, since Indri uses pseudo-relevance feedback while CAF´E uses feedback based on actual relevance judgments, the improvement in case of Indri is less dramatic than that of CAF´E. 7.",
                "CONCLUDING REMARKS This paper presents the first investigation on utility-based information distillation with a system that learns the longlasting information needs from fine-grained user feedback over a sequence of ranked passages.",
                "Our system, called CAF´E, combines adaptive filtering, <br>novelty detection</br> and antiredundant passage ranking in a unified framework for utility optimization.",
                "We developed a new scheme for automated evaluation and feedback based on a semi-automatic procedure for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also proposed an extension of the NDCG metric for assessing the utility of ranked passages as a weighted combination of relevance and novelty.",
                "Our experiments on the newly annotated TDT4 benchmark corpus show encouraging utility enhancement over Indri, and also over our own system with incremental learning and <br>novelty detection</br> turned off. 8.",
                "ACKNOWLEDGMENTS We would like to thank Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng, and the Qualitative Data Analysis Program at the University of Pittsburgh lead by Dr. Stuart Shulman for their help with collecting and processing the extended TDT4 annotations used in our experiments.",
                "This work is supported in parts by the National Science Foundation (NSF) under grant IIS0434035, and the Defense Advanced Research Project Agency (DARPA) under contracts NBCHD030010 and W0550432.",
                "Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors. 9.",
                "ADDITIONAL AUTHORS Jian Zhang (jianzhan@stat.purdue.edu)∗ , Jaime Carbonell (jgc@cs.cmu.edu)† , Peter Brusilovsky (peterb+@pitt.edu)‡ , Daqing He(dah44@pitt.edu)‡ 10.",
                "REFERENCES [1] J. Allan.",
                "Incremental Relevance Feedback for Information Filtering.",
                "Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, pages 270-278, 1996. [2] J. Allan, C. Wade, and A. Bolivar.",
                "Retrieval and <br>novelty detection</br> at the Sentence Level.",
                "Proceedings of the ACM SIGIR conference on research and development in information retrieval, 2003. [3] C. Buckley, G. Salton, and J. Allan.",
                "Automatic Retrieval with Locality Information using SMART.",
                "NIST special publication, (500207):59-72, 1993. [4] J. Callan.",
                "Learning While Filtering Documents.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 224-231, 1998. [5] J. Carbonell and J. Goldstein.",
                "The use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 335-336, 1998. [6] E. Efthimiadis.",
                "Query Expansion.",
                "Annual Review of Information Science and Technology (ARIST), 31:p121-87, 1996. [7] J. Fiscus and G. Duddington.",
                "Topic Detection and Tracking Overview.",
                "Topic Detection and Tracking: Event-based Information Organization, pages 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos.",
                "A Statistical Model for Multilingual Entity Detection and Tracking.",
                "NAACL/HLT, 2004. [9] K. J¨arvelin and J. Kek¨al¨ainen.",
                "Cumulated Gain-based Evaluation of IR Techniques.",
                "ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin and D. Demner-Fushman.",
                "Automatically Evaluating Answers to Definition Questions.",
                "Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), 2005. ∗ Statistics Dept., Purdue University, West Lafayette, USA † Language Technologies Inst., Carnegie Mellon University, Pittsburgh, USA ‡ School of Information Sciences, Univ. of Pittsburgh, Pittsburgh, USA [11] J. Lin and D. Demner-Fushman.",
                "Will Pyramids Built of nUggets Topple Over.",
                "Proceedings of HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos.",
                "A Mention-synchronous Coreference Resolution Algorithm based on the Bell Tree.",
                "Proc. of ACL, 4:136-143, 2004. [13] G. Marton.",
                "Nuggeteer: Automatic Nugget-Based Evaluation Using Descriptions and Judgments.",
                "HLT/NAACL, 2006. [14] E. Riloff.",
                "Automatically Constructing a Dictionary for Information Extraction Tasks.",
                "Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 811-816, 1993. [15] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9: Filtering track.",
                "The Ninth Text REtrieval Conference (TREC-9), pages 361-368. [16] R. Schapire, Y.",
                "Singer, and A. Singhal.",
                "Boosting and Rocchio Applied to Text Filtering.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle, and W. Croft.",
                "Indri: A Language Model-based Search Engine for Complex Queries.",
                "Proceedings of the International Conference on Intelligence Analysis, 2004. [18] The Linguistic Data Consortium. http://www.ldc.upenn.edu/. [19] E. Voorhees.",
                "Overview of the TREC 2003 Question Answering Track.",
                "Proceedings of the Twelfth Text REtrieval Conference (TREC 2003), 2003. [20] Y. Yang and B. Kisiel.",
                "Margin-based Local Regression for Adaptive Filtering.",
                "Proceedings of the twelfth international conference on Information and knowledge management, pages 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of Adaptive Filtering Methods in a Cross-benchmark Evaluation.",
                "Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 98-105, 2005. [22] C. Zhai, W. Cohen, and J. Lafferty.",
                "Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 10-17, 2003. [23] J. Zhang and Y. Yang.",
                "Robustness of Regularized Linear Classification Methods in Text Categorization.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 190-197, 2003. [24] Y. Zhang.",
                "Using Bayesian Priors to Combine Classifiers for Adaptive Filtering.",
                "Proceedings of the 27th annual international conference on Research and development in information retrieval, pages 345-352, 2004. [25] Y. Zhang, J. Callan, and T. Minka.",
                "Novelty and Redundancy Detection in Adaptive Filtering.",
                "Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Combina las fortalezas y se extiende más allá del filtrado adaptativo convencional, la \"detección de novedad\" y la clasificación de pasaje no redundante con respecto a las necesidades de información duradera (tareas con múltiples consultas).",
                "Nuestros resultados muestran mejoras alentadoras de utilidad utilizando el nuevo enfoque, en comparación con los sistemas de referencia sin aprendizaje incremental o los componentes de \"detección de novedad\".",
                "Claramente, las técnicas para la \"detección de novedad\" pueden ayudar en principio [25, 2, 22] para mejorar la utilidad de los sistemas AF.",
                "Para abordar las limitaciones anteriores de los sistemas AF actuales, proponemos y examinamos un nuevo enfoque en este documento, combinando las fortalezas de la FA convencional (aprendizaje incremental de los modelos de temas), recuperación de pasaje de múltiples pasos para consultas duraderas condicionadas sobre el tema, y\"Detección de novedad\" para la eliminación de la redundancia de las interacciones del usuario con el sistema.",
                "Claramente, la \"detección de novedad\" es muy importante para la utilidad de dicho sistema debido a la búsqueda iterativa.",
                "Sin \"detección de novedad\", los viejos pasajes relevantes se mostrarían al usuario repetidamente en cada lista clasificada.",
                "Nuestro marco combina y extiende el poder del filtrado adaptativo (AF), la recuperación ad-hoc (IR) y la \"detección de novedad\" (ND).",
                "En comparación con el trabajo pasado, esta es la primera evaluación de la \"detección de novedad\" integrada con el filtrado adaptativo para consultas secuenciadas que permite la retroalimentación flexible de los usuarios sobre los pasajes clasificados.",
                "Dado un perfil de consulta, es decir, la solución de regresión logística w ∗ como se describe en la Sección 3.1, el sistema calcula la probabilidad posterior de relevancia para cada pasaje x como frl (x) ≡ p (y = 1 | x, w ∗) = 1 (1 + E - W ∗ · x) (1) Los pasajes se ordenan por sus puntajes de relevancia, y los que tienen puntajes por encima de un umbral (sintonizado en un conjunto de entrenamiento) comprenden la lista de relevancia que se pasa a la \"detección de novedad\" paso.3.3 El componente de \"detección de novedades\" mantiene un historial de usuario H (t), que contiene todos los tramos de texto HI que el usuario destacó (como retroalimentación) durante sus interacciones pasadas con el sistema, hasta el tiempo actual t.Denotando la historia como h (t) = n h1, h2, ..., ht o, (2) El puntaje de novedad de un nuevo pasaje candidato X se calcula como: fnd (x) = 1 - max i∈1 ..t {cos (x, hi)} (3) donde ambos pasos candidatos X y los tramos resaltados de texto HI se representan como vectores TF-IDF.",
                "El puntaje de novedad de cada pasaje se compara con un umbral presespecificado (también sintonizado en un conjunto de entrenamiento), y cualquier pasaje con un puntaje por debajo de este umbral se elimina de la lista de relevancia.3.4 Componente de clasificación anti-redundante Aunque el componente de \"detección de novedad\" asegura que solo la información novedosa (previamente invisible) permanezca en la lista de relevancia, esta lista aún podría contener la misma información novedosa en múltiples posiciones en la lista clasificada.",
                "Nuestro procedimiento comienza con la lista actual de pasajes ordenados por relevancia (Sección 3.2), filtrado por el componente de \"detección de novedad\" (Sección 3.3), y genera una nueva lista no redundante de la siguiente manera: 1.",
                "Ninguna de las medidas existentes en recuperación ad-hoc, filtrado adaptativo, \"detección de novedad\" u otras áreas relacionadas (resumen de texto y respuesta de preguntas) tienen propiedades deseables en los tres aspectos.",
                "Indri no admite ningún tipo de \"detección de novedad\".",
                "Comparamos Indri con PRF activado y apagado, contra Caf'e con comentarios de los usuarios, \"detección de novedad\" y clasificación antiredenundante activada y apagada.6.2 Configuración experimental Dividimos el corpus TDT4 que abarca 4 meses en 10 trozos, cada uno definido como un período de 12 días consecutivos.",
                "A veces, sin embargo, los usuarios pueden estar interesados en ver la misma información de múltiples fuentes, como una Tabla 1: puntajes NDCU de Indri y Caf´e para dos factores de amortiguación (γ) y diversos configuraciones (F: retroalimentación, n: \"Detección de novedad \", A: clasificación anti-redundante) Indri Caf´e γ Base +PRF Base +F +F +N +F +A +F +N +A 0 0.19 0.19 0.22 0.23 0.24 0.24 0.24 0.1 0.28 0.29 0.24 0.35 0.350.36 0.36 Indicador de su importancia o confiabilidad.",
                "Nuestro sistema, llamado CAF´E, combina filtrado adaptativo, \"detección de novedad\" y clasificación de pasaje antiredado en un marco unificado para la optimización de servicios públicos.",
                "Nuestros experimentos en el recientemente anotado TDT4 Benchmark Corpus muestran una mejora de la utilidad alentadora sobre Indri, y también sobre nuestro propio sistema con aprendizaje incremental y \"detección de novedad\" desactivadas.8.",
                "Recuperación y \"detección de novedad\" a nivel de oración."
            ],
            "translated_text": "",
            "candidates": [
                "detección de novedad",
                "detección de novedad",
                "detección de novedad",
                "detección de novedad",
                "detección de novedad",
                "detección de novedad",
                "detección de novedad",
                "Detección de novedad",
                "detección de novedad",
                "detección de novedad",
                "detección de novedad",
                "detección de novedad",
                "detección de novedad",
                "detección de novedad",
                "detección de novedad",
                "detección de novedad",
                "detección de novedad",
                "detección de novedad",
                "detección de novedades",
                "detección de novedad",
                "detección de novedad",
                "Detección de novedad",
                "detección de novedad",
                "detección de novedad",
                "detección de novedad",
                "detección de novedad",
                "detección de novedad",
                "detección de novedad",
                "detección de novedad",
                "detección de novedad",
                "Detección de novedad ",
                "detección de novedad",
                "detección de novedad",
                "Detección de novedad",
                "detección de novedad",
                "detección de novedad",
                "detección de novedad"
            ],
            "error": []
        },
        "new evaluation methodology": {
            "translated_key": "Nueva metodología de evaluación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Utility-based Information Distillation Over Temporally Sequenced Documents Yiming Yang Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA yiming@cs.cmu.edu Abhimanyu Lad Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA alad@cs.cmu.edu Ni Lao Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA nlao@cs.cmu.edu Abhay Harpale Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA aharpale@cs.cmu.edu Bryan Kisiel Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA bkisiel@cs.cmu.edu Monica Rogati Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA mrogati@cs.cmu.edu ABSTRACT This paper examines a new approach to information distillation over temporally ordered documents, and proposes a novel evaluation scheme for such a framework.",
                "It combines the strengths of and extends beyond conventional adaptive filtering, novelty detection and non-redundant passage ranking with respect to long-lasting information needs (tasks with multiple queries).",
                "Our approach supports fine-grained user feedback via highlighting of arbitrary spans of text, and leverages such information for utility optimization in adaptive settings.",
                "For our experiments, we defined hypothetical tasks based on news events in the TDT4 corpus, with multiple queries per task.",
                "Answer keys (nuggets) were generated for each query and a semiautomatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also propose an extension of the NDCG metric for assessing the utility of ranked passages as a combination of relevance and novelty.",
                "Our results show encouraging utility enhancements using the new approach, compared to the baseline systems without incremental learning or the novelty detection components.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 General Terms Design, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Tracking new and relevant information from temporal data streams for users with long-lasting needs has been a challenging research topic in information retrieval.",
                "Adaptive filtering (AF) is one such task of online prediction of the relevance of each new document with respect to pre-defined topics.",
                "Based on the initial query and a few positive examples (if available), an AF system maintains a profile for each such topic of interest, and constantly updates it based on feedback from the user.",
                "The incremental learning nature of AF systems makes them more powerful than standard search engines that support ad-hoc retrieval (e.g.",
                "Google and Yahoo) in terms of finding relevant information with respect to long-lasting topics of interest, and more attractive for users who are willing to provide feedback to adapt the system towards their specific information needs, without having to modify their queries manually.",
                "A variety of supervised learning algorithms (Rocchio-style classifiers, Exponential-Gaussian models, local regression and logistic regression approaches) have been studied for adaptive settings, examined with explicit and implicit relevance feedback, and evaluated with respect to utility optimization on large benchmark data collections in TREC (Text Retrieval Conferences) and TDT (Topic Detection and Tracking) forums [1, 4, 7, 15, 16, 20, 24, 23].",
                "Regularized logistic regression [21] has been found representative for the state-of-the-art approaches, and highly efficient for frequent model adaptations over large document collections such as the TREC-10 corpus (over 800,000 documents and 84 topics).",
                "Despite substantial achievements in recent adaptive filtering research, significant problems remain unsolved regarding how to leverage user feedback effectively and efficiently.",
                "Specifically, the following issues may seriously limit the true utility of AF systems in real-world applications: 1.",
                "User has a rather passive role in the conventional adaptive filtering setup - he or she reacts to the system only when the system makes a yes decision on a document, by confirming or rejecting that decision.",
                "A more active alternative would be to allow the user to issue multiple queries for a topic, review a ranked list of candidate documents (or passages) per query, and provide feedback on the ranked list, thus refining their information need and requesting updated ranked lists.",
                "The latter form of user interaction has been highly effective in standard retrieval for ad-hoc queries.",
                "How to deploy such a strategy for long-lasting information needs in AF settings is an open question for research. 2.",
                "The unit for receiving a relevance judgment (yes or no) is restricted to the document level in conventional AF.",
                "However, a real user may be willing to provide more informative, fine-grained feedback via highlighting some pieces of text in a retrieved document as relevant, instead of labeling the entire document as relevant.",
                "Effectively leveraging such fine-grained feedback could substantially enhance the quality of an AF system.",
                "For this, we need to enable supervised learning from labeled pieces of text of arbitrary span instead of just allowing labeled documents. 3.",
                "System-selected documents are often highly redundant.",
                "A major news event, for example, would be reported by multiple sources repeatedly for a while, making most of the information content in those articles redundant with each other.",
                "A conventional AF system would select all these redundant news stories for user feedback, wasting the users time while offering little gain.",
                "Clearly, techniques for novelty detection can help in principle [25, 2, 22] for improving the utility of the AF systems.",
                "However, the effectiveness of such techniques at passage level to detect novelty with respect to users (fine-grained) feedback and to detect redundancy in ranked lists remains to be evaluated using a measure of utility that mimics the needs of a real user.",
                "To address the above limitations of current AF systems, we propose and examine a new approach in this paper, combining the strengths of conventional AF (incremental learning of topic models), multi-pass passage retrieval for long-lasting queries conditioned on topic, and novelty detection for removal of redundancy from user interactions with the system.",
                "We call the new process utility-based information distillation.",
                "Note that conventional benchmark corpora for AF evaluations, which have relevance judgments at the document level and do not define tasks with multiple queries, are insufficient for evaluating the new approach.",
                "Therefore, we extended a benchmark corpus - the TDT4 collection of news stories and TV broadcasts - with task definitions, multiple queries per task, and answer keys per query.",
                "We have conducted our experiments on this extended TDT4 corpus and have made the additionally generated data publicly available for future comparative evaluations 1 .",
                "To automatically evaluate the system-returned arbitrary spans of text using our answer keys, we further developed an evaluation scheme with semi-automatic procedure for 1 URL: http://nyc.lti.cs.cmu.edu/downloads acquiring rules that can match nuggets against system responses.",
                "Moreover, we propose an extension of NDCG (Normalized Discounted Cumulated Gain) [9] for assessing the utility of ranked passages as a function of both relevance and novelty.",
                "The rest of this paper is organized as follows.",
                "Section 2 outlines the information distillation process with a concrete example.",
                "Section 3 describes the technical cores of our system called CAF´E - CMU Adaptive Filtering Engine.",
                "Section 4 discusses issues with respect to evaluation methodology and proposes a new scheme.",
                "Section 5 describes the extended TDT4 corpus.",
                "Section 6 presents our experiments and results.",
                "Section 7 concludes the study and gives future perspectives. 2.",
                "A SAMPLE TASK Consider a news event - the escape of seven convicts from a Texas prison in December 2000 and their capture a month later.",
                "Assuming a user were interested in this event since its early stage, the information need could be: Find information about the escape of convicts from Texas prison, and information related to their recapture.",
                "The associated lower-level questions could be: 1.",
                "How many prisoners escaped? 2.",
                "Where and when were they sighted? 3. Who are their known contacts inside and outside the prison? 4.",
                "How are they armed? 5.",
                "Do they have any vehicles? 6.",
                "What steps have been taken so far?",
                "We call such an information need a task, and the associated questions as the queries in this task.",
                "A distillation system is supposed to monitor the incoming documents, process them chunk by chunk in a temporal order, select potentially relevant and novel passages from each chunk with respect to each query, and present a ranked list of passages to the user.",
                "Passage ranking here is based on how relevant a passage is with respect to the current query, how novel it is with respect to the current user history (of his or her interactions with the system), and how redundant it is compared to other passages with a higher rank in the list.",
                "When presented with a list of passages, the user may provide feedback by highlighting arbitrary spans of text that he or she found relevant.",
                "These spans of text are taken as positive examples in the adaptation of the query profile, and also added to the users history.",
                "Passages not marked by the user are taken as negative examples.",
                "As soon as the query profile is updated, the system re-issues a search and returns another ranked list of passages where the previously seen passages are either removed or ranked low, based on user preference.",
                "For example, if the user highlights ...officials have posted a $100,000 reward for their capture... as relevant answer to the query What steps have been taken so far?, then the highlighted piece is used as an additional positive training example in the adaptation of the query profile.",
                "This piece of feedback is also added to the user history as a seen example, so that in future, the system will not place another passage mentioning $100,000 reward at the top of the ranked list.",
                "However, an article mentioning ...officials have doubled the reward money to $200,000... might be ranked high since it is both relevant to the (updated) query profile and novel with respect to the (updated) user history.",
                "The user may modify the original queries or add a new query during the process; the query profiles will be changed accordingly.",
                "Clearly, novelty detection is very important for the utility of such a system because of the iterative search.",
                "Without novelty detection, the old relevant passages would be shown to the user repeatedly in each ranked list.",
                "Through the above example, we can see the main properties of our new framework for utility-based information distillation over temporally ordered documents.",
                "Our framework combines and extends the power of adaptive filtering (AF), ad-hoc retrieval (IR) and novelty detection (ND).",
                "Compared to standard IR, our approach has the power of incrementally learning long-term information needs and modeling a sequence of queries within a task.",
                "Compared to conventional AF, it enables a more active role of the user in refining his or her information needs and requesting new results by allowing relevance and novelty feedback via highlighting of arbitrary spans of text in passages returned by the system.",
                "Compared to past work, this is the first evaluation of novelty detection integrated with adaptive filtering for sequenced queries that allows flexible user feedback over ranked passages.",
                "The combination of AF, IR and ND with the new extensions raises an important research question regarding evaluation methodology: how can we measure the utility of such an information distillation system?",
                "Existing metrics in standard IR, AF and ND are insufficient, and new solutions must be explored, as we will discuss in Section 4, after describing the technical cores of our system in the next section. 3.",
                "TECHNICAL CORES The core components of CAF´E are - 1) AF for incremental learning of query profiles, 2) IR for estimating relevance of passages with respect to query profiles, 3) ND for assessing novelty of passages with respect to users history, and 4) anti-redundancy component to remove redundancy from ranked lists. 3.1 Adaptive Filtering Component We use a state-of-the-art algorithm in the field - the regularized logistic regression method which had the best results on several benchmark evaluation corpora for AF [21].",
                "Logistic regression (LR) is a supervised learning algorithm for statistical classification.",
                "Based on a training set of labeled instances, it learns a class model which can then by used to predict the labels of unseen instances.",
                "Its performance as well as efficiency in terms of training time makes it a good candidate when frequent updates of the class model are required, as is the case in adaptive filtering, where the system must learn from each new feedback provided by the user. (See [21] and [23] for computational complexity and implementation issues).",
                "In adaptive filtering, each query is considered as a class and the probability of a passage belonging to this class corresponds to the degree of relevance of the passage with respect to the query.",
                "For training the model, we use the query itself as the initial positive training example of the class, and the user-highlighted pieces of text (marked as Relevant or Not-relevant) during feedback as additional training examples.",
                "To address the cold start issue in the early stage before any user feedback is obtained, the system uses a small sample from a retrospective corpus as the initial negative examples in the training set.",
                "The details of using logistic regression for adaptive filtering (assigning different weights to positive and negative training instances, and regularizing the objective function to prevent over-fitting on training data) are presented in [21].",
                "The class model w∗ learned by Logistic Regression, or the query profile, is a vector whose dimensions are individual terms and whose elements are the regression coefficients, indicating how influential each term is in the query profile.",
                "The query profile is updated whenever a new piece of user feedback is received.",
                "A temporally decaying weight can be applied to each training example, as an option, to emphasize the most recent user feedback. 3.2 Passage Retrieval Component We use standard IR techniques in this part of our system.",
                "Incoming documents are processed in chunks, where each chunk can be defined as a fixed span of time or as a fixed number of documents, as preferred by the user.",
                "For each incoming document, corpus statistics like the IDF (Inverted Document Frequency) of each term are updated.",
                "We use a state-of-the-art named entity identifier and tracker [8, 12] to identify person and location names, and merge them with co-referent named entities seen in the past.",
                "Then the documents are segmented into passages, which can be a whole document, a paragraph, a sentence, or any other continuous span of text, as preferred.",
                "Each passage is represented using a vector of TF-IDF (Term FrequencyInverse Document Frequency) weights, where term can be a word or a named entity.",
                "Given a query profile, i.e. the logistic regression solution w∗ as described in Section 3.1, the system computes the posterior probability of relevance for each passage x as fRL(x) ≡ P(y = 1|x, w∗ ) = 1 (1 + e−w∗·x) (1) Passages are ordered by their relevance scores, and the ones with scores above a threshold (tuned on a training set) comprise the relevance list that is passed on to the novelty detection step. 3.3 Novelty Detection Component CAF´E maintains a user history H(t), which contains all the spans of text hi that the user highlighted (as feedback) during his or her past interactions with the system, up to the current time t. Denoting the history as H(t) = n h1, h2, ..., ht o , (2) the novelty score of a new candidate passage x is computed as: fND(x) = 1 − max i∈1..t {cos(x, hi)} (3) where both candidate passage x and highlighted spans of text hi are represented as TF-IDF vectors.",
                "The novelty score of each passage is compared to a prespecified threshold (also tuned on a training set), and any passage with a score below this threshold is removed from the relevance list. 3.4 Anti-redundant Ranking Component Although the novelty detection component ensures that only novel (previously unseen) information remains in the relevance list, this list might still contain the same novel information at multiple positions in the ranked list.",
                "Suppose, for example, that the user has already read about a $100,000 reward for information about the escaped convicts.",
                "A new piece of news that the award has been increased to $200,000 is novel since the user hasnt read about it yet.",
                "However, multiple news sources would report this news and we might end up showing (redundant) articles from all these sources in a ranked list.",
                "Hence, a ranked list should also be made non-redundant with respect to its own contents.",
                "We use a simplified version of the Maximal Marginal Relevance method [5], originally developed for combining relevance and novelty in text retrieval and summarization.",
                "Our procedure starts with the current list of passages sorted by relevance (section 3.2), filtered by Novelty Detection component (section 3.3), and generates a new non-redundant list as follows: 1.",
                "Take the top passage in the current list as the top one in the new list. 2.",
                "Add the next passage x in the current list to the new list only if fAR(x) > t where fAR(x) = 1 − max pi∈Lnew {cos(x, pi)} and Lnew is the set of passages already selected in the new list. 3.",
                "Repeat step 2 until all the passages in the current list have been examined.",
                "After applying the above-mentioned algorithm, each passage in the new list is sufficiently dissimilar to others, thus favoring diversity rather than redundancy in the new ranked list.",
                "The anti-redundancy threshold t is tuned on a training set. 4.",
                "EVALUATION METHODOLOGY The approach we proposed above for information distillation raises important issues regarding evaluation methodology.",
                "Firstly, since our framework allows the output to be passages at different levels of granularity (e.g. k-sentence windows where k may vary) instead of a fixed length, it is not possible to have pre-annotated relevance judgments at all such granularity levels.",
                "Secondly, since we wish to measure the utility of the system output as a combination of both relevance and novelty, traditional relevance-only based measures must be replaced by measures that penalize the repetition of the same information in the system output across time.",
                "Thirdly, since the output of the system is ranked lists, we must reward those systems that present useful information (both relevant and previously unseen) using shorter ranked lists, and penalize those that present the same information using longer ranked lists.",
                "None of the existing measures in ad-hoc retrieval, adaptive filtering, novelty detection or other related areas (text summarization and question answering) have desirable properties in all the three aspects.",
                "Therefore, we must develop a <br>new evaluation methodology</br>. 4.1 Answer Keys To enable the evaluation of a system whose output consists of passages of arbitrary length, we borrow the concept of answer keys from the Question Answering (QA) community, where systems are allowed to return arbitrary spans of text as answers.",
                "Answer keys define what should be present in a system response to receive credit, and are comprised of a collection of information nuggets, i.e. factoid units about which human assessors can make binary decisions of whether or not a system response contains them.",
                "Defining answer keys and making the associated binary decisions are conceptual tasks that require semantic mapping [19], since system-returned passages can contain the same information expressed in many different ways.",
                "Hence, QA evaluations have relied on human assessors for the mapping between various expressions, making the process costly, time consuming, and not scalable to large query and document collections, and extensive system evaluations with various parameter settings. 4.1.1 Automating Evaluation based on Answer Keys Automatic evaluation methods would allow for faster system building and tuning, as well as provide an objective and affordable way of comparing various systems.",
                "Recently, such methods have been proposed, more or less, based on the idea of n-gram co-occurrences.",
                "Pourpre [10] assigns a fractional recall score to a system response based on its unigram overlap with a given nuggets description.",
                "For example, a system response A B C has recall 3/4 with respect to a nugget with description A B C D. However, such an approach is unfair to systems that present the same information but using words other than A, B, C, and D. Another open issue is how to weight individual words in measuring the closeness of a match.",
                "For example, consider the question How many prisoners escaped?.",
                "In the nugget Seven prisoners escaped from a Texas prison, there is no indication that seven is the keyword, and that it must be matched to get any relevance credit.",
                "Using IDF values does not help, since seven will generally not have a higher IDF than words like texas and prison.",
                "Also, redefining the nugget as just seven does not solve the problem since now it might spuriously match any mention of seven out of context.",
                "Nuggeteer [13] works on similar principles but makes binary decisions about whether a nugget is present in a given system response by tuning a threshold.",
                "However, it is also plagued by spurious relevance since not all words contained in the nugget description (or known correct responses) are central to the nugget. 4.1.2 Nugget-Matching Rules We propose a reliable automatic method for determining whether a snippet of text contains a given nugget, based on nugget-matching rules, which are generated using a semiautomatic procedure explained below.",
                "These rules are essentially Boolean queries that will only match against snippets that contain the nugget.",
                "For instance, a candidate rule for matching answers to How many prisoners escaped? is (Texas AND seven AND escape AND (convicts OR prisoners)), possibly with other synonyms and variants in the rule.",
                "For a corpus of news articles, which usually follow a typical formal prose, it is fairly easy to write such simple rules to match expected answers using a bootstrap approach, as described below.",
                "We propose a two-stage approach, inspired by Autoslog [14], that combines the strength of humans in identifying semantically equivalent expressions and the strength of the system in gathering statistical evidence from a humanannotated corpus of documents.",
                "In the first stage, human subjects annotated (using a highlighting tool) portions of ontopic documents that contained answers to each nugget 2 .",
                "In the second stage, subjects used our rule generation tool to create rules that would match the annotations for each nugget.",
                "The tool allows users to enter a Boolean rule as a disjunction of conjunctions (e.g. ((a AND b) OR (a AND c AND d) OR (e))).",
                "Given a candidate rule, our tool uses it as a Boolean query over the entire set of on-topic documents and calculates its recall and precision with respect to the annotations that it is expected to match.",
                "Hence, the subjects can start with a simple rule and iteratively refine it until they are satisfied with its recall and precision.",
                "We observed that it was very easy for humans to improve the precision of a rule by tweaking its existing conjunctions (adding more ANDs), and improving the recall by adding more conjunctions to the disjunction (adding more ORs).",
                "As an example, lets try to create a rule for the nugget which says that seven prisoners escaped from the Texas prison.",
                "We start with a simple rule - (seven).",
                "When we input this into the rule generation tool, we realize that this rule matches many spurious occurrences of seven (e.g. ...seven states...) and thus gets a low precision score.",
                "We can further qualify our rule - Texas AND seven AND convicts.",
                "Next, by looking at the missed annotations, we realize that some news articles mentioned ...seven prisoners escaped.... We then replace convicts with the disjunction (convicts OR prisoners).",
                "We continue tweaking the rule in this manner until we achieve a sufficiently high recall and precision - i.e. the (small number of) misses and false alarms can be safely ignored.",
                "Thus we can create nugget-matching rules that succinctly capture various ways of expressing a nugget, while avoiding matching incorrect (or out of context) responses.",
                "Human involvement in the rule creation process ensures high quality generic rules which can then be used to evaluate arbitrary system responses reliably. 4.2 Evaluating the Utility of a Sequence of Ranked Lists The utility of a retrieval system can be defined as the difference between how much the user gained in terms of useful information, and how much the user lost in terms of time and energy.",
                "We calculate this utility from the utilities of individual passages as follows.",
                "After reading each passage returned by the system, the user derives some gain depending on the presence of relevant and novel information, and incurs a loss in terms of the time and energy spent in going through the passage.",
                "However, the likelihood that the user would actually read a passage depends on its position in the ranked list.",
                "Hence, for a query q, the expected utility 2 LDC [18] already provides relevance judgments for 100 topics on the TDT4 corpus.",
                "We further ensured that these judgments are exhaustive on the entire corpus using pooling. of a passage pi at rank i can be defined as U(pi, q) = P(i) ∗ (Gain(pi, q) − Loss(pi, q)) (4) where P(i) is the probability that the user would go through a passage at rank i.",
                "The expected utility for an entire ranked list of length n can be calculated simply by adding the expected utility of each passage: U(q) = nX i=1 P(i) ∗ (Gain(pi, q) − Loss(pi, q)) (5) Note that if we ignore the loss term and define P(i) as P(i) ∝ 1/ logb(b + i − 1) (6) then we get the recently popularized metric called Discounted Cumulated Gain (DCG) [9], where Gain(pi, q) is defined as the graded relevance of passage pi.",
                "However, without the loss term, DCG is a purely recall-oriented metric and not suitable for an adaptive filtering setting, where the systems utility depends in part on its ability to limit the number of items shown to the user.",
                "Although P(i) could be defined based on empirical studies of user behavior, for simplicity, we use P(i) exactly as defined in equation 6.",
                "The gain G(pi, q) of passage pi with respect to the query q is a function of - 1) the number of relevant nuggets present in pi, and 2) the novelty of each of these nuggets.",
                "We combine these two factors as follows.",
                "For each nugget Nj, we assign an initial weight wj, and also keep a count nj of the number of times this nugget has been seen by the user in the past.",
                "The gain derived from each subsequent occurrence of the same nugget is assumed to reduce by a dampening factor γ.",
                "Thus, G(pi, q) is defined as G(pi, q) = X Nj ∈C(pi,q) wj ∗ γnj (7) where C(pi, q) is the set of all nuggets that appear in passage pi and also belong to the answer key of query q.",
                "The initial weights wj are all set of be 1.0 in our experiments, but can also be set based on a pyramid approach [11].",
                "The choice of dampening factor γ determines the users tolerance for redundancy.",
                "When γ = 0, a nugget will only receive credit for its first occurrence i.e. when nj is zero3 .",
                "For 0 < γ < 1, a nugget receives smaller credit for each successive occurrence.",
                "When γ = 1, no dampening occurs and repeated occurrences of a nugget receive the same credit.",
                "Note that the nugget occurrence counts are preserved between evaluation of successive ranked lists returned by the system, since the users are expected to remember what the system showed them in the past.",
                "We define the loss L(pi, q) as a constant cost c (we use 0.1) incurred when reading a system-returned passage.",
                "Thus, our metric can be re-written as U(q) = nX i=1 Gain(pi, q) logb(b + i − 1) − L(n) (8) where L(n) is the loss associated with a ranked list of length n: L(n) = c · nX i=1 1 logb(b + i − 1) (9) 3 Note that 00 = 1 Due to the similarity with Discounted Cumulated Gain (DCG), we call our metric Discounted Cumulated Utility (DCU).",
                "The DCU score obtained by the system is converted to a Normalized DCU (NDCU) score by dividing it by the DCU score of the ideal ranked list, which is created by ordering passages by their decreasing utility scores U(pi, q) and stopping when U(pi, q) ≤ 0 i.e. when the gain is less than or equal to the cost of reading the passage. 5.",
                "DATA TDT4 was the benchmark corpus used in TDT2002 and TDT2003 evaluations.",
                "The corpus consists of over 90, 000 news articles from multiple sources (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) published between October 2000 and January 2001, in the languages of Arabic, English, and Mandarin.",
                "Speech-recognized and machine-translated versions of the non-English articles were provided as well.",
                "LDC [18] has annotated the corpus with 100 topics, that correspond to various news events in this time period.",
                "Out of these, we selected a subset of 12 actionable events, and defined corresponding tasks for them4 .",
                "For each task, we manually defined a profile consisting of an initial set of (5 to 10) queries, a free-text description of the user history, i.e., what the user already knows about the event, and a list of known on-topic and off-topic documents (if available) as training examples.",
                "For each query, we generated answer keys and corresponding nugget matching rules using the procedure described in section 4.1.2, and produced a total of 120 queries, with an average of 7 nuggets per query. 6.",
                "EXPERIMENTS AND RESULTS 6.1 Baselines We used Indri [17], a popular language-model based retrieval engine, as a baseline for comparison with CAF´E.",
                "Indri supports standard search engine functionality, including pseudo-relevance feedback (PRF) [3, 6], and is representative of a typical query-based retrieval system.",
                "Indri does not support any kind of novelty detection.",
                "We compare Indri with PRF turned on and off, against CAF´E with user feedback, novelty detection and antiredundant ranking turned on and off. 6.2 Experimental Setup We divided the TDT4 corpus spanning 4 months into 10 chunks, each defined as a period of 12 consecutive days.",
                "At any given point of time in the distillation process, each system accessed the past data up to the current point, and returned a ranked list of up 50 passages per query.",
                "The 12 tasks defined on the corpus were divided into a training and test set with 6 tasks each.",
                "Each system was allowed to use the training set to tune its parameters for optimizing NDCU (equation 8), including the relevance threshold for both Indri and CAF´E, and the novelty and antiredundancy thresholds for CAF´E.",
                "The NDCU for each system run is calculated automatically.",
                "User feedback was also simulated - relevance judgments for each system-returned passage (as determined by the nugget matching rules described in section 4.1.2) were 4 URL: http://nyc.lti.cs.cmu.edu/downloads Figure 1: Performance of Indri across chunks Figure 2: Performance of CAF´E across chunks used as user feedback in the adaptation of query profiles and user histories. 6.3 Results In Table 1, we show the NDCU scores of the two systems under various settings.",
                "These scores are averaged over the six tasks in the test set, and are calculated with two dampening factors (see section 4.2): γ = 0 and 0.1, to simulate no tolerance and small tolerance for redundancy, respectively.",
                "Using γ = 0 creates a much more strict metric since it does not give any credit to a passage that contains relevant but redundant information.",
                "Hence, the improvement obtained from enabling user feedback is smaller with γ = 0 than the improvement obtained from feedback with γ = 0.1.",
                "This reveals a shortcoming of contemporary retrieval systemswhen the user gives positive feedback on a passage, the systems gives higher weights to the terms present in that passage and tends to retrieve other passages containing the same terms - and thus - usually the same information.",
                "However, the user does not benefit from seeing such redundant passages, and is usually interested in other passages containing related information.",
                "It is informative to evaluate retrieval systems using our utility measure (with γ = 0) which accounts for novelty and thus gives a more realistic picture of how well a system can generalize from user feedback, rather than using traditional IR measures like recall and precision which give an incomplete picture of improvement obtained from user feedback.",
                "Sometimes, however, users might indeed be interested in seeing the same information from multiple sources, as an Table 1: NDCU Scores of Indri and CAF´E for two dampening factors (γ), and various settings (F: Feedback, N: Novelty Detection, A: Anti-Redundant Ranking) Indri CAF´E γ Base +PRF Base +F +F+N +F+A +F+N+A 0 0.19 0.19 0.22 0.23 0.24 0.24 0.24 0.1 0.28 0.29 0.24 0.35 0.35 0.36 0.36 indicator of its importance or reliability.",
                "In such a case, we can simply choose a higher value for γ which corresponds to a higher tolerance for redundancy, and hence let the system tune its parameters accordingly.",
                "Since documents were processed chunk by chunk, it would be interesting to see how the performance of systems improves over time.",
                "Figures 1 and 2 show the performance trends for both the systems across chunks.",
                "While the performance with and without feedback on the first few chunks is expected to be close, for subsequent chunks, the performance curve with feedback enabled rises above the one with the no-feedback setting.",
                "The performance trends are not consistent across all chunks because on-topic documents are not uniformly distributed over all the chunks, making some queries easier than others in certain chunks.",
                "Moreover, since Indri uses pseudo-relevance feedback while CAF´E uses feedback based on actual relevance judgments, the improvement in case of Indri is less dramatic than that of CAF´E. 7.",
                "CONCLUDING REMARKS This paper presents the first investigation on utility-based information distillation with a system that learns the longlasting information needs from fine-grained user feedback over a sequence of ranked passages.",
                "Our system, called CAF´E, combines adaptive filtering, novelty detection and antiredundant passage ranking in a unified framework for utility optimization.",
                "We developed a new scheme for automated evaluation and feedback based on a semi-automatic procedure for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also proposed an extension of the NDCG metric for assessing the utility of ranked passages as a weighted combination of relevance and novelty.",
                "Our experiments on the newly annotated TDT4 benchmark corpus show encouraging utility enhancement over Indri, and also over our own system with incremental learning and novelty detection turned off. 8.",
                "ACKNOWLEDGMENTS We would like to thank Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng, and the Qualitative Data Analysis Program at the University of Pittsburgh lead by Dr. Stuart Shulman for their help with collecting and processing the extended TDT4 annotations used in our experiments.",
                "This work is supported in parts by the National Science Foundation (NSF) under grant IIS0434035, and the Defense Advanced Research Project Agency (DARPA) under contracts NBCHD030010 and W0550432.",
                "Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors. 9.",
                "ADDITIONAL AUTHORS Jian Zhang (jianzhan@stat.purdue.edu)∗ , Jaime Carbonell (jgc@cs.cmu.edu)† , Peter Brusilovsky (peterb+@pitt.edu)‡ , Daqing He(dah44@pitt.edu)‡ 10.",
                "REFERENCES [1] J. Allan.",
                "Incremental Relevance Feedback for Information Filtering.",
                "Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, pages 270-278, 1996. [2] J. Allan, C. Wade, and A. Bolivar.",
                "Retrieval and Novelty Detection at the Sentence Level.",
                "Proceedings of the ACM SIGIR conference on research and development in information retrieval, 2003. [3] C. Buckley, G. Salton, and J. Allan.",
                "Automatic Retrieval with Locality Information using SMART.",
                "NIST special publication, (500207):59-72, 1993. [4] J. Callan.",
                "Learning While Filtering Documents.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 224-231, 1998. [5] J. Carbonell and J. Goldstein.",
                "The use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 335-336, 1998. [6] E. Efthimiadis.",
                "Query Expansion.",
                "Annual Review of Information Science and Technology (ARIST), 31:p121-87, 1996. [7] J. Fiscus and G. Duddington.",
                "Topic Detection and Tracking Overview.",
                "Topic Detection and Tracking: Event-based Information Organization, pages 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos.",
                "A Statistical Model for Multilingual Entity Detection and Tracking.",
                "NAACL/HLT, 2004. [9] K. J¨arvelin and J. Kek¨al¨ainen.",
                "Cumulated Gain-based Evaluation of IR Techniques.",
                "ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin and D. Demner-Fushman.",
                "Automatically Evaluating Answers to Definition Questions.",
                "Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), 2005. ∗ Statistics Dept., Purdue University, West Lafayette, USA † Language Technologies Inst., Carnegie Mellon University, Pittsburgh, USA ‡ School of Information Sciences, Univ. of Pittsburgh, Pittsburgh, USA [11] J. Lin and D. Demner-Fushman.",
                "Will Pyramids Built of nUggets Topple Over.",
                "Proceedings of HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos.",
                "A Mention-synchronous Coreference Resolution Algorithm based on the Bell Tree.",
                "Proc. of ACL, 4:136-143, 2004. [13] G. Marton.",
                "Nuggeteer: Automatic Nugget-Based Evaluation Using Descriptions and Judgments.",
                "HLT/NAACL, 2006. [14] E. Riloff.",
                "Automatically Constructing a Dictionary for Information Extraction Tasks.",
                "Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 811-816, 1993. [15] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9: Filtering track.",
                "The Ninth Text REtrieval Conference (TREC-9), pages 361-368. [16] R. Schapire, Y.",
                "Singer, and A. Singhal.",
                "Boosting and Rocchio Applied to Text Filtering.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle, and W. Croft.",
                "Indri: A Language Model-based Search Engine for Complex Queries.",
                "Proceedings of the International Conference on Intelligence Analysis, 2004. [18] The Linguistic Data Consortium. http://www.ldc.upenn.edu/. [19] E. Voorhees.",
                "Overview of the TREC 2003 Question Answering Track.",
                "Proceedings of the Twelfth Text REtrieval Conference (TREC 2003), 2003. [20] Y. Yang and B. Kisiel.",
                "Margin-based Local Regression for Adaptive Filtering.",
                "Proceedings of the twelfth international conference on Information and knowledge management, pages 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of Adaptive Filtering Methods in a Cross-benchmark Evaluation.",
                "Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 98-105, 2005. [22] C. Zhai, W. Cohen, and J. Lafferty.",
                "Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 10-17, 2003. [23] J. Zhang and Y. Yang.",
                "Robustness of Regularized Linear Classification Methods in Text Categorization.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 190-197, 2003. [24] Y. Zhang.",
                "Using Bayesian Priors to Combine Classifiers for Adaptive Filtering.",
                "Proceedings of the 27th annual international conference on Research and development in information retrieval, pages 345-352, 2004. [25] Y. Zhang, J. Callan, and T. Minka.",
                "Novelty and Redundancy Detection in Adaptive Filtering.",
                "Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Por lo tanto, debemos desarrollar una \"nueva metodología de evaluación\".4.1 Claves de respuesta Para habilitar la evaluación de un sistema cuya salida consiste en pasajes de longitud arbitraria, tomamos prestado el concepto de claves de respuesta de la comunidad de respuesta (QA), donde los sistemas pueden devolver los tramos arbitrarios de texto como respuestas."
            ],
            "translated_text": "",
            "candidates": [
                "Nueva metodología de evaluación",
                "nueva metodología de evaluación"
            ],
            "error": []
        },
        "answer key": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Utility-based Information Distillation Over Temporally Sequenced Documents Yiming Yang Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA yiming@cs.cmu.edu Abhimanyu Lad Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA alad@cs.cmu.edu Ni Lao Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA nlao@cs.cmu.edu Abhay Harpale Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA aharpale@cs.cmu.edu Bryan Kisiel Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA bkisiel@cs.cmu.edu Monica Rogati Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA mrogati@cs.cmu.edu ABSTRACT This paper examines a new approach to information distillation over temporally ordered documents, and proposes a novel evaluation scheme for such a framework.",
                "It combines the strengths of and extends beyond conventional adaptive filtering, novelty detection and non-redundant passage ranking with respect to long-lasting information needs (tasks with multiple queries).",
                "Our approach supports fine-grained user feedback via highlighting of arbitrary spans of text, and leverages such information for utility optimization in adaptive settings.",
                "For our experiments, we defined hypothetical tasks based on news events in the TDT4 corpus, with multiple queries per task.",
                "Answer keys (nuggets) were generated for each query and a semiautomatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also propose an extension of the NDCG metric for assessing the utility of ranked passages as a combination of relevance and novelty.",
                "Our results show encouraging utility enhancements using the new approach, compared to the baseline systems without incremental learning or the novelty detection components.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 General Terms Design, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Tracking new and relevant information from temporal data streams for users with long-lasting needs has been a challenging research topic in information retrieval.",
                "Adaptive filtering (AF) is one such task of online prediction of the relevance of each new document with respect to pre-defined topics.",
                "Based on the initial query and a few positive examples (if available), an AF system maintains a profile for each such topic of interest, and constantly updates it based on feedback from the user.",
                "The incremental learning nature of AF systems makes them more powerful than standard search engines that support ad-hoc retrieval (e.g.",
                "Google and Yahoo) in terms of finding relevant information with respect to long-lasting topics of interest, and more attractive for users who are willing to provide feedback to adapt the system towards their specific information needs, without having to modify their queries manually.",
                "A variety of supervised learning algorithms (Rocchio-style classifiers, Exponential-Gaussian models, local regression and logistic regression approaches) have been studied for adaptive settings, examined with explicit and implicit relevance feedback, and evaluated with respect to utility optimization on large benchmark data collections in TREC (Text Retrieval Conferences) and TDT (Topic Detection and Tracking) forums [1, 4, 7, 15, 16, 20, 24, 23].",
                "Regularized logistic regression [21] has been found representative for the state-of-the-art approaches, and highly efficient for frequent model adaptations over large document collections such as the TREC-10 corpus (over 800,000 documents and 84 topics).",
                "Despite substantial achievements in recent adaptive filtering research, significant problems remain unsolved regarding how to leverage user feedback effectively and efficiently.",
                "Specifically, the following issues may seriously limit the true utility of AF systems in real-world applications: 1.",
                "User has a rather passive role in the conventional adaptive filtering setup - he or she reacts to the system only when the system makes a yes decision on a document, by confirming or rejecting that decision.",
                "A more active alternative would be to allow the user to issue multiple queries for a topic, review a ranked list of candidate documents (or passages) per query, and provide feedback on the ranked list, thus refining their information need and requesting updated ranked lists.",
                "The latter form of user interaction has been highly effective in standard retrieval for ad-hoc queries.",
                "How to deploy such a strategy for long-lasting information needs in AF settings is an open question for research. 2.",
                "The unit for receiving a relevance judgment (yes or no) is restricted to the document level in conventional AF.",
                "However, a real user may be willing to provide more informative, fine-grained feedback via highlighting some pieces of text in a retrieved document as relevant, instead of labeling the entire document as relevant.",
                "Effectively leveraging such fine-grained feedback could substantially enhance the quality of an AF system.",
                "For this, we need to enable supervised learning from labeled pieces of text of arbitrary span instead of just allowing labeled documents. 3.",
                "System-selected documents are often highly redundant.",
                "A major news event, for example, would be reported by multiple sources repeatedly for a while, making most of the information content in those articles redundant with each other.",
                "A conventional AF system would select all these redundant news stories for user feedback, wasting the users time while offering little gain.",
                "Clearly, techniques for novelty detection can help in principle [25, 2, 22] for improving the utility of the AF systems.",
                "However, the effectiveness of such techniques at passage level to detect novelty with respect to users (fine-grained) feedback and to detect redundancy in ranked lists remains to be evaluated using a measure of utility that mimics the needs of a real user.",
                "To address the above limitations of current AF systems, we propose and examine a new approach in this paper, combining the strengths of conventional AF (incremental learning of topic models), multi-pass passage retrieval for long-lasting queries conditioned on topic, and novelty detection for removal of redundancy from user interactions with the system.",
                "We call the new process utility-based information distillation.",
                "Note that conventional benchmark corpora for AF evaluations, which have relevance judgments at the document level and do not define tasks with multiple queries, are insufficient for evaluating the new approach.",
                "Therefore, we extended a benchmark corpus - the TDT4 collection of news stories and TV broadcasts - with task definitions, multiple queries per task, and answer keys per query.",
                "We have conducted our experiments on this extended TDT4 corpus and have made the additionally generated data publicly available for future comparative evaluations 1 .",
                "To automatically evaluate the system-returned arbitrary spans of text using our answer keys, we further developed an evaluation scheme with semi-automatic procedure for 1 URL: http://nyc.lti.cs.cmu.edu/downloads acquiring rules that can match nuggets against system responses.",
                "Moreover, we propose an extension of NDCG (Normalized Discounted Cumulated Gain) [9] for assessing the utility of ranked passages as a function of both relevance and novelty.",
                "The rest of this paper is organized as follows.",
                "Section 2 outlines the information distillation process with a concrete example.",
                "Section 3 describes the technical cores of our system called CAF´E - CMU Adaptive Filtering Engine.",
                "Section 4 discusses issues with respect to evaluation methodology and proposes a new scheme.",
                "Section 5 describes the extended TDT4 corpus.",
                "Section 6 presents our experiments and results.",
                "Section 7 concludes the study and gives future perspectives. 2.",
                "A SAMPLE TASK Consider a news event - the escape of seven convicts from a Texas prison in December 2000 and their capture a month later.",
                "Assuming a user were interested in this event since its early stage, the information need could be: Find information about the escape of convicts from Texas prison, and information related to their recapture.",
                "The associated lower-level questions could be: 1.",
                "How many prisoners escaped? 2.",
                "Where and when were they sighted? 3. Who are their known contacts inside and outside the prison? 4.",
                "How are they armed? 5.",
                "Do they have any vehicles? 6.",
                "What steps have been taken so far?",
                "We call such an information need a task, and the associated questions as the queries in this task.",
                "A distillation system is supposed to monitor the incoming documents, process them chunk by chunk in a temporal order, select potentially relevant and novel passages from each chunk with respect to each query, and present a ranked list of passages to the user.",
                "Passage ranking here is based on how relevant a passage is with respect to the current query, how novel it is with respect to the current user history (of his or her interactions with the system), and how redundant it is compared to other passages with a higher rank in the list.",
                "When presented with a list of passages, the user may provide feedback by highlighting arbitrary spans of text that he or she found relevant.",
                "These spans of text are taken as positive examples in the adaptation of the query profile, and also added to the users history.",
                "Passages not marked by the user are taken as negative examples.",
                "As soon as the query profile is updated, the system re-issues a search and returns another ranked list of passages where the previously seen passages are either removed or ranked low, based on user preference.",
                "For example, if the user highlights ...officials have posted a $100,000 reward for their capture... as relevant answer to the query What steps have been taken so far?, then the highlighted piece is used as an additional positive training example in the adaptation of the query profile.",
                "This piece of feedback is also added to the user history as a seen example, so that in future, the system will not place another passage mentioning $100,000 reward at the top of the ranked list.",
                "However, an article mentioning ...officials have doubled the reward money to $200,000... might be ranked high since it is both relevant to the (updated) query profile and novel with respect to the (updated) user history.",
                "The user may modify the original queries or add a new query during the process; the query profiles will be changed accordingly.",
                "Clearly, novelty detection is very important for the utility of such a system because of the iterative search.",
                "Without novelty detection, the old relevant passages would be shown to the user repeatedly in each ranked list.",
                "Through the above example, we can see the main properties of our new framework for utility-based information distillation over temporally ordered documents.",
                "Our framework combines and extends the power of adaptive filtering (AF), ad-hoc retrieval (IR) and novelty detection (ND).",
                "Compared to standard IR, our approach has the power of incrementally learning long-term information needs and modeling a sequence of queries within a task.",
                "Compared to conventional AF, it enables a more active role of the user in refining his or her information needs and requesting new results by allowing relevance and novelty feedback via highlighting of arbitrary spans of text in passages returned by the system.",
                "Compared to past work, this is the first evaluation of novelty detection integrated with adaptive filtering for sequenced queries that allows flexible user feedback over ranked passages.",
                "The combination of AF, IR and ND with the new extensions raises an important research question regarding evaluation methodology: how can we measure the utility of such an information distillation system?",
                "Existing metrics in standard IR, AF and ND are insufficient, and new solutions must be explored, as we will discuss in Section 4, after describing the technical cores of our system in the next section. 3.",
                "TECHNICAL CORES The core components of CAF´E are - 1) AF for incremental learning of query profiles, 2) IR for estimating relevance of passages with respect to query profiles, 3) ND for assessing novelty of passages with respect to users history, and 4) anti-redundancy component to remove redundancy from ranked lists. 3.1 Adaptive Filtering Component We use a state-of-the-art algorithm in the field - the regularized logistic regression method which had the best results on several benchmark evaluation corpora for AF [21].",
                "Logistic regression (LR) is a supervised learning algorithm for statistical classification.",
                "Based on a training set of labeled instances, it learns a class model which can then by used to predict the labels of unseen instances.",
                "Its performance as well as efficiency in terms of training time makes it a good candidate when frequent updates of the class model are required, as is the case in adaptive filtering, where the system must learn from each new feedback provided by the user. (See [21] and [23] for computational complexity and implementation issues).",
                "In adaptive filtering, each query is considered as a class and the probability of a passage belonging to this class corresponds to the degree of relevance of the passage with respect to the query.",
                "For training the model, we use the query itself as the initial positive training example of the class, and the user-highlighted pieces of text (marked as Relevant or Not-relevant) during feedback as additional training examples.",
                "To address the cold start issue in the early stage before any user feedback is obtained, the system uses a small sample from a retrospective corpus as the initial negative examples in the training set.",
                "The details of using logistic regression for adaptive filtering (assigning different weights to positive and negative training instances, and regularizing the objective function to prevent over-fitting on training data) are presented in [21].",
                "The class model w∗ learned by Logistic Regression, or the query profile, is a vector whose dimensions are individual terms and whose elements are the regression coefficients, indicating how influential each term is in the query profile.",
                "The query profile is updated whenever a new piece of user feedback is received.",
                "A temporally decaying weight can be applied to each training example, as an option, to emphasize the most recent user feedback. 3.2 Passage Retrieval Component We use standard IR techniques in this part of our system.",
                "Incoming documents are processed in chunks, where each chunk can be defined as a fixed span of time or as a fixed number of documents, as preferred by the user.",
                "For each incoming document, corpus statistics like the IDF (Inverted Document Frequency) of each term are updated.",
                "We use a state-of-the-art named entity identifier and tracker [8, 12] to identify person and location names, and merge them with co-referent named entities seen in the past.",
                "Then the documents are segmented into passages, which can be a whole document, a paragraph, a sentence, or any other continuous span of text, as preferred.",
                "Each passage is represented using a vector of TF-IDF (Term FrequencyInverse Document Frequency) weights, where term can be a word or a named entity.",
                "Given a query profile, i.e. the logistic regression solution w∗ as described in Section 3.1, the system computes the posterior probability of relevance for each passage x as fRL(x) ≡ P(y = 1|x, w∗ ) = 1 (1 + e−w∗·x) (1) Passages are ordered by their relevance scores, and the ones with scores above a threshold (tuned on a training set) comprise the relevance list that is passed on to the novelty detection step. 3.3 Novelty Detection Component CAF´E maintains a user history H(t), which contains all the spans of text hi that the user highlighted (as feedback) during his or her past interactions with the system, up to the current time t. Denoting the history as H(t) = n h1, h2, ..., ht o , (2) the novelty score of a new candidate passage x is computed as: fND(x) = 1 − max i∈1..t {cos(x, hi)} (3) where both candidate passage x and highlighted spans of text hi are represented as TF-IDF vectors.",
                "The novelty score of each passage is compared to a prespecified threshold (also tuned on a training set), and any passage with a score below this threshold is removed from the relevance list. 3.4 Anti-redundant Ranking Component Although the novelty detection component ensures that only novel (previously unseen) information remains in the relevance list, this list might still contain the same novel information at multiple positions in the ranked list.",
                "Suppose, for example, that the user has already read about a $100,000 reward for information about the escaped convicts.",
                "A new piece of news that the award has been increased to $200,000 is novel since the user hasnt read about it yet.",
                "However, multiple news sources would report this news and we might end up showing (redundant) articles from all these sources in a ranked list.",
                "Hence, a ranked list should also be made non-redundant with respect to its own contents.",
                "We use a simplified version of the Maximal Marginal Relevance method [5], originally developed for combining relevance and novelty in text retrieval and summarization.",
                "Our procedure starts with the current list of passages sorted by relevance (section 3.2), filtered by Novelty Detection component (section 3.3), and generates a new non-redundant list as follows: 1.",
                "Take the top passage in the current list as the top one in the new list. 2.",
                "Add the next passage x in the current list to the new list only if fAR(x) > t where fAR(x) = 1 − max pi∈Lnew {cos(x, pi)} and Lnew is the set of passages already selected in the new list. 3.",
                "Repeat step 2 until all the passages in the current list have been examined.",
                "After applying the above-mentioned algorithm, each passage in the new list is sufficiently dissimilar to others, thus favoring diversity rather than redundancy in the new ranked list.",
                "The anti-redundancy threshold t is tuned on a training set. 4.",
                "EVALUATION METHODOLOGY The approach we proposed above for information distillation raises important issues regarding evaluation methodology.",
                "Firstly, since our framework allows the output to be passages at different levels of granularity (e.g. k-sentence windows where k may vary) instead of a fixed length, it is not possible to have pre-annotated relevance judgments at all such granularity levels.",
                "Secondly, since we wish to measure the utility of the system output as a combination of both relevance and novelty, traditional relevance-only based measures must be replaced by measures that penalize the repetition of the same information in the system output across time.",
                "Thirdly, since the output of the system is ranked lists, we must reward those systems that present useful information (both relevant and previously unseen) using shorter ranked lists, and penalize those that present the same information using longer ranked lists.",
                "None of the existing measures in ad-hoc retrieval, adaptive filtering, novelty detection or other related areas (text summarization and question answering) have desirable properties in all the three aspects.",
                "Therefore, we must develop a new evaluation methodology. 4.1 Answer Keys To enable the evaluation of a system whose output consists of passages of arbitrary length, we borrow the concept of answer keys from the Question Answering (QA) community, where systems are allowed to return arbitrary spans of text as answers.",
                "Answer keys define what should be present in a system response to receive credit, and are comprised of a collection of information nuggets, i.e. factoid units about which human assessors can make binary decisions of whether or not a system response contains them.",
                "Defining answer keys and making the associated binary decisions are conceptual tasks that require semantic mapping [19], since system-returned passages can contain the same information expressed in many different ways.",
                "Hence, QA evaluations have relied on human assessors for the mapping between various expressions, making the process costly, time consuming, and not scalable to large query and document collections, and extensive system evaluations with various parameter settings. 4.1.1 Automating Evaluation based on Answer Keys Automatic evaluation methods would allow for faster system building and tuning, as well as provide an objective and affordable way of comparing various systems.",
                "Recently, such methods have been proposed, more or less, based on the idea of n-gram co-occurrences.",
                "Pourpre [10] assigns a fractional recall score to a system response based on its unigram overlap with a given nuggets description.",
                "For example, a system response A B C has recall 3/4 with respect to a nugget with description A B C D. However, such an approach is unfair to systems that present the same information but using words other than A, B, C, and D. Another open issue is how to weight individual words in measuring the closeness of a match.",
                "For example, consider the question How many prisoners escaped?.",
                "In the nugget Seven prisoners escaped from a Texas prison, there is no indication that seven is the keyword, and that it must be matched to get any relevance credit.",
                "Using IDF values does not help, since seven will generally not have a higher IDF than words like texas and prison.",
                "Also, redefining the nugget as just seven does not solve the problem since now it might spuriously match any mention of seven out of context.",
                "Nuggeteer [13] works on similar principles but makes binary decisions about whether a nugget is present in a given system response by tuning a threshold.",
                "However, it is also plagued by spurious relevance since not all words contained in the nugget description (or known correct responses) are central to the nugget. 4.1.2 Nugget-Matching Rules We propose a reliable automatic method for determining whether a snippet of text contains a given nugget, based on nugget-matching rules, which are generated using a semiautomatic procedure explained below.",
                "These rules are essentially Boolean queries that will only match against snippets that contain the nugget.",
                "For instance, a candidate rule for matching answers to How many prisoners escaped? is (Texas AND seven AND escape AND (convicts OR prisoners)), possibly with other synonyms and variants in the rule.",
                "For a corpus of news articles, which usually follow a typical formal prose, it is fairly easy to write such simple rules to match expected answers using a bootstrap approach, as described below.",
                "We propose a two-stage approach, inspired by Autoslog [14], that combines the strength of humans in identifying semantically equivalent expressions and the strength of the system in gathering statistical evidence from a humanannotated corpus of documents.",
                "In the first stage, human subjects annotated (using a highlighting tool) portions of ontopic documents that contained answers to each nugget 2 .",
                "In the second stage, subjects used our rule generation tool to create rules that would match the annotations for each nugget.",
                "The tool allows users to enter a Boolean rule as a disjunction of conjunctions (e.g. ((a AND b) OR (a AND c AND d) OR (e))).",
                "Given a candidate rule, our tool uses it as a Boolean query over the entire set of on-topic documents and calculates its recall and precision with respect to the annotations that it is expected to match.",
                "Hence, the subjects can start with a simple rule and iteratively refine it until they are satisfied with its recall and precision.",
                "We observed that it was very easy for humans to improve the precision of a rule by tweaking its existing conjunctions (adding more ANDs), and improving the recall by adding more conjunctions to the disjunction (adding more ORs).",
                "As an example, lets try to create a rule for the nugget which says that seven prisoners escaped from the Texas prison.",
                "We start with a simple rule - (seven).",
                "When we input this into the rule generation tool, we realize that this rule matches many spurious occurrences of seven (e.g. ...seven states...) and thus gets a low precision score.",
                "We can further qualify our rule - Texas AND seven AND convicts.",
                "Next, by looking at the missed annotations, we realize that some news articles mentioned ...seven prisoners escaped.... We then replace convicts with the disjunction (convicts OR prisoners).",
                "We continue tweaking the rule in this manner until we achieve a sufficiently high recall and precision - i.e. the (small number of) misses and false alarms can be safely ignored.",
                "Thus we can create nugget-matching rules that succinctly capture various ways of expressing a nugget, while avoiding matching incorrect (or out of context) responses.",
                "Human involvement in the rule creation process ensures high quality generic rules which can then be used to evaluate arbitrary system responses reliably. 4.2 Evaluating the Utility of a Sequence of Ranked Lists The utility of a retrieval system can be defined as the difference between how much the user gained in terms of useful information, and how much the user lost in terms of time and energy.",
                "We calculate this utility from the utilities of individual passages as follows.",
                "After reading each passage returned by the system, the user derives some gain depending on the presence of relevant and novel information, and incurs a loss in terms of the time and energy spent in going through the passage.",
                "However, the likelihood that the user would actually read a passage depends on its position in the ranked list.",
                "Hence, for a query q, the expected utility 2 LDC [18] already provides relevance judgments for 100 topics on the TDT4 corpus.",
                "We further ensured that these judgments are exhaustive on the entire corpus using pooling. of a passage pi at rank i can be defined as U(pi, q) = P(i) ∗ (Gain(pi, q) − Loss(pi, q)) (4) where P(i) is the probability that the user would go through a passage at rank i.",
                "The expected utility for an entire ranked list of length n can be calculated simply by adding the expected utility of each passage: U(q) = nX i=1 P(i) ∗ (Gain(pi, q) − Loss(pi, q)) (5) Note that if we ignore the loss term and define P(i) as P(i) ∝ 1/ logb(b + i − 1) (6) then we get the recently popularized metric called Discounted Cumulated Gain (DCG) [9], where Gain(pi, q) is defined as the graded relevance of passage pi.",
                "However, without the loss term, DCG is a purely recall-oriented metric and not suitable for an adaptive filtering setting, where the systems utility depends in part on its ability to limit the number of items shown to the user.",
                "Although P(i) could be defined based on empirical studies of user behavior, for simplicity, we use P(i) exactly as defined in equation 6.",
                "The gain G(pi, q) of passage pi with respect to the query q is a function of - 1) the number of relevant nuggets present in pi, and 2) the novelty of each of these nuggets.",
                "We combine these two factors as follows.",
                "For each nugget Nj, we assign an initial weight wj, and also keep a count nj of the number of times this nugget has been seen by the user in the past.",
                "The gain derived from each subsequent occurrence of the same nugget is assumed to reduce by a dampening factor γ.",
                "Thus, G(pi, q) is defined as G(pi, q) = X Nj ∈C(pi,q) wj ∗ γnj (7) where C(pi, q) is the set of all nuggets that appear in passage pi and also belong to the <br>answer key</br> of query q.",
                "The initial weights wj are all set of be 1.0 in our experiments, but can also be set based on a pyramid approach [11].",
                "The choice of dampening factor γ determines the users tolerance for redundancy.",
                "When γ = 0, a nugget will only receive credit for its first occurrence i.e. when nj is zero3 .",
                "For 0 < γ < 1, a nugget receives smaller credit for each successive occurrence.",
                "When γ = 1, no dampening occurs and repeated occurrences of a nugget receive the same credit.",
                "Note that the nugget occurrence counts are preserved between evaluation of successive ranked lists returned by the system, since the users are expected to remember what the system showed them in the past.",
                "We define the loss L(pi, q) as a constant cost c (we use 0.1) incurred when reading a system-returned passage.",
                "Thus, our metric can be re-written as U(q) = nX i=1 Gain(pi, q) logb(b + i − 1) − L(n) (8) where L(n) is the loss associated with a ranked list of length n: L(n) = c · nX i=1 1 logb(b + i − 1) (9) 3 Note that 00 = 1 Due to the similarity with Discounted Cumulated Gain (DCG), we call our metric Discounted Cumulated Utility (DCU).",
                "The DCU score obtained by the system is converted to a Normalized DCU (NDCU) score by dividing it by the DCU score of the ideal ranked list, which is created by ordering passages by their decreasing utility scores U(pi, q) and stopping when U(pi, q) ≤ 0 i.e. when the gain is less than or equal to the cost of reading the passage. 5.",
                "DATA TDT4 was the benchmark corpus used in TDT2002 and TDT2003 evaluations.",
                "The corpus consists of over 90, 000 news articles from multiple sources (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) published between October 2000 and January 2001, in the languages of Arabic, English, and Mandarin.",
                "Speech-recognized and machine-translated versions of the non-English articles were provided as well.",
                "LDC [18] has annotated the corpus with 100 topics, that correspond to various news events in this time period.",
                "Out of these, we selected a subset of 12 actionable events, and defined corresponding tasks for them4 .",
                "For each task, we manually defined a profile consisting of an initial set of (5 to 10) queries, a free-text description of the user history, i.e., what the user already knows about the event, and a list of known on-topic and off-topic documents (if available) as training examples.",
                "For each query, we generated answer keys and corresponding nugget matching rules using the procedure described in section 4.1.2, and produced a total of 120 queries, with an average of 7 nuggets per query. 6.",
                "EXPERIMENTS AND RESULTS 6.1 Baselines We used Indri [17], a popular language-model based retrieval engine, as a baseline for comparison with CAF´E.",
                "Indri supports standard search engine functionality, including pseudo-relevance feedback (PRF) [3, 6], and is representative of a typical query-based retrieval system.",
                "Indri does not support any kind of novelty detection.",
                "We compare Indri with PRF turned on and off, against CAF´E with user feedback, novelty detection and antiredundant ranking turned on and off. 6.2 Experimental Setup We divided the TDT4 corpus spanning 4 months into 10 chunks, each defined as a period of 12 consecutive days.",
                "At any given point of time in the distillation process, each system accessed the past data up to the current point, and returned a ranked list of up 50 passages per query.",
                "The 12 tasks defined on the corpus were divided into a training and test set with 6 tasks each.",
                "Each system was allowed to use the training set to tune its parameters for optimizing NDCU (equation 8), including the relevance threshold for both Indri and CAF´E, and the novelty and antiredundancy thresholds for CAF´E.",
                "The NDCU for each system run is calculated automatically.",
                "User feedback was also simulated - relevance judgments for each system-returned passage (as determined by the nugget matching rules described in section 4.1.2) were 4 URL: http://nyc.lti.cs.cmu.edu/downloads Figure 1: Performance of Indri across chunks Figure 2: Performance of CAF´E across chunks used as user feedback in the adaptation of query profiles and user histories. 6.3 Results In Table 1, we show the NDCU scores of the two systems under various settings.",
                "These scores are averaged over the six tasks in the test set, and are calculated with two dampening factors (see section 4.2): γ = 0 and 0.1, to simulate no tolerance and small tolerance for redundancy, respectively.",
                "Using γ = 0 creates a much more strict metric since it does not give any credit to a passage that contains relevant but redundant information.",
                "Hence, the improvement obtained from enabling user feedback is smaller with γ = 0 than the improvement obtained from feedback with γ = 0.1.",
                "This reveals a shortcoming of contemporary retrieval systemswhen the user gives positive feedback on a passage, the systems gives higher weights to the terms present in that passage and tends to retrieve other passages containing the same terms - and thus - usually the same information.",
                "However, the user does not benefit from seeing such redundant passages, and is usually interested in other passages containing related information.",
                "It is informative to evaluate retrieval systems using our utility measure (with γ = 0) which accounts for novelty and thus gives a more realistic picture of how well a system can generalize from user feedback, rather than using traditional IR measures like recall and precision which give an incomplete picture of improvement obtained from user feedback.",
                "Sometimes, however, users might indeed be interested in seeing the same information from multiple sources, as an Table 1: NDCU Scores of Indri and CAF´E for two dampening factors (γ), and various settings (F: Feedback, N: Novelty Detection, A: Anti-Redundant Ranking) Indri CAF´E γ Base +PRF Base +F +F+N +F+A +F+N+A 0 0.19 0.19 0.22 0.23 0.24 0.24 0.24 0.1 0.28 0.29 0.24 0.35 0.35 0.36 0.36 indicator of its importance or reliability.",
                "In such a case, we can simply choose a higher value for γ which corresponds to a higher tolerance for redundancy, and hence let the system tune its parameters accordingly.",
                "Since documents were processed chunk by chunk, it would be interesting to see how the performance of systems improves over time.",
                "Figures 1 and 2 show the performance trends for both the systems across chunks.",
                "While the performance with and without feedback on the first few chunks is expected to be close, for subsequent chunks, the performance curve with feedback enabled rises above the one with the no-feedback setting.",
                "The performance trends are not consistent across all chunks because on-topic documents are not uniformly distributed over all the chunks, making some queries easier than others in certain chunks.",
                "Moreover, since Indri uses pseudo-relevance feedback while CAF´E uses feedback based on actual relevance judgments, the improvement in case of Indri is less dramatic than that of CAF´E. 7.",
                "CONCLUDING REMARKS This paper presents the first investigation on utility-based information distillation with a system that learns the longlasting information needs from fine-grained user feedback over a sequence of ranked passages.",
                "Our system, called CAF´E, combines adaptive filtering, novelty detection and antiredundant passage ranking in a unified framework for utility optimization.",
                "We developed a new scheme for automated evaluation and feedback based on a semi-automatic procedure for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also proposed an extension of the NDCG metric for assessing the utility of ranked passages as a weighted combination of relevance and novelty.",
                "Our experiments on the newly annotated TDT4 benchmark corpus show encouraging utility enhancement over Indri, and also over our own system with incremental learning and novelty detection turned off. 8.",
                "ACKNOWLEDGMENTS We would like to thank Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng, and the Qualitative Data Analysis Program at the University of Pittsburgh lead by Dr. Stuart Shulman for their help with collecting and processing the extended TDT4 annotations used in our experiments.",
                "This work is supported in parts by the National Science Foundation (NSF) under grant IIS0434035, and the Defense Advanced Research Project Agency (DARPA) under contracts NBCHD030010 and W0550432.",
                "Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors. 9.",
                "ADDITIONAL AUTHORS Jian Zhang (jianzhan@stat.purdue.edu)∗ , Jaime Carbonell (jgc@cs.cmu.edu)† , Peter Brusilovsky (peterb+@pitt.edu)‡ , Daqing He(dah44@pitt.edu)‡ 10.",
                "REFERENCES [1] J. Allan.",
                "Incremental Relevance Feedback for Information Filtering.",
                "Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, pages 270-278, 1996. [2] J. Allan, C. Wade, and A. Bolivar.",
                "Retrieval and Novelty Detection at the Sentence Level.",
                "Proceedings of the ACM SIGIR conference on research and development in information retrieval, 2003. [3] C. Buckley, G. Salton, and J. Allan.",
                "Automatic Retrieval with Locality Information using SMART.",
                "NIST special publication, (500207):59-72, 1993. [4] J. Callan.",
                "Learning While Filtering Documents.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 224-231, 1998. [5] J. Carbonell and J. Goldstein.",
                "The use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 335-336, 1998. [6] E. Efthimiadis.",
                "Query Expansion.",
                "Annual Review of Information Science and Technology (ARIST), 31:p121-87, 1996. [7] J. Fiscus and G. Duddington.",
                "Topic Detection and Tracking Overview.",
                "Topic Detection and Tracking: Event-based Information Organization, pages 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos.",
                "A Statistical Model for Multilingual Entity Detection and Tracking.",
                "NAACL/HLT, 2004. [9] K. J¨arvelin and J. Kek¨al¨ainen.",
                "Cumulated Gain-based Evaluation of IR Techniques.",
                "ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin and D. Demner-Fushman.",
                "Automatically Evaluating Answers to Definition Questions.",
                "Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), 2005. ∗ Statistics Dept., Purdue University, West Lafayette, USA † Language Technologies Inst., Carnegie Mellon University, Pittsburgh, USA ‡ School of Information Sciences, Univ. of Pittsburgh, Pittsburgh, USA [11] J. Lin and D. Demner-Fushman.",
                "Will Pyramids Built of nUggets Topple Over.",
                "Proceedings of HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos.",
                "A Mention-synchronous Coreference Resolution Algorithm based on the Bell Tree.",
                "Proc. of ACL, 4:136-143, 2004. [13] G. Marton.",
                "Nuggeteer: Automatic Nugget-Based Evaluation Using Descriptions and Judgments.",
                "HLT/NAACL, 2006. [14] E. Riloff.",
                "Automatically Constructing a Dictionary for Information Extraction Tasks.",
                "Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 811-816, 1993. [15] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9: Filtering track.",
                "The Ninth Text REtrieval Conference (TREC-9), pages 361-368. [16] R. Schapire, Y.",
                "Singer, and A. Singhal.",
                "Boosting and Rocchio Applied to Text Filtering.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle, and W. Croft.",
                "Indri: A Language Model-based Search Engine for Complex Queries.",
                "Proceedings of the International Conference on Intelligence Analysis, 2004. [18] The Linguistic Data Consortium. http://www.ldc.upenn.edu/. [19] E. Voorhees.",
                "Overview of the TREC 2003 Question Answering Track.",
                "Proceedings of the Twelfth Text REtrieval Conference (TREC 2003), 2003. [20] Y. Yang and B. Kisiel.",
                "Margin-based Local Regression for Adaptive Filtering.",
                "Proceedings of the twelfth international conference on Information and knowledge management, pages 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of Adaptive Filtering Methods in a Cross-benchmark Evaluation.",
                "Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 98-105, 2005. [22] C. Zhai, W. Cohen, and J. Lafferty.",
                "Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 10-17, 2003. [23] J. Zhang and Y. Yang.",
                "Robustness of Regularized Linear Classification Methods in Text Categorization.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 190-197, 2003. [24] Y. Zhang.",
                "Using Bayesian Priors to Combine Classifiers for Adaptive Filtering.",
                "Proceedings of the 27th annual international conference on Research and development in information retrieval, pages 345-352, 2004. [25] Y. Zhang, J. Callan, and T. Minka.",
                "Novelty and Redundancy Detection in Adaptive Filtering.",
                "Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Por lo tanto, G (Pi, Q) se define como G (Pi, Q) = x nj ∈C (PI, Q) WJ ∗ γnj (7) donde C (Pi, Q) es el conjunto de todas las pepitas que aparecen en el pasoPi y también pertenecen a la \"clave de respuesta\" de la consulta q."
            ],
            "translated_text": "",
            "candidates": [
                "llave de respuestas",
                "clave de respuesta"
            ],
            "error": []
        },
        "nugget-matching rule": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Utility-based Information Distillation Over Temporally Sequenced Documents Yiming Yang Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA yiming@cs.cmu.edu Abhimanyu Lad Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA alad@cs.cmu.edu Ni Lao Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA nlao@cs.cmu.edu Abhay Harpale Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA aharpale@cs.cmu.edu Bryan Kisiel Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA bkisiel@cs.cmu.edu Monica Rogati Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA mrogati@cs.cmu.edu ABSTRACT This paper examines a new approach to information distillation over temporally ordered documents, and proposes a novel evaluation scheme for such a framework.",
                "It combines the strengths of and extends beyond conventional adaptive filtering, novelty detection and non-redundant passage ranking with respect to long-lasting information needs (tasks with multiple queries).",
                "Our approach supports fine-grained user feedback via highlighting of arbitrary spans of text, and leverages such information for utility optimization in adaptive settings.",
                "For our experiments, we defined hypothetical tasks based on news events in the TDT4 corpus, with multiple queries per task.",
                "Answer keys (nuggets) were generated for each query and a semiautomatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also propose an extension of the NDCG metric for assessing the utility of ranked passages as a combination of relevance and novelty.",
                "Our results show encouraging utility enhancements using the new approach, compared to the baseline systems without incremental learning or the novelty detection components.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 General Terms Design, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Tracking new and relevant information from temporal data streams for users with long-lasting needs has been a challenging research topic in information retrieval.",
                "Adaptive filtering (AF) is one such task of online prediction of the relevance of each new document with respect to pre-defined topics.",
                "Based on the initial query and a few positive examples (if available), an AF system maintains a profile for each such topic of interest, and constantly updates it based on feedback from the user.",
                "The incremental learning nature of AF systems makes them more powerful than standard search engines that support ad-hoc retrieval (e.g.",
                "Google and Yahoo) in terms of finding relevant information with respect to long-lasting topics of interest, and more attractive for users who are willing to provide feedback to adapt the system towards their specific information needs, without having to modify their queries manually.",
                "A variety of supervised learning algorithms (Rocchio-style classifiers, Exponential-Gaussian models, local regression and logistic regression approaches) have been studied for adaptive settings, examined with explicit and implicit relevance feedback, and evaluated with respect to utility optimization on large benchmark data collections in TREC (Text Retrieval Conferences) and TDT (Topic Detection and Tracking) forums [1, 4, 7, 15, 16, 20, 24, 23].",
                "Regularized logistic regression [21] has been found representative for the state-of-the-art approaches, and highly efficient for frequent model adaptations over large document collections such as the TREC-10 corpus (over 800,000 documents and 84 topics).",
                "Despite substantial achievements in recent adaptive filtering research, significant problems remain unsolved regarding how to leverage user feedback effectively and efficiently.",
                "Specifically, the following issues may seriously limit the true utility of AF systems in real-world applications: 1.",
                "User has a rather passive role in the conventional adaptive filtering setup - he or she reacts to the system only when the system makes a yes decision on a document, by confirming or rejecting that decision.",
                "A more active alternative would be to allow the user to issue multiple queries for a topic, review a ranked list of candidate documents (or passages) per query, and provide feedback on the ranked list, thus refining their information need and requesting updated ranked lists.",
                "The latter form of user interaction has been highly effective in standard retrieval for ad-hoc queries.",
                "How to deploy such a strategy for long-lasting information needs in AF settings is an open question for research. 2.",
                "The unit for receiving a relevance judgment (yes or no) is restricted to the document level in conventional AF.",
                "However, a real user may be willing to provide more informative, fine-grained feedback via highlighting some pieces of text in a retrieved document as relevant, instead of labeling the entire document as relevant.",
                "Effectively leveraging such fine-grained feedback could substantially enhance the quality of an AF system.",
                "For this, we need to enable supervised learning from labeled pieces of text of arbitrary span instead of just allowing labeled documents. 3.",
                "System-selected documents are often highly redundant.",
                "A major news event, for example, would be reported by multiple sources repeatedly for a while, making most of the information content in those articles redundant with each other.",
                "A conventional AF system would select all these redundant news stories for user feedback, wasting the users time while offering little gain.",
                "Clearly, techniques for novelty detection can help in principle [25, 2, 22] for improving the utility of the AF systems.",
                "However, the effectiveness of such techniques at passage level to detect novelty with respect to users (fine-grained) feedback and to detect redundancy in ranked lists remains to be evaluated using a measure of utility that mimics the needs of a real user.",
                "To address the above limitations of current AF systems, we propose and examine a new approach in this paper, combining the strengths of conventional AF (incremental learning of topic models), multi-pass passage retrieval for long-lasting queries conditioned on topic, and novelty detection for removal of redundancy from user interactions with the system.",
                "We call the new process utility-based information distillation.",
                "Note that conventional benchmark corpora for AF evaluations, which have relevance judgments at the document level and do not define tasks with multiple queries, are insufficient for evaluating the new approach.",
                "Therefore, we extended a benchmark corpus - the TDT4 collection of news stories and TV broadcasts - with task definitions, multiple queries per task, and answer keys per query.",
                "We have conducted our experiments on this extended TDT4 corpus and have made the additionally generated data publicly available for future comparative evaluations 1 .",
                "To automatically evaluate the system-returned arbitrary spans of text using our answer keys, we further developed an evaluation scheme with semi-automatic procedure for 1 URL: http://nyc.lti.cs.cmu.edu/downloads acquiring rules that can match nuggets against system responses.",
                "Moreover, we propose an extension of NDCG (Normalized Discounted Cumulated Gain) [9] for assessing the utility of ranked passages as a function of both relevance and novelty.",
                "The rest of this paper is organized as follows.",
                "Section 2 outlines the information distillation process with a concrete example.",
                "Section 3 describes the technical cores of our system called CAF´E - CMU Adaptive Filtering Engine.",
                "Section 4 discusses issues with respect to evaluation methodology and proposes a new scheme.",
                "Section 5 describes the extended TDT4 corpus.",
                "Section 6 presents our experiments and results.",
                "Section 7 concludes the study and gives future perspectives. 2.",
                "A SAMPLE TASK Consider a news event - the escape of seven convicts from a Texas prison in December 2000 and their capture a month later.",
                "Assuming a user were interested in this event since its early stage, the information need could be: Find information about the escape of convicts from Texas prison, and information related to their recapture.",
                "The associated lower-level questions could be: 1.",
                "How many prisoners escaped? 2.",
                "Where and when were they sighted? 3. Who are their known contacts inside and outside the prison? 4.",
                "How are they armed? 5.",
                "Do they have any vehicles? 6.",
                "What steps have been taken so far?",
                "We call such an information need a task, and the associated questions as the queries in this task.",
                "A distillation system is supposed to monitor the incoming documents, process them chunk by chunk in a temporal order, select potentially relevant and novel passages from each chunk with respect to each query, and present a ranked list of passages to the user.",
                "Passage ranking here is based on how relevant a passage is with respect to the current query, how novel it is with respect to the current user history (of his or her interactions with the system), and how redundant it is compared to other passages with a higher rank in the list.",
                "When presented with a list of passages, the user may provide feedback by highlighting arbitrary spans of text that he or she found relevant.",
                "These spans of text are taken as positive examples in the adaptation of the query profile, and also added to the users history.",
                "Passages not marked by the user are taken as negative examples.",
                "As soon as the query profile is updated, the system re-issues a search and returns another ranked list of passages where the previously seen passages are either removed or ranked low, based on user preference.",
                "For example, if the user highlights ...officials have posted a $100,000 reward for their capture... as relevant answer to the query What steps have been taken so far?, then the highlighted piece is used as an additional positive training example in the adaptation of the query profile.",
                "This piece of feedback is also added to the user history as a seen example, so that in future, the system will not place another passage mentioning $100,000 reward at the top of the ranked list.",
                "However, an article mentioning ...officials have doubled the reward money to $200,000... might be ranked high since it is both relevant to the (updated) query profile and novel with respect to the (updated) user history.",
                "The user may modify the original queries or add a new query during the process; the query profiles will be changed accordingly.",
                "Clearly, novelty detection is very important for the utility of such a system because of the iterative search.",
                "Without novelty detection, the old relevant passages would be shown to the user repeatedly in each ranked list.",
                "Through the above example, we can see the main properties of our new framework for utility-based information distillation over temporally ordered documents.",
                "Our framework combines and extends the power of adaptive filtering (AF), ad-hoc retrieval (IR) and novelty detection (ND).",
                "Compared to standard IR, our approach has the power of incrementally learning long-term information needs and modeling a sequence of queries within a task.",
                "Compared to conventional AF, it enables a more active role of the user in refining his or her information needs and requesting new results by allowing relevance and novelty feedback via highlighting of arbitrary spans of text in passages returned by the system.",
                "Compared to past work, this is the first evaluation of novelty detection integrated with adaptive filtering for sequenced queries that allows flexible user feedback over ranked passages.",
                "The combination of AF, IR and ND with the new extensions raises an important research question regarding evaluation methodology: how can we measure the utility of such an information distillation system?",
                "Existing metrics in standard IR, AF and ND are insufficient, and new solutions must be explored, as we will discuss in Section 4, after describing the technical cores of our system in the next section. 3.",
                "TECHNICAL CORES The core components of CAF´E are - 1) AF for incremental learning of query profiles, 2) IR for estimating relevance of passages with respect to query profiles, 3) ND for assessing novelty of passages with respect to users history, and 4) anti-redundancy component to remove redundancy from ranked lists. 3.1 Adaptive Filtering Component We use a state-of-the-art algorithm in the field - the regularized logistic regression method which had the best results on several benchmark evaluation corpora for AF [21].",
                "Logistic regression (LR) is a supervised learning algorithm for statistical classification.",
                "Based on a training set of labeled instances, it learns a class model which can then by used to predict the labels of unseen instances.",
                "Its performance as well as efficiency in terms of training time makes it a good candidate when frequent updates of the class model are required, as is the case in adaptive filtering, where the system must learn from each new feedback provided by the user. (See [21] and [23] for computational complexity and implementation issues).",
                "In adaptive filtering, each query is considered as a class and the probability of a passage belonging to this class corresponds to the degree of relevance of the passage with respect to the query.",
                "For training the model, we use the query itself as the initial positive training example of the class, and the user-highlighted pieces of text (marked as Relevant or Not-relevant) during feedback as additional training examples.",
                "To address the cold start issue in the early stage before any user feedback is obtained, the system uses a small sample from a retrospective corpus as the initial negative examples in the training set.",
                "The details of using logistic regression for adaptive filtering (assigning different weights to positive and negative training instances, and regularizing the objective function to prevent over-fitting on training data) are presented in [21].",
                "The class model w∗ learned by Logistic Regression, or the query profile, is a vector whose dimensions are individual terms and whose elements are the regression coefficients, indicating how influential each term is in the query profile.",
                "The query profile is updated whenever a new piece of user feedback is received.",
                "A temporally decaying weight can be applied to each training example, as an option, to emphasize the most recent user feedback. 3.2 Passage Retrieval Component We use standard IR techniques in this part of our system.",
                "Incoming documents are processed in chunks, where each chunk can be defined as a fixed span of time or as a fixed number of documents, as preferred by the user.",
                "For each incoming document, corpus statistics like the IDF (Inverted Document Frequency) of each term are updated.",
                "We use a state-of-the-art named entity identifier and tracker [8, 12] to identify person and location names, and merge them with co-referent named entities seen in the past.",
                "Then the documents are segmented into passages, which can be a whole document, a paragraph, a sentence, or any other continuous span of text, as preferred.",
                "Each passage is represented using a vector of TF-IDF (Term FrequencyInverse Document Frequency) weights, where term can be a word or a named entity.",
                "Given a query profile, i.e. the logistic regression solution w∗ as described in Section 3.1, the system computes the posterior probability of relevance for each passage x as fRL(x) ≡ P(y = 1|x, w∗ ) = 1 (1 + e−w∗·x) (1) Passages are ordered by their relevance scores, and the ones with scores above a threshold (tuned on a training set) comprise the relevance list that is passed on to the novelty detection step. 3.3 Novelty Detection Component CAF´E maintains a user history H(t), which contains all the spans of text hi that the user highlighted (as feedback) during his or her past interactions with the system, up to the current time t. Denoting the history as H(t) = n h1, h2, ..., ht o , (2) the novelty score of a new candidate passage x is computed as: fND(x) = 1 − max i∈1..t {cos(x, hi)} (3) where both candidate passage x and highlighted spans of text hi are represented as TF-IDF vectors.",
                "The novelty score of each passage is compared to a prespecified threshold (also tuned on a training set), and any passage with a score below this threshold is removed from the relevance list. 3.4 Anti-redundant Ranking Component Although the novelty detection component ensures that only novel (previously unseen) information remains in the relevance list, this list might still contain the same novel information at multiple positions in the ranked list.",
                "Suppose, for example, that the user has already read about a $100,000 reward for information about the escaped convicts.",
                "A new piece of news that the award has been increased to $200,000 is novel since the user hasnt read about it yet.",
                "However, multiple news sources would report this news and we might end up showing (redundant) articles from all these sources in a ranked list.",
                "Hence, a ranked list should also be made non-redundant with respect to its own contents.",
                "We use a simplified version of the Maximal Marginal Relevance method [5], originally developed for combining relevance and novelty in text retrieval and summarization.",
                "Our procedure starts with the current list of passages sorted by relevance (section 3.2), filtered by Novelty Detection component (section 3.3), and generates a new non-redundant list as follows: 1.",
                "Take the top passage in the current list as the top one in the new list. 2.",
                "Add the next passage x in the current list to the new list only if fAR(x) > t where fAR(x) = 1 − max pi∈Lnew {cos(x, pi)} and Lnew is the set of passages already selected in the new list. 3.",
                "Repeat step 2 until all the passages in the current list have been examined.",
                "After applying the above-mentioned algorithm, each passage in the new list is sufficiently dissimilar to others, thus favoring diversity rather than redundancy in the new ranked list.",
                "The anti-redundancy threshold t is tuned on a training set. 4.",
                "EVALUATION METHODOLOGY The approach we proposed above for information distillation raises important issues regarding evaluation methodology.",
                "Firstly, since our framework allows the output to be passages at different levels of granularity (e.g. k-sentence windows where k may vary) instead of a fixed length, it is not possible to have pre-annotated relevance judgments at all such granularity levels.",
                "Secondly, since we wish to measure the utility of the system output as a combination of both relevance and novelty, traditional relevance-only based measures must be replaced by measures that penalize the repetition of the same information in the system output across time.",
                "Thirdly, since the output of the system is ranked lists, we must reward those systems that present useful information (both relevant and previously unseen) using shorter ranked lists, and penalize those that present the same information using longer ranked lists.",
                "None of the existing measures in ad-hoc retrieval, adaptive filtering, novelty detection or other related areas (text summarization and question answering) have desirable properties in all the three aspects.",
                "Therefore, we must develop a new evaluation methodology. 4.1 Answer Keys To enable the evaluation of a system whose output consists of passages of arbitrary length, we borrow the concept of answer keys from the Question Answering (QA) community, where systems are allowed to return arbitrary spans of text as answers.",
                "Answer keys define what should be present in a system response to receive credit, and are comprised of a collection of information nuggets, i.e. factoid units about which human assessors can make binary decisions of whether or not a system response contains them.",
                "Defining answer keys and making the associated binary decisions are conceptual tasks that require semantic mapping [19], since system-returned passages can contain the same information expressed in many different ways.",
                "Hence, QA evaluations have relied on human assessors for the mapping between various expressions, making the process costly, time consuming, and not scalable to large query and document collections, and extensive system evaluations with various parameter settings. 4.1.1 Automating Evaluation based on Answer Keys Automatic evaluation methods would allow for faster system building and tuning, as well as provide an objective and affordable way of comparing various systems.",
                "Recently, such methods have been proposed, more or less, based on the idea of n-gram co-occurrences.",
                "Pourpre [10] assigns a fractional recall score to a system response based on its unigram overlap with a given nuggets description.",
                "For example, a system response A B C has recall 3/4 with respect to a nugget with description A B C D. However, such an approach is unfair to systems that present the same information but using words other than A, B, C, and D. Another open issue is how to weight individual words in measuring the closeness of a match.",
                "For example, consider the question How many prisoners escaped?.",
                "In the nugget Seven prisoners escaped from a Texas prison, there is no indication that seven is the keyword, and that it must be matched to get any relevance credit.",
                "Using IDF values does not help, since seven will generally not have a higher IDF than words like texas and prison.",
                "Also, redefining the nugget as just seven does not solve the problem since now it might spuriously match any mention of seven out of context.",
                "Nuggeteer [13] works on similar principles but makes binary decisions about whether a nugget is present in a given system response by tuning a threshold.",
                "However, it is also plagued by spurious relevance since not all words contained in the nugget description (or known correct responses) are central to the nugget. 4.1.2 Nugget-Matching Rules We propose a reliable automatic method for determining whether a snippet of text contains a given nugget, based on <br>nugget-matching rule</br>s, which are generated using a semiautomatic procedure explained below.",
                "These rules are essentially Boolean queries that will only match against snippets that contain the nugget.",
                "For instance, a candidate rule for matching answers to How many prisoners escaped? is (Texas AND seven AND escape AND (convicts OR prisoners)), possibly with other synonyms and variants in the rule.",
                "For a corpus of news articles, which usually follow a typical formal prose, it is fairly easy to write such simple rules to match expected answers using a bootstrap approach, as described below.",
                "We propose a two-stage approach, inspired by Autoslog [14], that combines the strength of humans in identifying semantically equivalent expressions and the strength of the system in gathering statistical evidence from a humanannotated corpus of documents.",
                "In the first stage, human subjects annotated (using a highlighting tool) portions of ontopic documents that contained answers to each nugget 2 .",
                "In the second stage, subjects used our rule generation tool to create rules that would match the annotations for each nugget.",
                "The tool allows users to enter a Boolean rule as a disjunction of conjunctions (e.g. ((a AND b) OR (a AND c AND d) OR (e))).",
                "Given a candidate rule, our tool uses it as a Boolean query over the entire set of on-topic documents and calculates its recall and precision with respect to the annotations that it is expected to match.",
                "Hence, the subjects can start with a simple rule and iteratively refine it until they are satisfied with its recall and precision.",
                "We observed that it was very easy for humans to improve the precision of a rule by tweaking its existing conjunctions (adding more ANDs), and improving the recall by adding more conjunctions to the disjunction (adding more ORs).",
                "As an example, lets try to create a rule for the nugget which says that seven prisoners escaped from the Texas prison.",
                "We start with a simple rule - (seven).",
                "When we input this into the rule generation tool, we realize that this rule matches many spurious occurrences of seven (e.g. ...seven states...) and thus gets a low precision score.",
                "We can further qualify our rule - Texas AND seven AND convicts.",
                "Next, by looking at the missed annotations, we realize that some news articles mentioned ...seven prisoners escaped.... We then replace convicts with the disjunction (convicts OR prisoners).",
                "We continue tweaking the rule in this manner until we achieve a sufficiently high recall and precision - i.e. the (small number of) misses and false alarms can be safely ignored.",
                "Thus we can create <br>nugget-matching rule</br>s that succinctly capture various ways of expressing a nugget, while avoiding matching incorrect (or out of context) responses.",
                "Human involvement in the rule creation process ensures high quality generic rules which can then be used to evaluate arbitrary system responses reliably. 4.2 Evaluating the Utility of a Sequence of Ranked Lists The utility of a retrieval system can be defined as the difference between how much the user gained in terms of useful information, and how much the user lost in terms of time and energy.",
                "We calculate this utility from the utilities of individual passages as follows.",
                "After reading each passage returned by the system, the user derives some gain depending on the presence of relevant and novel information, and incurs a loss in terms of the time and energy spent in going through the passage.",
                "However, the likelihood that the user would actually read a passage depends on its position in the ranked list.",
                "Hence, for a query q, the expected utility 2 LDC [18] already provides relevance judgments for 100 topics on the TDT4 corpus.",
                "We further ensured that these judgments are exhaustive on the entire corpus using pooling. of a passage pi at rank i can be defined as U(pi, q) = P(i) ∗ (Gain(pi, q) − Loss(pi, q)) (4) where P(i) is the probability that the user would go through a passage at rank i.",
                "The expected utility for an entire ranked list of length n can be calculated simply by adding the expected utility of each passage: U(q) = nX i=1 P(i) ∗ (Gain(pi, q) − Loss(pi, q)) (5) Note that if we ignore the loss term and define P(i) as P(i) ∝ 1/ logb(b + i − 1) (6) then we get the recently popularized metric called Discounted Cumulated Gain (DCG) [9], where Gain(pi, q) is defined as the graded relevance of passage pi.",
                "However, without the loss term, DCG is a purely recall-oriented metric and not suitable for an adaptive filtering setting, where the systems utility depends in part on its ability to limit the number of items shown to the user.",
                "Although P(i) could be defined based on empirical studies of user behavior, for simplicity, we use P(i) exactly as defined in equation 6.",
                "The gain G(pi, q) of passage pi with respect to the query q is a function of - 1) the number of relevant nuggets present in pi, and 2) the novelty of each of these nuggets.",
                "We combine these two factors as follows.",
                "For each nugget Nj, we assign an initial weight wj, and also keep a count nj of the number of times this nugget has been seen by the user in the past.",
                "The gain derived from each subsequent occurrence of the same nugget is assumed to reduce by a dampening factor γ.",
                "Thus, G(pi, q) is defined as G(pi, q) = X Nj ∈C(pi,q) wj ∗ γnj (7) where C(pi, q) is the set of all nuggets that appear in passage pi and also belong to the answer key of query q.",
                "The initial weights wj are all set of be 1.0 in our experiments, but can also be set based on a pyramid approach [11].",
                "The choice of dampening factor γ determines the users tolerance for redundancy.",
                "When γ = 0, a nugget will only receive credit for its first occurrence i.e. when nj is zero3 .",
                "For 0 < γ < 1, a nugget receives smaller credit for each successive occurrence.",
                "When γ = 1, no dampening occurs and repeated occurrences of a nugget receive the same credit.",
                "Note that the nugget occurrence counts are preserved between evaluation of successive ranked lists returned by the system, since the users are expected to remember what the system showed them in the past.",
                "We define the loss L(pi, q) as a constant cost c (we use 0.1) incurred when reading a system-returned passage.",
                "Thus, our metric can be re-written as U(q) = nX i=1 Gain(pi, q) logb(b + i − 1) − L(n) (8) where L(n) is the loss associated with a ranked list of length n: L(n) = c · nX i=1 1 logb(b + i − 1) (9) 3 Note that 00 = 1 Due to the similarity with Discounted Cumulated Gain (DCG), we call our metric Discounted Cumulated Utility (DCU).",
                "The DCU score obtained by the system is converted to a Normalized DCU (NDCU) score by dividing it by the DCU score of the ideal ranked list, which is created by ordering passages by their decreasing utility scores U(pi, q) and stopping when U(pi, q) ≤ 0 i.e. when the gain is less than or equal to the cost of reading the passage. 5.",
                "DATA TDT4 was the benchmark corpus used in TDT2002 and TDT2003 evaluations.",
                "The corpus consists of over 90, 000 news articles from multiple sources (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) published between October 2000 and January 2001, in the languages of Arabic, English, and Mandarin.",
                "Speech-recognized and machine-translated versions of the non-English articles were provided as well.",
                "LDC [18] has annotated the corpus with 100 topics, that correspond to various news events in this time period.",
                "Out of these, we selected a subset of 12 actionable events, and defined corresponding tasks for them4 .",
                "For each task, we manually defined a profile consisting of an initial set of (5 to 10) queries, a free-text description of the user history, i.e., what the user already knows about the event, and a list of known on-topic and off-topic documents (if available) as training examples.",
                "For each query, we generated answer keys and corresponding nugget matching rules using the procedure described in section 4.1.2, and produced a total of 120 queries, with an average of 7 nuggets per query. 6.",
                "EXPERIMENTS AND RESULTS 6.1 Baselines We used Indri [17], a popular language-model based retrieval engine, as a baseline for comparison with CAF´E.",
                "Indri supports standard search engine functionality, including pseudo-relevance feedback (PRF) [3, 6], and is representative of a typical query-based retrieval system.",
                "Indri does not support any kind of novelty detection.",
                "We compare Indri with PRF turned on and off, against CAF´E with user feedback, novelty detection and antiredundant ranking turned on and off. 6.2 Experimental Setup We divided the TDT4 corpus spanning 4 months into 10 chunks, each defined as a period of 12 consecutive days.",
                "At any given point of time in the distillation process, each system accessed the past data up to the current point, and returned a ranked list of up 50 passages per query.",
                "The 12 tasks defined on the corpus were divided into a training and test set with 6 tasks each.",
                "Each system was allowed to use the training set to tune its parameters for optimizing NDCU (equation 8), including the relevance threshold for both Indri and CAF´E, and the novelty and antiredundancy thresholds for CAF´E.",
                "The NDCU for each system run is calculated automatically.",
                "User feedback was also simulated - relevance judgments for each system-returned passage (as determined by the nugget matching rules described in section 4.1.2) were 4 URL: http://nyc.lti.cs.cmu.edu/downloads Figure 1: Performance of Indri across chunks Figure 2: Performance of CAF´E across chunks used as user feedback in the adaptation of query profiles and user histories. 6.3 Results In Table 1, we show the NDCU scores of the two systems under various settings.",
                "These scores are averaged over the six tasks in the test set, and are calculated with two dampening factors (see section 4.2): γ = 0 and 0.1, to simulate no tolerance and small tolerance for redundancy, respectively.",
                "Using γ = 0 creates a much more strict metric since it does not give any credit to a passage that contains relevant but redundant information.",
                "Hence, the improvement obtained from enabling user feedback is smaller with γ = 0 than the improvement obtained from feedback with γ = 0.1.",
                "This reveals a shortcoming of contemporary retrieval systemswhen the user gives positive feedback on a passage, the systems gives higher weights to the terms present in that passage and tends to retrieve other passages containing the same terms - and thus - usually the same information.",
                "However, the user does not benefit from seeing such redundant passages, and is usually interested in other passages containing related information.",
                "It is informative to evaluate retrieval systems using our utility measure (with γ = 0) which accounts for novelty and thus gives a more realistic picture of how well a system can generalize from user feedback, rather than using traditional IR measures like recall and precision which give an incomplete picture of improvement obtained from user feedback.",
                "Sometimes, however, users might indeed be interested in seeing the same information from multiple sources, as an Table 1: NDCU Scores of Indri and CAF´E for two dampening factors (γ), and various settings (F: Feedback, N: Novelty Detection, A: Anti-Redundant Ranking) Indri CAF´E γ Base +PRF Base +F +F+N +F+A +F+N+A 0 0.19 0.19 0.22 0.23 0.24 0.24 0.24 0.1 0.28 0.29 0.24 0.35 0.35 0.36 0.36 indicator of its importance or reliability.",
                "In such a case, we can simply choose a higher value for γ which corresponds to a higher tolerance for redundancy, and hence let the system tune its parameters accordingly.",
                "Since documents were processed chunk by chunk, it would be interesting to see how the performance of systems improves over time.",
                "Figures 1 and 2 show the performance trends for both the systems across chunks.",
                "While the performance with and without feedback on the first few chunks is expected to be close, for subsequent chunks, the performance curve with feedback enabled rises above the one with the no-feedback setting.",
                "The performance trends are not consistent across all chunks because on-topic documents are not uniformly distributed over all the chunks, making some queries easier than others in certain chunks.",
                "Moreover, since Indri uses pseudo-relevance feedback while CAF´E uses feedback based on actual relevance judgments, the improvement in case of Indri is less dramatic than that of CAF´E. 7.",
                "CONCLUDING REMARKS This paper presents the first investigation on utility-based information distillation with a system that learns the longlasting information needs from fine-grained user feedback over a sequence of ranked passages.",
                "Our system, called CAF´E, combines adaptive filtering, novelty detection and antiredundant passage ranking in a unified framework for utility optimization.",
                "We developed a new scheme for automated evaluation and feedback based on a semi-automatic procedure for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also proposed an extension of the NDCG metric for assessing the utility of ranked passages as a weighted combination of relevance and novelty.",
                "Our experiments on the newly annotated TDT4 benchmark corpus show encouraging utility enhancement over Indri, and also over our own system with incremental learning and novelty detection turned off. 8.",
                "ACKNOWLEDGMENTS We would like to thank Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng, and the Qualitative Data Analysis Program at the University of Pittsburgh lead by Dr. Stuart Shulman for their help with collecting and processing the extended TDT4 annotations used in our experiments.",
                "This work is supported in parts by the National Science Foundation (NSF) under grant IIS0434035, and the Defense Advanced Research Project Agency (DARPA) under contracts NBCHD030010 and W0550432.",
                "Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors. 9.",
                "ADDITIONAL AUTHORS Jian Zhang (jianzhan@stat.purdue.edu)∗ , Jaime Carbonell (jgc@cs.cmu.edu)† , Peter Brusilovsky (peterb+@pitt.edu)‡ , Daqing He(dah44@pitt.edu)‡ 10.",
                "REFERENCES [1] J. Allan.",
                "Incremental Relevance Feedback for Information Filtering.",
                "Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, pages 270-278, 1996. [2] J. Allan, C. Wade, and A. Bolivar.",
                "Retrieval and Novelty Detection at the Sentence Level.",
                "Proceedings of the ACM SIGIR conference on research and development in information retrieval, 2003. [3] C. Buckley, G. Salton, and J. Allan.",
                "Automatic Retrieval with Locality Information using SMART.",
                "NIST special publication, (500207):59-72, 1993. [4] J. Callan.",
                "Learning While Filtering Documents.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 224-231, 1998. [5] J. Carbonell and J. Goldstein.",
                "The use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 335-336, 1998. [6] E. Efthimiadis.",
                "Query Expansion.",
                "Annual Review of Information Science and Technology (ARIST), 31:p121-87, 1996. [7] J. Fiscus and G. Duddington.",
                "Topic Detection and Tracking Overview.",
                "Topic Detection and Tracking: Event-based Information Organization, pages 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos.",
                "A Statistical Model for Multilingual Entity Detection and Tracking.",
                "NAACL/HLT, 2004. [9] K. J¨arvelin and J. Kek¨al¨ainen.",
                "Cumulated Gain-based Evaluation of IR Techniques.",
                "ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin and D. Demner-Fushman.",
                "Automatically Evaluating Answers to Definition Questions.",
                "Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), 2005. ∗ Statistics Dept., Purdue University, West Lafayette, USA † Language Technologies Inst., Carnegie Mellon University, Pittsburgh, USA ‡ School of Information Sciences, Univ. of Pittsburgh, Pittsburgh, USA [11] J. Lin and D. Demner-Fushman.",
                "Will Pyramids Built of nUggets Topple Over.",
                "Proceedings of HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos.",
                "A Mention-synchronous Coreference Resolution Algorithm based on the Bell Tree.",
                "Proc. of ACL, 4:136-143, 2004. [13] G. Marton.",
                "Nuggeteer: Automatic Nugget-Based Evaluation Using Descriptions and Judgments.",
                "HLT/NAACL, 2006. [14] E. Riloff.",
                "Automatically Constructing a Dictionary for Information Extraction Tasks.",
                "Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 811-816, 1993. [15] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9: Filtering track.",
                "The Ninth Text REtrieval Conference (TREC-9), pages 361-368. [16] R. Schapire, Y.",
                "Singer, and A. Singhal.",
                "Boosting and Rocchio Applied to Text Filtering.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle, and W. Croft.",
                "Indri: A Language Model-based Search Engine for Complex Queries.",
                "Proceedings of the International Conference on Intelligence Analysis, 2004. [18] The Linguistic Data Consortium. http://www.ldc.upenn.edu/. [19] E. Voorhees.",
                "Overview of the TREC 2003 Question Answering Track.",
                "Proceedings of the Twelfth Text REtrieval Conference (TREC 2003), 2003. [20] Y. Yang and B. Kisiel.",
                "Margin-based Local Regression for Adaptive Filtering.",
                "Proceedings of the twelfth international conference on Information and knowledge management, pages 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of Adaptive Filtering Methods in a Cross-benchmark Evaluation.",
                "Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 98-105, 2005. [22] C. Zhai, W. Cohen, and J. Lafferty.",
                "Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 10-17, 2003. [23] J. Zhang and Y. Yang.",
                "Robustness of Regularized Linear Classification Methods in Text Categorization.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 190-197, 2003. [24] Y. Zhang.",
                "Using Bayesian Priors to Combine Classifiers for Adaptive Filtering.",
                "Proceedings of the 27th annual international conference on Research and development in information retrieval, pages 345-352, 2004. [25] Y. Zhang, J. Callan, and T. Minka.",
                "Novelty and Redundancy Detection in Adaptive Filtering.",
                "Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Sin embargo, también está plagado de relevancia espuria, ya que no todas las palabras contenidas en la descripción de la pepita (o respuestas correctas conocidas) son fundamentales para la pepita.4.1.",
                "Por lo tanto, podemos crear \"reglas de combate de pepitas\" que capturen sucintamente varias formas de expresar una pepita, mientras evitan las respuestas incorrectas (o fuera de contexto) coincidentes."
            ],
            "translated_text": "",
            "candidates": [
                "regla de coincidencia de pepitas",
                "regla de coincidencia de pepitas",
                "reglas de combate de pepitas"
            ],
            "error": []
        },
        "unified framework": {
            "translated_key": "marco unificado",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Utility-based Information Distillation Over Temporally Sequenced Documents Yiming Yang Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA yiming@cs.cmu.edu Abhimanyu Lad Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA alad@cs.cmu.edu Ni Lao Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA nlao@cs.cmu.edu Abhay Harpale Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA aharpale@cs.cmu.edu Bryan Kisiel Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA bkisiel@cs.cmu.edu Monica Rogati Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA mrogati@cs.cmu.edu ABSTRACT This paper examines a new approach to information distillation over temporally ordered documents, and proposes a novel evaluation scheme for such a framework.",
                "It combines the strengths of and extends beyond conventional adaptive filtering, novelty detection and non-redundant passage ranking with respect to long-lasting information needs (tasks with multiple queries).",
                "Our approach supports fine-grained user feedback via highlighting of arbitrary spans of text, and leverages such information for utility optimization in adaptive settings.",
                "For our experiments, we defined hypothetical tasks based on news events in the TDT4 corpus, with multiple queries per task.",
                "Answer keys (nuggets) were generated for each query and a semiautomatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also propose an extension of the NDCG metric for assessing the utility of ranked passages as a combination of relevance and novelty.",
                "Our results show encouraging utility enhancements using the new approach, compared to the baseline systems without incremental learning or the novelty detection components.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 General Terms Design, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Tracking new and relevant information from temporal data streams for users with long-lasting needs has been a challenging research topic in information retrieval.",
                "Adaptive filtering (AF) is one such task of online prediction of the relevance of each new document with respect to pre-defined topics.",
                "Based on the initial query and a few positive examples (if available), an AF system maintains a profile for each such topic of interest, and constantly updates it based on feedback from the user.",
                "The incremental learning nature of AF systems makes them more powerful than standard search engines that support ad-hoc retrieval (e.g.",
                "Google and Yahoo) in terms of finding relevant information with respect to long-lasting topics of interest, and more attractive for users who are willing to provide feedback to adapt the system towards their specific information needs, without having to modify their queries manually.",
                "A variety of supervised learning algorithms (Rocchio-style classifiers, Exponential-Gaussian models, local regression and logistic regression approaches) have been studied for adaptive settings, examined with explicit and implicit relevance feedback, and evaluated with respect to utility optimization on large benchmark data collections in TREC (Text Retrieval Conferences) and TDT (Topic Detection and Tracking) forums [1, 4, 7, 15, 16, 20, 24, 23].",
                "Regularized logistic regression [21] has been found representative for the state-of-the-art approaches, and highly efficient for frequent model adaptations over large document collections such as the TREC-10 corpus (over 800,000 documents and 84 topics).",
                "Despite substantial achievements in recent adaptive filtering research, significant problems remain unsolved regarding how to leverage user feedback effectively and efficiently.",
                "Specifically, the following issues may seriously limit the true utility of AF systems in real-world applications: 1.",
                "User has a rather passive role in the conventional adaptive filtering setup - he or she reacts to the system only when the system makes a yes decision on a document, by confirming or rejecting that decision.",
                "A more active alternative would be to allow the user to issue multiple queries for a topic, review a ranked list of candidate documents (or passages) per query, and provide feedback on the ranked list, thus refining their information need and requesting updated ranked lists.",
                "The latter form of user interaction has been highly effective in standard retrieval for ad-hoc queries.",
                "How to deploy such a strategy for long-lasting information needs in AF settings is an open question for research. 2.",
                "The unit for receiving a relevance judgment (yes or no) is restricted to the document level in conventional AF.",
                "However, a real user may be willing to provide more informative, fine-grained feedback via highlighting some pieces of text in a retrieved document as relevant, instead of labeling the entire document as relevant.",
                "Effectively leveraging such fine-grained feedback could substantially enhance the quality of an AF system.",
                "For this, we need to enable supervised learning from labeled pieces of text of arbitrary span instead of just allowing labeled documents. 3.",
                "System-selected documents are often highly redundant.",
                "A major news event, for example, would be reported by multiple sources repeatedly for a while, making most of the information content in those articles redundant with each other.",
                "A conventional AF system would select all these redundant news stories for user feedback, wasting the users time while offering little gain.",
                "Clearly, techniques for novelty detection can help in principle [25, 2, 22] for improving the utility of the AF systems.",
                "However, the effectiveness of such techniques at passage level to detect novelty with respect to users (fine-grained) feedback and to detect redundancy in ranked lists remains to be evaluated using a measure of utility that mimics the needs of a real user.",
                "To address the above limitations of current AF systems, we propose and examine a new approach in this paper, combining the strengths of conventional AF (incremental learning of topic models), multi-pass passage retrieval for long-lasting queries conditioned on topic, and novelty detection for removal of redundancy from user interactions with the system.",
                "We call the new process utility-based information distillation.",
                "Note that conventional benchmark corpora for AF evaluations, which have relevance judgments at the document level and do not define tasks with multiple queries, are insufficient for evaluating the new approach.",
                "Therefore, we extended a benchmark corpus - the TDT4 collection of news stories and TV broadcasts - with task definitions, multiple queries per task, and answer keys per query.",
                "We have conducted our experiments on this extended TDT4 corpus and have made the additionally generated data publicly available for future comparative evaluations 1 .",
                "To automatically evaluate the system-returned arbitrary spans of text using our answer keys, we further developed an evaluation scheme with semi-automatic procedure for 1 URL: http://nyc.lti.cs.cmu.edu/downloads acquiring rules that can match nuggets against system responses.",
                "Moreover, we propose an extension of NDCG (Normalized Discounted Cumulated Gain) [9] for assessing the utility of ranked passages as a function of both relevance and novelty.",
                "The rest of this paper is organized as follows.",
                "Section 2 outlines the information distillation process with a concrete example.",
                "Section 3 describes the technical cores of our system called CAF´E - CMU Adaptive Filtering Engine.",
                "Section 4 discusses issues with respect to evaluation methodology and proposes a new scheme.",
                "Section 5 describes the extended TDT4 corpus.",
                "Section 6 presents our experiments and results.",
                "Section 7 concludes the study and gives future perspectives. 2.",
                "A SAMPLE TASK Consider a news event - the escape of seven convicts from a Texas prison in December 2000 and their capture a month later.",
                "Assuming a user were interested in this event since its early stage, the information need could be: Find information about the escape of convicts from Texas prison, and information related to their recapture.",
                "The associated lower-level questions could be: 1.",
                "How many prisoners escaped? 2.",
                "Where and when were they sighted? 3. Who are their known contacts inside and outside the prison? 4.",
                "How are they armed? 5.",
                "Do they have any vehicles? 6.",
                "What steps have been taken so far?",
                "We call such an information need a task, and the associated questions as the queries in this task.",
                "A distillation system is supposed to monitor the incoming documents, process them chunk by chunk in a temporal order, select potentially relevant and novel passages from each chunk with respect to each query, and present a ranked list of passages to the user.",
                "Passage ranking here is based on how relevant a passage is with respect to the current query, how novel it is with respect to the current user history (of his or her interactions with the system), and how redundant it is compared to other passages with a higher rank in the list.",
                "When presented with a list of passages, the user may provide feedback by highlighting arbitrary spans of text that he or she found relevant.",
                "These spans of text are taken as positive examples in the adaptation of the query profile, and also added to the users history.",
                "Passages not marked by the user are taken as negative examples.",
                "As soon as the query profile is updated, the system re-issues a search and returns another ranked list of passages where the previously seen passages are either removed or ranked low, based on user preference.",
                "For example, if the user highlights ...officials have posted a $100,000 reward for their capture... as relevant answer to the query What steps have been taken so far?, then the highlighted piece is used as an additional positive training example in the adaptation of the query profile.",
                "This piece of feedback is also added to the user history as a seen example, so that in future, the system will not place another passage mentioning $100,000 reward at the top of the ranked list.",
                "However, an article mentioning ...officials have doubled the reward money to $200,000... might be ranked high since it is both relevant to the (updated) query profile and novel with respect to the (updated) user history.",
                "The user may modify the original queries or add a new query during the process; the query profiles will be changed accordingly.",
                "Clearly, novelty detection is very important for the utility of such a system because of the iterative search.",
                "Without novelty detection, the old relevant passages would be shown to the user repeatedly in each ranked list.",
                "Through the above example, we can see the main properties of our new framework for utility-based information distillation over temporally ordered documents.",
                "Our framework combines and extends the power of adaptive filtering (AF), ad-hoc retrieval (IR) and novelty detection (ND).",
                "Compared to standard IR, our approach has the power of incrementally learning long-term information needs and modeling a sequence of queries within a task.",
                "Compared to conventional AF, it enables a more active role of the user in refining his or her information needs and requesting new results by allowing relevance and novelty feedback via highlighting of arbitrary spans of text in passages returned by the system.",
                "Compared to past work, this is the first evaluation of novelty detection integrated with adaptive filtering for sequenced queries that allows flexible user feedback over ranked passages.",
                "The combination of AF, IR and ND with the new extensions raises an important research question regarding evaluation methodology: how can we measure the utility of such an information distillation system?",
                "Existing metrics in standard IR, AF and ND are insufficient, and new solutions must be explored, as we will discuss in Section 4, after describing the technical cores of our system in the next section. 3.",
                "TECHNICAL CORES The core components of CAF´E are - 1) AF for incremental learning of query profiles, 2) IR for estimating relevance of passages with respect to query profiles, 3) ND for assessing novelty of passages with respect to users history, and 4) anti-redundancy component to remove redundancy from ranked lists. 3.1 Adaptive Filtering Component We use a state-of-the-art algorithm in the field - the regularized logistic regression method which had the best results on several benchmark evaluation corpora for AF [21].",
                "Logistic regression (LR) is a supervised learning algorithm for statistical classification.",
                "Based on a training set of labeled instances, it learns a class model which can then by used to predict the labels of unseen instances.",
                "Its performance as well as efficiency in terms of training time makes it a good candidate when frequent updates of the class model are required, as is the case in adaptive filtering, where the system must learn from each new feedback provided by the user. (See [21] and [23] for computational complexity and implementation issues).",
                "In adaptive filtering, each query is considered as a class and the probability of a passage belonging to this class corresponds to the degree of relevance of the passage with respect to the query.",
                "For training the model, we use the query itself as the initial positive training example of the class, and the user-highlighted pieces of text (marked as Relevant or Not-relevant) during feedback as additional training examples.",
                "To address the cold start issue in the early stage before any user feedback is obtained, the system uses a small sample from a retrospective corpus as the initial negative examples in the training set.",
                "The details of using logistic regression for adaptive filtering (assigning different weights to positive and negative training instances, and regularizing the objective function to prevent over-fitting on training data) are presented in [21].",
                "The class model w∗ learned by Logistic Regression, or the query profile, is a vector whose dimensions are individual terms and whose elements are the regression coefficients, indicating how influential each term is in the query profile.",
                "The query profile is updated whenever a new piece of user feedback is received.",
                "A temporally decaying weight can be applied to each training example, as an option, to emphasize the most recent user feedback. 3.2 Passage Retrieval Component We use standard IR techniques in this part of our system.",
                "Incoming documents are processed in chunks, where each chunk can be defined as a fixed span of time or as a fixed number of documents, as preferred by the user.",
                "For each incoming document, corpus statistics like the IDF (Inverted Document Frequency) of each term are updated.",
                "We use a state-of-the-art named entity identifier and tracker [8, 12] to identify person and location names, and merge them with co-referent named entities seen in the past.",
                "Then the documents are segmented into passages, which can be a whole document, a paragraph, a sentence, or any other continuous span of text, as preferred.",
                "Each passage is represented using a vector of TF-IDF (Term FrequencyInverse Document Frequency) weights, where term can be a word or a named entity.",
                "Given a query profile, i.e. the logistic regression solution w∗ as described in Section 3.1, the system computes the posterior probability of relevance for each passage x as fRL(x) ≡ P(y = 1|x, w∗ ) = 1 (1 + e−w∗·x) (1) Passages are ordered by their relevance scores, and the ones with scores above a threshold (tuned on a training set) comprise the relevance list that is passed on to the novelty detection step. 3.3 Novelty Detection Component CAF´E maintains a user history H(t), which contains all the spans of text hi that the user highlighted (as feedback) during his or her past interactions with the system, up to the current time t. Denoting the history as H(t) = n h1, h2, ..., ht o , (2) the novelty score of a new candidate passage x is computed as: fND(x) = 1 − max i∈1..t {cos(x, hi)} (3) where both candidate passage x and highlighted spans of text hi are represented as TF-IDF vectors.",
                "The novelty score of each passage is compared to a prespecified threshold (also tuned on a training set), and any passage with a score below this threshold is removed from the relevance list. 3.4 Anti-redundant Ranking Component Although the novelty detection component ensures that only novel (previously unseen) information remains in the relevance list, this list might still contain the same novel information at multiple positions in the ranked list.",
                "Suppose, for example, that the user has already read about a $100,000 reward for information about the escaped convicts.",
                "A new piece of news that the award has been increased to $200,000 is novel since the user hasnt read about it yet.",
                "However, multiple news sources would report this news and we might end up showing (redundant) articles from all these sources in a ranked list.",
                "Hence, a ranked list should also be made non-redundant with respect to its own contents.",
                "We use a simplified version of the Maximal Marginal Relevance method [5], originally developed for combining relevance and novelty in text retrieval and summarization.",
                "Our procedure starts with the current list of passages sorted by relevance (section 3.2), filtered by Novelty Detection component (section 3.3), and generates a new non-redundant list as follows: 1.",
                "Take the top passage in the current list as the top one in the new list. 2.",
                "Add the next passage x in the current list to the new list only if fAR(x) > t where fAR(x) = 1 − max pi∈Lnew {cos(x, pi)} and Lnew is the set of passages already selected in the new list. 3.",
                "Repeat step 2 until all the passages in the current list have been examined.",
                "After applying the above-mentioned algorithm, each passage in the new list is sufficiently dissimilar to others, thus favoring diversity rather than redundancy in the new ranked list.",
                "The anti-redundancy threshold t is tuned on a training set. 4.",
                "EVALUATION METHODOLOGY The approach we proposed above for information distillation raises important issues regarding evaluation methodology.",
                "Firstly, since our framework allows the output to be passages at different levels of granularity (e.g. k-sentence windows where k may vary) instead of a fixed length, it is not possible to have pre-annotated relevance judgments at all such granularity levels.",
                "Secondly, since we wish to measure the utility of the system output as a combination of both relevance and novelty, traditional relevance-only based measures must be replaced by measures that penalize the repetition of the same information in the system output across time.",
                "Thirdly, since the output of the system is ranked lists, we must reward those systems that present useful information (both relevant and previously unseen) using shorter ranked lists, and penalize those that present the same information using longer ranked lists.",
                "None of the existing measures in ad-hoc retrieval, adaptive filtering, novelty detection or other related areas (text summarization and question answering) have desirable properties in all the three aspects.",
                "Therefore, we must develop a new evaluation methodology. 4.1 Answer Keys To enable the evaluation of a system whose output consists of passages of arbitrary length, we borrow the concept of answer keys from the Question Answering (QA) community, where systems are allowed to return arbitrary spans of text as answers.",
                "Answer keys define what should be present in a system response to receive credit, and are comprised of a collection of information nuggets, i.e. factoid units about which human assessors can make binary decisions of whether or not a system response contains them.",
                "Defining answer keys and making the associated binary decisions are conceptual tasks that require semantic mapping [19], since system-returned passages can contain the same information expressed in many different ways.",
                "Hence, QA evaluations have relied on human assessors for the mapping between various expressions, making the process costly, time consuming, and not scalable to large query and document collections, and extensive system evaluations with various parameter settings. 4.1.1 Automating Evaluation based on Answer Keys Automatic evaluation methods would allow for faster system building and tuning, as well as provide an objective and affordable way of comparing various systems.",
                "Recently, such methods have been proposed, more or less, based on the idea of n-gram co-occurrences.",
                "Pourpre [10] assigns a fractional recall score to a system response based on its unigram overlap with a given nuggets description.",
                "For example, a system response A B C has recall 3/4 with respect to a nugget with description A B C D. However, such an approach is unfair to systems that present the same information but using words other than A, B, C, and D. Another open issue is how to weight individual words in measuring the closeness of a match.",
                "For example, consider the question How many prisoners escaped?.",
                "In the nugget Seven prisoners escaped from a Texas prison, there is no indication that seven is the keyword, and that it must be matched to get any relevance credit.",
                "Using IDF values does not help, since seven will generally not have a higher IDF than words like texas and prison.",
                "Also, redefining the nugget as just seven does not solve the problem since now it might spuriously match any mention of seven out of context.",
                "Nuggeteer [13] works on similar principles but makes binary decisions about whether a nugget is present in a given system response by tuning a threshold.",
                "However, it is also plagued by spurious relevance since not all words contained in the nugget description (or known correct responses) are central to the nugget. 4.1.2 Nugget-Matching Rules We propose a reliable automatic method for determining whether a snippet of text contains a given nugget, based on nugget-matching rules, which are generated using a semiautomatic procedure explained below.",
                "These rules are essentially Boolean queries that will only match against snippets that contain the nugget.",
                "For instance, a candidate rule for matching answers to How many prisoners escaped? is (Texas AND seven AND escape AND (convicts OR prisoners)), possibly with other synonyms and variants in the rule.",
                "For a corpus of news articles, which usually follow a typical formal prose, it is fairly easy to write such simple rules to match expected answers using a bootstrap approach, as described below.",
                "We propose a two-stage approach, inspired by Autoslog [14], that combines the strength of humans in identifying semantically equivalent expressions and the strength of the system in gathering statistical evidence from a humanannotated corpus of documents.",
                "In the first stage, human subjects annotated (using a highlighting tool) portions of ontopic documents that contained answers to each nugget 2 .",
                "In the second stage, subjects used our rule generation tool to create rules that would match the annotations for each nugget.",
                "The tool allows users to enter a Boolean rule as a disjunction of conjunctions (e.g. ((a AND b) OR (a AND c AND d) OR (e))).",
                "Given a candidate rule, our tool uses it as a Boolean query over the entire set of on-topic documents and calculates its recall and precision with respect to the annotations that it is expected to match.",
                "Hence, the subjects can start with a simple rule and iteratively refine it until they are satisfied with its recall and precision.",
                "We observed that it was very easy for humans to improve the precision of a rule by tweaking its existing conjunctions (adding more ANDs), and improving the recall by adding more conjunctions to the disjunction (adding more ORs).",
                "As an example, lets try to create a rule for the nugget which says that seven prisoners escaped from the Texas prison.",
                "We start with a simple rule - (seven).",
                "When we input this into the rule generation tool, we realize that this rule matches many spurious occurrences of seven (e.g. ...seven states...) and thus gets a low precision score.",
                "We can further qualify our rule - Texas AND seven AND convicts.",
                "Next, by looking at the missed annotations, we realize that some news articles mentioned ...seven prisoners escaped.... We then replace convicts with the disjunction (convicts OR prisoners).",
                "We continue tweaking the rule in this manner until we achieve a sufficiently high recall and precision - i.e. the (small number of) misses and false alarms can be safely ignored.",
                "Thus we can create nugget-matching rules that succinctly capture various ways of expressing a nugget, while avoiding matching incorrect (or out of context) responses.",
                "Human involvement in the rule creation process ensures high quality generic rules which can then be used to evaluate arbitrary system responses reliably. 4.2 Evaluating the Utility of a Sequence of Ranked Lists The utility of a retrieval system can be defined as the difference between how much the user gained in terms of useful information, and how much the user lost in terms of time and energy.",
                "We calculate this utility from the utilities of individual passages as follows.",
                "After reading each passage returned by the system, the user derives some gain depending on the presence of relevant and novel information, and incurs a loss in terms of the time and energy spent in going through the passage.",
                "However, the likelihood that the user would actually read a passage depends on its position in the ranked list.",
                "Hence, for a query q, the expected utility 2 LDC [18] already provides relevance judgments for 100 topics on the TDT4 corpus.",
                "We further ensured that these judgments are exhaustive on the entire corpus using pooling. of a passage pi at rank i can be defined as U(pi, q) = P(i) ∗ (Gain(pi, q) − Loss(pi, q)) (4) where P(i) is the probability that the user would go through a passage at rank i.",
                "The expected utility for an entire ranked list of length n can be calculated simply by adding the expected utility of each passage: U(q) = nX i=1 P(i) ∗ (Gain(pi, q) − Loss(pi, q)) (5) Note that if we ignore the loss term and define P(i) as P(i) ∝ 1/ logb(b + i − 1) (6) then we get the recently popularized metric called Discounted Cumulated Gain (DCG) [9], where Gain(pi, q) is defined as the graded relevance of passage pi.",
                "However, without the loss term, DCG is a purely recall-oriented metric and not suitable for an adaptive filtering setting, where the systems utility depends in part on its ability to limit the number of items shown to the user.",
                "Although P(i) could be defined based on empirical studies of user behavior, for simplicity, we use P(i) exactly as defined in equation 6.",
                "The gain G(pi, q) of passage pi with respect to the query q is a function of - 1) the number of relevant nuggets present in pi, and 2) the novelty of each of these nuggets.",
                "We combine these two factors as follows.",
                "For each nugget Nj, we assign an initial weight wj, and also keep a count nj of the number of times this nugget has been seen by the user in the past.",
                "The gain derived from each subsequent occurrence of the same nugget is assumed to reduce by a dampening factor γ.",
                "Thus, G(pi, q) is defined as G(pi, q) = X Nj ∈C(pi,q) wj ∗ γnj (7) where C(pi, q) is the set of all nuggets that appear in passage pi and also belong to the answer key of query q.",
                "The initial weights wj are all set of be 1.0 in our experiments, but can also be set based on a pyramid approach [11].",
                "The choice of dampening factor γ determines the users tolerance for redundancy.",
                "When γ = 0, a nugget will only receive credit for its first occurrence i.e. when nj is zero3 .",
                "For 0 < γ < 1, a nugget receives smaller credit for each successive occurrence.",
                "When γ = 1, no dampening occurs and repeated occurrences of a nugget receive the same credit.",
                "Note that the nugget occurrence counts are preserved between evaluation of successive ranked lists returned by the system, since the users are expected to remember what the system showed them in the past.",
                "We define the loss L(pi, q) as a constant cost c (we use 0.1) incurred when reading a system-returned passage.",
                "Thus, our metric can be re-written as U(q) = nX i=1 Gain(pi, q) logb(b + i − 1) − L(n) (8) where L(n) is the loss associated with a ranked list of length n: L(n) = c · nX i=1 1 logb(b + i − 1) (9) 3 Note that 00 = 1 Due to the similarity with Discounted Cumulated Gain (DCG), we call our metric Discounted Cumulated Utility (DCU).",
                "The DCU score obtained by the system is converted to a Normalized DCU (NDCU) score by dividing it by the DCU score of the ideal ranked list, which is created by ordering passages by their decreasing utility scores U(pi, q) and stopping when U(pi, q) ≤ 0 i.e. when the gain is less than or equal to the cost of reading the passage. 5.",
                "DATA TDT4 was the benchmark corpus used in TDT2002 and TDT2003 evaluations.",
                "The corpus consists of over 90, 000 news articles from multiple sources (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) published between October 2000 and January 2001, in the languages of Arabic, English, and Mandarin.",
                "Speech-recognized and machine-translated versions of the non-English articles were provided as well.",
                "LDC [18] has annotated the corpus with 100 topics, that correspond to various news events in this time period.",
                "Out of these, we selected a subset of 12 actionable events, and defined corresponding tasks for them4 .",
                "For each task, we manually defined a profile consisting of an initial set of (5 to 10) queries, a free-text description of the user history, i.e., what the user already knows about the event, and a list of known on-topic and off-topic documents (if available) as training examples.",
                "For each query, we generated answer keys and corresponding nugget matching rules using the procedure described in section 4.1.2, and produced a total of 120 queries, with an average of 7 nuggets per query. 6.",
                "EXPERIMENTS AND RESULTS 6.1 Baselines We used Indri [17], a popular language-model based retrieval engine, as a baseline for comparison with CAF´E.",
                "Indri supports standard search engine functionality, including pseudo-relevance feedback (PRF) [3, 6], and is representative of a typical query-based retrieval system.",
                "Indri does not support any kind of novelty detection.",
                "We compare Indri with PRF turned on and off, against CAF´E with user feedback, novelty detection and antiredundant ranking turned on and off. 6.2 Experimental Setup We divided the TDT4 corpus spanning 4 months into 10 chunks, each defined as a period of 12 consecutive days.",
                "At any given point of time in the distillation process, each system accessed the past data up to the current point, and returned a ranked list of up 50 passages per query.",
                "The 12 tasks defined on the corpus were divided into a training and test set with 6 tasks each.",
                "Each system was allowed to use the training set to tune its parameters for optimizing NDCU (equation 8), including the relevance threshold for both Indri and CAF´E, and the novelty and antiredundancy thresholds for CAF´E.",
                "The NDCU for each system run is calculated automatically.",
                "User feedback was also simulated - relevance judgments for each system-returned passage (as determined by the nugget matching rules described in section 4.1.2) were 4 URL: http://nyc.lti.cs.cmu.edu/downloads Figure 1: Performance of Indri across chunks Figure 2: Performance of CAF´E across chunks used as user feedback in the adaptation of query profiles and user histories. 6.3 Results In Table 1, we show the NDCU scores of the two systems under various settings.",
                "These scores are averaged over the six tasks in the test set, and are calculated with two dampening factors (see section 4.2): γ = 0 and 0.1, to simulate no tolerance and small tolerance for redundancy, respectively.",
                "Using γ = 0 creates a much more strict metric since it does not give any credit to a passage that contains relevant but redundant information.",
                "Hence, the improvement obtained from enabling user feedback is smaller with γ = 0 than the improvement obtained from feedback with γ = 0.1.",
                "This reveals a shortcoming of contemporary retrieval systemswhen the user gives positive feedback on a passage, the systems gives higher weights to the terms present in that passage and tends to retrieve other passages containing the same terms - and thus - usually the same information.",
                "However, the user does not benefit from seeing such redundant passages, and is usually interested in other passages containing related information.",
                "It is informative to evaluate retrieval systems using our utility measure (with γ = 0) which accounts for novelty and thus gives a more realistic picture of how well a system can generalize from user feedback, rather than using traditional IR measures like recall and precision which give an incomplete picture of improvement obtained from user feedback.",
                "Sometimes, however, users might indeed be interested in seeing the same information from multiple sources, as an Table 1: NDCU Scores of Indri and CAF´E for two dampening factors (γ), and various settings (F: Feedback, N: Novelty Detection, A: Anti-Redundant Ranking) Indri CAF´E γ Base +PRF Base +F +F+N +F+A +F+N+A 0 0.19 0.19 0.22 0.23 0.24 0.24 0.24 0.1 0.28 0.29 0.24 0.35 0.35 0.36 0.36 indicator of its importance or reliability.",
                "In such a case, we can simply choose a higher value for γ which corresponds to a higher tolerance for redundancy, and hence let the system tune its parameters accordingly.",
                "Since documents were processed chunk by chunk, it would be interesting to see how the performance of systems improves over time.",
                "Figures 1 and 2 show the performance trends for both the systems across chunks.",
                "While the performance with and without feedback on the first few chunks is expected to be close, for subsequent chunks, the performance curve with feedback enabled rises above the one with the no-feedback setting.",
                "The performance trends are not consistent across all chunks because on-topic documents are not uniformly distributed over all the chunks, making some queries easier than others in certain chunks.",
                "Moreover, since Indri uses pseudo-relevance feedback while CAF´E uses feedback based on actual relevance judgments, the improvement in case of Indri is less dramatic than that of CAF´E. 7.",
                "CONCLUDING REMARKS This paper presents the first investigation on utility-based information distillation with a system that learns the longlasting information needs from fine-grained user feedback over a sequence of ranked passages.",
                "Our system, called CAF´E, combines adaptive filtering, novelty detection and antiredundant passage ranking in a <br>unified framework</br> for utility optimization.",
                "We developed a new scheme for automated evaluation and feedback based on a semi-automatic procedure for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also proposed an extension of the NDCG metric for assessing the utility of ranked passages as a weighted combination of relevance and novelty.",
                "Our experiments on the newly annotated TDT4 benchmark corpus show encouraging utility enhancement over Indri, and also over our own system with incremental learning and novelty detection turned off. 8.",
                "ACKNOWLEDGMENTS We would like to thank Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng, and the Qualitative Data Analysis Program at the University of Pittsburgh lead by Dr. Stuart Shulman for their help with collecting and processing the extended TDT4 annotations used in our experiments.",
                "This work is supported in parts by the National Science Foundation (NSF) under grant IIS0434035, and the Defense Advanced Research Project Agency (DARPA) under contracts NBCHD030010 and W0550432.",
                "Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors. 9.",
                "ADDITIONAL AUTHORS Jian Zhang (jianzhan@stat.purdue.edu)∗ , Jaime Carbonell (jgc@cs.cmu.edu)† , Peter Brusilovsky (peterb+@pitt.edu)‡ , Daqing He(dah44@pitt.edu)‡ 10.",
                "REFERENCES [1] J. Allan.",
                "Incremental Relevance Feedback for Information Filtering.",
                "Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, pages 270-278, 1996. [2] J. Allan, C. Wade, and A. Bolivar.",
                "Retrieval and Novelty Detection at the Sentence Level.",
                "Proceedings of the ACM SIGIR conference on research and development in information retrieval, 2003. [3] C. Buckley, G. Salton, and J. Allan.",
                "Automatic Retrieval with Locality Information using SMART.",
                "NIST special publication, (500207):59-72, 1993. [4] J. Callan.",
                "Learning While Filtering Documents.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 224-231, 1998. [5] J. Carbonell and J. Goldstein.",
                "The use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 335-336, 1998. [6] E. Efthimiadis.",
                "Query Expansion.",
                "Annual Review of Information Science and Technology (ARIST), 31:p121-87, 1996. [7] J. Fiscus and G. Duddington.",
                "Topic Detection and Tracking Overview.",
                "Topic Detection and Tracking: Event-based Information Organization, pages 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos.",
                "A Statistical Model for Multilingual Entity Detection and Tracking.",
                "NAACL/HLT, 2004. [9] K. J¨arvelin and J. Kek¨al¨ainen.",
                "Cumulated Gain-based Evaluation of IR Techniques.",
                "ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin and D. Demner-Fushman.",
                "Automatically Evaluating Answers to Definition Questions.",
                "Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), 2005. ∗ Statistics Dept., Purdue University, West Lafayette, USA † Language Technologies Inst., Carnegie Mellon University, Pittsburgh, USA ‡ School of Information Sciences, Univ. of Pittsburgh, Pittsburgh, USA [11] J. Lin and D. Demner-Fushman.",
                "Will Pyramids Built of nUggets Topple Over.",
                "Proceedings of HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos.",
                "A Mention-synchronous Coreference Resolution Algorithm based on the Bell Tree.",
                "Proc. of ACL, 4:136-143, 2004. [13] G. Marton.",
                "Nuggeteer: Automatic Nugget-Based Evaluation Using Descriptions and Judgments.",
                "HLT/NAACL, 2006. [14] E. Riloff.",
                "Automatically Constructing a Dictionary for Information Extraction Tasks.",
                "Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 811-816, 1993. [15] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9: Filtering track.",
                "The Ninth Text REtrieval Conference (TREC-9), pages 361-368. [16] R. Schapire, Y.",
                "Singer, and A. Singhal.",
                "Boosting and Rocchio Applied to Text Filtering.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle, and W. Croft.",
                "Indri: A Language Model-based Search Engine for Complex Queries.",
                "Proceedings of the International Conference on Intelligence Analysis, 2004. [18] The Linguistic Data Consortium. http://www.ldc.upenn.edu/. [19] E. Voorhees.",
                "Overview of the TREC 2003 Question Answering Track.",
                "Proceedings of the Twelfth Text REtrieval Conference (TREC 2003), 2003. [20] Y. Yang and B. Kisiel.",
                "Margin-based Local Regression for Adaptive Filtering.",
                "Proceedings of the twelfth international conference on Information and knowledge management, pages 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of Adaptive Filtering Methods in a Cross-benchmark Evaluation.",
                "Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 98-105, 2005. [22] C. Zhai, W. Cohen, and J. Lafferty.",
                "Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 10-17, 2003. [23] J. Zhang and Y. Yang.",
                "Robustness of Regularized Linear Classification Methods in Text Categorization.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 190-197, 2003. [24] Y. Zhang.",
                "Using Bayesian Priors to Combine Classifiers for Adaptive Filtering.",
                "Proceedings of the 27th annual international conference on Research and development in information retrieval, pages 345-352, 2004. [25] Y. Zhang, J. Callan, and T. Minka.",
                "Novelty and Redundancy Detection in Adaptive Filtering.",
                "Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Nuestro sistema, llamado CAF´E, combina filtrado adaptativo, detección de novedad y clasificación de pasaje antiredado en un \"marco unificado\" para la optimización de servicios públicos."
            ],
            "translated_text": "",
            "candidates": [
                "marco unificado",
                "marco unificado"
            ],
            "error": []
        },
        "ndcg metric": {
            "translated_key": "Métrica NDCG",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Utility-based Information Distillation Over Temporally Sequenced Documents Yiming Yang Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA yiming@cs.cmu.edu Abhimanyu Lad Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA alad@cs.cmu.edu Ni Lao Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA nlao@cs.cmu.edu Abhay Harpale Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA aharpale@cs.cmu.edu Bryan Kisiel Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA bkisiel@cs.cmu.edu Monica Rogati Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA mrogati@cs.cmu.edu ABSTRACT This paper examines a new approach to information distillation over temporally ordered documents, and proposes a novel evaluation scheme for such a framework.",
                "It combines the strengths of and extends beyond conventional adaptive filtering, novelty detection and non-redundant passage ranking with respect to long-lasting information needs (tasks with multiple queries).",
                "Our approach supports fine-grained user feedback via highlighting of arbitrary spans of text, and leverages such information for utility optimization in adaptive settings.",
                "For our experiments, we defined hypothetical tasks based on news events in the TDT4 corpus, with multiple queries per task.",
                "Answer keys (nuggets) were generated for each query and a semiautomatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also propose an extension of the <br>ndcg metric</br> for assessing the utility of ranked passages as a combination of relevance and novelty.",
                "Our results show encouraging utility enhancements using the new approach, compared to the baseline systems without incremental learning or the novelty detection components.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 General Terms Design, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Tracking new and relevant information from temporal data streams for users with long-lasting needs has been a challenging research topic in information retrieval.",
                "Adaptive filtering (AF) is one such task of online prediction of the relevance of each new document with respect to pre-defined topics.",
                "Based on the initial query and a few positive examples (if available), an AF system maintains a profile for each such topic of interest, and constantly updates it based on feedback from the user.",
                "The incremental learning nature of AF systems makes them more powerful than standard search engines that support ad-hoc retrieval (e.g.",
                "Google and Yahoo) in terms of finding relevant information with respect to long-lasting topics of interest, and more attractive for users who are willing to provide feedback to adapt the system towards their specific information needs, without having to modify their queries manually.",
                "A variety of supervised learning algorithms (Rocchio-style classifiers, Exponential-Gaussian models, local regression and logistic regression approaches) have been studied for adaptive settings, examined with explicit and implicit relevance feedback, and evaluated with respect to utility optimization on large benchmark data collections in TREC (Text Retrieval Conferences) and TDT (Topic Detection and Tracking) forums [1, 4, 7, 15, 16, 20, 24, 23].",
                "Regularized logistic regression [21] has been found representative for the state-of-the-art approaches, and highly efficient for frequent model adaptations over large document collections such as the TREC-10 corpus (over 800,000 documents and 84 topics).",
                "Despite substantial achievements in recent adaptive filtering research, significant problems remain unsolved regarding how to leverage user feedback effectively and efficiently.",
                "Specifically, the following issues may seriously limit the true utility of AF systems in real-world applications: 1.",
                "User has a rather passive role in the conventional adaptive filtering setup - he or she reacts to the system only when the system makes a yes decision on a document, by confirming or rejecting that decision.",
                "A more active alternative would be to allow the user to issue multiple queries for a topic, review a ranked list of candidate documents (or passages) per query, and provide feedback on the ranked list, thus refining their information need and requesting updated ranked lists.",
                "The latter form of user interaction has been highly effective in standard retrieval for ad-hoc queries.",
                "How to deploy such a strategy for long-lasting information needs in AF settings is an open question for research. 2.",
                "The unit for receiving a relevance judgment (yes or no) is restricted to the document level in conventional AF.",
                "However, a real user may be willing to provide more informative, fine-grained feedback via highlighting some pieces of text in a retrieved document as relevant, instead of labeling the entire document as relevant.",
                "Effectively leveraging such fine-grained feedback could substantially enhance the quality of an AF system.",
                "For this, we need to enable supervised learning from labeled pieces of text of arbitrary span instead of just allowing labeled documents. 3.",
                "System-selected documents are often highly redundant.",
                "A major news event, for example, would be reported by multiple sources repeatedly for a while, making most of the information content in those articles redundant with each other.",
                "A conventional AF system would select all these redundant news stories for user feedback, wasting the users time while offering little gain.",
                "Clearly, techniques for novelty detection can help in principle [25, 2, 22] for improving the utility of the AF systems.",
                "However, the effectiveness of such techniques at passage level to detect novelty with respect to users (fine-grained) feedback and to detect redundancy in ranked lists remains to be evaluated using a measure of utility that mimics the needs of a real user.",
                "To address the above limitations of current AF systems, we propose and examine a new approach in this paper, combining the strengths of conventional AF (incremental learning of topic models), multi-pass passage retrieval for long-lasting queries conditioned on topic, and novelty detection for removal of redundancy from user interactions with the system.",
                "We call the new process utility-based information distillation.",
                "Note that conventional benchmark corpora for AF evaluations, which have relevance judgments at the document level and do not define tasks with multiple queries, are insufficient for evaluating the new approach.",
                "Therefore, we extended a benchmark corpus - the TDT4 collection of news stories and TV broadcasts - with task definitions, multiple queries per task, and answer keys per query.",
                "We have conducted our experiments on this extended TDT4 corpus and have made the additionally generated data publicly available for future comparative evaluations 1 .",
                "To automatically evaluate the system-returned arbitrary spans of text using our answer keys, we further developed an evaluation scheme with semi-automatic procedure for 1 URL: http://nyc.lti.cs.cmu.edu/downloads acquiring rules that can match nuggets against system responses.",
                "Moreover, we propose an extension of NDCG (Normalized Discounted Cumulated Gain) [9] for assessing the utility of ranked passages as a function of both relevance and novelty.",
                "The rest of this paper is organized as follows.",
                "Section 2 outlines the information distillation process with a concrete example.",
                "Section 3 describes the technical cores of our system called CAF´E - CMU Adaptive Filtering Engine.",
                "Section 4 discusses issues with respect to evaluation methodology and proposes a new scheme.",
                "Section 5 describes the extended TDT4 corpus.",
                "Section 6 presents our experiments and results.",
                "Section 7 concludes the study and gives future perspectives. 2.",
                "A SAMPLE TASK Consider a news event - the escape of seven convicts from a Texas prison in December 2000 and their capture a month later.",
                "Assuming a user were interested in this event since its early stage, the information need could be: Find information about the escape of convicts from Texas prison, and information related to their recapture.",
                "The associated lower-level questions could be: 1.",
                "How many prisoners escaped? 2.",
                "Where and when were they sighted? 3. Who are their known contacts inside and outside the prison? 4.",
                "How are they armed? 5.",
                "Do they have any vehicles? 6.",
                "What steps have been taken so far?",
                "We call such an information need a task, and the associated questions as the queries in this task.",
                "A distillation system is supposed to monitor the incoming documents, process them chunk by chunk in a temporal order, select potentially relevant and novel passages from each chunk with respect to each query, and present a ranked list of passages to the user.",
                "Passage ranking here is based on how relevant a passage is with respect to the current query, how novel it is with respect to the current user history (of his or her interactions with the system), and how redundant it is compared to other passages with a higher rank in the list.",
                "When presented with a list of passages, the user may provide feedback by highlighting arbitrary spans of text that he or she found relevant.",
                "These spans of text are taken as positive examples in the adaptation of the query profile, and also added to the users history.",
                "Passages not marked by the user are taken as negative examples.",
                "As soon as the query profile is updated, the system re-issues a search and returns another ranked list of passages where the previously seen passages are either removed or ranked low, based on user preference.",
                "For example, if the user highlights ...officials have posted a $100,000 reward for their capture... as relevant answer to the query What steps have been taken so far?, then the highlighted piece is used as an additional positive training example in the adaptation of the query profile.",
                "This piece of feedback is also added to the user history as a seen example, so that in future, the system will not place another passage mentioning $100,000 reward at the top of the ranked list.",
                "However, an article mentioning ...officials have doubled the reward money to $200,000... might be ranked high since it is both relevant to the (updated) query profile and novel with respect to the (updated) user history.",
                "The user may modify the original queries or add a new query during the process; the query profiles will be changed accordingly.",
                "Clearly, novelty detection is very important for the utility of such a system because of the iterative search.",
                "Without novelty detection, the old relevant passages would be shown to the user repeatedly in each ranked list.",
                "Through the above example, we can see the main properties of our new framework for utility-based information distillation over temporally ordered documents.",
                "Our framework combines and extends the power of adaptive filtering (AF), ad-hoc retrieval (IR) and novelty detection (ND).",
                "Compared to standard IR, our approach has the power of incrementally learning long-term information needs and modeling a sequence of queries within a task.",
                "Compared to conventional AF, it enables a more active role of the user in refining his or her information needs and requesting new results by allowing relevance and novelty feedback via highlighting of arbitrary spans of text in passages returned by the system.",
                "Compared to past work, this is the first evaluation of novelty detection integrated with adaptive filtering for sequenced queries that allows flexible user feedback over ranked passages.",
                "The combination of AF, IR and ND with the new extensions raises an important research question regarding evaluation methodology: how can we measure the utility of such an information distillation system?",
                "Existing metrics in standard IR, AF and ND are insufficient, and new solutions must be explored, as we will discuss in Section 4, after describing the technical cores of our system in the next section. 3.",
                "TECHNICAL CORES The core components of CAF´E are - 1) AF for incremental learning of query profiles, 2) IR for estimating relevance of passages with respect to query profiles, 3) ND for assessing novelty of passages with respect to users history, and 4) anti-redundancy component to remove redundancy from ranked lists. 3.1 Adaptive Filtering Component We use a state-of-the-art algorithm in the field - the regularized logistic regression method which had the best results on several benchmark evaluation corpora for AF [21].",
                "Logistic regression (LR) is a supervised learning algorithm for statistical classification.",
                "Based on a training set of labeled instances, it learns a class model which can then by used to predict the labels of unseen instances.",
                "Its performance as well as efficiency in terms of training time makes it a good candidate when frequent updates of the class model are required, as is the case in adaptive filtering, where the system must learn from each new feedback provided by the user. (See [21] and [23] for computational complexity and implementation issues).",
                "In adaptive filtering, each query is considered as a class and the probability of a passage belonging to this class corresponds to the degree of relevance of the passage with respect to the query.",
                "For training the model, we use the query itself as the initial positive training example of the class, and the user-highlighted pieces of text (marked as Relevant or Not-relevant) during feedback as additional training examples.",
                "To address the cold start issue in the early stage before any user feedback is obtained, the system uses a small sample from a retrospective corpus as the initial negative examples in the training set.",
                "The details of using logistic regression for adaptive filtering (assigning different weights to positive and negative training instances, and regularizing the objective function to prevent over-fitting on training data) are presented in [21].",
                "The class model w∗ learned by Logistic Regression, or the query profile, is a vector whose dimensions are individual terms and whose elements are the regression coefficients, indicating how influential each term is in the query profile.",
                "The query profile is updated whenever a new piece of user feedback is received.",
                "A temporally decaying weight can be applied to each training example, as an option, to emphasize the most recent user feedback. 3.2 Passage Retrieval Component We use standard IR techniques in this part of our system.",
                "Incoming documents are processed in chunks, where each chunk can be defined as a fixed span of time or as a fixed number of documents, as preferred by the user.",
                "For each incoming document, corpus statistics like the IDF (Inverted Document Frequency) of each term are updated.",
                "We use a state-of-the-art named entity identifier and tracker [8, 12] to identify person and location names, and merge them with co-referent named entities seen in the past.",
                "Then the documents are segmented into passages, which can be a whole document, a paragraph, a sentence, or any other continuous span of text, as preferred.",
                "Each passage is represented using a vector of TF-IDF (Term FrequencyInverse Document Frequency) weights, where term can be a word or a named entity.",
                "Given a query profile, i.e. the logistic regression solution w∗ as described in Section 3.1, the system computes the posterior probability of relevance for each passage x as fRL(x) ≡ P(y = 1|x, w∗ ) = 1 (1 + e−w∗·x) (1) Passages are ordered by their relevance scores, and the ones with scores above a threshold (tuned on a training set) comprise the relevance list that is passed on to the novelty detection step. 3.3 Novelty Detection Component CAF´E maintains a user history H(t), which contains all the spans of text hi that the user highlighted (as feedback) during his or her past interactions with the system, up to the current time t. Denoting the history as H(t) = n h1, h2, ..., ht o , (2) the novelty score of a new candidate passage x is computed as: fND(x) = 1 − max i∈1..t {cos(x, hi)} (3) where both candidate passage x and highlighted spans of text hi are represented as TF-IDF vectors.",
                "The novelty score of each passage is compared to a prespecified threshold (also tuned on a training set), and any passage with a score below this threshold is removed from the relevance list. 3.4 Anti-redundant Ranking Component Although the novelty detection component ensures that only novel (previously unseen) information remains in the relevance list, this list might still contain the same novel information at multiple positions in the ranked list.",
                "Suppose, for example, that the user has already read about a $100,000 reward for information about the escaped convicts.",
                "A new piece of news that the award has been increased to $200,000 is novel since the user hasnt read about it yet.",
                "However, multiple news sources would report this news and we might end up showing (redundant) articles from all these sources in a ranked list.",
                "Hence, a ranked list should also be made non-redundant with respect to its own contents.",
                "We use a simplified version of the Maximal Marginal Relevance method [5], originally developed for combining relevance and novelty in text retrieval and summarization.",
                "Our procedure starts with the current list of passages sorted by relevance (section 3.2), filtered by Novelty Detection component (section 3.3), and generates a new non-redundant list as follows: 1.",
                "Take the top passage in the current list as the top one in the new list. 2.",
                "Add the next passage x in the current list to the new list only if fAR(x) > t where fAR(x) = 1 − max pi∈Lnew {cos(x, pi)} and Lnew is the set of passages already selected in the new list. 3.",
                "Repeat step 2 until all the passages in the current list have been examined.",
                "After applying the above-mentioned algorithm, each passage in the new list is sufficiently dissimilar to others, thus favoring diversity rather than redundancy in the new ranked list.",
                "The anti-redundancy threshold t is tuned on a training set. 4.",
                "EVALUATION METHODOLOGY The approach we proposed above for information distillation raises important issues regarding evaluation methodology.",
                "Firstly, since our framework allows the output to be passages at different levels of granularity (e.g. k-sentence windows where k may vary) instead of a fixed length, it is not possible to have pre-annotated relevance judgments at all such granularity levels.",
                "Secondly, since we wish to measure the utility of the system output as a combination of both relevance and novelty, traditional relevance-only based measures must be replaced by measures that penalize the repetition of the same information in the system output across time.",
                "Thirdly, since the output of the system is ranked lists, we must reward those systems that present useful information (both relevant and previously unseen) using shorter ranked lists, and penalize those that present the same information using longer ranked lists.",
                "None of the existing measures in ad-hoc retrieval, adaptive filtering, novelty detection or other related areas (text summarization and question answering) have desirable properties in all the three aspects.",
                "Therefore, we must develop a new evaluation methodology. 4.1 Answer Keys To enable the evaluation of a system whose output consists of passages of arbitrary length, we borrow the concept of answer keys from the Question Answering (QA) community, where systems are allowed to return arbitrary spans of text as answers.",
                "Answer keys define what should be present in a system response to receive credit, and are comprised of a collection of information nuggets, i.e. factoid units about which human assessors can make binary decisions of whether or not a system response contains them.",
                "Defining answer keys and making the associated binary decisions are conceptual tasks that require semantic mapping [19], since system-returned passages can contain the same information expressed in many different ways.",
                "Hence, QA evaluations have relied on human assessors for the mapping between various expressions, making the process costly, time consuming, and not scalable to large query and document collections, and extensive system evaluations with various parameter settings. 4.1.1 Automating Evaluation based on Answer Keys Automatic evaluation methods would allow for faster system building and tuning, as well as provide an objective and affordable way of comparing various systems.",
                "Recently, such methods have been proposed, more or less, based on the idea of n-gram co-occurrences.",
                "Pourpre [10] assigns a fractional recall score to a system response based on its unigram overlap with a given nuggets description.",
                "For example, a system response A B C has recall 3/4 with respect to a nugget with description A B C D. However, such an approach is unfair to systems that present the same information but using words other than A, B, C, and D. Another open issue is how to weight individual words in measuring the closeness of a match.",
                "For example, consider the question How many prisoners escaped?.",
                "In the nugget Seven prisoners escaped from a Texas prison, there is no indication that seven is the keyword, and that it must be matched to get any relevance credit.",
                "Using IDF values does not help, since seven will generally not have a higher IDF than words like texas and prison.",
                "Also, redefining the nugget as just seven does not solve the problem since now it might spuriously match any mention of seven out of context.",
                "Nuggeteer [13] works on similar principles but makes binary decisions about whether a nugget is present in a given system response by tuning a threshold.",
                "However, it is also plagued by spurious relevance since not all words contained in the nugget description (or known correct responses) are central to the nugget. 4.1.2 Nugget-Matching Rules We propose a reliable automatic method for determining whether a snippet of text contains a given nugget, based on nugget-matching rules, which are generated using a semiautomatic procedure explained below.",
                "These rules are essentially Boolean queries that will only match against snippets that contain the nugget.",
                "For instance, a candidate rule for matching answers to How many prisoners escaped? is (Texas AND seven AND escape AND (convicts OR prisoners)), possibly with other synonyms and variants in the rule.",
                "For a corpus of news articles, which usually follow a typical formal prose, it is fairly easy to write such simple rules to match expected answers using a bootstrap approach, as described below.",
                "We propose a two-stage approach, inspired by Autoslog [14], that combines the strength of humans in identifying semantically equivalent expressions and the strength of the system in gathering statistical evidence from a humanannotated corpus of documents.",
                "In the first stage, human subjects annotated (using a highlighting tool) portions of ontopic documents that contained answers to each nugget 2 .",
                "In the second stage, subjects used our rule generation tool to create rules that would match the annotations for each nugget.",
                "The tool allows users to enter a Boolean rule as a disjunction of conjunctions (e.g. ((a AND b) OR (a AND c AND d) OR (e))).",
                "Given a candidate rule, our tool uses it as a Boolean query over the entire set of on-topic documents and calculates its recall and precision with respect to the annotations that it is expected to match.",
                "Hence, the subjects can start with a simple rule and iteratively refine it until they are satisfied with its recall and precision.",
                "We observed that it was very easy for humans to improve the precision of a rule by tweaking its existing conjunctions (adding more ANDs), and improving the recall by adding more conjunctions to the disjunction (adding more ORs).",
                "As an example, lets try to create a rule for the nugget which says that seven prisoners escaped from the Texas prison.",
                "We start with a simple rule - (seven).",
                "When we input this into the rule generation tool, we realize that this rule matches many spurious occurrences of seven (e.g. ...seven states...) and thus gets a low precision score.",
                "We can further qualify our rule - Texas AND seven AND convicts.",
                "Next, by looking at the missed annotations, we realize that some news articles mentioned ...seven prisoners escaped.... We then replace convicts with the disjunction (convicts OR prisoners).",
                "We continue tweaking the rule in this manner until we achieve a sufficiently high recall and precision - i.e. the (small number of) misses and false alarms can be safely ignored.",
                "Thus we can create nugget-matching rules that succinctly capture various ways of expressing a nugget, while avoiding matching incorrect (or out of context) responses.",
                "Human involvement in the rule creation process ensures high quality generic rules which can then be used to evaluate arbitrary system responses reliably. 4.2 Evaluating the Utility of a Sequence of Ranked Lists The utility of a retrieval system can be defined as the difference between how much the user gained in terms of useful information, and how much the user lost in terms of time and energy.",
                "We calculate this utility from the utilities of individual passages as follows.",
                "After reading each passage returned by the system, the user derives some gain depending on the presence of relevant and novel information, and incurs a loss in terms of the time and energy spent in going through the passage.",
                "However, the likelihood that the user would actually read a passage depends on its position in the ranked list.",
                "Hence, for a query q, the expected utility 2 LDC [18] already provides relevance judgments for 100 topics on the TDT4 corpus.",
                "We further ensured that these judgments are exhaustive on the entire corpus using pooling. of a passage pi at rank i can be defined as U(pi, q) = P(i) ∗ (Gain(pi, q) − Loss(pi, q)) (4) where P(i) is the probability that the user would go through a passage at rank i.",
                "The expected utility for an entire ranked list of length n can be calculated simply by adding the expected utility of each passage: U(q) = nX i=1 P(i) ∗ (Gain(pi, q) − Loss(pi, q)) (5) Note that if we ignore the loss term and define P(i) as P(i) ∝ 1/ logb(b + i − 1) (6) then we get the recently popularized metric called Discounted Cumulated Gain (DCG) [9], where Gain(pi, q) is defined as the graded relevance of passage pi.",
                "However, without the loss term, DCG is a purely recall-oriented metric and not suitable for an adaptive filtering setting, where the systems utility depends in part on its ability to limit the number of items shown to the user.",
                "Although P(i) could be defined based on empirical studies of user behavior, for simplicity, we use P(i) exactly as defined in equation 6.",
                "The gain G(pi, q) of passage pi with respect to the query q is a function of - 1) the number of relevant nuggets present in pi, and 2) the novelty of each of these nuggets.",
                "We combine these two factors as follows.",
                "For each nugget Nj, we assign an initial weight wj, and also keep a count nj of the number of times this nugget has been seen by the user in the past.",
                "The gain derived from each subsequent occurrence of the same nugget is assumed to reduce by a dampening factor γ.",
                "Thus, G(pi, q) is defined as G(pi, q) = X Nj ∈C(pi,q) wj ∗ γnj (7) where C(pi, q) is the set of all nuggets that appear in passage pi and also belong to the answer key of query q.",
                "The initial weights wj are all set of be 1.0 in our experiments, but can also be set based on a pyramid approach [11].",
                "The choice of dampening factor γ determines the users tolerance for redundancy.",
                "When γ = 0, a nugget will only receive credit for its first occurrence i.e. when nj is zero3 .",
                "For 0 < γ < 1, a nugget receives smaller credit for each successive occurrence.",
                "When γ = 1, no dampening occurs and repeated occurrences of a nugget receive the same credit.",
                "Note that the nugget occurrence counts are preserved between evaluation of successive ranked lists returned by the system, since the users are expected to remember what the system showed them in the past.",
                "We define the loss L(pi, q) as a constant cost c (we use 0.1) incurred when reading a system-returned passage.",
                "Thus, our metric can be re-written as U(q) = nX i=1 Gain(pi, q) logb(b + i − 1) − L(n) (8) where L(n) is the loss associated with a ranked list of length n: L(n) = c · nX i=1 1 logb(b + i − 1) (9) 3 Note that 00 = 1 Due to the similarity with Discounted Cumulated Gain (DCG), we call our metric Discounted Cumulated Utility (DCU).",
                "The DCU score obtained by the system is converted to a Normalized DCU (NDCU) score by dividing it by the DCU score of the ideal ranked list, which is created by ordering passages by their decreasing utility scores U(pi, q) and stopping when U(pi, q) ≤ 0 i.e. when the gain is less than or equal to the cost of reading the passage. 5.",
                "DATA TDT4 was the benchmark corpus used in TDT2002 and TDT2003 evaluations.",
                "The corpus consists of over 90, 000 news articles from multiple sources (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) published between October 2000 and January 2001, in the languages of Arabic, English, and Mandarin.",
                "Speech-recognized and machine-translated versions of the non-English articles were provided as well.",
                "LDC [18] has annotated the corpus with 100 topics, that correspond to various news events in this time period.",
                "Out of these, we selected a subset of 12 actionable events, and defined corresponding tasks for them4 .",
                "For each task, we manually defined a profile consisting of an initial set of (5 to 10) queries, a free-text description of the user history, i.e., what the user already knows about the event, and a list of known on-topic and off-topic documents (if available) as training examples.",
                "For each query, we generated answer keys and corresponding nugget matching rules using the procedure described in section 4.1.2, and produced a total of 120 queries, with an average of 7 nuggets per query. 6.",
                "EXPERIMENTS AND RESULTS 6.1 Baselines We used Indri [17], a popular language-model based retrieval engine, as a baseline for comparison with CAF´E.",
                "Indri supports standard search engine functionality, including pseudo-relevance feedback (PRF) [3, 6], and is representative of a typical query-based retrieval system.",
                "Indri does not support any kind of novelty detection.",
                "We compare Indri with PRF turned on and off, against CAF´E with user feedback, novelty detection and antiredundant ranking turned on and off. 6.2 Experimental Setup We divided the TDT4 corpus spanning 4 months into 10 chunks, each defined as a period of 12 consecutive days.",
                "At any given point of time in the distillation process, each system accessed the past data up to the current point, and returned a ranked list of up 50 passages per query.",
                "The 12 tasks defined on the corpus were divided into a training and test set with 6 tasks each.",
                "Each system was allowed to use the training set to tune its parameters for optimizing NDCU (equation 8), including the relevance threshold for both Indri and CAF´E, and the novelty and antiredundancy thresholds for CAF´E.",
                "The NDCU for each system run is calculated automatically.",
                "User feedback was also simulated - relevance judgments for each system-returned passage (as determined by the nugget matching rules described in section 4.1.2) were 4 URL: http://nyc.lti.cs.cmu.edu/downloads Figure 1: Performance of Indri across chunks Figure 2: Performance of CAF´E across chunks used as user feedback in the adaptation of query profiles and user histories. 6.3 Results In Table 1, we show the NDCU scores of the two systems under various settings.",
                "These scores are averaged over the six tasks in the test set, and are calculated with two dampening factors (see section 4.2): γ = 0 and 0.1, to simulate no tolerance and small tolerance for redundancy, respectively.",
                "Using γ = 0 creates a much more strict metric since it does not give any credit to a passage that contains relevant but redundant information.",
                "Hence, the improvement obtained from enabling user feedback is smaller with γ = 0 than the improvement obtained from feedback with γ = 0.1.",
                "This reveals a shortcoming of contemporary retrieval systemswhen the user gives positive feedback on a passage, the systems gives higher weights to the terms present in that passage and tends to retrieve other passages containing the same terms - and thus - usually the same information.",
                "However, the user does not benefit from seeing such redundant passages, and is usually interested in other passages containing related information.",
                "It is informative to evaluate retrieval systems using our utility measure (with γ = 0) which accounts for novelty and thus gives a more realistic picture of how well a system can generalize from user feedback, rather than using traditional IR measures like recall and precision which give an incomplete picture of improvement obtained from user feedback.",
                "Sometimes, however, users might indeed be interested in seeing the same information from multiple sources, as an Table 1: NDCU Scores of Indri and CAF´E for two dampening factors (γ), and various settings (F: Feedback, N: Novelty Detection, A: Anti-Redundant Ranking) Indri CAF´E γ Base +PRF Base +F +F+N +F+A +F+N+A 0 0.19 0.19 0.22 0.23 0.24 0.24 0.24 0.1 0.28 0.29 0.24 0.35 0.35 0.36 0.36 indicator of its importance or reliability.",
                "In such a case, we can simply choose a higher value for γ which corresponds to a higher tolerance for redundancy, and hence let the system tune its parameters accordingly.",
                "Since documents were processed chunk by chunk, it would be interesting to see how the performance of systems improves over time.",
                "Figures 1 and 2 show the performance trends for both the systems across chunks.",
                "While the performance with and without feedback on the first few chunks is expected to be close, for subsequent chunks, the performance curve with feedback enabled rises above the one with the no-feedback setting.",
                "The performance trends are not consistent across all chunks because on-topic documents are not uniformly distributed over all the chunks, making some queries easier than others in certain chunks.",
                "Moreover, since Indri uses pseudo-relevance feedback while CAF´E uses feedback based on actual relevance judgments, the improvement in case of Indri is less dramatic than that of CAF´E. 7.",
                "CONCLUDING REMARKS This paper presents the first investigation on utility-based information distillation with a system that learns the longlasting information needs from fine-grained user feedback over a sequence of ranked passages.",
                "Our system, called CAF´E, combines adaptive filtering, novelty detection and antiredundant passage ranking in a unified framework for utility optimization.",
                "We developed a new scheme for automated evaluation and feedback based on a semi-automatic procedure for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also proposed an extension of the <br>ndcg metric</br> for assessing the utility of ranked passages as a weighted combination of relevance and novelty.",
                "Our experiments on the newly annotated TDT4 benchmark corpus show encouraging utility enhancement over Indri, and also over our own system with incremental learning and novelty detection turned off. 8.",
                "ACKNOWLEDGMENTS We would like to thank Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng, and the Qualitative Data Analysis Program at the University of Pittsburgh lead by Dr. Stuart Shulman for their help with collecting and processing the extended TDT4 annotations used in our experiments.",
                "This work is supported in parts by the National Science Foundation (NSF) under grant IIS0434035, and the Defense Advanced Research Project Agency (DARPA) under contracts NBCHD030010 and W0550432.",
                "Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors. 9.",
                "ADDITIONAL AUTHORS Jian Zhang (jianzhan@stat.purdue.edu)∗ , Jaime Carbonell (jgc@cs.cmu.edu)† , Peter Brusilovsky (peterb+@pitt.edu)‡ , Daqing He(dah44@pitt.edu)‡ 10.",
                "REFERENCES [1] J. Allan.",
                "Incremental Relevance Feedback for Information Filtering.",
                "Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, pages 270-278, 1996. [2] J. Allan, C. Wade, and A. Bolivar.",
                "Retrieval and Novelty Detection at the Sentence Level.",
                "Proceedings of the ACM SIGIR conference on research and development in information retrieval, 2003. [3] C. Buckley, G. Salton, and J. Allan.",
                "Automatic Retrieval with Locality Information using SMART.",
                "NIST special publication, (500207):59-72, 1993. [4] J. Callan.",
                "Learning While Filtering Documents.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 224-231, 1998. [5] J. Carbonell and J. Goldstein.",
                "The use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 335-336, 1998. [6] E. Efthimiadis.",
                "Query Expansion.",
                "Annual Review of Information Science and Technology (ARIST), 31:p121-87, 1996. [7] J. Fiscus and G. Duddington.",
                "Topic Detection and Tracking Overview.",
                "Topic Detection and Tracking: Event-based Information Organization, pages 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos.",
                "A Statistical Model for Multilingual Entity Detection and Tracking.",
                "NAACL/HLT, 2004. [9] K. J¨arvelin and J. Kek¨al¨ainen.",
                "Cumulated Gain-based Evaluation of IR Techniques.",
                "ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin and D. Demner-Fushman.",
                "Automatically Evaluating Answers to Definition Questions.",
                "Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), 2005. ∗ Statistics Dept., Purdue University, West Lafayette, USA † Language Technologies Inst., Carnegie Mellon University, Pittsburgh, USA ‡ School of Information Sciences, Univ. of Pittsburgh, Pittsburgh, USA [11] J. Lin and D. Demner-Fushman.",
                "Will Pyramids Built of nUggets Topple Over.",
                "Proceedings of HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos.",
                "A Mention-synchronous Coreference Resolution Algorithm based on the Bell Tree.",
                "Proc. of ACL, 4:136-143, 2004. [13] G. Marton.",
                "Nuggeteer: Automatic Nugget-Based Evaluation Using Descriptions and Judgments.",
                "HLT/NAACL, 2006. [14] E. Riloff.",
                "Automatically Constructing a Dictionary for Information Extraction Tasks.",
                "Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 811-816, 1993. [15] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9: Filtering track.",
                "The Ninth Text REtrieval Conference (TREC-9), pages 361-368. [16] R. Schapire, Y.",
                "Singer, and A. Singhal.",
                "Boosting and Rocchio Applied to Text Filtering.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle, and W. Croft.",
                "Indri: A Language Model-based Search Engine for Complex Queries.",
                "Proceedings of the International Conference on Intelligence Analysis, 2004. [18] The Linguistic Data Consortium. http://www.ldc.upenn.edu/. [19] E. Voorhees.",
                "Overview of the TREC 2003 Question Answering Track.",
                "Proceedings of the Twelfth Text REtrieval Conference (TREC 2003), 2003. [20] Y. Yang and B. Kisiel.",
                "Margin-based Local Regression for Adaptive Filtering.",
                "Proceedings of the twelfth international conference on Information and knowledge management, pages 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of Adaptive Filtering Methods in a Cross-benchmark Evaluation.",
                "Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 98-105, 2005. [22] C. Zhai, W. Cohen, and J. Lafferty.",
                "Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 10-17, 2003. [23] J. Zhang and Y. Yang.",
                "Robustness of Regularized Linear Classification Methods in Text Categorization.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 190-197, 2003. [24] Y. Zhang.",
                "Using Bayesian Priors to Combine Classifiers for Adaptive Filtering.",
                "Proceedings of the 27th annual international conference on Research and development in information retrieval, pages 345-352, 2004. [25] Y. Zhang, J. Callan, and T. Minka.",
                "Novelty and Redundancy Detection in Adaptive Filtering.",
                "Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "También proponemos una extensión de la \"métrica NDCG\" para evaluar la utilidad de los pasajes clasificados como una combinación de relevancia y novedad.",
                "También propusimos una extensión de la \"métrica NDCG\" para evaluar la utilidad de los pasajes clasificados como una combinación ponderada de relevancia y novedad."
            ],
            "translated_text": "",
            "candidates": [
                "Métrica NDCG",
                "métrica NDCG",
                "Métrica NDCG",
                "métrica NDCG"
            ],
            "error": []
        },
        "utility-base distillation": {
            "translated_key": "destilación de base de servicios públicos",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Utility-based Information Distillation Over Temporally Sequenced Documents Yiming Yang Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA yiming@cs.cmu.edu Abhimanyu Lad Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA alad@cs.cmu.edu Ni Lao Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA nlao@cs.cmu.edu Abhay Harpale Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA aharpale@cs.cmu.edu Bryan Kisiel Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA bkisiel@cs.cmu.edu Monica Rogati Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA mrogati@cs.cmu.edu ABSTRACT This paper examines a new approach to information distillation over temporally ordered documents, and proposes a novel evaluation scheme for such a framework.",
                "It combines the strengths of and extends beyond conventional adaptive filtering, novelty detection and non-redundant passage ranking with respect to long-lasting information needs (tasks with multiple queries).",
                "Our approach supports fine-grained user feedback via highlighting of arbitrary spans of text, and leverages such information for utility optimization in adaptive settings.",
                "For our experiments, we defined hypothetical tasks based on news events in the TDT4 corpus, with multiple queries per task.",
                "Answer keys (nuggets) were generated for each query and a semiautomatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also propose an extension of the NDCG metric for assessing the utility of ranked passages as a combination of relevance and novelty.",
                "Our results show encouraging utility enhancements using the new approach, compared to the baseline systems without incremental learning or the novelty detection components.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 General Terms Design, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Tracking new and relevant information from temporal data streams for users with long-lasting needs has been a challenging research topic in information retrieval.",
                "Adaptive filtering (AF) is one such task of online prediction of the relevance of each new document with respect to pre-defined topics.",
                "Based on the initial query and a few positive examples (if available), an AF system maintains a profile for each such topic of interest, and constantly updates it based on feedback from the user.",
                "The incremental learning nature of AF systems makes them more powerful than standard search engines that support ad-hoc retrieval (e.g.",
                "Google and Yahoo) in terms of finding relevant information with respect to long-lasting topics of interest, and more attractive for users who are willing to provide feedback to adapt the system towards their specific information needs, without having to modify their queries manually.",
                "A variety of supervised learning algorithms (Rocchio-style classifiers, Exponential-Gaussian models, local regression and logistic regression approaches) have been studied for adaptive settings, examined with explicit and implicit relevance feedback, and evaluated with respect to utility optimization on large benchmark data collections in TREC (Text Retrieval Conferences) and TDT (Topic Detection and Tracking) forums [1, 4, 7, 15, 16, 20, 24, 23].",
                "Regularized logistic regression [21] has been found representative for the state-of-the-art approaches, and highly efficient for frequent model adaptations over large document collections such as the TREC-10 corpus (over 800,000 documents and 84 topics).",
                "Despite substantial achievements in recent adaptive filtering research, significant problems remain unsolved regarding how to leverage user feedback effectively and efficiently.",
                "Specifically, the following issues may seriously limit the true utility of AF systems in real-world applications: 1.",
                "User has a rather passive role in the conventional adaptive filtering setup - he or she reacts to the system only when the system makes a yes decision on a document, by confirming or rejecting that decision.",
                "A more active alternative would be to allow the user to issue multiple queries for a topic, review a ranked list of candidate documents (or passages) per query, and provide feedback on the ranked list, thus refining their information need and requesting updated ranked lists.",
                "The latter form of user interaction has been highly effective in standard retrieval for ad-hoc queries.",
                "How to deploy such a strategy for long-lasting information needs in AF settings is an open question for research. 2.",
                "The unit for receiving a relevance judgment (yes or no) is restricted to the document level in conventional AF.",
                "However, a real user may be willing to provide more informative, fine-grained feedback via highlighting some pieces of text in a retrieved document as relevant, instead of labeling the entire document as relevant.",
                "Effectively leveraging such fine-grained feedback could substantially enhance the quality of an AF system.",
                "For this, we need to enable supervised learning from labeled pieces of text of arbitrary span instead of just allowing labeled documents. 3.",
                "System-selected documents are often highly redundant.",
                "A major news event, for example, would be reported by multiple sources repeatedly for a while, making most of the information content in those articles redundant with each other.",
                "A conventional AF system would select all these redundant news stories for user feedback, wasting the users time while offering little gain.",
                "Clearly, techniques for novelty detection can help in principle [25, 2, 22] for improving the utility of the AF systems.",
                "However, the effectiveness of such techniques at passage level to detect novelty with respect to users (fine-grained) feedback and to detect redundancy in ranked lists remains to be evaluated using a measure of utility that mimics the needs of a real user.",
                "To address the above limitations of current AF systems, we propose and examine a new approach in this paper, combining the strengths of conventional AF (incremental learning of topic models), multi-pass passage retrieval for long-lasting queries conditioned on topic, and novelty detection for removal of redundancy from user interactions with the system.",
                "We call the new process utility-based information distillation.",
                "Note that conventional benchmark corpora for AF evaluations, which have relevance judgments at the document level and do not define tasks with multiple queries, are insufficient for evaluating the new approach.",
                "Therefore, we extended a benchmark corpus - the TDT4 collection of news stories and TV broadcasts - with task definitions, multiple queries per task, and answer keys per query.",
                "We have conducted our experiments on this extended TDT4 corpus and have made the additionally generated data publicly available for future comparative evaluations 1 .",
                "To automatically evaluate the system-returned arbitrary spans of text using our answer keys, we further developed an evaluation scheme with semi-automatic procedure for 1 URL: http://nyc.lti.cs.cmu.edu/downloads acquiring rules that can match nuggets against system responses.",
                "Moreover, we propose an extension of NDCG (Normalized Discounted Cumulated Gain) [9] for assessing the utility of ranked passages as a function of both relevance and novelty.",
                "The rest of this paper is organized as follows.",
                "Section 2 outlines the information distillation process with a concrete example.",
                "Section 3 describes the technical cores of our system called CAF´E - CMU Adaptive Filtering Engine.",
                "Section 4 discusses issues with respect to evaluation methodology and proposes a new scheme.",
                "Section 5 describes the extended TDT4 corpus.",
                "Section 6 presents our experiments and results.",
                "Section 7 concludes the study and gives future perspectives. 2.",
                "A SAMPLE TASK Consider a news event - the escape of seven convicts from a Texas prison in December 2000 and their capture a month later.",
                "Assuming a user were interested in this event since its early stage, the information need could be: Find information about the escape of convicts from Texas prison, and information related to their recapture.",
                "The associated lower-level questions could be: 1.",
                "How many prisoners escaped? 2.",
                "Where and when were they sighted? 3. Who are their known contacts inside and outside the prison? 4.",
                "How are they armed? 5.",
                "Do they have any vehicles? 6.",
                "What steps have been taken so far?",
                "We call such an information need a task, and the associated questions as the queries in this task.",
                "A distillation system is supposed to monitor the incoming documents, process them chunk by chunk in a temporal order, select potentially relevant and novel passages from each chunk with respect to each query, and present a ranked list of passages to the user.",
                "Passage ranking here is based on how relevant a passage is with respect to the current query, how novel it is with respect to the current user history (of his or her interactions with the system), and how redundant it is compared to other passages with a higher rank in the list.",
                "When presented with a list of passages, the user may provide feedback by highlighting arbitrary spans of text that he or she found relevant.",
                "These spans of text are taken as positive examples in the adaptation of the query profile, and also added to the users history.",
                "Passages not marked by the user are taken as negative examples.",
                "As soon as the query profile is updated, the system re-issues a search and returns another ranked list of passages where the previously seen passages are either removed or ranked low, based on user preference.",
                "For example, if the user highlights ...officials have posted a $100,000 reward for their capture... as relevant answer to the query What steps have been taken so far?, then the highlighted piece is used as an additional positive training example in the adaptation of the query profile.",
                "This piece of feedback is also added to the user history as a seen example, so that in future, the system will not place another passage mentioning $100,000 reward at the top of the ranked list.",
                "However, an article mentioning ...officials have doubled the reward money to $200,000... might be ranked high since it is both relevant to the (updated) query profile and novel with respect to the (updated) user history.",
                "The user may modify the original queries or add a new query during the process; the query profiles will be changed accordingly.",
                "Clearly, novelty detection is very important for the utility of such a system because of the iterative search.",
                "Without novelty detection, the old relevant passages would be shown to the user repeatedly in each ranked list.",
                "Through the above example, we can see the main properties of our new framework for utility-based information distillation over temporally ordered documents.",
                "Our framework combines and extends the power of adaptive filtering (AF), ad-hoc retrieval (IR) and novelty detection (ND).",
                "Compared to standard IR, our approach has the power of incrementally learning long-term information needs and modeling a sequence of queries within a task.",
                "Compared to conventional AF, it enables a more active role of the user in refining his or her information needs and requesting new results by allowing relevance and novelty feedback via highlighting of arbitrary spans of text in passages returned by the system.",
                "Compared to past work, this is the first evaluation of novelty detection integrated with adaptive filtering for sequenced queries that allows flexible user feedback over ranked passages.",
                "The combination of AF, IR and ND with the new extensions raises an important research question regarding evaluation methodology: how can we measure the utility of such an information distillation system?",
                "Existing metrics in standard IR, AF and ND are insufficient, and new solutions must be explored, as we will discuss in Section 4, after describing the technical cores of our system in the next section. 3.",
                "TECHNICAL CORES The core components of CAF´E are - 1) AF for incremental learning of query profiles, 2) IR for estimating relevance of passages with respect to query profiles, 3) ND for assessing novelty of passages with respect to users history, and 4) anti-redundancy component to remove redundancy from ranked lists. 3.1 Adaptive Filtering Component We use a state-of-the-art algorithm in the field - the regularized logistic regression method which had the best results on several benchmark evaluation corpora for AF [21].",
                "Logistic regression (LR) is a supervised learning algorithm for statistical classification.",
                "Based on a training set of labeled instances, it learns a class model which can then by used to predict the labels of unseen instances.",
                "Its performance as well as efficiency in terms of training time makes it a good candidate when frequent updates of the class model are required, as is the case in adaptive filtering, where the system must learn from each new feedback provided by the user. (See [21] and [23] for computational complexity and implementation issues).",
                "In adaptive filtering, each query is considered as a class and the probability of a passage belonging to this class corresponds to the degree of relevance of the passage with respect to the query.",
                "For training the model, we use the query itself as the initial positive training example of the class, and the user-highlighted pieces of text (marked as Relevant or Not-relevant) during feedback as additional training examples.",
                "To address the cold start issue in the early stage before any user feedback is obtained, the system uses a small sample from a retrospective corpus as the initial negative examples in the training set.",
                "The details of using logistic regression for adaptive filtering (assigning different weights to positive and negative training instances, and regularizing the objective function to prevent over-fitting on training data) are presented in [21].",
                "The class model w∗ learned by Logistic Regression, or the query profile, is a vector whose dimensions are individual terms and whose elements are the regression coefficients, indicating how influential each term is in the query profile.",
                "The query profile is updated whenever a new piece of user feedback is received.",
                "A temporally decaying weight can be applied to each training example, as an option, to emphasize the most recent user feedback. 3.2 Passage Retrieval Component We use standard IR techniques in this part of our system.",
                "Incoming documents are processed in chunks, where each chunk can be defined as a fixed span of time or as a fixed number of documents, as preferred by the user.",
                "For each incoming document, corpus statistics like the IDF (Inverted Document Frequency) of each term are updated.",
                "We use a state-of-the-art named entity identifier and tracker [8, 12] to identify person and location names, and merge them with co-referent named entities seen in the past.",
                "Then the documents are segmented into passages, which can be a whole document, a paragraph, a sentence, or any other continuous span of text, as preferred.",
                "Each passage is represented using a vector of TF-IDF (Term FrequencyInverse Document Frequency) weights, where term can be a word or a named entity.",
                "Given a query profile, i.e. the logistic regression solution w∗ as described in Section 3.1, the system computes the posterior probability of relevance for each passage x as fRL(x) ≡ P(y = 1|x, w∗ ) = 1 (1 + e−w∗·x) (1) Passages are ordered by their relevance scores, and the ones with scores above a threshold (tuned on a training set) comprise the relevance list that is passed on to the novelty detection step. 3.3 Novelty Detection Component CAF´E maintains a user history H(t), which contains all the spans of text hi that the user highlighted (as feedback) during his or her past interactions with the system, up to the current time t. Denoting the history as H(t) = n h1, h2, ..., ht o , (2) the novelty score of a new candidate passage x is computed as: fND(x) = 1 − max i∈1..t {cos(x, hi)} (3) where both candidate passage x and highlighted spans of text hi are represented as TF-IDF vectors.",
                "The novelty score of each passage is compared to a prespecified threshold (also tuned on a training set), and any passage with a score below this threshold is removed from the relevance list. 3.4 Anti-redundant Ranking Component Although the novelty detection component ensures that only novel (previously unseen) information remains in the relevance list, this list might still contain the same novel information at multiple positions in the ranked list.",
                "Suppose, for example, that the user has already read about a $100,000 reward for information about the escaped convicts.",
                "A new piece of news that the award has been increased to $200,000 is novel since the user hasnt read about it yet.",
                "However, multiple news sources would report this news and we might end up showing (redundant) articles from all these sources in a ranked list.",
                "Hence, a ranked list should also be made non-redundant with respect to its own contents.",
                "We use a simplified version of the Maximal Marginal Relevance method [5], originally developed for combining relevance and novelty in text retrieval and summarization.",
                "Our procedure starts with the current list of passages sorted by relevance (section 3.2), filtered by Novelty Detection component (section 3.3), and generates a new non-redundant list as follows: 1.",
                "Take the top passage in the current list as the top one in the new list. 2.",
                "Add the next passage x in the current list to the new list only if fAR(x) > t where fAR(x) = 1 − max pi∈Lnew {cos(x, pi)} and Lnew is the set of passages already selected in the new list. 3.",
                "Repeat step 2 until all the passages in the current list have been examined.",
                "After applying the above-mentioned algorithm, each passage in the new list is sufficiently dissimilar to others, thus favoring diversity rather than redundancy in the new ranked list.",
                "The anti-redundancy threshold t is tuned on a training set. 4.",
                "EVALUATION METHODOLOGY The approach we proposed above for information distillation raises important issues regarding evaluation methodology.",
                "Firstly, since our framework allows the output to be passages at different levels of granularity (e.g. k-sentence windows where k may vary) instead of a fixed length, it is not possible to have pre-annotated relevance judgments at all such granularity levels.",
                "Secondly, since we wish to measure the utility of the system output as a combination of both relevance and novelty, traditional relevance-only based measures must be replaced by measures that penalize the repetition of the same information in the system output across time.",
                "Thirdly, since the output of the system is ranked lists, we must reward those systems that present useful information (both relevant and previously unseen) using shorter ranked lists, and penalize those that present the same information using longer ranked lists.",
                "None of the existing measures in ad-hoc retrieval, adaptive filtering, novelty detection or other related areas (text summarization and question answering) have desirable properties in all the three aspects.",
                "Therefore, we must develop a new evaluation methodology. 4.1 Answer Keys To enable the evaluation of a system whose output consists of passages of arbitrary length, we borrow the concept of answer keys from the Question Answering (QA) community, where systems are allowed to return arbitrary spans of text as answers.",
                "Answer keys define what should be present in a system response to receive credit, and are comprised of a collection of information nuggets, i.e. factoid units about which human assessors can make binary decisions of whether or not a system response contains them.",
                "Defining answer keys and making the associated binary decisions are conceptual tasks that require semantic mapping [19], since system-returned passages can contain the same information expressed in many different ways.",
                "Hence, QA evaluations have relied on human assessors for the mapping between various expressions, making the process costly, time consuming, and not scalable to large query and document collections, and extensive system evaluations with various parameter settings. 4.1.1 Automating Evaluation based on Answer Keys Automatic evaluation methods would allow for faster system building and tuning, as well as provide an objective and affordable way of comparing various systems.",
                "Recently, such methods have been proposed, more or less, based on the idea of n-gram co-occurrences.",
                "Pourpre [10] assigns a fractional recall score to a system response based on its unigram overlap with a given nuggets description.",
                "For example, a system response A B C has recall 3/4 with respect to a nugget with description A B C D. However, such an approach is unfair to systems that present the same information but using words other than A, B, C, and D. Another open issue is how to weight individual words in measuring the closeness of a match.",
                "For example, consider the question How many prisoners escaped?.",
                "In the nugget Seven prisoners escaped from a Texas prison, there is no indication that seven is the keyword, and that it must be matched to get any relevance credit.",
                "Using IDF values does not help, since seven will generally not have a higher IDF than words like texas and prison.",
                "Also, redefining the nugget as just seven does not solve the problem since now it might spuriously match any mention of seven out of context.",
                "Nuggeteer [13] works on similar principles but makes binary decisions about whether a nugget is present in a given system response by tuning a threshold.",
                "However, it is also plagued by spurious relevance since not all words contained in the nugget description (or known correct responses) are central to the nugget. 4.1.2 Nugget-Matching Rules We propose a reliable automatic method for determining whether a snippet of text contains a given nugget, based on nugget-matching rules, which are generated using a semiautomatic procedure explained below.",
                "These rules are essentially Boolean queries that will only match against snippets that contain the nugget.",
                "For instance, a candidate rule for matching answers to How many prisoners escaped? is (Texas AND seven AND escape AND (convicts OR prisoners)), possibly with other synonyms and variants in the rule.",
                "For a corpus of news articles, which usually follow a typical formal prose, it is fairly easy to write such simple rules to match expected answers using a bootstrap approach, as described below.",
                "We propose a two-stage approach, inspired by Autoslog [14], that combines the strength of humans in identifying semantically equivalent expressions and the strength of the system in gathering statistical evidence from a humanannotated corpus of documents.",
                "In the first stage, human subjects annotated (using a highlighting tool) portions of ontopic documents that contained answers to each nugget 2 .",
                "In the second stage, subjects used our rule generation tool to create rules that would match the annotations for each nugget.",
                "The tool allows users to enter a Boolean rule as a disjunction of conjunctions (e.g. ((a AND b) OR (a AND c AND d) OR (e))).",
                "Given a candidate rule, our tool uses it as a Boolean query over the entire set of on-topic documents and calculates its recall and precision with respect to the annotations that it is expected to match.",
                "Hence, the subjects can start with a simple rule and iteratively refine it until they are satisfied with its recall and precision.",
                "We observed that it was very easy for humans to improve the precision of a rule by tweaking its existing conjunctions (adding more ANDs), and improving the recall by adding more conjunctions to the disjunction (adding more ORs).",
                "As an example, lets try to create a rule for the nugget which says that seven prisoners escaped from the Texas prison.",
                "We start with a simple rule - (seven).",
                "When we input this into the rule generation tool, we realize that this rule matches many spurious occurrences of seven (e.g. ...seven states...) and thus gets a low precision score.",
                "We can further qualify our rule - Texas AND seven AND convicts.",
                "Next, by looking at the missed annotations, we realize that some news articles mentioned ...seven prisoners escaped.... We then replace convicts with the disjunction (convicts OR prisoners).",
                "We continue tweaking the rule in this manner until we achieve a sufficiently high recall and precision - i.e. the (small number of) misses and false alarms can be safely ignored.",
                "Thus we can create nugget-matching rules that succinctly capture various ways of expressing a nugget, while avoiding matching incorrect (or out of context) responses.",
                "Human involvement in the rule creation process ensures high quality generic rules which can then be used to evaluate arbitrary system responses reliably. 4.2 Evaluating the Utility of a Sequence of Ranked Lists The utility of a retrieval system can be defined as the difference between how much the user gained in terms of useful information, and how much the user lost in terms of time and energy.",
                "We calculate this utility from the utilities of individual passages as follows.",
                "After reading each passage returned by the system, the user derives some gain depending on the presence of relevant and novel information, and incurs a loss in terms of the time and energy spent in going through the passage.",
                "However, the likelihood that the user would actually read a passage depends on its position in the ranked list.",
                "Hence, for a query q, the expected utility 2 LDC [18] already provides relevance judgments for 100 topics on the TDT4 corpus.",
                "We further ensured that these judgments are exhaustive on the entire corpus using pooling. of a passage pi at rank i can be defined as U(pi, q) = P(i) ∗ (Gain(pi, q) − Loss(pi, q)) (4) where P(i) is the probability that the user would go through a passage at rank i.",
                "The expected utility for an entire ranked list of length n can be calculated simply by adding the expected utility of each passage: U(q) = nX i=1 P(i) ∗ (Gain(pi, q) − Loss(pi, q)) (5) Note that if we ignore the loss term and define P(i) as P(i) ∝ 1/ logb(b + i − 1) (6) then we get the recently popularized metric called Discounted Cumulated Gain (DCG) [9], where Gain(pi, q) is defined as the graded relevance of passage pi.",
                "However, without the loss term, DCG is a purely recall-oriented metric and not suitable for an adaptive filtering setting, where the systems utility depends in part on its ability to limit the number of items shown to the user.",
                "Although P(i) could be defined based on empirical studies of user behavior, for simplicity, we use P(i) exactly as defined in equation 6.",
                "The gain G(pi, q) of passage pi with respect to the query q is a function of - 1) the number of relevant nuggets present in pi, and 2) the novelty of each of these nuggets.",
                "We combine these two factors as follows.",
                "For each nugget Nj, we assign an initial weight wj, and also keep a count nj of the number of times this nugget has been seen by the user in the past.",
                "The gain derived from each subsequent occurrence of the same nugget is assumed to reduce by a dampening factor γ.",
                "Thus, G(pi, q) is defined as G(pi, q) = X Nj ∈C(pi,q) wj ∗ γnj (7) where C(pi, q) is the set of all nuggets that appear in passage pi and also belong to the answer key of query q.",
                "The initial weights wj are all set of be 1.0 in our experiments, but can also be set based on a pyramid approach [11].",
                "The choice of dampening factor γ determines the users tolerance for redundancy.",
                "When γ = 0, a nugget will only receive credit for its first occurrence i.e. when nj is zero3 .",
                "For 0 < γ < 1, a nugget receives smaller credit for each successive occurrence.",
                "When γ = 1, no dampening occurs and repeated occurrences of a nugget receive the same credit.",
                "Note that the nugget occurrence counts are preserved between evaluation of successive ranked lists returned by the system, since the users are expected to remember what the system showed them in the past.",
                "We define the loss L(pi, q) as a constant cost c (we use 0.1) incurred when reading a system-returned passage.",
                "Thus, our metric can be re-written as U(q) = nX i=1 Gain(pi, q) logb(b + i − 1) − L(n) (8) where L(n) is the loss associated with a ranked list of length n: L(n) = c · nX i=1 1 logb(b + i − 1) (9) 3 Note that 00 = 1 Due to the similarity with Discounted Cumulated Gain (DCG), we call our metric Discounted Cumulated Utility (DCU).",
                "The DCU score obtained by the system is converted to a Normalized DCU (NDCU) score by dividing it by the DCU score of the ideal ranked list, which is created by ordering passages by their decreasing utility scores U(pi, q) and stopping when U(pi, q) ≤ 0 i.e. when the gain is less than or equal to the cost of reading the passage. 5.",
                "DATA TDT4 was the benchmark corpus used in TDT2002 and TDT2003 evaluations.",
                "The corpus consists of over 90, 000 news articles from multiple sources (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) published between October 2000 and January 2001, in the languages of Arabic, English, and Mandarin.",
                "Speech-recognized and machine-translated versions of the non-English articles were provided as well.",
                "LDC [18] has annotated the corpus with 100 topics, that correspond to various news events in this time period.",
                "Out of these, we selected a subset of 12 actionable events, and defined corresponding tasks for them4 .",
                "For each task, we manually defined a profile consisting of an initial set of (5 to 10) queries, a free-text description of the user history, i.e., what the user already knows about the event, and a list of known on-topic and off-topic documents (if available) as training examples.",
                "For each query, we generated answer keys and corresponding nugget matching rules using the procedure described in section 4.1.2, and produced a total of 120 queries, with an average of 7 nuggets per query. 6.",
                "EXPERIMENTS AND RESULTS 6.1 Baselines We used Indri [17], a popular language-model based retrieval engine, as a baseline for comparison with CAF´E.",
                "Indri supports standard search engine functionality, including pseudo-relevance feedback (PRF) [3, 6], and is representative of a typical query-based retrieval system.",
                "Indri does not support any kind of novelty detection.",
                "We compare Indri with PRF turned on and off, against CAF´E with user feedback, novelty detection and antiredundant ranking turned on and off. 6.2 Experimental Setup We divided the TDT4 corpus spanning 4 months into 10 chunks, each defined as a period of 12 consecutive days.",
                "At any given point of time in the distillation process, each system accessed the past data up to the current point, and returned a ranked list of up 50 passages per query.",
                "The 12 tasks defined on the corpus were divided into a training and test set with 6 tasks each.",
                "Each system was allowed to use the training set to tune its parameters for optimizing NDCU (equation 8), including the relevance threshold for both Indri and CAF´E, and the novelty and antiredundancy thresholds for CAF´E.",
                "The NDCU for each system run is calculated automatically.",
                "User feedback was also simulated - relevance judgments for each system-returned passage (as determined by the nugget matching rules described in section 4.1.2) were 4 URL: http://nyc.lti.cs.cmu.edu/downloads Figure 1: Performance of Indri across chunks Figure 2: Performance of CAF´E across chunks used as user feedback in the adaptation of query profiles and user histories. 6.3 Results In Table 1, we show the NDCU scores of the two systems under various settings.",
                "These scores are averaged over the six tasks in the test set, and are calculated with two dampening factors (see section 4.2): γ = 0 and 0.1, to simulate no tolerance and small tolerance for redundancy, respectively.",
                "Using γ = 0 creates a much more strict metric since it does not give any credit to a passage that contains relevant but redundant information.",
                "Hence, the improvement obtained from enabling user feedback is smaller with γ = 0 than the improvement obtained from feedback with γ = 0.1.",
                "This reveals a shortcoming of contemporary retrieval systemswhen the user gives positive feedback on a passage, the systems gives higher weights to the terms present in that passage and tends to retrieve other passages containing the same terms - and thus - usually the same information.",
                "However, the user does not benefit from seeing such redundant passages, and is usually interested in other passages containing related information.",
                "It is informative to evaluate retrieval systems using our utility measure (with γ = 0) which accounts for novelty and thus gives a more realistic picture of how well a system can generalize from user feedback, rather than using traditional IR measures like recall and precision which give an incomplete picture of improvement obtained from user feedback.",
                "Sometimes, however, users might indeed be interested in seeing the same information from multiple sources, as an Table 1: NDCU Scores of Indri and CAF´E for two dampening factors (γ), and various settings (F: Feedback, N: Novelty Detection, A: Anti-Redundant Ranking) Indri CAF´E γ Base +PRF Base +F +F+N +F+A +F+N+A 0 0.19 0.19 0.22 0.23 0.24 0.24 0.24 0.1 0.28 0.29 0.24 0.35 0.35 0.36 0.36 indicator of its importance or reliability.",
                "In such a case, we can simply choose a higher value for γ which corresponds to a higher tolerance for redundancy, and hence let the system tune its parameters accordingly.",
                "Since documents were processed chunk by chunk, it would be interesting to see how the performance of systems improves over time.",
                "Figures 1 and 2 show the performance trends for both the systems across chunks.",
                "While the performance with and without feedback on the first few chunks is expected to be close, for subsequent chunks, the performance curve with feedback enabled rises above the one with the no-feedback setting.",
                "The performance trends are not consistent across all chunks because on-topic documents are not uniformly distributed over all the chunks, making some queries easier than others in certain chunks.",
                "Moreover, since Indri uses pseudo-relevance feedback while CAF´E uses feedback based on actual relevance judgments, the improvement in case of Indri is less dramatic than that of CAF´E. 7.",
                "CONCLUDING REMARKS This paper presents the first investigation on utility-based information distillation with a system that learns the longlasting information needs from fine-grained user feedback over a sequence of ranked passages.",
                "Our system, called CAF´E, combines adaptive filtering, novelty detection and antiredundant passage ranking in a unified framework for utility optimization.",
                "We developed a new scheme for automated evaluation and feedback based on a semi-automatic procedure for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also proposed an extension of the NDCG metric for assessing the utility of ranked passages as a weighted combination of relevance and novelty.",
                "Our experiments on the newly annotated TDT4 benchmark corpus show encouraging utility enhancement over Indri, and also over our own system with incremental learning and novelty detection turned off. 8.",
                "ACKNOWLEDGMENTS We would like to thank Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng, and the Qualitative Data Analysis Program at the University of Pittsburgh lead by Dr. Stuart Shulman for their help with collecting and processing the extended TDT4 annotations used in our experiments.",
                "This work is supported in parts by the National Science Foundation (NSF) under grant IIS0434035, and the Defense Advanced Research Project Agency (DARPA) under contracts NBCHD030010 and W0550432.",
                "Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors. 9.",
                "ADDITIONAL AUTHORS Jian Zhang (jianzhan@stat.purdue.edu)∗ , Jaime Carbonell (jgc@cs.cmu.edu)† , Peter Brusilovsky (peterb+@pitt.edu)‡ , Daqing He(dah44@pitt.edu)‡ 10.",
                "REFERENCES [1] J. Allan.",
                "Incremental Relevance Feedback for Information Filtering.",
                "Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, pages 270-278, 1996. [2] J. Allan, C. Wade, and A. Bolivar.",
                "Retrieval and Novelty Detection at the Sentence Level.",
                "Proceedings of the ACM SIGIR conference on research and development in information retrieval, 2003. [3] C. Buckley, G. Salton, and J. Allan.",
                "Automatic Retrieval with Locality Information using SMART.",
                "NIST special publication, (500207):59-72, 1993. [4] J. Callan.",
                "Learning While Filtering Documents.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 224-231, 1998. [5] J. Carbonell and J. Goldstein.",
                "The use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 335-336, 1998. [6] E. Efthimiadis.",
                "Query Expansion.",
                "Annual Review of Information Science and Technology (ARIST), 31:p121-87, 1996. [7] J. Fiscus and G. Duddington.",
                "Topic Detection and Tracking Overview.",
                "Topic Detection and Tracking: Event-based Information Organization, pages 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos.",
                "A Statistical Model for Multilingual Entity Detection and Tracking.",
                "NAACL/HLT, 2004. [9] K. J¨arvelin and J. Kek¨al¨ainen.",
                "Cumulated Gain-based Evaluation of IR Techniques.",
                "ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin and D. Demner-Fushman.",
                "Automatically Evaluating Answers to Definition Questions.",
                "Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), 2005. ∗ Statistics Dept., Purdue University, West Lafayette, USA † Language Technologies Inst., Carnegie Mellon University, Pittsburgh, USA ‡ School of Information Sciences, Univ. of Pittsburgh, Pittsburgh, USA [11] J. Lin and D. Demner-Fushman.",
                "Will Pyramids Built of nUggets Topple Over.",
                "Proceedings of HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos.",
                "A Mention-synchronous Coreference Resolution Algorithm based on the Bell Tree.",
                "Proc. of ACL, 4:136-143, 2004. [13] G. Marton.",
                "Nuggeteer: Automatic Nugget-Based Evaluation Using Descriptions and Judgments.",
                "HLT/NAACL, 2006. [14] E. Riloff.",
                "Automatically Constructing a Dictionary for Information Extraction Tasks.",
                "Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 811-816, 1993. [15] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9: Filtering track.",
                "The Ninth Text REtrieval Conference (TREC-9), pages 361-368. [16] R. Schapire, Y.",
                "Singer, and A. Singhal.",
                "Boosting and Rocchio Applied to Text Filtering.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle, and W. Croft.",
                "Indri: A Language Model-based Search Engine for Complex Queries.",
                "Proceedings of the International Conference on Intelligence Analysis, 2004. [18] The Linguistic Data Consortium. http://www.ldc.upenn.edu/. [19] E. Voorhees.",
                "Overview of the TREC 2003 Question Answering Track.",
                "Proceedings of the Twelfth Text REtrieval Conference (TREC 2003), 2003. [20] Y. Yang and B. Kisiel.",
                "Margin-based Local Regression for Adaptive Filtering.",
                "Proceedings of the twelfth international conference on Information and knowledge management, pages 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of Adaptive Filtering Methods in a Cross-benchmark Evaluation.",
                "Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 98-105, 2005. [22] C. Zhai, W. Cohen, and J. Lafferty.",
                "Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 10-17, 2003. [23] J. Zhang and Y. Yang.",
                "Robustness of Regularized Linear Classification Methods in Text Categorization.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 190-197, 2003. [24] Y. Zhang.",
                "Using Bayesian Priors to Combine Classifiers for Adaptive Filtering.",
                "Proceedings of the 27th annual international conference on Research and development in information retrieval, pages 345-352, 2004. [25] Y. Zhang, J. Callan, and T. Minka.",
                "Novelty and Redundancy Detection in Adaptive Filtering.",
                "Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "unify framework": {
            "translated_key": "Marco de unificar",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Utility-based Information Distillation Over Temporally Sequenced Documents Yiming Yang Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA yiming@cs.cmu.edu Abhimanyu Lad Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA alad@cs.cmu.edu Ni Lao Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA nlao@cs.cmu.edu Abhay Harpale Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA aharpale@cs.cmu.edu Bryan Kisiel Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA bkisiel@cs.cmu.edu Monica Rogati Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA mrogati@cs.cmu.edu ABSTRACT This paper examines a new approach to information distillation over temporally ordered documents, and proposes a novel evaluation scheme for such a framework.",
                "It combines the strengths of and extends beyond conventional adaptive filtering, novelty detection and non-redundant passage ranking with respect to long-lasting information needs (tasks with multiple queries).",
                "Our approach supports fine-grained user feedback via highlighting of arbitrary spans of text, and leverages such information for utility optimization in adaptive settings.",
                "For our experiments, we defined hypothetical tasks based on news events in the TDT4 corpus, with multiple queries per task.",
                "Answer keys (nuggets) were generated for each query and a semiautomatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also propose an extension of the NDCG metric for assessing the utility of ranked passages as a combination of relevance and novelty.",
                "Our results show encouraging utility enhancements using the new approach, compared to the baseline systems without incremental learning or the novelty detection components.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 General Terms Design, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Tracking new and relevant information from temporal data streams for users with long-lasting needs has been a challenging research topic in information retrieval.",
                "Adaptive filtering (AF) is one such task of online prediction of the relevance of each new document with respect to pre-defined topics.",
                "Based on the initial query and a few positive examples (if available), an AF system maintains a profile for each such topic of interest, and constantly updates it based on feedback from the user.",
                "The incremental learning nature of AF systems makes them more powerful than standard search engines that support ad-hoc retrieval (e.g.",
                "Google and Yahoo) in terms of finding relevant information with respect to long-lasting topics of interest, and more attractive for users who are willing to provide feedback to adapt the system towards their specific information needs, without having to modify their queries manually.",
                "A variety of supervised learning algorithms (Rocchio-style classifiers, Exponential-Gaussian models, local regression and logistic regression approaches) have been studied for adaptive settings, examined with explicit and implicit relevance feedback, and evaluated with respect to utility optimization on large benchmark data collections in TREC (Text Retrieval Conferences) and TDT (Topic Detection and Tracking) forums [1, 4, 7, 15, 16, 20, 24, 23].",
                "Regularized logistic regression [21] has been found representative for the state-of-the-art approaches, and highly efficient for frequent model adaptations over large document collections such as the TREC-10 corpus (over 800,000 documents and 84 topics).",
                "Despite substantial achievements in recent adaptive filtering research, significant problems remain unsolved regarding how to leverage user feedback effectively and efficiently.",
                "Specifically, the following issues may seriously limit the true utility of AF systems in real-world applications: 1.",
                "User has a rather passive role in the conventional adaptive filtering setup - he or she reacts to the system only when the system makes a yes decision on a document, by confirming or rejecting that decision.",
                "A more active alternative would be to allow the user to issue multiple queries for a topic, review a ranked list of candidate documents (or passages) per query, and provide feedback on the ranked list, thus refining their information need and requesting updated ranked lists.",
                "The latter form of user interaction has been highly effective in standard retrieval for ad-hoc queries.",
                "How to deploy such a strategy for long-lasting information needs in AF settings is an open question for research. 2.",
                "The unit for receiving a relevance judgment (yes or no) is restricted to the document level in conventional AF.",
                "However, a real user may be willing to provide more informative, fine-grained feedback via highlighting some pieces of text in a retrieved document as relevant, instead of labeling the entire document as relevant.",
                "Effectively leveraging such fine-grained feedback could substantially enhance the quality of an AF system.",
                "For this, we need to enable supervised learning from labeled pieces of text of arbitrary span instead of just allowing labeled documents. 3.",
                "System-selected documents are often highly redundant.",
                "A major news event, for example, would be reported by multiple sources repeatedly for a while, making most of the information content in those articles redundant with each other.",
                "A conventional AF system would select all these redundant news stories for user feedback, wasting the users time while offering little gain.",
                "Clearly, techniques for novelty detection can help in principle [25, 2, 22] for improving the utility of the AF systems.",
                "However, the effectiveness of such techniques at passage level to detect novelty with respect to users (fine-grained) feedback and to detect redundancy in ranked lists remains to be evaluated using a measure of utility that mimics the needs of a real user.",
                "To address the above limitations of current AF systems, we propose and examine a new approach in this paper, combining the strengths of conventional AF (incremental learning of topic models), multi-pass passage retrieval for long-lasting queries conditioned on topic, and novelty detection for removal of redundancy from user interactions with the system.",
                "We call the new process utility-based information distillation.",
                "Note that conventional benchmark corpora for AF evaluations, which have relevance judgments at the document level and do not define tasks with multiple queries, are insufficient for evaluating the new approach.",
                "Therefore, we extended a benchmark corpus - the TDT4 collection of news stories and TV broadcasts - with task definitions, multiple queries per task, and answer keys per query.",
                "We have conducted our experiments on this extended TDT4 corpus and have made the additionally generated data publicly available for future comparative evaluations 1 .",
                "To automatically evaluate the system-returned arbitrary spans of text using our answer keys, we further developed an evaluation scheme with semi-automatic procedure for 1 URL: http://nyc.lti.cs.cmu.edu/downloads acquiring rules that can match nuggets against system responses.",
                "Moreover, we propose an extension of NDCG (Normalized Discounted Cumulated Gain) [9] for assessing the utility of ranked passages as a function of both relevance and novelty.",
                "The rest of this paper is organized as follows.",
                "Section 2 outlines the information distillation process with a concrete example.",
                "Section 3 describes the technical cores of our system called CAF´E - CMU Adaptive Filtering Engine.",
                "Section 4 discusses issues with respect to evaluation methodology and proposes a new scheme.",
                "Section 5 describes the extended TDT4 corpus.",
                "Section 6 presents our experiments and results.",
                "Section 7 concludes the study and gives future perspectives. 2.",
                "A SAMPLE TASK Consider a news event - the escape of seven convicts from a Texas prison in December 2000 and their capture a month later.",
                "Assuming a user were interested in this event since its early stage, the information need could be: Find information about the escape of convicts from Texas prison, and information related to their recapture.",
                "The associated lower-level questions could be: 1.",
                "How many prisoners escaped? 2.",
                "Where and when were they sighted? 3. Who are their known contacts inside and outside the prison? 4.",
                "How are they armed? 5.",
                "Do they have any vehicles? 6.",
                "What steps have been taken so far?",
                "We call such an information need a task, and the associated questions as the queries in this task.",
                "A distillation system is supposed to monitor the incoming documents, process them chunk by chunk in a temporal order, select potentially relevant and novel passages from each chunk with respect to each query, and present a ranked list of passages to the user.",
                "Passage ranking here is based on how relevant a passage is with respect to the current query, how novel it is with respect to the current user history (of his or her interactions with the system), and how redundant it is compared to other passages with a higher rank in the list.",
                "When presented with a list of passages, the user may provide feedback by highlighting arbitrary spans of text that he or she found relevant.",
                "These spans of text are taken as positive examples in the adaptation of the query profile, and also added to the users history.",
                "Passages not marked by the user are taken as negative examples.",
                "As soon as the query profile is updated, the system re-issues a search and returns another ranked list of passages where the previously seen passages are either removed or ranked low, based on user preference.",
                "For example, if the user highlights ...officials have posted a $100,000 reward for their capture... as relevant answer to the query What steps have been taken so far?, then the highlighted piece is used as an additional positive training example in the adaptation of the query profile.",
                "This piece of feedback is also added to the user history as a seen example, so that in future, the system will not place another passage mentioning $100,000 reward at the top of the ranked list.",
                "However, an article mentioning ...officials have doubled the reward money to $200,000... might be ranked high since it is both relevant to the (updated) query profile and novel with respect to the (updated) user history.",
                "The user may modify the original queries or add a new query during the process; the query profiles will be changed accordingly.",
                "Clearly, novelty detection is very important for the utility of such a system because of the iterative search.",
                "Without novelty detection, the old relevant passages would be shown to the user repeatedly in each ranked list.",
                "Through the above example, we can see the main properties of our new framework for utility-based information distillation over temporally ordered documents.",
                "Our framework combines and extends the power of adaptive filtering (AF), ad-hoc retrieval (IR) and novelty detection (ND).",
                "Compared to standard IR, our approach has the power of incrementally learning long-term information needs and modeling a sequence of queries within a task.",
                "Compared to conventional AF, it enables a more active role of the user in refining his or her information needs and requesting new results by allowing relevance and novelty feedback via highlighting of arbitrary spans of text in passages returned by the system.",
                "Compared to past work, this is the first evaluation of novelty detection integrated with adaptive filtering for sequenced queries that allows flexible user feedback over ranked passages.",
                "The combination of AF, IR and ND with the new extensions raises an important research question regarding evaluation methodology: how can we measure the utility of such an information distillation system?",
                "Existing metrics in standard IR, AF and ND are insufficient, and new solutions must be explored, as we will discuss in Section 4, after describing the technical cores of our system in the next section. 3.",
                "TECHNICAL CORES The core components of CAF´E are - 1) AF for incremental learning of query profiles, 2) IR for estimating relevance of passages with respect to query profiles, 3) ND for assessing novelty of passages with respect to users history, and 4) anti-redundancy component to remove redundancy from ranked lists. 3.1 Adaptive Filtering Component We use a state-of-the-art algorithm in the field - the regularized logistic regression method which had the best results on several benchmark evaluation corpora for AF [21].",
                "Logistic regression (LR) is a supervised learning algorithm for statistical classification.",
                "Based on a training set of labeled instances, it learns a class model which can then by used to predict the labels of unseen instances.",
                "Its performance as well as efficiency in terms of training time makes it a good candidate when frequent updates of the class model are required, as is the case in adaptive filtering, where the system must learn from each new feedback provided by the user. (See [21] and [23] for computational complexity and implementation issues).",
                "In adaptive filtering, each query is considered as a class and the probability of a passage belonging to this class corresponds to the degree of relevance of the passage with respect to the query.",
                "For training the model, we use the query itself as the initial positive training example of the class, and the user-highlighted pieces of text (marked as Relevant or Not-relevant) during feedback as additional training examples.",
                "To address the cold start issue in the early stage before any user feedback is obtained, the system uses a small sample from a retrospective corpus as the initial negative examples in the training set.",
                "The details of using logistic regression for adaptive filtering (assigning different weights to positive and negative training instances, and regularizing the objective function to prevent over-fitting on training data) are presented in [21].",
                "The class model w∗ learned by Logistic Regression, or the query profile, is a vector whose dimensions are individual terms and whose elements are the regression coefficients, indicating how influential each term is in the query profile.",
                "The query profile is updated whenever a new piece of user feedback is received.",
                "A temporally decaying weight can be applied to each training example, as an option, to emphasize the most recent user feedback. 3.2 Passage Retrieval Component We use standard IR techniques in this part of our system.",
                "Incoming documents are processed in chunks, where each chunk can be defined as a fixed span of time or as a fixed number of documents, as preferred by the user.",
                "For each incoming document, corpus statistics like the IDF (Inverted Document Frequency) of each term are updated.",
                "We use a state-of-the-art named entity identifier and tracker [8, 12] to identify person and location names, and merge them with co-referent named entities seen in the past.",
                "Then the documents are segmented into passages, which can be a whole document, a paragraph, a sentence, or any other continuous span of text, as preferred.",
                "Each passage is represented using a vector of TF-IDF (Term FrequencyInverse Document Frequency) weights, where term can be a word or a named entity.",
                "Given a query profile, i.e. the logistic regression solution w∗ as described in Section 3.1, the system computes the posterior probability of relevance for each passage x as fRL(x) ≡ P(y = 1|x, w∗ ) = 1 (1 + e−w∗·x) (1) Passages are ordered by their relevance scores, and the ones with scores above a threshold (tuned on a training set) comprise the relevance list that is passed on to the novelty detection step. 3.3 Novelty Detection Component CAF´E maintains a user history H(t), which contains all the spans of text hi that the user highlighted (as feedback) during his or her past interactions with the system, up to the current time t. Denoting the history as H(t) = n h1, h2, ..., ht o , (2) the novelty score of a new candidate passage x is computed as: fND(x) = 1 − max i∈1..t {cos(x, hi)} (3) where both candidate passage x and highlighted spans of text hi are represented as TF-IDF vectors.",
                "The novelty score of each passage is compared to a prespecified threshold (also tuned on a training set), and any passage with a score below this threshold is removed from the relevance list. 3.4 Anti-redundant Ranking Component Although the novelty detection component ensures that only novel (previously unseen) information remains in the relevance list, this list might still contain the same novel information at multiple positions in the ranked list.",
                "Suppose, for example, that the user has already read about a $100,000 reward for information about the escaped convicts.",
                "A new piece of news that the award has been increased to $200,000 is novel since the user hasnt read about it yet.",
                "However, multiple news sources would report this news and we might end up showing (redundant) articles from all these sources in a ranked list.",
                "Hence, a ranked list should also be made non-redundant with respect to its own contents.",
                "We use a simplified version of the Maximal Marginal Relevance method [5], originally developed for combining relevance and novelty in text retrieval and summarization.",
                "Our procedure starts with the current list of passages sorted by relevance (section 3.2), filtered by Novelty Detection component (section 3.3), and generates a new non-redundant list as follows: 1.",
                "Take the top passage in the current list as the top one in the new list. 2.",
                "Add the next passage x in the current list to the new list only if fAR(x) > t where fAR(x) = 1 − max pi∈Lnew {cos(x, pi)} and Lnew is the set of passages already selected in the new list. 3.",
                "Repeat step 2 until all the passages in the current list have been examined.",
                "After applying the above-mentioned algorithm, each passage in the new list is sufficiently dissimilar to others, thus favoring diversity rather than redundancy in the new ranked list.",
                "The anti-redundancy threshold t is tuned on a training set. 4.",
                "EVALUATION METHODOLOGY The approach we proposed above for information distillation raises important issues regarding evaluation methodology.",
                "Firstly, since our framework allows the output to be passages at different levels of granularity (e.g. k-sentence windows where k may vary) instead of a fixed length, it is not possible to have pre-annotated relevance judgments at all such granularity levels.",
                "Secondly, since we wish to measure the utility of the system output as a combination of both relevance and novelty, traditional relevance-only based measures must be replaced by measures that penalize the repetition of the same information in the system output across time.",
                "Thirdly, since the output of the system is ranked lists, we must reward those systems that present useful information (both relevant and previously unseen) using shorter ranked lists, and penalize those that present the same information using longer ranked lists.",
                "None of the existing measures in ad-hoc retrieval, adaptive filtering, novelty detection or other related areas (text summarization and question answering) have desirable properties in all the three aspects.",
                "Therefore, we must develop a new evaluation methodology. 4.1 Answer Keys To enable the evaluation of a system whose output consists of passages of arbitrary length, we borrow the concept of answer keys from the Question Answering (QA) community, where systems are allowed to return arbitrary spans of text as answers.",
                "Answer keys define what should be present in a system response to receive credit, and are comprised of a collection of information nuggets, i.e. factoid units about which human assessors can make binary decisions of whether or not a system response contains them.",
                "Defining answer keys and making the associated binary decisions are conceptual tasks that require semantic mapping [19], since system-returned passages can contain the same information expressed in many different ways.",
                "Hence, QA evaluations have relied on human assessors for the mapping between various expressions, making the process costly, time consuming, and not scalable to large query and document collections, and extensive system evaluations with various parameter settings. 4.1.1 Automating Evaluation based on Answer Keys Automatic evaluation methods would allow for faster system building and tuning, as well as provide an objective and affordable way of comparing various systems.",
                "Recently, such methods have been proposed, more or less, based on the idea of n-gram co-occurrences.",
                "Pourpre [10] assigns a fractional recall score to a system response based on its unigram overlap with a given nuggets description.",
                "For example, a system response A B C has recall 3/4 with respect to a nugget with description A B C D. However, such an approach is unfair to systems that present the same information but using words other than A, B, C, and D. Another open issue is how to weight individual words in measuring the closeness of a match.",
                "For example, consider the question How many prisoners escaped?.",
                "In the nugget Seven prisoners escaped from a Texas prison, there is no indication that seven is the keyword, and that it must be matched to get any relevance credit.",
                "Using IDF values does not help, since seven will generally not have a higher IDF than words like texas and prison.",
                "Also, redefining the nugget as just seven does not solve the problem since now it might spuriously match any mention of seven out of context.",
                "Nuggeteer [13] works on similar principles but makes binary decisions about whether a nugget is present in a given system response by tuning a threshold.",
                "However, it is also plagued by spurious relevance since not all words contained in the nugget description (or known correct responses) are central to the nugget. 4.1.2 Nugget-Matching Rules We propose a reliable automatic method for determining whether a snippet of text contains a given nugget, based on nugget-matching rules, which are generated using a semiautomatic procedure explained below.",
                "These rules are essentially Boolean queries that will only match against snippets that contain the nugget.",
                "For instance, a candidate rule for matching answers to How many prisoners escaped? is (Texas AND seven AND escape AND (convicts OR prisoners)), possibly with other synonyms and variants in the rule.",
                "For a corpus of news articles, which usually follow a typical formal prose, it is fairly easy to write such simple rules to match expected answers using a bootstrap approach, as described below.",
                "We propose a two-stage approach, inspired by Autoslog [14], that combines the strength of humans in identifying semantically equivalent expressions and the strength of the system in gathering statistical evidence from a humanannotated corpus of documents.",
                "In the first stage, human subjects annotated (using a highlighting tool) portions of ontopic documents that contained answers to each nugget 2 .",
                "In the second stage, subjects used our rule generation tool to create rules that would match the annotations for each nugget.",
                "The tool allows users to enter a Boolean rule as a disjunction of conjunctions (e.g. ((a AND b) OR (a AND c AND d) OR (e))).",
                "Given a candidate rule, our tool uses it as a Boolean query over the entire set of on-topic documents and calculates its recall and precision with respect to the annotations that it is expected to match.",
                "Hence, the subjects can start with a simple rule and iteratively refine it until they are satisfied with its recall and precision.",
                "We observed that it was very easy for humans to improve the precision of a rule by tweaking its existing conjunctions (adding more ANDs), and improving the recall by adding more conjunctions to the disjunction (adding more ORs).",
                "As an example, lets try to create a rule for the nugget which says that seven prisoners escaped from the Texas prison.",
                "We start with a simple rule - (seven).",
                "When we input this into the rule generation tool, we realize that this rule matches many spurious occurrences of seven (e.g. ...seven states...) and thus gets a low precision score.",
                "We can further qualify our rule - Texas AND seven AND convicts.",
                "Next, by looking at the missed annotations, we realize that some news articles mentioned ...seven prisoners escaped.... We then replace convicts with the disjunction (convicts OR prisoners).",
                "We continue tweaking the rule in this manner until we achieve a sufficiently high recall and precision - i.e. the (small number of) misses and false alarms can be safely ignored.",
                "Thus we can create nugget-matching rules that succinctly capture various ways of expressing a nugget, while avoiding matching incorrect (or out of context) responses.",
                "Human involvement in the rule creation process ensures high quality generic rules which can then be used to evaluate arbitrary system responses reliably. 4.2 Evaluating the Utility of a Sequence of Ranked Lists The utility of a retrieval system can be defined as the difference between how much the user gained in terms of useful information, and how much the user lost in terms of time and energy.",
                "We calculate this utility from the utilities of individual passages as follows.",
                "After reading each passage returned by the system, the user derives some gain depending on the presence of relevant and novel information, and incurs a loss in terms of the time and energy spent in going through the passage.",
                "However, the likelihood that the user would actually read a passage depends on its position in the ranked list.",
                "Hence, for a query q, the expected utility 2 LDC [18] already provides relevance judgments for 100 topics on the TDT4 corpus.",
                "We further ensured that these judgments are exhaustive on the entire corpus using pooling. of a passage pi at rank i can be defined as U(pi, q) = P(i) ∗ (Gain(pi, q) − Loss(pi, q)) (4) where P(i) is the probability that the user would go through a passage at rank i.",
                "The expected utility for an entire ranked list of length n can be calculated simply by adding the expected utility of each passage: U(q) = nX i=1 P(i) ∗ (Gain(pi, q) − Loss(pi, q)) (5) Note that if we ignore the loss term and define P(i) as P(i) ∝ 1/ logb(b + i − 1) (6) then we get the recently popularized metric called Discounted Cumulated Gain (DCG) [9], where Gain(pi, q) is defined as the graded relevance of passage pi.",
                "However, without the loss term, DCG is a purely recall-oriented metric and not suitable for an adaptive filtering setting, where the systems utility depends in part on its ability to limit the number of items shown to the user.",
                "Although P(i) could be defined based on empirical studies of user behavior, for simplicity, we use P(i) exactly as defined in equation 6.",
                "The gain G(pi, q) of passage pi with respect to the query q is a function of - 1) the number of relevant nuggets present in pi, and 2) the novelty of each of these nuggets.",
                "We combine these two factors as follows.",
                "For each nugget Nj, we assign an initial weight wj, and also keep a count nj of the number of times this nugget has been seen by the user in the past.",
                "The gain derived from each subsequent occurrence of the same nugget is assumed to reduce by a dampening factor γ.",
                "Thus, G(pi, q) is defined as G(pi, q) = X Nj ∈C(pi,q) wj ∗ γnj (7) where C(pi, q) is the set of all nuggets that appear in passage pi and also belong to the answer key of query q.",
                "The initial weights wj are all set of be 1.0 in our experiments, but can also be set based on a pyramid approach [11].",
                "The choice of dampening factor γ determines the users tolerance for redundancy.",
                "When γ = 0, a nugget will only receive credit for its first occurrence i.e. when nj is zero3 .",
                "For 0 < γ < 1, a nugget receives smaller credit for each successive occurrence.",
                "When γ = 1, no dampening occurs and repeated occurrences of a nugget receive the same credit.",
                "Note that the nugget occurrence counts are preserved between evaluation of successive ranked lists returned by the system, since the users are expected to remember what the system showed them in the past.",
                "We define the loss L(pi, q) as a constant cost c (we use 0.1) incurred when reading a system-returned passage.",
                "Thus, our metric can be re-written as U(q) = nX i=1 Gain(pi, q) logb(b + i − 1) − L(n) (8) where L(n) is the loss associated with a ranked list of length n: L(n) = c · nX i=1 1 logb(b + i − 1) (9) 3 Note that 00 = 1 Due to the similarity with Discounted Cumulated Gain (DCG), we call our metric Discounted Cumulated Utility (DCU).",
                "The DCU score obtained by the system is converted to a Normalized DCU (NDCU) score by dividing it by the DCU score of the ideal ranked list, which is created by ordering passages by their decreasing utility scores U(pi, q) and stopping when U(pi, q) ≤ 0 i.e. when the gain is less than or equal to the cost of reading the passage. 5.",
                "DATA TDT4 was the benchmark corpus used in TDT2002 and TDT2003 evaluations.",
                "The corpus consists of over 90, 000 news articles from multiple sources (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) published between October 2000 and January 2001, in the languages of Arabic, English, and Mandarin.",
                "Speech-recognized and machine-translated versions of the non-English articles were provided as well.",
                "LDC [18] has annotated the corpus with 100 topics, that correspond to various news events in this time period.",
                "Out of these, we selected a subset of 12 actionable events, and defined corresponding tasks for them4 .",
                "For each task, we manually defined a profile consisting of an initial set of (5 to 10) queries, a free-text description of the user history, i.e., what the user already knows about the event, and a list of known on-topic and off-topic documents (if available) as training examples.",
                "For each query, we generated answer keys and corresponding nugget matching rules using the procedure described in section 4.1.2, and produced a total of 120 queries, with an average of 7 nuggets per query. 6.",
                "EXPERIMENTS AND RESULTS 6.1 Baselines We used Indri [17], a popular language-model based retrieval engine, as a baseline for comparison with CAF´E.",
                "Indri supports standard search engine functionality, including pseudo-relevance feedback (PRF) [3, 6], and is representative of a typical query-based retrieval system.",
                "Indri does not support any kind of novelty detection.",
                "We compare Indri with PRF turned on and off, against CAF´E with user feedback, novelty detection and antiredundant ranking turned on and off. 6.2 Experimental Setup We divided the TDT4 corpus spanning 4 months into 10 chunks, each defined as a period of 12 consecutive days.",
                "At any given point of time in the distillation process, each system accessed the past data up to the current point, and returned a ranked list of up 50 passages per query.",
                "The 12 tasks defined on the corpus were divided into a training and test set with 6 tasks each.",
                "Each system was allowed to use the training set to tune its parameters for optimizing NDCU (equation 8), including the relevance threshold for both Indri and CAF´E, and the novelty and antiredundancy thresholds for CAF´E.",
                "The NDCU for each system run is calculated automatically.",
                "User feedback was also simulated - relevance judgments for each system-returned passage (as determined by the nugget matching rules described in section 4.1.2) were 4 URL: http://nyc.lti.cs.cmu.edu/downloads Figure 1: Performance of Indri across chunks Figure 2: Performance of CAF´E across chunks used as user feedback in the adaptation of query profiles and user histories. 6.3 Results In Table 1, we show the NDCU scores of the two systems under various settings.",
                "These scores are averaged over the six tasks in the test set, and are calculated with two dampening factors (see section 4.2): γ = 0 and 0.1, to simulate no tolerance and small tolerance for redundancy, respectively.",
                "Using γ = 0 creates a much more strict metric since it does not give any credit to a passage that contains relevant but redundant information.",
                "Hence, the improvement obtained from enabling user feedback is smaller with γ = 0 than the improvement obtained from feedback with γ = 0.1.",
                "This reveals a shortcoming of contemporary retrieval systemswhen the user gives positive feedback on a passage, the systems gives higher weights to the terms present in that passage and tends to retrieve other passages containing the same terms - and thus - usually the same information.",
                "However, the user does not benefit from seeing such redundant passages, and is usually interested in other passages containing related information.",
                "It is informative to evaluate retrieval systems using our utility measure (with γ = 0) which accounts for novelty and thus gives a more realistic picture of how well a system can generalize from user feedback, rather than using traditional IR measures like recall and precision which give an incomplete picture of improvement obtained from user feedback.",
                "Sometimes, however, users might indeed be interested in seeing the same information from multiple sources, as an Table 1: NDCU Scores of Indri and CAF´E for two dampening factors (γ), and various settings (F: Feedback, N: Novelty Detection, A: Anti-Redundant Ranking) Indri CAF´E γ Base +PRF Base +F +F+N +F+A +F+N+A 0 0.19 0.19 0.22 0.23 0.24 0.24 0.24 0.1 0.28 0.29 0.24 0.35 0.35 0.36 0.36 indicator of its importance or reliability.",
                "In such a case, we can simply choose a higher value for γ which corresponds to a higher tolerance for redundancy, and hence let the system tune its parameters accordingly.",
                "Since documents were processed chunk by chunk, it would be interesting to see how the performance of systems improves over time.",
                "Figures 1 and 2 show the performance trends for both the systems across chunks.",
                "While the performance with and without feedback on the first few chunks is expected to be close, for subsequent chunks, the performance curve with feedback enabled rises above the one with the no-feedback setting.",
                "The performance trends are not consistent across all chunks because on-topic documents are not uniformly distributed over all the chunks, making some queries easier than others in certain chunks.",
                "Moreover, since Indri uses pseudo-relevance feedback while CAF´E uses feedback based on actual relevance judgments, the improvement in case of Indri is less dramatic than that of CAF´E. 7.",
                "CONCLUDING REMARKS This paper presents the first investigation on utility-based information distillation with a system that learns the longlasting information needs from fine-grained user feedback over a sequence of ranked passages.",
                "Our system, called CAF´E, combines adaptive filtering, novelty detection and antiredundant passage ranking in a unified framework for utility optimization.",
                "We developed a new scheme for automated evaluation and feedback based on a semi-automatic procedure for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also proposed an extension of the NDCG metric for assessing the utility of ranked passages as a weighted combination of relevance and novelty.",
                "Our experiments on the newly annotated TDT4 benchmark corpus show encouraging utility enhancement over Indri, and also over our own system with incremental learning and novelty detection turned off. 8.",
                "ACKNOWLEDGMENTS We would like to thank Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng, and the Qualitative Data Analysis Program at the University of Pittsburgh lead by Dr. Stuart Shulman for their help with collecting and processing the extended TDT4 annotations used in our experiments.",
                "This work is supported in parts by the National Science Foundation (NSF) under grant IIS0434035, and the Defense Advanced Research Project Agency (DARPA) under contracts NBCHD030010 and W0550432.",
                "Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors. 9.",
                "ADDITIONAL AUTHORS Jian Zhang (jianzhan@stat.purdue.edu)∗ , Jaime Carbonell (jgc@cs.cmu.edu)† , Peter Brusilovsky (peterb+@pitt.edu)‡ , Daqing He(dah44@pitt.edu)‡ 10.",
                "REFERENCES [1] J. Allan.",
                "Incremental Relevance Feedback for Information Filtering.",
                "Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, pages 270-278, 1996. [2] J. Allan, C. Wade, and A. Bolivar.",
                "Retrieval and Novelty Detection at the Sentence Level.",
                "Proceedings of the ACM SIGIR conference on research and development in information retrieval, 2003. [3] C. Buckley, G. Salton, and J. Allan.",
                "Automatic Retrieval with Locality Information using SMART.",
                "NIST special publication, (500207):59-72, 1993. [4] J. Callan.",
                "Learning While Filtering Documents.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 224-231, 1998. [5] J. Carbonell and J. Goldstein.",
                "The use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 335-336, 1998. [6] E. Efthimiadis.",
                "Query Expansion.",
                "Annual Review of Information Science and Technology (ARIST), 31:p121-87, 1996. [7] J. Fiscus and G. Duddington.",
                "Topic Detection and Tracking Overview.",
                "Topic Detection and Tracking: Event-based Information Organization, pages 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos.",
                "A Statistical Model for Multilingual Entity Detection and Tracking.",
                "NAACL/HLT, 2004. [9] K. J¨arvelin and J. Kek¨al¨ainen.",
                "Cumulated Gain-based Evaluation of IR Techniques.",
                "ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin and D. Demner-Fushman.",
                "Automatically Evaluating Answers to Definition Questions.",
                "Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), 2005. ∗ Statistics Dept., Purdue University, West Lafayette, USA † Language Technologies Inst., Carnegie Mellon University, Pittsburgh, USA ‡ School of Information Sciences, Univ. of Pittsburgh, Pittsburgh, USA [11] J. Lin and D. Demner-Fushman.",
                "Will Pyramids Built of nUggets Topple Over.",
                "Proceedings of HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos.",
                "A Mention-synchronous Coreference Resolution Algorithm based on the Bell Tree.",
                "Proc. of ACL, 4:136-143, 2004. [13] G. Marton.",
                "Nuggeteer: Automatic Nugget-Based Evaluation Using Descriptions and Judgments.",
                "HLT/NAACL, 2006. [14] E. Riloff.",
                "Automatically Constructing a Dictionary for Information Extraction Tasks.",
                "Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 811-816, 1993. [15] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9: Filtering track.",
                "The Ninth Text REtrieval Conference (TREC-9), pages 361-368. [16] R. Schapire, Y.",
                "Singer, and A. Singhal.",
                "Boosting and Rocchio Applied to Text Filtering.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle, and W. Croft.",
                "Indri: A Language Model-based Search Engine for Complex Queries.",
                "Proceedings of the International Conference on Intelligence Analysis, 2004. [18] The Linguistic Data Consortium. http://www.ldc.upenn.edu/. [19] E. Voorhees.",
                "Overview of the TREC 2003 Question Answering Track.",
                "Proceedings of the Twelfth Text REtrieval Conference (TREC 2003), 2003. [20] Y. Yang and B. Kisiel.",
                "Margin-based Local Regression for Adaptive Filtering.",
                "Proceedings of the twelfth international conference on Information and knowledge management, pages 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of Adaptive Filtering Methods in a Cross-benchmark Evaluation.",
                "Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 98-105, 2005. [22] C. Zhai, W. Cohen, and J. Lafferty.",
                "Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 10-17, 2003. [23] J. Zhang and Y. Yang.",
                "Robustness of Regularized Linear Classification Methods in Text Categorization.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 190-197, 2003. [24] Y. Zhang.",
                "Using Bayesian Priors to Combine Classifiers for Adaptive Filtering.",
                "Proceedings of the 27th annual international conference on Research and development in information retrieval, pages 345-352, 2004. [25] Y. Zhang, J. Callan, and T. Minka.",
                "Novelty and Redundancy Detection in Adaptive Filtering.",
                "Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "adaptive filter": {
            "translated_key": "filtro adaptativo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Utility-based Information Distillation Over Temporally Sequenced Documents Yiming Yang Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA yiming@cs.cmu.edu Abhimanyu Lad Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA alad@cs.cmu.edu Ni Lao Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA nlao@cs.cmu.edu Abhay Harpale Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA aharpale@cs.cmu.edu Bryan Kisiel Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA bkisiel@cs.cmu.edu Monica Rogati Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA mrogati@cs.cmu.edu ABSTRACT This paper examines a new approach to information distillation over temporally ordered documents, and proposes a novel evaluation scheme for such a framework.",
                "It combines the strengths of and extends beyond conventional <br>adaptive filter</br>ing, novelty detection and non-redundant passage ranking with respect to long-lasting information needs (tasks with multiple queries).",
                "Our approach supports fine-grained user feedback via highlighting of arbitrary spans of text, and leverages such information for utility optimization in adaptive settings.",
                "For our experiments, we defined hypothetical tasks based on news events in the TDT4 corpus, with multiple queries per task.",
                "Answer keys (nuggets) were generated for each query and a semiautomatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also propose an extension of the NDCG metric for assessing the utility of ranked passages as a combination of relevance and novelty.",
                "Our results show encouraging utility enhancements using the new approach, compared to the baseline systems without incremental learning or the novelty detection components.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 General Terms Design, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Tracking new and relevant information from temporal data streams for users with long-lasting needs has been a challenging research topic in information retrieval.",
                "Adaptive filtering (AF) is one such task of online prediction of the relevance of each new document with respect to pre-defined topics.",
                "Based on the initial query and a few positive examples (if available), an AF system maintains a profile for each such topic of interest, and constantly updates it based on feedback from the user.",
                "The incremental learning nature of AF systems makes them more powerful than standard search engines that support ad-hoc retrieval (e.g.",
                "Google and Yahoo) in terms of finding relevant information with respect to long-lasting topics of interest, and more attractive for users who are willing to provide feedback to adapt the system towards their specific information needs, without having to modify their queries manually.",
                "A variety of supervised learning algorithms (Rocchio-style classifiers, Exponential-Gaussian models, local regression and logistic regression approaches) have been studied for adaptive settings, examined with explicit and implicit relevance feedback, and evaluated with respect to utility optimization on large benchmark data collections in TREC (Text Retrieval Conferences) and TDT (Topic Detection and Tracking) forums [1, 4, 7, 15, 16, 20, 24, 23].",
                "Regularized logistic regression [21] has been found representative for the state-of-the-art approaches, and highly efficient for frequent model adaptations over large document collections such as the TREC-10 corpus (over 800,000 documents and 84 topics).",
                "Despite substantial achievements in recent <br>adaptive filter</br>ing research, significant problems remain unsolved regarding how to leverage user feedback effectively and efficiently.",
                "Specifically, the following issues may seriously limit the true utility of AF systems in real-world applications: 1.",
                "User has a rather passive role in the conventional <br>adaptive filter</br>ing setup - he or she reacts to the system only when the system makes a yes decision on a document, by confirming or rejecting that decision.",
                "A more active alternative would be to allow the user to issue multiple queries for a topic, review a ranked list of candidate documents (or passages) per query, and provide feedback on the ranked list, thus refining their information need and requesting updated ranked lists.",
                "The latter form of user interaction has been highly effective in standard retrieval for ad-hoc queries.",
                "How to deploy such a strategy for long-lasting information needs in AF settings is an open question for research. 2.",
                "The unit for receiving a relevance judgment (yes or no) is restricted to the document level in conventional AF.",
                "However, a real user may be willing to provide more informative, fine-grained feedback via highlighting some pieces of text in a retrieved document as relevant, instead of labeling the entire document as relevant.",
                "Effectively leveraging such fine-grained feedback could substantially enhance the quality of an AF system.",
                "For this, we need to enable supervised learning from labeled pieces of text of arbitrary span instead of just allowing labeled documents. 3.",
                "System-selected documents are often highly redundant.",
                "A major news event, for example, would be reported by multiple sources repeatedly for a while, making most of the information content in those articles redundant with each other.",
                "A conventional AF system would select all these redundant news stories for user feedback, wasting the users time while offering little gain.",
                "Clearly, techniques for novelty detection can help in principle [25, 2, 22] for improving the utility of the AF systems.",
                "However, the effectiveness of such techniques at passage level to detect novelty with respect to users (fine-grained) feedback and to detect redundancy in ranked lists remains to be evaluated using a measure of utility that mimics the needs of a real user.",
                "To address the above limitations of current AF systems, we propose and examine a new approach in this paper, combining the strengths of conventional AF (incremental learning of topic models), multi-pass passage retrieval for long-lasting queries conditioned on topic, and novelty detection for removal of redundancy from user interactions with the system.",
                "We call the new process utility-based information distillation.",
                "Note that conventional benchmark corpora for AF evaluations, which have relevance judgments at the document level and do not define tasks with multiple queries, are insufficient for evaluating the new approach.",
                "Therefore, we extended a benchmark corpus - the TDT4 collection of news stories and TV broadcasts - with task definitions, multiple queries per task, and answer keys per query.",
                "We have conducted our experiments on this extended TDT4 corpus and have made the additionally generated data publicly available for future comparative evaluations 1 .",
                "To automatically evaluate the system-returned arbitrary spans of text using our answer keys, we further developed an evaluation scheme with semi-automatic procedure for 1 URL: http://nyc.lti.cs.cmu.edu/downloads acquiring rules that can match nuggets against system responses.",
                "Moreover, we propose an extension of NDCG (Normalized Discounted Cumulated Gain) [9] for assessing the utility of ranked passages as a function of both relevance and novelty.",
                "The rest of this paper is organized as follows.",
                "Section 2 outlines the information distillation process with a concrete example.",
                "Section 3 describes the technical cores of our system called CAF´E - CMU Adaptive Filtering Engine.",
                "Section 4 discusses issues with respect to evaluation methodology and proposes a new scheme.",
                "Section 5 describes the extended TDT4 corpus.",
                "Section 6 presents our experiments and results.",
                "Section 7 concludes the study and gives future perspectives. 2.",
                "A SAMPLE TASK Consider a news event - the escape of seven convicts from a Texas prison in December 2000 and their capture a month later.",
                "Assuming a user were interested in this event since its early stage, the information need could be: Find information about the escape of convicts from Texas prison, and information related to their recapture.",
                "The associated lower-level questions could be: 1.",
                "How many prisoners escaped? 2.",
                "Where and when were they sighted? 3. Who are their known contacts inside and outside the prison? 4.",
                "How are they armed? 5.",
                "Do they have any vehicles? 6.",
                "What steps have been taken so far?",
                "We call such an information need a task, and the associated questions as the queries in this task.",
                "A distillation system is supposed to monitor the incoming documents, process them chunk by chunk in a temporal order, select potentially relevant and novel passages from each chunk with respect to each query, and present a ranked list of passages to the user.",
                "Passage ranking here is based on how relevant a passage is with respect to the current query, how novel it is with respect to the current user history (of his or her interactions with the system), and how redundant it is compared to other passages with a higher rank in the list.",
                "When presented with a list of passages, the user may provide feedback by highlighting arbitrary spans of text that he or she found relevant.",
                "These spans of text are taken as positive examples in the adaptation of the query profile, and also added to the users history.",
                "Passages not marked by the user are taken as negative examples.",
                "As soon as the query profile is updated, the system re-issues a search and returns another ranked list of passages where the previously seen passages are either removed or ranked low, based on user preference.",
                "For example, if the user highlights ...officials have posted a $100,000 reward for their capture... as relevant answer to the query What steps have been taken so far?, then the highlighted piece is used as an additional positive training example in the adaptation of the query profile.",
                "This piece of feedback is also added to the user history as a seen example, so that in future, the system will not place another passage mentioning $100,000 reward at the top of the ranked list.",
                "However, an article mentioning ...officials have doubled the reward money to $200,000... might be ranked high since it is both relevant to the (updated) query profile and novel with respect to the (updated) user history.",
                "The user may modify the original queries or add a new query during the process; the query profiles will be changed accordingly.",
                "Clearly, novelty detection is very important for the utility of such a system because of the iterative search.",
                "Without novelty detection, the old relevant passages would be shown to the user repeatedly in each ranked list.",
                "Through the above example, we can see the main properties of our new framework for utility-based information distillation over temporally ordered documents.",
                "Our framework combines and extends the power of <br>adaptive filter</br>ing (AF), ad-hoc retrieval (IR) and novelty detection (ND).",
                "Compared to standard IR, our approach has the power of incrementally learning long-term information needs and modeling a sequence of queries within a task.",
                "Compared to conventional AF, it enables a more active role of the user in refining his or her information needs and requesting new results by allowing relevance and novelty feedback via highlighting of arbitrary spans of text in passages returned by the system.",
                "Compared to past work, this is the first evaluation of novelty detection integrated with <br>adaptive filter</br>ing for sequenced queries that allows flexible user feedback over ranked passages.",
                "The combination of AF, IR and ND with the new extensions raises an important research question regarding evaluation methodology: how can we measure the utility of such an information distillation system?",
                "Existing metrics in standard IR, AF and ND are insufficient, and new solutions must be explored, as we will discuss in Section 4, after describing the technical cores of our system in the next section. 3.",
                "TECHNICAL CORES The core components of CAF´E are - 1) AF for incremental learning of query profiles, 2) IR for estimating relevance of passages with respect to query profiles, 3) ND for assessing novelty of passages with respect to users history, and 4) anti-redundancy component to remove redundancy from ranked lists. 3.1 Adaptive Filtering Component We use a state-of-the-art algorithm in the field - the regularized logistic regression method which had the best results on several benchmark evaluation corpora for AF [21].",
                "Logistic regression (LR) is a supervised learning algorithm for statistical classification.",
                "Based on a training set of labeled instances, it learns a class model which can then by used to predict the labels of unseen instances.",
                "Its performance as well as efficiency in terms of training time makes it a good candidate when frequent updates of the class model are required, as is the case in <br>adaptive filter</br>ing, where the system must learn from each new feedback provided by the user. (See [21] and [23] for computational complexity and implementation issues).",
                "In <br>adaptive filter</br>ing, each query is considered as a class and the probability of a passage belonging to this class corresponds to the degree of relevance of the passage with respect to the query.",
                "For training the model, we use the query itself as the initial positive training example of the class, and the user-highlighted pieces of text (marked as Relevant or Not-relevant) during feedback as additional training examples.",
                "To address the cold start issue in the early stage before any user feedback is obtained, the system uses a small sample from a retrospective corpus as the initial negative examples in the training set.",
                "The details of using logistic regression for <br>adaptive filter</br>ing (assigning different weights to positive and negative training instances, and regularizing the objective function to prevent over-fitting on training data) are presented in [21].",
                "The class model w∗ learned by Logistic Regression, or the query profile, is a vector whose dimensions are individual terms and whose elements are the regression coefficients, indicating how influential each term is in the query profile.",
                "The query profile is updated whenever a new piece of user feedback is received.",
                "A temporally decaying weight can be applied to each training example, as an option, to emphasize the most recent user feedback. 3.2 Passage Retrieval Component We use standard IR techniques in this part of our system.",
                "Incoming documents are processed in chunks, where each chunk can be defined as a fixed span of time or as a fixed number of documents, as preferred by the user.",
                "For each incoming document, corpus statistics like the IDF (Inverted Document Frequency) of each term are updated.",
                "We use a state-of-the-art named entity identifier and tracker [8, 12] to identify person and location names, and merge them with co-referent named entities seen in the past.",
                "Then the documents are segmented into passages, which can be a whole document, a paragraph, a sentence, or any other continuous span of text, as preferred.",
                "Each passage is represented using a vector of TF-IDF (Term FrequencyInverse Document Frequency) weights, where term can be a word or a named entity.",
                "Given a query profile, i.e. the logistic regression solution w∗ as described in Section 3.1, the system computes the posterior probability of relevance for each passage x as fRL(x) ≡ P(y = 1|x, w∗ ) = 1 (1 + e−w∗·x) (1) Passages are ordered by their relevance scores, and the ones with scores above a threshold (tuned on a training set) comprise the relevance list that is passed on to the novelty detection step. 3.3 Novelty Detection Component CAF´E maintains a user history H(t), which contains all the spans of text hi that the user highlighted (as feedback) during his or her past interactions with the system, up to the current time t. Denoting the history as H(t) = n h1, h2, ..., ht o , (2) the novelty score of a new candidate passage x is computed as: fND(x) = 1 − max i∈1..t {cos(x, hi)} (3) where both candidate passage x and highlighted spans of text hi are represented as TF-IDF vectors.",
                "The novelty score of each passage is compared to a prespecified threshold (also tuned on a training set), and any passage with a score below this threshold is removed from the relevance list. 3.4 Anti-redundant Ranking Component Although the novelty detection component ensures that only novel (previously unseen) information remains in the relevance list, this list might still contain the same novel information at multiple positions in the ranked list.",
                "Suppose, for example, that the user has already read about a $100,000 reward for information about the escaped convicts.",
                "A new piece of news that the award has been increased to $200,000 is novel since the user hasnt read about it yet.",
                "However, multiple news sources would report this news and we might end up showing (redundant) articles from all these sources in a ranked list.",
                "Hence, a ranked list should also be made non-redundant with respect to its own contents.",
                "We use a simplified version of the Maximal Marginal Relevance method [5], originally developed for combining relevance and novelty in text retrieval and summarization.",
                "Our procedure starts with the current list of passages sorted by relevance (section 3.2), filtered by Novelty Detection component (section 3.3), and generates a new non-redundant list as follows: 1.",
                "Take the top passage in the current list as the top one in the new list. 2.",
                "Add the next passage x in the current list to the new list only if fAR(x) > t where fAR(x) = 1 − max pi∈Lnew {cos(x, pi)} and Lnew is the set of passages already selected in the new list. 3.",
                "Repeat step 2 until all the passages in the current list have been examined.",
                "After applying the above-mentioned algorithm, each passage in the new list is sufficiently dissimilar to others, thus favoring diversity rather than redundancy in the new ranked list.",
                "The anti-redundancy threshold t is tuned on a training set. 4.",
                "EVALUATION METHODOLOGY The approach we proposed above for information distillation raises important issues regarding evaluation methodology.",
                "Firstly, since our framework allows the output to be passages at different levels of granularity (e.g. k-sentence windows where k may vary) instead of a fixed length, it is not possible to have pre-annotated relevance judgments at all such granularity levels.",
                "Secondly, since we wish to measure the utility of the system output as a combination of both relevance and novelty, traditional relevance-only based measures must be replaced by measures that penalize the repetition of the same information in the system output across time.",
                "Thirdly, since the output of the system is ranked lists, we must reward those systems that present useful information (both relevant and previously unseen) using shorter ranked lists, and penalize those that present the same information using longer ranked lists.",
                "None of the existing measures in ad-hoc retrieval, <br>adaptive filter</br>ing, novelty detection or other related areas (text summarization and question answering) have desirable properties in all the three aspects.",
                "Therefore, we must develop a new evaluation methodology. 4.1 Answer Keys To enable the evaluation of a system whose output consists of passages of arbitrary length, we borrow the concept of answer keys from the Question Answering (QA) community, where systems are allowed to return arbitrary spans of text as answers.",
                "Answer keys define what should be present in a system response to receive credit, and are comprised of a collection of information nuggets, i.e. factoid units about which human assessors can make binary decisions of whether or not a system response contains them.",
                "Defining answer keys and making the associated binary decisions are conceptual tasks that require semantic mapping [19], since system-returned passages can contain the same information expressed in many different ways.",
                "Hence, QA evaluations have relied on human assessors for the mapping between various expressions, making the process costly, time consuming, and not scalable to large query and document collections, and extensive system evaluations with various parameter settings. 4.1.1 Automating Evaluation based on Answer Keys Automatic evaluation methods would allow for faster system building and tuning, as well as provide an objective and affordable way of comparing various systems.",
                "Recently, such methods have been proposed, more or less, based on the idea of n-gram co-occurrences.",
                "Pourpre [10] assigns a fractional recall score to a system response based on its unigram overlap with a given nuggets description.",
                "For example, a system response A B C has recall 3/4 with respect to a nugget with description A B C D. However, such an approach is unfair to systems that present the same information but using words other than A, B, C, and D. Another open issue is how to weight individual words in measuring the closeness of a match.",
                "For example, consider the question How many prisoners escaped?.",
                "In the nugget Seven prisoners escaped from a Texas prison, there is no indication that seven is the keyword, and that it must be matched to get any relevance credit.",
                "Using IDF values does not help, since seven will generally not have a higher IDF than words like texas and prison.",
                "Also, redefining the nugget as just seven does not solve the problem since now it might spuriously match any mention of seven out of context.",
                "Nuggeteer [13] works on similar principles but makes binary decisions about whether a nugget is present in a given system response by tuning a threshold.",
                "However, it is also plagued by spurious relevance since not all words contained in the nugget description (or known correct responses) are central to the nugget. 4.1.2 Nugget-Matching Rules We propose a reliable automatic method for determining whether a snippet of text contains a given nugget, based on nugget-matching rules, which are generated using a semiautomatic procedure explained below.",
                "These rules are essentially Boolean queries that will only match against snippets that contain the nugget.",
                "For instance, a candidate rule for matching answers to How many prisoners escaped? is (Texas AND seven AND escape AND (convicts OR prisoners)), possibly with other synonyms and variants in the rule.",
                "For a corpus of news articles, which usually follow a typical formal prose, it is fairly easy to write such simple rules to match expected answers using a bootstrap approach, as described below.",
                "We propose a two-stage approach, inspired by Autoslog [14], that combines the strength of humans in identifying semantically equivalent expressions and the strength of the system in gathering statistical evidence from a humanannotated corpus of documents.",
                "In the first stage, human subjects annotated (using a highlighting tool) portions of ontopic documents that contained answers to each nugget 2 .",
                "In the second stage, subjects used our rule generation tool to create rules that would match the annotations for each nugget.",
                "The tool allows users to enter a Boolean rule as a disjunction of conjunctions (e.g. ((a AND b) OR (a AND c AND d) OR (e))).",
                "Given a candidate rule, our tool uses it as a Boolean query over the entire set of on-topic documents and calculates its recall and precision with respect to the annotations that it is expected to match.",
                "Hence, the subjects can start with a simple rule and iteratively refine it until they are satisfied with its recall and precision.",
                "We observed that it was very easy for humans to improve the precision of a rule by tweaking its existing conjunctions (adding more ANDs), and improving the recall by adding more conjunctions to the disjunction (adding more ORs).",
                "As an example, lets try to create a rule for the nugget which says that seven prisoners escaped from the Texas prison.",
                "We start with a simple rule - (seven).",
                "When we input this into the rule generation tool, we realize that this rule matches many spurious occurrences of seven (e.g. ...seven states...) and thus gets a low precision score.",
                "We can further qualify our rule - Texas AND seven AND convicts.",
                "Next, by looking at the missed annotations, we realize that some news articles mentioned ...seven prisoners escaped.... We then replace convicts with the disjunction (convicts OR prisoners).",
                "We continue tweaking the rule in this manner until we achieve a sufficiently high recall and precision - i.e. the (small number of) misses and false alarms can be safely ignored.",
                "Thus we can create nugget-matching rules that succinctly capture various ways of expressing a nugget, while avoiding matching incorrect (or out of context) responses.",
                "Human involvement in the rule creation process ensures high quality generic rules which can then be used to evaluate arbitrary system responses reliably. 4.2 Evaluating the Utility of a Sequence of Ranked Lists The utility of a retrieval system can be defined as the difference between how much the user gained in terms of useful information, and how much the user lost in terms of time and energy.",
                "We calculate this utility from the utilities of individual passages as follows.",
                "After reading each passage returned by the system, the user derives some gain depending on the presence of relevant and novel information, and incurs a loss in terms of the time and energy spent in going through the passage.",
                "However, the likelihood that the user would actually read a passage depends on its position in the ranked list.",
                "Hence, for a query q, the expected utility 2 LDC [18] already provides relevance judgments for 100 topics on the TDT4 corpus.",
                "We further ensured that these judgments are exhaustive on the entire corpus using pooling. of a passage pi at rank i can be defined as U(pi, q) = P(i) ∗ (Gain(pi, q) − Loss(pi, q)) (4) where P(i) is the probability that the user would go through a passage at rank i.",
                "The expected utility for an entire ranked list of length n can be calculated simply by adding the expected utility of each passage: U(q) = nX i=1 P(i) ∗ (Gain(pi, q) − Loss(pi, q)) (5) Note that if we ignore the loss term and define P(i) as P(i) ∝ 1/ logb(b + i − 1) (6) then we get the recently popularized metric called Discounted Cumulated Gain (DCG) [9], where Gain(pi, q) is defined as the graded relevance of passage pi.",
                "However, without the loss term, DCG is a purely recall-oriented metric and not suitable for an <br>adaptive filter</br>ing setting, where the systems utility depends in part on its ability to limit the number of items shown to the user.",
                "Although P(i) could be defined based on empirical studies of user behavior, for simplicity, we use P(i) exactly as defined in equation 6.",
                "The gain G(pi, q) of passage pi with respect to the query q is a function of - 1) the number of relevant nuggets present in pi, and 2) the novelty of each of these nuggets.",
                "We combine these two factors as follows.",
                "For each nugget Nj, we assign an initial weight wj, and also keep a count nj of the number of times this nugget has been seen by the user in the past.",
                "The gain derived from each subsequent occurrence of the same nugget is assumed to reduce by a dampening factor γ.",
                "Thus, G(pi, q) is defined as G(pi, q) = X Nj ∈C(pi,q) wj ∗ γnj (7) where C(pi, q) is the set of all nuggets that appear in passage pi and also belong to the answer key of query q.",
                "The initial weights wj are all set of be 1.0 in our experiments, but can also be set based on a pyramid approach [11].",
                "The choice of dampening factor γ determines the users tolerance for redundancy.",
                "When γ = 0, a nugget will only receive credit for its first occurrence i.e. when nj is zero3 .",
                "For 0 < γ < 1, a nugget receives smaller credit for each successive occurrence.",
                "When γ = 1, no dampening occurs and repeated occurrences of a nugget receive the same credit.",
                "Note that the nugget occurrence counts are preserved between evaluation of successive ranked lists returned by the system, since the users are expected to remember what the system showed them in the past.",
                "We define the loss L(pi, q) as a constant cost c (we use 0.1) incurred when reading a system-returned passage.",
                "Thus, our metric can be re-written as U(q) = nX i=1 Gain(pi, q) logb(b + i − 1) − L(n) (8) where L(n) is the loss associated with a ranked list of length n: L(n) = c · nX i=1 1 logb(b + i − 1) (9) 3 Note that 00 = 1 Due to the similarity with Discounted Cumulated Gain (DCG), we call our metric Discounted Cumulated Utility (DCU).",
                "The DCU score obtained by the system is converted to a Normalized DCU (NDCU) score by dividing it by the DCU score of the ideal ranked list, which is created by ordering passages by their decreasing utility scores U(pi, q) and stopping when U(pi, q) ≤ 0 i.e. when the gain is less than or equal to the cost of reading the passage. 5.",
                "DATA TDT4 was the benchmark corpus used in TDT2002 and TDT2003 evaluations.",
                "The corpus consists of over 90, 000 news articles from multiple sources (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) published between October 2000 and January 2001, in the languages of Arabic, English, and Mandarin.",
                "Speech-recognized and machine-translated versions of the non-English articles were provided as well.",
                "LDC [18] has annotated the corpus with 100 topics, that correspond to various news events in this time period.",
                "Out of these, we selected a subset of 12 actionable events, and defined corresponding tasks for them4 .",
                "For each task, we manually defined a profile consisting of an initial set of (5 to 10) queries, a free-text description of the user history, i.e., what the user already knows about the event, and a list of known on-topic and off-topic documents (if available) as training examples.",
                "For each query, we generated answer keys and corresponding nugget matching rules using the procedure described in section 4.1.2, and produced a total of 120 queries, with an average of 7 nuggets per query. 6.",
                "EXPERIMENTS AND RESULTS 6.1 Baselines We used Indri [17], a popular language-model based retrieval engine, as a baseline for comparison with CAF´E.",
                "Indri supports standard search engine functionality, including pseudo-relevance feedback (PRF) [3, 6], and is representative of a typical query-based retrieval system.",
                "Indri does not support any kind of novelty detection.",
                "We compare Indri with PRF turned on and off, against CAF´E with user feedback, novelty detection and antiredundant ranking turned on and off. 6.2 Experimental Setup We divided the TDT4 corpus spanning 4 months into 10 chunks, each defined as a period of 12 consecutive days.",
                "At any given point of time in the distillation process, each system accessed the past data up to the current point, and returned a ranked list of up 50 passages per query.",
                "The 12 tasks defined on the corpus were divided into a training and test set with 6 tasks each.",
                "Each system was allowed to use the training set to tune its parameters for optimizing NDCU (equation 8), including the relevance threshold for both Indri and CAF´E, and the novelty and antiredundancy thresholds for CAF´E.",
                "The NDCU for each system run is calculated automatically.",
                "User feedback was also simulated - relevance judgments for each system-returned passage (as determined by the nugget matching rules described in section 4.1.2) were 4 URL: http://nyc.lti.cs.cmu.edu/downloads Figure 1: Performance of Indri across chunks Figure 2: Performance of CAF´E across chunks used as user feedback in the adaptation of query profiles and user histories. 6.3 Results In Table 1, we show the NDCU scores of the two systems under various settings.",
                "These scores are averaged over the six tasks in the test set, and are calculated with two dampening factors (see section 4.2): γ = 0 and 0.1, to simulate no tolerance and small tolerance for redundancy, respectively.",
                "Using γ = 0 creates a much more strict metric since it does not give any credit to a passage that contains relevant but redundant information.",
                "Hence, the improvement obtained from enabling user feedback is smaller with γ = 0 than the improvement obtained from feedback with γ = 0.1.",
                "This reveals a shortcoming of contemporary retrieval systemswhen the user gives positive feedback on a passage, the systems gives higher weights to the terms present in that passage and tends to retrieve other passages containing the same terms - and thus - usually the same information.",
                "However, the user does not benefit from seeing such redundant passages, and is usually interested in other passages containing related information.",
                "It is informative to evaluate retrieval systems using our utility measure (with γ = 0) which accounts for novelty and thus gives a more realistic picture of how well a system can generalize from user feedback, rather than using traditional IR measures like recall and precision which give an incomplete picture of improvement obtained from user feedback.",
                "Sometimes, however, users might indeed be interested in seeing the same information from multiple sources, as an Table 1: NDCU Scores of Indri and CAF´E for two dampening factors (γ), and various settings (F: Feedback, N: Novelty Detection, A: Anti-Redundant Ranking) Indri CAF´E γ Base +PRF Base +F +F+N +F+A +F+N+A 0 0.19 0.19 0.22 0.23 0.24 0.24 0.24 0.1 0.28 0.29 0.24 0.35 0.35 0.36 0.36 indicator of its importance or reliability.",
                "In such a case, we can simply choose a higher value for γ which corresponds to a higher tolerance for redundancy, and hence let the system tune its parameters accordingly.",
                "Since documents were processed chunk by chunk, it would be interesting to see how the performance of systems improves over time.",
                "Figures 1 and 2 show the performance trends for both the systems across chunks.",
                "While the performance with and without feedback on the first few chunks is expected to be close, for subsequent chunks, the performance curve with feedback enabled rises above the one with the no-feedback setting.",
                "The performance trends are not consistent across all chunks because on-topic documents are not uniformly distributed over all the chunks, making some queries easier than others in certain chunks.",
                "Moreover, since Indri uses pseudo-relevance feedback while CAF´E uses feedback based on actual relevance judgments, the improvement in case of Indri is less dramatic than that of CAF´E. 7.",
                "CONCLUDING REMARKS This paper presents the first investigation on utility-based information distillation with a system that learns the longlasting information needs from fine-grained user feedback over a sequence of ranked passages.",
                "Our system, called CAF´E, combines <br>adaptive filter</br>ing, novelty detection and antiredundant passage ranking in a unified framework for utility optimization.",
                "We developed a new scheme for automated evaluation and feedback based on a semi-automatic procedure for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also proposed an extension of the NDCG metric for assessing the utility of ranked passages as a weighted combination of relevance and novelty.",
                "Our experiments on the newly annotated TDT4 benchmark corpus show encouraging utility enhancement over Indri, and also over our own system with incremental learning and novelty detection turned off. 8.",
                "ACKNOWLEDGMENTS We would like to thank Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng, and the Qualitative Data Analysis Program at the University of Pittsburgh lead by Dr. Stuart Shulman for their help with collecting and processing the extended TDT4 annotations used in our experiments.",
                "This work is supported in parts by the National Science Foundation (NSF) under grant IIS0434035, and the Defense Advanced Research Project Agency (DARPA) under contracts NBCHD030010 and W0550432.",
                "Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors. 9.",
                "ADDITIONAL AUTHORS Jian Zhang (jianzhan@stat.purdue.edu)∗ , Jaime Carbonell (jgc@cs.cmu.edu)† , Peter Brusilovsky (peterb+@pitt.edu)‡ , Daqing He(dah44@pitt.edu)‡ 10.",
                "REFERENCES [1] J. Allan.",
                "Incremental Relevance Feedback for Information Filtering.",
                "Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, pages 270-278, 1996. [2] J. Allan, C. Wade, and A. Bolivar.",
                "Retrieval and Novelty Detection at the Sentence Level.",
                "Proceedings of the ACM SIGIR conference on research and development in information retrieval, 2003. [3] C. Buckley, G. Salton, and J. Allan.",
                "Automatic Retrieval with Locality Information using SMART.",
                "NIST special publication, (500207):59-72, 1993. [4] J. Callan.",
                "Learning While Filtering Documents.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 224-231, 1998. [5] J. Carbonell and J. Goldstein.",
                "The use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 335-336, 1998. [6] E. Efthimiadis.",
                "Query Expansion.",
                "Annual Review of Information Science and Technology (ARIST), 31:p121-87, 1996. [7] J. Fiscus and G. Duddington.",
                "Topic Detection and Tracking Overview.",
                "Topic Detection and Tracking: Event-based Information Organization, pages 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos.",
                "A Statistical Model for Multilingual Entity Detection and Tracking.",
                "NAACL/HLT, 2004. [9] K. J¨arvelin and J. Kek¨al¨ainen.",
                "Cumulated Gain-based Evaluation of IR Techniques.",
                "ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin and D. Demner-Fushman.",
                "Automatically Evaluating Answers to Definition Questions.",
                "Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), 2005. ∗ Statistics Dept., Purdue University, West Lafayette, USA † Language Technologies Inst., Carnegie Mellon University, Pittsburgh, USA ‡ School of Information Sciences, Univ. of Pittsburgh, Pittsburgh, USA [11] J. Lin and D. Demner-Fushman.",
                "Will Pyramids Built of nUggets Topple Over.",
                "Proceedings of HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos.",
                "A Mention-synchronous Coreference Resolution Algorithm based on the Bell Tree.",
                "Proc. of ACL, 4:136-143, 2004. [13] G. Marton.",
                "Nuggeteer: Automatic Nugget-Based Evaluation Using Descriptions and Judgments.",
                "HLT/NAACL, 2006. [14] E. Riloff.",
                "Automatically Constructing a Dictionary for Information Extraction Tasks.",
                "Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 811-816, 1993. [15] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9: Filtering track.",
                "The Ninth Text REtrieval Conference (TREC-9), pages 361-368. [16] R. Schapire, Y.",
                "Singer, and A. Singhal.",
                "Boosting and Rocchio Applied to Text Filtering.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle, and W. Croft.",
                "Indri: A Language Model-based Search Engine for Complex Queries.",
                "Proceedings of the International Conference on Intelligence Analysis, 2004. [18] The Linguistic Data Consortium. http://www.ldc.upenn.edu/. [19] E. Voorhees.",
                "Overview of the TREC 2003 Question Answering Track.",
                "Proceedings of the Twelfth Text REtrieval Conference (TREC 2003), 2003. [20] Y. Yang and B. Kisiel.",
                "Margin-based Local Regression for Adaptive Filtering.",
                "Proceedings of the twelfth international conference on Information and knowledge management, pages 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of Adaptive Filtering Methods in a Cross-benchmark Evaluation.",
                "Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 98-105, 2005. [22] C. Zhai, W. Cohen, and J. Lafferty.",
                "Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 10-17, 2003. [23] J. Zhang and Y. Yang.",
                "Robustness of Regularized Linear Classification Methods in Text Categorization.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 190-197, 2003. [24] Y. Zhang.",
                "Using Bayesian Priors to Combine Classifiers for Adaptive Filtering.",
                "Proceedings of the 27th annual international conference on Research and development in information retrieval, pages 345-352, 2004. [25] Y. Zhang, J. Callan, and T. Minka.",
                "Novelty and Redundancy Detection in Adaptive Filtering.",
                "Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Combina las fortalezas y se extiende más allá del \"filtro adaptativo\" convencional, la detección de novedad y la clasificación de pasaje no redundante con respecto a las necesidades de información duradera (tareas con múltiples consultas).",
                "A pesar de los logros sustanciales en la investigación reciente de \"filtro adaptativo\", los problemas significativos siguen sin resolverse con respecto a cómo aprovechar la retroalimentación de los usuarios de manera efectiva y eficiente.",
                "El usuario tiene un papel bastante pasivo en la configuración convencional de \"filtro adaptativo\": él o ella reacciona al sistema solo cuando el sistema toma una decisión Sí en un documento, al confirmar o rechazar esa decisión.",
                "Nuestro marco combina y extiende el poder del \"filtro adaptativo\" ing (AF), la recuperación ad-hoc (IR) y la detección de novedad (ND).",
                "En comparación con el trabajo pasado, esta es la primera evaluación de la detección de novedad integrada con el \"filtro adaptativo\" para consultas secuenciadas que permite la retroalimentación flexible de los usuarios sobre los pasajes clasificados.",
                "Su rendimiento, así como la eficiencia en términos de tiempo de capacitación, lo convierten en un buen candidato cuando se requieren actualizaciones frecuentes del modelo de clase, como es el caso en el \"filtro adaptativo\", donde el sistema debe aprender de cada nuevo retroalimentación proporcionado por el usuario.(Ver [21] y [23] para los problemas de complejidad y implementación computacional).",
                "En el \"filtro adaptativo\", cada consulta se considera una clase y la probabilidad de un pasaje que pertenece a esta clase corresponde al grado de relevancia del pasaje con respecto a la consulta.",
                "Los detalles del uso de la regresión logística para el \"filtro adaptativo\" (asignando diferentes pesos a instancias de entrenamiento positivas y negativas, y regularizando la función objetivo para evitar el ajuste en exceso en los datos de entrenamiento) se presentan en [21].",
                "Ninguna de las medidas existentes en la recuperación ad-hoc, el \"filtro adaptativo\", la detección de novedades u otras áreas relacionadas (resumen de texto y respuesta de preguntas) tienen propiedades deseables en los tres aspectos.",
                "Sin embargo, sin el término de pérdida, DCG es una métrica puramente orientada al retiro y no es adecuada para una configuración de \"filtro adaptativo\", donde la utilidad de los sistemas depende en parte de su capacidad para limitar el número de elementos que se muestran al usuario.",
                "Nuestro sistema, llamado CAF´E, combina el \"filtro adaptativo\", la detección de novedad y la clasificación de pasos antiredes en un marco unificado para la optimización de servicios públicos."
            ],
            "translated_text": "",
            "candidates": [
                "filtro adaptativo",
                "filtro adaptativo",
                "filtro adaptativo",
                "filtro adaptativo",
                "filtro adaptativo",
                "filtro adaptativo",
                "filtro adaptativo",
                "filtro adaptativo",
                "filtro adaptativo",
                "filtro adaptativo",
                "filtro adaptativo",
                "filtro adaptativo",
                "filtro adaptativo",
                "filtro adaptativo",
                "filtro adaptativo",
                "filtro adaptativo",
                "filtro adaptativo",
                "filtro adaptativo",
                "filtro adaptativo",
                "filtro adaptativo",
                "filtro adaptativo",
                "filtro adaptativo"
            ],
            "error": []
        },
        "passage rank": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Utility-based Information Distillation Over Temporally Sequenced Documents Yiming Yang Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA yiming@cs.cmu.edu Abhimanyu Lad Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA alad@cs.cmu.edu Ni Lao Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA nlao@cs.cmu.edu Abhay Harpale Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA aharpale@cs.cmu.edu Bryan Kisiel Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA bkisiel@cs.cmu.edu Monica Rogati Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA mrogati@cs.cmu.edu ABSTRACT This paper examines a new approach to information distillation over temporally ordered documents, and proposes a novel evaluation scheme for such a framework.",
                "It combines the strengths of and extends beyond conventional adaptive filtering, novelty detection and non-redundant <br>passage rank</br>ing with respect to long-lasting information needs (tasks with multiple queries).",
                "Our approach supports fine-grained user feedback via highlighting of arbitrary spans of text, and leverages such information for utility optimization in adaptive settings.",
                "For our experiments, we defined hypothetical tasks based on news events in the TDT4 corpus, with multiple queries per task.",
                "Answer keys (nuggets) were generated for each query and a semiautomatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also propose an extension of the NDCG metric for assessing the utility of ranked passages as a combination of relevance and novelty.",
                "Our results show encouraging utility enhancements using the new approach, compared to the baseline systems without incremental learning or the novelty detection components.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 General Terms Design, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Tracking new and relevant information from temporal data streams for users with long-lasting needs has been a challenging research topic in information retrieval.",
                "Adaptive filtering (AF) is one such task of online prediction of the relevance of each new document with respect to pre-defined topics.",
                "Based on the initial query and a few positive examples (if available), an AF system maintains a profile for each such topic of interest, and constantly updates it based on feedback from the user.",
                "The incremental learning nature of AF systems makes them more powerful than standard search engines that support ad-hoc retrieval (e.g.",
                "Google and Yahoo) in terms of finding relevant information with respect to long-lasting topics of interest, and more attractive for users who are willing to provide feedback to adapt the system towards their specific information needs, without having to modify their queries manually.",
                "A variety of supervised learning algorithms (Rocchio-style classifiers, Exponential-Gaussian models, local regression and logistic regression approaches) have been studied for adaptive settings, examined with explicit and implicit relevance feedback, and evaluated with respect to utility optimization on large benchmark data collections in TREC (Text Retrieval Conferences) and TDT (Topic Detection and Tracking) forums [1, 4, 7, 15, 16, 20, 24, 23].",
                "Regularized logistic regression [21] has been found representative for the state-of-the-art approaches, and highly efficient for frequent model adaptations over large document collections such as the TREC-10 corpus (over 800,000 documents and 84 topics).",
                "Despite substantial achievements in recent adaptive filtering research, significant problems remain unsolved regarding how to leverage user feedback effectively and efficiently.",
                "Specifically, the following issues may seriously limit the true utility of AF systems in real-world applications: 1.",
                "User has a rather passive role in the conventional adaptive filtering setup - he or she reacts to the system only when the system makes a yes decision on a document, by confirming or rejecting that decision.",
                "A more active alternative would be to allow the user to issue multiple queries for a topic, review a ranked list of candidate documents (or passages) per query, and provide feedback on the ranked list, thus refining their information need and requesting updated ranked lists.",
                "The latter form of user interaction has been highly effective in standard retrieval for ad-hoc queries.",
                "How to deploy such a strategy for long-lasting information needs in AF settings is an open question for research. 2.",
                "The unit for receiving a relevance judgment (yes or no) is restricted to the document level in conventional AF.",
                "However, a real user may be willing to provide more informative, fine-grained feedback via highlighting some pieces of text in a retrieved document as relevant, instead of labeling the entire document as relevant.",
                "Effectively leveraging such fine-grained feedback could substantially enhance the quality of an AF system.",
                "For this, we need to enable supervised learning from labeled pieces of text of arbitrary span instead of just allowing labeled documents. 3.",
                "System-selected documents are often highly redundant.",
                "A major news event, for example, would be reported by multiple sources repeatedly for a while, making most of the information content in those articles redundant with each other.",
                "A conventional AF system would select all these redundant news stories for user feedback, wasting the users time while offering little gain.",
                "Clearly, techniques for novelty detection can help in principle [25, 2, 22] for improving the utility of the AF systems.",
                "However, the effectiveness of such techniques at passage level to detect novelty with respect to users (fine-grained) feedback and to detect redundancy in ranked lists remains to be evaluated using a measure of utility that mimics the needs of a real user.",
                "To address the above limitations of current AF systems, we propose and examine a new approach in this paper, combining the strengths of conventional AF (incremental learning of topic models), multi-pass passage retrieval for long-lasting queries conditioned on topic, and novelty detection for removal of redundancy from user interactions with the system.",
                "We call the new process utility-based information distillation.",
                "Note that conventional benchmark corpora for AF evaluations, which have relevance judgments at the document level and do not define tasks with multiple queries, are insufficient for evaluating the new approach.",
                "Therefore, we extended a benchmark corpus - the TDT4 collection of news stories and TV broadcasts - with task definitions, multiple queries per task, and answer keys per query.",
                "We have conducted our experiments on this extended TDT4 corpus and have made the additionally generated data publicly available for future comparative evaluations 1 .",
                "To automatically evaluate the system-returned arbitrary spans of text using our answer keys, we further developed an evaluation scheme with semi-automatic procedure for 1 URL: http://nyc.lti.cs.cmu.edu/downloads acquiring rules that can match nuggets against system responses.",
                "Moreover, we propose an extension of NDCG (Normalized Discounted Cumulated Gain) [9] for assessing the utility of ranked passages as a function of both relevance and novelty.",
                "The rest of this paper is organized as follows.",
                "Section 2 outlines the information distillation process with a concrete example.",
                "Section 3 describes the technical cores of our system called CAF´E - CMU Adaptive Filtering Engine.",
                "Section 4 discusses issues with respect to evaluation methodology and proposes a new scheme.",
                "Section 5 describes the extended TDT4 corpus.",
                "Section 6 presents our experiments and results.",
                "Section 7 concludes the study and gives future perspectives. 2.",
                "A SAMPLE TASK Consider a news event - the escape of seven convicts from a Texas prison in December 2000 and their capture a month later.",
                "Assuming a user were interested in this event since its early stage, the information need could be: Find information about the escape of convicts from Texas prison, and information related to their recapture.",
                "The associated lower-level questions could be: 1.",
                "How many prisoners escaped? 2.",
                "Where and when were they sighted? 3. Who are their known contacts inside and outside the prison? 4.",
                "How are they armed? 5.",
                "Do they have any vehicles? 6.",
                "What steps have been taken so far?",
                "We call such an information need a task, and the associated questions as the queries in this task.",
                "A distillation system is supposed to monitor the incoming documents, process them chunk by chunk in a temporal order, select potentially relevant and novel passages from each chunk with respect to each query, and present a ranked list of passages to the user.",
                "Passage ranking here is based on how relevant a passage is with respect to the current query, how novel it is with respect to the current user history (of his or her interactions with the system), and how redundant it is compared to other passages with a higher rank in the list.",
                "When presented with a list of passages, the user may provide feedback by highlighting arbitrary spans of text that he or she found relevant.",
                "These spans of text are taken as positive examples in the adaptation of the query profile, and also added to the users history.",
                "Passages not marked by the user are taken as negative examples.",
                "As soon as the query profile is updated, the system re-issues a search and returns another ranked list of passages where the previously seen passages are either removed or ranked low, based on user preference.",
                "For example, if the user highlights ...officials have posted a $100,000 reward for their capture... as relevant answer to the query What steps have been taken so far?, then the highlighted piece is used as an additional positive training example in the adaptation of the query profile.",
                "This piece of feedback is also added to the user history as a seen example, so that in future, the system will not place another passage mentioning $100,000 reward at the top of the ranked list.",
                "However, an article mentioning ...officials have doubled the reward money to $200,000... might be ranked high since it is both relevant to the (updated) query profile and novel with respect to the (updated) user history.",
                "The user may modify the original queries or add a new query during the process; the query profiles will be changed accordingly.",
                "Clearly, novelty detection is very important for the utility of such a system because of the iterative search.",
                "Without novelty detection, the old relevant passages would be shown to the user repeatedly in each ranked list.",
                "Through the above example, we can see the main properties of our new framework for utility-based information distillation over temporally ordered documents.",
                "Our framework combines and extends the power of adaptive filtering (AF), ad-hoc retrieval (IR) and novelty detection (ND).",
                "Compared to standard IR, our approach has the power of incrementally learning long-term information needs and modeling a sequence of queries within a task.",
                "Compared to conventional AF, it enables a more active role of the user in refining his or her information needs and requesting new results by allowing relevance and novelty feedback via highlighting of arbitrary spans of text in passages returned by the system.",
                "Compared to past work, this is the first evaluation of novelty detection integrated with adaptive filtering for sequenced queries that allows flexible user feedback over ranked passages.",
                "The combination of AF, IR and ND with the new extensions raises an important research question regarding evaluation methodology: how can we measure the utility of such an information distillation system?",
                "Existing metrics in standard IR, AF and ND are insufficient, and new solutions must be explored, as we will discuss in Section 4, after describing the technical cores of our system in the next section. 3.",
                "TECHNICAL CORES The core components of CAF´E are - 1) AF for incremental learning of query profiles, 2) IR for estimating relevance of passages with respect to query profiles, 3) ND for assessing novelty of passages with respect to users history, and 4) anti-redundancy component to remove redundancy from ranked lists. 3.1 Adaptive Filtering Component We use a state-of-the-art algorithm in the field - the regularized logistic regression method which had the best results on several benchmark evaluation corpora for AF [21].",
                "Logistic regression (LR) is a supervised learning algorithm for statistical classification.",
                "Based on a training set of labeled instances, it learns a class model which can then by used to predict the labels of unseen instances.",
                "Its performance as well as efficiency in terms of training time makes it a good candidate when frequent updates of the class model are required, as is the case in adaptive filtering, where the system must learn from each new feedback provided by the user. (See [21] and [23] for computational complexity and implementation issues).",
                "In adaptive filtering, each query is considered as a class and the probability of a passage belonging to this class corresponds to the degree of relevance of the passage with respect to the query.",
                "For training the model, we use the query itself as the initial positive training example of the class, and the user-highlighted pieces of text (marked as Relevant or Not-relevant) during feedback as additional training examples.",
                "To address the cold start issue in the early stage before any user feedback is obtained, the system uses a small sample from a retrospective corpus as the initial negative examples in the training set.",
                "The details of using logistic regression for adaptive filtering (assigning different weights to positive and negative training instances, and regularizing the objective function to prevent over-fitting on training data) are presented in [21].",
                "The class model w∗ learned by Logistic Regression, or the query profile, is a vector whose dimensions are individual terms and whose elements are the regression coefficients, indicating how influential each term is in the query profile.",
                "The query profile is updated whenever a new piece of user feedback is received.",
                "A temporally decaying weight can be applied to each training example, as an option, to emphasize the most recent user feedback. 3.2 Passage Retrieval Component We use standard IR techniques in this part of our system.",
                "Incoming documents are processed in chunks, where each chunk can be defined as a fixed span of time or as a fixed number of documents, as preferred by the user.",
                "For each incoming document, corpus statistics like the IDF (Inverted Document Frequency) of each term are updated.",
                "We use a state-of-the-art named entity identifier and tracker [8, 12] to identify person and location names, and merge them with co-referent named entities seen in the past.",
                "Then the documents are segmented into passages, which can be a whole document, a paragraph, a sentence, or any other continuous span of text, as preferred.",
                "Each passage is represented using a vector of TF-IDF (Term FrequencyInverse Document Frequency) weights, where term can be a word or a named entity.",
                "Given a query profile, i.e. the logistic regression solution w∗ as described in Section 3.1, the system computes the posterior probability of relevance for each passage x as fRL(x) ≡ P(y = 1|x, w∗ ) = 1 (1 + e−w∗·x) (1) Passages are ordered by their relevance scores, and the ones with scores above a threshold (tuned on a training set) comprise the relevance list that is passed on to the novelty detection step. 3.3 Novelty Detection Component CAF´E maintains a user history H(t), which contains all the spans of text hi that the user highlighted (as feedback) during his or her past interactions with the system, up to the current time t. Denoting the history as H(t) = n h1, h2, ..., ht o , (2) the novelty score of a new candidate passage x is computed as: fND(x) = 1 − max i∈1..t {cos(x, hi)} (3) where both candidate passage x and highlighted spans of text hi are represented as TF-IDF vectors.",
                "The novelty score of each passage is compared to a prespecified threshold (also tuned on a training set), and any passage with a score below this threshold is removed from the relevance list. 3.4 Anti-redundant Ranking Component Although the novelty detection component ensures that only novel (previously unseen) information remains in the relevance list, this list might still contain the same novel information at multiple positions in the ranked list.",
                "Suppose, for example, that the user has already read about a $100,000 reward for information about the escaped convicts.",
                "A new piece of news that the award has been increased to $200,000 is novel since the user hasnt read about it yet.",
                "However, multiple news sources would report this news and we might end up showing (redundant) articles from all these sources in a ranked list.",
                "Hence, a ranked list should also be made non-redundant with respect to its own contents.",
                "We use a simplified version of the Maximal Marginal Relevance method [5], originally developed for combining relevance and novelty in text retrieval and summarization.",
                "Our procedure starts with the current list of passages sorted by relevance (section 3.2), filtered by Novelty Detection component (section 3.3), and generates a new non-redundant list as follows: 1.",
                "Take the top passage in the current list as the top one in the new list. 2.",
                "Add the next passage x in the current list to the new list only if fAR(x) > t where fAR(x) = 1 − max pi∈Lnew {cos(x, pi)} and Lnew is the set of passages already selected in the new list. 3.",
                "Repeat step 2 until all the passages in the current list have been examined.",
                "After applying the above-mentioned algorithm, each passage in the new list is sufficiently dissimilar to others, thus favoring diversity rather than redundancy in the new ranked list.",
                "The anti-redundancy threshold t is tuned on a training set. 4.",
                "EVALUATION METHODOLOGY The approach we proposed above for information distillation raises important issues regarding evaluation methodology.",
                "Firstly, since our framework allows the output to be passages at different levels of granularity (e.g. k-sentence windows where k may vary) instead of a fixed length, it is not possible to have pre-annotated relevance judgments at all such granularity levels.",
                "Secondly, since we wish to measure the utility of the system output as a combination of both relevance and novelty, traditional relevance-only based measures must be replaced by measures that penalize the repetition of the same information in the system output across time.",
                "Thirdly, since the output of the system is ranked lists, we must reward those systems that present useful information (both relevant and previously unseen) using shorter ranked lists, and penalize those that present the same information using longer ranked lists.",
                "None of the existing measures in ad-hoc retrieval, adaptive filtering, novelty detection or other related areas (text summarization and question answering) have desirable properties in all the three aspects.",
                "Therefore, we must develop a new evaluation methodology. 4.1 Answer Keys To enable the evaluation of a system whose output consists of passages of arbitrary length, we borrow the concept of answer keys from the Question Answering (QA) community, where systems are allowed to return arbitrary spans of text as answers.",
                "Answer keys define what should be present in a system response to receive credit, and are comprised of a collection of information nuggets, i.e. factoid units about which human assessors can make binary decisions of whether or not a system response contains them.",
                "Defining answer keys and making the associated binary decisions are conceptual tasks that require semantic mapping [19], since system-returned passages can contain the same information expressed in many different ways.",
                "Hence, QA evaluations have relied on human assessors for the mapping between various expressions, making the process costly, time consuming, and not scalable to large query and document collections, and extensive system evaluations with various parameter settings. 4.1.1 Automating Evaluation based on Answer Keys Automatic evaluation methods would allow for faster system building and tuning, as well as provide an objective and affordable way of comparing various systems.",
                "Recently, such methods have been proposed, more or less, based on the idea of n-gram co-occurrences.",
                "Pourpre [10] assigns a fractional recall score to a system response based on its unigram overlap with a given nuggets description.",
                "For example, a system response A B C has recall 3/4 with respect to a nugget with description A B C D. However, such an approach is unfair to systems that present the same information but using words other than A, B, C, and D. Another open issue is how to weight individual words in measuring the closeness of a match.",
                "For example, consider the question How many prisoners escaped?.",
                "In the nugget Seven prisoners escaped from a Texas prison, there is no indication that seven is the keyword, and that it must be matched to get any relevance credit.",
                "Using IDF values does not help, since seven will generally not have a higher IDF than words like texas and prison.",
                "Also, redefining the nugget as just seven does not solve the problem since now it might spuriously match any mention of seven out of context.",
                "Nuggeteer [13] works on similar principles but makes binary decisions about whether a nugget is present in a given system response by tuning a threshold.",
                "However, it is also plagued by spurious relevance since not all words contained in the nugget description (or known correct responses) are central to the nugget. 4.1.2 Nugget-Matching Rules We propose a reliable automatic method for determining whether a snippet of text contains a given nugget, based on nugget-matching rules, which are generated using a semiautomatic procedure explained below.",
                "These rules are essentially Boolean queries that will only match against snippets that contain the nugget.",
                "For instance, a candidate rule for matching answers to How many prisoners escaped? is (Texas AND seven AND escape AND (convicts OR prisoners)), possibly with other synonyms and variants in the rule.",
                "For a corpus of news articles, which usually follow a typical formal prose, it is fairly easy to write such simple rules to match expected answers using a bootstrap approach, as described below.",
                "We propose a two-stage approach, inspired by Autoslog [14], that combines the strength of humans in identifying semantically equivalent expressions and the strength of the system in gathering statistical evidence from a humanannotated corpus of documents.",
                "In the first stage, human subjects annotated (using a highlighting tool) portions of ontopic documents that contained answers to each nugget 2 .",
                "In the second stage, subjects used our rule generation tool to create rules that would match the annotations for each nugget.",
                "The tool allows users to enter a Boolean rule as a disjunction of conjunctions (e.g. ((a AND b) OR (a AND c AND d) OR (e))).",
                "Given a candidate rule, our tool uses it as a Boolean query over the entire set of on-topic documents and calculates its recall and precision with respect to the annotations that it is expected to match.",
                "Hence, the subjects can start with a simple rule and iteratively refine it until they are satisfied with its recall and precision.",
                "We observed that it was very easy for humans to improve the precision of a rule by tweaking its existing conjunctions (adding more ANDs), and improving the recall by adding more conjunctions to the disjunction (adding more ORs).",
                "As an example, lets try to create a rule for the nugget which says that seven prisoners escaped from the Texas prison.",
                "We start with a simple rule - (seven).",
                "When we input this into the rule generation tool, we realize that this rule matches many spurious occurrences of seven (e.g. ...seven states...) and thus gets a low precision score.",
                "We can further qualify our rule - Texas AND seven AND convicts.",
                "Next, by looking at the missed annotations, we realize that some news articles mentioned ...seven prisoners escaped.... We then replace convicts with the disjunction (convicts OR prisoners).",
                "We continue tweaking the rule in this manner until we achieve a sufficiently high recall and precision - i.e. the (small number of) misses and false alarms can be safely ignored.",
                "Thus we can create nugget-matching rules that succinctly capture various ways of expressing a nugget, while avoiding matching incorrect (or out of context) responses.",
                "Human involvement in the rule creation process ensures high quality generic rules which can then be used to evaluate arbitrary system responses reliably. 4.2 Evaluating the Utility of a Sequence of Ranked Lists The utility of a retrieval system can be defined as the difference between how much the user gained in terms of useful information, and how much the user lost in terms of time and energy.",
                "We calculate this utility from the utilities of individual passages as follows.",
                "After reading each passage returned by the system, the user derives some gain depending on the presence of relevant and novel information, and incurs a loss in terms of the time and energy spent in going through the passage.",
                "However, the likelihood that the user would actually read a passage depends on its position in the ranked list.",
                "Hence, for a query q, the expected utility 2 LDC [18] already provides relevance judgments for 100 topics on the TDT4 corpus.",
                "We further ensured that these judgments are exhaustive on the entire corpus using pooling. of a passage pi at rank i can be defined as U(pi, q) = P(i) ∗ (Gain(pi, q) − Loss(pi, q)) (4) where P(i) is the probability that the user would go through a passage at rank i.",
                "The expected utility for an entire ranked list of length n can be calculated simply by adding the expected utility of each passage: U(q) = nX i=1 P(i) ∗ (Gain(pi, q) − Loss(pi, q)) (5) Note that if we ignore the loss term and define P(i) as P(i) ∝ 1/ logb(b + i − 1) (6) then we get the recently popularized metric called Discounted Cumulated Gain (DCG) [9], where Gain(pi, q) is defined as the graded relevance of passage pi.",
                "However, without the loss term, DCG is a purely recall-oriented metric and not suitable for an adaptive filtering setting, where the systems utility depends in part on its ability to limit the number of items shown to the user.",
                "Although P(i) could be defined based on empirical studies of user behavior, for simplicity, we use P(i) exactly as defined in equation 6.",
                "The gain G(pi, q) of passage pi with respect to the query q is a function of - 1) the number of relevant nuggets present in pi, and 2) the novelty of each of these nuggets.",
                "We combine these two factors as follows.",
                "For each nugget Nj, we assign an initial weight wj, and also keep a count nj of the number of times this nugget has been seen by the user in the past.",
                "The gain derived from each subsequent occurrence of the same nugget is assumed to reduce by a dampening factor γ.",
                "Thus, G(pi, q) is defined as G(pi, q) = X Nj ∈C(pi,q) wj ∗ γnj (7) where C(pi, q) is the set of all nuggets that appear in passage pi and also belong to the answer key of query q.",
                "The initial weights wj are all set of be 1.0 in our experiments, but can also be set based on a pyramid approach [11].",
                "The choice of dampening factor γ determines the users tolerance for redundancy.",
                "When γ = 0, a nugget will only receive credit for its first occurrence i.e. when nj is zero3 .",
                "For 0 < γ < 1, a nugget receives smaller credit for each successive occurrence.",
                "When γ = 1, no dampening occurs and repeated occurrences of a nugget receive the same credit.",
                "Note that the nugget occurrence counts are preserved between evaluation of successive ranked lists returned by the system, since the users are expected to remember what the system showed them in the past.",
                "We define the loss L(pi, q) as a constant cost c (we use 0.1) incurred when reading a system-returned passage.",
                "Thus, our metric can be re-written as U(q) = nX i=1 Gain(pi, q) logb(b + i − 1) − L(n) (8) where L(n) is the loss associated with a ranked list of length n: L(n) = c · nX i=1 1 logb(b + i − 1) (9) 3 Note that 00 = 1 Due to the similarity with Discounted Cumulated Gain (DCG), we call our metric Discounted Cumulated Utility (DCU).",
                "The DCU score obtained by the system is converted to a Normalized DCU (NDCU) score by dividing it by the DCU score of the ideal ranked list, which is created by ordering passages by their decreasing utility scores U(pi, q) and stopping when U(pi, q) ≤ 0 i.e. when the gain is less than or equal to the cost of reading the passage. 5.",
                "DATA TDT4 was the benchmark corpus used in TDT2002 and TDT2003 evaluations.",
                "The corpus consists of over 90, 000 news articles from multiple sources (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) published between October 2000 and January 2001, in the languages of Arabic, English, and Mandarin.",
                "Speech-recognized and machine-translated versions of the non-English articles were provided as well.",
                "LDC [18] has annotated the corpus with 100 topics, that correspond to various news events in this time period.",
                "Out of these, we selected a subset of 12 actionable events, and defined corresponding tasks for them4 .",
                "For each task, we manually defined a profile consisting of an initial set of (5 to 10) queries, a free-text description of the user history, i.e., what the user already knows about the event, and a list of known on-topic and off-topic documents (if available) as training examples.",
                "For each query, we generated answer keys and corresponding nugget matching rules using the procedure described in section 4.1.2, and produced a total of 120 queries, with an average of 7 nuggets per query. 6.",
                "EXPERIMENTS AND RESULTS 6.1 Baselines We used Indri [17], a popular language-model based retrieval engine, as a baseline for comparison with CAF´E.",
                "Indri supports standard search engine functionality, including pseudo-relevance feedback (PRF) [3, 6], and is representative of a typical query-based retrieval system.",
                "Indri does not support any kind of novelty detection.",
                "We compare Indri with PRF turned on and off, against CAF´E with user feedback, novelty detection and antiredundant ranking turned on and off. 6.2 Experimental Setup We divided the TDT4 corpus spanning 4 months into 10 chunks, each defined as a period of 12 consecutive days.",
                "At any given point of time in the distillation process, each system accessed the past data up to the current point, and returned a ranked list of up 50 passages per query.",
                "The 12 tasks defined on the corpus were divided into a training and test set with 6 tasks each.",
                "Each system was allowed to use the training set to tune its parameters for optimizing NDCU (equation 8), including the relevance threshold for both Indri and CAF´E, and the novelty and antiredundancy thresholds for CAF´E.",
                "The NDCU for each system run is calculated automatically.",
                "User feedback was also simulated - relevance judgments for each system-returned passage (as determined by the nugget matching rules described in section 4.1.2) were 4 URL: http://nyc.lti.cs.cmu.edu/downloads Figure 1: Performance of Indri across chunks Figure 2: Performance of CAF´E across chunks used as user feedback in the adaptation of query profiles and user histories. 6.3 Results In Table 1, we show the NDCU scores of the two systems under various settings.",
                "These scores are averaged over the six tasks in the test set, and are calculated with two dampening factors (see section 4.2): γ = 0 and 0.1, to simulate no tolerance and small tolerance for redundancy, respectively.",
                "Using γ = 0 creates a much more strict metric since it does not give any credit to a passage that contains relevant but redundant information.",
                "Hence, the improvement obtained from enabling user feedback is smaller with γ = 0 than the improvement obtained from feedback with γ = 0.1.",
                "This reveals a shortcoming of contemporary retrieval systemswhen the user gives positive feedback on a passage, the systems gives higher weights to the terms present in that passage and tends to retrieve other passages containing the same terms - and thus - usually the same information.",
                "However, the user does not benefit from seeing such redundant passages, and is usually interested in other passages containing related information.",
                "It is informative to evaluate retrieval systems using our utility measure (with γ = 0) which accounts for novelty and thus gives a more realistic picture of how well a system can generalize from user feedback, rather than using traditional IR measures like recall and precision which give an incomplete picture of improvement obtained from user feedback.",
                "Sometimes, however, users might indeed be interested in seeing the same information from multiple sources, as an Table 1: NDCU Scores of Indri and CAF´E for two dampening factors (γ), and various settings (F: Feedback, N: Novelty Detection, A: Anti-Redundant Ranking) Indri CAF´E γ Base +PRF Base +F +F+N +F+A +F+N+A 0 0.19 0.19 0.22 0.23 0.24 0.24 0.24 0.1 0.28 0.29 0.24 0.35 0.35 0.36 0.36 indicator of its importance or reliability.",
                "In such a case, we can simply choose a higher value for γ which corresponds to a higher tolerance for redundancy, and hence let the system tune its parameters accordingly.",
                "Since documents were processed chunk by chunk, it would be interesting to see how the performance of systems improves over time.",
                "Figures 1 and 2 show the performance trends for both the systems across chunks.",
                "While the performance with and without feedback on the first few chunks is expected to be close, for subsequent chunks, the performance curve with feedback enabled rises above the one with the no-feedback setting.",
                "The performance trends are not consistent across all chunks because on-topic documents are not uniformly distributed over all the chunks, making some queries easier than others in certain chunks.",
                "Moreover, since Indri uses pseudo-relevance feedback while CAF´E uses feedback based on actual relevance judgments, the improvement in case of Indri is less dramatic than that of CAF´E. 7.",
                "CONCLUDING REMARKS This paper presents the first investigation on utility-based information distillation with a system that learns the longlasting information needs from fine-grained user feedback over a sequence of ranked passages.",
                "Our system, called CAF´E, combines adaptive filtering, novelty detection and antiredundant <br>passage rank</br>ing in a unified framework for utility optimization.",
                "We developed a new scheme for automated evaluation and feedback based on a semi-automatic procedure for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also proposed an extension of the NDCG metric for assessing the utility of ranked passages as a weighted combination of relevance and novelty.",
                "Our experiments on the newly annotated TDT4 benchmark corpus show encouraging utility enhancement over Indri, and also over our own system with incremental learning and novelty detection turned off. 8.",
                "ACKNOWLEDGMENTS We would like to thank Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng, and the Qualitative Data Analysis Program at the University of Pittsburgh lead by Dr. Stuart Shulman for their help with collecting and processing the extended TDT4 annotations used in our experiments.",
                "This work is supported in parts by the National Science Foundation (NSF) under grant IIS0434035, and the Defense Advanced Research Project Agency (DARPA) under contracts NBCHD030010 and W0550432.",
                "Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors. 9.",
                "ADDITIONAL AUTHORS Jian Zhang (jianzhan@stat.purdue.edu)∗ , Jaime Carbonell (jgc@cs.cmu.edu)† , Peter Brusilovsky (peterb+@pitt.edu)‡ , Daqing He(dah44@pitt.edu)‡ 10.",
                "REFERENCES [1] J. Allan.",
                "Incremental Relevance Feedback for Information Filtering.",
                "Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, pages 270-278, 1996. [2] J. Allan, C. Wade, and A. Bolivar.",
                "Retrieval and Novelty Detection at the Sentence Level.",
                "Proceedings of the ACM SIGIR conference on research and development in information retrieval, 2003. [3] C. Buckley, G. Salton, and J. Allan.",
                "Automatic Retrieval with Locality Information using SMART.",
                "NIST special publication, (500207):59-72, 1993. [4] J. Callan.",
                "Learning While Filtering Documents.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 224-231, 1998. [5] J. Carbonell and J. Goldstein.",
                "The use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 335-336, 1998. [6] E. Efthimiadis.",
                "Query Expansion.",
                "Annual Review of Information Science and Technology (ARIST), 31:p121-87, 1996. [7] J. Fiscus and G. Duddington.",
                "Topic Detection and Tracking Overview.",
                "Topic Detection and Tracking: Event-based Information Organization, pages 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos.",
                "A Statistical Model for Multilingual Entity Detection and Tracking.",
                "NAACL/HLT, 2004. [9] K. J¨arvelin and J. Kek¨al¨ainen.",
                "Cumulated Gain-based Evaluation of IR Techniques.",
                "ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin and D. Demner-Fushman.",
                "Automatically Evaluating Answers to Definition Questions.",
                "Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), 2005. ∗ Statistics Dept., Purdue University, West Lafayette, USA † Language Technologies Inst., Carnegie Mellon University, Pittsburgh, USA ‡ School of Information Sciences, Univ. of Pittsburgh, Pittsburgh, USA [11] J. Lin and D. Demner-Fushman.",
                "Will Pyramids Built of nUggets Topple Over.",
                "Proceedings of HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos.",
                "A Mention-synchronous Coreference Resolution Algorithm based on the Bell Tree.",
                "Proc. of ACL, 4:136-143, 2004. [13] G. Marton.",
                "Nuggeteer: Automatic Nugget-Based Evaluation Using Descriptions and Judgments.",
                "HLT/NAACL, 2006. [14] E. Riloff.",
                "Automatically Constructing a Dictionary for Information Extraction Tasks.",
                "Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 811-816, 1993. [15] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9: Filtering track.",
                "The Ninth Text REtrieval Conference (TREC-9), pages 361-368. [16] R. Schapire, Y.",
                "Singer, and A. Singhal.",
                "Boosting and Rocchio Applied to Text Filtering.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle, and W. Croft.",
                "Indri: A Language Model-based Search Engine for Complex Queries.",
                "Proceedings of the International Conference on Intelligence Analysis, 2004. [18] The Linguistic Data Consortium. http://www.ldc.upenn.edu/. [19] E. Voorhees.",
                "Overview of the TREC 2003 Question Answering Track.",
                "Proceedings of the Twelfth Text REtrieval Conference (TREC 2003), 2003. [20] Y. Yang and B. Kisiel.",
                "Margin-based Local Regression for Adaptive Filtering.",
                "Proceedings of the twelfth international conference on Information and knowledge management, pages 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of Adaptive Filtering Methods in a Cross-benchmark Evaluation.",
                "Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 98-105, 2005. [22] C. Zhai, W. Cohen, and J. Lafferty.",
                "Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 10-17, 2003. [23] J. Zhang and Y. Yang.",
                "Robustness of Regularized Linear Classification Methods in Text Categorization.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 190-197, 2003. [24] Y. Zhang.",
                "Using Bayesian Priors to Combine Classifiers for Adaptive Filtering.",
                "Proceedings of the 27th annual international conference on Research and development in information retrieval, pages 345-352, 2004. [25] Y. Zhang, J. Callan, and T. Minka.",
                "Novelty and Redundancy Detection in Adaptive Filtering.",
                "Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Combina las fortalezas y se extiende más allá del filtrado adaptativo convencional, la detección de novedad y el \"rango de paso\" no redundante con respecto a las necesidades de información duradera (tareas con múltiples consultas).",
                "Nuestro sistema, llamado CAF´E, combina filtrado adaptativo, detección de novedad y \"rango de paso\" antiredado en un marco unificado para la optimización de servicios públicos."
            ],
            "translated_text": "",
            "candidates": [
                "rango de pasaje",
                "rango de paso",
                "rango de pasaje",
                "rango de paso"
            ],
            "error": []
        },
        "ﬂexible user feedback": {
            "translated_key": "Comentarios de los usuarios flexibles",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Utility-based Information Distillation Over Temporally Sequenced Documents Yiming Yang Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA yiming@cs.cmu.edu Abhimanyu Lad Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA alad@cs.cmu.edu Ni Lao Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA nlao@cs.cmu.edu Abhay Harpale Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA aharpale@cs.cmu.edu Bryan Kisiel Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA bkisiel@cs.cmu.edu Monica Rogati Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA mrogati@cs.cmu.edu ABSTRACT This paper examines a new approach to information distillation over temporally ordered documents, and proposes a novel evaluation scheme for such a framework.",
                "It combines the strengths of and extends beyond conventional adaptive filtering, novelty detection and non-redundant passage ranking with respect to long-lasting information needs (tasks with multiple queries).",
                "Our approach supports fine-grained user feedback via highlighting of arbitrary spans of text, and leverages such information for utility optimization in adaptive settings.",
                "For our experiments, we defined hypothetical tasks based on news events in the TDT4 corpus, with multiple queries per task.",
                "Answer keys (nuggets) were generated for each query and a semiautomatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also propose an extension of the NDCG metric for assessing the utility of ranked passages as a combination of relevance and novelty.",
                "Our results show encouraging utility enhancements using the new approach, compared to the baseline systems without incremental learning or the novelty detection components.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 General Terms Design, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Tracking new and relevant information from temporal data streams for users with long-lasting needs has been a challenging research topic in information retrieval.",
                "Adaptive filtering (AF) is one such task of online prediction of the relevance of each new document with respect to pre-defined topics.",
                "Based on the initial query and a few positive examples (if available), an AF system maintains a profile for each such topic of interest, and constantly updates it based on feedback from the user.",
                "The incremental learning nature of AF systems makes them more powerful than standard search engines that support ad-hoc retrieval (e.g.",
                "Google and Yahoo) in terms of finding relevant information with respect to long-lasting topics of interest, and more attractive for users who are willing to provide feedback to adapt the system towards their specific information needs, without having to modify their queries manually.",
                "A variety of supervised learning algorithms (Rocchio-style classifiers, Exponential-Gaussian models, local regression and logistic regression approaches) have been studied for adaptive settings, examined with explicit and implicit relevance feedback, and evaluated with respect to utility optimization on large benchmark data collections in TREC (Text Retrieval Conferences) and TDT (Topic Detection and Tracking) forums [1, 4, 7, 15, 16, 20, 24, 23].",
                "Regularized logistic regression [21] has been found representative for the state-of-the-art approaches, and highly efficient for frequent model adaptations over large document collections such as the TREC-10 corpus (over 800,000 documents and 84 topics).",
                "Despite substantial achievements in recent adaptive filtering research, significant problems remain unsolved regarding how to leverage user feedback effectively and efficiently.",
                "Specifically, the following issues may seriously limit the true utility of AF systems in real-world applications: 1.",
                "User has a rather passive role in the conventional adaptive filtering setup - he or she reacts to the system only when the system makes a yes decision on a document, by confirming or rejecting that decision.",
                "A more active alternative would be to allow the user to issue multiple queries for a topic, review a ranked list of candidate documents (or passages) per query, and provide feedback on the ranked list, thus refining their information need and requesting updated ranked lists.",
                "The latter form of user interaction has been highly effective in standard retrieval for ad-hoc queries.",
                "How to deploy such a strategy for long-lasting information needs in AF settings is an open question for research. 2.",
                "The unit for receiving a relevance judgment (yes or no) is restricted to the document level in conventional AF.",
                "However, a real user may be willing to provide more informative, fine-grained feedback via highlighting some pieces of text in a retrieved document as relevant, instead of labeling the entire document as relevant.",
                "Effectively leveraging such fine-grained feedback could substantially enhance the quality of an AF system.",
                "For this, we need to enable supervised learning from labeled pieces of text of arbitrary span instead of just allowing labeled documents. 3.",
                "System-selected documents are often highly redundant.",
                "A major news event, for example, would be reported by multiple sources repeatedly for a while, making most of the information content in those articles redundant with each other.",
                "A conventional AF system would select all these redundant news stories for user feedback, wasting the users time while offering little gain.",
                "Clearly, techniques for novelty detection can help in principle [25, 2, 22] for improving the utility of the AF systems.",
                "However, the effectiveness of such techniques at passage level to detect novelty with respect to users (fine-grained) feedback and to detect redundancy in ranked lists remains to be evaluated using a measure of utility that mimics the needs of a real user.",
                "To address the above limitations of current AF systems, we propose and examine a new approach in this paper, combining the strengths of conventional AF (incremental learning of topic models), multi-pass passage retrieval for long-lasting queries conditioned on topic, and novelty detection for removal of redundancy from user interactions with the system.",
                "We call the new process utility-based information distillation.",
                "Note that conventional benchmark corpora for AF evaluations, which have relevance judgments at the document level and do not define tasks with multiple queries, are insufficient for evaluating the new approach.",
                "Therefore, we extended a benchmark corpus - the TDT4 collection of news stories and TV broadcasts - with task definitions, multiple queries per task, and answer keys per query.",
                "We have conducted our experiments on this extended TDT4 corpus and have made the additionally generated data publicly available for future comparative evaluations 1 .",
                "To automatically evaluate the system-returned arbitrary spans of text using our answer keys, we further developed an evaluation scheme with semi-automatic procedure for 1 URL: http://nyc.lti.cs.cmu.edu/downloads acquiring rules that can match nuggets against system responses.",
                "Moreover, we propose an extension of NDCG (Normalized Discounted Cumulated Gain) [9] for assessing the utility of ranked passages as a function of both relevance and novelty.",
                "The rest of this paper is organized as follows.",
                "Section 2 outlines the information distillation process with a concrete example.",
                "Section 3 describes the technical cores of our system called CAF´E - CMU Adaptive Filtering Engine.",
                "Section 4 discusses issues with respect to evaluation methodology and proposes a new scheme.",
                "Section 5 describes the extended TDT4 corpus.",
                "Section 6 presents our experiments and results.",
                "Section 7 concludes the study and gives future perspectives. 2.",
                "A SAMPLE TASK Consider a news event - the escape of seven convicts from a Texas prison in December 2000 and their capture a month later.",
                "Assuming a user were interested in this event since its early stage, the information need could be: Find information about the escape of convicts from Texas prison, and information related to their recapture.",
                "The associated lower-level questions could be: 1.",
                "How many prisoners escaped? 2.",
                "Where and when were they sighted? 3. Who are their known contacts inside and outside the prison? 4.",
                "How are they armed? 5.",
                "Do they have any vehicles? 6.",
                "What steps have been taken so far?",
                "We call such an information need a task, and the associated questions as the queries in this task.",
                "A distillation system is supposed to monitor the incoming documents, process them chunk by chunk in a temporal order, select potentially relevant and novel passages from each chunk with respect to each query, and present a ranked list of passages to the user.",
                "Passage ranking here is based on how relevant a passage is with respect to the current query, how novel it is with respect to the current user history (of his or her interactions with the system), and how redundant it is compared to other passages with a higher rank in the list.",
                "When presented with a list of passages, the user may provide feedback by highlighting arbitrary spans of text that he or she found relevant.",
                "These spans of text are taken as positive examples in the adaptation of the query profile, and also added to the users history.",
                "Passages not marked by the user are taken as negative examples.",
                "As soon as the query profile is updated, the system re-issues a search and returns another ranked list of passages where the previously seen passages are either removed or ranked low, based on user preference.",
                "For example, if the user highlights ...officials have posted a $100,000 reward for their capture... as relevant answer to the query What steps have been taken so far?, then the highlighted piece is used as an additional positive training example in the adaptation of the query profile.",
                "This piece of feedback is also added to the user history as a seen example, so that in future, the system will not place another passage mentioning $100,000 reward at the top of the ranked list.",
                "However, an article mentioning ...officials have doubled the reward money to $200,000... might be ranked high since it is both relevant to the (updated) query profile and novel with respect to the (updated) user history.",
                "The user may modify the original queries or add a new query during the process; the query profiles will be changed accordingly.",
                "Clearly, novelty detection is very important for the utility of such a system because of the iterative search.",
                "Without novelty detection, the old relevant passages would be shown to the user repeatedly in each ranked list.",
                "Through the above example, we can see the main properties of our new framework for utility-based information distillation over temporally ordered documents.",
                "Our framework combines and extends the power of adaptive filtering (AF), ad-hoc retrieval (IR) and novelty detection (ND).",
                "Compared to standard IR, our approach has the power of incrementally learning long-term information needs and modeling a sequence of queries within a task.",
                "Compared to conventional AF, it enables a more active role of the user in refining his or her information needs and requesting new results by allowing relevance and novelty feedback via highlighting of arbitrary spans of text in passages returned by the system.",
                "Compared to past work, this is the first evaluation of novelty detection integrated with adaptive filtering for sequenced queries that allows flexible user feedback over ranked passages.",
                "The combination of AF, IR and ND with the new extensions raises an important research question regarding evaluation methodology: how can we measure the utility of such an information distillation system?",
                "Existing metrics in standard IR, AF and ND are insufficient, and new solutions must be explored, as we will discuss in Section 4, after describing the technical cores of our system in the next section. 3.",
                "TECHNICAL CORES The core components of CAF´E are - 1) AF for incremental learning of query profiles, 2) IR for estimating relevance of passages with respect to query profiles, 3) ND for assessing novelty of passages with respect to users history, and 4) anti-redundancy component to remove redundancy from ranked lists. 3.1 Adaptive Filtering Component We use a state-of-the-art algorithm in the field - the regularized logistic regression method which had the best results on several benchmark evaluation corpora for AF [21].",
                "Logistic regression (LR) is a supervised learning algorithm for statistical classification.",
                "Based on a training set of labeled instances, it learns a class model which can then by used to predict the labels of unseen instances.",
                "Its performance as well as efficiency in terms of training time makes it a good candidate when frequent updates of the class model are required, as is the case in adaptive filtering, where the system must learn from each new feedback provided by the user. (See [21] and [23] for computational complexity and implementation issues).",
                "In adaptive filtering, each query is considered as a class and the probability of a passage belonging to this class corresponds to the degree of relevance of the passage with respect to the query.",
                "For training the model, we use the query itself as the initial positive training example of the class, and the user-highlighted pieces of text (marked as Relevant or Not-relevant) during feedback as additional training examples.",
                "To address the cold start issue in the early stage before any user feedback is obtained, the system uses a small sample from a retrospective corpus as the initial negative examples in the training set.",
                "The details of using logistic regression for adaptive filtering (assigning different weights to positive and negative training instances, and regularizing the objective function to prevent over-fitting on training data) are presented in [21].",
                "The class model w∗ learned by Logistic Regression, or the query profile, is a vector whose dimensions are individual terms and whose elements are the regression coefficients, indicating how influential each term is in the query profile.",
                "The query profile is updated whenever a new piece of user feedback is received.",
                "A temporally decaying weight can be applied to each training example, as an option, to emphasize the most recent user feedback. 3.2 Passage Retrieval Component We use standard IR techniques in this part of our system.",
                "Incoming documents are processed in chunks, where each chunk can be defined as a fixed span of time or as a fixed number of documents, as preferred by the user.",
                "For each incoming document, corpus statistics like the IDF (Inverted Document Frequency) of each term are updated.",
                "We use a state-of-the-art named entity identifier and tracker [8, 12] to identify person and location names, and merge them with co-referent named entities seen in the past.",
                "Then the documents are segmented into passages, which can be a whole document, a paragraph, a sentence, or any other continuous span of text, as preferred.",
                "Each passage is represented using a vector of TF-IDF (Term FrequencyInverse Document Frequency) weights, where term can be a word or a named entity.",
                "Given a query profile, i.e. the logistic regression solution w∗ as described in Section 3.1, the system computes the posterior probability of relevance for each passage x as fRL(x) ≡ P(y = 1|x, w∗ ) = 1 (1 + e−w∗·x) (1) Passages are ordered by their relevance scores, and the ones with scores above a threshold (tuned on a training set) comprise the relevance list that is passed on to the novelty detection step. 3.3 Novelty Detection Component CAF´E maintains a user history H(t), which contains all the spans of text hi that the user highlighted (as feedback) during his or her past interactions with the system, up to the current time t. Denoting the history as H(t) = n h1, h2, ..., ht o , (2) the novelty score of a new candidate passage x is computed as: fND(x) = 1 − max i∈1..t {cos(x, hi)} (3) where both candidate passage x and highlighted spans of text hi are represented as TF-IDF vectors.",
                "The novelty score of each passage is compared to a prespecified threshold (also tuned on a training set), and any passage with a score below this threshold is removed from the relevance list. 3.4 Anti-redundant Ranking Component Although the novelty detection component ensures that only novel (previously unseen) information remains in the relevance list, this list might still contain the same novel information at multiple positions in the ranked list.",
                "Suppose, for example, that the user has already read about a $100,000 reward for information about the escaped convicts.",
                "A new piece of news that the award has been increased to $200,000 is novel since the user hasnt read about it yet.",
                "However, multiple news sources would report this news and we might end up showing (redundant) articles from all these sources in a ranked list.",
                "Hence, a ranked list should also be made non-redundant with respect to its own contents.",
                "We use a simplified version of the Maximal Marginal Relevance method [5], originally developed for combining relevance and novelty in text retrieval and summarization.",
                "Our procedure starts with the current list of passages sorted by relevance (section 3.2), filtered by Novelty Detection component (section 3.3), and generates a new non-redundant list as follows: 1.",
                "Take the top passage in the current list as the top one in the new list. 2.",
                "Add the next passage x in the current list to the new list only if fAR(x) > t where fAR(x) = 1 − max pi∈Lnew {cos(x, pi)} and Lnew is the set of passages already selected in the new list. 3.",
                "Repeat step 2 until all the passages in the current list have been examined.",
                "After applying the above-mentioned algorithm, each passage in the new list is sufficiently dissimilar to others, thus favoring diversity rather than redundancy in the new ranked list.",
                "The anti-redundancy threshold t is tuned on a training set. 4.",
                "EVALUATION METHODOLOGY The approach we proposed above for information distillation raises important issues regarding evaluation methodology.",
                "Firstly, since our framework allows the output to be passages at different levels of granularity (e.g. k-sentence windows where k may vary) instead of a fixed length, it is not possible to have pre-annotated relevance judgments at all such granularity levels.",
                "Secondly, since we wish to measure the utility of the system output as a combination of both relevance and novelty, traditional relevance-only based measures must be replaced by measures that penalize the repetition of the same information in the system output across time.",
                "Thirdly, since the output of the system is ranked lists, we must reward those systems that present useful information (both relevant and previously unseen) using shorter ranked lists, and penalize those that present the same information using longer ranked lists.",
                "None of the existing measures in ad-hoc retrieval, adaptive filtering, novelty detection or other related areas (text summarization and question answering) have desirable properties in all the three aspects.",
                "Therefore, we must develop a new evaluation methodology. 4.1 Answer Keys To enable the evaluation of a system whose output consists of passages of arbitrary length, we borrow the concept of answer keys from the Question Answering (QA) community, where systems are allowed to return arbitrary spans of text as answers.",
                "Answer keys define what should be present in a system response to receive credit, and are comprised of a collection of information nuggets, i.e. factoid units about which human assessors can make binary decisions of whether or not a system response contains them.",
                "Defining answer keys and making the associated binary decisions are conceptual tasks that require semantic mapping [19], since system-returned passages can contain the same information expressed in many different ways.",
                "Hence, QA evaluations have relied on human assessors for the mapping between various expressions, making the process costly, time consuming, and not scalable to large query and document collections, and extensive system evaluations with various parameter settings. 4.1.1 Automating Evaluation based on Answer Keys Automatic evaluation methods would allow for faster system building and tuning, as well as provide an objective and affordable way of comparing various systems.",
                "Recently, such methods have been proposed, more or less, based on the idea of n-gram co-occurrences.",
                "Pourpre [10] assigns a fractional recall score to a system response based on its unigram overlap with a given nuggets description.",
                "For example, a system response A B C has recall 3/4 with respect to a nugget with description A B C D. However, such an approach is unfair to systems that present the same information but using words other than A, B, C, and D. Another open issue is how to weight individual words in measuring the closeness of a match.",
                "For example, consider the question How many prisoners escaped?.",
                "In the nugget Seven prisoners escaped from a Texas prison, there is no indication that seven is the keyword, and that it must be matched to get any relevance credit.",
                "Using IDF values does not help, since seven will generally not have a higher IDF than words like texas and prison.",
                "Also, redefining the nugget as just seven does not solve the problem since now it might spuriously match any mention of seven out of context.",
                "Nuggeteer [13] works on similar principles but makes binary decisions about whether a nugget is present in a given system response by tuning a threshold.",
                "However, it is also plagued by spurious relevance since not all words contained in the nugget description (or known correct responses) are central to the nugget. 4.1.2 Nugget-Matching Rules We propose a reliable automatic method for determining whether a snippet of text contains a given nugget, based on nugget-matching rules, which are generated using a semiautomatic procedure explained below.",
                "These rules are essentially Boolean queries that will only match against snippets that contain the nugget.",
                "For instance, a candidate rule for matching answers to How many prisoners escaped? is (Texas AND seven AND escape AND (convicts OR prisoners)), possibly with other synonyms and variants in the rule.",
                "For a corpus of news articles, which usually follow a typical formal prose, it is fairly easy to write such simple rules to match expected answers using a bootstrap approach, as described below.",
                "We propose a two-stage approach, inspired by Autoslog [14], that combines the strength of humans in identifying semantically equivalent expressions and the strength of the system in gathering statistical evidence from a humanannotated corpus of documents.",
                "In the first stage, human subjects annotated (using a highlighting tool) portions of ontopic documents that contained answers to each nugget 2 .",
                "In the second stage, subjects used our rule generation tool to create rules that would match the annotations for each nugget.",
                "The tool allows users to enter a Boolean rule as a disjunction of conjunctions (e.g. ((a AND b) OR (a AND c AND d) OR (e))).",
                "Given a candidate rule, our tool uses it as a Boolean query over the entire set of on-topic documents and calculates its recall and precision with respect to the annotations that it is expected to match.",
                "Hence, the subjects can start with a simple rule and iteratively refine it until they are satisfied with its recall and precision.",
                "We observed that it was very easy for humans to improve the precision of a rule by tweaking its existing conjunctions (adding more ANDs), and improving the recall by adding more conjunctions to the disjunction (adding more ORs).",
                "As an example, lets try to create a rule for the nugget which says that seven prisoners escaped from the Texas prison.",
                "We start with a simple rule - (seven).",
                "When we input this into the rule generation tool, we realize that this rule matches many spurious occurrences of seven (e.g. ...seven states...) and thus gets a low precision score.",
                "We can further qualify our rule - Texas AND seven AND convicts.",
                "Next, by looking at the missed annotations, we realize that some news articles mentioned ...seven prisoners escaped.... We then replace convicts with the disjunction (convicts OR prisoners).",
                "We continue tweaking the rule in this manner until we achieve a sufficiently high recall and precision - i.e. the (small number of) misses and false alarms can be safely ignored.",
                "Thus we can create nugget-matching rules that succinctly capture various ways of expressing a nugget, while avoiding matching incorrect (or out of context) responses.",
                "Human involvement in the rule creation process ensures high quality generic rules which can then be used to evaluate arbitrary system responses reliably. 4.2 Evaluating the Utility of a Sequence of Ranked Lists The utility of a retrieval system can be defined as the difference between how much the user gained in terms of useful information, and how much the user lost in terms of time and energy.",
                "We calculate this utility from the utilities of individual passages as follows.",
                "After reading each passage returned by the system, the user derives some gain depending on the presence of relevant and novel information, and incurs a loss in terms of the time and energy spent in going through the passage.",
                "However, the likelihood that the user would actually read a passage depends on its position in the ranked list.",
                "Hence, for a query q, the expected utility 2 LDC [18] already provides relevance judgments for 100 topics on the TDT4 corpus.",
                "We further ensured that these judgments are exhaustive on the entire corpus using pooling. of a passage pi at rank i can be defined as U(pi, q) = P(i) ∗ (Gain(pi, q) − Loss(pi, q)) (4) where P(i) is the probability that the user would go through a passage at rank i.",
                "The expected utility for an entire ranked list of length n can be calculated simply by adding the expected utility of each passage: U(q) = nX i=1 P(i) ∗ (Gain(pi, q) − Loss(pi, q)) (5) Note that if we ignore the loss term and define P(i) as P(i) ∝ 1/ logb(b + i − 1) (6) then we get the recently popularized metric called Discounted Cumulated Gain (DCG) [9], where Gain(pi, q) is defined as the graded relevance of passage pi.",
                "However, without the loss term, DCG is a purely recall-oriented metric and not suitable for an adaptive filtering setting, where the systems utility depends in part on its ability to limit the number of items shown to the user.",
                "Although P(i) could be defined based on empirical studies of user behavior, for simplicity, we use P(i) exactly as defined in equation 6.",
                "The gain G(pi, q) of passage pi with respect to the query q is a function of - 1) the number of relevant nuggets present in pi, and 2) the novelty of each of these nuggets.",
                "We combine these two factors as follows.",
                "For each nugget Nj, we assign an initial weight wj, and also keep a count nj of the number of times this nugget has been seen by the user in the past.",
                "The gain derived from each subsequent occurrence of the same nugget is assumed to reduce by a dampening factor γ.",
                "Thus, G(pi, q) is defined as G(pi, q) = X Nj ∈C(pi,q) wj ∗ γnj (7) where C(pi, q) is the set of all nuggets that appear in passage pi and also belong to the answer key of query q.",
                "The initial weights wj are all set of be 1.0 in our experiments, but can also be set based on a pyramid approach [11].",
                "The choice of dampening factor γ determines the users tolerance for redundancy.",
                "When γ = 0, a nugget will only receive credit for its first occurrence i.e. when nj is zero3 .",
                "For 0 < γ < 1, a nugget receives smaller credit for each successive occurrence.",
                "When γ = 1, no dampening occurs and repeated occurrences of a nugget receive the same credit.",
                "Note that the nugget occurrence counts are preserved between evaluation of successive ranked lists returned by the system, since the users are expected to remember what the system showed them in the past.",
                "We define the loss L(pi, q) as a constant cost c (we use 0.1) incurred when reading a system-returned passage.",
                "Thus, our metric can be re-written as U(q) = nX i=1 Gain(pi, q) logb(b + i − 1) − L(n) (8) where L(n) is the loss associated with a ranked list of length n: L(n) = c · nX i=1 1 logb(b + i − 1) (9) 3 Note that 00 = 1 Due to the similarity with Discounted Cumulated Gain (DCG), we call our metric Discounted Cumulated Utility (DCU).",
                "The DCU score obtained by the system is converted to a Normalized DCU (NDCU) score by dividing it by the DCU score of the ideal ranked list, which is created by ordering passages by their decreasing utility scores U(pi, q) and stopping when U(pi, q) ≤ 0 i.e. when the gain is less than or equal to the cost of reading the passage. 5.",
                "DATA TDT4 was the benchmark corpus used in TDT2002 and TDT2003 evaluations.",
                "The corpus consists of over 90, 000 news articles from multiple sources (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) published between October 2000 and January 2001, in the languages of Arabic, English, and Mandarin.",
                "Speech-recognized and machine-translated versions of the non-English articles were provided as well.",
                "LDC [18] has annotated the corpus with 100 topics, that correspond to various news events in this time period.",
                "Out of these, we selected a subset of 12 actionable events, and defined corresponding tasks for them4 .",
                "For each task, we manually defined a profile consisting of an initial set of (5 to 10) queries, a free-text description of the user history, i.e., what the user already knows about the event, and a list of known on-topic and off-topic documents (if available) as training examples.",
                "For each query, we generated answer keys and corresponding nugget matching rules using the procedure described in section 4.1.2, and produced a total of 120 queries, with an average of 7 nuggets per query. 6.",
                "EXPERIMENTS AND RESULTS 6.1 Baselines We used Indri [17], a popular language-model based retrieval engine, as a baseline for comparison with CAF´E.",
                "Indri supports standard search engine functionality, including pseudo-relevance feedback (PRF) [3, 6], and is representative of a typical query-based retrieval system.",
                "Indri does not support any kind of novelty detection.",
                "We compare Indri with PRF turned on and off, against CAF´E with user feedback, novelty detection and antiredundant ranking turned on and off. 6.2 Experimental Setup We divided the TDT4 corpus spanning 4 months into 10 chunks, each defined as a period of 12 consecutive days.",
                "At any given point of time in the distillation process, each system accessed the past data up to the current point, and returned a ranked list of up 50 passages per query.",
                "The 12 tasks defined on the corpus were divided into a training and test set with 6 tasks each.",
                "Each system was allowed to use the training set to tune its parameters for optimizing NDCU (equation 8), including the relevance threshold for both Indri and CAF´E, and the novelty and antiredundancy thresholds for CAF´E.",
                "The NDCU for each system run is calculated automatically.",
                "User feedback was also simulated - relevance judgments for each system-returned passage (as determined by the nugget matching rules described in section 4.1.2) were 4 URL: http://nyc.lti.cs.cmu.edu/downloads Figure 1: Performance of Indri across chunks Figure 2: Performance of CAF´E across chunks used as user feedback in the adaptation of query profiles and user histories. 6.3 Results In Table 1, we show the NDCU scores of the two systems under various settings.",
                "These scores are averaged over the six tasks in the test set, and are calculated with two dampening factors (see section 4.2): γ = 0 and 0.1, to simulate no tolerance and small tolerance for redundancy, respectively.",
                "Using γ = 0 creates a much more strict metric since it does not give any credit to a passage that contains relevant but redundant information.",
                "Hence, the improvement obtained from enabling user feedback is smaller with γ = 0 than the improvement obtained from feedback with γ = 0.1.",
                "This reveals a shortcoming of contemporary retrieval systemswhen the user gives positive feedback on a passage, the systems gives higher weights to the terms present in that passage and tends to retrieve other passages containing the same terms - and thus - usually the same information.",
                "However, the user does not benefit from seeing such redundant passages, and is usually interested in other passages containing related information.",
                "It is informative to evaluate retrieval systems using our utility measure (with γ = 0) which accounts for novelty and thus gives a more realistic picture of how well a system can generalize from user feedback, rather than using traditional IR measures like recall and precision which give an incomplete picture of improvement obtained from user feedback.",
                "Sometimes, however, users might indeed be interested in seeing the same information from multiple sources, as an Table 1: NDCU Scores of Indri and CAF´E for two dampening factors (γ), and various settings (F: Feedback, N: Novelty Detection, A: Anti-Redundant Ranking) Indri CAF´E γ Base +PRF Base +F +F+N +F+A +F+N+A 0 0.19 0.19 0.22 0.23 0.24 0.24 0.24 0.1 0.28 0.29 0.24 0.35 0.35 0.36 0.36 indicator of its importance or reliability.",
                "In such a case, we can simply choose a higher value for γ which corresponds to a higher tolerance for redundancy, and hence let the system tune its parameters accordingly.",
                "Since documents were processed chunk by chunk, it would be interesting to see how the performance of systems improves over time.",
                "Figures 1 and 2 show the performance trends for both the systems across chunks.",
                "While the performance with and without feedback on the first few chunks is expected to be close, for subsequent chunks, the performance curve with feedback enabled rises above the one with the no-feedback setting.",
                "The performance trends are not consistent across all chunks because on-topic documents are not uniformly distributed over all the chunks, making some queries easier than others in certain chunks.",
                "Moreover, since Indri uses pseudo-relevance feedback while CAF´E uses feedback based on actual relevance judgments, the improvement in case of Indri is less dramatic than that of CAF´E. 7.",
                "CONCLUDING REMARKS This paper presents the first investigation on utility-based information distillation with a system that learns the longlasting information needs from fine-grained user feedback over a sequence of ranked passages.",
                "Our system, called CAF´E, combines adaptive filtering, novelty detection and antiredundant passage ranking in a unified framework for utility optimization.",
                "We developed a new scheme for automated evaluation and feedback based on a semi-automatic procedure for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also proposed an extension of the NDCG metric for assessing the utility of ranked passages as a weighted combination of relevance and novelty.",
                "Our experiments on the newly annotated TDT4 benchmark corpus show encouraging utility enhancement over Indri, and also over our own system with incremental learning and novelty detection turned off. 8.",
                "ACKNOWLEDGMENTS We would like to thank Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng, and the Qualitative Data Analysis Program at the University of Pittsburgh lead by Dr. Stuart Shulman for their help with collecting and processing the extended TDT4 annotations used in our experiments.",
                "This work is supported in parts by the National Science Foundation (NSF) under grant IIS0434035, and the Defense Advanced Research Project Agency (DARPA) under contracts NBCHD030010 and W0550432.",
                "Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors. 9.",
                "ADDITIONAL AUTHORS Jian Zhang (jianzhan@stat.purdue.edu)∗ , Jaime Carbonell (jgc@cs.cmu.edu)† , Peter Brusilovsky (peterb+@pitt.edu)‡ , Daqing He(dah44@pitt.edu)‡ 10.",
                "REFERENCES [1] J. Allan.",
                "Incremental Relevance Feedback for Information Filtering.",
                "Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, pages 270-278, 1996. [2] J. Allan, C. Wade, and A. Bolivar.",
                "Retrieval and Novelty Detection at the Sentence Level.",
                "Proceedings of the ACM SIGIR conference on research and development in information retrieval, 2003. [3] C. Buckley, G. Salton, and J. Allan.",
                "Automatic Retrieval with Locality Information using SMART.",
                "NIST special publication, (500207):59-72, 1993. [4] J. Callan.",
                "Learning While Filtering Documents.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 224-231, 1998. [5] J. Carbonell and J. Goldstein.",
                "The use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 335-336, 1998. [6] E. Efthimiadis.",
                "Query Expansion.",
                "Annual Review of Information Science and Technology (ARIST), 31:p121-87, 1996. [7] J. Fiscus and G. Duddington.",
                "Topic Detection and Tracking Overview.",
                "Topic Detection and Tracking: Event-based Information Organization, pages 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos.",
                "A Statistical Model for Multilingual Entity Detection and Tracking.",
                "NAACL/HLT, 2004. [9] K. J¨arvelin and J. Kek¨al¨ainen.",
                "Cumulated Gain-based Evaluation of IR Techniques.",
                "ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin and D. Demner-Fushman.",
                "Automatically Evaluating Answers to Definition Questions.",
                "Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), 2005. ∗ Statistics Dept., Purdue University, West Lafayette, USA † Language Technologies Inst., Carnegie Mellon University, Pittsburgh, USA ‡ School of Information Sciences, Univ. of Pittsburgh, Pittsburgh, USA [11] J. Lin and D. Demner-Fushman.",
                "Will Pyramids Built of nUggets Topple Over.",
                "Proceedings of HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos.",
                "A Mention-synchronous Coreference Resolution Algorithm based on the Bell Tree.",
                "Proc. of ACL, 4:136-143, 2004. [13] G. Marton.",
                "Nuggeteer: Automatic Nugget-Based Evaluation Using Descriptions and Judgments.",
                "HLT/NAACL, 2006. [14] E. Riloff.",
                "Automatically Constructing a Dictionary for Information Extraction Tasks.",
                "Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 811-816, 1993. [15] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9: Filtering track.",
                "The Ninth Text REtrieval Conference (TREC-9), pages 361-368. [16] R. Schapire, Y.",
                "Singer, and A. Singhal.",
                "Boosting and Rocchio Applied to Text Filtering.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle, and W. Croft.",
                "Indri: A Language Model-based Search Engine for Complex Queries.",
                "Proceedings of the International Conference on Intelligence Analysis, 2004. [18] The Linguistic Data Consortium. http://www.ldc.upenn.edu/. [19] E. Voorhees.",
                "Overview of the TREC 2003 Question Answering Track.",
                "Proceedings of the Twelfth Text REtrieval Conference (TREC 2003), 2003. [20] Y. Yang and B. Kisiel.",
                "Margin-based Local Regression for Adaptive Filtering.",
                "Proceedings of the twelfth international conference on Information and knowledge management, pages 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of Adaptive Filtering Methods in a Cross-benchmark Evaluation.",
                "Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 98-105, 2005. [22] C. Zhai, W. Cohen, and J. Lafferty.",
                "Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 10-17, 2003. [23] J. Zhang and Y. Yang.",
                "Robustness of Regularized Linear Classification Methods in Text Categorization.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 190-197, 2003. [24] Y. Zhang.",
                "Using Bayesian Priors to Combine Classifiers for Adaptive Filtering.",
                "Proceedings of the 27th annual international conference on Research and development in information retrieval, pages 345-352, 2004. [25] Y. Zhang, J. Callan, and T. Minka.",
                "Novelty and Redundancy Detection in Adaptive Filtering.",
                "Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "evaluation methodology": {
            "translated_key": "metodología de evaluación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Utility-based Information Distillation Over Temporally Sequenced Documents Yiming Yang Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA yiming@cs.cmu.edu Abhimanyu Lad Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA alad@cs.cmu.edu Ni Lao Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA nlao@cs.cmu.edu Abhay Harpale Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA aharpale@cs.cmu.edu Bryan Kisiel Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA bkisiel@cs.cmu.edu Monica Rogati Language Technologies Inst.",
                "Carnegie Mellon University Pittsburgh, USA mrogati@cs.cmu.edu ABSTRACT This paper examines a new approach to information distillation over temporally ordered documents, and proposes a novel evaluation scheme for such a framework.",
                "It combines the strengths of and extends beyond conventional adaptive filtering, novelty detection and non-redundant passage ranking with respect to long-lasting information needs (tasks with multiple queries).",
                "Our approach supports fine-grained user feedback via highlighting of arbitrary spans of text, and leverages such information for utility optimization in adaptive settings.",
                "For our experiments, we defined hypothetical tasks based on news events in the TDT4 corpus, with multiple queries per task.",
                "Answer keys (nuggets) were generated for each query and a semiautomatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also propose an extension of the NDCG metric for assessing the utility of ranked passages as a combination of relevance and novelty.",
                "Our results show encouraging utility enhancements using the new approach, compared to the baseline systems without incremental learning or the novelty detection components.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 General Terms Design, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Tracking new and relevant information from temporal data streams for users with long-lasting needs has been a challenging research topic in information retrieval.",
                "Adaptive filtering (AF) is one such task of online prediction of the relevance of each new document with respect to pre-defined topics.",
                "Based on the initial query and a few positive examples (if available), an AF system maintains a profile for each such topic of interest, and constantly updates it based on feedback from the user.",
                "The incremental learning nature of AF systems makes them more powerful than standard search engines that support ad-hoc retrieval (e.g.",
                "Google and Yahoo) in terms of finding relevant information with respect to long-lasting topics of interest, and more attractive for users who are willing to provide feedback to adapt the system towards their specific information needs, without having to modify their queries manually.",
                "A variety of supervised learning algorithms (Rocchio-style classifiers, Exponential-Gaussian models, local regression and logistic regression approaches) have been studied for adaptive settings, examined with explicit and implicit relevance feedback, and evaluated with respect to utility optimization on large benchmark data collections in TREC (Text Retrieval Conferences) and TDT (Topic Detection and Tracking) forums [1, 4, 7, 15, 16, 20, 24, 23].",
                "Regularized logistic regression [21] has been found representative for the state-of-the-art approaches, and highly efficient for frequent model adaptations over large document collections such as the TREC-10 corpus (over 800,000 documents and 84 topics).",
                "Despite substantial achievements in recent adaptive filtering research, significant problems remain unsolved regarding how to leverage user feedback effectively and efficiently.",
                "Specifically, the following issues may seriously limit the true utility of AF systems in real-world applications: 1.",
                "User has a rather passive role in the conventional adaptive filtering setup - he or she reacts to the system only when the system makes a yes decision on a document, by confirming or rejecting that decision.",
                "A more active alternative would be to allow the user to issue multiple queries for a topic, review a ranked list of candidate documents (or passages) per query, and provide feedback on the ranked list, thus refining their information need and requesting updated ranked lists.",
                "The latter form of user interaction has been highly effective in standard retrieval for ad-hoc queries.",
                "How to deploy such a strategy for long-lasting information needs in AF settings is an open question for research. 2.",
                "The unit for receiving a relevance judgment (yes or no) is restricted to the document level in conventional AF.",
                "However, a real user may be willing to provide more informative, fine-grained feedback via highlighting some pieces of text in a retrieved document as relevant, instead of labeling the entire document as relevant.",
                "Effectively leveraging such fine-grained feedback could substantially enhance the quality of an AF system.",
                "For this, we need to enable supervised learning from labeled pieces of text of arbitrary span instead of just allowing labeled documents. 3.",
                "System-selected documents are often highly redundant.",
                "A major news event, for example, would be reported by multiple sources repeatedly for a while, making most of the information content in those articles redundant with each other.",
                "A conventional AF system would select all these redundant news stories for user feedback, wasting the users time while offering little gain.",
                "Clearly, techniques for novelty detection can help in principle [25, 2, 22] for improving the utility of the AF systems.",
                "However, the effectiveness of such techniques at passage level to detect novelty with respect to users (fine-grained) feedback and to detect redundancy in ranked lists remains to be evaluated using a measure of utility that mimics the needs of a real user.",
                "To address the above limitations of current AF systems, we propose and examine a new approach in this paper, combining the strengths of conventional AF (incremental learning of topic models), multi-pass passage retrieval for long-lasting queries conditioned on topic, and novelty detection for removal of redundancy from user interactions with the system.",
                "We call the new process utility-based information distillation.",
                "Note that conventional benchmark corpora for AF evaluations, which have relevance judgments at the document level and do not define tasks with multiple queries, are insufficient for evaluating the new approach.",
                "Therefore, we extended a benchmark corpus - the TDT4 collection of news stories and TV broadcasts - with task definitions, multiple queries per task, and answer keys per query.",
                "We have conducted our experiments on this extended TDT4 corpus and have made the additionally generated data publicly available for future comparative evaluations 1 .",
                "To automatically evaluate the system-returned arbitrary spans of text using our answer keys, we further developed an evaluation scheme with semi-automatic procedure for 1 URL: http://nyc.lti.cs.cmu.edu/downloads acquiring rules that can match nuggets against system responses.",
                "Moreover, we propose an extension of NDCG (Normalized Discounted Cumulated Gain) [9] for assessing the utility of ranked passages as a function of both relevance and novelty.",
                "The rest of this paper is organized as follows.",
                "Section 2 outlines the information distillation process with a concrete example.",
                "Section 3 describes the technical cores of our system called CAF´E - CMU Adaptive Filtering Engine.",
                "Section 4 discusses issues with respect to <br>evaluation methodology</br> and proposes a new scheme.",
                "Section 5 describes the extended TDT4 corpus.",
                "Section 6 presents our experiments and results.",
                "Section 7 concludes the study and gives future perspectives. 2.",
                "A SAMPLE TASK Consider a news event - the escape of seven convicts from a Texas prison in December 2000 and their capture a month later.",
                "Assuming a user were interested in this event since its early stage, the information need could be: Find information about the escape of convicts from Texas prison, and information related to their recapture.",
                "The associated lower-level questions could be: 1.",
                "How many prisoners escaped? 2.",
                "Where and when were they sighted? 3. Who are their known contacts inside and outside the prison? 4.",
                "How are they armed? 5.",
                "Do they have any vehicles? 6.",
                "What steps have been taken so far?",
                "We call such an information need a task, and the associated questions as the queries in this task.",
                "A distillation system is supposed to monitor the incoming documents, process them chunk by chunk in a temporal order, select potentially relevant and novel passages from each chunk with respect to each query, and present a ranked list of passages to the user.",
                "Passage ranking here is based on how relevant a passage is with respect to the current query, how novel it is with respect to the current user history (of his or her interactions with the system), and how redundant it is compared to other passages with a higher rank in the list.",
                "When presented with a list of passages, the user may provide feedback by highlighting arbitrary spans of text that he or she found relevant.",
                "These spans of text are taken as positive examples in the adaptation of the query profile, and also added to the users history.",
                "Passages not marked by the user are taken as negative examples.",
                "As soon as the query profile is updated, the system re-issues a search and returns another ranked list of passages where the previously seen passages are either removed or ranked low, based on user preference.",
                "For example, if the user highlights ...officials have posted a $100,000 reward for their capture... as relevant answer to the query What steps have been taken so far?, then the highlighted piece is used as an additional positive training example in the adaptation of the query profile.",
                "This piece of feedback is also added to the user history as a seen example, so that in future, the system will not place another passage mentioning $100,000 reward at the top of the ranked list.",
                "However, an article mentioning ...officials have doubled the reward money to $200,000... might be ranked high since it is both relevant to the (updated) query profile and novel with respect to the (updated) user history.",
                "The user may modify the original queries or add a new query during the process; the query profiles will be changed accordingly.",
                "Clearly, novelty detection is very important for the utility of such a system because of the iterative search.",
                "Without novelty detection, the old relevant passages would be shown to the user repeatedly in each ranked list.",
                "Through the above example, we can see the main properties of our new framework for utility-based information distillation over temporally ordered documents.",
                "Our framework combines and extends the power of adaptive filtering (AF), ad-hoc retrieval (IR) and novelty detection (ND).",
                "Compared to standard IR, our approach has the power of incrementally learning long-term information needs and modeling a sequence of queries within a task.",
                "Compared to conventional AF, it enables a more active role of the user in refining his or her information needs and requesting new results by allowing relevance and novelty feedback via highlighting of arbitrary spans of text in passages returned by the system.",
                "Compared to past work, this is the first evaluation of novelty detection integrated with adaptive filtering for sequenced queries that allows flexible user feedback over ranked passages.",
                "The combination of AF, IR and ND with the new extensions raises an important research question regarding <br>evaluation methodology</br>: how can we measure the utility of such an information distillation system?",
                "Existing metrics in standard IR, AF and ND are insufficient, and new solutions must be explored, as we will discuss in Section 4, after describing the technical cores of our system in the next section. 3.",
                "TECHNICAL CORES The core components of CAF´E are - 1) AF for incremental learning of query profiles, 2) IR for estimating relevance of passages with respect to query profiles, 3) ND for assessing novelty of passages with respect to users history, and 4) anti-redundancy component to remove redundancy from ranked lists. 3.1 Adaptive Filtering Component We use a state-of-the-art algorithm in the field - the regularized logistic regression method which had the best results on several benchmark evaluation corpora for AF [21].",
                "Logistic regression (LR) is a supervised learning algorithm for statistical classification.",
                "Based on a training set of labeled instances, it learns a class model which can then by used to predict the labels of unseen instances.",
                "Its performance as well as efficiency in terms of training time makes it a good candidate when frequent updates of the class model are required, as is the case in adaptive filtering, where the system must learn from each new feedback provided by the user. (See [21] and [23] for computational complexity and implementation issues).",
                "In adaptive filtering, each query is considered as a class and the probability of a passage belonging to this class corresponds to the degree of relevance of the passage with respect to the query.",
                "For training the model, we use the query itself as the initial positive training example of the class, and the user-highlighted pieces of text (marked as Relevant or Not-relevant) during feedback as additional training examples.",
                "To address the cold start issue in the early stage before any user feedback is obtained, the system uses a small sample from a retrospective corpus as the initial negative examples in the training set.",
                "The details of using logistic regression for adaptive filtering (assigning different weights to positive and negative training instances, and regularizing the objective function to prevent over-fitting on training data) are presented in [21].",
                "The class model w∗ learned by Logistic Regression, or the query profile, is a vector whose dimensions are individual terms and whose elements are the regression coefficients, indicating how influential each term is in the query profile.",
                "The query profile is updated whenever a new piece of user feedback is received.",
                "A temporally decaying weight can be applied to each training example, as an option, to emphasize the most recent user feedback. 3.2 Passage Retrieval Component We use standard IR techniques in this part of our system.",
                "Incoming documents are processed in chunks, where each chunk can be defined as a fixed span of time or as a fixed number of documents, as preferred by the user.",
                "For each incoming document, corpus statistics like the IDF (Inverted Document Frequency) of each term are updated.",
                "We use a state-of-the-art named entity identifier and tracker [8, 12] to identify person and location names, and merge them with co-referent named entities seen in the past.",
                "Then the documents are segmented into passages, which can be a whole document, a paragraph, a sentence, or any other continuous span of text, as preferred.",
                "Each passage is represented using a vector of TF-IDF (Term FrequencyInverse Document Frequency) weights, where term can be a word or a named entity.",
                "Given a query profile, i.e. the logistic regression solution w∗ as described in Section 3.1, the system computes the posterior probability of relevance for each passage x as fRL(x) ≡ P(y = 1|x, w∗ ) = 1 (1 + e−w∗·x) (1) Passages are ordered by their relevance scores, and the ones with scores above a threshold (tuned on a training set) comprise the relevance list that is passed on to the novelty detection step. 3.3 Novelty Detection Component CAF´E maintains a user history H(t), which contains all the spans of text hi that the user highlighted (as feedback) during his or her past interactions with the system, up to the current time t. Denoting the history as H(t) = n h1, h2, ..., ht o , (2) the novelty score of a new candidate passage x is computed as: fND(x) = 1 − max i∈1..t {cos(x, hi)} (3) where both candidate passage x and highlighted spans of text hi are represented as TF-IDF vectors.",
                "The novelty score of each passage is compared to a prespecified threshold (also tuned on a training set), and any passage with a score below this threshold is removed from the relevance list. 3.4 Anti-redundant Ranking Component Although the novelty detection component ensures that only novel (previously unseen) information remains in the relevance list, this list might still contain the same novel information at multiple positions in the ranked list.",
                "Suppose, for example, that the user has already read about a $100,000 reward for information about the escaped convicts.",
                "A new piece of news that the award has been increased to $200,000 is novel since the user hasnt read about it yet.",
                "However, multiple news sources would report this news and we might end up showing (redundant) articles from all these sources in a ranked list.",
                "Hence, a ranked list should also be made non-redundant with respect to its own contents.",
                "We use a simplified version of the Maximal Marginal Relevance method [5], originally developed for combining relevance and novelty in text retrieval and summarization.",
                "Our procedure starts with the current list of passages sorted by relevance (section 3.2), filtered by Novelty Detection component (section 3.3), and generates a new non-redundant list as follows: 1.",
                "Take the top passage in the current list as the top one in the new list. 2.",
                "Add the next passage x in the current list to the new list only if fAR(x) > t where fAR(x) = 1 − max pi∈Lnew {cos(x, pi)} and Lnew is the set of passages already selected in the new list. 3.",
                "Repeat step 2 until all the passages in the current list have been examined.",
                "After applying the above-mentioned algorithm, each passage in the new list is sufficiently dissimilar to others, thus favoring diversity rather than redundancy in the new ranked list.",
                "The anti-redundancy threshold t is tuned on a training set. 4.",
                "<br>evaluation methodology</br> The approach we proposed above for information distillation raises important issues regarding <br>evaluation methodology</br>.",
                "Firstly, since our framework allows the output to be passages at different levels of granularity (e.g. k-sentence windows where k may vary) instead of a fixed length, it is not possible to have pre-annotated relevance judgments at all such granularity levels.",
                "Secondly, since we wish to measure the utility of the system output as a combination of both relevance and novelty, traditional relevance-only based measures must be replaced by measures that penalize the repetition of the same information in the system output across time.",
                "Thirdly, since the output of the system is ranked lists, we must reward those systems that present useful information (both relevant and previously unseen) using shorter ranked lists, and penalize those that present the same information using longer ranked lists.",
                "None of the existing measures in ad-hoc retrieval, adaptive filtering, novelty detection or other related areas (text summarization and question answering) have desirable properties in all the three aspects.",
                "Therefore, we must develop a new <br>evaluation methodology</br>. 4.1 Answer Keys To enable the evaluation of a system whose output consists of passages of arbitrary length, we borrow the concept of answer keys from the Question Answering (QA) community, where systems are allowed to return arbitrary spans of text as answers.",
                "Answer keys define what should be present in a system response to receive credit, and are comprised of a collection of information nuggets, i.e. factoid units about which human assessors can make binary decisions of whether or not a system response contains them.",
                "Defining answer keys and making the associated binary decisions are conceptual tasks that require semantic mapping [19], since system-returned passages can contain the same information expressed in many different ways.",
                "Hence, QA evaluations have relied on human assessors for the mapping between various expressions, making the process costly, time consuming, and not scalable to large query and document collections, and extensive system evaluations with various parameter settings. 4.1.1 Automating Evaluation based on Answer Keys Automatic evaluation methods would allow for faster system building and tuning, as well as provide an objective and affordable way of comparing various systems.",
                "Recently, such methods have been proposed, more or less, based on the idea of n-gram co-occurrences.",
                "Pourpre [10] assigns a fractional recall score to a system response based on its unigram overlap with a given nuggets description.",
                "For example, a system response A B C has recall 3/4 with respect to a nugget with description A B C D. However, such an approach is unfair to systems that present the same information but using words other than A, B, C, and D. Another open issue is how to weight individual words in measuring the closeness of a match.",
                "For example, consider the question How many prisoners escaped?.",
                "In the nugget Seven prisoners escaped from a Texas prison, there is no indication that seven is the keyword, and that it must be matched to get any relevance credit.",
                "Using IDF values does not help, since seven will generally not have a higher IDF than words like texas and prison.",
                "Also, redefining the nugget as just seven does not solve the problem since now it might spuriously match any mention of seven out of context.",
                "Nuggeteer [13] works on similar principles but makes binary decisions about whether a nugget is present in a given system response by tuning a threshold.",
                "However, it is also plagued by spurious relevance since not all words contained in the nugget description (or known correct responses) are central to the nugget. 4.1.2 Nugget-Matching Rules We propose a reliable automatic method for determining whether a snippet of text contains a given nugget, based on nugget-matching rules, which are generated using a semiautomatic procedure explained below.",
                "These rules are essentially Boolean queries that will only match against snippets that contain the nugget.",
                "For instance, a candidate rule for matching answers to How many prisoners escaped? is (Texas AND seven AND escape AND (convicts OR prisoners)), possibly with other synonyms and variants in the rule.",
                "For a corpus of news articles, which usually follow a typical formal prose, it is fairly easy to write such simple rules to match expected answers using a bootstrap approach, as described below.",
                "We propose a two-stage approach, inspired by Autoslog [14], that combines the strength of humans in identifying semantically equivalent expressions and the strength of the system in gathering statistical evidence from a humanannotated corpus of documents.",
                "In the first stage, human subjects annotated (using a highlighting tool) portions of ontopic documents that contained answers to each nugget 2 .",
                "In the second stage, subjects used our rule generation tool to create rules that would match the annotations for each nugget.",
                "The tool allows users to enter a Boolean rule as a disjunction of conjunctions (e.g. ((a AND b) OR (a AND c AND d) OR (e))).",
                "Given a candidate rule, our tool uses it as a Boolean query over the entire set of on-topic documents and calculates its recall and precision with respect to the annotations that it is expected to match.",
                "Hence, the subjects can start with a simple rule and iteratively refine it until they are satisfied with its recall and precision.",
                "We observed that it was very easy for humans to improve the precision of a rule by tweaking its existing conjunctions (adding more ANDs), and improving the recall by adding more conjunctions to the disjunction (adding more ORs).",
                "As an example, lets try to create a rule for the nugget which says that seven prisoners escaped from the Texas prison.",
                "We start with a simple rule - (seven).",
                "When we input this into the rule generation tool, we realize that this rule matches many spurious occurrences of seven (e.g. ...seven states...) and thus gets a low precision score.",
                "We can further qualify our rule - Texas AND seven AND convicts.",
                "Next, by looking at the missed annotations, we realize that some news articles mentioned ...seven prisoners escaped.... We then replace convicts with the disjunction (convicts OR prisoners).",
                "We continue tweaking the rule in this manner until we achieve a sufficiently high recall and precision - i.e. the (small number of) misses and false alarms can be safely ignored.",
                "Thus we can create nugget-matching rules that succinctly capture various ways of expressing a nugget, while avoiding matching incorrect (or out of context) responses.",
                "Human involvement in the rule creation process ensures high quality generic rules which can then be used to evaluate arbitrary system responses reliably. 4.2 Evaluating the Utility of a Sequence of Ranked Lists The utility of a retrieval system can be defined as the difference between how much the user gained in terms of useful information, and how much the user lost in terms of time and energy.",
                "We calculate this utility from the utilities of individual passages as follows.",
                "After reading each passage returned by the system, the user derives some gain depending on the presence of relevant and novel information, and incurs a loss in terms of the time and energy spent in going through the passage.",
                "However, the likelihood that the user would actually read a passage depends on its position in the ranked list.",
                "Hence, for a query q, the expected utility 2 LDC [18] already provides relevance judgments for 100 topics on the TDT4 corpus.",
                "We further ensured that these judgments are exhaustive on the entire corpus using pooling. of a passage pi at rank i can be defined as U(pi, q) = P(i) ∗ (Gain(pi, q) − Loss(pi, q)) (4) where P(i) is the probability that the user would go through a passage at rank i.",
                "The expected utility for an entire ranked list of length n can be calculated simply by adding the expected utility of each passage: U(q) = nX i=1 P(i) ∗ (Gain(pi, q) − Loss(pi, q)) (5) Note that if we ignore the loss term and define P(i) as P(i) ∝ 1/ logb(b + i − 1) (6) then we get the recently popularized metric called Discounted Cumulated Gain (DCG) [9], where Gain(pi, q) is defined as the graded relevance of passage pi.",
                "However, without the loss term, DCG is a purely recall-oriented metric and not suitable for an adaptive filtering setting, where the systems utility depends in part on its ability to limit the number of items shown to the user.",
                "Although P(i) could be defined based on empirical studies of user behavior, for simplicity, we use P(i) exactly as defined in equation 6.",
                "The gain G(pi, q) of passage pi with respect to the query q is a function of - 1) the number of relevant nuggets present in pi, and 2) the novelty of each of these nuggets.",
                "We combine these two factors as follows.",
                "For each nugget Nj, we assign an initial weight wj, and also keep a count nj of the number of times this nugget has been seen by the user in the past.",
                "The gain derived from each subsequent occurrence of the same nugget is assumed to reduce by a dampening factor γ.",
                "Thus, G(pi, q) is defined as G(pi, q) = X Nj ∈C(pi,q) wj ∗ γnj (7) where C(pi, q) is the set of all nuggets that appear in passage pi and also belong to the answer key of query q.",
                "The initial weights wj are all set of be 1.0 in our experiments, but can also be set based on a pyramid approach [11].",
                "The choice of dampening factor γ determines the users tolerance for redundancy.",
                "When γ = 0, a nugget will only receive credit for its first occurrence i.e. when nj is zero3 .",
                "For 0 < γ < 1, a nugget receives smaller credit for each successive occurrence.",
                "When γ = 1, no dampening occurs and repeated occurrences of a nugget receive the same credit.",
                "Note that the nugget occurrence counts are preserved between evaluation of successive ranked lists returned by the system, since the users are expected to remember what the system showed them in the past.",
                "We define the loss L(pi, q) as a constant cost c (we use 0.1) incurred when reading a system-returned passage.",
                "Thus, our metric can be re-written as U(q) = nX i=1 Gain(pi, q) logb(b + i − 1) − L(n) (8) where L(n) is the loss associated with a ranked list of length n: L(n) = c · nX i=1 1 logb(b + i − 1) (9) 3 Note that 00 = 1 Due to the similarity with Discounted Cumulated Gain (DCG), we call our metric Discounted Cumulated Utility (DCU).",
                "The DCU score obtained by the system is converted to a Normalized DCU (NDCU) score by dividing it by the DCU score of the ideal ranked list, which is created by ordering passages by their decreasing utility scores U(pi, q) and stopping when U(pi, q) ≤ 0 i.e. when the gain is less than or equal to the cost of reading the passage. 5.",
                "DATA TDT4 was the benchmark corpus used in TDT2002 and TDT2003 evaluations.",
                "The corpus consists of over 90, 000 news articles from multiple sources (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) published between October 2000 and January 2001, in the languages of Arabic, English, and Mandarin.",
                "Speech-recognized and machine-translated versions of the non-English articles were provided as well.",
                "LDC [18] has annotated the corpus with 100 topics, that correspond to various news events in this time period.",
                "Out of these, we selected a subset of 12 actionable events, and defined corresponding tasks for them4 .",
                "For each task, we manually defined a profile consisting of an initial set of (5 to 10) queries, a free-text description of the user history, i.e., what the user already knows about the event, and a list of known on-topic and off-topic documents (if available) as training examples.",
                "For each query, we generated answer keys and corresponding nugget matching rules using the procedure described in section 4.1.2, and produced a total of 120 queries, with an average of 7 nuggets per query. 6.",
                "EXPERIMENTS AND RESULTS 6.1 Baselines We used Indri [17], a popular language-model based retrieval engine, as a baseline for comparison with CAF´E.",
                "Indri supports standard search engine functionality, including pseudo-relevance feedback (PRF) [3, 6], and is representative of a typical query-based retrieval system.",
                "Indri does not support any kind of novelty detection.",
                "We compare Indri with PRF turned on and off, against CAF´E with user feedback, novelty detection and antiredundant ranking turned on and off. 6.2 Experimental Setup We divided the TDT4 corpus spanning 4 months into 10 chunks, each defined as a period of 12 consecutive days.",
                "At any given point of time in the distillation process, each system accessed the past data up to the current point, and returned a ranked list of up 50 passages per query.",
                "The 12 tasks defined on the corpus were divided into a training and test set with 6 tasks each.",
                "Each system was allowed to use the training set to tune its parameters for optimizing NDCU (equation 8), including the relevance threshold for both Indri and CAF´E, and the novelty and antiredundancy thresholds for CAF´E.",
                "The NDCU for each system run is calculated automatically.",
                "User feedback was also simulated - relevance judgments for each system-returned passage (as determined by the nugget matching rules described in section 4.1.2) were 4 URL: http://nyc.lti.cs.cmu.edu/downloads Figure 1: Performance of Indri across chunks Figure 2: Performance of CAF´E across chunks used as user feedback in the adaptation of query profiles and user histories. 6.3 Results In Table 1, we show the NDCU scores of the two systems under various settings.",
                "These scores are averaged over the six tasks in the test set, and are calculated with two dampening factors (see section 4.2): γ = 0 and 0.1, to simulate no tolerance and small tolerance for redundancy, respectively.",
                "Using γ = 0 creates a much more strict metric since it does not give any credit to a passage that contains relevant but redundant information.",
                "Hence, the improvement obtained from enabling user feedback is smaller with γ = 0 than the improvement obtained from feedback with γ = 0.1.",
                "This reveals a shortcoming of contemporary retrieval systemswhen the user gives positive feedback on a passage, the systems gives higher weights to the terms present in that passage and tends to retrieve other passages containing the same terms - and thus - usually the same information.",
                "However, the user does not benefit from seeing such redundant passages, and is usually interested in other passages containing related information.",
                "It is informative to evaluate retrieval systems using our utility measure (with γ = 0) which accounts for novelty and thus gives a more realistic picture of how well a system can generalize from user feedback, rather than using traditional IR measures like recall and precision which give an incomplete picture of improvement obtained from user feedback.",
                "Sometimes, however, users might indeed be interested in seeing the same information from multiple sources, as an Table 1: NDCU Scores of Indri and CAF´E for two dampening factors (γ), and various settings (F: Feedback, N: Novelty Detection, A: Anti-Redundant Ranking) Indri CAF´E γ Base +PRF Base +F +F+N +F+A +F+N+A 0 0.19 0.19 0.22 0.23 0.24 0.24 0.24 0.1 0.28 0.29 0.24 0.35 0.35 0.36 0.36 indicator of its importance or reliability.",
                "In such a case, we can simply choose a higher value for γ which corresponds to a higher tolerance for redundancy, and hence let the system tune its parameters accordingly.",
                "Since documents were processed chunk by chunk, it would be interesting to see how the performance of systems improves over time.",
                "Figures 1 and 2 show the performance trends for both the systems across chunks.",
                "While the performance with and without feedback on the first few chunks is expected to be close, for subsequent chunks, the performance curve with feedback enabled rises above the one with the no-feedback setting.",
                "The performance trends are not consistent across all chunks because on-topic documents are not uniformly distributed over all the chunks, making some queries easier than others in certain chunks.",
                "Moreover, since Indri uses pseudo-relevance feedback while CAF´E uses feedback based on actual relevance judgments, the improvement in case of Indri is less dramatic than that of CAF´E. 7.",
                "CONCLUDING REMARKS This paper presents the first investigation on utility-based information distillation with a system that learns the longlasting information needs from fine-grained user feedback over a sequence of ranked passages.",
                "Our system, called CAF´E, combines adaptive filtering, novelty detection and antiredundant passage ranking in a unified framework for utility optimization.",
                "We developed a new scheme for automated evaluation and feedback based on a semi-automatic procedure for acquiring rules that allow automatically matching nuggets against system responses.",
                "We also proposed an extension of the NDCG metric for assessing the utility of ranked passages as a weighted combination of relevance and novelty.",
                "Our experiments on the newly annotated TDT4 benchmark corpus show encouraging utility enhancement over Indri, and also over our own system with incremental learning and novelty detection turned off. 8.",
                "ACKNOWLEDGMENTS We would like to thank Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng, and the Qualitative Data Analysis Program at the University of Pittsburgh lead by Dr. Stuart Shulman for their help with collecting and processing the extended TDT4 annotations used in our experiments.",
                "This work is supported in parts by the National Science Foundation (NSF) under grant IIS0434035, and the Defense Advanced Research Project Agency (DARPA) under contracts NBCHD030010 and W0550432.",
                "Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors. 9.",
                "ADDITIONAL AUTHORS Jian Zhang (jianzhan@stat.purdue.edu)∗ , Jaime Carbonell (jgc@cs.cmu.edu)† , Peter Brusilovsky (peterb+@pitt.edu)‡ , Daqing He(dah44@pitt.edu)‡ 10.",
                "REFERENCES [1] J. Allan.",
                "Incremental Relevance Feedback for Information Filtering.",
                "Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, pages 270-278, 1996. [2] J. Allan, C. Wade, and A. Bolivar.",
                "Retrieval and Novelty Detection at the Sentence Level.",
                "Proceedings of the ACM SIGIR conference on research and development in information retrieval, 2003. [3] C. Buckley, G. Salton, and J. Allan.",
                "Automatic Retrieval with Locality Information using SMART.",
                "NIST special publication, (500207):59-72, 1993. [4] J. Callan.",
                "Learning While Filtering Documents.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 224-231, 1998. [5] J. Carbonell and J. Goldstein.",
                "The use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 335-336, 1998. [6] E. Efthimiadis.",
                "Query Expansion.",
                "Annual Review of Information Science and Technology (ARIST), 31:p121-87, 1996. [7] J. Fiscus and G. Duddington.",
                "Topic Detection and Tracking Overview.",
                "Topic Detection and Tracking: Event-based Information Organization, pages 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos.",
                "A Statistical Model for Multilingual Entity Detection and Tracking.",
                "NAACL/HLT, 2004. [9] K. J¨arvelin and J. Kek¨al¨ainen.",
                "Cumulated Gain-based Evaluation of IR Techniques.",
                "ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin and D. Demner-Fushman.",
                "Automatically Evaluating Answers to Definition Questions.",
                "Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), 2005. ∗ Statistics Dept., Purdue University, West Lafayette, USA † Language Technologies Inst., Carnegie Mellon University, Pittsburgh, USA ‡ School of Information Sciences, Univ. of Pittsburgh, Pittsburgh, USA [11] J. Lin and D. Demner-Fushman.",
                "Will Pyramids Built of nUggets Topple Over.",
                "Proceedings of HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos.",
                "A Mention-synchronous Coreference Resolution Algorithm based on the Bell Tree.",
                "Proc. of ACL, 4:136-143, 2004. [13] G. Marton.",
                "Nuggeteer: Automatic Nugget-Based Evaluation Using Descriptions and Judgments.",
                "HLT/NAACL, 2006. [14] E. Riloff.",
                "Automatically Constructing a Dictionary for Information Extraction Tasks.",
                "Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 811-816, 1993. [15] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9: Filtering track.",
                "The Ninth Text REtrieval Conference (TREC-9), pages 361-368. [16] R. Schapire, Y.",
                "Singer, and A. Singhal.",
                "Boosting and Rocchio Applied to Text Filtering.",
                "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle, and W. Croft.",
                "Indri: A Language Model-based Search Engine for Complex Queries.",
                "Proceedings of the International Conference on Intelligence Analysis, 2004. [18] The Linguistic Data Consortium. http://www.ldc.upenn.edu/. [19] E. Voorhees.",
                "Overview of the TREC 2003 Question Answering Track.",
                "Proceedings of the Twelfth Text REtrieval Conference (TREC 2003), 2003. [20] Y. Yang and B. Kisiel.",
                "Margin-based Local Regression for Adaptive Filtering.",
                "Proceedings of the twelfth international conference on Information and knowledge management, pages 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of Adaptive Filtering Methods in a Cross-benchmark Evaluation.",
                "Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 98-105, 2005. [22] C. Zhai, W. Cohen, and J. Lafferty.",
                "Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 10-17, 2003. [23] J. Zhang and Y. Yang.",
                "Robustness of Regularized Linear Classification Methods in Text Categorization.",
                "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 190-197, 2003. [24] Y. Zhang.",
                "Using Bayesian Priors to Combine Classifiers for Adaptive Filtering.",
                "Proceedings of the 27th annual international conference on Research and development in information retrieval, pages 345-352, 2004. [25] Y. Zhang, J. Callan, and T. Minka.",
                "Novelty and Redundancy Detection in Adaptive Filtering.",
                "Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "La Sección 4 discute temas con respecto a la \"metodología de evaluación\" y propone un nuevo esquema.",
                "La combinación de AF, IR y ND con las nuevas extensiones plantea una importante pregunta de investigación sobre \"metodología de evaluación\": ¿Cómo podemos medir la utilidad de dicho sistema de destilación de información?",
                "\"Metodología de evaluación\" El enfoque que propusimos anteriormente para la destilación de información plantea problemas importantes con respecto a la \"metodología de evaluación\".",
                "Por lo tanto, debemos desarrollar una nueva \"metodología de evaluación\".4.1 Claves de respuesta Para habilitar la evaluación de un sistema cuya salida consiste en pasajes de longitud arbitraria, tomamos prestado el concepto de claves de respuesta de la comunidad de respuesta (QA), donde los sistemas pueden devolver los tramos arbitrarios de texto como respuestas."
            ],
            "translated_text": "",
            "candidates": [
                "metodología de evaluación",
                "metodología de evaluación",
                "metodología de evaluación",
                "metodología de evaluación",
                "metodología de evaluación",
                "Metodología de evaluación",
                "metodología de evaluación",
                "metodología de evaluación",
                "metodología de evaluación"
            ],
            "error": []
        }
    }
}