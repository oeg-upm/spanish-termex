{
    "id": "I-51",
    "original_text": "Learning and Joint Deliberation through Argumentation in Multi-Agent Systems Santi Ontañón CCL, Cognitive Computing Lab Georgia Institute of Technology Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Artificial Intelligence Research Institute CSIC, Spanish Council for Scientific Research Campus UAB, 08193 Bellaterra, Catalonia (Spain) enric@iiia.csic.es ABSTRACT In this paper we will present an argumentation framework for learning agents (AMAL) designed for two purposes: (1) for joint deliberation, and (2) for learning from communication. The AMAL framework is completely based on learning from examples: the argument preference relation, the argument generation policy, and the counterargument generation policy are case-based techniques. For join deliberation, learning agents share their experience by forming a committee to decide upon some joint decision. We experimentally show that the argumentation among committees of agents improves both the individual and joint performance. For learning from communication, an agent engages into arguing with other agents in order to contrast its individual hypotheses and receive counterexamples; the argumentation process improves their learning scope and individual performance. Categories and Subject Descriptors I.2.6 [Artificial Intelligence]: Learning; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems, Intelligent Agents 1. INTRODUCTION Argumentation frameworks for multi-agent systems can be used for different purposes like joint deliberation, persuasion, negotiation, and conflict resolution. In this paper we will present an argumentation framework for learning agents, and show that it can be used for two purposes: (1) joint deliberation, and (2) learning from communication. Argumentation-based joint deliberation involves discussion over the outcome of a particular situation or the appropriate course of action for a particular situation. Learning agents are capable of learning from experience, in the sense that past examples (situations and their outcomes) are used to predict the outcome for the situation at hand. However, since individual agents experience may be limited, individual knowledge and prediction accuracy is also limited. Thus, learning agents that are capable of arguing their individual predictions with other agents may reach better prediction accuracy after such an argumentation process. Most existing argumentation frameworks for multi-agent systems are based on deductive logic or some other deductive logic formalism specifically designed to support argumentation, such as default logic [3]). Usually, an argument is seen as a logical statement, while a counterargument is an argument offered in opposition to another argument [4, 13]; agents use a preference relation to resolve conflicting arguments. However, logic-based argumentation frameworks assume agents with preloaded knowledge and preference relation. In this paper, we focus on an Argumentation-based Multi-Agent Learning (AMAL) framework where both knowledge and preference relation are learned from experience. Thus, we consider a scenario with agents that (1) work in the same domain using a shared ontology, (2) are capable of learning from examples, and (3) communicate using an argumentative framework. Having learning capabilities allows agents effectively use a specific form of counterargument, namely the use of counterexamples. Counterexamples offer the possibility of agents learning during the argumentation process. Moreover, learning agents allow techniques that use learnt experience to generate adequate arguments and counterarguments. Specifically, we will need to address two issues: (1) how to define a technique to generate arguments and counterarguments from examples, and (2)how to define a preference relation over two conflicting arguments that have been induced from examples. This paper presents a case-based approach to address both issues. The agents use case-based reasoning (CBR) [1] to learn from past cases (where a case is a situation and its outcome) in order to predict the outcome of a new situation. We propose an argumentation protocol inside the AMAL framework at supports agents in reaching a joint prediction over a specific situation or problem - moreover, the reasoning needed to support the argumentation process will also be based on cases. In particular, we present two case-based measures, one for generating the arguments and counterarguments adequate to a particular situation and another for determining preference relation among arguments. Finally, we evaluate (1) if argumentation between learning agents can produce a joint prediction that improves over individual learning performance and (2) if learning from the counterexamples conveyed during the argumentation process increases the individual performance with precisely those cases being used while arguing among them. The paper is structured as follows. Section 2 discusses the relation among argumentation, collaboration and learning. Then Section 3 introduces our multi-agent CBR (MAC) framework and the notion of justified prediction. After that, Section 4 formally defines our argumentation framework. Sections 5 and 6 present our case-based preference relation and argument generation policies respectively. Later, Section 7 presents the argumentation protocol in our AMAL framework. After that, Section 8 presents an exemplification of the argumentation framework. Finally, Section 9 presents an empirical evaluation of our two main hypotheses. The paper closes with related work and conclusions sections. 2. ARGUMENTATION,COLLABORATION AND LEARNING Both learning and collaboration are ways in which an agent can improve individual performance. In fact, there is a clear parallelism between learning and collaboration in multi-agent systems, since both are ways in which agents can deal with their shortcomings. Let us show which are the main motivations that an agent can have to learn or to collaborate. • Motivations to learn: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems. • Motivations to collaborate: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems, - Increase the range of accessible resources. Looking at the above lists of motivation, we can easily see that learning and collaboration are very related in multi-agent systems. In fact, with the exception of the last item in the motivations to collaborate list, they are two extremes of a continuum of strategies to improve performance. An agent may choose to increase performance by learning, by collaborating, or by finding an intermediate point that combines learning and collaboration in order to improve performance. In this paper we will propose AMAL, an argumentation framework for learning agents, and will also also show how AMAL can be used both for learning from communication and for solving problems in a collaborative way: • Agents can solve problems in a collaborative way via engaging an argumentation process about the prediction for the situation at hand. Using this collaboration, the prediction can be done in a more informed way, since the information known by several agents has been taken into account. • Agents can also learn from communication with other agents by engaging an argumentation process. Agents that engage in such argumentation processes can learn from the arguments and counterexamples received from other agents, and use this information for predicting the outcomes of future situations. In the rest of this paper we will propose an argumentation framework and show how it can be used both for learning and for solving problems in a collaborative way. 3. MULTI-AGENT CBR SYSTEMS A Multi-Agent Case Based Reasoning System (MAC) M = {(A1, C1), ..., (An, Cn)} is a multi-agent system composed of A = {Ai, ..., An}, a set of CBR agents, where each agent Ai ∈ A possesses an individual case base Ci. Each individual agent Ai in a MAC is completely autonomous and each agent Ai has access only to its individual and private case base Ci. A case base Ci = {c1, ..., cm} is a collection of cases. Agents in a MAC system are able to individually solve problems, but they can also collaborate with other agents to solve problems. In this framework, we will restrict ourselves to analytical tasks, i.e. tasks like classification, where the solution of a problem is achieved by selecting a solution class from an enumerated set of solution classes. In the following we will note the set of all the solution classes by S = {S1, ..., SK }. Therefore, a case c = P, S is a tuple containing a case description P and a solution class S ∈ S. In the following, we will use the terms problem and case description indistinctly. Moreover, we will use the dot notation to refer to elements inside a tuple; e.g., to refer to the solution class of a case c, we will write c.S. Therefore, we say a group of agents perform joint deliberation, when they collaborate to find a joint solution by means of an argumentation process. However, in order to do so, an agent has to be able to justify its prediction to the other agents (i.e. generate an argument for its predicted solution that can be examined and critiqued by the other agents). The next section addresses this issue. 3.1 Justified Predictions Both expert systems and CBR systems may have an explanation component [14] in charge of justifying why the system has provided a specific answer to the user. The line of reasoning of the system can then be examined by a human expert, thus increasing the reliability of the system. Most of the existing work on explanation generation focuses on generating explanations to be provided to the user. However, in our approach we use explanations (or justifications) as a tool for improving communication and coordination among agents. We are interested in justifications since they can be used as arguments. For that purpose, we will benefit from the ability of some machine learning methods to provide justifications. A justification built by a CBR method after determining that the solution of a particular problem P was Sk is a description that contains the relevant information from the problem P that the CBR method has considered to predict Sk as the solution of P. In particular, CBR methods work by retrieving similar cases to the problem at hand, and then reusing their solutions for the current problem, expecting that since the problem and the cases are similar, the solutions will also be similar. Thus, if a CBR method has retrieved a set of cases C1, ..., Cn to solve a particular problem P the justification built will contain the relevant information from the problem P that made the CBR system retrieve that particular set of cases, i.e. it will contain the relevant information that P and C1, ..., Cn have in common. For example, Figure 1 shows a justification build by a CBR system for a toy problem (in the following sections we will show justifications for real problems). In the figure, a problem has two attributes (Traffic_light, and Cars_passing), the retrieval mechanism of the CBR system notices that by considering only the attribute Traffic_light, it can retrieve two cases that predict the same solution: wait. Thus, since only this attribute has been used, it is the only one appearing in the justification. The values of the rest of attributes are irrelevant, since whatever their value the solution class would have been the same. 976 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Problem Traffic_light: red Cars_passing: no Case 1 Traffic_light: red Cars_passing: no Solution: wait Case 3 Traffic_light: red Cars_passing: yes Solution: wait Case 4 Traffic_light: green Cars_passing: yes Solution: wait Case 2 Traffic_light: green Cars_passing: no Solution: cross Retrieved cases Solution: wait Justification Traffic_light: red Figure 1: An example of justification generation in a CBR system. Notice that, since the only relevant feature to decide is Traffic_light (the only one used to retrieve cases), it is the only one appearing in the justification. In general, the meaning of a justification is that all (or most of) the cases in the case base of an agent that satisfy the justification (i.e. all the cases that are subsumed by the justification) belong to the predicted solution class. In the rest of the paper, we will use to denote the subsumption relation. In our work, we use LID [2], a CBR method capable of building symbolic justifications such as the one exemplified in Figure 1. When an agent provides a justification for a prediction, the agent generates a justified prediction: DEFINITION 3.1. A Justified Prediction is a tuple J = A, P, S, D where agent A considers S the correct solution for problem P, and that prediction is justified a symbolic description D such that J.D J.P. Justifications can have many uses for CBR systems [8, 9]. In this paper, we are going to use justifications as arguments, in order to allow learning agents to engage in argumentation processes. 4. ARGUMENTS AND COUNTERARGUMENTS For our purposes an argument α generated by an agent A is composed of a statement S and some evidence D supporting S as correct. In the remainder of this section we will see how this general definition of argument can be instantiated in specific kind of arguments that the agents can generate. In the context of MAC systems, agents argue about predictions for new problems and can provide two kinds of information: a) specific cases P, S , and b) justified predictions: A, P, S, D . Using this information, we can define three types of arguments: justified predictions, counterarguments, and counterexamples. A justified prediction α is generated by an agent Ai to argue that Ai believes that the correct solution for a given problem P is α.S, and the evidence provided is the justification α.D. In the example depicted in Figure 1, an agent Ai may generate the argument α = Ai, P, Wait, (Traffic_light = red) , meaning that the agent Ai believes that the correct solution for P is Wait because the attribute Traffic_light equals red. A counterargument β is an argument offered in opposition to another argument α. In our framework, a counterargument consists of a justified prediction Aj, P, S , D generated by an agent Aj with the intention to rebut an argument α generated by another agent Ai, that endorses a solution class S different from that of α.S for the problem at hand and justifies this with a justification D . In the example in Figure 1, if an agent generates the argument α = Ai, P, Walk, (Cars_passing = no) , an agent that thinks that the correct solution is Wait might answer with the counterargument β = Aj, P, Wait, (Cars_passing = no ∧ Traffic_light = red) , meaning that, although there are no cars passing, the traffic light is red, and the street cannot be crossed. A counterexample c is a case that contradicts an argument α. Thus a counterexample is also a counterargument, one that states that a specific argument α is not always true, and the evidence provided is the case c. Specifically, for a case c to be a counterexample of an argument α, the following conditions have to be met: α.D c and α.S = c.S, i.e. the case must satisfy the justification α.D and the solution of c must be different than the predicted by α. By exchanging arguments and counterarguments (including counterexamples), agents can argue about the correct solution of a given problem, i.e. they can engage a joint deliberation process. However, in order to do so, they need a specific interaction protocol, a preference relation between contradicting arguments, and a decision policy to generate counterarguments (including counterexamples). In the following sections we will present these elements. 5. PREFERENCE RELATION A specific argument provided by an agent might not be consistent with the information known to other agents (or even to some of the information known by the agent that has generated the justification due to noise in training data). For that reason, we are going to define a preference relation over contradicting justified predictions based on cases. Basically, we will define a confidence measure for each justified prediction (that takes into account the cases owned by each agent), and the justified prediction with the highest confidence will be the preferred one. The idea behind case-based confidence is to count how many of the cases in an individual case base endorse a justified prediction, and how many of them are counterexamples of it. The more the endorsing cases, the higher the confidence; and the more the counterexamples, the lower the confidence. Specifically, to assess the confidence of a justified prediction α, an agent obtains the set of cases in its individual case base that are subsumed by α.D. With them, an agent Ai obtains the Y (aye) and N (nay) values: • Y Ai α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by the justification α.D that belong to the solution class α.S, • NAi α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by justification α.D that do not belong to that solution class. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 977 + + + + + +  - - + Figure 2: Confidence of arguments is evaluated by contrasting them against the case bases of the agents. An agent estimates the confidence of an argument as: CAi (α) = Y Ai α 1 + Y Ai α + NAi α i.e. the confidence on a justified prediction is the number of endorsing cases divided by the number of endorsing cases plus counterexamples. Notice that we add 1 to the denominator, this is to avoid giving excessively high confidences to justified predictions whose confidence has been computed using a small number of cases. Notice that this correction follows the same idea than the Laplace correction to estimate probabilities. Figure 2 illustrates the individual evaluation of the confidence of an argument, in particular, three endorsing cases and one counterexample are found in the case base of agents Ai, giving an estimated confidence of 0.6 Moreover, we can also define the joint confidence of an argument α as the confidence computed using the cases present in the case bases of all the agents in the group: C(α) = i Y Ai α 1 + i Y Ai α + NAi α Notice that, to collaboratively compute the joint confidence, the agents only have to make public the aye and nay values locally computed for a given argument. In our framework, agents use this joint confidence as the preference relation: a justified prediction α is preferred over another one β if C(α) ≥ C(β). 6. GENERATION OF ARGUMENTS In our framework, arguments are generated by the agents from cases, using learning methods. Any learning method able to provide a justified prediction can be used to generate arguments. For instance, decision trees and LID [2] are suitable learning methods. Specifically, in the experiments reported in this paper agents use LID. Thus, when an agent wants to generate an argument endorsing that a specific solution class is the correct solution for a problem P, it generates a justified prediction as explained in Section 3.1. For instance, Figure 3 shows a real justification generated by LID after solving a problem P in the domain of marine sponges identification. In particular, Figure 3 shows how when an agent receives a new problem to solve (in this case, a new sponge to determine its order), the agent uses LID to generate an argument (consisting on a justified prediction) using the cases in the case base of the agent. The justification shown in Figure 3 can be interpreted saying that the predicted solution is hadromerida because the smooth form of the megascleres of the spiculate skeleton of the sponge is of type tylostyle, the spikulate skeleton of the sponge has no uniform length, and there is no gemmules in the external features of the sponge. Thus, the argument generated will be α = A1, P, hadromerida, D1 . 6.1 Generation of Counterarguments As previously stated, agents may try to rebut arguments by generating counterargument or by finding counterexamples. Let us explain how they can be generated. An agent Ai wants to generate a counterargument β to rebut an argument α when α is in contradiction with the local case base of Ai. Moreover, while generating such counterargument β, Ai expects that β is preferred over α. For that purpose, we will present a specific policy to generate counterarguments based on the specificity criterion [10]. The specificity criterion is widely used in deductive frameworks for argumentation, and states that between two conflicting arguments, the most specific should be preferred since it is, in principle, more informed. Thus, counterarguments generated based on the specificity criterion are expected to be preferable (since they are more informed) to the arguments they try to rebut. However, there is no guarantee that such counterarguments will always win, since, as we have stated in Section 5, agents in our framework use a preference relation based on joint confidence. Moreover, one may think that it would be better that the agents generate counterarguments based on the joint confidence preference relation; however it is not obvious how to generate counterarguments based on joint confidence in an efficient way, since collaboration is required in order to evaluate joint confidence. Thus, the agent generating the counterargument should constantly communicate with the other agents at each step of the induction algorithm used to generate counterarguments (presently one of our future research lines). Thus, in our framework, when an agent wants to generate a counterargument β to an argument α, β has to be more specific than α (i.e. α.D < β.D). The generation of counterarguments using the specificity criterion imposes some restrictions over the learning method, although LID or ID3 can be easily adapted for this task. For instance, LID is an algorithm that generates a description starting from scratch and heuristically adding features to that term. Thus, at every step, the description is made more specific than in the previous step, and the number of cases that are subsumed by that description is reduced. When the description covers only (or almost only) cases of a single solution class LID terminates and predicts that solution class. To generate a counterargument to an argument α LID just has to use as starting point the description α.D instead of starting from scratch. In this way, the justification provided by LID will always be subsumed by α.D, and thus the resulting counterargument will be more specific than α. However, notice that LID may sometimes not be able to generate counterarguments, since LID may not be able to specialize the description α.D any further, or because the agent Ai has no case inCi that is subsumed by α.D. Figure 4 shows how an agent A2 that disagreed with the argument shown in Figure 3, generates a counterargument using LID. Moreover, Figure 4 shows the generation of a counterargument β1 2 for the argument α0 1 (in Figure 3) that is a specialization of α0 1. 978 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Solution: hadromerida Justification: D1 Sponge Spikulate skeleton External features External features Gemmules: no Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Case Base of A1 LID New sponge P Figure 3: Example of a real justification generated by LID in the marine sponges data set. Specifically, in our experiments, when an agent Ai wants to rebut an argument α, uses the following policy: 1. Agent Ai uses LID to try to find a counterargument β more specific than α; if found, β is sent to the other agent as a counterargument of α. 2. If not found, then Ai searches for a counterexample c ∈ Ci of α. If a case c is found, then c is sent to the other agent as a counterexample of α. 3. If no counterexamples are found, then Ai cannot rebut the argument α. 7. ARGUMENTATION-BASED MULTI-AGENT LEARNING The interaction protocol of AMAL allows a group of agents A1, ..., An to deliberate about the correct solution of a problem P by means of an argumentation process. If the argumentation process arrives to a consensual solution, the joint deliberation ends; otherwise a weighted vote is used to determine the joint solution. Moreover, AMAL also allows the agents to learn from the counterexamples received from other agents. The AMAL protocol consists on a series of rounds. In the initial round, each agent states which is its individual prediction for P. Then, at each round an agent can try to rebut the prediction made by any of the other agents. The protocol uses a token passing mechanism so that agents (one at a time) can send counterarguments or counterexamples if they disagree with the prediction made by any other agent. Specifically, each agent is allowed to send one counterargument or counterexample each time he gets the token (notice that this restriction is just to simplify the protocol, and that it does not restrict the number of counterargument an agent can sent, since they can be delayed for subsequent rounds). When an agent receives a counterargument or counterexample, it informs the other agents if it accepts the counterargument (and changes its prediction) or not. Moreover, agents have also the opportunity to answer to counterarguments when they receive the token, by trying to generate a counterargument to the counterargument. When all the agents have had the token once, the token returns to the first agent, and so on. If at any time in the protocol, all the agents agree or during the last n rounds no agent has generated any counterargument, the protocol ends. Moreover, if at the end of the argumentation the agents have not reached an agreement, then a voting mechanism that uses the confidence of each prediction as weights is used to decide the final solution (Thus, AMAL follows the same mechanism as human committees, first each individual member of a committee exposes his arguments and discuses those of the other members (joint deliberation), and if no consensus is reached, then a voting mechanism is required). At each iteration, agents can use the following performatives: • assert(α): the justified prediction held during the next round will be α. An agent can only hold a single prediction at each round, thus is multiple asserts are send, only the last one is considered as the currently held prediction. • rebut(β, α): the agent has found a counterargument β to the prediction α. We will define Ht = αt 1, ..., αt n as the predictions that each of the n agents hold at a round t. Moreover, we will also define contradict(αt i) = {α ∈ Ht|α.S = αt i.S} as the set of contradicting arguments for an agent Ai in a round t, i.e. the set of arguments at round t that support a different solution class than αt i. The protocol is initiated because one of the agents receives a problem P to be solved. After that, the agent informs all the other agents about the problem P to solve, and the protocol starts: 1. At round t = 0, each one of the agents individually solves P, and builds a justified prediction using its own CBR method. Then, each agent Ai sends the performative assert(α0 i ) to the other agents. Thus, the agents know H0 = α0 i , ..., α0 n . Once all the predictions have been sent the token is given to the first agent A1. 2. At each round t (other than 0), the agents check whether their arguments in Ht agree. If they do, the protocol moves to step 5. Moreover, if during the last n rounds no agent has sent any counterexample or counterargument, the protocol also moves to step 5. Otherwise, the agent Ai owner of the token tries to generate a counterargument for each of the opposing arguments in contradict(αt i) ⊆ Ht (see Section 6.1). Then, the counterargument βt i against the prediction αt j with the lowest confidence C(αt j) is selected (since αt j is the prediction more likely to be successfully rebutted). • If βt i is a counterargument, then, Ai locally compares αt i with βt i by assessing their confidence against its individual case base Ci (see Section 5) (notice that Ai is comparing its previous argument with the counterargument that Ai itself has just generated and that is about The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 979 Sponge Spikulate skeleton External features External features Gemmules: no Growing: Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Growing Grow: massive Case Base of A2 LID Solution: astrophorida Justification: D2 Figure 4: Generation of a counterargument using LID in the sponges data set. to send to Aj). If CAi (βt i ) > CAi (αt i), then Ai considers that βt i is stronger than its previous argument, changes its argument to βt i by sending assert(βt i ) to the rest of the agents (the intuition behind this is that since a counterargument is also an argument, Ai checks if the newly counterargument is a better argument than the one he was previously holding) and rebut(βt i , αt j) to Aj. Otherwise (i.e. CAi (βt i ) ≤ CAi (αt i)), Ai will send only rebut(βt i , αt j) to Aj. In any of the two situations the protocol moves to step 3. • If βt i is a counterexample c, then Ai sends rebut(c, αt j) to Aj. The protocol moves to step 4. • If Ai cannot generate any counterargument or counterexample, the token is sent to the next agent, a new round t + 1 starts, and the protocol moves to state 2. 3. The agent Aj that has received the counterargument βt i , locally compares it against its own argument, αt j, by locally assessing their confidence. If CAj (βt i ) > CAj (αt j), then Aj will accept the counterargument as stronger than its own argument, and it will send assert(βt i ) to the other agents. Otherwise (i.e. CAj (βt i ) ≤ CAj (αt j)), Aj will not accept the counterargument, and will inform the other agents accordingly. Any of the two situations start a new round t + 1, Ai sends the token to the next agent, and the protocol moves back to state 2. 4. The agent Aj that has received the counterexample c retains it into its case base and generates a new argument αt+1 j that takes into account c, and informs the rest of the agents by sending assert(αt+1 j ) to all of them. Then, Ai sends the token to the next agent, a new round t + 1 starts, and the protocol moves back to step 2. 5. The protocol ends yielding a joint prediction, as follows: if the arguments in Ht agree then their prediction is the joint prediction, otherwise a voting mechanism is used to decide the joint prediction. The voting mechanism uses the joint confidence measure as the voting weights, as follows: S = arg max Sk∈S αi∈Ht|αi.S=Sk C(αi) Moreover, in order to avoid infinite iterations, if an agent sends twice the same argument or counterargument to the same agent, the message is not considered. 8. EXEMPLIFICATION Let us consider a system composed of three agents A1, A2 and A3. One of the agents, A1 receives a problem P to solve, and decides to use AMAL to solve it. For that reason, invites A2 and A3 to take part in the argumentation process. They accept the invitation, and the argumentation protocol starts. Initially, each agent generates its individual prediction for P, and broadcasts it to the other agents. Thus, all of them can compute H0 = α0 1, α0 2, α0 3 . In particular, in this example: • α0 1 = A1, P, hadromerida, D1 • α0 2 = A2, P, astrophorida, D2 • α0 3 = A3, P, axinellida, D3 A1 starts owning the token and tries to generate counterarguments for α0 2 and α0 3, but does not succeed, however it has one counterexample c13 for α0 3. Thus, A1 sends the the message rebut( c13, α0 3) to A3. A3 incorporates c13 into its case base and tries to solve the problem P again, now taking c13 into consideration. A3 comes up with the justified prediction α1 3 = A3, P, hadromerida, D4 , and broadcasts it to the rest of the agents with the message assert(α1 3). Thus, all of them know the new H1 = α0 1, α0 2, α1 3 . Round 1 starts and A2 gets the token. A2 tries to generate counterarguments for α0 1 and α1 3 and only succeeds to generate a counterargument β1 2 = A2, P, astrophorida, D5 against α1 3. The counterargument is sent to A3 with the message rebut(β1 2 , α1 3). Agent A3 receives the counterargument and assesses its local confidence. The result is that the individual confidence of the counterargument β1 2 is lower than the local confidence of α1 3. Therefore, A3 does not accept the counterargument, and thus H2 = α0 1, α0 2, α1 3 . Round 2 starts and A3 gets the token. A3 generates a counterargument β2 3 = A3, P, hadromerida, D6 for α0 2 and sends it to A2 with the message rebut(β2 3 , α0 2). Agent A2 receives the counterargument and assesses its local confidence. The result is that the local confidence of the counterargument β2 3 is higher than the local confidence of α0 2. Therefore, A2 accepts the counterargument and informs the rest of the agents with the message assert(β2 3 ). After that, H3 = α0 1, β2 3 , α1 3 . At Round 3, since all the agents agree (all the justified predictions in H3 predict hadromerida as the solution class) The protocol ends, and A1 (the agent that received the problem) considers hadromerida as the joint solution for the problem P. 9. EXPERIMENTAL EVALUATION 980 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) SPONGE 75 77 79 81 83 85 87 89 91 2 3 4 5 AMAL Voting Individual SOYBEAN 55 60 65 70 75 80 85 90 2 3 4 5 AMAL Voting Individual Figure 5: Individual and joint accuracy for 2 to 5 agents. In this section we empirically evaluate the AMAL argumentation framework. We have made experiments in two different data sets: soybean (from the UCI machine learning repository) and sponge (a relational data set). The soybean data set has 307 examples and 19 solution classes, while the sponge data set has 280 examples and 3 solution classes. In an experimental run, the data set is divided in 2 sets: the training set and the test set. The training set examples are distributed among 5 different agents without replication, i.e. there is no example shared by two agents. In the testing stage, problems in the test set arrive randomly to one of the agents, and their goal is to predict the correct solution. The experiments are designed to test two hypotheses: (H1) that argumentation is a useful framework for joint deliberation and can improve over other typical methods such as voting; and (H2) that learning from communication improves the individual performance of a learning agent participating in an argumentation process. Moreover, we also expect that the improvement achieved from argumentation will increase as the number of agents participating in the argumentation increases (since more information will be taken into account). Concerning H1 (argumentation is a useful framework for joint deliberation), we ran 4 experiments, using 2, 3, 4, and 5 agents respectively (in all experiments each agent has a 20% of the training data, since the training is always distributed among 5 agents). Figure 5 shows the result of those experiments in the sponge and soybean data sets. Classification accuracy is plotted in the vertical axis, and in the horizontal axis the number of agents that took part in the argumentation processes is shown. For each number of agents, three bars are shown: individual, Voting, and AMAL. The individual bar shows the average accuracy of individual agents predictions; the voting bar shows the average accuracy of the joint prediction achieved by voting but without any argumentation; and finally the AMAL bar shows the average accuracy of the joint prediction using argumentation. The results shown are the average of 5 10-fold cross validation runs. Figure 5 shows that collaboration (voting and AMAL) outperforms individual problem solving. Moreover, as we expected, the accuracy improves as more agents collaborate, since more information is taken into account. We can also see that AMAL always outperforms standard voting, proving that joint decisions are based on better information as provided by the argumentation process. For instance, the joint accuracy for 2 agents in the sponge data set is of 87.57% for AMAL and 86.57% for voting (while individual accuracy is just 80.07%). Moreover, the improvement achieved by AMAL over Voting is even larger in the soybean data set. The reason is that the soybean data set is more difficult (in the sense that agents need more data to produce good predictions). These experimental results show that AMAL effectively exploits the opportunity for improvement: the accuracy is higher only because more agents have changed their opinion during argumentation (otherwise they would achieve the same result as Voting). Concerning H2 (learning from communication in argumentation processes improves individual prediction ), we ran the following experiment: initially, we distributed a 25% of the training set among the five agents; after that, the rest of the cases in the training set is sent to the agents one by one; when an agent receives a new training case, it has several options: the agent can discard it, the agent can retain it, or the agent can use it for engaging an argumentation process. Figure 6 shows the result of that experiment for the two data sets. Figure 6 contains three plots, where NL (not learning) shows accuracy of an agent with no learning at all; L (learning), shows the evolution of the individual classification accuracy when agents learn by retaining the training cases they individually receive (notice that when all the training cases have been retained at 100%, the accuracy should be equal to that of Figure 5 for individual agents); and finally LFC (learning from communication) shows the evolution of the individual classification accuracy of learning agents that also learn by retaining those counterexamples received during argumentation (i.e. they learn both from training examples and counterexamples). Figure 6 shows that if an agent Ai learns also from communication, Ai can significantly improve its individual performance with just a small number of additional cases (those selected as relevant counterexamples for Ai during argumentation). For instance, in the soybean data set, individual agents have achieved an accuracy of 70.62% when they also learn from communication versus an accuracy of 59.93% when they only learn from their individual experience. The number of cases learnt from communication depends on the properties of the data set: in the sponges data set, agents have retained only very few additional cases, and significantly improved individual accuracy; namely they retain 59.96 cases in average (compared to the 50.4 cases retained if they do not learn from communication). In the soybean data set more counterexamples are learnt to significantly improve individual accuracy, namely they retain 87.16 cases in average (compared to 55.27 cases retained if they do not learn from communication). Finally, the fact that both data sets show a significant improvement points out the adaptive nature of the argumentation-based approach to learning from communication: the useful cases are selected as counterexamples (and no more than those needed), and they have the intended effect. 10. RELATED WORK Concerning CBR in a multi-agent setting, the first research was on negotiated case retrieval [11] among groups of agents. Our work on multi-agent case-based learning started in 1999 [6]; later Mc Ginty and Smyth [7] presented a multi-agent collaborative CBR approach (CCBR) for planning. Finally, another interesting approach is multi-case-base reasoning (MCBR) [5], that deals with The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 981 SPONGE 60 65 70 75 80 85 25% 40% 55% 70% 85% 100% LFC L NL SOYBEAN 20 30 40 50 60 70 80 90 25% 40% 55% 70% 85% 100% LFC L NL Figure 6: Learning from communication resulting from argumentation in a system composed of 5 agents. distributed systems where there are several case bases available for the same task and addresses the problems of cross-case base adaptation. The main difference is that our MAC approach is a way to distribute the Reuse process of CBR (using a voting system) while Retrieve is performed individually by each agent; the other multiagent CBR approaches, however, focus on distributing the Retrieve process. Research on MAS argumentation focus on several issues like a) logics, protocols and languages that support argumentation, b) argument selection and c) argument interpretation. Approaches for logic and languages that support argumentation include defeasible logic [4] and BDI models [13]. Although argument selection is a key aspect of automated argumentation (see [12] and [13]), most research has been focused on preference relations among arguments. In our framework we have addressed both argument selection and preference relations using a case-based approach. 11. CONCLUSIONS AND FUTURE WORK In this paper we have presented an argumentation-based framework for multi-agent learning. Specifically, we have presented AMAL, a framework that allows a group of learning agents to argue about the solution of a given problem and we have shown how the learning capabilities can be used to generate arguments and counterarguments. The experimental evaluation shows that the increased amount of information provided to the agents by the argumentation process increases their predictive accuracy, and specially when an adequate number of agents take part in the argumentation. The main contributions of this work are: a) an argumentation framework for learning agents; b) a case-based preference relation over arguments, based on computing an overall confidence estimation of arguments; c) a case-based policy to generate counterarguments and select counterexamples; and d) an argumentation-based approach for learning from communication. Finally, in the experiments presented here a learning agent would retain all counterexamples submitted by the other agent; however, this is a very simple case retention policy, and we will like to experiment with more informed policies - with the goal that individual learning agents could significantly improve using only a small set of cases proposed by other agents. Finally, our approach is focused on lazy learning, and future works aims at incorporating eager inductive learning inside the argumentative framework for learning from communication. 12. REFERENCES [1] Agnar Aamodt and Enric Plaza. Case-based reasoning: Foundational issues, methodological variations, and system approaches. Artificial Intelligence Communications, 7(1):39-59, 1994. [2] E. Armengol and E. Plaza. Lazy induction of descriptions for relational case-based learning. In ECML2001, pages 13-24, 2001. [3] Gerhard Brewka. Dynamic argument systems: A formal model of argumentation processes based on situation calculus. Journal of Logic and Computation, 11(2):257-282, 2001. [4] Carlos I. Chesñevar and Guillermo R. Simari. Formalizing Defeasible Argumentation using Labelled Deductive Systems. Journal of Computer Science & Technology, 1(4):18-33, 2000. [5] D. Leake and R. Sooriamurthi. Automatically selecting strategies for multi-case-base reasoning. In S. Craw and A. Preece, editors, ECCBR2002, pages 204-219, Berlin, 2002. Springer Verlag. [6] Francisco J. Martín, Enric Plaza, and Josep-Lluis Arcos. Knowledge and experience reuse through communications among competent (peer) agents. International Journal of Software Engineering and Knowledge Engineering, 9(3):319-341, 1999. [7] Lorraine McGinty and Barry Smyth. Collaborative case-based reasoning: Applications in personalized route planning. In I. Watson and Q. Yang, editors, ICCBR, number 2080 in LNAI, pages 362-376. Springer-Verlag, 2001. [8] Santi Ontañón and Enric Plaza. Justification-based multiagent learning. In ICML2003, pages 576-583. Morgan Kaufmann, 2003. [9] Enric Plaza, Eva Armengol, and Santiago Ontañón. The explanatory power of symbolic similarity in case-based reasoning. Artificial Intelligence Review, 24(2):145-161, 2005. [10] David Poole. On the comparison of theories: Preferring the most specific explanation. In IJCAI-85, pages 144-147, 1985. [11] M V Nagendra Prassad, Victor R Lesser, and Susan Lander. Retrieval and reasoning in distributed case bases. Technical report, UMass Computer Science Department, 1995. [12] K. Sycara S. Kraus and A. Evenchik. Reaching agreements through argumentation: a logical model and implementation. Artificial Intelligence Journal, 104:1-69, 1998. [13] N. R. Jennings S. Parsons, C. Sierra. Agents that reason and negotiate by arguing. Journal of Logic and Computation, 8:261-292, 1998. [14] Bruce A. Wooley. Explanation component of software systems. ACM CrossRoads, 5.1, 1998. 982 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)",
    "original_translation": "Aprendizaje y deliberación conjunta a través de la argumentación en sistemas multiagente. Santi Ontañón CCL, Laboratorio de Computación Cognitiva, Instituto de Tecnología de Georgia, Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Instituto de Investigación en Inteligencia Artificial, CSIC, Consejo Superior de Investigaciones Científicas, Campus UAB, 08193 Bellaterra, Cataluña (España) enric@iiia.csic.es RESUMEN En este artículo presentaremos un marco de argumentación para agentes de aprendizaje (AMAL) diseñado con dos propósitos: (1) para deliberación conjunta, y (2) para aprender a través de la comunicación. El marco AMAL se basa completamente en el aprendizaje a partir de ejemplos: la relación de preferencia de argumentos, la política de generación de argumentos y la política de generación de contraargumentos son técnicas basadas en casos. Para unirse en la deliberación, los agentes de aprendizaje comparten su experiencia formando un comité para decidir sobre alguna decisión conjunta. Experimentalmente demostramos que la argumentación entre comités de agentes mejora tanto el rendimiento individual como el conjunto. Para aprender a través de la comunicación, un agente se involucra en discusiones con otros agentes para contrastar sus hipótesis individuales y recibir contraejemplos; el proceso de argumentación mejora su alcance de aprendizaje y rendimiento individual. Categorías y Descriptores de Asignaturas I.2.6 [Inteligencia Artificial]: Aprendizaje; I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente, Agentes Inteligentes 1. Los marcos de argumentación para sistemas multiagente pueden ser utilizados para diferentes propósitos como la deliberación conjunta, la persuasión, la negociación y la resolución de conflictos. En este artículo presentaremos un marco de argumentación para agentes de aprendizaje, y mostraremos que puede ser utilizado para dos propósitos: (1) deliberación conjunta, y (2) aprendizaje a partir de la comunicación. La deliberación conjunta basada en argumentos implica discutir sobre el resultado de una situación particular o la acción apropiada a tomar en una situación específica. Los agentes de aprendizaje son capaces de aprender de la experiencia, en el sentido de que ejemplos pasados (situaciones y sus resultados) se utilizan para predecir el resultado para la situación actual. Sin embargo, dado que la experiencia de los agentes individuales puede ser limitada, el conocimiento individual y la precisión de la predicción también son limitados. Por lo tanto, los agentes de aprendizaje que son capaces de argumentar sus predicciones individuales con otros agentes pueden alcanzar una mayor precisión en las predicciones después de dicho proceso de argumentación. La mayoría de los marcos de argumentación existentes para sistemas multiagente se basan en lógica deductiva u otra formalización de lógica deductiva diseñada específicamente para respaldar la argumentación, como la lógica por defecto [3]. Por lo general, un argumento se ve como una declaración lógica, mientras que un contraargumento es un argumento ofrecido en oposición a otro argumento; los agentes utilizan una relación de preferencia para resolver argumentos conflictivos. Sin embargo, los marcos de argumentación basados en lógica asumen agentes con conocimiento predefinido y relación de preferencia. En este artículo, nos enfocamos en un marco de Aprendizaje Multiagente basado en Argumentación (AMAL) donde tanto el conocimiento como la relación de preferencia se aprenden a partir de la experiencia. Por lo tanto, consideramos un escenario con agentes que (1) trabajan en el mismo dominio utilizando una ontología compartida, (2) son capaces de aprender a partir de ejemplos y (3) se comunican utilizando un marco argumentativo. Tener capacidades de aprendizaje permite a los agentes utilizar de manera efectiva una forma específica de contraargumento, a saber, el uso de contraejemplos. Los contraejemplos ofrecen la posibilidad de que los agentes aprendan durante el proceso de argumentación. Además, los agentes de aprendizaje permiten técnicas que utilizan la experiencia aprendida para generar argumentos y contraargumentos adecuados. Específicamente, necesitaremos abordar dos problemas: (1) cómo definir una técnica para generar argumentos y contraargumentos a partir de ejemplos, y (2) cómo definir una relación de preferencia sobre dos argumentos en conflicto que han sido inducidos a partir de ejemplos. Este documento presenta un enfoque basado en casos para abordar ambas cuestiones. Los agentes utilizan el razonamiento basado en casos (CBR) [1] para aprender de casos pasados (donde un caso es una situación y su resultado) con el fin de predecir el resultado de una nueva situación. Proponemos un protocolo de argumentación dentro del marco de AMAL que apoya a los agentes en llegar a una predicción conjunta sobre una situación o problema específico; además, el razonamiento necesario para respaldar el proceso de argumentación también se basará en casos. En particular, presentamos dos medidas basadas en casos, una para generar los argumentos y contraargumentos adecuados para una situación particular y otra para determinar la relación de preferencia entre los argumentos. Finalmente, evaluamos (1) si la argumentación entre agentes de aprendizaje puede producir una predicción conjunta que mejore el rendimiento de aprendizaje individual y (2) si el aprendizaje a partir de los contraejemplos transmitidos durante el proceso de argumentación aumenta el rendimiento individual con precisamente aquellos casos que se utilizan mientras discuten entre ellos. El documento está estructurado de la siguiente manera. La sección 2 discute la relación entre la argumentación, la colaboración y el aprendizaje. Luego, la Sección 3 introduce nuestro marco de trabajo de CBR multiagente (MAC) y el concepto de predicción justificada. Después de eso, la Sección 4 define formalmente nuestro marco de argumentación. Las secciones 5 y 6 presentan nuestra relación de preferencia basada en casos y las políticas de generación de argumentos, respectivamente. Más adelante, la Sección 7 presenta el protocolo de argumentación en nuestro marco AMAL. Después de eso, la Sección 8 presenta una ejemplificación del marco de argumentación. Finalmente, la Sección 9 presenta una evaluación empírica de nuestras dos hipótesis principales. El artículo concluye con secciones de trabajo relacionado y conclusiones. 2. ARGUMENTACIÓN, COLABORACIÓN Y APRENDIZAJE Tanto el aprendizaje como la colaboración son formas en las que un agente puede mejorar el rendimiento individual. De hecho, existe un claro paralelismo entre el aprendizaje y la colaboración en sistemas multiagentes, ya que ambos son formas en las que los agentes pueden lidiar con sus deficiencias. Mostremos cuáles son las principales motivaciones que un agente puede tener para aprender o colaborar. • Motivaciones para aprender: - Aumentar la calidad de la predicción, - Aumentar la eficiencia, - Ampliar el rango de problemas resolubles. • Motivaciones para colaborar: - Aumentar la calidad de la predicción, - Aumentar la eficiencia, - Ampliar el rango de problemas resolubles, - Ampliar el rango de recursos accesibles. Al observar las listas de motivación anteriores, podemos ver fácilmente que el aprendizaje y la colaboración están muy relacionados en los sistemas multiagente. De hecho, con la excepción del último elemento en la lista de motivaciones para colaborar, son dos extremos de un continuo de estrategias para mejorar el rendimiento. Un agente puede optar por aumentar el rendimiento mediante el aprendizaje, la colaboración o encontrando un punto intermedio que combine el aprendizaje y la colaboración para mejorar el rendimiento. En este artículo propondremos AMAL, un marco de argumentación para agentes de aprendizaje, y también mostraremos cómo AMAL puede ser utilizado tanto para aprender a través de la comunicación como para resolver problemas de manera colaborativa: • Los agentes pueden resolver problemas de manera colaborativa al participar en un proceso de argumentación sobre la predicción para la situación actual. Utilizando esta colaboración, la predicción puede realizarse de una manera más informada, ya que se ha tenido en cuenta la información conocida por varios agentes. Los agentes también pueden aprender de la comunicación con otros agentes al participar en un proceso de argumentación. Los agentes que participan en estos procesos de argumentación pueden aprender de los argumentos y contraejemplos recibidos de otros agentes, y utilizar esta información para predecir los resultados de situaciones futuras. En el resto de este documento propondremos un marco de argumentación y mostraremos cómo puede ser utilizado tanto para el aprendizaje como para la resolución de problemas de manera colaborativa. SISTEMAS CBR MULTIAGENTE Un Sistema de Razonamiento Basado en Casos Multiagente (MAC) M = {(A1, C1), ..., (An, Cn)} es un sistema multiagente compuesto por A = {Ai, ..., An}, un conjunto de agentes CBR, donde cada agente Ai ∈ A posee una base de casos individual Ci. Cada agente individual Ai en un MAC es completamente autónomo y cada agente Ai solo tiene acceso a su base de casos individual y privada Ci. Un conjunto de casos Ci = {c1, ..., cm} es una colección de casos. Los agentes en un sistema MAC pueden resolver problemas de forma individual, pero también pueden colaborar con otros agentes para resolver problemas. En este marco, nos limitaremos a tareas analíticas, es decir, tareas como la clasificación, donde la solución de un problema se logra seleccionando una clase de solución de un conjunto enumerado de clases de solución. En lo siguiente, notaremos el conjunto de todas las clases de soluciones por S = {S1, ..., SK}. Por lo tanto, un caso c = P, S es una tupla que contiene una descripción del caso P y una clase de solución S ∈ S. En lo siguiente, utilizaremos indistintamente los términos problema y descripción del caso. Además, utilizaremos la notación de punto para referirnos a elementos dentro de una tupla; por ejemplo, para referirnos a la clase de solución de un caso c, escribiremos c.S. Por lo tanto, decimos que un grupo de agentes realiza una deliberación conjunta cuando colaboran para encontrar una solución conjunta mediante un proceso de argumentación. Sin embargo, para hacerlo, un agente debe ser capaz de justificar su predicción a los otros agentes (es decir, generar un argumento para su solución predicha que pueda ser examinado y criticado por los otros agentes). La siguiente sección aborda este tema. 3.1 Predicciones Justificadas Tanto los sistemas expertos como los sistemas CBR pueden tener un componente de explicación [14] encargado de justificar por qué el sistema ha proporcionado una respuesta específica al usuario. La línea de razonamiento del sistema puede ser examinada por un experto humano, aumentando así la fiabilidad del sistema. La mayoría de los trabajos existentes sobre generación de explicaciones se centran en generar explicaciones para ser proporcionadas al usuario. Sin embargo, en nuestro enfoque utilizamos explicaciones (o justificaciones) como una herramienta para mejorar la comunicación y coordinación entre agentes. Estamos interesados en las justificaciones ya que pueden ser utilizadas como argumentos. Para ese propósito, nos beneficiaremos de la capacidad de algunos métodos de aprendizaje automático para proporcionar justificaciones. Una justificación construida por un método CBR después de determinar que la solución de un problema particular P fue Sk es una descripción que contiene la información relevante del problema P que el método CBR ha considerado para predecir Sk como la solución de P. En particular, los métodos CBR funcionan recuperando casos similares al problema en cuestión, y luego reutilizando sus soluciones para el problema actual, esperando que dado que el problema y los casos son similares, las soluciones también serán similares. Por lo tanto, si un método CBR ha recuperado un conjunto de casos C1, ..., Cn para resolver un problema particular P, la justificación construida contendrá la información relevante del problema P que hizo que el sistema CBR recuperara ese conjunto particular de casos, es decir, contendrá la información relevante que P y C1, ..., Cn tienen en común. Por ejemplo, la Figura 1 muestra una justificación construida por un sistema CBR para un problema de juguete (en las siguientes secciones mostraremos justificaciones para problemas reales). En la figura, un problema tiene dos atributos (Semáforo y Coches_pasando), el mecanismo de recuperación del sistema CBR nota que al considerar solo el atributo Semáforo, puede recuperar dos casos que predicen la misma solución: esperar. Por lo tanto, dado que solo se ha utilizado este atributo, es el único que aparece en la justificación. Los valores del resto de atributos son irrelevantes, ya que, sea cual sea su valor, la clase de solución habría sido la misma. 976 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Problema Semáforo: rojo Pasan coches: no Caso 1 Semáforo: rojo Pasan coches: no Solución: esperar Caso 3 Semáforo: rojo Pasan coches: sí Solución: esperar Caso 4 Semáforo: verde Pasan coches: sí Solución: esperar Caso 2 Semáforo: verde Pasan coches: no Solución: cruzar Casos recuperados Solución: esperar Justificación Semáforo: rojo Figura 1: Un ejemplo de generación de justificación en un sistema de RCB. Observa que, dado que la única característica relevante para decidir es Traffic_light (la única utilizada para recuperar casos), es la única que aparece en la justificación. En general, el significado de una justificación es que todos (o la mayoría) de los casos en la base de casos de un agente que satisfacen la justificación (es decir, todos los casos que están subsumidos por la justificación) pertenecen a la clase de solución predicha. En el resto del documento, usaremos para denotar la relación de subsumpción. En nuestro trabajo, utilizamos LID [2], un método CBR capaz de construir justificaciones simbólicas como la ejemplificada en la Figura 1. Cuando un agente proporciona una justificación para una predicción, el agente genera una predicción justificada: DEFINICIÓN 3.1. Una Predicción Justificada es una tupla J = A, P, S, D donde el agente A considera que S es la solución correcta para el problema P, y esa predicción está justificada por una descripción simbólica D tal que J.D J.P. Las justificaciones pueden tener muchos usos para los sistemas de RCB [8, 9]. En este documento, vamos a utilizar justificaciones como argumentos, con el fin de permitir que los agentes de aprendizaje participen en procesos de argumentación. 4. ARGUMENTOS Y CONTRAARGUMENTOS Para nuestros propósitos, un argumento α generado por un agente A está compuesto por una afirmación S y alguna evidencia D que respalda a S como correcta. En el resto de esta sección veremos cómo esta definición general de argumento puede ser concretada en un tipo específico de argumentos que los agentes pueden generar. En el contexto de los sistemas MAC, los agentes discuten sobre predicciones para nuevos problemas y pueden proporcionar dos tipos de información: a) casos específicos P, S, y b) predicciones justificadas: A, P, S, D. Usando esta información, podemos definir tres tipos de argumentos: predicciones justificadas, contraargumentos y contraejemplos. Una predicción justificada α es generada por un agente Ai para argumentar que Ai cree que la solución correcta para un problema dado P es α.S, y la evidencia proporcionada es la justificación α.D. En el ejemplo representado en la Figura 1, un agente Ai puede generar el argumento α = Ai, P, Esperar, (Semáforo = rojo), lo que significa que el agente Ai cree que la solución correcta para P es Esperar porque el atributo Semáforo es igual a rojo. Un contraargumento β es un argumento ofrecido en oposición a otro argumento α. En nuestro marco de trabajo, un contraargumento consiste en una predicción justificada Aj, P, S, D generada por un agente Aj con la intención de refutar un argumento α generado por otro agente Ai, que respalda una clase de solución S diferente a la de α.S para el problema en cuestión y justifica esto con una justificación D. En el ejemplo de la Figura 1, si un agente genera el argumento α = Ai, P, Caminar, (Pasando_autos = no), un agente que piensa que la solución correcta es Esperar podría responder con el contraargumento β = Aj, P, Esperar, (Pasando_autos = no ∧ Semáforo = rojo), lo que significa que, aunque no hay autos pasando, el semáforo está en rojo y la calle no se puede cruzar. Un contraejemplo c es un caso que contradice un argumento α. Por lo tanto, un contraejemplo también es un contraargumento, uno que afirma que un argumento específico α no siempre es verdadero, y la evidencia proporcionada es el caso c. Específicamente, para que un caso c sea un contraejemplo de un argumento α, se deben cumplir las siguientes condiciones: α.D c y α.S = c.S, es decir, el caso debe satisfacer la justificación α.D y la solución de c debe ser diferente a la predicha por α. Al intercambiar argumentos y contraargumentos (incluyendo contraejemplos), los agentes pueden discutir sobre la solución correcta de un problema dado, es decir, pueden participar en un proceso de deliberación conjunto. Sin embargo, para hacerlo, necesitan un protocolo de interacción específico, una relación de preferencia entre argumentos contradictorios y una política de decisión para generar contraargumentos (incluidos contraejemplos). En las siguientes secciones presentaremos estos elementos. 5. RELACIÓN DE PREFERENCIA Un argumento específico proporcionado por un agente podría no ser consistente con la información conocida por otros agentes (o incluso con parte de la información conocida por el agente que ha generado la justificación debido al ruido en los datos de entrenamiento). Por esa razón, vamos a definir una relación de preferencia sobre predicciones justificadas contradictorias basadas en casos. Básicamente, definiremos una medida de confianza para cada predicción justificada (que tenga en cuenta los casos poseídos por cada agente), y la predicción justificada con la mayor confianza será la preferida. La idea detrás de la confianza basada en casos es contar cuántos de los casos en una base de casos individual respaldan una predicción justificada, y cuántos de ellos son ejemplos en contra de ella. Cuanto más casos de respaldo, mayor es la confianza; y cuanto más contrajemplos, menor es la confianza. Específicamente, para evaluar la confianza de una predicción justificada α, un agente obtiene el conjunto de casos en su base de casos individuales que están subsumidos por α.D. Con ellos, un agente Ai obtiene los valores Y (sí) y N (no): • Y Ai α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| es el número de casos en la base de casos del agente subsumidos por la justificación α.D que pertenecen a la clase de solución α.S, • NAi α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| es el número de casos en la base de casos del agente subsumidos por la justificación α.D que no pertenecen a esa clase de solución. El Sexto Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 977 + + + + + +  - - + Figura 2: La confianza de los argumentos se evalúa contrastándolos con las bases de casos de los agentes. Un agente estima la confianza de un argumento como: CAi (α) = Y Ai α 1 + Y Ai α + NAi α es decir, la confianza en una predicción justificada es el número de casos que la respaldan dividido por el número de casos que la respaldan más los contraejemplos. Observa que sumamos 1 al denominador, esto es para evitar otorgar confianzas excesivamente altas a predicciones justificadas cuya confianza se ha calculado utilizando un número pequeño de casos. Ten en cuenta que esta corrección sigue la misma idea que la corrección de Laplace para estimar probabilidades. La Figura 2 ilustra la evaluación individual de la confianza de un argumento, en particular, se encuentran tres casos de respaldo y un contraejemplo en la base de casos de los agentes Ai, lo que da una confianza estimada de 0.6. Además, también podemos definir la confianza conjunta de un argumento α como la confianza calculada utilizando los casos presentes en las bases de casos de todos los agentes en el grupo: C(α) = Π Ai α / (1 + Σ Ai α + NAi α). Nótese que, para calcular conjuntamente la confianza conjunta, los agentes solo tienen que hacer públicos los valores de aprobación y rechazo calculados localmente para un argumento dado. En nuestro marco de trabajo, los agentes utilizan esta confianza conjunta como la relación de preferencia: una predicción justificada α se prefiere sobre otra β si C(α) ≥ C(β). GENERACIÓN DE ARGUMENTOS En nuestro marco de trabajo, los argumentos son generados por los agentes a partir de casos, utilizando métodos de aprendizaje. Cualquier método de aprendizaje capaz de proporcionar una predicción justificada puede ser utilizado para generar argumentos. Por ejemplo, los árboles de decisión y LID [2] son métodos de aprendizaje adecuados. Específicamente, en los experimentos reportados en este artículo, los agentes utilizan LID. Por lo tanto, cuando un agente desea generar un argumento que respalde que una clase de solución específica es la solución correcta para un problema P, genera una predicción justificada como se explica en la Sección 3.1. Por ejemplo, la Figura 3 muestra una justificación real generada por LID después de resolver un problema P en el dominio de la identificación de esponjas marinas. En particular, la Figura 3 muestra cómo cuando un agente recibe un nuevo problema para resolver (en este caso, un nuevo esponja para determinar su orden), el agente utiliza LID para generar un argumento (que consiste en una predicción justificada) utilizando los casos en la base de casos del agente. La justificación mostrada en la Figura 3 se puede interpretar diciendo que la solución predicha es hadromerida porque la forma lisa de las megascleras del esqueleto espicular de la esponja es de tipo tilostilo, el esqueleto espicular de la esponja no tiene una longitud uniforme y no hay gemas en las características externas de la esponja. Por lo tanto, el argumento generado será α = A1, P, hadromerida, D1. 6.1 Generación de Contraargumentos Como se mencionó anteriormente, los agentes pueden intentar refutar argumentos generando contraargumentos o encontrando contraejemplos. Permítanos explicar cómo pueden ser generados. Un agente Ai quiere generar un contraargumento β para rebatir un argumento α cuando α está en contradicción con la base de casos locales de Ai. Además, al generar dicho contraargumento β, Ai espera que β sea preferido sobre α. Con ese propósito, presentaremos una política específica para generar contraargumentos basados en el criterio de especificidad [10]. El criterio de especificidad se utiliza ampliamente en marcos deductivos para la argumentación, y establece que entre dos argumentos en conflicto, se debe preferir el más específico ya que, en principio, está más informado. Por lo tanto, se espera que los contraargumentos generados basados en el criterio de especificidad sean preferibles (ya que están más informados) que los argumentos que intentan refutar. Sin embargo, no hay garantía de que tales contraargumentos siempre ganen, ya que, como hemos indicado en la Sección 5, los agentes en nuestro marco utilizan una relación de preferencia basada en la confianza conjunta. Además, uno podría pensar que sería mejor que los agentes generaran contraargumentos basados en la relación de preferencia de confianza conjunta; sin embargo, no es obvio cómo generar contraargumentos basados en la confianza conjunta de manera eficiente, ya que se requiere colaboración para evaluar la confianza conjunta. Por lo tanto, el agente que genera el contraargumento debe comunicarse constantemente con los otros agentes en cada paso del algoritmo de inducción utilizado para generar contraargumentos (actualmente una de nuestras líneas de investigación futuras). Por lo tanto, en nuestro marco de trabajo, cuando un agente quiere generar un contraargumento β a un argumento α, β tiene que ser más específico que α (es decir, α.D < β.D). La generación de contraargumentos utilizando el criterio de especificidad impone algunas restricciones sobre el método de aprendizaje, aunque LID o ID3 pueden adaptarse fácilmente para esta tarea. Por ejemplo, LID es un algoritmo que genera una descripción desde cero y añade heurísticamente características a ese término. Por lo tanto, en cada paso, la descripción se vuelve más específica que en el paso anterior, y el número de casos que se incluyen en esa descripción se reduce. Cuando la descripción cubre solo (o casi solo) casos de una única clase de solución, LID termina y predice esa clase de solución. Para generar un contraargumento a un argumento α, LID solo tiene que usar como punto de partida la descripción α.D en lugar de empezar desde cero. De esta manera, la justificación proporcionada por LID siempre estará subsumida por α.D, y por lo tanto, el contraargumento resultante será más específico que α. Sin embargo, hay que tener en cuenta que LID a veces puede no ser capaz de generar contraargumentos, ya que LID puede no ser capaz de especializar aún más la descripción α.D, o porque el agente Ai no tiene ningún caso en Ci que esté subsumido por α.D. La Figura 4 muestra cómo un agente A2 que no estaba de acuerdo con el argumento mostrado en la Figura 3, genera un contraargumento utilizando LID. Además, la Figura 4 muestra la generación de un contraargumento β1 2 para el argumento α0 1 (en la Figura 3) que es una especialización de α0 1. 978 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Solución: hadromerida Justificación: D1 Esponja Esqueleto espiculado Características externas Características externas Gemmulas: no Esqueleto espiculado Megascleres Longitud uniforme: no Megascleres Forma lisa: tilostilo Caso Base de A1 LID Nueva esponja P Figura 3: Ejemplo de una justificación real generada por LID en el conjunto de datos de esponjas marinas. Específicamente, en nuestros experimentos, cuando un agente Ai quiere refutar un argumento α, utiliza la siguiente política: 1. El Agente Ai utiliza LID para intentar encontrar un contraargumento β más específico que α; si se encuentra, β se envía al otro agente como un contraargumento de α. 2. Si no se encuentra, entonces Ai busca un contraejemplo c ∈ Ci de α. Si se encuentra un caso c, entonces c se envía al otro agente como un contraejemplo de α. Si no se encuentran contraejemplos, entonces Ai no puede refutar el argumento α. 7. APRENDIZAJE MULTIAGENTE BASADO EN ARGUMENTACIÓN El protocolo de interacción de AMAL permite que un grupo de agentes A1, ..., An deliberen sobre la solución correcta de un problema P mediante un proceso de argumentación. Si el proceso de argumentación llega a una solución consensuada, la deliberación conjunta termina; de lo contrario, se utiliza una votación ponderada para determinar la solución conjunta. Además, AMAL también permite a los agentes aprender de los contraejemplos recibidos de otros agentes. El protocolo AMAL consiste en una serie de rondas. En la ronda inicial, cada agente declara cuál es su predicción individual para P. Luego, en cada ronda, un agente puede intentar refutar la predicción hecha por cualquiera de los otros agentes. El protocolo utiliza un mecanismo de paso de token para que los agentes (uno a la vez) puedan enviar contraargumentos o contraejemplos si no están de acuerdo con la predicción hecha por cualquier otro agente. Específicamente, a cada agente se le permite enviar un contraargumento o contraejemplo cada vez que recibe el token (nota que esta restricción es solo para simplificar el protocolo, y que no restringe la cantidad de contraargumentos que un agente puede enviar, ya que pueden ser retrasados para rondas posteriores). Cuando un agente recibe un contraargumento o contraejemplo, informa a los otros agentes si acepta el contraargumento (y cambia su predicción) o no. Además, los agentes también tienen la oportunidad de responder a los contraargumentos cuando reciben el token, intentando generar un contraargumento al contraargumento. Cuando todos los agentes hayan tenido el token una vez, el token regresa al primer agente, y así sucesivamente. Si en algún momento del protocolo, todos los agentes están de acuerdo o durante las últimas n rondas ningún agente ha generado ninguna contra-argumentación, el protocolo termina. Además, si al final de la argumentación los agentes no han llegado a un acuerdo, entonces se utiliza un mecanismo de votación que utiliza la confianza de cada predicción como pesos para decidir la solución final (Así, AMAL sigue el mismo mecanismo que los comités humanos, primero cada miembro individual de un comité expone sus argumentos y discute los de los otros miembros (deliberación conjunta), y si no se alcanza un consenso, entonces se requiere un mecanismo de votación). En cada iteración, los agentes pueden usar los siguientes performativos: • afirmar(α): la predicción justificada que se mantendrá durante la próxima ronda será α. Un agente solo puede tener una única predicción en cada ronda, por lo tanto, si se envían múltiples afirmaciones, solo se considera la última como la predicción actualmente mantenida. • rebut(β, α): el agente ha encontrado un contraargumento β a la predicción α. Definiremos Ht = αt 1, ..., αt n como las predicciones que cada uno de los n agentes tiene en una ronda t. Además, también definiremos contradict(αt i) = {α ∈ Ht|α.S = αt i.S} como el conjunto de argumentos contradictorios para un agente Ai en una ronda t, es decir, el conjunto de argumentos en la ronda t que respaldan una clase de solución diferente a la de αt i. El protocolo se inicia porque uno de los agentes recibe un problema P que debe ser resuelto. Después de eso, el agente informa a todos los demás agentes sobre el problema P a resolver, y el protocolo comienza: 1. En la ronda t = 0, cada uno de los agentes resuelve individualmente P y construye una predicción justificada utilizando su propio método de RBC. Entonces, cada agente Ai envía el assertivo performativo assert(α0 i) a los otros agentes. Por lo tanto, los agentes conocen H0 = α0 i , ..., α0 n. Una vez que todas las predicciones hayan sido enviadas, el token se le da al primer agente A1. 2. En cada ronda t (distinta de 0), los agentes verifican si sus argumentos en Ht coinciden. Si lo hacen, el protocolo avanza al paso 5. Además, si durante las últimas n rondas ningún agente ha enviado ningún contraejemplo o contraargumento, el protocolo también avanza al paso 5. De lo contrario, el agente Ai propietario del token intenta generar un contraargumento para cada uno de los argumentos opuestos en contradict(αt i) ⊆ Ht (ver Sección 6.1). Entonces, se selecciona el contraargumento βt i contra la predicción αt j con la menor confianza C(αt j) (ya que αt j es la predicción más probable de ser refutada con éxito). • Si βt i es un contraargumento, entonces, Ai compara localmente αt i con βt i evaluando su confianza contra su base de casos individuales Ci (ver Sección 5) (nótese que Ai está comparando su argumento previo con el contraargumento que Ai mismo acaba de generar y que trata sobre The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 979 Esponja Esqueleto espiculado Características externas Características externas Gemas: no Crecimiento: Esqueleto espiculado Megascleres Longitud uniforme: no Megascleres Forma lisa: tilostilo Crecimiento Crecimiento: masivo Caso Base de A2 LID Solución: astrophorida Justificación: D2 Figura 4: Generación de un contraargumento utilizando LID en el conjunto de datos de esponjas. para enviar a Aj. Si CAi (βt i) > CAi (αt i), entonces Ai considera que βt i es más fuerte que su argumento anterior, cambia su argumento a βt i enviando assert(βt i) al resto de los agentes (la intuición detrás de esto es que dado que un contraargumento también es un argumento, Ai verifica si el nuevo contraargumento es un mejor argumento que el que sostenía anteriormente) y rebut(βt i, αt j) a Aj. De lo contrario (es decir, Si CAi (βt i ) ≤ CAi (αt i), Ai solo enviará rebut(βt i , αt j) a Aj. En cualquiera de las dos situaciones, el protocolo avanza al paso 3. • Si βt i es un contraejemplo c, entonces Ai envía una refutación(c, αt j) a Aj. El protocolo avanza al paso 4. • Si Ai no puede generar ningún contraargumento o contraejemplo, el token se envía al siguiente agente, comienza una nueva ronda t + 1, y el protocolo avanza al estado 2. 3. El agente Aj que ha recibido el contraargumento βt i, lo compara localmente con su propio argumento, αt j, evaluando localmente su confianza. Si CAj (βt i ) > CAj (αt j), entonces Aj aceptará el contraargumento como más fuerte que su propio argumento, y enviará assert(βt i ) a los otros agentes. De lo contrario (es decir, Si CAj (βt i ) ≤ CAj (αt j)), Aj no aceptará el contraargumento e informará a los otros agentes en consecuencia. Cualquiera de las dos situaciones inicia una nueva ronda t + 1, Ai envía el token al siguiente agente y el protocolo vuelve al estado 2. 4. El agente Aj que ha recibido el contraejemplo c lo retiene en su base de casos y genera un nuevo argumento αt+1 j que tiene en cuenta c, e informa al resto de los agentes enviando assert(αt+1 j) a todos ellos. Entonces, Ai envía el token al siguiente agente, comienza una nueva ronda t + 1 y el protocolo vuelve al paso 2. 5. El protocolo finaliza generando una predicción conjunta, de la siguiente manera: si los argumentos en Ht están de acuerdo, entonces su predicción es la predicción conjunta; de lo contrario, se utiliza un mecanismo de votación para decidir la predicción conjunta. El mecanismo de votación utiliza la medida de confianza conjunta como los pesos de votación, de la siguiente manera: S = arg max Sk∈S αi∈Ht|αi.S=Sk C(αi) Además, para evitar iteraciones infinitas, si un agente envía dos veces el mismo argumento o contraargumento al mismo agente, el mensaje no se considera. 8. EJEMPLIFICACIÓN Consideremos un sistema compuesto por tres agentes A1, A2 y A3. Uno de los agentes, A1, recibe un problema P para resolver y decide usar AMAL para resolverlo. Por esa razón, invita a A2 y A3 a participar en el proceso de argumentación. Aceptan la invitación y comienza el protocolo de argumentación. Inicialmente, cada agente genera su predicción individual para P y la transmite a los otros agentes. Por lo tanto, todos ellos pueden calcular H0 = α0 1, α0 2, α0 3. En particular, en este ejemplo: • α0 1 = A1, P, hadromerida, D1 • α0 2 = A2, P, astrophorida, D2 • α0 3 = A3, P, axinellida, D3 A1 comienza poseyendo el token e intenta generar argumentos en contra de α0 2 y α0 3, pero no tiene éxito; sin embargo, tiene un contraejemplo c13 para α0 3. Por lo tanto, A1 envía el mensaje rebut(c13, α03) a A3. A3 incorpora c13 en su base de casos e intenta resolver el problema P nuevamente, ahora teniendo en cuenta c13. A3 elabora la predicción justificada α1 3 = A3, P, hadromerida, D4, y la transmite al resto de los agentes con el mensaje assert(α1 3). Por lo tanto, todos ellos conocen el nuevo H1 = α0 1, α0 2, α1 3. La ronda 1 comienza y A2 obtiene el token. A2 intenta generar contraargumentos para α0 1 y α1 3 y solo logra generar un contraargumento β1 2 = A2, P, astrophorida, D5 contra α1 3. El contraargumento se envía a A3 con el mensaje rebut(β1 2 , α1 3). El agente A3 recibe el contraargumento y evalúa su confianza local. El resultado es que la confianza individual del contraargumento β1 2 es menor que la confianza local de α1 3. Por lo tanto, A3 no acepta el contraargumento, y así H2 = α0 1, α0 2, α1 3. La ronda 2 comienza y A3 recibe el token. A3 genera un contraargumento β2 3 = A3, P, hadromerida, D6 para α0 2 y lo envía a A2 con el mensaje refutar(β2 3 , α0 2). El agente A2 recibe el contraargumento y evalúa su confianza local. El resultado es que la confianza local del contraargumento β2 3 es mayor que la confianza local de α0 2. Por lo tanto, A2 acepta el contraargumento e informa al resto de los agentes con el mensaje assert(β2 3). Después de eso, H3 = α0 1, β2 3 , α1 3. En la Ronda 3, dado que todos los agentes están de acuerdo (todas las predicciones justificadas en H3 predicen hadromerida como la clase de solución), el protocolo finaliza, y A1 (el agente que recibió el problema) considera hadromerida como la solución conjunta para el problema P. 9. EVALUACIÓN EXPERIMENTAL 980 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) SPONGE 75 77 79 81 83 85 87 89 91 2 3 4 5 AMAL Votación Individual SOYBEAN 55 60 65 70 75 80 85 90 2 3 4 5 AMAL Votación Individual Figura 5: Precisión individual y conjunta para 2 a 5 agentes. En esta sección evaluamos empíricamente el marco de argumentación AMAL. Hemos realizado experimentos en dos conjuntos de datos diferentes: soja (del repositorio de aprendizaje automático de UCI) y esponja (un conjunto de datos relacional). El conjunto de datos de soja tiene 307 ejemplos y 19 clases de solución, mientras que el conjunto de datos de esponja tiene 280 ejemplos y 3 clases de solución. En una ejecución experimental, el conjunto de datos se divide en 2 conjuntos: el conjunto de entrenamiento y el conjunto de prueba. Los ejemplos del conjunto de entrenamiento están distribuidos entre 5 agentes diferentes sin replicación, es decir, no hay ningún ejemplo compartido por dos agentes. En la etapa de pruebas, los problemas en el conjunto de pruebas llegan aleatoriamente a uno de los agentes, y su objetivo es predecir la solución correcta. Los experimentos están diseñados para probar dos hipótesis: (H1) que la argumentación es un marco útil para la deliberación conjunta y puede mejorar sobre otros métodos típicos como la votación; y (H2) que el aprendizaje a través de la comunicación mejora el rendimiento individual de un agente de aprendizaje que participa en un proceso de argumentación. Además, también esperamos que la mejora lograda a través de la argumentación aumente a medida que aumente el número de agentes que participan en la argumentación (ya que se tomará en cuenta más información). En relación con H1 (la argumentación es un marco útil para la deliberación conjunta), realizamos 4 experimentos, utilizando 2, 3, 4 y 5 agentes respectivamente (en todos los experimentos cada agente tiene un 20% de los datos de entrenamiento, ya que el entrenamiento siempre se distribuye entre 5 agentes). La Figura 5 muestra el resultado de esos experimentos en los conjuntos de datos de esponjas y soja. La precisión de la clasificación se representa en el eje vertical, y en el eje horizontal se muestra el número de agentes que participaron en los procesos de argumentación. Para cada número de agentes, se muestran tres barras: individual, Voting y AMAL. La barra individual muestra la precisión promedio de las predicciones de los agentes individuales; la barra de votación muestra la precisión promedio de la predicción conjunta lograda mediante votación pero sin argumentación; y finalmente la barra de AMAL muestra la precisión promedio de la predicción conjunta utilizando argumentación. Los resultados mostrados son el promedio de 5 ejecuciones de validación cruzada de 10 pliegues. La Figura 5 muestra que la colaboración (votación y AMAL) supera el rendimiento de la resolución individual de problemas. Además, como esperábamos, la precisión mejora a medida que más agentes colaboran, ya que se tiene en cuenta más información. También podemos ver que AMAL siempre supera al voto estándar, demostrando que las decisiones conjuntas se basan en una mejor información proporcionada por el proceso de argumentación. Por ejemplo, la precisión conjunta para 2 agentes en el conjunto de datos de esponjas es del 87.57% para AMAL y del 86.57% para la votación (mientras que la precisión individual es solo del 80.07%). Además, la mejora lograda por AMAL sobre Voting es aún mayor en el conjunto de datos de soja. La razón es que el conjunto de datos de soja es más difícil (en el sentido de que los agentes necesitan más datos para producir buenas predicciones). Estos resultados experimentales muestran que AMAL aprovecha de manera efectiva la oportunidad de mejora: la precisión es mayor solo porque más agentes han cambiado su opinión durante la argumentación (de lo contrario, lograrían el mismo resultado que la Votación). En relación con H2 (el aprendizaje a partir de la comunicación en procesos de argumentación mejora la predicción individual), llevamos a cabo el siguiente experimento: inicialmente, distribuimos un 25% del conjunto de entrenamiento entre los cinco agentes; después, el resto de los casos en el conjunto de entrenamiento se envía a los agentes uno por uno; cuando un agente recibe un nuevo caso de entrenamiento, tiene varias opciones: el agente puede descartarlo, el agente puede retenerlo, o el agente puede usarlo para participar en un proceso de argumentación. La Figura 6 muestra el resultado de ese experimento para los dos conjuntos de datos. La Figura 6 contiene tres gráficos, donde NL (no aprendizaje) muestra la precisión de un agente sin aprendizaje alguno; L (aprendizaje) muestra la evolución de la precisión de clasificación individual cuando los agentes aprenden reteniendo los casos de entrenamiento que reciben individualmente (notar que cuando todos los casos de entrenamiento han sido retenidos al 100%, la precisión debería ser igual a la de la Figura 5 para los agentes individuales); y finalmente LFC (aprendizaje a partir de la comunicación) muestra la evolución de la precisión de clasificación individual de los agentes de aprendizaje que también aprenden reteniendo los contraejemplos recibidos durante la argumentación (es decir, aprenden tanto de ejemplos de entrenamiento como de contraejemplos). La Figura 6 muestra que si un agente Ai también aprende de la comunicación, Ai puede mejorar significativamente su rendimiento individual con solo un pequeño número de casos adicionales (seleccionados como contraejemplos relevantes para Ai durante la argumentación). Por ejemplo, en el conjunto de datos de soja, los agentes individuales han logrado una precisión del 70.62% cuando también aprenden de la comunicación, en comparación con una precisión del 59.93% cuando solo aprenden de su experiencia individual. El número de casos aprendidos a través de la comunicación depende de las propiedades del conjunto de datos: en el conjunto de datos de esponjas, los agentes solo han retenido muy pocos casos adicionales y han mejorado significativamente la precisión individual; es decir, retienen en promedio 59.96 casos (en comparación con los 50.4 casos retenidos si no aprenden de la comunicación). En el conjunto de datos de soja se aprenden más contraejemplos para mejorar significativamente la precisión individual, es decir, retienen en promedio 87.16 casos (en comparación con 55.27 casos retenidos si no aprenden de la comunicación). Finalmente, el hecho de que ambos conjuntos de datos muestren una mejora significativa señala la naturaleza adaptativa del enfoque basado en la argumentación para el aprendizaje a partir de la comunicación: los casos útiles se seleccionan como contraejemplos (y no más de los necesarios), y tienen el efecto deseado. 10. TRABAJO RELACIONADO En cuanto al RCB en un entorno multiagente, la primera investigación se centró en la recuperación de casos negociada [11] entre grupos de agentes. Nuestro trabajo sobre el aprendizaje basado en casos multiagente comenzó en 1999; más tarde Mc Ginty y Smyth presentaron un enfoque colaborativo de CBR multiagente (CCBR) para la planificación. Finalmente, otro enfoque interesante es el razonamiento de múltiples bases de casos (MCBR) [5], que trata sobre The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 981 ESPONJA 60 65 70 75 80 85 25% 40% 55% 70% 85% 100% LFC L NL SOJA 20 30 40 50 60 70 80 90 25% 40% 55% 70% 85% 100% LFC L NL Figura 6: Aprendizaje a partir de la comunicación resultante de la argumentación en un sistema compuesto por 5 agentes. sistemas distribuidos donde hay varias bases de casos disponibles para la misma tarea y aborda los problemas de adaptación entre bases de casos. La principal diferencia es que nuestro enfoque MAC es una forma de distribuir el proceso de Reutilización de CBR (utilizando un sistema de votación) mientras que la Recuperación se realiza individualmente por cada agente; sin embargo, los otros enfoques de CBR multiagente se centran en distribuir el proceso de Recuperación. La investigación sobre la argumentación en MAS se centra en varios temas como a) lógicas, protocolos y lenguajes que respaldan la argumentación, b) selección de argumentos y c) interpretación de argumentos. Los enfoques para la lógica y los lenguajes que respaldan la argumentación incluyen la lógica no definitiva [4] y los modelos BDI [13]. Aunque la selección de argumentos es un aspecto clave de la argumentación automatizada (ver [12] y [13]), la mayoría de las investigaciones se han centrado en las relaciones de preferencia entre argumentos. En nuestro marco de trabajo hemos abordado tanto la selección de argumentos como las relaciones de preferencia utilizando un enfoque basado en casos. 11. CONCLUSIONES Y TRABAJO FUTURO En este artículo hemos presentado un marco basado en argumentación para el aprendizaje multiagente. Específicamente, hemos presentado AMAL, un marco que permite a un grupo de agentes de aprendizaje discutir sobre la solución de un problema dado y hemos demostrado cómo las capacidades de aprendizaje pueden ser utilizadas para generar argumentos y contraargumentos. La evaluación experimental muestra que la cantidad aumentada de información proporcionada a los agentes por el proceso de argumentación aumenta su precisión predictiva, especialmente cuando un número adecuado de agentes participa en la argumentación. Las principales contribuciones de este trabajo son: a) un marco de argumentación para agentes de aprendizaje; b) una relación de preferencia basada en casos sobre argumentos, basada en el cálculo de una estimación general de confianza de los argumentos; c) una política basada en casos para generar contraargumentos y seleccionar contraejemplos; y d) un enfoque basado en la argumentación para aprender a partir de la comunicación. Finalmente, en los experimentos presentados aquí, un agente de aprendizaje retendría todos los contraejemplos presentados por el otro agente; sin embargo, esta es una política de retención de casos muy simple, y nos gustaría experimentar con políticas más informadas, con el objetivo de que los agentes de aprendizaje individuales puedan mejorar significativamente utilizando solo un pequeño conjunto de casos propuestos por otros agentes. Finalmente, nuestro enfoque se centra en el aprendizaje perezoso, y los trabajos futuros tienen como objetivo incorporar el aprendizaje inductivo ansioso dentro del marco argumentativo para el aprendizaje a partir de la comunicación. 12. REFERENCIAS [1] Agnar Aamodt y Enric Plaza. Razonamiento basado en casos: Aspectos fundamentales, variaciones metodológicas y enfoques de sistemas. Comunicaciones de Inteligencia Artificial, 7(1):39-59, 1994. [2] E. Armengol y E. Plaza. Inducción perezosa de descripciones para el aprendizaje basado en casos relacionales. En ECML2001, páginas 13-24, 2001. [3] Gerhard Brewka. Sistemas de argumentación dinámica: Un modelo formal de procesos de argumentación basado en el cálculo de situaciones. Revista de Lógica y Computación, 11(2):257-282, 2001. [4] Carlos I. Chesñevar y Guillermo R. Simari. Formalizando la argumentación no definitiva utilizando sistemas deductivos etiquetados. Revista de Ciencias de la Computación y Tecnología, 1(4):18-33, 2000. [5] D. Leake y R. Sooriamurthi. Seleccionando estrategias automáticamente para el razonamiento multi-base de casos. En S. Craw y A. Preece, editores, ECCBR2002, páginas 204-219, Berlín, 2002. Springer Verlag. [6] Francisco J. Martín, Enric Plaza y Josep-Lluis Arcos. Reutilización de conocimiento y experiencia a través de comunicaciones entre agentes competentes (pares). Revista Internacional de Ingeniería de Software e Ingeniería del Conocimiento, 9(3):319-341, 1999. [7] Lorraine McGinty y Barry Smyth. Razonamiento basado en casos colaborativo: Aplicaciones en la planificación de rutas personalizadas. En I. Watson y Q. Yang, editores, ICCBR, número 2080 en LNAI, páginas 362-376. Springer-Verlag, 2001. [8] Santi Ontañón y Enric Plaza. Aprendizaje multiagente basado en justificaciones. En ICML2003, páginas 576-583. Morgan Kaufmann, 2003. [9] Enric Plaza, Eva Armengol y Santiago Ontañón. El poder explicativo de la similitud simbólica en el razonamiento basado en casos. Revisión de Inteligencia Artificial, 24(2):145-161, 2005. [10] David Poole. En la comparación de teorías: prefiriendo la explicación más específica. En IJCAI-85, páginas 144-147, 1985. [11] M V Nagendra Prassad, Victor R Lesser y Susan Lander. Recuperación y razonamiento en bases de casos distribuidas. Informe técnico, Departamento de Ciencias de la Computación de UMass, 1995. [12] K. Sycara S. Kraus y A. Evenchik. Alcanzando acuerdos a través de la argumentación: un modelo lógico y su implementación. Revista de Inteligencia Artificial, 104:1-69, 1998. [13] N. R. Jennings, S. Parsons, C. Sierra. Agentes que razonan y negocian mediante argumentos. Revista de Lógica y Computación, 8:261-292, 1998. [14] Bruce A. Wooley. Componente de explicación de sistemas de software. ACM CrossRoads, 5.1, 1998. 982 The Sixth Intl. \n\nACM CrossRoads, 5.1, 1998. 982 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07)",
    "original_sentences": [
        "Learning and Joint Deliberation through Argumentation in Multi-Agent Systems Santi Ontañón CCL, Cognitive Computing Lab Georgia Institute of Technology Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Artificial Intelligence Research Institute CSIC, Spanish Council for Scientific Research Campus UAB, 08193 Bellaterra, Catalonia (Spain) enric@iiia.csic.es ABSTRACT In this paper we will present an argumentation framework for learning agents (AMAL) designed for two purposes: (1) for joint deliberation, and (2) for learning from communication.",
        "The AMAL framework is completely based on learning from examples: the argument preference relation, the argument generation policy, and the counterargument generation policy are case-based techniques.",
        "For join deliberation, learning agents share their experience by forming a committee to decide upon some joint decision.",
        "We experimentally show that the argumentation among committees of agents improves both the individual and joint performance.",
        "For learning from communication, an agent engages into arguing with other agents in order to contrast its individual hypotheses and receive counterexamples; the argumentation process improves their learning scope and individual performance.",
        "Categories and Subject Descriptors I.2.6 [Artificial Intelligence]: Learning; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems, Intelligent Agents 1.",
        "INTRODUCTION Argumentation frameworks for multi-agent systems can be used for different purposes like joint deliberation, persuasion, negotiation, and conflict resolution.",
        "In this paper we will present an argumentation framework for learning agents, and show that it can be used for two purposes: (1) joint deliberation, and (2) learning from communication.",
        "Argumentation-based joint deliberation involves discussion over the outcome of a particular situation or the appropriate course of action for a particular situation.",
        "Learning agents are capable of learning from experience, in the sense that past examples (situations and their outcomes) are used to predict the outcome for the situation at hand.",
        "However, since individual agents experience may be limited, individual knowledge and prediction accuracy is also limited.",
        "Thus, learning agents that are capable of arguing their individual predictions with other agents may reach better prediction accuracy after such an argumentation process.",
        "Most existing argumentation frameworks for multi-agent systems are based on deductive logic or some other deductive logic formalism specifically designed to support argumentation, such as default logic [3]).",
        "Usually, an argument is seen as a logical statement, while a counterargument is an argument offered in opposition to another argument [4, 13]; agents use a preference relation to resolve conflicting arguments.",
        "However, logic-based argumentation frameworks assume agents with preloaded knowledge and preference relation.",
        "In this paper, we focus on an Argumentation-based Multi-Agent Learning (AMAL) framework where both knowledge and preference relation are learned from experience.",
        "Thus, we consider a scenario with agents that (1) work in the same domain using a shared ontology, (2) are capable of learning from examples, and (3) communicate using an argumentative framework.",
        "Having learning capabilities allows agents effectively use a specific form of counterargument, namely the use of counterexamples.",
        "Counterexamples offer the possibility of agents learning during the argumentation process.",
        "Moreover, learning agents allow techniques that use learnt experience to generate adequate arguments and counterarguments.",
        "Specifically, we will need to address two issues: (1) how to define a technique to generate arguments and counterarguments from examples, and (2)how to define a preference relation over two conflicting arguments that have been induced from examples.",
        "This paper presents a case-based approach to address both issues.",
        "The agents use case-based reasoning (CBR) [1] to learn from past cases (where a case is a situation and its outcome) in order to predict the outcome of a new situation.",
        "We propose an argumentation protocol inside the AMAL framework at supports agents in reaching a joint prediction over a specific situation or problem - moreover, the reasoning needed to support the argumentation process will also be based on cases.",
        "In particular, we present two case-based measures, one for generating the arguments and counterarguments adequate to a particular situation and another for determining preference relation among arguments.",
        "Finally, we evaluate (1) if argumentation between learning agents can produce a joint prediction that improves over individual learning performance and (2) if learning from the counterexamples conveyed during the argumentation process increases the individual performance with precisely those cases being used while arguing among them.",
        "The paper is structured as follows.",
        "Section 2 discusses the relation among argumentation, collaboration and learning.",
        "Then Section 3 introduces our multi-agent CBR (MAC) framework and the notion of justified prediction.",
        "After that, Section 4 formally defines our argumentation framework.",
        "Sections 5 and 6 present our case-based preference relation and argument generation policies respectively.",
        "Later, Section 7 presents the argumentation protocol in our AMAL framework.",
        "After that, Section 8 presents an exemplification of the argumentation framework.",
        "Finally, Section 9 presents an empirical evaluation of our two main hypotheses.",
        "The paper closes with related work and conclusions sections. 2.",
        "ARGUMENTATION,COLLABORATION AND LEARNING Both learning and collaboration are ways in which an agent can improve individual performance.",
        "In fact, there is a clear parallelism between learning and collaboration in multi-agent systems, since both are ways in which agents can deal with their shortcomings.",
        "Let us show which are the main motivations that an agent can have to learn or to collaborate. • Motivations to learn: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems. • Motivations to collaborate: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems, - Increase the range of accessible resources.",
        "Looking at the above lists of motivation, we can easily see that learning and collaboration are very related in multi-agent systems.",
        "In fact, with the exception of the last item in the motivations to collaborate list, they are two extremes of a continuum of strategies to improve performance.",
        "An agent may choose to increase performance by learning, by collaborating, or by finding an intermediate point that combines learning and collaboration in order to improve performance.",
        "In this paper we will propose AMAL, an argumentation framework for learning agents, and will also also show how AMAL can be used both for learning from communication and for solving problems in a collaborative way: • Agents can solve problems in a collaborative way via engaging an argumentation process about the prediction for the situation at hand.",
        "Using this collaboration, the prediction can be done in a more informed way, since the information known by several agents has been taken into account. • Agents can also learn from communication with other agents by engaging an argumentation process.",
        "Agents that engage in such argumentation processes can learn from the arguments and counterexamples received from other agents, and use this information for predicting the outcomes of future situations.",
        "In the rest of this paper we will propose an argumentation framework and show how it can be used both for learning and for solving problems in a collaborative way. 3.",
        "MULTI-AGENT CBR SYSTEMS A Multi-Agent Case Based Reasoning System (MAC) M = {(A1, C1), ..., (An, Cn)} is a multi-agent system composed of A = {Ai, ..., An}, a set of CBR agents, where each agent Ai ∈ A possesses an individual case base Ci.",
        "Each individual agent Ai in a MAC is completely autonomous and each agent Ai has access only to its individual and private case base Ci.",
        "A case base Ci = {c1, ..., cm} is a collection of cases.",
        "Agents in a MAC system are able to individually solve problems, but they can also collaborate with other agents to solve problems.",
        "In this framework, we will restrict ourselves to analytical tasks, i.e. tasks like classification, where the solution of a problem is achieved by selecting a solution class from an enumerated set of solution classes.",
        "In the following we will note the set of all the solution classes by S = {S1, ..., SK }.",
        "Therefore, a case c = P, S is a tuple containing a case description P and a solution class S ∈ S. In the following, we will use the terms problem and case description indistinctly.",
        "Moreover, we will use the dot notation to refer to elements inside a tuple; e.g., to refer to the solution class of a case c, we will write c.S.",
        "Therefore, we say a group of agents perform joint deliberation, when they collaborate to find a joint solution by means of an argumentation process.",
        "However, in order to do so, an agent has to be able to justify its prediction to the other agents (i.e. generate an argument for its predicted solution that can be examined and critiqued by the other agents).",
        "The next section addresses this issue. 3.1 Justified Predictions Both expert systems and CBR systems may have an explanation component [14] in charge of justifying why the system has provided a specific answer to the user.",
        "The line of reasoning of the system can then be examined by a human expert, thus increasing the reliability of the system.",
        "Most of the existing work on explanation generation focuses on generating explanations to be provided to the user.",
        "However, in our approach we use explanations (or justifications) as a tool for improving communication and coordination among agents.",
        "We are interested in justifications since they can be used as arguments.",
        "For that purpose, we will benefit from the ability of some machine learning methods to provide justifications.",
        "A justification built by a CBR method after determining that the solution of a particular problem P was Sk is a description that contains the relevant information from the problem P that the CBR method has considered to predict Sk as the solution of P. In particular, CBR methods work by retrieving similar cases to the problem at hand, and then reusing their solutions for the current problem, expecting that since the problem and the cases are similar, the solutions will also be similar.",
        "Thus, if a CBR method has retrieved a set of cases C1, ..., Cn to solve a particular problem P the justification built will contain the relevant information from the problem P that made the CBR system retrieve that particular set of cases, i.e. it will contain the relevant information that P and C1, ..., Cn have in common.",
        "For example, Figure 1 shows a justification build by a CBR system for a toy problem (in the following sections we will show justifications for real problems).",
        "In the figure, a problem has two attributes (Traffic_light, and Cars_passing), the retrieval mechanism of the CBR system notices that by considering only the attribute Traffic_light, it can retrieve two cases that predict the same solution: wait.",
        "Thus, since only this attribute has been used, it is the only one appearing in the justification.",
        "The values of the rest of attributes are irrelevant, since whatever their value the solution class would have been the same. 976 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Problem Traffic_light: red Cars_passing: no Case 1 Traffic_light: red Cars_passing: no Solution: wait Case 3 Traffic_light: red Cars_passing: yes Solution: wait Case 4 Traffic_light: green Cars_passing: yes Solution: wait Case 2 Traffic_light: green Cars_passing: no Solution: cross Retrieved cases Solution: wait Justification Traffic_light: red Figure 1: An example of justification generation in a CBR system.",
        "Notice that, since the only relevant feature to decide is Traffic_light (the only one used to retrieve cases), it is the only one appearing in the justification.",
        "In general, the meaning of a justification is that all (or most of) the cases in the case base of an agent that satisfy the justification (i.e. all the cases that are subsumed by the justification) belong to the predicted solution class.",
        "In the rest of the paper, we will use to denote the subsumption relation.",
        "In our work, we use LID [2], a CBR method capable of building symbolic justifications such as the one exemplified in Figure 1.",
        "When an agent provides a justification for a prediction, the agent generates a justified prediction: DEFINITION 3.1.",
        "A Justified Prediction is a tuple J = A, P, S, D where agent A considers S the correct solution for problem P, and that prediction is justified a symbolic description D such that J.D J.P. Justifications can have many uses for CBR systems [8, 9].",
        "In this paper, we are going to use justifications as arguments, in order to allow learning agents to engage in argumentation processes. 4.",
        "ARGUMENTS AND COUNTERARGUMENTS For our purposes an argument α generated by an agent A is composed of a statement S and some evidence D supporting S as correct.",
        "In the remainder of this section we will see how this general definition of argument can be instantiated in specific kind of arguments that the agents can generate.",
        "In the context of MAC systems, agents argue about predictions for new problems and can provide two kinds of information: a) specific cases P, S , and b) justified predictions: A, P, S, D .",
        "Using this information, we can define three types of arguments: justified predictions, counterarguments, and counterexamples.",
        "A justified prediction α is generated by an agent Ai to argue that Ai believes that the correct solution for a given problem P is α.S, and the evidence provided is the justification α.D.",
        "In the example depicted in Figure 1, an agent Ai may generate the argument α = Ai, P, Wait, (Traffic_light = red) , meaning that the agent Ai believes that the correct solution for P is Wait because the attribute Traffic_light equals red.",
        "A counterargument β is an argument offered in opposition to another argument α.",
        "In our framework, a counterargument consists of a justified prediction Aj, P, S , D generated by an agent Aj with the intention to rebut an argument α generated by another agent Ai, that endorses a solution class S different from that of α.S for the problem at hand and justifies this with a justification D .",
        "In the example in Figure 1, if an agent generates the argument α = Ai, P, Walk, (Cars_passing = no) , an agent that thinks that the correct solution is Wait might answer with the counterargument β = Aj, P, Wait, (Cars_passing = no ∧ Traffic_light = red) , meaning that, although there are no cars passing, the traffic light is red, and the street cannot be crossed.",
        "A counterexample c is a case that contradicts an argument α.",
        "Thus a counterexample is also a counterargument, one that states that a specific argument α is not always true, and the evidence provided is the case c. Specifically, for a case c to be a counterexample of an argument α, the following conditions have to be met: α.D c and α.S = c.S, i.e. the case must satisfy the justification α.D and the solution of c must be different than the predicted by α.",
        "By exchanging arguments and counterarguments (including counterexamples), agents can argue about the correct solution of a given problem, i.e. they can engage a joint deliberation process.",
        "However, in order to do so, they need a specific interaction protocol, a preference relation between contradicting arguments, and a decision policy to generate counterarguments (including counterexamples).",
        "In the following sections we will present these elements. 5.",
        "PREFERENCE RELATION A specific argument provided by an agent might not be consistent with the information known to other agents (or even to some of the information known by the agent that has generated the justification due to noise in training data).",
        "For that reason, we are going to define a preference relation over contradicting justified predictions based on cases.",
        "Basically, we will define a confidence measure for each justified prediction (that takes into account the cases owned by each agent), and the justified prediction with the highest confidence will be the preferred one.",
        "The idea behind case-based confidence is to count how many of the cases in an individual case base endorse a justified prediction, and how many of them are counterexamples of it.",
        "The more the endorsing cases, the higher the confidence; and the more the counterexamples, the lower the confidence.",
        "Specifically, to assess the confidence of a justified prediction α, an agent obtains the set of cases in its individual case base that are subsumed by α.D.",
        "With them, an agent Ai obtains the Y (aye) and N (nay) values: • Y Ai α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by the justification α.D that belong to the solution class α.S, • NAi α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by justification α.D that do not belong to that solution class.",
        "The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 977 + + + + + +  - - + Figure 2: Confidence of arguments is evaluated by contrasting them against the case bases of the agents.",
        "An agent estimates the confidence of an argument as: CAi (α) = Y Ai α 1 + Y Ai α + NAi α i.e. the confidence on a justified prediction is the number of endorsing cases divided by the number of endorsing cases plus counterexamples.",
        "Notice that we add 1 to the denominator, this is to avoid giving excessively high confidences to justified predictions whose confidence has been computed using a small number of cases.",
        "Notice that this correction follows the same idea than the Laplace correction to estimate probabilities.",
        "Figure 2 illustrates the individual evaluation of the confidence of an argument, in particular, three endorsing cases and one counterexample are found in the case base of agents Ai, giving an estimated confidence of 0.6 Moreover, we can also define the joint confidence of an argument α as the confidence computed using the cases present in the case bases of all the agents in the group: C(α) = i Y Ai α 1 + i Y Ai α + NAi α Notice that, to collaboratively compute the joint confidence, the agents only have to make public the aye and nay values locally computed for a given argument.",
        "In our framework, agents use this joint confidence as the preference relation: a justified prediction α is preferred over another one β if C(α) ≥ C(β). 6.",
        "GENERATION OF ARGUMENTS In our framework, arguments are generated by the agents from cases, using learning methods.",
        "Any learning method able to provide a justified prediction can be used to generate arguments.",
        "For instance, decision trees and LID [2] are suitable learning methods.",
        "Specifically, in the experiments reported in this paper agents use LID.",
        "Thus, when an agent wants to generate an argument endorsing that a specific solution class is the correct solution for a problem P, it generates a justified prediction as explained in Section 3.1.",
        "For instance, Figure 3 shows a real justification generated by LID after solving a problem P in the domain of marine sponges identification.",
        "In particular, Figure 3 shows how when an agent receives a new problem to solve (in this case, a new sponge to determine its order), the agent uses LID to generate an argument (consisting on a justified prediction) using the cases in the case base of the agent.",
        "The justification shown in Figure 3 can be interpreted saying that the predicted solution is hadromerida because the smooth form of the megascleres of the spiculate skeleton of the sponge is of type tylostyle, the spikulate skeleton of the sponge has no uniform length, and there is no gemmules in the external features of the sponge.",
        "Thus, the argument generated will be α = A1, P, hadromerida, D1 . 6.1 Generation of Counterarguments As previously stated, agents may try to rebut arguments by generating counterargument or by finding counterexamples.",
        "Let us explain how they can be generated.",
        "An agent Ai wants to generate a counterargument β to rebut an argument α when α is in contradiction with the local case base of Ai.",
        "Moreover, while generating such counterargument β, Ai expects that β is preferred over α.",
        "For that purpose, we will present a specific policy to generate counterarguments based on the specificity criterion [10].",
        "The specificity criterion is widely used in deductive frameworks for argumentation, and states that between two conflicting arguments, the most specific should be preferred since it is, in principle, more informed.",
        "Thus, counterarguments generated based on the specificity criterion are expected to be preferable (since they are more informed) to the arguments they try to rebut.",
        "However, there is no guarantee that such counterarguments will always win, since, as we have stated in Section 5, agents in our framework use a preference relation based on joint confidence.",
        "Moreover, one may think that it would be better that the agents generate counterarguments based on the joint confidence preference relation; however it is not obvious how to generate counterarguments based on joint confidence in an efficient way, since collaboration is required in order to evaluate joint confidence.",
        "Thus, the agent generating the counterargument should constantly communicate with the other agents at each step of the induction algorithm used to generate counterarguments (presently one of our future research lines).",
        "Thus, in our framework, when an agent wants to generate a counterargument β to an argument α, β has to be more specific than α (i.e. α.D < β.D).",
        "The generation of counterarguments using the specificity criterion imposes some restrictions over the learning method, although LID or ID3 can be easily adapted for this task.",
        "For instance, LID is an algorithm that generates a description starting from scratch and heuristically adding features to that term.",
        "Thus, at every step, the description is made more specific than in the previous step, and the number of cases that are subsumed by that description is reduced.",
        "When the description covers only (or almost only) cases of a single solution class LID terminates and predicts that solution class.",
        "To generate a counterargument to an argument α LID just has to use as starting point the description α.D instead of starting from scratch.",
        "In this way, the justification provided by LID will always be subsumed by α.D, and thus the resulting counterargument will be more specific than α.",
        "However, notice that LID may sometimes not be able to generate counterarguments, since LID may not be able to specialize the description α.D any further, or because the agent Ai has no case inCi that is subsumed by α.D.",
        "Figure 4 shows how an agent A2 that disagreed with the argument shown in Figure 3, generates a counterargument using LID.",
        "Moreover, Figure 4 shows the generation of a counterargument β1 2 for the argument α0 1 (in Figure 3) that is a specialization of α0 1. 978 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Solution: hadromerida Justification: D1 Sponge Spikulate skeleton External features External features Gemmules: no Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Case Base of A1 LID New sponge P Figure 3: Example of a real justification generated by LID in the marine sponges data set.",
        "Specifically, in our experiments, when an agent Ai wants to rebut an argument α, uses the following policy: 1.",
        "Agent Ai uses LID to try to find a counterargument β more specific than α; if found, β is sent to the other agent as a counterargument of α. 2.",
        "If not found, then Ai searches for a counterexample c ∈ Ci of α.",
        "If a case c is found, then c is sent to the other agent as a counterexample of α. 3.",
        "If no counterexamples are found, then Ai cannot rebut the argument α. 7.",
        "ARGUMENTATION-BASED MULTI-AGENT LEARNING The interaction protocol of AMAL allows a group of agents A1, ..., An to deliberate about the correct solution of a problem P by means of an argumentation process.",
        "If the argumentation process arrives to a consensual solution, the joint deliberation ends; otherwise a weighted vote is used to determine the joint solution.",
        "Moreover, AMAL also allows the agents to learn from the counterexamples received from other agents.",
        "The AMAL protocol consists on a series of rounds.",
        "In the initial round, each agent states which is its individual prediction for P. Then, at each round an agent can try to rebut the prediction made by any of the other agents.",
        "The protocol uses a token passing mechanism so that agents (one at a time) can send counterarguments or counterexamples if they disagree with the prediction made by any other agent.",
        "Specifically, each agent is allowed to send one counterargument or counterexample each time he gets the token (notice that this restriction is just to simplify the protocol, and that it does not restrict the number of counterargument an agent can sent, since they can be delayed for subsequent rounds).",
        "When an agent receives a counterargument or counterexample, it informs the other agents if it accepts the counterargument (and changes its prediction) or not.",
        "Moreover, agents have also the opportunity to answer to counterarguments when they receive the token, by trying to generate a counterargument to the counterargument.",
        "When all the agents have had the token once, the token returns to the first agent, and so on.",
        "If at any time in the protocol, all the agents agree or during the last n rounds no agent has generated any counterargument, the protocol ends.",
        "Moreover, if at the end of the argumentation the agents have not reached an agreement, then a voting mechanism that uses the confidence of each prediction as weights is used to decide the final solution (Thus, AMAL follows the same mechanism as human committees, first each individual member of a committee exposes his arguments and discuses those of the other members (joint deliberation), and if no consensus is reached, then a voting mechanism is required).",
        "At each iteration, agents can use the following performatives: • assert(α): the justified prediction held during the next round will be α.",
        "An agent can only hold a single prediction at each round, thus is multiple asserts are send, only the last one is considered as the currently held prediction. • rebut(β, α): the agent has found a counterargument β to the prediction α.",
        "We will define Ht = αt 1, ..., αt n as the predictions that each of the n agents hold at a round t. Moreover, we will also define contradict(αt i) = {α ∈ Ht|α.S = αt i.S} as the set of contradicting arguments for an agent Ai in a round t, i.e. the set of arguments at round t that support a different solution class than αt i.",
        "The protocol is initiated because one of the agents receives a problem P to be solved.",
        "After that, the agent informs all the other agents about the problem P to solve, and the protocol starts: 1.",
        "At round t = 0, each one of the agents individually solves P, and builds a justified prediction using its own CBR method.",
        "Then, each agent Ai sends the performative assert(α0 i ) to the other agents.",
        "Thus, the agents know H0 = α0 i , ..., α0 n .",
        "Once all the predictions have been sent the token is given to the first agent A1. 2.",
        "At each round t (other than 0), the agents check whether their arguments in Ht agree.",
        "If they do, the protocol moves to step 5.",
        "Moreover, if during the last n rounds no agent has sent any counterexample or counterargument, the protocol also moves to step 5.",
        "Otherwise, the agent Ai owner of the token tries to generate a counterargument for each of the opposing arguments in contradict(αt i) ⊆ Ht (see Section 6.1).",
        "Then, the counterargument βt i against the prediction αt j with the lowest confidence C(αt j) is selected (since αt j is the prediction more likely to be successfully rebutted). • If βt i is a counterargument, then, Ai locally compares αt i with βt i by assessing their confidence against its individual case base Ci (see Section 5) (notice that Ai is comparing its previous argument with the counterargument that Ai itself has just generated and that is about The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 979 Sponge Spikulate skeleton External features External features Gemmules: no Growing: Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Growing Grow: massive Case Base of A2 LID Solution: astrophorida Justification: D2 Figure 4: Generation of a counterargument using LID in the sponges data set. to send to Aj).",
        "If CAi (βt i ) > CAi (αt i), then Ai considers that βt i is stronger than its previous argument, changes its argument to βt i by sending assert(βt i ) to the rest of the agents (the intuition behind this is that since a counterargument is also an argument, Ai checks if the newly counterargument is a better argument than the one he was previously holding) and rebut(βt i , αt j) to Aj.",
        "Otherwise (i.e.",
        "CAi (βt i ) ≤ CAi (αt i)), Ai will send only rebut(βt i , αt j) to Aj.",
        "In any of the two situations the protocol moves to step 3. • If βt i is a counterexample c, then Ai sends rebut(c, αt j) to Aj.",
        "The protocol moves to step 4. • If Ai cannot generate any counterargument or counterexample, the token is sent to the next agent, a new round t + 1 starts, and the protocol moves to state 2. 3.",
        "The agent Aj that has received the counterargument βt i , locally compares it against its own argument, αt j, by locally assessing their confidence.",
        "If CAj (βt i ) > CAj (αt j), then Aj will accept the counterargument as stronger than its own argument, and it will send assert(βt i ) to the other agents.",
        "Otherwise (i.e.",
        "CAj (βt i ) ≤ CAj (αt j)), Aj will not accept the counterargument, and will inform the other agents accordingly.",
        "Any of the two situations start a new round t + 1, Ai sends the token to the next agent, and the protocol moves back to state 2. 4.",
        "The agent Aj that has received the counterexample c retains it into its case base and generates a new argument αt+1 j that takes into account c, and informs the rest of the agents by sending assert(αt+1 j ) to all of them.",
        "Then, Ai sends the token to the next agent, a new round t + 1 starts, and the protocol moves back to step 2. 5.",
        "The protocol ends yielding a joint prediction, as follows: if the arguments in Ht agree then their prediction is the joint prediction, otherwise a voting mechanism is used to decide the joint prediction.",
        "The voting mechanism uses the joint confidence measure as the voting weights, as follows: S = arg max Sk∈S αi∈Ht|αi.S=Sk C(αi) Moreover, in order to avoid infinite iterations, if an agent sends twice the same argument or counterargument to the same agent, the message is not considered. 8.",
        "EXEMPLIFICATION Let us consider a system composed of three agents A1, A2 and A3.",
        "One of the agents, A1 receives a problem P to solve, and decides to use AMAL to solve it.",
        "For that reason, invites A2 and A3 to take part in the argumentation process.",
        "They accept the invitation, and the argumentation protocol starts.",
        "Initially, each agent generates its individual prediction for P, and broadcasts it to the other agents.",
        "Thus, all of them can compute H0 = α0 1, α0 2, α0 3 .",
        "In particular, in this example: • α0 1 = A1, P, hadromerida, D1 • α0 2 = A2, P, astrophorida, D2 • α0 3 = A3, P, axinellida, D3 A1 starts owning the token and tries to generate counterarguments for α0 2 and α0 3, but does not succeed, however it has one counterexample c13 for α0 3.",
        "Thus, A1 sends the the message rebut( c13, α0 3) to A3.",
        "A3 incorporates c13 into its case base and tries to solve the problem P again, now taking c13 into consideration.",
        "A3 comes up with the justified prediction α1 3 = A3, P, hadromerida, D4 , and broadcasts it to the rest of the agents with the message assert(α1 3).",
        "Thus, all of them know the new H1 = α0 1, α0 2, α1 3 .",
        "Round 1 starts and A2 gets the token.",
        "A2 tries to generate counterarguments for α0 1 and α1 3 and only succeeds to generate a counterargument β1 2 = A2, P, astrophorida, D5 against α1 3.",
        "The counterargument is sent to A3 with the message rebut(β1 2 , α1 3).",
        "Agent A3 receives the counterargument and assesses its local confidence.",
        "The result is that the individual confidence of the counterargument β1 2 is lower than the local confidence of α1 3.",
        "Therefore, A3 does not accept the counterargument, and thus H2 = α0 1, α0 2, α1 3 .",
        "Round 2 starts and A3 gets the token.",
        "A3 generates a counterargument β2 3 = A3, P, hadromerida, D6 for α0 2 and sends it to A2 with the message rebut(β2 3 , α0 2).",
        "Agent A2 receives the counterargument and assesses its local confidence.",
        "The result is that the local confidence of the counterargument β2 3 is higher than the local confidence of α0 2.",
        "Therefore, A2 accepts the counterargument and informs the rest of the agents with the message assert(β2 3 ).",
        "After that, H3 = α0 1, β2 3 , α1 3 .",
        "At Round 3, since all the agents agree (all the justified predictions in H3 predict hadromerida as the solution class) The protocol ends, and A1 (the agent that received the problem) considers hadromerida as the joint solution for the problem P. 9.",
        "EXPERIMENTAL EVALUATION 980 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) SPONGE 75 77 79 81 83 85 87 89 91 2 3 4 5 AMAL Voting Individual SOYBEAN 55 60 65 70 75 80 85 90 2 3 4 5 AMAL Voting Individual Figure 5: Individual and joint accuracy for 2 to 5 agents.",
        "In this section we empirically evaluate the AMAL argumentation framework.",
        "We have made experiments in two different data sets: soybean (from the UCI machine learning repository) and sponge (a relational data set).",
        "The soybean data set has 307 examples and 19 solution classes, while the sponge data set has 280 examples and 3 solution classes.",
        "In an experimental run, the data set is divided in 2 sets: the training set and the test set.",
        "The training set examples are distributed among 5 different agents without replication, i.e. there is no example shared by two agents.",
        "In the testing stage, problems in the test set arrive randomly to one of the agents, and their goal is to predict the correct solution.",
        "The experiments are designed to test two hypotheses: (H1) that argumentation is a useful framework for joint deliberation and can improve over other typical methods such as voting; and (H2) that learning from communication improves the individual performance of a learning agent participating in an argumentation process.",
        "Moreover, we also expect that the improvement achieved from argumentation will increase as the number of agents participating in the argumentation increases (since more information will be taken into account).",
        "Concerning H1 (argumentation is a useful framework for joint deliberation), we ran 4 experiments, using 2, 3, 4, and 5 agents respectively (in all experiments each agent has a 20% of the training data, since the training is always distributed among 5 agents).",
        "Figure 5 shows the result of those experiments in the sponge and soybean data sets.",
        "Classification accuracy is plotted in the vertical axis, and in the horizontal axis the number of agents that took part in the argumentation processes is shown.",
        "For each number of agents, three bars are shown: individual, Voting, and AMAL.",
        "The individual bar shows the average accuracy of individual agents predictions; the voting bar shows the average accuracy of the joint prediction achieved by voting but without any argumentation; and finally the AMAL bar shows the average accuracy of the joint prediction using argumentation.",
        "The results shown are the average of 5 10-fold cross validation runs.",
        "Figure 5 shows that collaboration (voting and AMAL) outperforms individual problem solving.",
        "Moreover, as we expected, the accuracy improves as more agents collaborate, since more information is taken into account.",
        "We can also see that AMAL always outperforms standard voting, proving that joint decisions are based on better information as provided by the argumentation process.",
        "For instance, the joint accuracy for 2 agents in the sponge data set is of 87.57% for AMAL and 86.57% for voting (while individual accuracy is just 80.07%).",
        "Moreover, the improvement achieved by AMAL over Voting is even larger in the soybean data set.",
        "The reason is that the soybean data set is more difficult (in the sense that agents need more data to produce good predictions).",
        "These experimental results show that AMAL effectively exploits the opportunity for improvement: the accuracy is higher only because more agents have changed their opinion during argumentation (otherwise they would achieve the same result as Voting).",
        "Concerning H2 (learning from communication in argumentation processes improves individual prediction ), we ran the following experiment: initially, we distributed a 25% of the training set among the five agents; after that, the rest of the cases in the training set is sent to the agents one by one; when an agent receives a new training case, it has several options: the agent can discard it, the agent can retain it, or the agent can use it for engaging an argumentation process.",
        "Figure 6 shows the result of that experiment for the two data sets.",
        "Figure 6 contains three plots, where NL (not learning) shows accuracy of an agent with no learning at all; L (learning), shows the evolution of the individual classification accuracy when agents learn by retaining the training cases they individually receive (notice that when all the training cases have been retained at 100%, the accuracy should be equal to that of Figure 5 for individual agents); and finally LFC (learning from communication) shows the evolution of the individual classification accuracy of learning agents that also learn by retaining those counterexamples received during argumentation (i.e. they learn both from training examples and counterexamples).",
        "Figure 6 shows that if an agent Ai learns also from communication, Ai can significantly improve its individual performance with just a small number of additional cases (those selected as relevant counterexamples for Ai during argumentation).",
        "For instance, in the soybean data set, individual agents have achieved an accuracy of 70.62% when they also learn from communication versus an accuracy of 59.93% when they only learn from their individual experience.",
        "The number of cases learnt from communication depends on the properties of the data set: in the sponges data set, agents have retained only very few additional cases, and significantly improved individual accuracy; namely they retain 59.96 cases in average (compared to the 50.4 cases retained if they do not learn from communication).",
        "In the soybean data set more counterexamples are learnt to significantly improve individual accuracy, namely they retain 87.16 cases in average (compared to 55.27 cases retained if they do not learn from communication).",
        "Finally, the fact that both data sets show a significant improvement points out the adaptive nature of the argumentation-based approach to learning from communication: the useful cases are selected as counterexamples (and no more than those needed), and they have the intended effect. 10.",
        "RELATED WORK Concerning CBR in a multi-agent setting, the first research was on negotiated case retrieval [11] among groups of agents.",
        "Our work on multi-agent case-based learning started in 1999 [6]; later Mc Ginty and Smyth [7] presented a multi-agent collaborative CBR approach (CCBR) for planning.",
        "Finally, another interesting approach is multi-case-base reasoning (MCBR) [5], that deals with The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 981 SPONGE 60 65 70 75 80 85 25% 40% 55% 70% 85% 100% LFC L NL SOYBEAN 20 30 40 50 60 70 80 90 25% 40% 55% 70% 85% 100% LFC L NL Figure 6: Learning from communication resulting from argumentation in a system composed of 5 agents. distributed systems where there are several case bases available for the same task and addresses the problems of cross-case base adaptation.",
        "The main difference is that our MAC approach is a way to distribute the Reuse process of CBR (using a voting system) while Retrieve is performed individually by each agent; the other multiagent CBR approaches, however, focus on distributing the Retrieve process.",
        "Research on MAS argumentation focus on several issues like a) logics, protocols and languages that support argumentation, b) argument selection and c) argument interpretation.",
        "Approaches for logic and languages that support argumentation include defeasible logic [4] and BDI models [13].",
        "Although argument selection is a key aspect of automated argumentation (see [12] and [13]), most research has been focused on preference relations among arguments.",
        "In our framework we have addressed both argument selection and preference relations using a case-based approach. 11.",
        "CONCLUSIONS AND FUTURE WORK In this paper we have presented an argumentation-based framework for multi-agent learning.",
        "Specifically, we have presented AMAL, a framework that allows a group of learning agents to argue about the solution of a given problem and we have shown how the learning capabilities can be used to generate arguments and counterarguments.",
        "The experimental evaluation shows that the increased amount of information provided to the agents by the argumentation process increases their predictive accuracy, and specially when an adequate number of agents take part in the argumentation.",
        "The main contributions of this work are: a) an argumentation framework for learning agents; b) a case-based preference relation over arguments, based on computing an overall confidence estimation of arguments; c) a case-based policy to generate counterarguments and select counterexamples; and d) an argumentation-based approach for learning from communication.",
        "Finally, in the experiments presented here a learning agent would retain all counterexamples submitted by the other agent; however, this is a very simple case retention policy, and we will like to experiment with more informed policies - with the goal that individual learning agents could significantly improve using only a small set of cases proposed by other agents.",
        "Finally, our approach is focused on lazy learning, and future works aims at incorporating eager inductive learning inside the argumentative framework for learning from communication. 12.",
        "REFERENCES [1] Agnar Aamodt and Enric Plaza.",
        "Case-based reasoning: Foundational issues, methodological variations, and system approaches.",
        "Artificial Intelligence Communications, 7(1):39-59, 1994. [2] E. Armengol and E. Plaza.",
        "Lazy induction of descriptions for relational case-based learning.",
        "In ECML2001, pages 13-24, 2001. [3] Gerhard Brewka.",
        "Dynamic argument systems: A formal model of argumentation processes based on situation calculus.",
        "Journal of Logic and Computation, 11(2):257-282, 2001. [4] Carlos I. Chesñevar and Guillermo R. Simari.",
        "Formalizing Defeasible Argumentation using Labelled Deductive Systems.",
        "Journal of Computer Science & Technology, 1(4):18-33, 2000. [5] D. Leake and R. Sooriamurthi.",
        "Automatically selecting strategies for multi-case-base reasoning.",
        "In S. Craw and A. Preece, editors, ECCBR2002, pages 204-219, Berlin, 2002.",
        "Springer Verlag. [6] Francisco J. Martín, Enric Plaza, and Josep-Lluis Arcos.",
        "Knowledge and experience reuse through communications among competent (peer) agents.",
        "International Journal of Software Engineering and Knowledge Engineering, 9(3):319-341, 1999. [7] Lorraine McGinty and Barry Smyth.",
        "Collaborative case-based reasoning: Applications in personalized route planning.",
        "In I. Watson and Q. Yang, editors, ICCBR, number 2080 in LNAI, pages 362-376.",
        "Springer-Verlag, 2001. [8] Santi Ontañón and Enric Plaza.",
        "Justification-based multiagent learning.",
        "In ICML2003, pages 576-583.",
        "Morgan Kaufmann, 2003. [9] Enric Plaza, Eva Armengol, and Santiago Ontañón.",
        "The explanatory power of symbolic similarity in case-based reasoning.",
        "Artificial Intelligence Review, 24(2):145-161, 2005. [10] David Poole.",
        "On the comparison of theories: Preferring the most specific explanation.",
        "In IJCAI-85, pages 144-147, 1985. [11] M V Nagendra Prassad, Victor R Lesser, and Susan Lander.",
        "Retrieval and reasoning in distributed case bases.",
        "Technical report, UMass Computer Science Department, 1995. [12] K. Sycara S. Kraus and A. Evenchik.",
        "Reaching agreements through argumentation: a logical model and implementation.",
        "Artificial Intelligence Journal, 104:1-69, 1998. [13] N. R. Jennings S. Parsons, C. Sierra.",
        "Agents that reason and negotiate by arguing.",
        "Journal of Logic and Computation, 8:261-292, 1998. [14] Bruce A. Wooley.",
        "Explanation component of software systems.",
        "ACM CrossRoads, 5.1, 1998. 982 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
    ],
    "translated_text_sentences": [
        "Aprendizaje y deliberación conjunta a través de la argumentación en sistemas multiagente. Santi Ontañón CCL, Laboratorio de Computación Cognitiva, Instituto de Tecnología de Georgia, Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Instituto de Investigación en Inteligencia Artificial, CSIC, Consejo Superior de Investigaciones Científicas, Campus UAB, 08193 Bellaterra, Cataluña (España) enric@iiia.csic.es RESUMEN En este artículo presentaremos un marco de argumentación para agentes de aprendizaje (AMAL) diseñado con dos propósitos: (1) para deliberación conjunta, y (2) para aprender a través de la comunicación.",
        "El marco AMAL se basa completamente en el aprendizaje a partir de ejemplos: la relación de preferencia de argumentos, la política de generación de argumentos y la política de generación de contraargumentos son técnicas basadas en casos.",
        "Para unirse en la deliberación, los agentes de aprendizaje comparten su experiencia formando un comité para decidir sobre alguna decisión conjunta.",
        "Experimentalmente demostramos que la argumentación entre comités de agentes mejora tanto el rendimiento individual como el conjunto.",
        "Para aprender a través de la comunicación, un agente se involucra en discusiones con otros agentes para contrastar sus hipótesis individuales y recibir contraejemplos; el proceso de argumentación mejora su alcance de aprendizaje y rendimiento individual.",
        "Categorías y Descriptores de Asignaturas I.2.6 [Inteligencia Artificial]: Aprendizaje; I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente, Agentes Inteligentes 1.",
        "Los marcos de argumentación para sistemas multiagente pueden ser utilizados para diferentes propósitos como la deliberación conjunta, la persuasión, la negociación y la resolución de conflictos.",
        "En este artículo presentaremos un marco de argumentación para agentes de aprendizaje, y mostraremos que puede ser utilizado para dos propósitos: (1) deliberación conjunta, y (2) aprendizaje a partir de la comunicación.",
        "La deliberación conjunta basada en argumentos implica discutir sobre el resultado de una situación particular o la acción apropiada a tomar en una situación específica.",
        "Los agentes de aprendizaje son capaces de aprender de la experiencia, en el sentido de que ejemplos pasados (situaciones y sus resultados) se utilizan para predecir el resultado para la situación actual.",
        "Sin embargo, dado que la experiencia de los agentes individuales puede ser limitada, el conocimiento individual y la precisión de la predicción también son limitados.",
        "Por lo tanto, los agentes de aprendizaje que son capaces de argumentar sus predicciones individuales con otros agentes pueden alcanzar una mayor precisión en las predicciones después de dicho proceso de argumentación.",
        "La mayoría de los marcos de argumentación existentes para sistemas multiagente se basan en lógica deductiva u otra formalización de lógica deductiva diseñada específicamente para respaldar la argumentación, como la lógica por defecto [3].",
        "Por lo general, un argumento se ve como una declaración lógica, mientras que un contraargumento es un argumento ofrecido en oposición a otro argumento; los agentes utilizan una relación de preferencia para resolver argumentos conflictivos.",
        "Sin embargo, los marcos de argumentación basados en lógica asumen agentes con conocimiento predefinido y relación de preferencia.",
        "En este artículo, nos enfocamos en un marco de Aprendizaje Multiagente basado en Argumentación (AMAL) donde tanto el conocimiento como la relación de preferencia se aprenden a partir de la experiencia.",
        "Por lo tanto, consideramos un escenario con agentes que (1) trabajan en el mismo dominio utilizando una ontología compartida, (2) son capaces de aprender a partir de ejemplos y (3) se comunican utilizando un marco argumentativo.",
        "Tener capacidades de aprendizaje permite a los agentes utilizar de manera efectiva una forma específica de contraargumento, a saber, el uso de contraejemplos.",
        "Los contraejemplos ofrecen la posibilidad de que los agentes aprendan durante el proceso de argumentación.",
        "Además, los agentes de aprendizaje permiten técnicas que utilizan la experiencia aprendida para generar argumentos y contraargumentos adecuados.",
        "Específicamente, necesitaremos abordar dos problemas: (1) cómo definir una técnica para generar argumentos y contraargumentos a partir de ejemplos, y (2) cómo definir una relación de preferencia sobre dos argumentos en conflicto que han sido inducidos a partir de ejemplos.",
        "Este documento presenta un enfoque basado en casos para abordar ambas cuestiones.",
        "Los agentes utilizan el razonamiento basado en casos (CBR) [1] para aprender de casos pasados (donde un caso es una situación y su resultado) con el fin de predecir el resultado de una nueva situación.",
        "Proponemos un protocolo de argumentación dentro del marco de AMAL que apoya a los agentes en llegar a una predicción conjunta sobre una situación o problema específico; además, el razonamiento necesario para respaldar el proceso de argumentación también se basará en casos.",
        "En particular, presentamos dos medidas basadas en casos, una para generar los argumentos y contraargumentos adecuados para una situación particular y otra para determinar la relación de preferencia entre los argumentos.",
        "Finalmente, evaluamos (1) si la argumentación entre agentes de aprendizaje puede producir una predicción conjunta que mejore el rendimiento de aprendizaje individual y (2) si el aprendizaje a partir de los contraejemplos transmitidos durante el proceso de argumentación aumenta el rendimiento individual con precisamente aquellos casos que se utilizan mientras discuten entre ellos.",
        "El documento está estructurado de la siguiente manera.",
        "La sección 2 discute la relación entre la argumentación, la colaboración y el aprendizaje.",
        "Luego, la Sección 3 introduce nuestro marco de trabajo de CBR multiagente (MAC) y el concepto de predicción justificada.",
        "Después de eso, la Sección 4 define formalmente nuestro marco de argumentación.",
        "Las secciones 5 y 6 presentan nuestra relación de preferencia basada en casos y las políticas de generación de argumentos, respectivamente.",
        "Más adelante, la Sección 7 presenta el protocolo de argumentación en nuestro marco AMAL.",
        "Después de eso, la Sección 8 presenta una ejemplificación del marco de argumentación.",
        "Finalmente, la Sección 9 presenta una evaluación empírica de nuestras dos hipótesis principales.",
        "El artículo concluye con secciones de trabajo relacionado y conclusiones. 2.",
        "ARGUMENTACIÓN, COLABORACIÓN Y APRENDIZAJE Tanto el aprendizaje como la colaboración son formas en las que un agente puede mejorar el rendimiento individual.",
        "De hecho, existe un claro paralelismo entre el aprendizaje y la colaboración en sistemas multiagentes, ya que ambos son formas en las que los agentes pueden lidiar con sus deficiencias.",
        "Mostremos cuáles son las principales motivaciones que un agente puede tener para aprender o colaborar. • Motivaciones para aprender: - Aumentar la calidad de la predicción, - Aumentar la eficiencia, - Ampliar el rango de problemas resolubles. • Motivaciones para colaborar: - Aumentar la calidad de la predicción, - Aumentar la eficiencia, - Ampliar el rango de problemas resolubles, - Ampliar el rango de recursos accesibles.",
        "Al observar las listas de motivación anteriores, podemos ver fácilmente que el aprendizaje y la colaboración están muy relacionados en los sistemas multiagente.",
        "De hecho, con la excepción del último elemento en la lista de motivaciones para colaborar, son dos extremos de un continuo de estrategias para mejorar el rendimiento.",
        "Un agente puede optar por aumentar el rendimiento mediante el aprendizaje, la colaboración o encontrando un punto intermedio que combine el aprendizaje y la colaboración para mejorar el rendimiento.",
        "En este artículo propondremos AMAL, un marco de argumentación para agentes de aprendizaje, y también mostraremos cómo AMAL puede ser utilizado tanto para aprender a través de la comunicación como para resolver problemas de manera colaborativa: • Los agentes pueden resolver problemas de manera colaborativa al participar en un proceso de argumentación sobre la predicción para la situación actual.",
        "Utilizando esta colaboración, la predicción puede realizarse de una manera más informada, ya que se ha tenido en cuenta la información conocida por varios agentes. Los agentes también pueden aprender de la comunicación con otros agentes al participar en un proceso de argumentación.",
        "Los agentes que participan en estos procesos de argumentación pueden aprender de los argumentos y contraejemplos recibidos de otros agentes, y utilizar esta información para predecir los resultados de situaciones futuras.",
        "En el resto de este documento propondremos un marco de argumentación y mostraremos cómo puede ser utilizado tanto para el aprendizaje como para la resolución de problemas de manera colaborativa.",
        "SISTEMAS CBR MULTIAGENTE Un Sistema de Razonamiento Basado en Casos Multiagente (MAC) M = {(A1, C1), ..., (An, Cn)} es un sistema multiagente compuesto por A = {Ai, ..., An}, un conjunto de agentes CBR, donde cada agente Ai ∈ A posee una base de casos individual Ci.",
        "Cada agente individual Ai en un MAC es completamente autónomo y cada agente Ai solo tiene acceso a su base de casos individual y privada Ci.",
        "Un conjunto de casos Ci = {c1, ..., cm} es una colección de casos.",
        "Los agentes en un sistema MAC pueden resolver problemas de forma individual, pero también pueden colaborar con otros agentes para resolver problemas.",
        "En este marco, nos limitaremos a tareas analíticas, es decir, tareas como la clasificación, donde la solución de un problema se logra seleccionando una clase de solución de un conjunto enumerado de clases de solución.",
        "En lo siguiente, notaremos el conjunto de todas las clases de soluciones por S = {S1, ..., SK}.",
        "Por lo tanto, un caso c = P, S es una tupla que contiene una descripción del caso P y una clase de solución S ∈ S. En lo siguiente, utilizaremos indistintamente los términos problema y descripción del caso.",
        "Además, utilizaremos la notación de punto para referirnos a elementos dentro de una tupla; por ejemplo, para referirnos a la clase de solución de un caso c, escribiremos c.S.",
        "Por lo tanto, decimos que un grupo de agentes realiza una deliberación conjunta cuando colaboran para encontrar una solución conjunta mediante un proceso de argumentación.",
        "Sin embargo, para hacerlo, un agente debe ser capaz de justificar su predicción a los otros agentes (es decir, generar un argumento para su solución predicha que pueda ser examinado y criticado por los otros agentes).",
        "La siguiente sección aborda este tema. 3.1 Predicciones Justificadas Tanto los sistemas expertos como los sistemas CBR pueden tener un componente de explicación [14] encargado de justificar por qué el sistema ha proporcionado una respuesta específica al usuario.",
        "La línea de razonamiento del sistema puede ser examinada por un experto humano, aumentando así la fiabilidad del sistema.",
        "La mayoría de los trabajos existentes sobre generación de explicaciones se centran en generar explicaciones para ser proporcionadas al usuario.",
        "Sin embargo, en nuestro enfoque utilizamos explicaciones (o justificaciones) como una herramienta para mejorar la comunicación y coordinación entre agentes.",
        "Estamos interesados en las justificaciones ya que pueden ser utilizadas como argumentos.",
        "Para ese propósito, nos beneficiaremos de la capacidad de algunos métodos de aprendizaje automático para proporcionar justificaciones.",
        "Una justificación construida por un método CBR después de determinar que la solución de un problema particular P fue Sk es una descripción que contiene la información relevante del problema P que el método CBR ha considerado para predecir Sk como la solución de P. En particular, los métodos CBR funcionan recuperando casos similares al problema en cuestión, y luego reutilizando sus soluciones para el problema actual, esperando que dado que el problema y los casos son similares, las soluciones también serán similares.",
        "Por lo tanto, si un método CBR ha recuperado un conjunto de casos C1, ..., Cn para resolver un problema particular P, la justificación construida contendrá la información relevante del problema P que hizo que el sistema CBR recuperara ese conjunto particular de casos, es decir, contendrá la información relevante que P y C1, ..., Cn tienen en común.",
        "Por ejemplo, la Figura 1 muestra una justificación construida por un sistema CBR para un problema de juguete (en las siguientes secciones mostraremos justificaciones para problemas reales).",
        "En la figura, un problema tiene dos atributos (Semáforo y Coches_pasando), el mecanismo de recuperación del sistema CBR nota que al considerar solo el atributo Semáforo, puede recuperar dos casos que predicen la misma solución: esperar.",
        "Por lo tanto, dado que solo se ha utilizado este atributo, es el único que aparece en la justificación.",
        "Los valores del resto de atributos son irrelevantes, ya que, sea cual sea su valor, la clase de solución habría sido la misma. 976 The Sixth Intl.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Problema Semáforo: rojo Pasan coches: no Caso 1 Semáforo: rojo Pasan coches: no Solución: esperar Caso 3 Semáforo: rojo Pasan coches: sí Solución: esperar Caso 4 Semáforo: verde Pasan coches: sí Solución: esperar Caso 2 Semáforo: verde Pasan coches: no Solución: cruzar Casos recuperados Solución: esperar Justificación Semáforo: rojo Figura 1: Un ejemplo de generación de justificación en un sistema de RCB.",
        "Observa que, dado que la única característica relevante para decidir es Traffic_light (la única utilizada para recuperar casos), es la única que aparece en la justificación.",
        "En general, el significado de una justificación es que todos (o la mayoría) de los casos en la base de casos de un agente que satisfacen la justificación (es decir, todos los casos que están subsumidos por la justificación) pertenecen a la clase de solución predicha.",
        "En el resto del documento, usaremos para denotar la relación de subsumpción.",
        "En nuestro trabajo, utilizamos LID [2], un método CBR capaz de construir justificaciones simbólicas como la ejemplificada en la Figura 1.",
        "Cuando un agente proporciona una justificación para una predicción, el agente genera una predicción justificada: DEFINICIÓN 3.1.",
        "Una Predicción Justificada es una tupla J = A, P, S, D donde el agente A considera que S es la solución correcta para el problema P, y esa predicción está justificada por una descripción simbólica D tal que J.D J.P. Las justificaciones pueden tener muchos usos para los sistemas de RCB [8, 9].",
        "En este documento, vamos a utilizar justificaciones como argumentos, con el fin de permitir que los agentes de aprendizaje participen en procesos de argumentación. 4.",
        "ARGUMENTOS Y CONTRAARGUMENTOS Para nuestros propósitos, un argumento α generado por un agente A está compuesto por una afirmación S y alguna evidencia D que respalda a S como correcta.",
        "En el resto de esta sección veremos cómo esta definición general de argumento puede ser concretada en un tipo específico de argumentos que los agentes pueden generar.",
        "En el contexto de los sistemas MAC, los agentes discuten sobre predicciones para nuevos problemas y pueden proporcionar dos tipos de información: a) casos específicos P, S, y b) predicciones justificadas: A, P, S, D.",
        "Usando esta información, podemos definir tres tipos de argumentos: predicciones justificadas, contraargumentos y contraejemplos.",
        "Una predicción justificada α es generada por un agente Ai para argumentar que Ai cree que la solución correcta para un problema dado P es α.S, y la evidencia proporcionada es la justificación α.D.",
        "En el ejemplo representado en la Figura 1, un agente Ai puede generar el argumento α = Ai, P, Esperar, (Semáforo = rojo), lo que significa que el agente Ai cree que la solución correcta para P es Esperar porque el atributo Semáforo es igual a rojo.",
        "Un contraargumento β es un argumento ofrecido en oposición a otro argumento α.",
        "En nuestro marco de trabajo, un contraargumento consiste en una predicción justificada Aj, P, S, D generada por un agente Aj con la intención de refutar un argumento α generado por otro agente Ai, que respalda una clase de solución S diferente a la de α.S para el problema en cuestión y justifica esto con una justificación D.",
        "En el ejemplo de la Figura 1, si un agente genera el argumento α = Ai, P, Caminar, (Pasando_autos = no), un agente que piensa que la solución correcta es Esperar podría responder con el contraargumento β = Aj, P, Esperar, (Pasando_autos = no ∧ Semáforo = rojo), lo que significa que, aunque no hay autos pasando, el semáforo está en rojo y la calle no se puede cruzar.",
        "Un contraejemplo c es un caso que contradice un argumento α.",
        "Por lo tanto, un contraejemplo también es un contraargumento, uno que afirma que un argumento específico α no siempre es verdadero, y la evidencia proporcionada es el caso c. Específicamente, para que un caso c sea un contraejemplo de un argumento α, se deben cumplir las siguientes condiciones: α.D c y α.S = c.S, es decir, el caso debe satisfacer la justificación α.D y la solución de c debe ser diferente a la predicha por α.",
        "Al intercambiar argumentos y contraargumentos (incluyendo contraejemplos), los agentes pueden discutir sobre la solución correcta de un problema dado, es decir, pueden participar en un proceso de deliberación conjunto.",
        "Sin embargo, para hacerlo, necesitan un protocolo de interacción específico, una relación de preferencia entre argumentos contradictorios y una política de decisión para generar contraargumentos (incluidos contraejemplos).",
        "En las siguientes secciones presentaremos estos elementos. 5.",
        "RELACIÓN DE PREFERENCIA Un argumento específico proporcionado por un agente podría no ser consistente con la información conocida por otros agentes (o incluso con parte de la información conocida por el agente que ha generado la justificación debido al ruido en los datos de entrenamiento).",
        "Por esa razón, vamos a definir una relación de preferencia sobre predicciones justificadas contradictorias basadas en casos.",
        "Básicamente, definiremos una medida de confianza para cada predicción justificada (que tenga en cuenta los casos poseídos por cada agente), y la predicción justificada con la mayor confianza será la preferida.",
        "La idea detrás de la confianza basada en casos es contar cuántos de los casos en una base de casos individual respaldan una predicción justificada, y cuántos de ellos son ejemplos en contra de ella.",
        "Cuanto más casos de respaldo, mayor es la confianza; y cuanto más contrajemplos, menor es la confianza.",
        "Específicamente, para evaluar la confianza de una predicción justificada α, un agente obtiene el conjunto de casos en su base de casos individuales que están subsumidos por α.D.",
        "Con ellos, un agente Ai obtiene los valores Y (sí) y N (no): • Y Ai α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| es el número de casos en la base de casos del agente subsumidos por la justificación α.D que pertenecen a la clase de solución α.S, • NAi α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| es el número de casos en la base de casos del agente subsumidos por la justificación α.D que no pertenecen a esa clase de solución.",
        "El Sexto Internacional.",
        "La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 977 + + + + + +  - - + Figura 2: La confianza de los argumentos se evalúa contrastándolos con las bases de casos de los agentes.",
        "Un agente estima la confianza de un argumento como: CAi (α) = Y Ai α 1 + Y Ai α + NAi α es decir, la confianza en una predicción justificada es el número de casos que la respaldan dividido por el número de casos que la respaldan más los contraejemplos.",
        "Observa que sumamos 1 al denominador, esto es para evitar otorgar confianzas excesivamente altas a predicciones justificadas cuya confianza se ha calculado utilizando un número pequeño de casos.",
        "Ten en cuenta que esta corrección sigue la misma idea que la corrección de Laplace para estimar probabilidades.",
        "La Figura 2 ilustra la evaluación individual de la confianza de un argumento, en particular, se encuentran tres casos de respaldo y un contraejemplo en la base de casos de los agentes Ai, lo que da una confianza estimada de 0.6. Además, también podemos definir la confianza conjunta de un argumento α como la confianza calculada utilizando los casos presentes en las bases de casos de todos los agentes en el grupo: C(α) = Π Ai α / (1 + Σ Ai α + NAi α). Nótese que, para calcular conjuntamente la confianza conjunta, los agentes solo tienen que hacer públicos los valores de aprobación y rechazo calculados localmente para un argumento dado.",
        "En nuestro marco de trabajo, los agentes utilizan esta confianza conjunta como la relación de preferencia: una predicción justificada α se prefiere sobre otra β si C(α) ≥ C(β).",
        "GENERACIÓN DE ARGUMENTOS En nuestro marco de trabajo, los argumentos son generados por los agentes a partir de casos, utilizando métodos de aprendizaje.",
        "Cualquier método de aprendizaje capaz de proporcionar una predicción justificada puede ser utilizado para generar argumentos.",
        "Por ejemplo, los árboles de decisión y LID [2] son métodos de aprendizaje adecuados.",
        "Específicamente, en los experimentos reportados en este artículo, los agentes utilizan LID.",
        "Por lo tanto, cuando un agente desea generar un argumento que respalde que una clase de solución específica es la solución correcta para un problema P, genera una predicción justificada como se explica en la Sección 3.1.",
        "Por ejemplo, la Figura 3 muestra una justificación real generada por LID después de resolver un problema P en el dominio de la identificación de esponjas marinas.",
        "En particular, la Figura 3 muestra cómo cuando un agente recibe un nuevo problema para resolver (en este caso, un nuevo esponja para determinar su orden), el agente utiliza LID para generar un argumento (que consiste en una predicción justificada) utilizando los casos en la base de casos del agente.",
        "La justificación mostrada en la Figura 3 se puede interpretar diciendo que la solución predicha es hadromerida porque la forma lisa de las megascleras del esqueleto espicular de la esponja es de tipo tilostilo, el esqueleto espicular de la esponja no tiene una longitud uniforme y no hay gemas en las características externas de la esponja.",
        "Por lo tanto, el argumento generado será α = A1, P, hadromerida, D1. 6.1 Generación de Contraargumentos Como se mencionó anteriormente, los agentes pueden intentar refutar argumentos generando contraargumentos o encontrando contraejemplos.",
        "Permítanos explicar cómo pueden ser generados.",
        "Un agente Ai quiere generar un contraargumento β para rebatir un argumento α cuando α está en contradicción con la base de casos locales de Ai.",
        "Además, al generar dicho contraargumento β, Ai espera que β sea preferido sobre α.",
        "Con ese propósito, presentaremos una política específica para generar contraargumentos basados en el criterio de especificidad [10].",
        "El criterio de especificidad se utiliza ampliamente en marcos deductivos para la argumentación, y establece que entre dos argumentos en conflicto, se debe preferir el más específico ya que, en principio, está más informado.",
        "Por lo tanto, se espera que los contraargumentos generados basados en el criterio de especificidad sean preferibles (ya que están más informados) que los argumentos que intentan refutar.",
        "Sin embargo, no hay garantía de que tales contraargumentos siempre ganen, ya que, como hemos indicado en la Sección 5, los agentes en nuestro marco utilizan una relación de preferencia basada en la confianza conjunta.",
        "Además, uno podría pensar que sería mejor que los agentes generaran contraargumentos basados en la relación de preferencia de confianza conjunta; sin embargo, no es obvio cómo generar contraargumentos basados en la confianza conjunta de manera eficiente, ya que se requiere colaboración para evaluar la confianza conjunta.",
        "Por lo tanto, el agente que genera el contraargumento debe comunicarse constantemente con los otros agentes en cada paso del algoritmo de inducción utilizado para generar contraargumentos (actualmente una de nuestras líneas de investigación futuras).",
        "Por lo tanto, en nuestro marco de trabajo, cuando un agente quiere generar un contraargumento β a un argumento α, β tiene que ser más específico que α (es decir, α.D < β.D).",
        "La generación de contraargumentos utilizando el criterio de especificidad impone algunas restricciones sobre el método de aprendizaje, aunque LID o ID3 pueden adaptarse fácilmente para esta tarea.",
        "Por ejemplo, LID es un algoritmo que genera una descripción desde cero y añade heurísticamente características a ese término.",
        "Por lo tanto, en cada paso, la descripción se vuelve más específica que en el paso anterior, y el número de casos que se incluyen en esa descripción se reduce.",
        "Cuando la descripción cubre solo (o casi solo) casos de una única clase de solución, LID termina y predice esa clase de solución.",
        "Para generar un contraargumento a un argumento α, LID solo tiene que usar como punto de partida la descripción α.D en lugar de empezar desde cero.",
        "De esta manera, la justificación proporcionada por LID siempre estará subsumida por α.D, y por lo tanto, el contraargumento resultante será más específico que α.",
        "Sin embargo, hay que tener en cuenta que LID a veces puede no ser capaz de generar contraargumentos, ya que LID puede no ser capaz de especializar aún más la descripción α.D, o porque el agente Ai no tiene ningún caso en Ci que esté subsumido por α.D.",
        "La Figura 4 muestra cómo un agente A2 que no estaba de acuerdo con el argumento mostrado en la Figura 3, genera un contraargumento utilizando LID.",
        "Además, la Figura 4 muestra la generación de un contraargumento β1 2 para el argumento α0 1 (en la Figura 3) que es una especialización de α0 1. 978 The Sixth Intl.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Solución: hadromerida Justificación: D1 Esponja Esqueleto espiculado Características externas Características externas Gemmulas: no Esqueleto espiculado Megascleres Longitud uniforme: no Megascleres Forma lisa: tilostilo Caso Base de A1 LID Nueva esponja P Figura 3: Ejemplo de una justificación real generada por LID en el conjunto de datos de esponjas marinas.",
        "Específicamente, en nuestros experimentos, cuando un agente Ai quiere refutar un argumento α, utiliza la siguiente política: 1.",
        "El Agente Ai utiliza LID para intentar encontrar un contraargumento β más específico que α; si se encuentra, β se envía al otro agente como un contraargumento de α. 2.",
        "Si no se encuentra, entonces Ai busca un contraejemplo c ∈ Ci de α.",
        "Si se encuentra un caso c, entonces c se envía al otro agente como un contraejemplo de α.",
        "Si no se encuentran contraejemplos, entonces Ai no puede refutar el argumento α. 7.",
        "APRENDIZAJE MULTIAGENTE BASADO EN ARGUMENTACIÓN El protocolo de interacción de AMAL permite que un grupo de agentes A1, ..., An deliberen sobre la solución correcta de un problema P mediante un proceso de argumentación.",
        "Si el proceso de argumentación llega a una solución consensuada, la deliberación conjunta termina; de lo contrario, se utiliza una votación ponderada para determinar la solución conjunta.",
        "Además, AMAL también permite a los agentes aprender de los contraejemplos recibidos de otros agentes.",
        "El protocolo AMAL consiste en una serie de rondas.",
        "En la ronda inicial, cada agente declara cuál es su predicción individual para P. Luego, en cada ronda, un agente puede intentar refutar la predicción hecha por cualquiera de los otros agentes.",
        "El protocolo utiliza un mecanismo de paso de token para que los agentes (uno a la vez) puedan enviar contraargumentos o contraejemplos si no están de acuerdo con la predicción hecha por cualquier otro agente.",
        "Específicamente, a cada agente se le permite enviar un contraargumento o contraejemplo cada vez que recibe el token (nota que esta restricción es solo para simplificar el protocolo, y que no restringe la cantidad de contraargumentos que un agente puede enviar, ya que pueden ser retrasados para rondas posteriores).",
        "Cuando un agente recibe un contraargumento o contraejemplo, informa a los otros agentes si acepta el contraargumento (y cambia su predicción) o no.",
        "Además, los agentes también tienen la oportunidad de responder a los contraargumentos cuando reciben el token, intentando generar un contraargumento al contraargumento.",
        "Cuando todos los agentes hayan tenido el token una vez, el token regresa al primer agente, y así sucesivamente.",
        "Si en algún momento del protocolo, todos los agentes están de acuerdo o durante las últimas n rondas ningún agente ha generado ninguna contra-argumentación, el protocolo termina.",
        "Además, si al final de la argumentación los agentes no han llegado a un acuerdo, entonces se utiliza un mecanismo de votación que utiliza la confianza de cada predicción como pesos para decidir la solución final (Así, AMAL sigue el mismo mecanismo que los comités humanos, primero cada miembro individual de un comité expone sus argumentos y discute los de los otros miembros (deliberación conjunta), y si no se alcanza un consenso, entonces se requiere un mecanismo de votación).",
        "En cada iteración, los agentes pueden usar los siguientes performativos: • afirmar(α): la predicción justificada que se mantendrá durante la próxima ronda será α.",
        "Un agente solo puede tener una única predicción en cada ronda, por lo tanto, si se envían múltiples afirmaciones, solo se considera la última como la predicción actualmente mantenida. • rebut(β, α): el agente ha encontrado un contraargumento β a la predicción α.",
        "Definiremos Ht = αt 1, ..., αt n como las predicciones que cada uno de los n agentes tiene en una ronda t. Además, también definiremos contradict(αt i) = {α ∈ Ht|α.S = αt i.S} como el conjunto de argumentos contradictorios para un agente Ai en una ronda t, es decir, el conjunto de argumentos en la ronda t que respaldan una clase de solución diferente a la de αt i.",
        "El protocolo se inicia porque uno de los agentes recibe un problema P que debe ser resuelto.",
        "Después de eso, el agente informa a todos los demás agentes sobre el problema P a resolver, y el protocolo comienza: 1.",
        "En la ronda t = 0, cada uno de los agentes resuelve individualmente P y construye una predicción justificada utilizando su propio método de RBC.",
        "Entonces, cada agente Ai envía el assertivo performativo assert(α0 i) a los otros agentes.",
        "Por lo tanto, los agentes conocen H0 = α0 i , ..., α0 n.",
        "Una vez que todas las predicciones hayan sido enviadas, el token se le da al primer agente A1. 2.",
        "En cada ronda t (distinta de 0), los agentes verifican si sus argumentos en Ht coinciden.",
        "Si lo hacen, el protocolo avanza al paso 5.",
        "Además, si durante las últimas n rondas ningún agente ha enviado ningún contraejemplo o contraargumento, el protocolo también avanza al paso 5.",
        "De lo contrario, el agente Ai propietario del token intenta generar un contraargumento para cada uno de los argumentos opuestos en contradict(αt i) ⊆ Ht (ver Sección 6.1).",
        "Entonces, se selecciona el contraargumento βt i contra la predicción αt j con la menor confianza C(αt j) (ya que αt j es la predicción más probable de ser refutada con éxito). • Si βt i es un contraargumento, entonces, Ai compara localmente αt i con βt i evaluando su confianza contra su base de casos individuales Ci (ver Sección 5) (nótese que Ai está comparando su argumento previo con el contraargumento que Ai mismo acaba de generar y que trata sobre The Sixth Intl.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 979 Esponja Esqueleto espiculado Características externas Características externas Gemas: no Crecimiento: Esqueleto espiculado Megascleres Longitud uniforme: no Megascleres Forma lisa: tilostilo Crecimiento Crecimiento: masivo Caso Base de A2 LID Solución: astrophorida Justificación: D2 Figura 4: Generación de un contraargumento utilizando LID en el conjunto de datos de esponjas. para enviar a Aj.",
        "Si CAi (βt i) > CAi (αt i), entonces Ai considera que βt i es más fuerte que su argumento anterior, cambia su argumento a βt i enviando assert(βt i) al resto de los agentes (la intuición detrás de esto es que dado que un contraargumento también es un argumento, Ai verifica si el nuevo contraargumento es un mejor argumento que el que sostenía anteriormente) y rebut(βt i, αt j) a Aj.",
        "De lo contrario (es decir,",
        "Si CAi (βt i ) ≤ CAi (αt i), Ai solo enviará rebut(βt i , αt j) a Aj.",
        "En cualquiera de las dos situaciones, el protocolo avanza al paso 3. • Si βt i es un contraejemplo c, entonces Ai envía una refutación(c, αt j) a Aj.",
        "El protocolo avanza al paso 4. • Si Ai no puede generar ningún contraargumento o contraejemplo, el token se envía al siguiente agente, comienza una nueva ronda t + 1, y el protocolo avanza al estado 2. 3.",
        "El agente Aj que ha recibido el contraargumento βt i, lo compara localmente con su propio argumento, αt j, evaluando localmente su confianza.",
        "Si CAj (βt i ) > CAj (αt j), entonces Aj aceptará el contraargumento como más fuerte que su propio argumento, y enviará assert(βt i ) a los otros agentes.",
        "De lo contrario (es decir,",
        "Si CAj (βt i ) ≤ CAj (αt j)), Aj no aceptará el contraargumento e informará a los otros agentes en consecuencia.",
        "Cualquiera de las dos situaciones inicia una nueva ronda t + 1, Ai envía el token al siguiente agente y el protocolo vuelve al estado 2. 4.",
        "El agente Aj que ha recibido el contraejemplo c lo retiene en su base de casos y genera un nuevo argumento αt+1 j que tiene en cuenta c, e informa al resto de los agentes enviando assert(αt+1 j) a todos ellos.",
        "Entonces, Ai envía el token al siguiente agente, comienza una nueva ronda t + 1 y el protocolo vuelve al paso 2. 5.",
        "El protocolo finaliza generando una predicción conjunta, de la siguiente manera: si los argumentos en Ht están de acuerdo, entonces su predicción es la predicción conjunta; de lo contrario, se utiliza un mecanismo de votación para decidir la predicción conjunta.",
        "El mecanismo de votación utiliza la medida de confianza conjunta como los pesos de votación, de la siguiente manera: S = arg max Sk∈S αi∈Ht|αi.S=Sk C(αi) Además, para evitar iteraciones infinitas, si un agente envía dos veces el mismo argumento o contraargumento al mismo agente, el mensaje no se considera. 8.",
        "EJEMPLIFICACIÓN Consideremos un sistema compuesto por tres agentes A1, A2 y A3.",
        "Uno de los agentes, A1, recibe un problema P para resolver y decide usar AMAL para resolverlo.",
        "Por esa razón, invita a A2 y A3 a participar en el proceso de argumentación.",
        "Aceptan la invitación y comienza el protocolo de argumentación.",
        "Inicialmente, cada agente genera su predicción individual para P y la transmite a los otros agentes.",
        "Por lo tanto, todos ellos pueden calcular H0 = α0 1, α0 2, α0 3.",
        "En particular, en este ejemplo: • α0 1 = A1, P, hadromerida, D1 • α0 2 = A2, P, astrophorida, D2 • α0 3 = A3, P, axinellida, D3 A1 comienza poseyendo el token e intenta generar argumentos en contra de α0 2 y α0 3, pero no tiene éxito; sin embargo, tiene un contraejemplo c13 para α0 3.",
        "Por lo tanto, A1 envía el mensaje rebut(c13, α03) a A3.",
        "A3 incorpora c13 en su base de casos e intenta resolver el problema P nuevamente, ahora teniendo en cuenta c13.",
        "A3 elabora la predicción justificada α1 3 = A3, P, hadromerida, D4, y la transmite al resto de los agentes con el mensaje assert(α1 3).",
        "Por lo tanto, todos ellos conocen el nuevo H1 = α0 1, α0 2, α1 3.",
        "La ronda 1 comienza y A2 obtiene el token.",
        "A2 intenta generar contraargumentos para α0 1 y α1 3 y solo logra generar un contraargumento β1 2 = A2, P, astrophorida, D5 contra α1 3.",
        "El contraargumento se envía a A3 con el mensaje rebut(β1 2 , α1 3).",
        "El agente A3 recibe el contraargumento y evalúa su confianza local.",
        "El resultado es que la confianza individual del contraargumento β1 2 es menor que la confianza local de α1 3.",
        "Por lo tanto, A3 no acepta el contraargumento, y así H2 = α0 1, α0 2, α1 3.",
        "La ronda 2 comienza y A3 recibe el token.",
        "A3 genera un contraargumento β2 3 = A3, P, hadromerida, D6 para α0 2 y lo envía a A2 con el mensaje refutar(β2 3 , α0 2).",
        "El agente A2 recibe el contraargumento y evalúa su confianza local.",
        "El resultado es que la confianza local del contraargumento β2 3 es mayor que la confianza local de α0 2.",
        "Por lo tanto, A2 acepta el contraargumento e informa al resto de los agentes con el mensaje assert(β2 3).",
        "Después de eso, H3 = α0 1, β2 3 , α1 3.",
        "En la Ronda 3, dado que todos los agentes están de acuerdo (todas las predicciones justificadas en H3 predicen hadromerida como la clase de solución), el protocolo finaliza, y A1 (el agente que recibió el problema) considera hadromerida como la solución conjunta para el problema P. 9.",
        "EVALUACIÓN EXPERIMENTAL 980 El Sexto Internacional.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) SPONGE 75 77 79 81 83 85 87 89 91 2 3 4 5 AMAL Votación Individual SOYBEAN 55 60 65 70 75 80 85 90 2 3 4 5 AMAL Votación Individual Figura 5: Precisión individual y conjunta para 2 a 5 agentes.",
        "En esta sección evaluamos empíricamente el marco de argumentación AMAL.",
        "Hemos realizado experimentos en dos conjuntos de datos diferentes: soja (del repositorio de aprendizaje automático de UCI) y esponja (un conjunto de datos relacional).",
        "El conjunto de datos de soja tiene 307 ejemplos y 19 clases de solución, mientras que el conjunto de datos de esponja tiene 280 ejemplos y 3 clases de solución.",
        "En una ejecución experimental, el conjunto de datos se divide en 2 conjuntos: el conjunto de entrenamiento y el conjunto de prueba.",
        "Los ejemplos del conjunto de entrenamiento están distribuidos entre 5 agentes diferentes sin replicación, es decir, no hay ningún ejemplo compartido por dos agentes.",
        "En la etapa de pruebas, los problemas en el conjunto de pruebas llegan aleatoriamente a uno de los agentes, y su objetivo es predecir la solución correcta.",
        "Los experimentos están diseñados para probar dos hipótesis: (H1) que la argumentación es un marco útil para la deliberación conjunta y puede mejorar sobre otros métodos típicos como la votación; y (H2) que el aprendizaje a través de la comunicación mejora el rendimiento individual de un agente de aprendizaje que participa en un proceso de argumentación.",
        "Además, también esperamos que la mejora lograda a través de la argumentación aumente a medida que aumente el número de agentes que participan en la argumentación (ya que se tomará en cuenta más información).",
        "En relación con H1 (la argumentación es un marco útil para la deliberación conjunta), realizamos 4 experimentos, utilizando 2, 3, 4 y 5 agentes respectivamente (en todos los experimentos cada agente tiene un 20% de los datos de entrenamiento, ya que el entrenamiento siempre se distribuye entre 5 agentes).",
        "La Figura 5 muestra el resultado de esos experimentos en los conjuntos de datos de esponjas y soja.",
        "La precisión de la clasificación se representa en el eje vertical, y en el eje horizontal se muestra el número de agentes que participaron en los procesos de argumentación.",
        "Para cada número de agentes, se muestran tres barras: individual, Voting y AMAL.",
        "La barra individual muestra la precisión promedio de las predicciones de los agentes individuales; la barra de votación muestra la precisión promedio de la predicción conjunta lograda mediante votación pero sin argumentación; y finalmente la barra de AMAL muestra la precisión promedio de la predicción conjunta utilizando argumentación.",
        "Los resultados mostrados son el promedio de 5 ejecuciones de validación cruzada de 10 pliegues.",
        "La Figura 5 muestra que la colaboración (votación y AMAL) supera el rendimiento de la resolución individual de problemas.",
        "Además, como esperábamos, la precisión mejora a medida que más agentes colaboran, ya que se tiene en cuenta más información.",
        "También podemos ver que AMAL siempre supera al voto estándar, demostrando que las decisiones conjuntas se basan en una mejor información proporcionada por el proceso de argumentación.",
        "Por ejemplo, la precisión conjunta para 2 agentes en el conjunto de datos de esponjas es del 87.57% para AMAL y del 86.57% para la votación (mientras que la precisión individual es solo del 80.07%).",
        "Además, la mejora lograda por AMAL sobre Voting es aún mayor en el conjunto de datos de soja.",
        "La razón es que el conjunto de datos de soja es más difícil (en el sentido de que los agentes necesitan más datos para producir buenas predicciones).",
        "Estos resultados experimentales muestran que AMAL aprovecha de manera efectiva la oportunidad de mejora: la precisión es mayor solo porque más agentes han cambiado su opinión durante la argumentación (de lo contrario, lograrían el mismo resultado que la Votación).",
        "En relación con H2 (el aprendizaje a partir de la comunicación en procesos de argumentación mejora la predicción individual), llevamos a cabo el siguiente experimento: inicialmente, distribuimos un 25% del conjunto de entrenamiento entre los cinco agentes; después, el resto de los casos en el conjunto de entrenamiento se envía a los agentes uno por uno; cuando un agente recibe un nuevo caso de entrenamiento, tiene varias opciones: el agente puede descartarlo, el agente puede retenerlo, o el agente puede usarlo para participar en un proceso de argumentación.",
        "La Figura 6 muestra el resultado de ese experimento para los dos conjuntos de datos.",
        "La Figura 6 contiene tres gráficos, donde NL (no aprendizaje) muestra la precisión de un agente sin aprendizaje alguno; L (aprendizaje) muestra la evolución de la precisión de clasificación individual cuando los agentes aprenden reteniendo los casos de entrenamiento que reciben individualmente (notar que cuando todos los casos de entrenamiento han sido retenidos al 100%, la precisión debería ser igual a la de la Figura 5 para los agentes individuales); y finalmente LFC (aprendizaje a partir de la comunicación) muestra la evolución de la precisión de clasificación individual de los agentes de aprendizaje que también aprenden reteniendo los contraejemplos recibidos durante la argumentación (es decir, aprenden tanto de ejemplos de entrenamiento como de contraejemplos).",
        "La Figura 6 muestra que si un agente Ai también aprende de la comunicación, Ai puede mejorar significativamente su rendimiento individual con solo un pequeño número de casos adicionales (seleccionados como contraejemplos relevantes para Ai durante la argumentación).",
        "Por ejemplo, en el conjunto de datos de soja, los agentes individuales han logrado una precisión del 70.62% cuando también aprenden de la comunicación, en comparación con una precisión del 59.93% cuando solo aprenden de su experiencia individual.",
        "El número de casos aprendidos a través de la comunicación depende de las propiedades del conjunto de datos: en el conjunto de datos de esponjas, los agentes solo han retenido muy pocos casos adicionales y han mejorado significativamente la precisión individual; es decir, retienen en promedio 59.96 casos (en comparación con los 50.4 casos retenidos si no aprenden de la comunicación).",
        "En el conjunto de datos de soja se aprenden más contraejemplos para mejorar significativamente la precisión individual, es decir, retienen en promedio 87.16 casos (en comparación con 55.27 casos retenidos si no aprenden de la comunicación).",
        "Finalmente, el hecho de que ambos conjuntos de datos muestren una mejora significativa señala la naturaleza adaptativa del enfoque basado en la argumentación para el aprendizaje a partir de la comunicación: los casos útiles se seleccionan como contraejemplos (y no más de los necesarios), y tienen el efecto deseado. 10.",
        "TRABAJO RELACIONADO En cuanto al RCB en un entorno multiagente, la primera investigación se centró en la recuperación de casos negociada [11] entre grupos de agentes.",
        "Nuestro trabajo sobre el aprendizaje basado en casos multiagente comenzó en 1999; más tarde Mc Ginty y Smyth presentaron un enfoque colaborativo de CBR multiagente (CCBR) para la planificación.",
        "Finalmente, otro enfoque interesante es el razonamiento de múltiples bases de casos (MCBR) [5], que trata sobre The Sixth Intl.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 981 ESPONJA 60 65 70 75 80 85 25% 40% 55% 70% 85% 100% LFC L NL SOJA 20 30 40 50 60 70 80 90 25% 40% 55% 70% 85% 100% LFC L NL Figura 6: Aprendizaje a partir de la comunicación resultante de la argumentación en un sistema compuesto por 5 agentes. sistemas distribuidos donde hay varias bases de casos disponibles para la misma tarea y aborda los problemas de adaptación entre bases de casos.",
        "La principal diferencia es que nuestro enfoque MAC es una forma de distribuir el proceso de Reutilización de CBR (utilizando un sistema de votación) mientras que la Recuperación se realiza individualmente por cada agente; sin embargo, los otros enfoques de CBR multiagente se centran en distribuir el proceso de Recuperación.",
        "La investigación sobre la argumentación en MAS se centra en varios temas como a) lógicas, protocolos y lenguajes que respaldan la argumentación, b) selección de argumentos y c) interpretación de argumentos.",
        "Los enfoques para la lógica y los lenguajes que respaldan la argumentación incluyen la lógica no definitiva [4] y los modelos BDI [13].",
        "Aunque la selección de argumentos es un aspecto clave de la argumentación automatizada (ver [12] y [13]), la mayoría de las investigaciones se han centrado en las relaciones de preferencia entre argumentos.",
        "En nuestro marco de trabajo hemos abordado tanto la selección de argumentos como las relaciones de preferencia utilizando un enfoque basado en casos. 11.",
        "CONCLUSIONES Y TRABAJO FUTURO En este artículo hemos presentado un marco basado en argumentación para el aprendizaje multiagente.",
        "Específicamente, hemos presentado AMAL, un marco que permite a un grupo de agentes de aprendizaje discutir sobre la solución de un problema dado y hemos demostrado cómo las capacidades de aprendizaje pueden ser utilizadas para generar argumentos y contraargumentos.",
        "La evaluación experimental muestra que la cantidad aumentada de información proporcionada a los agentes por el proceso de argumentación aumenta su precisión predictiva, especialmente cuando un número adecuado de agentes participa en la argumentación.",
        "Las principales contribuciones de este trabajo son: a) un marco de argumentación para agentes de aprendizaje; b) una relación de preferencia basada en casos sobre argumentos, basada en el cálculo de una estimación general de confianza de los argumentos; c) una política basada en casos para generar contraargumentos y seleccionar contraejemplos; y d) un enfoque basado en la argumentación para aprender a partir de la comunicación.",
        "Finalmente, en los experimentos presentados aquí, un agente de aprendizaje retendría todos los contraejemplos presentados por el otro agente; sin embargo, esta es una política de retención de casos muy simple, y nos gustaría experimentar con políticas más informadas, con el objetivo de que los agentes de aprendizaje individuales puedan mejorar significativamente utilizando solo un pequeño conjunto de casos propuestos por otros agentes.",
        "Finalmente, nuestro enfoque se centra en el aprendizaje perezoso, y los trabajos futuros tienen como objetivo incorporar el aprendizaje inductivo ansioso dentro del marco argumentativo para el aprendizaje a partir de la comunicación. 12.",
        "REFERENCIAS [1] Agnar Aamodt y Enric Plaza.",
        "Razonamiento basado en casos: Aspectos fundamentales, variaciones metodológicas y enfoques de sistemas.",
        "Comunicaciones de Inteligencia Artificial, 7(1):39-59, 1994. [2] E. Armengol y E. Plaza.",
        "Inducción perezosa de descripciones para el aprendizaje basado en casos relacionales.",
        "En ECML2001, páginas 13-24, 2001. [3] Gerhard Brewka.",
        "Sistemas de argumentación dinámica: Un modelo formal de procesos de argumentación basado en el cálculo de situaciones.",
        "Revista de Lógica y Computación, 11(2):257-282, 2001. [4] Carlos I. Chesñevar y Guillermo R. Simari.",
        "Formalizando la argumentación no definitiva utilizando sistemas deductivos etiquetados.",
        "Revista de Ciencias de la Computación y Tecnología, 1(4):18-33, 2000. [5] D. Leake y R. Sooriamurthi.",
        "Seleccionando estrategias automáticamente para el razonamiento multi-base de casos.",
        "En S. Craw y A. Preece, editores, ECCBR2002, páginas 204-219, Berlín, 2002.",
        "Springer Verlag. [6] Francisco J. Martín, Enric Plaza y Josep-Lluis Arcos.",
        "Reutilización de conocimiento y experiencia a través de comunicaciones entre agentes competentes (pares).",
        "Revista Internacional de Ingeniería de Software e Ingeniería del Conocimiento, 9(3):319-341, 1999. [7] Lorraine McGinty y Barry Smyth.",
        "Razonamiento basado en casos colaborativo: Aplicaciones en la planificación de rutas personalizadas.",
        "En I. Watson y Q. Yang, editores, ICCBR, número 2080 en LNAI, páginas 362-376.",
        "Springer-Verlag, 2001. [8] Santi Ontañón y Enric Plaza.",
        "Aprendizaje multiagente basado en justificaciones.",
        "En ICML2003, páginas 576-583.",
        "Morgan Kaufmann, 2003. [9] Enric Plaza, Eva Armengol y Santiago Ontañón.",
        "El poder explicativo de la similitud simbólica en el razonamiento basado en casos.",
        "Revisión de Inteligencia Artificial, 24(2):145-161, 2005. [10] David Poole.",
        "En la comparación de teorías: prefiriendo la explicación más específica.",
        "En IJCAI-85, páginas 144-147, 1985. [11] M V Nagendra Prassad, Victor R Lesser y Susan Lander.",
        "Recuperación y razonamiento en bases de casos distribuidas.",
        "Informe técnico, Departamento de Ciencias de la Computación de UMass, 1995. [12] K. Sycara S. Kraus y A. Evenchik.",
        "Alcanzando acuerdos a través de la argumentación: un modelo lógico y su implementación.",
        "Revista de Inteligencia Artificial, 104:1-69, 1998. [13] N. R. Jennings, S. Parsons, C. Sierra.",
        "Agentes que razonan y negocian mediante argumentos.",
        "Revista de Lógica y Computación, 8:261-292, 1998. [14] Bruce A. Wooley.",
        "Componente de explicación de sistemas de software.",
        "ACM CrossRoads, 5.1, 1998. 982 The Sixth Intl. \n\nACM CrossRoads, 5.1, 1998. 982 El Sexto Internacional.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07)"
    ],
    "error_count": 2,
    "keys": {
        "multi-agent system": {
            "translated_key": "sistema multiagente",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning and Joint Deliberation through Argumentation in Multi-Agent Systems Santi Ontañón CCL, Cognitive Computing Lab Georgia Institute of Technology Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Artificial Intelligence Research Institute CSIC, Spanish Council for Scientific Research Campus UAB, 08193 Bellaterra, Catalonia (Spain) enric@iiia.csic.es ABSTRACT In this paper we will present an argumentation framework for learning agents (AMAL) designed for two purposes: (1) for joint deliberation, and (2) for learning from communication.",
                "The AMAL framework is completely based on learning from examples: the argument preference relation, the argument generation policy, and the counterargument generation policy are case-based techniques.",
                "For join deliberation, learning agents share their experience by forming a committee to decide upon some joint decision.",
                "We experimentally show that the argumentation among committees of agents improves both the individual and joint performance.",
                "For learning from communication, an agent engages into arguing with other agents in order to contrast its individual hypotheses and receive counterexamples; the argumentation process improves their learning scope and individual performance.",
                "Categories and Subject Descriptors I.2.6 [Artificial Intelligence]: Learning; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems, Intelligent Agents 1.",
                "INTRODUCTION Argumentation frameworks for multi-agent systems can be used for different purposes like joint deliberation, persuasion, negotiation, and conflict resolution.",
                "In this paper we will present an argumentation framework for learning agents, and show that it can be used for two purposes: (1) joint deliberation, and (2) learning from communication.",
                "Argumentation-based joint deliberation involves discussion over the outcome of a particular situation or the appropriate course of action for a particular situation.",
                "Learning agents are capable of learning from experience, in the sense that past examples (situations and their outcomes) are used to predict the outcome for the situation at hand.",
                "However, since individual agents experience may be limited, individual knowledge and prediction accuracy is also limited.",
                "Thus, learning agents that are capable of arguing their individual predictions with other agents may reach better prediction accuracy after such an argumentation process.",
                "Most existing argumentation frameworks for multi-agent systems are based on deductive logic or some other deductive logic formalism specifically designed to support argumentation, such as default logic [3]).",
                "Usually, an argument is seen as a logical statement, while a counterargument is an argument offered in opposition to another argument [4, 13]; agents use a preference relation to resolve conflicting arguments.",
                "However, logic-based argumentation frameworks assume agents with preloaded knowledge and preference relation.",
                "In this paper, we focus on an Argumentation-based Multi-Agent Learning (AMAL) framework where both knowledge and preference relation are learned from experience.",
                "Thus, we consider a scenario with agents that (1) work in the same domain using a shared ontology, (2) are capable of learning from examples, and (3) communicate using an argumentative framework.",
                "Having learning capabilities allows agents effectively use a specific form of counterargument, namely the use of counterexamples.",
                "Counterexamples offer the possibility of agents learning during the argumentation process.",
                "Moreover, learning agents allow techniques that use learnt experience to generate adequate arguments and counterarguments.",
                "Specifically, we will need to address two issues: (1) how to define a technique to generate arguments and counterarguments from examples, and (2)how to define a preference relation over two conflicting arguments that have been induced from examples.",
                "This paper presents a case-based approach to address both issues.",
                "The agents use case-based reasoning (CBR) [1] to learn from past cases (where a case is a situation and its outcome) in order to predict the outcome of a new situation.",
                "We propose an argumentation protocol inside the AMAL framework at supports agents in reaching a joint prediction over a specific situation or problem - moreover, the reasoning needed to support the argumentation process will also be based on cases.",
                "In particular, we present two case-based measures, one for generating the arguments and counterarguments adequate to a particular situation and another for determining preference relation among arguments.",
                "Finally, we evaluate (1) if argumentation between learning agents can produce a joint prediction that improves over individual learning performance and (2) if learning from the counterexamples conveyed during the argumentation process increases the individual performance with precisely those cases being used while arguing among them.",
                "The paper is structured as follows.",
                "Section 2 discusses the relation among argumentation, collaboration and learning.",
                "Then Section 3 introduces our multi-agent CBR (MAC) framework and the notion of justified prediction.",
                "After that, Section 4 formally defines our argumentation framework.",
                "Sections 5 and 6 present our case-based preference relation and argument generation policies respectively.",
                "Later, Section 7 presents the argumentation protocol in our AMAL framework.",
                "After that, Section 8 presents an exemplification of the argumentation framework.",
                "Finally, Section 9 presents an empirical evaluation of our two main hypotheses.",
                "The paper closes with related work and conclusions sections. 2.",
                "ARGUMENTATION,COLLABORATION AND LEARNING Both learning and collaboration are ways in which an agent can improve individual performance.",
                "In fact, there is a clear parallelism between learning and collaboration in multi-agent systems, since both are ways in which agents can deal with their shortcomings.",
                "Let us show which are the main motivations that an agent can have to learn or to collaborate. • Motivations to learn: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems. • Motivations to collaborate: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems, - Increase the range of accessible resources.",
                "Looking at the above lists of motivation, we can easily see that learning and collaboration are very related in multi-agent systems.",
                "In fact, with the exception of the last item in the motivations to collaborate list, they are two extremes of a continuum of strategies to improve performance.",
                "An agent may choose to increase performance by learning, by collaborating, or by finding an intermediate point that combines learning and collaboration in order to improve performance.",
                "In this paper we will propose AMAL, an argumentation framework for learning agents, and will also also show how AMAL can be used both for learning from communication and for solving problems in a collaborative way: • Agents can solve problems in a collaborative way via engaging an argumentation process about the prediction for the situation at hand.",
                "Using this collaboration, the prediction can be done in a more informed way, since the information known by several agents has been taken into account. • Agents can also learn from communication with other agents by engaging an argumentation process.",
                "Agents that engage in such argumentation processes can learn from the arguments and counterexamples received from other agents, and use this information for predicting the outcomes of future situations.",
                "In the rest of this paper we will propose an argumentation framework and show how it can be used both for learning and for solving problems in a collaborative way. 3.",
                "MULTI-AGENT CBR SYSTEMS A Multi-Agent Case Based Reasoning System (MAC) M = {(A1, C1), ..., (An, Cn)} is a <br>multi-agent system</br> composed of A = {Ai, ..., An}, a set of CBR agents, where each agent Ai ∈ A possesses an individual case base Ci.",
                "Each individual agent Ai in a MAC is completely autonomous and each agent Ai has access only to its individual and private case base Ci.",
                "A case base Ci = {c1, ..., cm} is a collection of cases.",
                "Agents in a MAC system are able to individually solve problems, but they can also collaborate with other agents to solve problems.",
                "In this framework, we will restrict ourselves to analytical tasks, i.e. tasks like classification, where the solution of a problem is achieved by selecting a solution class from an enumerated set of solution classes.",
                "In the following we will note the set of all the solution classes by S = {S1, ..., SK }.",
                "Therefore, a case c = P, S is a tuple containing a case description P and a solution class S ∈ S. In the following, we will use the terms problem and case description indistinctly.",
                "Moreover, we will use the dot notation to refer to elements inside a tuple; e.g., to refer to the solution class of a case c, we will write c.S.",
                "Therefore, we say a group of agents perform joint deliberation, when they collaborate to find a joint solution by means of an argumentation process.",
                "However, in order to do so, an agent has to be able to justify its prediction to the other agents (i.e. generate an argument for its predicted solution that can be examined and critiqued by the other agents).",
                "The next section addresses this issue. 3.1 Justified Predictions Both expert systems and CBR systems may have an explanation component [14] in charge of justifying why the system has provided a specific answer to the user.",
                "The line of reasoning of the system can then be examined by a human expert, thus increasing the reliability of the system.",
                "Most of the existing work on explanation generation focuses on generating explanations to be provided to the user.",
                "However, in our approach we use explanations (or justifications) as a tool for improving communication and coordination among agents.",
                "We are interested in justifications since they can be used as arguments.",
                "For that purpose, we will benefit from the ability of some machine learning methods to provide justifications.",
                "A justification built by a CBR method after determining that the solution of a particular problem P was Sk is a description that contains the relevant information from the problem P that the CBR method has considered to predict Sk as the solution of P. In particular, CBR methods work by retrieving similar cases to the problem at hand, and then reusing their solutions for the current problem, expecting that since the problem and the cases are similar, the solutions will also be similar.",
                "Thus, if a CBR method has retrieved a set of cases C1, ..., Cn to solve a particular problem P the justification built will contain the relevant information from the problem P that made the CBR system retrieve that particular set of cases, i.e. it will contain the relevant information that P and C1, ..., Cn have in common.",
                "For example, Figure 1 shows a justification build by a CBR system for a toy problem (in the following sections we will show justifications for real problems).",
                "In the figure, a problem has two attributes (Traffic_light, and Cars_passing), the retrieval mechanism of the CBR system notices that by considering only the attribute Traffic_light, it can retrieve two cases that predict the same solution: wait.",
                "Thus, since only this attribute has been used, it is the only one appearing in the justification.",
                "The values of the rest of attributes are irrelevant, since whatever their value the solution class would have been the same. 976 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Problem Traffic_light: red Cars_passing: no Case 1 Traffic_light: red Cars_passing: no Solution: wait Case 3 Traffic_light: red Cars_passing: yes Solution: wait Case 4 Traffic_light: green Cars_passing: yes Solution: wait Case 2 Traffic_light: green Cars_passing: no Solution: cross Retrieved cases Solution: wait Justification Traffic_light: red Figure 1: An example of justification generation in a CBR system.",
                "Notice that, since the only relevant feature to decide is Traffic_light (the only one used to retrieve cases), it is the only one appearing in the justification.",
                "In general, the meaning of a justification is that all (or most of) the cases in the case base of an agent that satisfy the justification (i.e. all the cases that are subsumed by the justification) belong to the predicted solution class.",
                "In the rest of the paper, we will use to denote the subsumption relation.",
                "In our work, we use LID [2], a CBR method capable of building symbolic justifications such as the one exemplified in Figure 1.",
                "When an agent provides a justification for a prediction, the agent generates a justified prediction: DEFINITION 3.1.",
                "A Justified Prediction is a tuple J = A, P, S, D where agent A considers S the correct solution for problem P, and that prediction is justified a symbolic description D such that J.D J.P. Justifications can have many uses for CBR systems [8, 9].",
                "In this paper, we are going to use justifications as arguments, in order to allow learning agents to engage in argumentation processes. 4.",
                "ARGUMENTS AND COUNTERARGUMENTS For our purposes an argument α generated by an agent A is composed of a statement S and some evidence D supporting S as correct.",
                "In the remainder of this section we will see how this general definition of argument can be instantiated in specific kind of arguments that the agents can generate.",
                "In the context of MAC systems, agents argue about predictions for new problems and can provide two kinds of information: a) specific cases P, S , and b) justified predictions: A, P, S, D .",
                "Using this information, we can define three types of arguments: justified predictions, counterarguments, and counterexamples.",
                "A justified prediction α is generated by an agent Ai to argue that Ai believes that the correct solution for a given problem P is α.S, and the evidence provided is the justification α.D.",
                "In the example depicted in Figure 1, an agent Ai may generate the argument α = Ai, P, Wait, (Traffic_light = red) , meaning that the agent Ai believes that the correct solution for P is Wait because the attribute Traffic_light equals red.",
                "A counterargument β is an argument offered in opposition to another argument α.",
                "In our framework, a counterargument consists of a justified prediction Aj, P, S , D generated by an agent Aj with the intention to rebut an argument α generated by another agent Ai, that endorses a solution class S different from that of α.S for the problem at hand and justifies this with a justification D .",
                "In the example in Figure 1, if an agent generates the argument α = Ai, P, Walk, (Cars_passing = no) , an agent that thinks that the correct solution is Wait might answer with the counterargument β = Aj, P, Wait, (Cars_passing = no ∧ Traffic_light = red) , meaning that, although there are no cars passing, the traffic light is red, and the street cannot be crossed.",
                "A counterexample c is a case that contradicts an argument α.",
                "Thus a counterexample is also a counterargument, one that states that a specific argument α is not always true, and the evidence provided is the case c. Specifically, for a case c to be a counterexample of an argument α, the following conditions have to be met: α.D c and α.S = c.S, i.e. the case must satisfy the justification α.D and the solution of c must be different than the predicted by α.",
                "By exchanging arguments and counterarguments (including counterexamples), agents can argue about the correct solution of a given problem, i.e. they can engage a joint deliberation process.",
                "However, in order to do so, they need a specific interaction protocol, a preference relation between contradicting arguments, and a decision policy to generate counterarguments (including counterexamples).",
                "In the following sections we will present these elements. 5.",
                "PREFERENCE RELATION A specific argument provided by an agent might not be consistent with the information known to other agents (or even to some of the information known by the agent that has generated the justification due to noise in training data).",
                "For that reason, we are going to define a preference relation over contradicting justified predictions based on cases.",
                "Basically, we will define a confidence measure for each justified prediction (that takes into account the cases owned by each agent), and the justified prediction with the highest confidence will be the preferred one.",
                "The idea behind case-based confidence is to count how many of the cases in an individual case base endorse a justified prediction, and how many of them are counterexamples of it.",
                "The more the endorsing cases, the higher the confidence; and the more the counterexamples, the lower the confidence.",
                "Specifically, to assess the confidence of a justified prediction α, an agent obtains the set of cases in its individual case base that are subsumed by α.D.",
                "With them, an agent Ai obtains the Y (aye) and N (nay) values: • Y Ai α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by the justification α.D that belong to the solution class α.S, • NAi α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by justification α.D that do not belong to that solution class.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 977 + + + + + +  - - + Figure 2: Confidence of arguments is evaluated by contrasting them against the case bases of the agents.",
                "An agent estimates the confidence of an argument as: CAi (α) = Y Ai α 1 + Y Ai α + NAi α i.e. the confidence on a justified prediction is the number of endorsing cases divided by the number of endorsing cases plus counterexamples.",
                "Notice that we add 1 to the denominator, this is to avoid giving excessively high confidences to justified predictions whose confidence has been computed using a small number of cases.",
                "Notice that this correction follows the same idea than the Laplace correction to estimate probabilities.",
                "Figure 2 illustrates the individual evaluation of the confidence of an argument, in particular, three endorsing cases and one counterexample are found in the case base of agents Ai, giving an estimated confidence of 0.6 Moreover, we can also define the joint confidence of an argument α as the confidence computed using the cases present in the case bases of all the agents in the group: C(α) = i Y Ai α 1 + i Y Ai α + NAi α Notice that, to collaboratively compute the joint confidence, the agents only have to make public the aye and nay values locally computed for a given argument.",
                "In our framework, agents use this joint confidence as the preference relation: a justified prediction α is preferred over another one β if C(α) ≥ C(β). 6.",
                "GENERATION OF ARGUMENTS In our framework, arguments are generated by the agents from cases, using learning methods.",
                "Any learning method able to provide a justified prediction can be used to generate arguments.",
                "For instance, decision trees and LID [2] are suitable learning methods.",
                "Specifically, in the experiments reported in this paper agents use LID.",
                "Thus, when an agent wants to generate an argument endorsing that a specific solution class is the correct solution for a problem P, it generates a justified prediction as explained in Section 3.1.",
                "For instance, Figure 3 shows a real justification generated by LID after solving a problem P in the domain of marine sponges identification.",
                "In particular, Figure 3 shows how when an agent receives a new problem to solve (in this case, a new sponge to determine its order), the agent uses LID to generate an argument (consisting on a justified prediction) using the cases in the case base of the agent.",
                "The justification shown in Figure 3 can be interpreted saying that the predicted solution is hadromerida because the smooth form of the megascleres of the spiculate skeleton of the sponge is of type tylostyle, the spikulate skeleton of the sponge has no uniform length, and there is no gemmules in the external features of the sponge.",
                "Thus, the argument generated will be α = A1, P, hadromerida, D1 . 6.1 Generation of Counterarguments As previously stated, agents may try to rebut arguments by generating counterargument or by finding counterexamples.",
                "Let us explain how they can be generated.",
                "An agent Ai wants to generate a counterargument β to rebut an argument α when α is in contradiction with the local case base of Ai.",
                "Moreover, while generating such counterargument β, Ai expects that β is preferred over α.",
                "For that purpose, we will present a specific policy to generate counterarguments based on the specificity criterion [10].",
                "The specificity criterion is widely used in deductive frameworks for argumentation, and states that between two conflicting arguments, the most specific should be preferred since it is, in principle, more informed.",
                "Thus, counterarguments generated based on the specificity criterion are expected to be preferable (since they are more informed) to the arguments they try to rebut.",
                "However, there is no guarantee that such counterarguments will always win, since, as we have stated in Section 5, agents in our framework use a preference relation based on joint confidence.",
                "Moreover, one may think that it would be better that the agents generate counterarguments based on the joint confidence preference relation; however it is not obvious how to generate counterarguments based on joint confidence in an efficient way, since collaboration is required in order to evaluate joint confidence.",
                "Thus, the agent generating the counterargument should constantly communicate with the other agents at each step of the induction algorithm used to generate counterarguments (presently one of our future research lines).",
                "Thus, in our framework, when an agent wants to generate a counterargument β to an argument α, β has to be more specific than α (i.e. α.D < β.D).",
                "The generation of counterarguments using the specificity criterion imposes some restrictions over the learning method, although LID or ID3 can be easily adapted for this task.",
                "For instance, LID is an algorithm that generates a description starting from scratch and heuristically adding features to that term.",
                "Thus, at every step, the description is made more specific than in the previous step, and the number of cases that are subsumed by that description is reduced.",
                "When the description covers only (or almost only) cases of a single solution class LID terminates and predicts that solution class.",
                "To generate a counterargument to an argument α LID just has to use as starting point the description α.D instead of starting from scratch.",
                "In this way, the justification provided by LID will always be subsumed by α.D, and thus the resulting counterargument will be more specific than α.",
                "However, notice that LID may sometimes not be able to generate counterarguments, since LID may not be able to specialize the description α.D any further, or because the agent Ai has no case inCi that is subsumed by α.D.",
                "Figure 4 shows how an agent A2 that disagreed with the argument shown in Figure 3, generates a counterargument using LID.",
                "Moreover, Figure 4 shows the generation of a counterargument β1 2 for the argument α0 1 (in Figure 3) that is a specialization of α0 1. 978 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Solution: hadromerida Justification: D1 Sponge Spikulate skeleton External features External features Gemmules: no Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Case Base of A1 LID New sponge P Figure 3: Example of a real justification generated by LID in the marine sponges data set.",
                "Specifically, in our experiments, when an agent Ai wants to rebut an argument α, uses the following policy: 1.",
                "Agent Ai uses LID to try to find a counterargument β more specific than α; if found, β is sent to the other agent as a counterargument of α. 2.",
                "If not found, then Ai searches for a counterexample c ∈ Ci of α.",
                "If a case c is found, then c is sent to the other agent as a counterexample of α. 3.",
                "If no counterexamples are found, then Ai cannot rebut the argument α. 7.",
                "ARGUMENTATION-BASED MULTI-AGENT LEARNING The interaction protocol of AMAL allows a group of agents A1, ..., An to deliberate about the correct solution of a problem P by means of an argumentation process.",
                "If the argumentation process arrives to a consensual solution, the joint deliberation ends; otherwise a weighted vote is used to determine the joint solution.",
                "Moreover, AMAL also allows the agents to learn from the counterexamples received from other agents.",
                "The AMAL protocol consists on a series of rounds.",
                "In the initial round, each agent states which is its individual prediction for P. Then, at each round an agent can try to rebut the prediction made by any of the other agents.",
                "The protocol uses a token passing mechanism so that agents (one at a time) can send counterarguments or counterexamples if they disagree with the prediction made by any other agent.",
                "Specifically, each agent is allowed to send one counterargument or counterexample each time he gets the token (notice that this restriction is just to simplify the protocol, and that it does not restrict the number of counterargument an agent can sent, since they can be delayed for subsequent rounds).",
                "When an agent receives a counterargument or counterexample, it informs the other agents if it accepts the counterargument (and changes its prediction) or not.",
                "Moreover, agents have also the opportunity to answer to counterarguments when they receive the token, by trying to generate a counterargument to the counterargument.",
                "When all the agents have had the token once, the token returns to the first agent, and so on.",
                "If at any time in the protocol, all the agents agree or during the last n rounds no agent has generated any counterargument, the protocol ends.",
                "Moreover, if at the end of the argumentation the agents have not reached an agreement, then a voting mechanism that uses the confidence of each prediction as weights is used to decide the final solution (Thus, AMAL follows the same mechanism as human committees, first each individual member of a committee exposes his arguments and discuses those of the other members (joint deliberation), and if no consensus is reached, then a voting mechanism is required).",
                "At each iteration, agents can use the following performatives: • assert(α): the justified prediction held during the next round will be α.",
                "An agent can only hold a single prediction at each round, thus is multiple asserts are send, only the last one is considered as the currently held prediction. • rebut(β, α): the agent has found a counterargument β to the prediction α.",
                "We will define Ht = αt 1, ..., αt n as the predictions that each of the n agents hold at a round t. Moreover, we will also define contradict(αt i) = {α ∈ Ht|α.S = αt i.S} as the set of contradicting arguments for an agent Ai in a round t, i.e. the set of arguments at round t that support a different solution class than αt i.",
                "The protocol is initiated because one of the agents receives a problem P to be solved.",
                "After that, the agent informs all the other agents about the problem P to solve, and the protocol starts: 1.",
                "At round t = 0, each one of the agents individually solves P, and builds a justified prediction using its own CBR method.",
                "Then, each agent Ai sends the performative assert(α0 i ) to the other agents.",
                "Thus, the agents know H0 = α0 i , ..., α0 n .",
                "Once all the predictions have been sent the token is given to the first agent A1. 2.",
                "At each round t (other than 0), the agents check whether their arguments in Ht agree.",
                "If they do, the protocol moves to step 5.",
                "Moreover, if during the last n rounds no agent has sent any counterexample or counterargument, the protocol also moves to step 5.",
                "Otherwise, the agent Ai owner of the token tries to generate a counterargument for each of the opposing arguments in contradict(αt i) ⊆ Ht (see Section 6.1).",
                "Then, the counterargument βt i against the prediction αt j with the lowest confidence C(αt j) is selected (since αt j is the prediction more likely to be successfully rebutted). • If βt i is a counterargument, then, Ai locally compares αt i with βt i by assessing their confidence against its individual case base Ci (see Section 5) (notice that Ai is comparing its previous argument with the counterargument that Ai itself has just generated and that is about The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 979 Sponge Spikulate skeleton External features External features Gemmules: no Growing: Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Growing Grow: massive Case Base of A2 LID Solution: astrophorida Justification: D2 Figure 4: Generation of a counterargument using LID in the sponges data set. to send to Aj).",
                "If CAi (βt i ) > CAi (αt i), then Ai considers that βt i is stronger than its previous argument, changes its argument to βt i by sending assert(βt i ) to the rest of the agents (the intuition behind this is that since a counterargument is also an argument, Ai checks if the newly counterargument is a better argument than the one he was previously holding) and rebut(βt i , αt j) to Aj.",
                "Otherwise (i.e.",
                "CAi (βt i ) ≤ CAi (αt i)), Ai will send only rebut(βt i , αt j) to Aj.",
                "In any of the two situations the protocol moves to step 3. • If βt i is a counterexample c, then Ai sends rebut(c, αt j) to Aj.",
                "The protocol moves to step 4. • If Ai cannot generate any counterargument or counterexample, the token is sent to the next agent, a new round t + 1 starts, and the protocol moves to state 2. 3.",
                "The agent Aj that has received the counterargument βt i , locally compares it against its own argument, αt j, by locally assessing their confidence.",
                "If CAj (βt i ) > CAj (αt j), then Aj will accept the counterargument as stronger than its own argument, and it will send assert(βt i ) to the other agents.",
                "Otherwise (i.e.",
                "CAj (βt i ) ≤ CAj (αt j)), Aj will not accept the counterargument, and will inform the other agents accordingly.",
                "Any of the two situations start a new round t + 1, Ai sends the token to the next agent, and the protocol moves back to state 2. 4.",
                "The agent Aj that has received the counterexample c retains it into its case base and generates a new argument αt+1 j that takes into account c, and informs the rest of the agents by sending assert(αt+1 j ) to all of them.",
                "Then, Ai sends the token to the next agent, a new round t + 1 starts, and the protocol moves back to step 2. 5.",
                "The protocol ends yielding a joint prediction, as follows: if the arguments in Ht agree then their prediction is the joint prediction, otherwise a voting mechanism is used to decide the joint prediction.",
                "The voting mechanism uses the joint confidence measure as the voting weights, as follows: S = arg max Sk∈S αi∈Ht|αi.S=Sk C(αi) Moreover, in order to avoid infinite iterations, if an agent sends twice the same argument or counterargument to the same agent, the message is not considered. 8.",
                "EXEMPLIFICATION Let us consider a system composed of three agents A1, A2 and A3.",
                "One of the agents, A1 receives a problem P to solve, and decides to use AMAL to solve it.",
                "For that reason, invites A2 and A3 to take part in the argumentation process.",
                "They accept the invitation, and the argumentation protocol starts.",
                "Initially, each agent generates its individual prediction for P, and broadcasts it to the other agents.",
                "Thus, all of them can compute H0 = α0 1, α0 2, α0 3 .",
                "In particular, in this example: • α0 1 = A1, P, hadromerida, D1 • α0 2 = A2, P, astrophorida, D2 • α0 3 = A3, P, axinellida, D3 A1 starts owning the token and tries to generate counterarguments for α0 2 and α0 3, but does not succeed, however it has one counterexample c13 for α0 3.",
                "Thus, A1 sends the the message rebut( c13, α0 3) to A3.",
                "A3 incorporates c13 into its case base and tries to solve the problem P again, now taking c13 into consideration.",
                "A3 comes up with the justified prediction α1 3 = A3, P, hadromerida, D4 , and broadcasts it to the rest of the agents with the message assert(α1 3).",
                "Thus, all of them know the new H1 = α0 1, α0 2, α1 3 .",
                "Round 1 starts and A2 gets the token.",
                "A2 tries to generate counterarguments for α0 1 and α1 3 and only succeeds to generate a counterargument β1 2 = A2, P, astrophorida, D5 against α1 3.",
                "The counterargument is sent to A3 with the message rebut(β1 2 , α1 3).",
                "Agent A3 receives the counterargument and assesses its local confidence.",
                "The result is that the individual confidence of the counterargument β1 2 is lower than the local confidence of α1 3.",
                "Therefore, A3 does not accept the counterargument, and thus H2 = α0 1, α0 2, α1 3 .",
                "Round 2 starts and A3 gets the token.",
                "A3 generates a counterargument β2 3 = A3, P, hadromerida, D6 for α0 2 and sends it to A2 with the message rebut(β2 3 , α0 2).",
                "Agent A2 receives the counterargument and assesses its local confidence.",
                "The result is that the local confidence of the counterargument β2 3 is higher than the local confidence of α0 2.",
                "Therefore, A2 accepts the counterargument and informs the rest of the agents with the message assert(β2 3 ).",
                "After that, H3 = α0 1, β2 3 , α1 3 .",
                "At Round 3, since all the agents agree (all the justified predictions in H3 predict hadromerida as the solution class) The protocol ends, and A1 (the agent that received the problem) considers hadromerida as the joint solution for the problem P. 9.",
                "EXPERIMENTAL EVALUATION 980 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) SPONGE 75 77 79 81 83 85 87 89 91 2 3 4 5 AMAL Voting Individual SOYBEAN 55 60 65 70 75 80 85 90 2 3 4 5 AMAL Voting Individual Figure 5: Individual and joint accuracy for 2 to 5 agents.",
                "In this section we empirically evaluate the AMAL argumentation framework.",
                "We have made experiments in two different data sets: soybean (from the UCI machine learning repository) and sponge (a relational data set).",
                "The soybean data set has 307 examples and 19 solution classes, while the sponge data set has 280 examples and 3 solution classes.",
                "In an experimental run, the data set is divided in 2 sets: the training set and the test set.",
                "The training set examples are distributed among 5 different agents without replication, i.e. there is no example shared by two agents.",
                "In the testing stage, problems in the test set arrive randomly to one of the agents, and their goal is to predict the correct solution.",
                "The experiments are designed to test two hypotheses: (H1) that argumentation is a useful framework for joint deliberation and can improve over other typical methods such as voting; and (H2) that learning from communication improves the individual performance of a learning agent participating in an argumentation process.",
                "Moreover, we also expect that the improvement achieved from argumentation will increase as the number of agents participating in the argumentation increases (since more information will be taken into account).",
                "Concerning H1 (argumentation is a useful framework for joint deliberation), we ran 4 experiments, using 2, 3, 4, and 5 agents respectively (in all experiments each agent has a 20% of the training data, since the training is always distributed among 5 agents).",
                "Figure 5 shows the result of those experiments in the sponge and soybean data sets.",
                "Classification accuracy is plotted in the vertical axis, and in the horizontal axis the number of agents that took part in the argumentation processes is shown.",
                "For each number of agents, three bars are shown: individual, Voting, and AMAL.",
                "The individual bar shows the average accuracy of individual agents predictions; the voting bar shows the average accuracy of the joint prediction achieved by voting but without any argumentation; and finally the AMAL bar shows the average accuracy of the joint prediction using argumentation.",
                "The results shown are the average of 5 10-fold cross validation runs.",
                "Figure 5 shows that collaboration (voting and AMAL) outperforms individual problem solving.",
                "Moreover, as we expected, the accuracy improves as more agents collaborate, since more information is taken into account.",
                "We can also see that AMAL always outperforms standard voting, proving that joint decisions are based on better information as provided by the argumentation process.",
                "For instance, the joint accuracy for 2 agents in the sponge data set is of 87.57% for AMAL and 86.57% for voting (while individual accuracy is just 80.07%).",
                "Moreover, the improvement achieved by AMAL over Voting is even larger in the soybean data set.",
                "The reason is that the soybean data set is more difficult (in the sense that agents need more data to produce good predictions).",
                "These experimental results show that AMAL effectively exploits the opportunity for improvement: the accuracy is higher only because more agents have changed their opinion during argumentation (otherwise they would achieve the same result as Voting).",
                "Concerning H2 (learning from communication in argumentation processes improves individual prediction ), we ran the following experiment: initially, we distributed a 25% of the training set among the five agents; after that, the rest of the cases in the training set is sent to the agents one by one; when an agent receives a new training case, it has several options: the agent can discard it, the agent can retain it, or the agent can use it for engaging an argumentation process.",
                "Figure 6 shows the result of that experiment for the two data sets.",
                "Figure 6 contains three plots, where NL (not learning) shows accuracy of an agent with no learning at all; L (learning), shows the evolution of the individual classification accuracy when agents learn by retaining the training cases they individually receive (notice that when all the training cases have been retained at 100%, the accuracy should be equal to that of Figure 5 for individual agents); and finally LFC (learning from communication) shows the evolution of the individual classification accuracy of learning agents that also learn by retaining those counterexamples received during argumentation (i.e. they learn both from training examples and counterexamples).",
                "Figure 6 shows that if an agent Ai learns also from communication, Ai can significantly improve its individual performance with just a small number of additional cases (those selected as relevant counterexamples for Ai during argumentation).",
                "For instance, in the soybean data set, individual agents have achieved an accuracy of 70.62% when they also learn from communication versus an accuracy of 59.93% when they only learn from their individual experience.",
                "The number of cases learnt from communication depends on the properties of the data set: in the sponges data set, agents have retained only very few additional cases, and significantly improved individual accuracy; namely they retain 59.96 cases in average (compared to the 50.4 cases retained if they do not learn from communication).",
                "In the soybean data set more counterexamples are learnt to significantly improve individual accuracy, namely they retain 87.16 cases in average (compared to 55.27 cases retained if they do not learn from communication).",
                "Finally, the fact that both data sets show a significant improvement points out the adaptive nature of the argumentation-based approach to learning from communication: the useful cases are selected as counterexamples (and no more than those needed), and they have the intended effect. 10.",
                "RELATED WORK Concerning CBR in a multi-agent setting, the first research was on negotiated case retrieval [11] among groups of agents.",
                "Our work on multi-agent case-based learning started in 1999 [6]; later Mc Ginty and Smyth [7] presented a multi-agent collaborative CBR approach (CCBR) for planning.",
                "Finally, another interesting approach is multi-case-base reasoning (MCBR) [5], that deals with The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 981 SPONGE 60 65 70 75 80 85 25% 40% 55% 70% 85% 100% LFC L NL SOYBEAN 20 30 40 50 60 70 80 90 25% 40% 55% 70% 85% 100% LFC L NL Figure 6: Learning from communication resulting from argumentation in a system composed of 5 agents. distributed systems where there are several case bases available for the same task and addresses the problems of cross-case base adaptation.",
                "The main difference is that our MAC approach is a way to distribute the Reuse process of CBR (using a voting system) while Retrieve is performed individually by each agent; the other multiagent CBR approaches, however, focus on distributing the Retrieve process.",
                "Research on MAS argumentation focus on several issues like a) logics, protocols and languages that support argumentation, b) argument selection and c) argument interpretation.",
                "Approaches for logic and languages that support argumentation include defeasible logic [4] and BDI models [13].",
                "Although argument selection is a key aspect of automated argumentation (see [12] and [13]), most research has been focused on preference relations among arguments.",
                "In our framework we have addressed both argument selection and preference relations using a case-based approach. 11.",
                "CONCLUSIONS AND FUTURE WORK In this paper we have presented an argumentation-based framework for multi-agent learning.",
                "Specifically, we have presented AMAL, a framework that allows a group of learning agents to argue about the solution of a given problem and we have shown how the learning capabilities can be used to generate arguments and counterarguments.",
                "The experimental evaluation shows that the increased amount of information provided to the agents by the argumentation process increases their predictive accuracy, and specially when an adequate number of agents take part in the argumentation.",
                "The main contributions of this work are: a) an argumentation framework for learning agents; b) a case-based preference relation over arguments, based on computing an overall confidence estimation of arguments; c) a case-based policy to generate counterarguments and select counterexamples; and d) an argumentation-based approach for learning from communication.",
                "Finally, in the experiments presented here a learning agent would retain all counterexamples submitted by the other agent; however, this is a very simple case retention policy, and we will like to experiment with more informed policies - with the goal that individual learning agents could significantly improve using only a small set of cases proposed by other agents.",
                "Finally, our approach is focused on lazy learning, and future works aims at incorporating eager inductive learning inside the argumentative framework for learning from communication. 12.",
                "REFERENCES [1] Agnar Aamodt and Enric Plaza.",
                "Case-based reasoning: Foundational issues, methodological variations, and system approaches.",
                "Artificial Intelligence Communications, 7(1):39-59, 1994. [2] E. Armengol and E. Plaza.",
                "Lazy induction of descriptions for relational case-based learning.",
                "In ECML2001, pages 13-24, 2001. [3] Gerhard Brewka.",
                "Dynamic argument systems: A formal model of argumentation processes based on situation calculus.",
                "Journal of Logic and Computation, 11(2):257-282, 2001. [4] Carlos I. Chesñevar and Guillermo R. Simari.",
                "Formalizing Defeasible Argumentation using Labelled Deductive Systems.",
                "Journal of Computer Science & Technology, 1(4):18-33, 2000. [5] D. Leake and R. Sooriamurthi.",
                "Automatically selecting strategies for multi-case-base reasoning.",
                "In S. Craw and A. Preece, editors, ECCBR2002, pages 204-219, Berlin, 2002.",
                "Springer Verlag. [6] Francisco J. Martín, Enric Plaza, and Josep-Lluis Arcos.",
                "Knowledge and experience reuse through communications among competent (peer) agents.",
                "International Journal of Software Engineering and Knowledge Engineering, 9(3):319-341, 1999. [7] Lorraine McGinty and Barry Smyth.",
                "Collaborative case-based reasoning: Applications in personalized route planning.",
                "In I. Watson and Q. Yang, editors, ICCBR, number 2080 in LNAI, pages 362-376.",
                "Springer-Verlag, 2001. [8] Santi Ontañón and Enric Plaza.",
                "Justification-based multiagent learning.",
                "In ICML2003, pages 576-583.",
                "Morgan Kaufmann, 2003. [9] Enric Plaza, Eva Armengol, and Santiago Ontañón.",
                "The explanatory power of symbolic similarity in case-based reasoning.",
                "Artificial Intelligence Review, 24(2):145-161, 2005. [10] David Poole.",
                "On the comparison of theories: Preferring the most specific explanation.",
                "In IJCAI-85, pages 144-147, 1985. [11] M V Nagendra Prassad, Victor R Lesser, and Susan Lander.",
                "Retrieval and reasoning in distributed case bases.",
                "Technical report, UMass Computer Science Department, 1995. [12] K. Sycara S. Kraus and A. Evenchik.",
                "Reaching agreements through argumentation: a logical model and implementation.",
                "Artificial Intelligence Journal, 104:1-69, 1998. [13] N. R. Jennings S. Parsons, C. Sierra.",
                "Agents that reason and negotiate by arguing.",
                "Journal of Logic and Computation, 8:261-292, 1998. [14] Bruce A. Wooley.",
                "Explanation component of software systems.",
                "ACM CrossRoads, 5.1, 1998. 982 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "MULTI-AGENT CBR SYSTEMS A Multi-Agent Case Based Reasoning System (MAC) M = {(A1, C1), ..., (An, Cn)} is a <br>multi-agent system</br> composed of A = {Ai, ..., An}, a set of CBR agents, where each agent Ai ∈ A possesses an individual case base Ci."
            ],
            "translated_annotated_samples": [
                "SISTEMAS CBR MULTIAGENTE Un Sistema de Razonamiento Basado en Casos Multiagente (MAC) M = {(A1, C1), ..., (An, Cn)} es un <br>sistema multiagente</br> compuesto por A = {Ai, ..., An}, un conjunto de agentes CBR, donde cada agente Ai ∈ A posee una base de casos individual Ci."
            ],
            "translated_text": "Aprendizaje y deliberación conjunta a través de la argumentación en sistemas multiagente. Santi Ontañón CCL, Laboratorio de Computación Cognitiva, Instituto de Tecnología de Georgia, Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Instituto de Investigación en Inteligencia Artificial, CSIC, Consejo Superior de Investigaciones Científicas, Campus UAB, 08193 Bellaterra, Cataluña (España) enric@iiia.csic.es RESUMEN En este artículo presentaremos un marco de argumentación para agentes de aprendizaje (AMAL) diseñado con dos propósitos: (1) para deliberación conjunta, y (2) para aprender a través de la comunicación. El marco AMAL se basa completamente en el aprendizaje a partir de ejemplos: la relación de preferencia de argumentos, la política de generación de argumentos y la política de generación de contraargumentos son técnicas basadas en casos. Para unirse en la deliberación, los agentes de aprendizaje comparten su experiencia formando un comité para decidir sobre alguna decisión conjunta. Experimentalmente demostramos que la argumentación entre comités de agentes mejora tanto el rendimiento individual como el conjunto. Para aprender a través de la comunicación, un agente se involucra en discusiones con otros agentes para contrastar sus hipótesis individuales y recibir contraejemplos; el proceso de argumentación mejora su alcance de aprendizaje y rendimiento individual. Categorías y Descriptores de Asignaturas I.2.6 [Inteligencia Artificial]: Aprendizaje; I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente, Agentes Inteligentes 1. Los marcos de argumentación para sistemas multiagente pueden ser utilizados para diferentes propósitos como la deliberación conjunta, la persuasión, la negociación y la resolución de conflictos. En este artículo presentaremos un marco de argumentación para agentes de aprendizaje, y mostraremos que puede ser utilizado para dos propósitos: (1) deliberación conjunta, y (2) aprendizaje a partir de la comunicación. La deliberación conjunta basada en argumentos implica discutir sobre el resultado de una situación particular o la acción apropiada a tomar en una situación específica. Los agentes de aprendizaje son capaces de aprender de la experiencia, en el sentido de que ejemplos pasados (situaciones y sus resultados) se utilizan para predecir el resultado para la situación actual. Sin embargo, dado que la experiencia de los agentes individuales puede ser limitada, el conocimiento individual y la precisión de la predicción también son limitados. Por lo tanto, los agentes de aprendizaje que son capaces de argumentar sus predicciones individuales con otros agentes pueden alcanzar una mayor precisión en las predicciones después de dicho proceso de argumentación. La mayoría de los marcos de argumentación existentes para sistemas multiagente se basan en lógica deductiva u otra formalización de lógica deductiva diseñada específicamente para respaldar la argumentación, como la lógica por defecto [3]. Por lo general, un argumento se ve como una declaración lógica, mientras que un contraargumento es un argumento ofrecido en oposición a otro argumento; los agentes utilizan una relación de preferencia para resolver argumentos conflictivos. Sin embargo, los marcos de argumentación basados en lógica asumen agentes con conocimiento predefinido y relación de preferencia. En este artículo, nos enfocamos en un marco de Aprendizaje Multiagente basado en Argumentación (AMAL) donde tanto el conocimiento como la relación de preferencia se aprenden a partir de la experiencia. Por lo tanto, consideramos un escenario con agentes que (1) trabajan en el mismo dominio utilizando una ontología compartida, (2) son capaces de aprender a partir de ejemplos y (3) se comunican utilizando un marco argumentativo. Tener capacidades de aprendizaje permite a los agentes utilizar de manera efectiva una forma específica de contraargumento, a saber, el uso de contraejemplos. Los contraejemplos ofrecen la posibilidad de que los agentes aprendan durante el proceso de argumentación. Además, los agentes de aprendizaje permiten técnicas que utilizan la experiencia aprendida para generar argumentos y contraargumentos adecuados. Específicamente, necesitaremos abordar dos problemas: (1) cómo definir una técnica para generar argumentos y contraargumentos a partir de ejemplos, y (2) cómo definir una relación de preferencia sobre dos argumentos en conflicto que han sido inducidos a partir de ejemplos. Este documento presenta un enfoque basado en casos para abordar ambas cuestiones. Los agentes utilizan el razonamiento basado en casos (CBR) [1] para aprender de casos pasados (donde un caso es una situación y su resultado) con el fin de predecir el resultado de una nueva situación. Proponemos un protocolo de argumentación dentro del marco de AMAL que apoya a los agentes en llegar a una predicción conjunta sobre una situación o problema específico; además, el razonamiento necesario para respaldar el proceso de argumentación también se basará en casos. En particular, presentamos dos medidas basadas en casos, una para generar los argumentos y contraargumentos adecuados para una situación particular y otra para determinar la relación de preferencia entre los argumentos. Finalmente, evaluamos (1) si la argumentación entre agentes de aprendizaje puede producir una predicción conjunta que mejore el rendimiento de aprendizaje individual y (2) si el aprendizaje a partir de los contraejemplos transmitidos durante el proceso de argumentación aumenta el rendimiento individual con precisamente aquellos casos que se utilizan mientras discuten entre ellos. El documento está estructurado de la siguiente manera. La sección 2 discute la relación entre la argumentación, la colaboración y el aprendizaje. Luego, la Sección 3 introduce nuestro marco de trabajo de CBR multiagente (MAC) y el concepto de predicción justificada. Después de eso, la Sección 4 define formalmente nuestro marco de argumentación. Las secciones 5 y 6 presentan nuestra relación de preferencia basada en casos y las políticas de generación de argumentos, respectivamente. Más adelante, la Sección 7 presenta el protocolo de argumentación en nuestro marco AMAL. Después de eso, la Sección 8 presenta una ejemplificación del marco de argumentación. Finalmente, la Sección 9 presenta una evaluación empírica de nuestras dos hipótesis principales. El artículo concluye con secciones de trabajo relacionado y conclusiones. 2. ARGUMENTACIÓN, COLABORACIÓN Y APRENDIZAJE Tanto el aprendizaje como la colaboración son formas en las que un agente puede mejorar el rendimiento individual. De hecho, existe un claro paralelismo entre el aprendizaje y la colaboración en sistemas multiagentes, ya que ambos son formas en las que los agentes pueden lidiar con sus deficiencias. Mostremos cuáles son las principales motivaciones que un agente puede tener para aprender o colaborar. • Motivaciones para aprender: - Aumentar la calidad de la predicción, - Aumentar la eficiencia, - Ampliar el rango de problemas resolubles. • Motivaciones para colaborar: - Aumentar la calidad de la predicción, - Aumentar la eficiencia, - Ampliar el rango de problemas resolubles, - Ampliar el rango de recursos accesibles. Al observar las listas de motivación anteriores, podemos ver fácilmente que el aprendizaje y la colaboración están muy relacionados en los sistemas multiagente. De hecho, con la excepción del último elemento en la lista de motivaciones para colaborar, son dos extremos de un continuo de estrategias para mejorar el rendimiento. Un agente puede optar por aumentar el rendimiento mediante el aprendizaje, la colaboración o encontrando un punto intermedio que combine el aprendizaje y la colaboración para mejorar el rendimiento. En este artículo propondremos AMAL, un marco de argumentación para agentes de aprendizaje, y también mostraremos cómo AMAL puede ser utilizado tanto para aprender a través de la comunicación como para resolver problemas de manera colaborativa: • Los agentes pueden resolver problemas de manera colaborativa al participar en un proceso de argumentación sobre la predicción para la situación actual. Utilizando esta colaboración, la predicción puede realizarse de una manera más informada, ya que se ha tenido en cuenta la información conocida por varios agentes. Los agentes también pueden aprender de la comunicación con otros agentes al participar en un proceso de argumentación. Los agentes que participan en estos procesos de argumentación pueden aprender de los argumentos y contraejemplos recibidos de otros agentes, y utilizar esta información para predecir los resultados de situaciones futuras. En el resto de este documento propondremos un marco de argumentación y mostraremos cómo puede ser utilizado tanto para el aprendizaje como para la resolución de problemas de manera colaborativa. SISTEMAS CBR MULTIAGENTE Un Sistema de Razonamiento Basado en Casos Multiagente (MAC) M = {(A1, C1), ..., (An, Cn)} es un <br>sistema multiagente</br> compuesto por A = {Ai, ..., An}, un conjunto de agentes CBR, donde cada agente Ai ∈ A posee una base de casos individual Ci. Cada agente individual Ai en un MAC es completamente autónomo y cada agente Ai solo tiene acceso a su base de casos individual y privada Ci. Un conjunto de casos Ci = {c1, ..., cm} es una colección de casos. Los agentes en un sistema MAC pueden resolver problemas de forma individual, pero también pueden colaborar con otros agentes para resolver problemas. En este marco, nos limitaremos a tareas analíticas, es decir, tareas como la clasificación, donde la solución de un problema se logra seleccionando una clase de solución de un conjunto enumerado de clases de solución. En lo siguiente, notaremos el conjunto de todas las clases de soluciones por S = {S1, ..., SK}. Por lo tanto, un caso c = P, S es una tupla que contiene una descripción del caso P y una clase de solución S ∈ S. En lo siguiente, utilizaremos indistintamente los términos problema y descripción del caso. Además, utilizaremos la notación de punto para referirnos a elementos dentro de una tupla; por ejemplo, para referirnos a la clase de solución de un caso c, escribiremos c.S. Por lo tanto, decimos que un grupo de agentes realiza una deliberación conjunta cuando colaboran para encontrar una solución conjunta mediante un proceso de argumentación. Sin embargo, para hacerlo, un agente debe ser capaz de justificar su predicción a los otros agentes (es decir, generar un argumento para su solución predicha que pueda ser examinado y criticado por los otros agentes). La siguiente sección aborda este tema. 3.1 Predicciones Justificadas Tanto los sistemas expertos como los sistemas CBR pueden tener un componente de explicación [14] encargado de justificar por qué el sistema ha proporcionado una respuesta específica al usuario. La línea de razonamiento del sistema puede ser examinada por un experto humano, aumentando así la fiabilidad del sistema. La mayoría de los trabajos existentes sobre generación de explicaciones se centran en generar explicaciones para ser proporcionadas al usuario. Sin embargo, en nuestro enfoque utilizamos explicaciones (o justificaciones) como una herramienta para mejorar la comunicación y coordinación entre agentes. Estamos interesados en las justificaciones ya que pueden ser utilizadas como argumentos. Para ese propósito, nos beneficiaremos de la capacidad de algunos métodos de aprendizaje automático para proporcionar justificaciones. Una justificación construida por un método CBR después de determinar que la solución de un problema particular P fue Sk es una descripción que contiene la información relevante del problema P que el método CBR ha considerado para predecir Sk como la solución de P. En particular, los métodos CBR funcionan recuperando casos similares al problema en cuestión, y luego reutilizando sus soluciones para el problema actual, esperando que dado que el problema y los casos son similares, las soluciones también serán similares. Por lo tanto, si un método CBR ha recuperado un conjunto de casos C1, ..., Cn para resolver un problema particular P, la justificación construida contendrá la información relevante del problema P que hizo que el sistema CBR recuperara ese conjunto particular de casos, es decir, contendrá la información relevante que P y C1, ..., Cn tienen en común. Por ejemplo, la Figura 1 muestra una justificación construida por un sistema CBR para un problema de juguete (en las siguientes secciones mostraremos justificaciones para problemas reales). En la figura, un problema tiene dos atributos (Semáforo y Coches_pasando), el mecanismo de recuperación del sistema CBR nota que al considerar solo el atributo Semáforo, puede recuperar dos casos que predicen la misma solución: esperar. Por lo tanto, dado que solo se ha utilizado este atributo, es el único que aparece en la justificación. Los valores del resto de atributos son irrelevantes, ya que, sea cual sea su valor, la clase de solución habría sido la misma. 976 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Problema Semáforo: rojo Pasan coches: no Caso 1 Semáforo: rojo Pasan coches: no Solución: esperar Caso 3 Semáforo: rojo Pasan coches: sí Solución: esperar Caso 4 Semáforo: verde Pasan coches: sí Solución: esperar Caso 2 Semáforo: verde Pasan coches: no Solución: cruzar Casos recuperados Solución: esperar Justificación Semáforo: rojo Figura 1: Un ejemplo de generación de justificación en un sistema de RCB. Observa que, dado que la única característica relevante para decidir es Traffic_light (la única utilizada para recuperar casos), es la única que aparece en la justificación. En general, el significado de una justificación es que todos (o la mayoría) de los casos en la base de casos de un agente que satisfacen la justificación (es decir, todos los casos que están subsumidos por la justificación) pertenecen a la clase de solución predicha. En el resto del documento, usaremos para denotar la relación de subsumpción. En nuestro trabajo, utilizamos LID [2], un método CBR capaz de construir justificaciones simbólicas como la ejemplificada en la Figura 1. Cuando un agente proporciona una justificación para una predicción, el agente genera una predicción justificada: DEFINICIÓN 3.1. Una Predicción Justificada es una tupla J = A, P, S, D donde el agente A considera que S es la solución correcta para el problema P, y esa predicción está justificada por una descripción simbólica D tal que J.D J.P. Las justificaciones pueden tener muchos usos para los sistemas de RCB [8, 9]. En este documento, vamos a utilizar justificaciones como argumentos, con el fin de permitir que los agentes de aprendizaje participen en procesos de argumentación. 4. ARGUMENTOS Y CONTRAARGUMENTOS Para nuestros propósitos, un argumento α generado por un agente A está compuesto por una afirmación S y alguna evidencia D que respalda a S como correcta. En el resto de esta sección veremos cómo esta definición general de argumento puede ser concretada en un tipo específico de argumentos que los agentes pueden generar. En el contexto de los sistemas MAC, los agentes discuten sobre predicciones para nuevos problemas y pueden proporcionar dos tipos de información: a) casos específicos P, S, y b) predicciones justificadas: A, P, S, D. Usando esta información, podemos definir tres tipos de argumentos: predicciones justificadas, contraargumentos y contraejemplos. Una predicción justificada α es generada por un agente Ai para argumentar que Ai cree que la solución correcta para un problema dado P es α.S, y la evidencia proporcionada es la justificación α.D. En el ejemplo representado en la Figura 1, un agente Ai puede generar el argumento α = Ai, P, Esperar, (Semáforo = rojo), lo que significa que el agente Ai cree que la solución correcta para P es Esperar porque el atributo Semáforo es igual a rojo. Un contraargumento β es un argumento ofrecido en oposición a otro argumento α. En nuestro marco de trabajo, un contraargumento consiste en una predicción justificada Aj, P, S, D generada por un agente Aj con la intención de refutar un argumento α generado por otro agente Ai, que respalda una clase de solución S diferente a la de α.S para el problema en cuestión y justifica esto con una justificación D. En el ejemplo de la Figura 1, si un agente genera el argumento α = Ai, P, Caminar, (Pasando_autos = no), un agente que piensa que la solución correcta es Esperar podría responder con el contraargumento β = Aj, P, Esperar, (Pasando_autos = no ∧ Semáforo = rojo), lo que significa que, aunque no hay autos pasando, el semáforo está en rojo y la calle no se puede cruzar. Un contraejemplo c es un caso que contradice un argumento α. Por lo tanto, un contraejemplo también es un contraargumento, uno que afirma que un argumento específico α no siempre es verdadero, y la evidencia proporcionada es el caso c. Específicamente, para que un caso c sea un contraejemplo de un argumento α, se deben cumplir las siguientes condiciones: α.D c y α.S = c.S, es decir, el caso debe satisfacer la justificación α.D y la solución de c debe ser diferente a la predicha por α. Al intercambiar argumentos y contraargumentos (incluyendo contraejemplos), los agentes pueden discutir sobre la solución correcta de un problema dado, es decir, pueden participar en un proceso de deliberación conjunto. Sin embargo, para hacerlo, necesitan un protocolo de interacción específico, una relación de preferencia entre argumentos contradictorios y una política de decisión para generar contraargumentos (incluidos contraejemplos). En las siguientes secciones presentaremos estos elementos. 5. RELACIÓN DE PREFERENCIA Un argumento específico proporcionado por un agente podría no ser consistente con la información conocida por otros agentes (o incluso con parte de la información conocida por el agente que ha generado la justificación debido al ruido en los datos de entrenamiento). Por esa razón, vamos a definir una relación de preferencia sobre predicciones justificadas contradictorias basadas en casos. Básicamente, definiremos una medida de confianza para cada predicción justificada (que tenga en cuenta los casos poseídos por cada agente), y la predicción justificada con la mayor confianza será la preferida. La idea detrás de la confianza basada en casos es contar cuántos de los casos en una base de casos individual respaldan una predicción justificada, y cuántos de ellos son ejemplos en contra de ella. Cuanto más casos de respaldo, mayor es la confianza; y cuanto más contrajemplos, menor es la confianza. Específicamente, para evaluar la confianza de una predicción justificada α, un agente obtiene el conjunto de casos en su base de casos individuales que están subsumidos por α.D. Con ellos, un agente Ai obtiene los valores Y (sí) y N (no): • Y Ai α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| es el número de casos en la base de casos del agente subsumidos por la justificación α.D que pertenecen a la clase de solución α.S, • NAi α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| es el número de casos en la base de casos del agente subsumidos por la justificación α.D que no pertenecen a esa clase de solución. El Sexto Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 977 + + + + + +  - - + Figura 2: La confianza de los argumentos se evalúa contrastándolos con las bases de casos de los agentes. Un agente estima la confianza de un argumento como: CAi (α) = Y Ai α 1 + Y Ai α + NAi α es decir, la confianza en una predicción justificada es el número de casos que la respaldan dividido por el número de casos que la respaldan más los contraejemplos. Observa que sumamos 1 al denominador, esto es para evitar otorgar confianzas excesivamente altas a predicciones justificadas cuya confianza se ha calculado utilizando un número pequeño de casos. Ten en cuenta que esta corrección sigue la misma idea que la corrección de Laplace para estimar probabilidades. La Figura 2 ilustra la evaluación individual de la confianza de un argumento, en particular, se encuentran tres casos de respaldo y un contraejemplo en la base de casos de los agentes Ai, lo que da una confianza estimada de 0.6. Además, también podemos definir la confianza conjunta de un argumento α como la confianza calculada utilizando los casos presentes en las bases de casos de todos los agentes en el grupo: C(α) = Π Ai α / (1 + Σ Ai α + NAi α). Nótese que, para calcular conjuntamente la confianza conjunta, los agentes solo tienen que hacer públicos los valores de aprobación y rechazo calculados localmente para un argumento dado. En nuestro marco de trabajo, los agentes utilizan esta confianza conjunta como la relación de preferencia: una predicción justificada α se prefiere sobre otra β si C(α) ≥ C(β). GENERACIÓN DE ARGUMENTOS En nuestro marco de trabajo, los argumentos son generados por los agentes a partir de casos, utilizando métodos de aprendizaje. Cualquier método de aprendizaje capaz de proporcionar una predicción justificada puede ser utilizado para generar argumentos. Por ejemplo, los árboles de decisión y LID [2] son métodos de aprendizaje adecuados. Específicamente, en los experimentos reportados en este artículo, los agentes utilizan LID. Por lo tanto, cuando un agente desea generar un argumento que respalde que una clase de solución específica es la solución correcta para un problema P, genera una predicción justificada como se explica en la Sección 3.1. Por ejemplo, la Figura 3 muestra una justificación real generada por LID después de resolver un problema P en el dominio de la identificación de esponjas marinas. En particular, la Figura 3 muestra cómo cuando un agente recibe un nuevo problema para resolver (en este caso, un nuevo esponja para determinar su orden), el agente utiliza LID para generar un argumento (que consiste en una predicción justificada) utilizando los casos en la base de casos del agente. La justificación mostrada en la Figura 3 se puede interpretar diciendo que la solución predicha es hadromerida porque la forma lisa de las megascleras del esqueleto espicular de la esponja es de tipo tilostilo, el esqueleto espicular de la esponja no tiene una longitud uniforme y no hay gemas en las características externas de la esponja. Por lo tanto, el argumento generado será α = A1, P, hadromerida, D1. 6.1 Generación de Contraargumentos Como se mencionó anteriormente, los agentes pueden intentar refutar argumentos generando contraargumentos o encontrando contraejemplos. Permítanos explicar cómo pueden ser generados. Un agente Ai quiere generar un contraargumento β para rebatir un argumento α cuando α está en contradicción con la base de casos locales de Ai. Además, al generar dicho contraargumento β, Ai espera que β sea preferido sobre α. Con ese propósito, presentaremos una política específica para generar contraargumentos basados en el criterio de especificidad [10]. El criterio de especificidad se utiliza ampliamente en marcos deductivos para la argumentación, y establece que entre dos argumentos en conflicto, se debe preferir el más específico ya que, en principio, está más informado. Por lo tanto, se espera que los contraargumentos generados basados en el criterio de especificidad sean preferibles (ya que están más informados) que los argumentos que intentan refutar. Sin embargo, no hay garantía de que tales contraargumentos siempre ganen, ya que, como hemos indicado en la Sección 5, los agentes en nuestro marco utilizan una relación de preferencia basada en la confianza conjunta. Además, uno podría pensar que sería mejor que los agentes generaran contraargumentos basados en la relación de preferencia de confianza conjunta; sin embargo, no es obvio cómo generar contraargumentos basados en la confianza conjunta de manera eficiente, ya que se requiere colaboración para evaluar la confianza conjunta. Por lo tanto, el agente que genera el contraargumento debe comunicarse constantemente con los otros agentes en cada paso del algoritmo de inducción utilizado para generar contraargumentos (actualmente una de nuestras líneas de investigación futuras). Por lo tanto, en nuestro marco de trabajo, cuando un agente quiere generar un contraargumento β a un argumento α, β tiene que ser más específico que α (es decir, α.D < β.D). La generación de contraargumentos utilizando el criterio de especificidad impone algunas restricciones sobre el método de aprendizaje, aunque LID o ID3 pueden adaptarse fácilmente para esta tarea. Por ejemplo, LID es un algoritmo que genera una descripción desde cero y añade heurísticamente características a ese término. Por lo tanto, en cada paso, la descripción se vuelve más específica que en el paso anterior, y el número de casos que se incluyen en esa descripción se reduce. Cuando la descripción cubre solo (o casi solo) casos de una única clase de solución, LID termina y predice esa clase de solución. Para generar un contraargumento a un argumento α, LID solo tiene que usar como punto de partida la descripción α.D en lugar de empezar desde cero. De esta manera, la justificación proporcionada por LID siempre estará subsumida por α.D, y por lo tanto, el contraargumento resultante será más específico que α. Sin embargo, hay que tener en cuenta que LID a veces puede no ser capaz de generar contraargumentos, ya que LID puede no ser capaz de especializar aún más la descripción α.D, o porque el agente Ai no tiene ningún caso en Ci que esté subsumido por α.D. La Figura 4 muestra cómo un agente A2 que no estaba de acuerdo con el argumento mostrado en la Figura 3, genera un contraargumento utilizando LID. Además, la Figura 4 muestra la generación de un contraargumento β1 2 para el argumento α0 1 (en la Figura 3) que es una especialización de α0 1. 978 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Solución: hadromerida Justificación: D1 Esponja Esqueleto espiculado Características externas Características externas Gemmulas: no Esqueleto espiculado Megascleres Longitud uniforme: no Megascleres Forma lisa: tilostilo Caso Base de A1 LID Nueva esponja P Figura 3: Ejemplo de una justificación real generada por LID en el conjunto de datos de esponjas marinas. Específicamente, en nuestros experimentos, cuando un agente Ai quiere refutar un argumento α, utiliza la siguiente política: 1. El Agente Ai utiliza LID para intentar encontrar un contraargumento β más específico que α; si se encuentra, β se envía al otro agente como un contraargumento de α. 2. Si no se encuentra, entonces Ai busca un contraejemplo c ∈ Ci de α. Si se encuentra un caso c, entonces c se envía al otro agente como un contraejemplo de α. Si no se encuentran contraejemplos, entonces Ai no puede refutar el argumento α. 7. APRENDIZAJE MULTIAGENTE BASADO EN ARGUMENTACIÓN El protocolo de interacción de AMAL permite que un grupo de agentes A1, ..., An deliberen sobre la solución correcta de un problema P mediante un proceso de argumentación. Si el proceso de argumentación llega a una solución consensuada, la deliberación conjunta termina; de lo contrario, se utiliza una votación ponderada para determinar la solución conjunta. Además, AMAL también permite a los agentes aprender de los contraejemplos recibidos de otros agentes. El protocolo AMAL consiste en una serie de rondas. En la ronda inicial, cada agente declara cuál es su predicción individual para P. Luego, en cada ronda, un agente puede intentar refutar la predicción hecha por cualquiera de los otros agentes. El protocolo utiliza un mecanismo de paso de token para que los agentes (uno a la vez) puedan enviar contraargumentos o contraejemplos si no están de acuerdo con la predicción hecha por cualquier otro agente. Específicamente, a cada agente se le permite enviar un contraargumento o contraejemplo cada vez que recibe el token (nota que esta restricción es solo para simplificar el protocolo, y que no restringe la cantidad de contraargumentos que un agente puede enviar, ya que pueden ser retrasados para rondas posteriores). Cuando un agente recibe un contraargumento o contraejemplo, informa a los otros agentes si acepta el contraargumento (y cambia su predicción) o no. Además, los agentes también tienen la oportunidad de responder a los contraargumentos cuando reciben el token, intentando generar un contraargumento al contraargumento. Cuando todos los agentes hayan tenido el token una vez, el token regresa al primer agente, y así sucesivamente. Si en algún momento del protocolo, todos los agentes están de acuerdo o durante las últimas n rondas ningún agente ha generado ninguna contra-argumentación, el protocolo termina. Además, si al final de la argumentación los agentes no han llegado a un acuerdo, entonces se utiliza un mecanismo de votación que utiliza la confianza de cada predicción como pesos para decidir la solución final (Así, AMAL sigue el mismo mecanismo que los comités humanos, primero cada miembro individual de un comité expone sus argumentos y discute los de los otros miembros (deliberación conjunta), y si no se alcanza un consenso, entonces se requiere un mecanismo de votación). En cada iteración, los agentes pueden usar los siguientes performativos: • afirmar(α): la predicción justificada que se mantendrá durante la próxima ronda será α. Un agente solo puede tener una única predicción en cada ronda, por lo tanto, si se envían múltiples afirmaciones, solo se considera la última como la predicción actualmente mantenida. • rebut(β, α): el agente ha encontrado un contraargumento β a la predicción α. Definiremos Ht = αt 1, ..., αt n como las predicciones que cada uno de los n agentes tiene en una ronda t. Además, también definiremos contradict(αt i) = {α ∈ Ht|α.S = αt i.S} como el conjunto de argumentos contradictorios para un agente Ai en una ronda t, es decir, el conjunto de argumentos en la ronda t que respaldan una clase de solución diferente a la de αt i. El protocolo se inicia porque uno de los agentes recibe un problema P que debe ser resuelto. Después de eso, el agente informa a todos los demás agentes sobre el problema P a resolver, y el protocolo comienza: 1. En la ronda t = 0, cada uno de los agentes resuelve individualmente P y construye una predicción justificada utilizando su propio método de RBC. Entonces, cada agente Ai envía el assertivo performativo assert(α0 i) a los otros agentes. Por lo tanto, los agentes conocen H0 = α0 i , ..., α0 n. Una vez que todas las predicciones hayan sido enviadas, el token se le da al primer agente A1. 2. En cada ronda t (distinta de 0), los agentes verifican si sus argumentos en Ht coinciden. Si lo hacen, el protocolo avanza al paso 5. Además, si durante las últimas n rondas ningún agente ha enviado ningún contraejemplo o contraargumento, el protocolo también avanza al paso 5. De lo contrario, el agente Ai propietario del token intenta generar un contraargumento para cada uno de los argumentos opuestos en contradict(αt i) ⊆ Ht (ver Sección 6.1). Entonces, se selecciona el contraargumento βt i contra la predicción αt j con la menor confianza C(αt j) (ya que αt j es la predicción más probable de ser refutada con éxito). • Si βt i es un contraargumento, entonces, Ai compara localmente αt i con βt i evaluando su confianza contra su base de casos individuales Ci (ver Sección 5) (nótese que Ai está comparando su argumento previo con el contraargumento que Ai mismo acaba de generar y que trata sobre The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 979 Esponja Esqueleto espiculado Características externas Características externas Gemas: no Crecimiento: Esqueleto espiculado Megascleres Longitud uniforme: no Megascleres Forma lisa: tilostilo Crecimiento Crecimiento: masivo Caso Base de A2 LID Solución: astrophorida Justificación: D2 Figura 4: Generación de un contraargumento utilizando LID en el conjunto de datos de esponjas. para enviar a Aj. Si CAi (βt i) > CAi (αt i), entonces Ai considera que βt i es más fuerte que su argumento anterior, cambia su argumento a βt i enviando assert(βt i) al resto de los agentes (la intuición detrás de esto es que dado que un contraargumento también es un argumento, Ai verifica si el nuevo contraargumento es un mejor argumento que el que sostenía anteriormente) y rebut(βt i, αt j) a Aj. De lo contrario (es decir, Si CAi (βt i ) ≤ CAi (αt i), Ai solo enviará rebut(βt i , αt j) a Aj. En cualquiera de las dos situaciones, el protocolo avanza al paso 3. • Si βt i es un contraejemplo c, entonces Ai envía una refutación(c, αt j) a Aj. El protocolo avanza al paso 4. • Si Ai no puede generar ningún contraargumento o contraejemplo, el token se envía al siguiente agente, comienza una nueva ronda t + 1, y el protocolo avanza al estado 2. 3. El agente Aj que ha recibido el contraargumento βt i, lo compara localmente con su propio argumento, αt j, evaluando localmente su confianza. Si CAj (βt i ) > CAj (αt j), entonces Aj aceptará el contraargumento como más fuerte que su propio argumento, y enviará assert(βt i ) a los otros agentes. De lo contrario (es decir, Si CAj (βt i ) ≤ CAj (αt j)), Aj no aceptará el contraargumento e informará a los otros agentes en consecuencia. Cualquiera de las dos situaciones inicia una nueva ronda t + 1, Ai envía el token al siguiente agente y el protocolo vuelve al estado 2. 4. El agente Aj que ha recibido el contraejemplo c lo retiene en su base de casos y genera un nuevo argumento αt+1 j que tiene en cuenta c, e informa al resto de los agentes enviando assert(αt+1 j) a todos ellos. Entonces, Ai envía el token al siguiente agente, comienza una nueva ronda t + 1 y el protocolo vuelve al paso 2. 5. El protocolo finaliza generando una predicción conjunta, de la siguiente manera: si los argumentos en Ht están de acuerdo, entonces su predicción es la predicción conjunta; de lo contrario, se utiliza un mecanismo de votación para decidir la predicción conjunta. El mecanismo de votación utiliza la medida de confianza conjunta como los pesos de votación, de la siguiente manera: S = arg max Sk∈S αi∈Ht|αi.S=Sk C(αi) Además, para evitar iteraciones infinitas, si un agente envía dos veces el mismo argumento o contraargumento al mismo agente, el mensaje no se considera. 8. EJEMPLIFICACIÓN Consideremos un sistema compuesto por tres agentes A1, A2 y A3. Uno de los agentes, A1, recibe un problema P para resolver y decide usar AMAL para resolverlo. Por esa razón, invita a A2 y A3 a participar en el proceso de argumentación. Aceptan la invitación y comienza el protocolo de argumentación. Inicialmente, cada agente genera su predicción individual para P y la transmite a los otros agentes. Por lo tanto, todos ellos pueden calcular H0 = α0 1, α0 2, α0 3. En particular, en este ejemplo: • α0 1 = A1, P, hadromerida, D1 • α0 2 = A2, P, astrophorida, D2 • α0 3 = A3, P, axinellida, D3 A1 comienza poseyendo el token e intenta generar argumentos en contra de α0 2 y α0 3, pero no tiene éxito; sin embargo, tiene un contraejemplo c13 para α0 3. Por lo tanto, A1 envía el mensaje rebut(c13, α03) a A3. A3 incorpora c13 en su base de casos e intenta resolver el problema P nuevamente, ahora teniendo en cuenta c13. A3 elabora la predicción justificada α1 3 = A3, P, hadromerida, D4, y la transmite al resto de los agentes con el mensaje assert(α1 3). Por lo tanto, todos ellos conocen el nuevo H1 = α0 1, α0 2, α1 3. La ronda 1 comienza y A2 obtiene el token. A2 intenta generar contraargumentos para α0 1 y α1 3 y solo logra generar un contraargumento β1 2 = A2, P, astrophorida, D5 contra α1 3. El contraargumento se envía a A3 con el mensaje rebut(β1 2 , α1 3). El agente A3 recibe el contraargumento y evalúa su confianza local. El resultado es que la confianza individual del contraargumento β1 2 es menor que la confianza local de α1 3. Por lo tanto, A3 no acepta el contraargumento, y así H2 = α0 1, α0 2, α1 3. La ronda 2 comienza y A3 recibe el token. A3 genera un contraargumento β2 3 = A3, P, hadromerida, D6 para α0 2 y lo envía a A2 con el mensaje refutar(β2 3 , α0 2). El agente A2 recibe el contraargumento y evalúa su confianza local. El resultado es que la confianza local del contraargumento β2 3 es mayor que la confianza local de α0 2. Por lo tanto, A2 acepta el contraargumento e informa al resto de los agentes con el mensaje assert(β2 3). Después de eso, H3 = α0 1, β2 3 , α1 3. En la Ronda 3, dado que todos los agentes están de acuerdo (todas las predicciones justificadas en H3 predicen hadromerida como la clase de solución), el protocolo finaliza, y A1 (el agente que recibió el problema) considera hadromerida como la solución conjunta para el problema P. 9. EVALUACIÓN EXPERIMENTAL 980 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) SPONGE 75 77 79 81 83 85 87 89 91 2 3 4 5 AMAL Votación Individual SOYBEAN 55 60 65 70 75 80 85 90 2 3 4 5 AMAL Votación Individual Figura 5: Precisión individual y conjunta para 2 a 5 agentes. En esta sección evaluamos empíricamente el marco de argumentación AMAL. Hemos realizado experimentos en dos conjuntos de datos diferentes: soja (del repositorio de aprendizaje automático de UCI) y esponja (un conjunto de datos relacional). El conjunto de datos de soja tiene 307 ejemplos y 19 clases de solución, mientras que el conjunto de datos de esponja tiene 280 ejemplos y 3 clases de solución. En una ejecución experimental, el conjunto de datos se divide en 2 conjuntos: el conjunto de entrenamiento y el conjunto de prueba. Los ejemplos del conjunto de entrenamiento están distribuidos entre 5 agentes diferentes sin replicación, es decir, no hay ningún ejemplo compartido por dos agentes. En la etapa de pruebas, los problemas en el conjunto de pruebas llegan aleatoriamente a uno de los agentes, y su objetivo es predecir la solución correcta. Los experimentos están diseñados para probar dos hipótesis: (H1) que la argumentación es un marco útil para la deliberación conjunta y puede mejorar sobre otros métodos típicos como la votación; y (H2) que el aprendizaje a través de la comunicación mejora el rendimiento individual de un agente de aprendizaje que participa en un proceso de argumentación. Además, también esperamos que la mejora lograda a través de la argumentación aumente a medida que aumente el número de agentes que participan en la argumentación (ya que se tomará en cuenta más información). En relación con H1 (la argumentación es un marco útil para la deliberación conjunta), realizamos 4 experimentos, utilizando 2, 3, 4 y 5 agentes respectivamente (en todos los experimentos cada agente tiene un 20% de los datos de entrenamiento, ya que el entrenamiento siempre se distribuye entre 5 agentes). La Figura 5 muestra el resultado de esos experimentos en los conjuntos de datos de esponjas y soja. La precisión de la clasificación se representa en el eje vertical, y en el eje horizontal se muestra el número de agentes que participaron en los procesos de argumentación. Para cada número de agentes, se muestran tres barras: individual, Voting y AMAL. La barra individual muestra la precisión promedio de las predicciones de los agentes individuales; la barra de votación muestra la precisión promedio de la predicción conjunta lograda mediante votación pero sin argumentación; y finalmente la barra de AMAL muestra la precisión promedio de la predicción conjunta utilizando argumentación. Los resultados mostrados son el promedio de 5 ejecuciones de validación cruzada de 10 pliegues. La Figura 5 muestra que la colaboración (votación y AMAL) supera el rendimiento de la resolución individual de problemas. Además, como esperábamos, la precisión mejora a medida que más agentes colaboran, ya que se tiene en cuenta más información. También podemos ver que AMAL siempre supera al voto estándar, demostrando que las decisiones conjuntas se basan en una mejor información proporcionada por el proceso de argumentación. Por ejemplo, la precisión conjunta para 2 agentes en el conjunto de datos de esponjas es del 87.57% para AMAL y del 86.57% para la votación (mientras que la precisión individual es solo del 80.07%). Además, la mejora lograda por AMAL sobre Voting es aún mayor en el conjunto de datos de soja. La razón es que el conjunto de datos de soja es más difícil (en el sentido de que los agentes necesitan más datos para producir buenas predicciones). Estos resultados experimentales muestran que AMAL aprovecha de manera efectiva la oportunidad de mejora: la precisión es mayor solo porque más agentes han cambiado su opinión durante la argumentación (de lo contrario, lograrían el mismo resultado que la Votación). En relación con H2 (el aprendizaje a partir de la comunicación en procesos de argumentación mejora la predicción individual), llevamos a cabo el siguiente experimento: inicialmente, distribuimos un 25% del conjunto de entrenamiento entre los cinco agentes; después, el resto de los casos en el conjunto de entrenamiento se envía a los agentes uno por uno; cuando un agente recibe un nuevo caso de entrenamiento, tiene varias opciones: el agente puede descartarlo, el agente puede retenerlo, o el agente puede usarlo para participar en un proceso de argumentación. La Figura 6 muestra el resultado de ese experimento para los dos conjuntos de datos. La Figura 6 contiene tres gráficos, donde NL (no aprendizaje) muestra la precisión de un agente sin aprendizaje alguno; L (aprendizaje) muestra la evolución de la precisión de clasificación individual cuando los agentes aprenden reteniendo los casos de entrenamiento que reciben individualmente (notar que cuando todos los casos de entrenamiento han sido retenidos al 100%, la precisión debería ser igual a la de la Figura 5 para los agentes individuales); y finalmente LFC (aprendizaje a partir de la comunicación) muestra la evolución de la precisión de clasificación individual de los agentes de aprendizaje que también aprenden reteniendo los contraejemplos recibidos durante la argumentación (es decir, aprenden tanto de ejemplos de entrenamiento como de contraejemplos). La Figura 6 muestra que si un agente Ai también aprende de la comunicación, Ai puede mejorar significativamente su rendimiento individual con solo un pequeño número de casos adicionales (seleccionados como contraejemplos relevantes para Ai durante la argumentación). Por ejemplo, en el conjunto de datos de soja, los agentes individuales han logrado una precisión del 70.62% cuando también aprenden de la comunicación, en comparación con una precisión del 59.93% cuando solo aprenden de su experiencia individual. El número de casos aprendidos a través de la comunicación depende de las propiedades del conjunto de datos: en el conjunto de datos de esponjas, los agentes solo han retenido muy pocos casos adicionales y han mejorado significativamente la precisión individual; es decir, retienen en promedio 59.96 casos (en comparación con los 50.4 casos retenidos si no aprenden de la comunicación). En el conjunto de datos de soja se aprenden más contraejemplos para mejorar significativamente la precisión individual, es decir, retienen en promedio 87.16 casos (en comparación con 55.27 casos retenidos si no aprenden de la comunicación). Finalmente, el hecho de que ambos conjuntos de datos muestren una mejora significativa señala la naturaleza adaptativa del enfoque basado en la argumentación para el aprendizaje a partir de la comunicación: los casos útiles se seleccionan como contraejemplos (y no más de los necesarios), y tienen el efecto deseado. 10. TRABAJO RELACIONADO En cuanto al RCB en un entorno multiagente, la primera investigación se centró en la recuperación de casos negociada [11] entre grupos de agentes. Nuestro trabajo sobre el aprendizaje basado en casos multiagente comenzó en 1999; más tarde Mc Ginty y Smyth presentaron un enfoque colaborativo de CBR multiagente (CCBR) para la planificación. Finalmente, otro enfoque interesante es el razonamiento de múltiples bases de casos (MCBR) [5], que trata sobre The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 981 ESPONJA 60 65 70 75 80 85 25% 40% 55% 70% 85% 100% LFC L NL SOJA 20 30 40 50 60 70 80 90 25% 40% 55% 70% 85% 100% LFC L NL Figura 6: Aprendizaje a partir de la comunicación resultante de la argumentación en un sistema compuesto por 5 agentes. sistemas distribuidos donde hay varias bases de casos disponibles para la misma tarea y aborda los problemas de adaptación entre bases de casos. La principal diferencia es que nuestro enfoque MAC es una forma de distribuir el proceso de Reutilización de CBR (utilizando un sistema de votación) mientras que la Recuperación se realiza individualmente por cada agente; sin embargo, los otros enfoques de CBR multiagente se centran en distribuir el proceso de Recuperación. La investigación sobre la argumentación en MAS se centra en varios temas como a) lógicas, protocolos y lenguajes que respaldan la argumentación, b) selección de argumentos y c) interpretación de argumentos. Los enfoques para la lógica y los lenguajes que respaldan la argumentación incluyen la lógica no definitiva [4] y los modelos BDI [13]. Aunque la selección de argumentos es un aspecto clave de la argumentación automatizada (ver [12] y [13]), la mayoría de las investigaciones se han centrado en las relaciones de preferencia entre argumentos. En nuestro marco de trabajo hemos abordado tanto la selección de argumentos como las relaciones de preferencia utilizando un enfoque basado en casos. 11. CONCLUSIONES Y TRABAJO FUTURO En este artículo hemos presentado un marco basado en argumentación para el aprendizaje multiagente. Específicamente, hemos presentado AMAL, un marco que permite a un grupo de agentes de aprendizaje discutir sobre la solución de un problema dado y hemos demostrado cómo las capacidades de aprendizaje pueden ser utilizadas para generar argumentos y contraargumentos. La evaluación experimental muestra que la cantidad aumentada de información proporcionada a los agentes por el proceso de argumentación aumenta su precisión predictiva, especialmente cuando un número adecuado de agentes participa en la argumentación. Las principales contribuciones de este trabajo son: a) un marco de argumentación para agentes de aprendizaje; b) una relación de preferencia basada en casos sobre argumentos, basada en el cálculo de una estimación general de confianza de los argumentos; c) una política basada en casos para generar contraargumentos y seleccionar contraejemplos; y d) un enfoque basado en la argumentación para aprender a partir de la comunicación. Finalmente, en los experimentos presentados aquí, un agente de aprendizaje retendría todos los contraejemplos presentados por el otro agente; sin embargo, esta es una política de retención de casos muy simple, y nos gustaría experimentar con políticas más informadas, con el objetivo de que los agentes de aprendizaje individuales puedan mejorar significativamente utilizando solo un pequeño conjunto de casos propuestos por otros agentes. Finalmente, nuestro enfoque se centra en el aprendizaje perezoso, y los trabajos futuros tienen como objetivo incorporar el aprendizaje inductivo ansioso dentro del marco argumentativo para el aprendizaje a partir de la comunicación. 12. REFERENCIAS [1] Agnar Aamodt y Enric Plaza. Razonamiento basado en casos: Aspectos fundamentales, variaciones metodológicas y enfoques de sistemas. Comunicaciones de Inteligencia Artificial, 7(1):39-59, 1994. [2] E. Armengol y E. Plaza. Inducción perezosa de descripciones para el aprendizaje basado en casos relacionales. En ECML2001, páginas 13-24, 2001. [3] Gerhard Brewka. Sistemas de argumentación dinámica: Un modelo formal de procesos de argumentación basado en el cálculo de situaciones. Revista de Lógica y Computación, 11(2):257-282, 2001. [4] Carlos I. Chesñevar y Guillermo R. Simari. Formalizando la argumentación no definitiva utilizando sistemas deductivos etiquetados. Revista de Ciencias de la Computación y Tecnología, 1(4):18-33, 2000. [5] D. Leake y R. Sooriamurthi. Seleccionando estrategias automáticamente para el razonamiento multi-base de casos. En S. Craw y A. Preece, editores, ECCBR2002, páginas 204-219, Berlín, 2002. Springer Verlag. [6] Francisco J. Martín, Enric Plaza y Josep-Lluis Arcos. Reutilización de conocimiento y experiencia a través de comunicaciones entre agentes competentes (pares). Revista Internacional de Ingeniería de Software e Ingeniería del Conocimiento, 9(3):319-341, 1999. [7] Lorraine McGinty y Barry Smyth. Razonamiento basado en casos colaborativo: Aplicaciones en la planificación de rutas personalizadas. En I. Watson y Q. Yang, editores, ICCBR, número 2080 en LNAI, páginas 362-376. Springer-Verlag, 2001. [8] Santi Ontañón y Enric Plaza. Aprendizaje multiagente basado en justificaciones. En ICML2003, páginas 576-583. Morgan Kaufmann, 2003. [9] Enric Plaza, Eva Armengol y Santiago Ontañón. El poder explicativo de la similitud simbólica en el razonamiento basado en casos. Revisión de Inteligencia Artificial, 24(2):145-161, 2005. [10] David Poole. En la comparación de teorías: prefiriendo la explicación más específica. En IJCAI-85, páginas 144-147, 1985. [11] M V Nagendra Prassad, Victor R Lesser y Susan Lander. Recuperación y razonamiento en bases de casos distribuidas. Informe técnico, Departamento de Ciencias de la Computación de UMass, 1995. [12] K. Sycara S. Kraus y A. Evenchik. Alcanzando acuerdos a través de la argumentación: un modelo lógico y su implementación. Revista de Inteligencia Artificial, 104:1-69, 1998. [13] N. R. Jennings, S. Parsons, C. Sierra. Agentes que razonan y negocian mediante argumentos. Revista de Lógica y Computación, 8:261-292, 1998. [14] Bruce A. Wooley. Componente de explicación de sistemas de software. ACM CrossRoads, 5.1, 1998. 982 The Sixth Intl. \n\nACM CrossRoads, 5.1, 1998. 982 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "argumentation framework": {
            "translated_key": "marco de argumentación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning and Joint Deliberation through Argumentation in Multi-Agent Systems Santi Ontañón CCL, Cognitive Computing Lab Georgia Institute of Technology Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Artificial Intelligence Research Institute CSIC, Spanish Council for Scientific Research Campus UAB, 08193 Bellaterra, Catalonia (Spain) enric@iiia.csic.es ABSTRACT In this paper we will present an <br>argumentation framework</br> for learning agents (AMAL) designed for two purposes: (1) for joint deliberation, and (2) for learning from communication.",
                "The AMAL framework is completely based on learning from examples: the argument preference relation, the argument generation policy, and the counterargument generation policy are case-based techniques.",
                "For join deliberation, learning agents share their experience by forming a committee to decide upon some joint decision.",
                "We experimentally show that the argumentation among committees of agents improves both the individual and joint performance.",
                "For learning from communication, an agent engages into arguing with other agents in order to contrast its individual hypotheses and receive counterexamples; the argumentation process improves their learning scope and individual performance.",
                "Categories and Subject Descriptors I.2.6 [Artificial Intelligence]: Learning; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems, Intelligent Agents 1.",
                "INTRODUCTION Argumentation frameworks for multi-agent systems can be used for different purposes like joint deliberation, persuasion, negotiation, and conflict resolution.",
                "In this paper we will present an <br>argumentation framework</br> for learning agents, and show that it can be used for two purposes: (1) joint deliberation, and (2) learning from communication.",
                "Argumentation-based joint deliberation involves discussion over the outcome of a particular situation or the appropriate course of action for a particular situation.",
                "Learning agents are capable of learning from experience, in the sense that past examples (situations and their outcomes) are used to predict the outcome for the situation at hand.",
                "However, since individual agents experience may be limited, individual knowledge and prediction accuracy is also limited.",
                "Thus, learning agents that are capable of arguing their individual predictions with other agents may reach better prediction accuracy after such an argumentation process.",
                "Most existing argumentation frameworks for multi-agent systems are based on deductive logic or some other deductive logic formalism specifically designed to support argumentation, such as default logic [3]).",
                "Usually, an argument is seen as a logical statement, while a counterargument is an argument offered in opposition to another argument [4, 13]; agents use a preference relation to resolve conflicting arguments.",
                "However, logic-based argumentation frameworks assume agents with preloaded knowledge and preference relation.",
                "In this paper, we focus on an Argumentation-based Multi-Agent Learning (AMAL) framework where both knowledge and preference relation are learned from experience.",
                "Thus, we consider a scenario with agents that (1) work in the same domain using a shared ontology, (2) are capable of learning from examples, and (3) communicate using an argumentative framework.",
                "Having learning capabilities allows agents effectively use a specific form of counterargument, namely the use of counterexamples.",
                "Counterexamples offer the possibility of agents learning during the argumentation process.",
                "Moreover, learning agents allow techniques that use learnt experience to generate adequate arguments and counterarguments.",
                "Specifically, we will need to address two issues: (1) how to define a technique to generate arguments and counterarguments from examples, and (2)how to define a preference relation over two conflicting arguments that have been induced from examples.",
                "This paper presents a case-based approach to address both issues.",
                "The agents use case-based reasoning (CBR) [1] to learn from past cases (where a case is a situation and its outcome) in order to predict the outcome of a new situation.",
                "We propose an argumentation protocol inside the AMAL framework at supports agents in reaching a joint prediction over a specific situation or problem - moreover, the reasoning needed to support the argumentation process will also be based on cases.",
                "In particular, we present two case-based measures, one for generating the arguments and counterarguments adequate to a particular situation and another for determining preference relation among arguments.",
                "Finally, we evaluate (1) if argumentation between learning agents can produce a joint prediction that improves over individual learning performance and (2) if learning from the counterexamples conveyed during the argumentation process increases the individual performance with precisely those cases being used while arguing among them.",
                "The paper is structured as follows.",
                "Section 2 discusses the relation among argumentation, collaboration and learning.",
                "Then Section 3 introduces our multi-agent CBR (MAC) framework and the notion of justified prediction.",
                "After that, Section 4 formally defines our <br>argumentation framework</br>.",
                "Sections 5 and 6 present our case-based preference relation and argument generation policies respectively.",
                "Later, Section 7 presents the argumentation protocol in our AMAL framework.",
                "After that, Section 8 presents an exemplification of the <br>argumentation framework</br>.",
                "Finally, Section 9 presents an empirical evaluation of our two main hypotheses.",
                "The paper closes with related work and conclusions sections. 2.",
                "ARGUMENTATION,COLLABORATION AND LEARNING Both learning and collaboration are ways in which an agent can improve individual performance.",
                "In fact, there is a clear parallelism between learning and collaboration in multi-agent systems, since both are ways in which agents can deal with their shortcomings.",
                "Let us show which are the main motivations that an agent can have to learn or to collaborate. • Motivations to learn: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems. • Motivations to collaborate: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems, - Increase the range of accessible resources.",
                "Looking at the above lists of motivation, we can easily see that learning and collaboration are very related in multi-agent systems.",
                "In fact, with the exception of the last item in the motivations to collaborate list, they are two extremes of a continuum of strategies to improve performance.",
                "An agent may choose to increase performance by learning, by collaborating, or by finding an intermediate point that combines learning and collaboration in order to improve performance.",
                "In this paper we will propose AMAL, an <br>argumentation framework</br> for learning agents, and will also also show how AMAL can be used both for learning from communication and for solving problems in a collaborative way: • Agents can solve problems in a collaborative way via engaging an argumentation process about the prediction for the situation at hand.",
                "Using this collaboration, the prediction can be done in a more informed way, since the information known by several agents has been taken into account. • Agents can also learn from communication with other agents by engaging an argumentation process.",
                "Agents that engage in such argumentation processes can learn from the arguments and counterexamples received from other agents, and use this information for predicting the outcomes of future situations.",
                "In the rest of this paper we will propose an <br>argumentation framework</br> and show how it can be used both for learning and for solving problems in a collaborative way. 3.",
                "MULTI-AGENT CBR SYSTEMS A Multi-Agent Case Based Reasoning System (MAC) M = {(A1, C1), ..., (An, Cn)} is a multi-agent system composed of A = {Ai, ..., An}, a set of CBR agents, where each agent Ai ∈ A possesses an individual case base Ci.",
                "Each individual agent Ai in a MAC is completely autonomous and each agent Ai has access only to its individual and private case base Ci.",
                "A case base Ci = {c1, ..., cm} is a collection of cases.",
                "Agents in a MAC system are able to individually solve problems, but they can also collaborate with other agents to solve problems.",
                "In this framework, we will restrict ourselves to analytical tasks, i.e. tasks like classification, where the solution of a problem is achieved by selecting a solution class from an enumerated set of solution classes.",
                "In the following we will note the set of all the solution classes by S = {S1, ..., SK }.",
                "Therefore, a case c = P, S is a tuple containing a case description P and a solution class S ∈ S. In the following, we will use the terms problem and case description indistinctly.",
                "Moreover, we will use the dot notation to refer to elements inside a tuple; e.g., to refer to the solution class of a case c, we will write c.S.",
                "Therefore, we say a group of agents perform joint deliberation, when they collaborate to find a joint solution by means of an argumentation process.",
                "However, in order to do so, an agent has to be able to justify its prediction to the other agents (i.e. generate an argument for its predicted solution that can be examined and critiqued by the other agents).",
                "The next section addresses this issue. 3.1 Justified Predictions Both expert systems and CBR systems may have an explanation component [14] in charge of justifying why the system has provided a specific answer to the user.",
                "The line of reasoning of the system can then be examined by a human expert, thus increasing the reliability of the system.",
                "Most of the existing work on explanation generation focuses on generating explanations to be provided to the user.",
                "However, in our approach we use explanations (or justifications) as a tool for improving communication and coordination among agents.",
                "We are interested in justifications since they can be used as arguments.",
                "For that purpose, we will benefit from the ability of some machine learning methods to provide justifications.",
                "A justification built by a CBR method after determining that the solution of a particular problem P was Sk is a description that contains the relevant information from the problem P that the CBR method has considered to predict Sk as the solution of P. In particular, CBR methods work by retrieving similar cases to the problem at hand, and then reusing their solutions for the current problem, expecting that since the problem and the cases are similar, the solutions will also be similar.",
                "Thus, if a CBR method has retrieved a set of cases C1, ..., Cn to solve a particular problem P the justification built will contain the relevant information from the problem P that made the CBR system retrieve that particular set of cases, i.e. it will contain the relevant information that P and C1, ..., Cn have in common.",
                "For example, Figure 1 shows a justification build by a CBR system for a toy problem (in the following sections we will show justifications for real problems).",
                "In the figure, a problem has two attributes (Traffic_light, and Cars_passing), the retrieval mechanism of the CBR system notices that by considering only the attribute Traffic_light, it can retrieve two cases that predict the same solution: wait.",
                "Thus, since only this attribute has been used, it is the only one appearing in the justification.",
                "The values of the rest of attributes are irrelevant, since whatever their value the solution class would have been the same. 976 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Problem Traffic_light: red Cars_passing: no Case 1 Traffic_light: red Cars_passing: no Solution: wait Case 3 Traffic_light: red Cars_passing: yes Solution: wait Case 4 Traffic_light: green Cars_passing: yes Solution: wait Case 2 Traffic_light: green Cars_passing: no Solution: cross Retrieved cases Solution: wait Justification Traffic_light: red Figure 1: An example of justification generation in a CBR system.",
                "Notice that, since the only relevant feature to decide is Traffic_light (the only one used to retrieve cases), it is the only one appearing in the justification.",
                "In general, the meaning of a justification is that all (or most of) the cases in the case base of an agent that satisfy the justification (i.e. all the cases that are subsumed by the justification) belong to the predicted solution class.",
                "In the rest of the paper, we will use to denote the subsumption relation.",
                "In our work, we use LID [2], a CBR method capable of building symbolic justifications such as the one exemplified in Figure 1.",
                "When an agent provides a justification for a prediction, the agent generates a justified prediction: DEFINITION 3.1.",
                "A Justified Prediction is a tuple J = A, P, S, D where agent A considers S the correct solution for problem P, and that prediction is justified a symbolic description D such that J.D J.P. Justifications can have many uses for CBR systems [8, 9].",
                "In this paper, we are going to use justifications as arguments, in order to allow learning agents to engage in argumentation processes. 4.",
                "ARGUMENTS AND COUNTERARGUMENTS For our purposes an argument α generated by an agent A is composed of a statement S and some evidence D supporting S as correct.",
                "In the remainder of this section we will see how this general definition of argument can be instantiated in specific kind of arguments that the agents can generate.",
                "In the context of MAC systems, agents argue about predictions for new problems and can provide two kinds of information: a) specific cases P, S , and b) justified predictions: A, P, S, D .",
                "Using this information, we can define three types of arguments: justified predictions, counterarguments, and counterexamples.",
                "A justified prediction α is generated by an agent Ai to argue that Ai believes that the correct solution for a given problem P is α.S, and the evidence provided is the justification α.D.",
                "In the example depicted in Figure 1, an agent Ai may generate the argument α = Ai, P, Wait, (Traffic_light = red) , meaning that the agent Ai believes that the correct solution for P is Wait because the attribute Traffic_light equals red.",
                "A counterargument β is an argument offered in opposition to another argument α.",
                "In our framework, a counterargument consists of a justified prediction Aj, P, S , D generated by an agent Aj with the intention to rebut an argument α generated by another agent Ai, that endorses a solution class S different from that of α.S for the problem at hand and justifies this with a justification D .",
                "In the example in Figure 1, if an agent generates the argument α = Ai, P, Walk, (Cars_passing = no) , an agent that thinks that the correct solution is Wait might answer with the counterargument β = Aj, P, Wait, (Cars_passing = no ∧ Traffic_light = red) , meaning that, although there are no cars passing, the traffic light is red, and the street cannot be crossed.",
                "A counterexample c is a case that contradicts an argument α.",
                "Thus a counterexample is also a counterargument, one that states that a specific argument α is not always true, and the evidence provided is the case c. Specifically, for a case c to be a counterexample of an argument α, the following conditions have to be met: α.D c and α.S = c.S, i.e. the case must satisfy the justification α.D and the solution of c must be different than the predicted by α.",
                "By exchanging arguments and counterarguments (including counterexamples), agents can argue about the correct solution of a given problem, i.e. they can engage a joint deliberation process.",
                "However, in order to do so, they need a specific interaction protocol, a preference relation between contradicting arguments, and a decision policy to generate counterarguments (including counterexamples).",
                "In the following sections we will present these elements. 5.",
                "PREFERENCE RELATION A specific argument provided by an agent might not be consistent with the information known to other agents (or even to some of the information known by the agent that has generated the justification due to noise in training data).",
                "For that reason, we are going to define a preference relation over contradicting justified predictions based on cases.",
                "Basically, we will define a confidence measure for each justified prediction (that takes into account the cases owned by each agent), and the justified prediction with the highest confidence will be the preferred one.",
                "The idea behind case-based confidence is to count how many of the cases in an individual case base endorse a justified prediction, and how many of them are counterexamples of it.",
                "The more the endorsing cases, the higher the confidence; and the more the counterexamples, the lower the confidence.",
                "Specifically, to assess the confidence of a justified prediction α, an agent obtains the set of cases in its individual case base that are subsumed by α.D.",
                "With them, an agent Ai obtains the Y (aye) and N (nay) values: • Y Ai α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by the justification α.D that belong to the solution class α.S, • NAi α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by justification α.D that do not belong to that solution class.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 977 + + + + + +  - - + Figure 2: Confidence of arguments is evaluated by contrasting them against the case bases of the agents.",
                "An agent estimates the confidence of an argument as: CAi (α) = Y Ai α 1 + Y Ai α + NAi α i.e. the confidence on a justified prediction is the number of endorsing cases divided by the number of endorsing cases plus counterexamples.",
                "Notice that we add 1 to the denominator, this is to avoid giving excessively high confidences to justified predictions whose confidence has been computed using a small number of cases.",
                "Notice that this correction follows the same idea than the Laplace correction to estimate probabilities.",
                "Figure 2 illustrates the individual evaluation of the confidence of an argument, in particular, three endorsing cases and one counterexample are found in the case base of agents Ai, giving an estimated confidence of 0.6 Moreover, we can also define the joint confidence of an argument α as the confidence computed using the cases present in the case bases of all the agents in the group: C(α) = i Y Ai α 1 + i Y Ai α + NAi α Notice that, to collaboratively compute the joint confidence, the agents only have to make public the aye and nay values locally computed for a given argument.",
                "In our framework, agents use this joint confidence as the preference relation: a justified prediction α is preferred over another one β if C(α) ≥ C(β). 6.",
                "GENERATION OF ARGUMENTS In our framework, arguments are generated by the agents from cases, using learning methods.",
                "Any learning method able to provide a justified prediction can be used to generate arguments.",
                "For instance, decision trees and LID [2] are suitable learning methods.",
                "Specifically, in the experiments reported in this paper agents use LID.",
                "Thus, when an agent wants to generate an argument endorsing that a specific solution class is the correct solution for a problem P, it generates a justified prediction as explained in Section 3.1.",
                "For instance, Figure 3 shows a real justification generated by LID after solving a problem P in the domain of marine sponges identification.",
                "In particular, Figure 3 shows how when an agent receives a new problem to solve (in this case, a new sponge to determine its order), the agent uses LID to generate an argument (consisting on a justified prediction) using the cases in the case base of the agent.",
                "The justification shown in Figure 3 can be interpreted saying that the predicted solution is hadromerida because the smooth form of the megascleres of the spiculate skeleton of the sponge is of type tylostyle, the spikulate skeleton of the sponge has no uniform length, and there is no gemmules in the external features of the sponge.",
                "Thus, the argument generated will be α = A1, P, hadromerida, D1 . 6.1 Generation of Counterarguments As previously stated, agents may try to rebut arguments by generating counterargument or by finding counterexamples.",
                "Let us explain how they can be generated.",
                "An agent Ai wants to generate a counterargument β to rebut an argument α when α is in contradiction with the local case base of Ai.",
                "Moreover, while generating such counterargument β, Ai expects that β is preferred over α.",
                "For that purpose, we will present a specific policy to generate counterarguments based on the specificity criterion [10].",
                "The specificity criterion is widely used in deductive frameworks for argumentation, and states that between two conflicting arguments, the most specific should be preferred since it is, in principle, more informed.",
                "Thus, counterarguments generated based on the specificity criterion are expected to be preferable (since they are more informed) to the arguments they try to rebut.",
                "However, there is no guarantee that such counterarguments will always win, since, as we have stated in Section 5, agents in our framework use a preference relation based on joint confidence.",
                "Moreover, one may think that it would be better that the agents generate counterarguments based on the joint confidence preference relation; however it is not obvious how to generate counterarguments based on joint confidence in an efficient way, since collaboration is required in order to evaluate joint confidence.",
                "Thus, the agent generating the counterargument should constantly communicate with the other agents at each step of the induction algorithm used to generate counterarguments (presently one of our future research lines).",
                "Thus, in our framework, when an agent wants to generate a counterargument β to an argument α, β has to be more specific than α (i.e. α.D < β.D).",
                "The generation of counterarguments using the specificity criterion imposes some restrictions over the learning method, although LID or ID3 can be easily adapted for this task.",
                "For instance, LID is an algorithm that generates a description starting from scratch and heuristically adding features to that term.",
                "Thus, at every step, the description is made more specific than in the previous step, and the number of cases that are subsumed by that description is reduced.",
                "When the description covers only (or almost only) cases of a single solution class LID terminates and predicts that solution class.",
                "To generate a counterargument to an argument α LID just has to use as starting point the description α.D instead of starting from scratch.",
                "In this way, the justification provided by LID will always be subsumed by α.D, and thus the resulting counterargument will be more specific than α.",
                "However, notice that LID may sometimes not be able to generate counterarguments, since LID may not be able to specialize the description α.D any further, or because the agent Ai has no case inCi that is subsumed by α.D.",
                "Figure 4 shows how an agent A2 that disagreed with the argument shown in Figure 3, generates a counterargument using LID.",
                "Moreover, Figure 4 shows the generation of a counterargument β1 2 for the argument α0 1 (in Figure 3) that is a specialization of α0 1. 978 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Solution: hadromerida Justification: D1 Sponge Spikulate skeleton External features External features Gemmules: no Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Case Base of A1 LID New sponge P Figure 3: Example of a real justification generated by LID in the marine sponges data set.",
                "Specifically, in our experiments, when an agent Ai wants to rebut an argument α, uses the following policy: 1.",
                "Agent Ai uses LID to try to find a counterargument β more specific than α; if found, β is sent to the other agent as a counterargument of α. 2.",
                "If not found, then Ai searches for a counterexample c ∈ Ci of α.",
                "If a case c is found, then c is sent to the other agent as a counterexample of α. 3.",
                "If no counterexamples are found, then Ai cannot rebut the argument α. 7.",
                "ARGUMENTATION-BASED MULTI-AGENT LEARNING The interaction protocol of AMAL allows a group of agents A1, ..., An to deliberate about the correct solution of a problem P by means of an argumentation process.",
                "If the argumentation process arrives to a consensual solution, the joint deliberation ends; otherwise a weighted vote is used to determine the joint solution.",
                "Moreover, AMAL also allows the agents to learn from the counterexamples received from other agents.",
                "The AMAL protocol consists on a series of rounds.",
                "In the initial round, each agent states which is its individual prediction for P. Then, at each round an agent can try to rebut the prediction made by any of the other agents.",
                "The protocol uses a token passing mechanism so that agents (one at a time) can send counterarguments or counterexamples if they disagree with the prediction made by any other agent.",
                "Specifically, each agent is allowed to send one counterargument or counterexample each time he gets the token (notice that this restriction is just to simplify the protocol, and that it does not restrict the number of counterargument an agent can sent, since they can be delayed for subsequent rounds).",
                "When an agent receives a counterargument or counterexample, it informs the other agents if it accepts the counterargument (and changes its prediction) or not.",
                "Moreover, agents have also the opportunity to answer to counterarguments when they receive the token, by trying to generate a counterargument to the counterargument.",
                "When all the agents have had the token once, the token returns to the first agent, and so on.",
                "If at any time in the protocol, all the agents agree or during the last n rounds no agent has generated any counterargument, the protocol ends.",
                "Moreover, if at the end of the argumentation the agents have not reached an agreement, then a voting mechanism that uses the confidence of each prediction as weights is used to decide the final solution (Thus, AMAL follows the same mechanism as human committees, first each individual member of a committee exposes his arguments and discuses those of the other members (joint deliberation), and if no consensus is reached, then a voting mechanism is required).",
                "At each iteration, agents can use the following performatives: • assert(α): the justified prediction held during the next round will be α.",
                "An agent can only hold a single prediction at each round, thus is multiple asserts are send, only the last one is considered as the currently held prediction. • rebut(β, α): the agent has found a counterargument β to the prediction α.",
                "We will define Ht = αt 1, ..., αt n as the predictions that each of the n agents hold at a round t. Moreover, we will also define contradict(αt i) = {α ∈ Ht|α.S = αt i.S} as the set of contradicting arguments for an agent Ai in a round t, i.e. the set of arguments at round t that support a different solution class than αt i.",
                "The protocol is initiated because one of the agents receives a problem P to be solved.",
                "After that, the agent informs all the other agents about the problem P to solve, and the protocol starts: 1.",
                "At round t = 0, each one of the agents individually solves P, and builds a justified prediction using its own CBR method.",
                "Then, each agent Ai sends the performative assert(α0 i ) to the other agents.",
                "Thus, the agents know H0 = α0 i , ..., α0 n .",
                "Once all the predictions have been sent the token is given to the first agent A1. 2.",
                "At each round t (other than 0), the agents check whether their arguments in Ht agree.",
                "If they do, the protocol moves to step 5.",
                "Moreover, if during the last n rounds no agent has sent any counterexample or counterargument, the protocol also moves to step 5.",
                "Otherwise, the agent Ai owner of the token tries to generate a counterargument for each of the opposing arguments in contradict(αt i) ⊆ Ht (see Section 6.1).",
                "Then, the counterargument βt i against the prediction αt j with the lowest confidence C(αt j) is selected (since αt j is the prediction more likely to be successfully rebutted). • If βt i is a counterargument, then, Ai locally compares αt i with βt i by assessing their confidence against its individual case base Ci (see Section 5) (notice that Ai is comparing its previous argument with the counterargument that Ai itself has just generated and that is about The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 979 Sponge Spikulate skeleton External features External features Gemmules: no Growing: Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Growing Grow: massive Case Base of A2 LID Solution: astrophorida Justification: D2 Figure 4: Generation of a counterargument using LID in the sponges data set. to send to Aj).",
                "If CAi (βt i ) > CAi (αt i), then Ai considers that βt i is stronger than its previous argument, changes its argument to βt i by sending assert(βt i ) to the rest of the agents (the intuition behind this is that since a counterargument is also an argument, Ai checks if the newly counterargument is a better argument than the one he was previously holding) and rebut(βt i , αt j) to Aj.",
                "Otherwise (i.e.",
                "CAi (βt i ) ≤ CAi (αt i)), Ai will send only rebut(βt i , αt j) to Aj.",
                "In any of the two situations the protocol moves to step 3. • If βt i is a counterexample c, then Ai sends rebut(c, αt j) to Aj.",
                "The protocol moves to step 4. • If Ai cannot generate any counterargument or counterexample, the token is sent to the next agent, a new round t + 1 starts, and the protocol moves to state 2. 3.",
                "The agent Aj that has received the counterargument βt i , locally compares it against its own argument, αt j, by locally assessing their confidence.",
                "If CAj (βt i ) > CAj (αt j), then Aj will accept the counterargument as stronger than its own argument, and it will send assert(βt i ) to the other agents.",
                "Otherwise (i.e.",
                "CAj (βt i ) ≤ CAj (αt j)), Aj will not accept the counterargument, and will inform the other agents accordingly.",
                "Any of the two situations start a new round t + 1, Ai sends the token to the next agent, and the protocol moves back to state 2. 4.",
                "The agent Aj that has received the counterexample c retains it into its case base and generates a new argument αt+1 j that takes into account c, and informs the rest of the agents by sending assert(αt+1 j ) to all of them.",
                "Then, Ai sends the token to the next agent, a new round t + 1 starts, and the protocol moves back to step 2. 5.",
                "The protocol ends yielding a joint prediction, as follows: if the arguments in Ht agree then their prediction is the joint prediction, otherwise a voting mechanism is used to decide the joint prediction.",
                "The voting mechanism uses the joint confidence measure as the voting weights, as follows: S = arg max Sk∈S αi∈Ht|αi.S=Sk C(αi) Moreover, in order to avoid infinite iterations, if an agent sends twice the same argument or counterargument to the same agent, the message is not considered. 8.",
                "EXEMPLIFICATION Let us consider a system composed of three agents A1, A2 and A3.",
                "One of the agents, A1 receives a problem P to solve, and decides to use AMAL to solve it.",
                "For that reason, invites A2 and A3 to take part in the argumentation process.",
                "They accept the invitation, and the argumentation protocol starts.",
                "Initially, each agent generates its individual prediction for P, and broadcasts it to the other agents.",
                "Thus, all of them can compute H0 = α0 1, α0 2, α0 3 .",
                "In particular, in this example: • α0 1 = A1, P, hadromerida, D1 • α0 2 = A2, P, astrophorida, D2 • α0 3 = A3, P, axinellida, D3 A1 starts owning the token and tries to generate counterarguments for α0 2 and α0 3, but does not succeed, however it has one counterexample c13 for α0 3.",
                "Thus, A1 sends the the message rebut( c13, α0 3) to A3.",
                "A3 incorporates c13 into its case base and tries to solve the problem P again, now taking c13 into consideration.",
                "A3 comes up with the justified prediction α1 3 = A3, P, hadromerida, D4 , and broadcasts it to the rest of the agents with the message assert(α1 3).",
                "Thus, all of them know the new H1 = α0 1, α0 2, α1 3 .",
                "Round 1 starts and A2 gets the token.",
                "A2 tries to generate counterarguments for α0 1 and α1 3 and only succeeds to generate a counterargument β1 2 = A2, P, astrophorida, D5 against α1 3.",
                "The counterargument is sent to A3 with the message rebut(β1 2 , α1 3).",
                "Agent A3 receives the counterargument and assesses its local confidence.",
                "The result is that the individual confidence of the counterargument β1 2 is lower than the local confidence of α1 3.",
                "Therefore, A3 does not accept the counterargument, and thus H2 = α0 1, α0 2, α1 3 .",
                "Round 2 starts and A3 gets the token.",
                "A3 generates a counterargument β2 3 = A3, P, hadromerida, D6 for α0 2 and sends it to A2 with the message rebut(β2 3 , α0 2).",
                "Agent A2 receives the counterargument and assesses its local confidence.",
                "The result is that the local confidence of the counterargument β2 3 is higher than the local confidence of α0 2.",
                "Therefore, A2 accepts the counterargument and informs the rest of the agents with the message assert(β2 3 ).",
                "After that, H3 = α0 1, β2 3 , α1 3 .",
                "At Round 3, since all the agents agree (all the justified predictions in H3 predict hadromerida as the solution class) The protocol ends, and A1 (the agent that received the problem) considers hadromerida as the joint solution for the problem P. 9.",
                "EXPERIMENTAL EVALUATION 980 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) SPONGE 75 77 79 81 83 85 87 89 91 2 3 4 5 AMAL Voting Individual SOYBEAN 55 60 65 70 75 80 85 90 2 3 4 5 AMAL Voting Individual Figure 5: Individual and joint accuracy for 2 to 5 agents.",
                "In this section we empirically evaluate the AMAL <br>argumentation framework</br>.",
                "We have made experiments in two different data sets: soybean (from the UCI machine learning repository) and sponge (a relational data set).",
                "The soybean data set has 307 examples and 19 solution classes, while the sponge data set has 280 examples and 3 solution classes.",
                "In an experimental run, the data set is divided in 2 sets: the training set and the test set.",
                "The training set examples are distributed among 5 different agents without replication, i.e. there is no example shared by two agents.",
                "In the testing stage, problems in the test set arrive randomly to one of the agents, and their goal is to predict the correct solution.",
                "The experiments are designed to test two hypotheses: (H1) that argumentation is a useful framework for joint deliberation and can improve over other typical methods such as voting; and (H2) that learning from communication improves the individual performance of a learning agent participating in an argumentation process.",
                "Moreover, we also expect that the improvement achieved from argumentation will increase as the number of agents participating in the argumentation increases (since more information will be taken into account).",
                "Concerning H1 (argumentation is a useful framework for joint deliberation), we ran 4 experiments, using 2, 3, 4, and 5 agents respectively (in all experiments each agent has a 20% of the training data, since the training is always distributed among 5 agents).",
                "Figure 5 shows the result of those experiments in the sponge and soybean data sets.",
                "Classification accuracy is plotted in the vertical axis, and in the horizontal axis the number of agents that took part in the argumentation processes is shown.",
                "For each number of agents, three bars are shown: individual, Voting, and AMAL.",
                "The individual bar shows the average accuracy of individual agents predictions; the voting bar shows the average accuracy of the joint prediction achieved by voting but without any argumentation; and finally the AMAL bar shows the average accuracy of the joint prediction using argumentation.",
                "The results shown are the average of 5 10-fold cross validation runs.",
                "Figure 5 shows that collaboration (voting and AMAL) outperforms individual problem solving.",
                "Moreover, as we expected, the accuracy improves as more agents collaborate, since more information is taken into account.",
                "We can also see that AMAL always outperforms standard voting, proving that joint decisions are based on better information as provided by the argumentation process.",
                "For instance, the joint accuracy for 2 agents in the sponge data set is of 87.57% for AMAL and 86.57% for voting (while individual accuracy is just 80.07%).",
                "Moreover, the improvement achieved by AMAL over Voting is even larger in the soybean data set.",
                "The reason is that the soybean data set is more difficult (in the sense that agents need more data to produce good predictions).",
                "These experimental results show that AMAL effectively exploits the opportunity for improvement: the accuracy is higher only because more agents have changed their opinion during argumentation (otherwise they would achieve the same result as Voting).",
                "Concerning H2 (learning from communication in argumentation processes improves individual prediction ), we ran the following experiment: initially, we distributed a 25% of the training set among the five agents; after that, the rest of the cases in the training set is sent to the agents one by one; when an agent receives a new training case, it has several options: the agent can discard it, the agent can retain it, or the agent can use it for engaging an argumentation process.",
                "Figure 6 shows the result of that experiment for the two data sets.",
                "Figure 6 contains three plots, where NL (not learning) shows accuracy of an agent with no learning at all; L (learning), shows the evolution of the individual classification accuracy when agents learn by retaining the training cases they individually receive (notice that when all the training cases have been retained at 100%, the accuracy should be equal to that of Figure 5 for individual agents); and finally LFC (learning from communication) shows the evolution of the individual classification accuracy of learning agents that also learn by retaining those counterexamples received during argumentation (i.e. they learn both from training examples and counterexamples).",
                "Figure 6 shows that if an agent Ai learns also from communication, Ai can significantly improve its individual performance with just a small number of additional cases (those selected as relevant counterexamples for Ai during argumentation).",
                "For instance, in the soybean data set, individual agents have achieved an accuracy of 70.62% when they also learn from communication versus an accuracy of 59.93% when they only learn from their individual experience.",
                "The number of cases learnt from communication depends on the properties of the data set: in the sponges data set, agents have retained only very few additional cases, and significantly improved individual accuracy; namely they retain 59.96 cases in average (compared to the 50.4 cases retained if they do not learn from communication).",
                "In the soybean data set more counterexamples are learnt to significantly improve individual accuracy, namely they retain 87.16 cases in average (compared to 55.27 cases retained if they do not learn from communication).",
                "Finally, the fact that both data sets show a significant improvement points out the adaptive nature of the argumentation-based approach to learning from communication: the useful cases are selected as counterexamples (and no more than those needed), and they have the intended effect. 10.",
                "RELATED WORK Concerning CBR in a multi-agent setting, the first research was on negotiated case retrieval [11] among groups of agents.",
                "Our work on multi-agent case-based learning started in 1999 [6]; later Mc Ginty and Smyth [7] presented a multi-agent collaborative CBR approach (CCBR) for planning.",
                "Finally, another interesting approach is multi-case-base reasoning (MCBR) [5], that deals with The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 981 SPONGE 60 65 70 75 80 85 25% 40% 55% 70% 85% 100% LFC L NL SOYBEAN 20 30 40 50 60 70 80 90 25% 40% 55% 70% 85% 100% LFC L NL Figure 6: Learning from communication resulting from argumentation in a system composed of 5 agents. distributed systems where there are several case bases available for the same task and addresses the problems of cross-case base adaptation.",
                "The main difference is that our MAC approach is a way to distribute the Reuse process of CBR (using a voting system) while Retrieve is performed individually by each agent; the other multiagent CBR approaches, however, focus on distributing the Retrieve process.",
                "Research on MAS argumentation focus on several issues like a) logics, protocols and languages that support argumentation, b) argument selection and c) argument interpretation.",
                "Approaches for logic and languages that support argumentation include defeasible logic [4] and BDI models [13].",
                "Although argument selection is a key aspect of automated argumentation (see [12] and [13]), most research has been focused on preference relations among arguments.",
                "In our framework we have addressed both argument selection and preference relations using a case-based approach. 11.",
                "CONCLUSIONS AND FUTURE WORK In this paper we have presented an argumentation-based framework for multi-agent learning.",
                "Specifically, we have presented AMAL, a framework that allows a group of learning agents to argue about the solution of a given problem and we have shown how the learning capabilities can be used to generate arguments and counterarguments.",
                "The experimental evaluation shows that the increased amount of information provided to the agents by the argumentation process increases their predictive accuracy, and specially when an adequate number of agents take part in the argumentation.",
                "The main contributions of this work are: a) an <br>argumentation framework</br> for learning agents; b) a case-based preference relation over arguments, based on computing an overall confidence estimation of arguments; c) a case-based policy to generate counterarguments and select counterexamples; and d) an argumentation-based approach for learning from communication.",
                "Finally, in the experiments presented here a learning agent would retain all counterexamples submitted by the other agent; however, this is a very simple case retention policy, and we will like to experiment with more informed policies - with the goal that individual learning agents could significantly improve using only a small set of cases proposed by other agents.",
                "Finally, our approach is focused on lazy learning, and future works aims at incorporating eager inductive learning inside the argumentative framework for learning from communication. 12.",
                "REFERENCES [1] Agnar Aamodt and Enric Plaza.",
                "Case-based reasoning: Foundational issues, methodological variations, and system approaches.",
                "Artificial Intelligence Communications, 7(1):39-59, 1994. [2] E. Armengol and E. Plaza.",
                "Lazy induction of descriptions for relational case-based learning.",
                "In ECML2001, pages 13-24, 2001. [3] Gerhard Brewka.",
                "Dynamic argument systems: A formal model of argumentation processes based on situation calculus.",
                "Journal of Logic and Computation, 11(2):257-282, 2001. [4] Carlos I. Chesñevar and Guillermo R. Simari.",
                "Formalizing Defeasible Argumentation using Labelled Deductive Systems.",
                "Journal of Computer Science & Technology, 1(4):18-33, 2000. [5] D. Leake and R. Sooriamurthi.",
                "Automatically selecting strategies for multi-case-base reasoning.",
                "In S. Craw and A. Preece, editors, ECCBR2002, pages 204-219, Berlin, 2002.",
                "Springer Verlag. [6] Francisco J. Martín, Enric Plaza, and Josep-Lluis Arcos.",
                "Knowledge and experience reuse through communications among competent (peer) agents.",
                "International Journal of Software Engineering and Knowledge Engineering, 9(3):319-341, 1999. [7] Lorraine McGinty and Barry Smyth.",
                "Collaborative case-based reasoning: Applications in personalized route planning.",
                "In I. Watson and Q. Yang, editors, ICCBR, number 2080 in LNAI, pages 362-376.",
                "Springer-Verlag, 2001. [8] Santi Ontañón and Enric Plaza.",
                "Justification-based multiagent learning.",
                "In ICML2003, pages 576-583.",
                "Morgan Kaufmann, 2003. [9] Enric Plaza, Eva Armengol, and Santiago Ontañón.",
                "The explanatory power of symbolic similarity in case-based reasoning.",
                "Artificial Intelligence Review, 24(2):145-161, 2005. [10] David Poole.",
                "On the comparison of theories: Preferring the most specific explanation.",
                "In IJCAI-85, pages 144-147, 1985. [11] M V Nagendra Prassad, Victor R Lesser, and Susan Lander.",
                "Retrieval and reasoning in distributed case bases.",
                "Technical report, UMass Computer Science Department, 1995. [12] K. Sycara S. Kraus and A. Evenchik.",
                "Reaching agreements through argumentation: a logical model and implementation.",
                "Artificial Intelligence Journal, 104:1-69, 1998. [13] N. R. Jennings S. Parsons, C. Sierra.",
                "Agents that reason and negotiate by arguing.",
                "Journal of Logic and Computation, 8:261-292, 1998. [14] Bruce A. Wooley.",
                "Explanation component of software systems.",
                "ACM CrossRoads, 5.1, 1998. 982 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "Learning and Joint Deliberation through Argumentation in Multi-Agent Systems Santi Ontañón CCL, Cognitive Computing Lab Georgia Institute of Technology Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Artificial Intelligence Research Institute CSIC, Spanish Council for Scientific Research Campus UAB, 08193 Bellaterra, Catalonia (Spain) enric@iiia.csic.es ABSTRACT In this paper we will present an <br>argumentation framework</br> for learning agents (AMAL) designed for two purposes: (1) for joint deliberation, and (2) for learning from communication.",
                "In this paper we will present an <br>argumentation framework</br> for learning agents, and show that it can be used for two purposes: (1) joint deliberation, and (2) learning from communication.",
                "After that, Section 4 formally defines our <br>argumentation framework</br>.",
                "After that, Section 8 presents an exemplification of the <br>argumentation framework</br>.",
                "In this paper we will propose AMAL, an <br>argumentation framework</br> for learning agents, and will also also show how AMAL can be used both for learning from communication and for solving problems in a collaborative way: • Agents can solve problems in a collaborative way via engaging an argumentation process about the prediction for the situation at hand."
            ],
            "translated_annotated_samples": [
                "Aprendizaje y deliberación conjunta a través de la argumentación en sistemas multiagente. Santi Ontañón CCL, Laboratorio de Computación Cognitiva, Instituto de Tecnología de Georgia, Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Instituto de Investigación en Inteligencia Artificial, CSIC, Consejo Superior de Investigaciones Científicas, Campus UAB, 08193 Bellaterra, Cataluña (España) enric@iiia.csic.es RESUMEN En este artículo presentaremos un <br>marco de argumentación</br> para agentes de aprendizaje (AMAL) diseñado con dos propósitos: (1) para deliberación conjunta, y (2) para aprender a través de la comunicación.",
                "En este artículo presentaremos un <br>marco de argumentación</br> para agentes de aprendizaje, y mostraremos que puede ser utilizado para dos propósitos: (1) deliberación conjunta, y (2) aprendizaje a partir de la comunicación.",
                "Después de eso, la Sección 4 define formalmente nuestro <br>marco de argumentación</br>.",
                "Después de eso, la Sección 8 presenta una ejemplificación del <br>marco de argumentación</br>.",
                "En este artículo propondremos AMAL, un <br>marco de argumentación</br> para agentes de aprendizaje, y también mostraremos cómo AMAL puede ser utilizado tanto para aprender a través de la comunicación como para resolver problemas de manera colaborativa: • Los agentes pueden resolver problemas de manera colaborativa al participar en un proceso de argumentación sobre la predicción para la situación actual."
            ],
            "translated_text": "Aprendizaje y deliberación conjunta a través de la argumentación en sistemas multiagente. Santi Ontañón CCL, Laboratorio de Computación Cognitiva, Instituto de Tecnología de Georgia, Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Instituto de Investigación en Inteligencia Artificial, CSIC, Consejo Superior de Investigaciones Científicas, Campus UAB, 08193 Bellaterra, Cataluña (España) enric@iiia.csic.es RESUMEN En este artículo presentaremos un <br>marco de argumentación</br> para agentes de aprendizaje (AMAL) diseñado con dos propósitos: (1) para deliberación conjunta, y (2) para aprender a través de la comunicación. El marco AMAL se basa completamente en el aprendizaje a partir de ejemplos: la relación de preferencia de argumentos, la política de generación de argumentos y la política de generación de contraargumentos son técnicas basadas en casos. Para unirse en la deliberación, los agentes de aprendizaje comparten su experiencia formando un comité para decidir sobre alguna decisión conjunta. Experimentalmente demostramos que la argumentación entre comités de agentes mejora tanto el rendimiento individual como el conjunto. Para aprender a través de la comunicación, un agente se involucra en discusiones con otros agentes para contrastar sus hipótesis individuales y recibir contraejemplos; el proceso de argumentación mejora su alcance de aprendizaje y rendimiento individual. Categorías y Descriptores de Asignaturas I.2.6 [Inteligencia Artificial]: Aprendizaje; I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente, Agentes Inteligentes 1. Los marcos de argumentación para sistemas multiagente pueden ser utilizados para diferentes propósitos como la deliberación conjunta, la persuasión, la negociación y la resolución de conflictos. En este artículo presentaremos un <br>marco de argumentación</br> para agentes de aprendizaje, y mostraremos que puede ser utilizado para dos propósitos: (1) deliberación conjunta, y (2) aprendizaje a partir de la comunicación. La deliberación conjunta basada en argumentos implica discutir sobre el resultado de una situación particular o la acción apropiada a tomar en una situación específica. Los agentes de aprendizaje son capaces de aprender de la experiencia, en el sentido de que ejemplos pasados (situaciones y sus resultados) se utilizan para predecir el resultado para la situación actual. Sin embargo, dado que la experiencia de los agentes individuales puede ser limitada, el conocimiento individual y la precisión de la predicción también son limitados. Por lo tanto, los agentes de aprendizaje que son capaces de argumentar sus predicciones individuales con otros agentes pueden alcanzar una mayor precisión en las predicciones después de dicho proceso de argumentación. La mayoría de los marcos de argumentación existentes para sistemas multiagente se basan en lógica deductiva u otra formalización de lógica deductiva diseñada específicamente para respaldar la argumentación, como la lógica por defecto [3]. Por lo general, un argumento se ve como una declaración lógica, mientras que un contraargumento es un argumento ofrecido en oposición a otro argumento; los agentes utilizan una relación de preferencia para resolver argumentos conflictivos. Sin embargo, los marcos de argumentación basados en lógica asumen agentes con conocimiento predefinido y relación de preferencia. En este artículo, nos enfocamos en un marco de Aprendizaje Multiagente basado en Argumentación (AMAL) donde tanto el conocimiento como la relación de preferencia se aprenden a partir de la experiencia. Por lo tanto, consideramos un escenario con agentes que (1) trabajan en el mismo dominio utilizando una ontología compartida, (2) son capaces de aprender a partir de ejemplos y (3) se comunican utilizando un marco argumentativo. Tener capacidades de aprendizaje permite a los agentes utilizar de manera efectiva una forma específica de contraargumento, a saber, el uso de contraejemplos. Los contraejemplos ofrecen la posibilidad de que los agentes aprendan durante el proceso de argumentación. Además, los agentes de aprendizaje permiten técnicas que utilizan la experiencia aprendida para generar argumentos y contraargumentos adecuados. Específicamente, necesitaremos abordar dos problemas: (1) cómo definir una técnica para generar argumentos y contraargumentos a partir de ejemplos, y (2) cómo definir una relación de preferencia sobre dos argumentos en conflicto que han sido inducidos a partir de ejemplos. Este documento presenta un enfoque basado en casos para abordar ambas cuestiones. Los agentes utilizan el razonamiento basado en casos (CBR) [1] para aprender de casos pasados (donde un caso es una situación y su resultado) con el fin de predecir el resultado de una nueva situación. Proponemos un protocolo de argumentación dentro del marco de AMAL que apoya a los agentes en llegar a una predicción conjunta sobre una situación o problema específico; además, el razonamiento necesario para respaldar el proceso de argumentación también se basará en casos. En particular, presentamos dos medidas basadas en casos, una para generar los argumentos y contraargumentos adecuados para una situación particular y otra para determinar la relación de preferencia entre los argumentos. Finalmente, evaluamos (1) si la argumentación entre agentes de aprendizaje puede producir una predicción conjunta que mejore el rendimiento de aprendizaje individual y (2) si el aprendizaje a partir de los contraejemplos transmitidos durante el proceso de argumentación aumenta el rendimiento individual con precisamente aquellos casos que se utilizan mientras discuten entre ellos. El documento está estructurado de la siguiente manera. La sección 2 discute la relación entre la argumentación, la colaboración y el aprendizaje. Luego, la Sección 3 introduce nuestro marco de trabajo de CBR multiagente (MAC) y el concepto de predicción justificada. Después de eso, la Sección 4 define formalmente nuestro <br>marco de argumentación</br>. Las secciones 5 y 6 presentan nuestra relación de preferencia basada en casos y las políticas de generación de argumentos, respectivamente. Más adelante, la Sección 7 presenta el protocolo de argumentación en nuestro marco AMAL. Después de eso, la Sección 8 presenta una ejemplificación del <br>marco de argumentación</br>. Finalmente, la Sección 9 presenta una evaluación empírica de nuestras dos hipótesis principales. El artículo concluye con secciones de trabajo relacionado y conclusiones. 2. ARGUMENTACIÓN, COLABORACIÓN Y APRENDIZAJE Tanto el aprendizaje como la colaboración son formas en las que un agente puede mejorar el rendimiento individual. De hecho, existe un claro paralelismo entre el aprendizaje y la colaboración en sistemas multiagentes, ya que ambos son formas en las que los agentes pueden lidiar con sus deficiencias. Mostremos cuáles son las principales motivaciones que un agente puede tener para aprender o colaborar. • Motivaciones para aprender: - Aumentar la calidad de la predicción, - Aumentar la eficiencia, - Ampliar el rango de problemas resolubles. • Motivaciones para colaborar: - Aumentar la calidad de la predicción, - Aumentar la eficiencia, - Ampliar el rango de problemas resolubles, - Ampliar el rango de recursos accesibles. Al observar las listas de motivación anteriores, podemos ver fácilmente que el aprendizaje y la colaboración están muy relacionados en los sistemas multiagente. De hecho, con la excepción del último elemento en la lista de motivaciones para colaborar, son dos extremos de un continuo de estrategias para mejorar el rendimiento. Un agente puede optar por aumentar el rendimiento mediante el aprendizaje, la colaboración o encontrando un punto intermedio que combine el aprendizaje y la colaboración para mejorar el rendimiento. En este artículo propondremos AMAL, un <br>marco de argumentación</br> para agentes de aprendizaje, y también mostraremos cómo AMAL puede ser utilizado tanto para aprender a través de la comunicación como para resolver problemas de manera colaborativa: • Los agentes pueden resolver problemas de manera colaborativa al participar en un proceso de argumentación sobre la predicción para la situación actual. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "learning agent": {
            "translated_key": "agente de aprendizaje",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning and Joint Deliberation through Argumentation in Multi-Agent Systems Santi Ontañón CCL, Cognitive Computing Lab Georgia Institute of Technology Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Artificial Intelligence Research Institute CSIC, Spanish Council for Scientific Research Campus UAB, 08193 Bellaterra, Catalonia (Spain) enric@iiia.csic.es ABSTRACT In this paper we will present an argumentation framework for learning agents (AMAL) designed for two purposes: (1) for joint deliberation, and (2) for learning from communication.",
                "The AMAL framework is completely based on learning from examples: the argument preference relation, the argument generation policy, and the counterargument generation policy are case-based techniques.",
                "For join deliberation, learning agents share their experience by forming a committee to decide upon some joint decision.",
                "We experimentally show that the argumentation among committees of agents improves both the individual and joint performance.",
                "For learning from communication, an agent engages into arguing with other agents in order to contrast its individual hypotheses and receive counterexamples; the argumentation process improves their learning scope and individual performance.",
                "Categories and Subject Descriptors I.2.6 [Artificial Intelligence]: Learning; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems, Intelligent Agents 1.",
                "INTRODUCTION Argumentation frameworks for multi-agent systems can be used for different purposes like joint deliberation, persuasion, negotiation, and conflict resolution.",
                "In this paper we will present an argumentation framework for learning agents, and show that it can be used for two purposes: (1) joint deliberation, and (2) learning from communication.",
                "Argumentation-based joint deliberation involves discussion over the outcome of a particular situation or the appropriate course of action for a particular situation.",
                "Learning agents are capable of learning from experience, in the sense that past examples (situations and their outcomes) are used to predict the outcome for the situation at hand.",
                "However, since individual agents experience may be limited, individual knowledge and prediction accuracy is also limited.",
                "Thus, learning agents that are capable of arguing their individual predictions with other agents may reach better prediction accuracy after such an argumentation process.",
                "Most existing argumentation frameworks for multi-agent systems are based on deductive logic or some other deductive logic formalism specifically designed to support argumentation, such as default logic [3]).",
                "Usually, an argument is seen as a logical statement, while a counterargument is an argument offered in opposition to another argument [4, 13]; agents use a preference relation to resolve conflicting arguments.",
                "However, logic-based argumentation frameworks assume agents with preloaded knowledge and preference relation.",
                "In this paper, we focus on an Argumentation-based Multi-Agent Learning (AMAL) framework where both knowledge and preference relation are learned from experience.",
                "Thus, we consider a scenario with agents that (1) work in the same domain using a shared ontology, (2) are capable of learning from examples, and (3) communicate using an argumentative framework.",
                "Having learning capabilities allows agents effectively use a specific form of counterargument, namely the use of counterexamples.",
                "Counterexamples offer the possibility of agents learning during the argumentation process.",
                "Moreover, learning agents allow techniques that use learnt experience to generate adequate arguments and counterarguments.",
                "Specifically, we will need to address two issues: (1) how to define a technique to generate arguments and counterarguments from examples, and (2)how to define a preference relation over two conflicting arguments that have been induced from examples.",
                "This paper presents a case-based approach to address both issues.",
                "The agents use case-based reasoning (CBR) [1] to learn from past cases (where a case is a situation and its outcome) in order to predict the outcome of a new situation.",
                "We propose an argumentation protocol inside the AMAL framework at supports agents in reaching a joint prediction over a specific situation or problem - moreover, the reasoning needed to support the argumentation process will also be based on cases.",
                "In particular, we present two case-based measures, one for generating the arguments and counterarguments adequate to a particular situation and another for determining preference relation among arguments.",
                "Finally, we evaluate (1) if argumentation between learning agents can produce a joint prediction that improves over individual learning performance and (2) if learning from the counterexamples conveyed during the argumentation process increases the individual performance with precisely those cases being used while arguing among them.",
                "The paper is structured as follows.",
                "Section 2 discusses the relation among argumentation, collaboration and learning.",
                "Then Section 3 introduces our multi-agent CBR (MAC) framework and the notion of justified prediction.",
                "After that, Section 4 formally defines our argumentation framework.",
                "Sections 5 and 6 present our case-based preference relation and argument generation policies respectively.",
                "Later, Section 7 presents the argumentation protocol in our AMAL framework.",
                "After that, Section 8 presents an exemplification of the argumentation framework.",
                "Finally, Section 9 presents an empirical evaluation of our two main hypotheses.",
                "The paper closes with related work and conclusions sections. 2.",
                "ARGUMENTATION,COLLABORATION AND LEARNING Both learning and collaboration are ways in which an agent can improve individual performance.",
                "In fact, there is a clear parallelism between learning and collaboration in multi-agent systems, since both are ways in which agents can deal with their shortcomings.",
                "Let us show which are the main motivations that an agent can have to learn or to collaborate. • Motivations to learn: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems. • Motivations to collaborate: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems, - Increase the range of accessible resources.",
                "Looking at the above lists of motivation, we can easily see that learning and collaboration are very related in multi-agent systems.",
                "In fact, with the exception of the last item in the motivations to collaborate list, they are two extremes of a continuum of strategies to improve performance.",
                "An agent may choose to increase performance by learning, by collaborating, or by finding an intermediate point that combines learning and collaboration in order to improve performance.",
                "In this paper we will propose AMAL, an argumentation framework for learning agents, and will also also show how AMAL can be used both for learning from communication and for solving problems in a collaborative way: • Agents can solve problems in a collaborative way via engaging an argumentation process about the prediction for the situation at hand.",
                "Using this collaboration, the prediction can be done in a more informed way, since the information known by several agents has been taken into account. • Agents can also learn from communication with other agents by engaging an argumentation process.",
                "Agents that engage in such argumentation processes can learn from the arguments and counterexamples received from other agents, and use this information for predicting the outcomes of future situations.",
                "In the rest of this paper we will propose an argumentation framework and show how it can be used both for learning and for solving problems in a collaborative way. 3.",
                "MULTI-AGENT CBR SYSTEMS A Multi-Agent Case Based Reasoning System (MAC) M = {(A1, C1), ..., (An, Cn)} is a multi-agent system composed of A = {Ai, ..., An}, a set of CBR agents, where each agent Ai ∈ A possesses an individual case base Ci.",
                "Each individual agent Ai in a MAC is completely autonomous and each agent Ai has access only to its individual and private case base Ci.",
                "A case base Ci = {c1, ..., cm} is a collection of cases.",
                "Agents in a MAC system are able to individually solve problems, but they can also collaborate with other agents to solve problems.",
                "In this framework, we will restrict ourselves to analytical tasks, i.e. tasks like classification, where the solution of a problem is achieved by selecting a solution class from an enumerated set of solution classes.",
                "In the following we will note the set of all the solution classes by S = {S1, ..., SK }.",
                "Therefore, a case c = P, S is a tuple containing a case description P and a solution class S ∈ S. In the following, we will use the terms problem and case description indistinctly.",
                "Moreover, we will use the dot notation to refer to elements inside a tuple; e.g., to refer to the solution class of a case c, we will write c.S.",
                "Therefore, we say a group of agents perform joint deliberation, when they collaborate to find a joint solution by means of an argumentation process.",
                "However, in order to do so, an agent has to be able to justify its prediction to the other agents (i.e. generate an argument for its predicted solution that can be examined and critiqued by the other agents).",
                "The next section addresses this issue. 3.1 Justified Predictions Both expert systems and CBR systems may have an explanation component [14] in charge of justifying why the system has provided a specific answer to the user.",
                "The line of reasoning of the system can then be examined by a human expert, thus increasing the reliability of the system.",
                "Most of the existing work on explanation generation focuses on generating explanations to be provided to the user.",
                "However, in our approach we use explanations (or justifications) as a tool for improving communication and coordination among agents.",
                "We are interested in justifications since they can be used as arguments.",
                "For that purpose, we will benefit from the ability of some machine learning methods to provide justifications.",
                "A justification built by a CBR method after determining that the solution of a particular problem P was Sk is a description that contains the relevant information from the problem P that the CBR method has considered to predict Sk as the solution of P. In particular, CBR methods work by retrieving similar cases to the problem at hand, and then reusing their solutions for the current problem, expecting that since the problem and the cases are similar, the solutions will also be similar.",
                "Thus, if a CBR method has retrieved a set of cases C1, ..., Cn to solve a particular problem P the justification built will contain the relevant information from the problem P that made the CBR system retrieve that particular set of cases, i.e. it will contain the relevant information that P and C1, ..., Cn have in common.",
                "For example, Figure 1 shows a justification build by a CBR system for a toy problem (in the following sections we will show justifications for real problems).",
                "In the figure, a problem has two attributes (Traffic_light, and Cars_passing), the retrieval mechanism of the CBR system notices that by considering only the attribute Traffic_light, it can retrieve two cases that predict the same solution: wait.",
                "Thus, since only this attribute has been used, it is the only one appearing in the justification.",
                "The values of the rest of attributes are irrelevant, since whatever their value the solution class would have been the same. 976 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Problem Traffic_light: red Cars_passing: no Case 1 Traffic_light: red Cars_passing: no Solution: wait Case 3 Traffic_light: red Cars_passing: yes Solution: wait Case 4 Traffic_light: green Cars_passing: yes Solution: wait Case 2 Traffic_light: green Cars_passing: no Solution: cross Retrieved cases Solution: wait Justification Traffic_light: red Figure 1: An example of justification generation in a CBR system.",
                "Notice that, since the only relevant feature to decide is Traffic_light (the only one used to retrieve cases), it is the only one appearing in the justification.",
                "In general, the meaning of a justification is that all (or most of) the cases in the case base of an agent that satisfy the justification (i.e. all the cases that are subsumed by the justification) belong to the predicted solution class.",
                "In the rest of the paper, we will use to denote the subsumption relation.",
                "In our work, we use LID [2], a CBR method capable of building symbolic justifications such as the one exemplified in Figure 1.",
                "When an agent provides a justification for a prediction, the agent generates a justified prediction: DEFINITION 3.1.",
                "A Justified Prediction is a tuple J = A, P, S, D where agent A considers S the correct solution for problem P, and that prediction is justified a symbolic description D such that J.D J.P. Justifications can have many uses for CBR systems [8, 9].",
                "In this paper, we are going to use justifications as arguments, in order to allow learning agents to engage in argumentation processes. 4.",
                "ARGUMENTS AND COUNTERARGUMENTS For our purposes an argument α generated by an agent A is composed of a statement S and some evidence D supporting S as correct.",
                "In the remainder of this section we will see how this general definition of argument can be instantiated in specific kind of arguments that the agents can generate.",
                "In the context of MAC systems, agents argue about predictions for new problems and can provide two kinds of information: a) specific cases P, S , and b) justified predictions: A, P, S, D .",
                "Using this information, we can define three types of arguments: justified predictions, counterarguments, and counterexamples.",
                "A justified prediction α is generated by an agent Ai to argue that Ai believes that the correct solution for a given problem P is α.S, and the evidence provided is the justification α.D.",
                "In the example depicted in Figure 1, an agent Ai may generate the argument α = Ai, P, Wait, (Traffic_light = red) , meaning that the agent Ai believes that the correct solution for P is Wait because the attribute Traffic_light equals red.",
                "A counterargument β is an argument offered in opposition to another argument α.",
                "In our framework, a counterargument consists of a justified prediction Aj, P, S , D generated by an agent Aj with the intention to rebut an argument α generated by another agent Ai, that endorses a solution class S different from that of α.S for the problem at hand and justifies this with a justification D .",
                "In the example in Figure 1, if an agent generates the argument α = Ai, P, Walk, (Cars_passing = no) , an agent that thinks that the correct solution is Wait might answer with the counterargument β = Aj, P, Wait, (Cars_passing = no ∧ Traffic_light = red) , meaning that, although there are no cars passing, the traffic light is red, and the street cannot be crossed.",
                "A counterexample c is a case that contradicts an argument α.",
                "Thus a counterexample is also a counterargument, one that states that a specific argument α is not always true, and the evidence provided is the case c. Specifically, for a case c to be a counterexample of an argument α, the following conditions have to be met: α.D c and α.S = c.S, i.e. the case must satisfy the justification α.D and the solution of c must be different than the predicted by α.",
                "By exchanging arguments and counterarguments (including counterexamples), agents can argue about the correct solution of a given problem, i.e. they can engage a joint deliberation process.",
                "However, in order to do so, they need a specific interaction protocol, a preference relation between contradicting arguments, and a decision policy to generate counterarguments (including counterexamples).",
                "In the following sections we will present these elements. 5.",
                "PREFERENCE RELATION A specific argument provided by an agent might not be consistent with the information known to other agents (or even to some of the information known by the agent that has generated the justification due to noise in training data).",
                "For that reason, we are going to define a preference relation over contradicting justified predictions based on cases.",
                "Basically, we will define a confidence measure for each justified prediction (that takes into account the cases owned by each agent), and the justified prediction with the highest confidence will be the preferred one.",
                "The idea behind case-based confidence is to count how many of the cases in an individual case base endorse a justified prediction, and how many of them are counterexamples of it.",
                "The more the endorsing cases, the higher the confidence; and the more the counterexamples, the lower the confidence.",
                "Specifically, to assess the confidence of a justified prediction α, an agent obtains the set of cases in its individual case base that are subsumed by α.D.",
                "With them, an agent Ai obtains the Y (aye) and N (nay) values: • Y Ai α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by the justification α.D that belong to the solution class α.S, • NAi α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by justification α.D that do not belong to that solution class.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 977 + + + + + +  - - + Figure 2: Confidence of arguments is evaluated by contrasting them against the case bases of the agents.",
                "An agent estimates the confidence of an argument as: CAi (α) = Y Ai α 1 + Y Ai α + NAi α i.e. the confidence on a justified prediction is the number of endorsing cases divided by the number of endorsing cases plus counterexamples.",
                "Notice that we add 1 to the denominator, this is to avoid giving excessively high confidences to justified predictions whose confidence has been computed using a small number of cases.",
                "Notice that this correction follows the same idea than the Laplace correction to estimate probabilities.",
                "Figure 2 illustrates the individual evaluation of the confidence of an argument, in particular, three endorsing cases and one counterexample are found in the case base of agents Ai, giving an estimated confidence of 0.6 Moreover, we can also define the joint confidence of an argument α as the confidence computed using the cases present in the case bases of all the agents in the group: C(α) = i Y Ai α 1 + i Y Ai α + NAi α Notice that, to collaboratively compute the joint confidence, the agents only have to make public the aye and nay values locally computed for a given argument.",
                "In our framework, agents use this joint confidence as the preference relation: a justified prediction α is preferred over another one β if C(α) ≥ C(β). 6.",
                "GENERATION OF ARGUMENTS In our framework, arguments are generated by the agents from cases, using learning methods.",
                "Any learning method able to provide a justified prediction can be used to generate arguments.",
                "For instance, decision trees and LID [2] are suitable learning methods.",
                "Specifically, in the experiments reported in this paper agents use LID.",
                "Thus, when an agent wants to generate an argument endorsing that a specific solution class is the correct solution for a problem P, it generates a justified prediction as explained in Section 3.1.",
                "For instance, Figure 3 shows a real justification generated by LID after solving a problem P in the domain of marine sponges identification.",
                "In particular, Figure 3 shows how when an agent receives a new problem to solve (in this case, a new sponge to determine its order), the agent uses LID to generate an argument (consisting on a justified prediction) using the cases in the case base of the agent.",
                "The justification shown in Figure 3 can be interpreted saying that the predicted solution is hadromerida because the smooth form of the megascleres of the spiculate skeleton of the sponge is of type tylostyle, the spikulate skeleton of the sponge has no uniform length, and there is no gemmules in the external features of the sponge.",
                "Thus, the argument generated will be α = A1, P, hadromerida, D1 . 6.1 Generation of Counterarguments As previously stated, agents may try to rebut arguments by generating counterargument or by finding counterexamples.",
                "Let us explain how they can be generated.",
                "An agent Ai wants to generate a counterargument β to rebut an argument α when α is in contradiction with the local case base of Ai.",
                "Moreover, while generating such counterargument β, Ai expects that β is preferred over α.",
                "For that purpose, we will present a specific policy to generate counterarguments based on the specificity criterion [10].",
                "The specificity criterion is widely used in deductive frameworks for argumentation, and states that between two conflicting arguments, the most specific should be preferred since it is, in principle, more informed.",
                "Thus, counterarguments generated based on the specificity criterion are expected to be preferable (since they are more informed) to the arguments they try to rebut.",
                "However, there is no guarantee that such counterarguments will always win, since, as we have stated in Section 5, agents in our framework use a preference relation based on joint confidence.",
                "Moreover, one may think that it would be better that the agents generate counterarguments based on the joint confidence preference relation; however it is not obvious how to generate counterarguments based on joint confidence in an efficient way, since collaboration is required in order to evaluate joint confidence.",
                "Thus, the agent generating the counterargument should constantly communicate with the other agents at each step of the induction algorithm used to generate counterarguments (presently one of our future research lines).",
                "Thus, in our framework, when an agent wants to generate a counterargument β to an argument α, β has to be more specific than α (i.e. α.D < β.D).",
                "The generation of counterarguments using the specificity criterion imposes some restrictions over the learning method, although LID or ID3 can be easily adapted for this task.",
                "For instance, LID is an algorithm that generates a description starting from scratch and heuristically adding features to that term.",
                "Thus, at every step, the description is made more specific than in the previous step, and the number of cases that are subsumed by that description is reduced.",
                "When the description covers only (or almost only) cases of a single solution class LID terminates and predicts that solution class.",
                "To generate a counterargument to an argument α LID just has to use as starting point the description α.D instead of starting from scratch.",
                "In this way, the justification provided by LID will always be subsumed by α.D, and thus the resulting counterargument will be more specific than α.",
                "However, notice that LID may sometimes not be able to generate counterarguments, since LID may not be able to specialize the description α.D any further, or because the agent Ai has no case inCi that is subsumed by α.D.",
                "Figure 4 shows how an agent A2 that disagreed with the argument shown in Figure 3, generates a counterargument using LID.",
                "Moreover, Figure 4 shows the generation of a counterargument β1 2 for the argument α0 1 (in Figure 3) that is a specialization of α0 1. 978 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Solution: hadromerida Justification: D1 Sponge Spikulate skeleton External features External features Gemmules: no Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Case Base of A1 LID New sponge P Figure 3: Example of a real justification generated by LID in the marine sponges data set.",
                "Specifically, in our experiments, when an agent Ai wants to rebut an argument α, uses the following policy: 1.",
                "Agent Ai uses LID to try to find a counterargument β more specific than α; if found, β is sent to the other agent as a counterargument of α. 2.",
                "If not found, then Ai searches for a counterexample c ∈ Ci of α.",
                "If a case c is found, then c is sent to the other agent as a counterexample of α. 3.",
                "If no counterexamples are found, then Ai cannot rebut the argument α. 7.",
                "ARGUMENTATION-BASED MULTI-AGENT LEARNING The interaction protocol of AMAL allows a group of agents A1, ..., An to deliberate about the correct solution of a problem P by means of an argumentation process.",
                "If the argumentation process arrives to a consensual solution, the joint deliberation ends; otherwise a weighted vote is used to determine the joint solution.",
                "Moreover, AMAL also allows the agents to learn from the counterexamples received from other agents.",
                "The AMAL protocol consists on a series of rounds.",
                "In the initial round, each agent states which is its individual prediction for P. Then, at each round an agent can try to rebut the prediction made by any of the other agents.",
                "The protocol uses a token passing mechanism so that agents (one at a time) can send counterarguments or counterexamples if they disagree with the prediction made by any other agent.",
                "Specifically, each agent is allowed to send one counterargument or counterexample each time he gets the token (notice that this restriction is just to simplify the protocol, and that it does not restrict the number of counterargument an agent can sent, since they can be delayed for subsequent rounds).",
                "When an agent receives a counterargument or counterexample, it informs the other agents if it accepts the counterargument (and changes its prediction) or not.",
                "Moreover, agents have also the opportunity to answer to counterarguments when they receive the token, by trying to generate a counterargument to the counterargument.",
                "When all the agents have had the token once, the token returns to the first agent, and so on.",
                "If at any time in the protocol, all the agents agree or during the last n rounds no agent has generated any counterargument, the protocol ends.",
                "Moreover, if at the end of the argumentation the agents have not reached an agreement, then a voting mechanism that uses the confidence of each prediction as weights is used to decide the final solution (Thus, AMAL follows the same mechanism as human committees, first each individual member of a committee exposes his arguments and discuses those of the other members (joint deliberation), and if no consensus is reached, then a voting mechanism is required).",
                "At each iteration, agents can use the following performatives: • assert(α): the justified prediction held during the next round will be α.",
                "An agent can only hold a single prediction at each round, thus is multiple asserts are send, only the last one is considered as the currently held prediction. • rebut(β, α): the agent has found a counterargument β to the prediction α.",
                "We will define Ht = αt 1, ..., αt n as the predictions that each of the n agents hold at a round t. Moreover, we will also define contradict(αt i) = {α ∈ Ht|α.S = αt i.S} as the set of contradicting arguments for an agent Ai in a round t, i.e. the set of arguments at round t that support a different solution class than αt i.",
                "The protocol is initiated because one of the agents receives a problem P to be solved.",
                "After that, the agent informs all the other agents about the problem P to solve, and the protocol starts: 1.",
                "At round t = 0, each one of the agents individually solves P, and builds a justified prediction using its own CBR method.",
                "Then, each agent Ai sends the performative assert(α0 i ) to the other agents.",
                "Thus, the agents know H0 = α0 i , ..., α0 n .",
                "Once all the predictions have been sent the token is given to the first agent A1. 2.",
                "At each round t (other than 0), the agents check whether their arguments in Ht agree.",
                "If they do, the protocol moves to step 5.",
                "Moreover, if during the last n rounds no agent has sent any counterexample or counterargument, the protocol also moves to step 5.",
                "Otherwise, the agent Ai owner of the token tries to generate a counterargument for each of the opposing arguments in contradict(αt i) ⊆ Ht (see Section 6.1).",
                "Then, the counterargument βt i against the prediction αt j with the lowest confidence C(αt j) is selected (since αt j is the prediction more likely to be successfully rebutted). • If βt i is a counterargument, then, Ai locally compares αt i with βt i by assessing their confidence against its individual case base Ci (see Section 5) (notice that Ai is comparing its previous argument with the counterargument that Ai itself has just generated and that is about The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 979 Sponge Spikulate skeleton External features External features Gemmules: no Growing: Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Growing Grow: massive Case Base of A2 LID Solution: astrophorida Justification: D2 Figure 4: Generation of a counterargument using LID in the sponges data set. to send to Aj).",
                "If CAi (βt i ) > CAi (αt i), then Ai considers that βt i is stronger than its previous argument, changes its argument to βt i by sending assert(βt i ) to the rest of the agents (the intuition behind this is that since a counterargument is also an argument, Ai checks if the newly counterargument is a better argument than the one he was previously holding) and rebut(βt i , αt j) to Aj.",
                "Otherwise (i.e.",
                "CAi (βt i ) ≤ CAi (αt i)), Ai will send only rebut(βt i , αt j) to Aj.",
                "In any of the two situations the protocol moves to step 3. • If βt i is a counterexample c, then Ai sends rebut(c, αt j) to Aj.",
                "The protocol moves to step 4. • If Ai cannot generate any counterargument or counterexample, the token is sent to the next agent, a new round t + 1 starts, and the protocol moves to state 2. 3.",
                "The agent Aj that has received the counterargument βt i , locally compares it against its own argument, αt j, by locally assessing their confidence.",
                "If CAj (βt i ) > CAj (αt j), then Aj will accept the counterargument as stronger than its own argument, and it will send assert(βt i ) to the other agents.",
                "Otherwise (i.e.",
                "CAj (βt i ) ≤ CAj (αt j)), Aj will not accept the counterargument, and will inform the other agents accordingly.",
                "Any of the two situations start a new round t + 1, Ai sends the token to the next agent, and the protocol moves back to state 2. 4.",
                "The agent Aj that has received the counterexample c retains it into its case base and generates a new argument αt+1 j that takes into account c, and informs the rest of the agents by sending assert(αt+1 j ) to all of them.",
                "Then, Ai sends the token to the next agent, a new round t + 1 starts, and the protocol moves back to step 2. 5.",
                "The protocol ends yielding a joint prediction, as follows: if the arguments in Ht agree then their prediction is the joint prediction, otherwise a voting mechanism is used to decide the joint prediction.",
                "The voting mechanism uses the joint confidence measure as the voting weights, as follows: S = arg max Sk∈S αi∈Ht|αi.S=Sk C(αi) Moreover, in order to avoid infinite iterations, if an agent sends twice the same argument or counterargument to the same agent, the message is not considered. 8.",
                "EXEMPLIFICATION Let us consider a system composed of three agents A1, A2 and A3.",
                "One of the agents, A1 receives a problem P to solve, and decides to use AMAL to solve it.",
                "For that reason, invites A2 and A3 to take part in the argumentation process.",
                "They accept the invitation, and the argumentation protocol starts.",
                "Initially, each agent generates its individual prediction for P, and broadcasts it to the other agents.",
                "Thus, all of them can compute H0 = α0 1, α0 2, α0 3 .",
                "In particular, in this example: • α0 1 = A1, P, hadromerida, D1 • α0 2 = A2, P, astrophorida, D2 • α0 3 = A3, P, axinellida, D3 A1 starts owning the token and tries to generate counterarguments for α0 2 and α0 3, but does not succeed, however it has one counterexample c13 for α0 3.",
                "Thus, A1 sends the the message rebut( c13, α0 3) to A3.",
                "A3 incorporates c13 into its case base and tries to solve the problem P again, now taking c13 into consideration.",
                "A3 comes up with the justified prediction α1 3 = A3, P, hadromerida, D4 , and broadcasts it to the rest of the agents with the message assert(α1 3).",
                "Thus, all of them know the new H1 = α0 1, α0 2, α1 3 .",
                "Round 1 starts and A2 gets the token.",
                "A2 tries to generate counterarguments for α0 1 and α1 3 and only succeeds to generate a counterargument β1 2 = A2, P, astrophorida, D5 against α1 3.",
                "The counterargument is sent to A3 with the message rebut(β1 2 , α1 3).",
                "Agent A3 receives the counterargument and assesses its local confidence.",
                "The result is that the individual confidence of the counterargument β1 2 is lower than the local confidence of α1 3.",
                "Therefore, A3 does not accept the counterargument, and thus H2 = α0 1, α0 2, α1 3 .",
                "Round 2 starts and A3 gets the token.",
                "A3 generates a counterargument β2 3 = A3, P, hadromerida, D6 for α0 2 and sends it to A2 with the message rebut(β2 3 , α0 2).",
                "Agent A2 receives the counterargument and assesses its local confidence.",
                "The result is that the local confidence of the counterargument β2 3 is higher than the local confidence of α0 2.",
                "Therefore, A2 accepts the counterargument and informs the rest of the agents with the message assert(β2 3 ).",
                "After that, H3 = α0 1, β2 3 , α1 3 .",
                "At Round 3, since all the agents agree (all the justified predictions in H3 predict hadromerida as the solution class) The protocol ends, and A1 (the agent that received the problem) considers hadromerida as the joint solution for the problem P. 9.",
                "EXPERIMENTAL EVALUATION 980 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) SPONGE 75 77 79 81 83 85 87 89 91 2 3 4 5 AMAL Voting Individual SOYBEAN 55 60 65 70 75 80 85 90 2 3 4 5 AMAL Voting Individual Figure 5: Individual and joint accuracy for 2 to 5 agents.",
                "In this section we empirically evaluate the AMAL argumentation framework.",
                "We have made experiments in two different data sets: soybean (from the UCI machine learning repository) and sponge (a relational data set).",
                "The soybean data set has 307 examples and 19 solution classes, while the sponge data set has 280 examples and 3 solution classes.",
                "In an experimental run, the data set is divided in 2 sets: the training set and the test set.",
                "The training set examples are distributed among 5 different agents without replication, i.e. there is no example shared by two agents.",
                "In the testing stage, problems in the test set arrive randomly to one of the agents, and their goal is to predict the correct solution.",
                "The experiments are designed to test two hypotheses: (H1) that argumentation is a useful framework for joint deliberation and can improve over other typical methods such as voting; and (H2) that learning from communication improves the individual performance of a <br>learning agent</br> participating in an argumentation process.",
                "Moreover, we also expect that the improvement achieved from argumentation will increase as the number of agents participating in the argumentation increases (since more information will be taken into account).",
                "Concerning H1 (argumentation is a useful framework for joint deliberation), we ran 4 experiments, using 2, 3, 4, and 5 agents respectively (in all experiments each agent has a 20% of the training data, since the training is always distributed among 5 agents).",
                "Figure 5 shows the result of those experiments in the sponge and soybean data sets.",
                "Classification accuracy is plotted in the vertical axis, and in the horizontal axis the number of agents that took part in the argumentation processes is shown.",
                "For each number of agents, three bars are shown: individual, Voting, and AMAL.",
                "The individual bar shows the average accuracy of individual agents predictions; the voting bar shows the average accuracy of the joint prediction achieved by voting but without any argumentation; and finally the AMAL bar shows the average accuracy of the joint prediction using argumentation.",
                "The results shown are the average of 5 10-fold cross validation runs.",
                "Figure 5 shows that collaboration (voting and AMAL) outperforms individual problem solving.",
                "Moreover, as we expected, the accuracy improves as more agents collaborate, since more information is taken into account.",
                "We can also see that AMAL always outperforms standard voting, proving that joint decisions are based on better information as provided by the argumentation process.",
                "For instance, the joint accuracy for 2 agents in the sponge data set is of 87.57% for AMAL and 86.57% for voting (while individual accuracy is just 80.07%).",
                "Moreover, the improvement achieved by AMAL over Voting is even larger in the soybean data set.",
                "The reason is that the soybean data set is more difficult (in the sense that agents need more data to produce good predictions).",
                "These experimental results show that AMAL effectively exploits the opportunity for improvement: the accuracy is higher only because more agents have changed their opinion during argumentation (otherwise they would achieve the same result as Voting).",
                "Concerning H2 (learning from communication in argumentation processes improves individual prediction ), we ran the following experiment: initially, we distributed a 25% of the training set among the five agents; after that, the rest of the cases in the training set is sent to the agents one by one; when an agent receives a new training case, it has several options: the agent can discard it, the agent can retain it, or the agent can use it for engaging an argumentation process.",
                "Figure 6 shows the result of that experiment for the two data sets.",
                "Figure 6 contains three plots, where NL (not learning) shows accuracy of an agent with no learning at all; L (learning), shows the evolution of the individual classification accuracy when agents learn by retaining the training cases they individually receive (notice that when all the training cases have been retained at 100%, the accuracy should be equal to that of Figure 5 for individual agents); and finally LFC (learning from communication) shows the evolution of the individual classification accuracy of learning agents that also learn by retaining those counterexamples received during argumentation (i.e. they learn both from training examples and counterexamples).",
                "Figure 6 shows that if an agent Ai learns also from communication, Ai can significantly improve its individual performance with just a small number of additional cases (those selected as relevant counterexamples for Ai during argumentation).",
                "For instance, in the soybean data set, individual agents have achieved an accuracy of 70.62% when they also learn from communication versus an accuracy of 59.93% when they only learn from their individual experience.",
                "The number of cases learnt from communication depends on the properties of the data set: in the sponges data set, agents have retained only very few additional cases, and significantly improved individual accuracy; namely they retain 59.96 cases in average (compared to the 50.4 cases retained if they do not learn from communication).",
                "In the soybean data set more counterexamples are learnt to significantly improve individual accuracy, namely they retain 87.16 cases in average (compared to 55.27 cases retained if they do not learn from communication).",
                "Finally, the fact that both data sets show a significant improvement points out the adaptive nature of the argumentation-based approach to learning from communication: the useful cases are selected as counterexamples (and no more than those needed), and they have the intended effect. 10.",
                "RELATED WORK Concerning CBR in a multi-agent setting, the first research was on negotiated case retrieval [11] among groups of agents.",
                "Our work on multi-agent case-based learning started in 1999 [6]; later Mc Ginty and Smyth [7] presented a multi-agent collaborative CBR approach (CCBR) for planning.",
                "Finally, another interesting approach is multi-case-base reasoning (MCBR) [5], that deals with The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 981 SPONGE 60 65 70 75 80 85 25% 40% 55% 70% 85% 100% LFC L NL SOYBEAN 20 30 40 50 60 70 80 90 25% 40% 55% 70% 85% 100% LFC L NL Figure 6: Learning from communication resulting from argumentation in a system composed of 5 agents. distributed systems where there are several case bases available for the same task and addresses the problems of cross-case base adaptation.",
                "The main difference is that our MAC approach is a way to distribute the Reuse process of CBR (using a voting system) while Retrieve is performed individually by each agent; the other multiagent CBR approaches, however, focus on distributing the Retrieve process.",
                "Research on MAS argumentation focus on several issues like a) logics, protocols and languages that support argumentation, b) argument selection and c) argument interpretation.",
                "Approaches for logic and languages that support argumentation include defeasible logic [4] and BDI models [13].",
                "Although argument selection is a key aspect of automated argumentation (see [12] and [13]), most research has been focused on preference relations among arguments.",
                "In our framework we have addressed both argument selection and preference relations using a case-based approach. 11.",
                "CONCLUSIONS AND FUTURE WORK In this paper we have presented an argumentation-based framework for multi-agent learning.",
                "Specifically, we have presented AMAL, a framework that allows a group of learning agents to argue about the solution of a given problem and we have shown how the learning capabilities can be used to generate arguments and counterarguments.",
                "The experimental evaluation shows that the increased amount of information provided to the agents by the argumentation process increases their predictive accuracy, and specially when an adequate number of agents take part in the argumentation.",
                "The main contributions of this work are: a) an argumentation framework for learning agents; b) a case-based preference relation over arguments, based on computing an overall confidence estimation of arguments; c) a case-based policy to generate counterarguments and select counterexamples; and d) an argumentation-based approach for learning from communication.",
                "Finally, in the experiments presented here a <br>learning agent</br> would retain all counterexamples submitted by the other agent; however, this is a very simple case retention policy, and we will like to experiment with more informed policies - with the goal that individual learning agents could significantly improve using only a small set of cases proposed by other agents.",
                "Finally, our approach is focused on lazy learning, and future works aims at incorporating eager inductive learning inside the argumentative framework for learning from communication. 12.",
                "REFERENCES [1] Agnar Aamodt and Enric Plaza.",
                "Case-based reasoning: Foundational issues, methodological variations, and system approaches.",
                "Artificial Intelligence Communications, 7(1):39-59, 1994. [2] E. Armengol and E. Plaza.",
                "Lazy induction of descriptions for relational case-based learning.",
                "In ECML2001, pages 13-24, 2001. [3] Gerhard Brewka.",
                "Dynamic argument systems: A formal model of argumentation processes based on situation calculus.",
                "Journal of Logic and Computation, 11(2):257-282, 2001. [4] Carlos I. Chesñevar and Guillermo R. Simari.",
                "Formalizing Defeasible Argumentation using Labelled Deductive Systems.",
                "Journal of Computer Science & Technology, 1(4):18-33, 2000. [5] D. Leake and R. Sooriamurthi.",
                "Automatically selecting strategies for multi-case-base reasoning.",
                "In S. Craw and A. Preece, editors, ECCBR2002, pages 204-219, Berlin, 2002.",
                "Springer Verlag. [6] Francisco J. Martín, Enric Plaza, and Josep-Lluis Arcos.",
                "Knowledge and experience reuse through communications among competent (peer) agents.",
                "International Journal of Software Engineering and Knowledge Engineering, 9(3):319-341, 1999. [7] Lorraine McGinty and Barry Smyth.",
                "Collaborative case-based reasoning: Applications in personalized route planning.",
                "In I. Watson and Q. Yang, editors, ICCBR, number 2080 in LNAI, pages 362-376.",
                "Springer-Verlag, 2001. [8] Santi Ontañón and Enric Plaza.",
                "Justification-based multiagent learning.",
                "In ICML2003, pages 576-583.",
                "Morgan Kaufmann, 2003. [9] Enric Plaza, Eva Armengol, and Santiago Ontañón.",
                "The explanatory power of symbolic similarity in case-based reasoning.",
                "Artificial Intelligence Review, 24(2):145-161, 2005. [10] David Poole.",
                "On the comparison of theories: Preferring the most specific explanation.",
                "In IJCAI-85, pages 144-147, 1985. [11] M V Nagendra Prassad, Victor R Lesser, and Susan Lander.",
                "Retrieval and reasoning in distributed case bases.",
                "Technical report, UMass Computer Science Department, 1995. [12] K. Sycara S. Kraus and A. Evenchik.",
                "Reaching agreements through argumentation: a logical model and implementation.",
                "Artificial Intelligence Journal, 104:1-69, 1998. [13] N. R. Jennings S. Parsons, C. Sierra.",
                "Agents that reason and negotiate by arguing.",
                "Journal of Logic and Computation, 8:261-292, 1998. [14] Bruce A. Wooley.",
                "Explanation component of software systems.",
                "ACM CrossRoads, 5.1, 1998. 982 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "The experiments are designed to test two hypotheses: (H1) that argumentation is a useful framework for joint deliberation and can improve over other typical methods such as voting; and (H2) that learning from communication improves the individual performance of a <br>learning agent</br> participating in an argumentation process.",
                "Finally, in the experiments presented here a <br>learning agent</br> would retain all counterexamples submitted by the other agent; however, this is a very simple case retention policy, and we will like to experiment with more informed policies - with the goal that individual learning agents could significantly improve using only a small set of cases proposed by other agents."
            ],
            "translated_annotated_samples": [
                "Los experimentos están diseñados para probar dos hipótesis: (H1) que la argumentación es un marco útil para la deliberación conjunta y puede mejorar sobre otros métodos típicos como la votación; y (H2) que el aprendizaje a través de la comunicación mejora el rendimiento individual de un <br>agente de aprendizaje</br> que participa en un proceso de argumentación.",
                "Finalmente, en los experimentos presentados aquí, un <br>agente de aprendizaje</br> retendría todos los contraejemplos presentados por el otro agente; sin embargo, esta es una política de retención de casos muy simple, y nos gustaría experimentar con políticas más informadas, con el objetivo de que los agentes de aprendizaje individuales puedan mejorar significativamente utilizando solo un pequeño conjunto de casos propuestos por otros agentes."
            ],
            "translated_text": "Aprendizaje y deliberación conjunta a través de la argumentación en sistemas multiagente. Santi Ontañón CCL, Laboratorio de Computación Cognitiva, Instituto de Tecnología de Georgia, Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Instituto de Investigación en Inteligencia Artificial, CSIC, Consejo Superior de Investigaciones Científicas, Campus UAB, 08193 Bellaterra, Cataluña (España) enric@iiia.csic.es RESUMEN En este artículo presentaremos un marco de argumentación para agentes de aprendizaje (AMAL) diseñado con dos propósitos: (1) para deliberación conjunta, y (2) para aprender a través de la comunicación. El marco AMAL se basa completamente en el aprendizaje a partir de ejemplos: la relación de preferencia de argumentos, la política de generación de argumentos y la política de generación de contraargumentos son técnicas basadas en casos. Para unirse en la deliberación, los agentes de aprendizaje comparten su experiencia formando un comité para decidir sobre alguna decisión conjunta. Experimentalmente demostramos que la argumentación entre comités de agentes mejora tanto el rendimiento individual como el conjunto. Para aprender a través de la comunicación, un agente se involucra en discusiones con otros agentes para contrastar sus hipótesis individuales y recibir contraejemplos; el proceso de argumentación mejora su alcance de aprendizaje y rendimiento individual. Categorías y Descriptores de Asignaturas I.2.6 [Inteligencia Artificial]: Aprendizaje; I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente, Agentes Inteligentes 1. Los marcos de argumentación para sistemas multiagente pueden ser utilizados para diferentes propósitos como la deliberación conjunta, la persuasión, la negociación y la resolución de conflictos. En este artículo presentaremos un marco de argumentación para agentes de aprendizaje, y mostraremos que puede ser utilizado para dos propósitos: (1) deliberación conjunta, y (2) aprendizaje a partir de la comunicación. La deliberación conjunta basada en argumentos implica discutir sobre el resultado de una situación particular o la acción apropiada a tomar en una situación específica. Los agentes de aprendizaje son capaces de aprender de la experiencia, en el sentido de que ejemplos pasados (situaciones y sus resultados) se utilizan para predecir el resultado para la situación actual. Sin embargo, dado que la experiencia de los agentes individuales puede ser limitada, el conocimiento individual y la precisión de la predicción también son limitados. Por lo tanto, los agentes de aprendizaje que son capaces de argumentar sus predicciones individuales con otros agentes pueden alcanzar una mayor precisión en las predicciones después de dicho proceso de argumentación. La mayoría de los marcos de argumentación existentes para sistemas multiagente se basan en lógica deductiva u otra formalización de lógica deductiva diseñada específicamente para respaldar la argumentación, como la lógica por defecto [3]. Por lo general, un argumento se ve como una declaración lógica, mientras que un contraargumento es un argumento ofrecido en oposición a otro argumento; los agentes utilizan una relación de preferencia para resolver argumentos conflictivos. Sin embargo, los marcos de argumentación basados en lógica asumen agentes con conocimiento predefinido y relación de preferencia. En este artículo, nos enfocamos en un marco de Aprendizaje Multiagente basado en Argumentación (AMAL) donde tanto el conocimiento como la relación de preferencia se aprenden a partir de la experiencia. Por lo tanto, consideramos un escenario con agentes que (1) trabajan en el mismo dominio utilizando una ontología compartida, (2) son capaces de aprender a partir de ejemplos y (3) se comunican utilizando un marco argumentativo. Tener capacidades de aprendizaje permite a los agentes utilizar de manera efectiva una forma específica de contraargumento, a saber, el uso de contraejemplos. Los contraejemplos ofrecen la posibilidad de que los agentes aprendan durante el proceso de argumentación. Además, los agentes de aprendizaje permiten técnicas que utilizan la experiencia aprendida para generar argumentos y contraargumentos adecuados. Específicamente, necesitaremos abordar dos problemas: (1) cómo definir una técnica para generar argumentos y contraargumentos a partir de ejemplos, y (2) cómo definir una relación de preferencia sobre dos argumentos en conflicto que han sido inducidos a partir de ejemplos. Este documento presenta un enfoque basado en casos para abordar ambas cuestiones. Los agentes utilizan el razonamiento basado en casos (CBR) [1] para aprender de casos pasados (donde un caso es una situación y su resultado) con el fin de predecir el resultado de una nueva situación. Proponemos un protocolo de argumentación dentro del marco de AMAL que apoya a los agentes en llegar a una predicción conjunta sobre una situación o problema específico; además, el razonamiento necesario para respaldar el proceso de argumentación también se basará en casos. En particular, presentamos dos medidas basadas en casos, una para generar los argumentos y contraargumentos adecuados para una situación particular y otra para determinar la relación de preferencia entre los argumentos. Finalmente, evaluamos (1) si la argumentación entre agentes de aprendizaje puede producir una predicción conjunta que mejore el rendimiento de aprendizaje individual y (2) si el aprendizaje a partir de los contraejemplos transmitidos durante el proceso de argumentación aumenta el rendimiento individual con precisamente aquellos casos que se utilizan mientras discuten entre ellos. El documento está estructurado de la siguiente manera. La sección 2 discute la relación entre la argumentación, la colaboración y el aprendizaje. Luego, la Sección 3 introduce nuestro marco de trabajo de CBR multiagente (MAC) y el concepto de predicción justificada. Después de eso, la Sección 4 define formalmente nuestro marco de argumentación. Las secciones 5 y 6 presentan nuestra relación de preferencia basada en casos y las políticas de generación de argumentos, respectivamente. Más adelante, la Sección 7 presenta el protocolo de argumentación en nuestro marco AMAL. Después de eso, la Sección 8 presenta una ejemplificación del marco de argumentación. Finalmente, la Sección 9 presenta una evaluación empírica de nuestras dos hipótesis principales. El artículo concluye con secciones de trabajo relacionado y conclusiones. 2. ARGUMENTACIÓN, COLABORACIÓN Y APRENDIZAJE Tanto el aprendizaje como la colaboración son formas en las que un agente puede mejorar el rendimiento individual. De hecho, existe un claro paralelismo entre el aprendizaje y la colaboración en sistemas multiagentes, ya que ambos son formas en las que los agentes pueden lidiar con sus deficiencias. Mostremos cuáles son las principales motivaciones que un agente puede tener para aprender o colaborar. • Motivaciones para aprender: - Aumentar la calidad de la predicción, - Aumentar la eficiencia, - Ampliar el rango de problemas resolubles. • Motivaciones para colaborar: - Aumentar la calidad de la predicción, - Aumentar la eficiencia, - Ampliar el rango de problemas resolubles, - Ampliar el rango de recursos accesibles. Al observar las listas de motivación anteriores, podemos ver fácilmente que el aprendizaje y la colaboración están muy relacionados en los sistemas multiagente. De hecho, con la excepción del último elemento en la lista de motivaciones para colaborar, son dos extremos de un continuo de estrategias para mejorar el rendimiento. Un agente puede optar por aumentar el rendimiento mediante el aprendizaje, la colaboración o encontrando un punto intermedio que combine el aprendizaje y la colaboración para mejorar el rendimiento. En este artículo propondremos AMAL, un marco de argumentación para agentes de aprendizaje, y también mostraremos cómo AMAL puede ser utilizado tanto para aprender a través de la comunicación como para resolver problemas de manera colaborativa: • Los agentes pueden resolver problemas de manera colaborativa al participar en un proceso de argumentación sobre la predicción para la situación actual. Utilizando esta colaboración, la predicción puede realizarse de una manera más informada, ya que se ha tenido en cuenta la información conocida por varios agentes. Los agentes también pueden aprender de la comunicación con otros agentes al participar en un proceso de argumentación. Los agentes que participan en estos procesos de argumentación pueden aprender de los argumentos y contraejemplos recibidos de otros agentes, y utilizar esta información para predecir los resultados de situaciones futuras. En el resto de este documento propondremos un marco de argumentación y mostraremos cómo puede ser utilizado tanto para el aprendizaje como para la resolución de problemas de manera colaborativa. SISTEMAS CBR MULTIAGENTE Un Sistema de Razonamiento Basado en Casos Multiagente (MAC) M = {(A1, C1), ..., (An, Cn)} es un sistema multiagente compuesto por A = {Ai, ..., An}, un conjunto de agentes CBR, donde cada agente Ai ∈ A posee una base de casos individual Ci. Cada agente individual Ai en un MAC es completamente autónomo y cada agente Ai solo tiene acceso a su base de casos individual y privada Ci. Un conjunto de casos Ci = {c1, ..., cm} es una colección de casos. Los agentes en un sistema MAC pueden resolver problemas de forma individual, pero también pueden colaborar con otros agentes para resolver problemas. En este marco, nos limitaremos a tareas analíticas, es decir, tareas como la clasificación, donde la solución de un problema se logra seleccionando una clase de solución de un conjunto enumerado de clases de solución. En lo siguiente, notaremos el conjunto de todas las clases de soluciones por S = {S1, ..., SK}. Por lo tanto, un caso c = P, S es una tupla que contiene una descripción del caso P y una clase de solución S ∈ S. En lo siguiente, utilizaremos indistintamente los términos problema y descripción del caso. Además, utilizaremos la notación de punto para referirnos a elementos dentro de una tupla; por ejemplo, para referirnos a la clase de solución de un caso c, escribiremos c.S. Por lo tanto, decimos que un grupo de agentes realiza una deliberación conjunta cuando colaboran para encontrar una solución conjunta mediante un proceso de argumentación. Sin embargo, para hacerlo, un agente debe ser capaz de justificar su predicción a los otros agentes (es decir, generar un argumento para su solución predicha que pueda ser examinado y criticado por los otros agentes). La siguiente sección aborda este tema. 3.1 Predicciones Justificadas Tanto los sistemas expertos como los sistemas CBR pueden tener un componente de explicación [14] encargado de justificar por qué el sistema ha proporcionado una respuesta específica al usuario. La línea de razonamiento del sistema puede ser examinada por un experto humano, aumentando así la fiabilidad del sistema. La mayoría de los trabajos existentes sobre generación de explicaciones se centran en generar explicaciones para ser proporcionadas al usuario. Sin embargo, en nuestro enfoque utilizamos explicaciones (o justificaciones) como una herramienta para mejorar la comunicación y coordinación entre agentes. Estamos interesados en las justificaciones ya que pueden ser utilizadas como argumentos. Para ese propósito, nos beneficiaremos de la capacidad de algunos métodos de aprendizaje automático para proporcionar justificaciones. Una justificación construida por un método CBR después de determinar que la solución de un problema particular P fue Sk es una descripción que contiene la información relevante del problema P que el método CBR ha considerado para predecir Sk como la solución de P. En particular, los métodos CBR funcionan recuperando casos similares al problema en cuestión, y luego reutilizando sus soluciones para el problema actual, esperando que dado que el problema y los casos son similares, las soluciones también serán similares. Por lo tanto, si un método CBR ha recuperado un conjunto de casos C1, ..., Cn para resolver un problema particular P, la justificación construida contendrá la información relevante del problema P que hizo que el sistema CBR recuperara ese conjunto particular de casos, es decir, contendrá la información relevante que P y C1, ..., Cn tienen en común. Por ejemplo, la Figura 1 muestra una justificación construida por un sistema CBR para un problema de juguete (en las siguientes secciones mostraremos justificaciones para problemas reales). En la figura, un problema tiene dos atributos (Semáforo y Coches_pasando), el mecanismo de recuperación del sistema CBR nota que al considerar solo el atributo Semáforo, puede recuperar dos casos que predicen la misma solución: esperar. Por lo tanto, dado que solo se ha utilizado este atributo, es el único que aparece en la justificación. Los valores del resto de atributos son irrelevantes, ya que, sea cual sea su valor, la clase de solución habría sido la misma. 976 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Problema Semáforo: rojo Pasan coches: no Caso 1 Semáforo: rojo Pasan coches: no Solución: esperar Caso 3 Semáforo: rojo Pasan coches: sí Solución: esperar Caso 4 Semáforo: verde Pasan coches: sí Solución: esperar Caso 2 Semáforo: verde Pasan coches: no Solución: cruzar Casos recuperados Solución: esperar Justificación Semáforo: rojo Figura 1: Un ejemplo de generación de justificación en un sistema de RCB. Observa que, dado que la única característica relevante para decidir es Traffic_light (la única utilizada para recuperar casos), es la única que aparece en la justificación. En general, el significado de una justificación es que todos (o la mayoría) de los casos en la base de casos de un agente que satisfacen la justificación (es decir, todos los casos que están subsumidos por la justificación) pertenecen a la clase de solución predicha. En el resto del documento, usaremos para denotar la relación de subsumpción. En nuestro trabajo, utilizamos LID [2], un método CBR capaz de construir justificaciones simbólicas como la ejemplificada en la Figura 1. Cuando un agente proporciona una justificación para una predicción, el agente genera una predicción justificada: DEFINICIÓN 3.1. Una Predicción Justificada es una tupla J = A, P, S, D donde el agente A considera que S es la solución correcta para el problema P, y esa predicción está justificada por una descripción simbólica D tal que J.D J.P. Las justificaciones pueden tener muchos usos para los sistemas de RCB [8, 9]. En este documento, vamos a utilizar justificaciones como argumentos, con el fin de permitir que los agentes de aprendizaje participen en procesos de argumentación. 4. ARGUMENTOS Y CONTRAARGUMENTOS Para nuestros propósitos, un argumento α generado por un agente A está compuesto por una afirmación S y alguna evidencia D que respalda a S como correcta. En el resto de esta sección veremos cómo esta definición general de argumento puede ser concretada en un tipo específico de argumentos que los agentes pueden generar. En el contexto de los sistemas MAC, los agentes discuten sobre predicciones para nuevos problemas y pueden proporcionar dos tipos de información: a) casos específicos P, S, y b) predicciones justificadas: A, P, S, D. Usando esta información, podemos definir tres tipos de argumentos: predicciones justificadas, contraargumentos y contraejemplos. Una predicción justificada α es generada por un agente Ai para argumentar que Ai cree que la solución correcta para un problema dado P es α.S, y la evidencia proporcionada es la justificación α.D. En el ejemplo representado en la Figura 1, un agente Ai puede generar el argumento α = Ai, P, Esperar, (Semáforo = rojo), lo que significa que el agente Ai cree que la solución correcta para P es Esperar porque el atributo Semáforo es igual a rojo. Un contraargumento β es un argumento ofrecido en oposición a otro argumento α. En nuestro marco de trabajo, un contraargumento consiste en una predicción justificada Aj, P, S, D generada por un agente Aj con la intención de refutar un argumento α generado por otro agente Ai, que respalda una clase de solución S diferente a la de α.S para el problema en cuestión y justifica esto con una justificación D. En el ejemplo de la Figura 1, si un agente genera el argumento α = Ai, P, Caminar, (Pasando_autos = no), un agente que piensa que la solución correcta es Esperar podría responder con el contraargumento β = Aj, P, Esperar, (Pasando_autos = no ∧ Semáforo = rojo), lo que significa que, aunque no hay autos pasando, el semáforo está en rojo y la calle no se puede cruzar. Un contraejemplo c es un caso que contradice un argumento α. Por lo tanto, un contraejemplo también es un contraargumento, uno que afirma que un argumento específico α no siempre es verdadero, y la evidencia proporcionada es el caso c. Específicamente, para que un caso c sea un contraejemplo de un argumento α, se deben cumplir las siguientes condiciones: α.D c y α.S = c.S, es decir, el caso debe satisfacer la justificación α.D y la solución de c debe ser diferente a la predicha por α. Al intercambiar argumentos y contraargumentos (incluyendo contraejemplos), los agentes pueden discutir sobre la solución correcta de un problema dado, es decir, pueden participar en un proceso de deliberación conjunto. Sin embargo, para hacerlo, necesitan un protocolo de interacción específico, una relación de preferencia entre argumentos contradictorios y una política de decisión para generar contraargumentos (incluidos contraejemplos). En las siguientes secciones presentaremos estos elementos. 5. RELACIÓN DE PREFERENCIA Un argumento específico proporcionado por un agente podría no ser consistente con la información conocida por otros agentes (o incluso con parte de la información conocida por el agente que ha generado la justificación debido al ruido en los datos de entrenamiento). Por esa razón, vamos a definir una relación de preferencia sobre predicciones justificadas contradictorias basadas en casos. Básicamente, definiremos una medida de confianza para cada predicción justificada (que tenga en cuenta los casos poseídos por cada agente), y la predicción justificada con la mayor confianza será la preferida. La idea detrás de la confianza basada en casos es contar cuántos de los casos en una base de casos individual respaldan una predicción justificada, y cuántos de ellos son ejemplos en contra de ella. Cuanto más casos de respaldo, mayor es la confianza; y cuanto más contrajemplos, menor es la confianza. Específicamente, para evaluar la confianza de una predicción justificada α, un agente obtiene el conjunto de casos en su base de casos individuales que están subsumidos por α.D. Con ellos, un agente Ai obtiene los valores Y (sí) y N (no): • Y Ai α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| es el número de casos en la base de casos del agente subsumidos por la justificación α.D que pertenecen a la clase de solución α.S, • NAi α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| es el número de casos en la base de casos del agente subsumidos por la justificación α.D que no pertenecen a esa clase de solución. El Sexto Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 977 + + + + + +  - - + Figura 2: La confianza de los argumentos se evalúa contrastándolos con las bases de casos de los agentes. Un agente estima la confianza de un argumento como: CAi (α) = Y Ai α 1 + Y Ai α + NAi α es decir, la confianza en una predicción justificada es el número de casos que la respaldan dividido por el número de casos que la respaldan más los contraejemplos. Observa que sumamos 1 al denominador, esto es para evitar otorgar confianzas excesivamente altas a predicciones justificadas cuya confianza se ha calculado utilizando un número pequeño de casos. Ten en cuenta que esta corrección sigue la misma idea que la corrección de Laplace para estimar probabilidades. La Figura 2 ilustra la evaluación individual de la confianza de un argumento, en particular, se encuentran tres casos de respaldo y un contraejemplo en la base de casos de los agentes Ai, lo que da una confianza estimada de 0.6. Además, también podemos definir la confianza conjunta de un argumento α como la confianza calculada utilizando los casos presentes en las bases de casos de todos los agentes en el grupo: C(α) = Π Ai α / (1 + Σ Ai α + NAi α). Nótese que, para calcular conjuntamente la confianza conjunta, los agentes solo tienen que hacer públicos los valores de aprobación y rechazo calculados localmente para un argumento dado. En nuestro marco de trabajo, los agentes utilizan esta confianza conjunta como la relación de preferencia: una predicción justificada α se prefiere sobre otra β si C(α) ≥ C(β). GENERACIÓN DE ARGUMENTOS En nuestro marco de trabajo, los argumentos son generados por los agentes a partir de casos, utilizando métodos de aprendizaje. Cualquier método de aprendizaje capaz de proporcionar una predicción justificada puede ser utilizado para generar argumentos. Por ejemplo, los árboles de decisión y LID [2] son métodos de aprendizaje adecuados. Específicamente, en los experimentos reportados en este artículo, los agentes utilizan LID. Por lo tanto, cuando un agente desea generar un argumento que respalde que una clase de solución específica es la solución correcta para un problema P, genera una predicción justificada como se explica en la Sección 3.1. Por ejemplo, la Figura 3 muestra una justificación real generada por LID después de resolver un problema P en el dominio de la identificación de esponjas marinas. En particular, la Figura 3 muestra cómo cuando un agente recibe un nuevo problema para resolver (en este caso, un nuevo esponja para determinar su orden), el agente utiliza LID para generar un argumento (que consiste en una predicción justificada) utilizando los casos en la base de casos del agente. La justificación mostrada en la Figura 3 se puede interpretar diciendo que la solución predicha es hadromerida porque la forma lisa de las megascleras del esqueleto espicular de la esponja es de tipo tilostilo, el esqueleto espicular de la esponja no tiene una longitud uniforme y no hay gemas en las características externas de la esponja. Por lo tanto, el argumento generado será α = A1, P, hadromerida, D1. 6.1 Generación de Contraargumentos Como se mencionó anteriormente, los agentes pueden intentar refutar argumentos generando contraargumentos o encontrando contraejemplos. Permítanos explicar cómo pueden ser generados. Un agente Ai quiere generar un contraargumento β para rebatir un argumento α cuando α está en contradicción con la base de casos locales de Ai. Además, al generar dicho contraargumento β, Ai espera que β sea preferido sobre α. Con ese propósito, presentaremos una política específica para generar contraargumentos basados en el criterio de especificidad [10]. El criterio de especificidad se utiliza ampliamente en marcos deductivos para la argumentación, y establece que entre dos argumentos en conflicto, se debe preferir el más específico ya que, en principio, está más informado. Por lo tanto, se espera que los contraargumentos generados basados en el criterio de especificidad sean preferibles (ya que están más informados) que los argumentos que intentan refutar. Sin embargo, no hay garantía de que tales contraargumentos siempre ganen, ya que, como hemos indicado en la Sección 5, los agentes en nuestro marco utilizan una relación de preferencia basada en la confianza conjunta. Además, uno podría pensar que sería mejor que los agentes generaran contraargumentos basados en la relación de preferencia de confianza conjunta; sin embargo, no es obvio cómo generar contraargumentos basados en la confianza conjunta de manera eficiente, ya que se requiere colaboración para evaluar la confianza conjunta. Por lo tanto, el agente que genera el contraargumento debe comunicarse constantemente con los otros agentes en cada paso del algoritmo de inducción utilizado para generar contraargumentos (actualmente una de nuestras líneas de investigación futuras). Por lo tanto, en nuestro marco de trabajo, cuando un agente quiere generar un contraargumento β a un argumento α, β tiene que ser más específico que α (es decir, α.D < β.D). La generación de contraargumentos utilizando el criterio de especificidad impone algunas restricciones sobre el método de aprendizaje, aunque LID o ID3 pueden adaptarse fácilmente para esta tarea. Por ejemplo, LID es un algoritmo que genera una descripción desde cero y añade heurísticamente características a ese término. Por lo tanto, en cada paso, la descripción se vuelve más específica que en el paso anterior, y el número de casos que se incluyen en esa descripción se reduce. Cuando la descripción cubre solo (o casi solo) casos de una única clase de solución, LID termina y predice esa clase de solución. Para generar un contraargumento a un argumento α, LID solo tiene que usar como punto de partida la descripción α.D en lugar de empezar desde cero. De esta manera, la justificación proporcionada por LID siempre estará subsumida por α.D, y por lo tanto, el contraargumento resultante será más específico que α. Sin embargo, hay que tener en cuenta que LID a veces puede no ser capaz de generar contraargumentos, ya que LID puede no ser capaz de especializar aún más la descripción α.D, o porque el agente Ai no tiene ningún caso en Ci que esté subsumido por α.D. La Figura 4 muestra cómo un agente A2 que no estaba de acuerdo con el argumento mostrado en la Figura 3, genera un contraargumento utilizando LID. Además, la Figura 4 muestra la generación de un contraargumento β1 2 para el argumento α0 1 (en la Figura 3) que es una especialización de α0 1. 978 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Solución: hadromerida Justificación: D1 Esponja Esqueleto espiculado Características externas Características externas Gemmulas: no Esqueleto espiculado Megascleres Longitud uniforme: no Megascleres Forma lisa: tilostilo Caso Base de A1 LID Nueva esponja P Figura 3: Ejemplo de una justificación real generada por LID en el conjunto de datos de esponjas marinas. Específicamente, en nuestros experimentos, cuando un agente Ai quiere refutar un argumento α, utiliza la siguiente política: 1. El Agente Ai utiliza LID para intentar encontrar un contraargumento β más específico que α; si se encuentra, β se envía al otro agente como un contraargumento de α. 2. Si no se encuentra, entonces Ai busca un contraejemplo c ∈ Ci de α. Si se encuentra un caso c, entonces c se envía al otro agente como un contraejemplo de α. Si no se encuentran contraejemplos, entonces Ai no puede refutar el argumento α. 7. APRENDIZAJE MULTIAGENTE BASADO EN ARGUMENTACIÓN El protocolo de interacción de AMAL permite que un grupo de agentes A1, ..., An deliberen sobre la solución correcta de un problema P mediante un proceso de argumentación. Si el proceso de argumentación llega a una solución consensuada, la deliberación conjunta termina; de lo contrario, se utiliza una votación ponderada para determinar la solución conjunta. Además, AMAL también permite a los agentes aprender de los contraejemplos recibidos de otros agentes. El protocolo AMAL consiste en una serie de rondas. En la ronda inicial, cada agente declara cuál es su predicción individual para P. Luego, en cada ronda, un agente puede intentar refutar la predicción hecha por cualquiera de los otros agentes. El protocolo utiliza un mecanismo de paso de token para que los agentes (uno a la vez) puedan enviar contraargumentos o contraejemplos si no están de acuerdo con la predicción hecha por cualquier otro agente. Específicamente, a cada agente se le permite enviar un contraargumento o contraejemplo cada vez que recibe el token (nota que esta restricción es solo para simplificar el protocolo, y que no restringe la cantidad de contraargumentos que un agente puede enviar, ya que pueden ser retrasados para rondas posteriores). Cuando un agente recibe un contraargumento o contraejemplo, informa a los otros agentes si acepta el contraargumento (y cambia su predicción) o no. Además, los agentes también tienen la oportunidad de responder a los contraargumentos cuando reciben el token, intentando generar un contraargumento al contraargumento. Cuando todos los agentes hayan tenido el token una vez, el token regresa al primer agente, y así sucesivamente. Si en algún momento del protocolo, todos los agentes están de acuerdo o durante las últimas n rondas ningún agente ha generado ninguna contra-argumentación, el protocolo termina. Además, si al final de la argumentación los agentes no han llegado a un acuerdo, entonces se utiliza un mecanismo de votación que utiliza la confianza de cada predicción como pesos para decidir la solución final (Así, AMAL sigue el mismo mecanismo que los comités humanos, primero cada miembro individual de un comité expone sus argumentos y discute los de los otros miembros (deliberación conjunta), y si no se alcanza un consenso, entonces se requiere un mecanismo de votación). En cada iteración, los agentes pueden usar los siguientes performativos: • afirmar(α): la predicción justificada que se mantendrá durante la próxima ronda será α. Un agente solo puede tener una única predicción en cada ronda, por lo tanto, si se envían múltiples afirmaciones, solo se considera la última como la predicción actualmente mantenida. • rebut(β, α): el agente ha encontrado un contraargumento β a la predicción α. Definiremos Ht = αt 1, ..., αt n como las predicciones que cada uno de los n agentes tiene en una ronda t. Además, también definiremos contradict(αt i) = {α ∈ Ht|α.S = αt i.S} como el conjunto de argumentos contradictorios para un agente Ai en una ronda t, es decir, el conjunto de argumentos en la ronda t que respaldan una clase de solución diferente a la de αt i. El protocolo se inicia porque uno de los agentes recibe un problema P que debe ser resuelto. Después de eso, el agente informa a todos los demás agentes sobre el problema P a resolver, y el protocolo comienza: 1. En la ronda t = 0, cada uno de los agentes resuelve individualmente P y construye una predicción justificada utilizando su propio método de RBC. Entonces, cada agente Ai envía el assertivo performativo assert(α0 i) a los otros agentes. Por lo tanto, los agentes conocen H0 = α0 i , ..., α0 n. Una vez que todas las predicciones hayan sido enviadas, el token se le da al primer agente A1. 2. En cada ronda t (distinta de 0), los agentes verifican si sus argumentos en Ht coinciden. Si lo hacen, el protocolo avanza al paso 5. Además, si durante las últimas n rondas ningún agente ha enviado ningún contraejemplo o contraargumento, el protocolo también avanza al paso 5. De lo contrario, el agente Ai propietario del token intenta generar un contraargumento para cada uno de los argumentos opuestos en contradict(αt i) ⊆ Ht (ver Sección 6.1). Entonces, se selecciona el contraargumento βt i contra la predicción αt j con la menor confianza C(αt j) (ya que αt j es la predicción más probable de ser refutada con éxito). • Si βt i es un contraargumento, entonces, Ai compara localmente αt i con βt i evaluando su confianza contra su base de casos individuales Ci (ver Sección 5) (nótese que Ai está comparando su argumento previo con el contraargumento que Ai mismo acaba de generar y que trata sobre The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 979 Esponja Esqueleto espiculado Características externas Características externas Gemas: no Crecimiento: Esqueleto espiculado Megascleres Longitud uniforme: no Megascleres Forma lisa: tilostilo Crecimiento Crecimiento: masivo Caso Base de A2 LID Solución: astrophorida Justificación: D2 Figura 4: Generación de un contraargumento utilizando LID en el conjunto de datos de esponjas. para enviar a Aj. Si CAi (βt i) > CAi (αt i), entonces Ai considera que βt i es más fuerte que su argumento anterior, cambia su argumento a βt i enviando assert(βt i) al resto de los agentes (la intuición detrás de esto es que dado que un contraargumento también es un argumento, Ai verifica si el nuevo contraargumento es un mejor argumento que el que sostenía anteriormente) y rebut(βt i, αt j) a Aj. De lo contrario (es decir, Si CAi (βt i ) ≤ CAi (αt i), Ai solo enviará rebut(βt i , αt j) a Aj. En cualquiera de las dos situaciones, el protocolo avanza al paso 3. • Si βt i es un contraejemplo c, entonces Ai envía una refutación(c, αt j) a Aj. El protocolo avanza al paso 4. • Si Ai no puede generar ningún contraargumento o contraejemplo, el token se envía al siguiente agente, comienza una nueva ronda t + 1, y el protocolo avanza al estado 2. 3. El agente Aj que ha recibido el contraargumento βt i, lo compara localmente con su propio argumento, αt j, evaluando localmente su confianza. Si CAj (βt i ) > CAj (αt j), entonces Aj aceptará el contraargumento como más fuerte que su propio argumento, y enviará assert(βt i ) a los otros agentes. De lo contrario (es decir, Si CAj (βt i ) ≤ CAj (αt j)), Aj no aceptará el contraargumento e informará a los otros agentes en consecuencia. Cualquiera de las dos situaciones inicia una nueva ronda t + 1, Ai envía el token al siguiente agente y el protocolo vuelve al estado 2. 4. El agente Aj que ha recibido el contraejemplo c lo retiene en su base de casos y genera un nuevo argumento αt+1 j que tiene en cuenta c, e informa al resto de los agentes enviando assert(αt+1 j) a todos ellos. Entonces, Ai envía el token al siguiente agente, comienza una nueva ronda t + 1 y el protocolo vuelve al paso 2. 5. El protocolo finaliza generando una predicción conjunta, de la siguiente manera: si los argumentos en Ht están de acuerdo, entonces su predicción es la predicción conjunta; de lo contrario, se utiliza un mecanismo de votación para decidir la predicción conjunta. El mecanismo de votación utiliza la medida de confianza conjunta como los pesos de votación, de la siguiente manera: S = arg max Sk∈S αi∈Ht|αi.S=Sk C(αi) Además, para evitar iteraciones infinitas, si un agente envía dos veces el mismo argumento o contraargumento al mismo agente, el mensaje no se considera. 8. EJEMPLIFICACIÓN Consideremos un sistema compuesto por tres agentes A1, A2 y A3. Uno de los agentes, A1, recibe un problema P para resolver y decide usar AMAL para resolverlo. Por esa razón, invita a A2 y A3 a participar en el proceso de argumentación. Aceptan la invitación y comienza el protocolo de argumentación. Inicialmente, cada agente genera su predicción individual para P y la transmite a los otros agentes. Por lo tanto, todos ellos pueden calcular H0 = α0 1, α0 2, α0 3. En particular, en este ejemplo: • α0 1 = A1, P, hadromerida, D1 • α0 2 = A2, P, astrophorida, D2 • α0 3 = A3, P, axinellida, D3 A1 comienza poseyendo el token e intenta generar argumentos en contra de α0 2 y α0 3, pero no tiene éxito; sin embargo, tiene un contraejemplo c13 para α0 3. Por lo tanto, A1 envía el mensaje rebut(c13, α03) a A3. A3 incorpora c13 en su base de casos e intenta resolver el problema P nuevamente, ahora teniendo en cuenta c13. A3 elabora la predicción justificada α1 3 = A3, P, hadromerida, D4, y la transmite al resto de los agentes con el mensaje assert(α1 3). Por lo tanto, todos ellos conocen el nuevo H1 = α0 1, α0 2, α1 3. La ronda 1 comienza y A2 obtiene el token. A2 intenta generar contraargumentos para α0 1 y α1 3 y solo logra generar un contraargumento β1 2 = A2, P, astrophorida, D5 contra α1 3. El contraargumento se envía a A3 con el mensaje rebut(β1 2 , α1 3). El agente A3 recibe el contraargumento y evalúa su confianza local. El resultado es que la confianza individual del contraargumento β1 2 es menor que la confianza local de α1 3. Por lo tanto, A3 no acepta el contraargumento, y así H2 = α0 1, α0 2, α1 3. La ronda 2 comienza y A3 recibe el token. A3 genera un contraargumento β2 3 = A3, P, hadromerida, D6 para α0 2 y lo envía a A2 con el mensaje refutar(β2 3 , α0 2). El agente A2 recibe el contraargumento y evalúa su confianza local. El resultado es que la confianza local del contraargumento β2 3 es mayor que la confianza local de α0 2. Por lo tanto, A2 acepta el contraargumento e informa al resto de los agentes con el mensaje assert(β2 3). Después de eso, H3 = α0 1, β2 3 , α1 3. En la Ronda 3, dado que todos los agentes están de acuerdo (todas las predicciones justificadas en H3 predicen hadromerida como la clase de solución), el protocolo finaliza, y A1 (el agente que recibió el problema) considera hadromerida como la solución conjunta para el problema P. 9. EVALUACIÓN EXPERIMENTAL 980 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) SPONGE 75 77 79 81 83 85 87 89 91 2 3 4 5 AMAL Votación Individual SOYBEAN 55 60 65 70 75 80 85 90 2 3 4 5 AMAL Votación Individual Figura 5: Precisión individual y conjunta para 2 a 5 agentes. En esta sección evaluamos empíricamente el marco de argumentación AMAL. Hemos realizado experimentos en dos conjuntos de datos diferentes: soja (del repositorio de aprendizaje automático de UCI) y esponja (un conjunto de datos relacional). El conjunto de datos de soja tiene 307 ejemplos y 19 clases de solución, mientras que el conjunto de datos de esponja tiene 280 ejemplos y 3 clases de solución. En una ejecución experimental, el conjunto de datos se divide en 2 conjuntos: el conjunto de entrenamiento y el conjunto de prueba. Los ejemplos del conjunto de entrenamiento están distribuidos entre 5 agentes diferentes sin replicación, es decir, no hay ningún ejemplo compartido por dos agentes. En la etapa de pruebas, los problemas en el conjunto de pruebas llegan aleatoriamente a uno de los agentes, y su objetivo es predecir la solución correcta. Los experimentos están diseñados para probar dos hipótesis: (H1) que la argumentación es un marco útil para la deliberación conjunta y puede mejorar sobre otros métodos típicos como la votación; y (H2) que el aprendizaje a través de la comunicación mejora el rendimiento individual de un <br>agente de aprendizaje</br> que participa en un proceso de argumentación. Además, también esperamos que la mejora lograda a través de la argumentación aumente a medida que aumente el número de agentes que participan en la argumentación (ya que se tomará en cuenta más información). En relación con H1 (la argumentación es un marco útil para la deliberación conjunta), realizamos 4 experimentos, utilizando 2, 3, 4 y 5 agentes respectivamente (en todos los experimentos cada agente tiene un 20% de los datos de entrenamiento, ya que el entrenamiento siempre se distribuye entre 5 agentes). La Figura 5 muestra el resultado de esos experimentos en los conjuntos de datos de esponjas y soja. La precisión de la clasificación se representa en el eje vertical, y en el eje horizontal se muestra el número de agentes que participaron en los procesos de argumentación. Para cada número de agentes, se muestran tres barras: individual, Voting y AMAL. La barra individual muestra la precisión promedio de las predicciones de los agentes individuales; la barra de votación muestra la precisión promedio de la predicción conjunta lograda mediante votación pero sin argumentación; y finalmente la barra de AMAL muestra la precisión promedio de la predicción conjunta utilizando argumentación. Los resultados mostrados son el promedio de 5 ejecuciones de validación cruzada de 10 pliegues. La Figura 5 muestra que la colaboración (votación y AMAL) supera el rendimiento de la resolución individual de problemas. Además, como esperábamos, la precisión mejora a medida que más agentes colaboran, ya que se tiene en cuenta más información. También podemos ver que AMAL siempre supera al voto estándar, demostrando que las decisiones conjuntas se basan en una mejor información proporcionada por el proceso de argumentación. Por ejemplo, la precisión conjunta para 2 agentes en el conjunto de datos de esponjas es del 87.57% para AMAL y del 86.57% para la votación (mientras que la precisión individual es solo del 80.07%). Además, la mejora lograda por AMAL sobre Voting es aún mayor en el conjunto de datos de soja. La razón es que el conjunto de datos de soja es más difícil (en el sentido de que los agentes necesitan más datos para producir buenas predicciones). Estos resultados experimentales muestran que AMAL aprovecha de manera efectiva la oportunidad de mejora: la precisión es mayor solo porque más agentes han cambiado su opinión durante la argumentación (de lo contrario, lograrían el mismo resultado que la Votación). En relación con H2 (el aprendizaje a partir de la comunicación en procesos de argumentación mejora la predicción individual), llevamos a cabo el siguiente experimento: inicialmente, distribuimos un 25% del conjunto de entrenamiento entre los cinco agentes; después, el resto de los casos en el conjunto de entrenamiento se envía a los agentes uno por uno; cuando un agente recibe un nuevo caso de entrenamiento, tiene varias opciones: el agente puede descartarlo, el agente puede retenerlo, o el agente puede usarlo para participar en un proceso de argumentación. La Figura 6 muestra el resultado de ese experimento para los dos conjuntos de datos. La Figura 6 contiene tres gráficos, donde NL (no aprendizaje) muestra la precisión de un agente sin aprendizaje alguno; L (aprendizaje) muestra la evolución de la precisión de clasificación individual cuando los agentes aprenden reteniendo los casos de entrenamiento que reciben individualmente (notar que cuando todos los casos de entrenamiento han sido retenidos al 100%, la precisión debería ser igual a la de la Figura 5 para los agentes individuales); y finalmente LFC (aprendizaje a partir de la comunicación) muestra la evolución de la precisión de clasificación individual de los agentes de aprendizaje que también aprenden reteniendo los contraejemplos recibidos durante la argumentación (es decir, aprenden tanto de ejemplos de entrenamiento como de contraejemplos). La Figura 6 muestra que si un agente Ai también aprende de la comunicación, Ai puede mejorar significativamente su rendimiento individual con solo un pequeño número de casos adicionales (seleccionados como contraejemplos relevantes para Ai durante la argumentación). Por ejemplo, en el conjunto de datos de soja, los agentes individuales han logrado una precisión del 70.62% cuando también aprenden de la comunicación, en comparación con una precisión del 59.93% cuando solo aprenden de su experiencia individual. El número de casos aprendidos a través de la comunicación depende de las propiedades del conjunto de datos: en el conjunto de datos de esponjas, los agentes solo han retenido muy pocos casos adicionales y han mejorado significativamente la precisión individual; es decir, retienen en promedio 59.96 casos (en comparación con los 50.4 casos retenidos si no aprenden de la comunicación). En el conjunto de datos de soja se aprenden más contraejemplos para mejorar significativamente la precisión individual, es decir, retienen en promedio 87.16 casos (en comparación con 55.27 casos retenidos si no aprenden de la comunicación). Finalmente, el hecho de que ambos conjuntos de datos muestren una mejora significativa señala la naturaleza adaptativa del enfoque basado en la argumentación para el aprendizaje a partir de la comunicación: los casos útiles se seleccionan como contraejemplos (y no más de los necesarios), y tienen el efecto deseado. 10. TRABAJO RELACIONADO En cuanto al RCB en un entorno multiagente, la primera investigación se centró en la recuperación de casos negociada [11] entre grupos de agentes. Nuestro trabajo sobre el aprendizaje basado en casos multiagente comenzó en 1999; más tarde Mc Ginty y Smyth presentaron un enfoque colaborativo de CBR multiagente (CCBR) para la planificación. Finalmente, otro enfoque interesante es el razonamiento de múltiples bases de casos (MCBR) [5], que trata sobre The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 981 ESPONJA 60 65 70 75 80 85 25% 40% 55% 70% 85% 100% LFC L NL SOJA 20 30 40 50 60 70 80 90 25% 40% 55% 70% 85% 100% LFC L NL Figura 6: Aprendizaje a partir de la comunicación resultante de la argumentación en un sistema compuesto por 5 agentes. sistemas distribuidos donde hay varias bases de casos disponibles para la misma tarea y aborda los problemas de adaptación entre bases de casos. La principal diferencia es que nuestro enfoque MAC es una forma de distribuir el proceso de Reutilización de CBR (utilizando un sistema de votación) mientras que la Recuperación se realiza individualmente por cada agente; sin embargo, los otros enfoques de CBR multiagente se centran en distribuir el proceso de Recuperación. La investigación sobre la argumentación en MAS se centra en varios temas como a) lógicas, protocolos y lenguajes que respaldan la argumentación, b) selección de argumentos y c) interpretación de argumentos. Los enfoques para la lógica y los lenguajes que respaldan la argumentación incluyen la lógica no definitiva [4] y los modelos BDI [13]. Aunque la selección de argumentos es un aspecto clave de la argumentación automatizada (ver [12] y [13]), la mayoría de las investigaciones se han centrado en las relaciones de preferencia entre argumentos. En nuestro marco de trabajo hemos abordado tanto la selección de argumentos como las relaciones de preferencia utilizando un enfoque basado en casos. 11. CONCLUSIONES Y TRABAJO FUTURO En este artículo hemos presentado un marco basado en argumentación para el aprendizaje multiagente. Específicamente, hemos presentado AMAL, un marco que permite a un grupo de agentes de aprendizaje discutir sobre la solución de un problema dado y hemos demostrado cómo las capacidades de aprendizaje pueden ser utilizadas para generar argumentos y contraargumentos. La evaluación experimental muestra que la cantidad aumentada de información proporcionada a los agentes por el proceso de argumentación aumenta su precisión predictiva, especialmente cuando un número adecuado de agentes participa en la argumentación. Las principales contribuciones de este trabajo son: a) un marco de argumentación para agentes de aprendizaje; b) una relación de preferencia basada en casos sobre argumentos, basada en el cálculo de una estimación general de confianza de los argumentos; c) una política basada en casos para generar contraargumentos y seleccionar contraejemplos; y d) un enfoque basado en la argumentación para aprender a partir de la comunicación. Finalmente, en los experimentos presentados aquí, un <br>agente de aprendizaje</br> retendría todos los contraejemplos presentados por el otro agente; sin embargo, esta es una política de retención de casos muy simple, y nos gustaría experimentar con políticas más informadas, con el objetivo de que los agentes de aprendizaje individuales puedan mejorar significativamente utilizando solo un pequeño conjunto de casos propuestos por otros agentes. Finalmente, nuestro enfoque se centra en el aprendizaje perezoso, y los trabajos futuros tienen como objetivo incorporar el aprendizaje inductivo ansioso dentro del marco argumentativo para el aprendizaje a partir de la comunicación. 12. REFERENCIAS [1] Agnar Aamodt y Enric Plaza. Razonamiento basado en casos: Aspectos fundamentales, variaciones metodológicas y enfoques de sistemas. Comunicaciones de Inteligencia Artificial, 7(1):39-59, 1994. [2] E. Armengol y E. Plaza. Inducción perezosa de descripciones para el aprendizaje basado en casos relacionales. En ECML2001, páginas 13-24, 2001. [3] Gerhard Brewka. Sistemas de argumentación dinámica: Un modelo formal de procesos de argumentación basado en el cálculo de situaciones. Revista de Lógica y Computación, 11(2):257-282, 2001. [4] Carlos I. Chesñevar y Guillermo R. Simari. Formalizando la argumentación no definitiva utilizando sistemas deductivos etiquetados. Revista de Ciencias de la Computación y Tecnología, 1(4):18-33, 2000. [5] D. Leake y R. Sooriamurthi. Seleccionando estrategias automáticamente para el razonamiento multi-base de casos. En S. Craw y A. Preece, editores, ECCBR2002, páginas 204-219, Berlín, 2002. Springer Verlag. [6] Francisco J. Martín, Enric Plaza y Josep-Lluis Arcos. Reutilización de conocimiento y experiencia a través de comunicaciones entre agentes competentes (pares). Revista Internacional de Ingeniería de Software e Ingeniería del Conocimiento, 9(3):319-341, 1999. [7] Lorraine McGinty y Barry Smyth. Razonamiento basado en casos colaborativo: Aplicaciones en la planificación de rutas personalizadas. En I. Watson y Q. Yang, editores, ICCBR, número 2080 en LNAI, páginas 362-376. Springer-Verlag, 2001. [8] Santi Ontañón y Enric Plaza. Aprendizaje multiagente basado en justificaciones. En ICML2003, páginas 576-583. Morgan Kaufmann, 2003. [9] Enric Plaza, Eva Armengol y Santiago Ontañón. El poder explicativo de la similitud simbólica en el razonamiento basado en casos. Revisión de Inteligencia Artificial, 24(2):145-161, 2005. [10] David Poole. En la comparación de teorías: prefiriendo la explicación más específica. En IJCAI-85, páginas 144-147, 1985. [11] M V Nagendra Prassad, Victor R Lesser y Susan Lander. Recuperación y razonamiento en bases de casos distribuidas. Informe técnico, Departamento de Ciencias de la Computación de UMass, 1995. [12] K. Sycara S. Kraus y A. Evenchik. Alcanzando acuerdos a través de la argumentación: un modelo lógico y su implementación. Revista de Inteligencia Artificial, 104:1-69, 1998. [13] N. R. Jennings, S. Parsons, C. Sierra. Agentes que razonan y negocian mediante argumentos. Revista de Lógica y Computación, 8:261-292, 1998. [14] Bruce A. Wooley. Componente de explicación de sistemas de software. ACM CrossRoads, 5.1, 1998. 982 The Sixth Intl. \n\nACM CrossRoads, 5.1, 1998. 982 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "learning from communication": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning and Joint Deliberation through Argumentation in Multi-Agent Systems Santi Ontañón CCL, Cognitive Computing Lab Georgia Institute of Technology Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Artificial Intelligence Research Institute CSIC, Spanish Council for Scientific Research Campus UAB, 08193 Bellaterra, Catalonia (Spain) enric@iiia.csic.es ABSTRACT In this paper we will present an argumentation framework for learning agents (AMAL) designed for two purposes: (1) for joint deliberation, and (2) for <br>learning from communication</br>.",
                "The AMAL framework is completely based on learning from examples: the argument preference relation, the argument generation policy, and the counterargument generation policy are case-based techniques.",
                "For join deliberation, learning agents share their experience by forming a committee to decide upon some joint decision.",
                "We experimentally show that the argumentation among committees of agents improves both the individual and joint performance.",
                "For <br>learning from communication</br>, an agent engages into arguing with other agents in order to contrast its individual hypotheses and receive counterexamples; the argumentation process improves their learning scope and individual performance.",
                "Categories and Subject Descriptors I.2.6 [Artificial Intelligence]: Learning; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems, Intelligent Agents 1.",
                "INTRODUCTION Argumentation frameworks for multi-agent systems can be used for different purposes like joint deliberation, persuasion, negotiation, and conflict resolution.",
                "In this paper we will present an argumentation framework for learning agents, and show that it can be used for two purposes: (1) joint deliberation, and (2) <br>learning from communication</br>.",
                "Argumentation-based joint deliberation involves discussion over the outcome of a particular situation or the appropriate course of action for a particular situation.",
                "Learning agents are capable of learning from experience, in the sense that past examples (situations and their outcomes) are used to predict the outcome for the situation at hand.",
                "However, since individual agents experience may be limited, individual knowledge and prediction accuracy is also limited.",
                "Thus, learning agents that are capable of arguing their individual predictions with other agents may reach better prediction accuracy after such an argumentation process.",
                "Most existing argumentation frameworks for multi-agent systems are based on deductive logic or some other deductive logic formalism specifically designed to support argumentation, such as default logic [3]).",
                "Usually, an argument is seen as a logical statement, while a counterargument is an argument offered in opposition to another argument [4, 13]; agents use a preference relation to resolve conflicting arguments.",
                "However, logic-based argumentation frameworks assume agents with preloaded knowledge and preference relation.",
                "In this paper, we focus on an Argumentation-based Multi-Agent Learning (AMAL) framework where both knowledge and preference relation are learned from experience.",
                "Thus, we consider a scenario with agents that (1) work in the same domain using a shared ontology, (2) are capable of learning from examples, and (3) communicate using an argumentative framework.",
                "Having learning capabilities allows agents effectively use a specific form of counterargument, namely the use of counterexamples.",
                "Counterexamples offer the possibility of agents learning during the argumentation process.",
                "Moreover, learning agents allow techniques that use learnt experience to generate adequate arguments and counterarguments.",
                "Specifically, we will need to address two issues: (1) how to define a technique to generate arguments and counterarguments from examples, and (2)how to define a preference relation over two conflicting arguments that have been induced from examples.",
                "This paper presents a case-based approach to address both issues.",
                "The agents use case-based reasoning (CBR) [1] to learn from past cases (where a case is a situation and its outcome) in order to predict the outcome of a new situation.",
                "We propose an argumentation protocol inside the AMAL framework at supports agents in reaching a joint prediction over a specific situation or problem - moreover, the reasoning needed to support the argumentation process will also be based on cases.",
                "In particular, we present two case-based measures, one for generating the arguments and counterarguments adequate to a particular situation and another for determining preference relation among arguments.",
                "Finally, we evaluate (1) if argumentation between learning agents can produce a joint prediction that improves over individual learning performance and (2) if learning from the counterexamples conveyed during the argumentation process increases the individual performance with precisely those cases being used while arguing among them.",
                "The paper is structured as follows.",
                "Section 2 discusses the relation among argumentation, collaboration and learning.",
                "Then Section 3 introduces our multi-agent CBR (MAC) framework and the notion of justified prediction.",
                "After that, Section 4 formally defines our argumentation framework.",
                "Sections 5 and 6 present our case-based preference relation and argument generation policies respectively.",
                "Later, Section 7 presents the argumentation protocol in our AMAL framework.",
                "After that, Section 8 presents an exemplification of the argumentation framework.",
                "Finally, Section 9 presents an empirical evaluation of our two main hypotheses.",
                "The paper closes with related work and conclusions sections. 2.",
                "ARGUMENTATION,COLLABORATION AND LEARNING Both learning and collaboration are ways in which an agent can improve individual performance.",
                "In fact, there is a clear parallelism between learning and collaboration in multi-agent systems, since both are ways in which agents can deal with their shortcomings.",
                "Let us show which are the main motivations that an agent can have to learn or to collaborate. • Motivations to learn: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems. • Motivations to collaborate: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems, - Increase the range of accessible resources.",
                "Looking at the above lists of motivation, we can easily see that learning and collaboration are very related in multi-agent systems.",
                "In fact, with the exception of the last item in the motivations to collaborate list, they are two extremes of a continuum of strategies to improve performance.",
                "An agent may choose to increase performance by learning, by collaborating, or by finding an intermediate point that combines learning and collaboration in order to improve performance.",
                "In this paper we will propose AMAL, an argumentation framework for learning agents, and will also also show how AMAL can be used both for <br>learning from communication</br> and for solving problems in a collaborative way: • Agents can solve problems in a collaborative way via engaging an argumentation process about the prediction for the situation at hand.",
                "Using this collaboration, the prediction can be done in a more informed way, since the information known by several agents has been taken into account. • Agents can also learn from communication with other agents by engaging an argumentation process.",
                "Agents that engage in such argumentation processes can learn from the arguments and counterexamples received from other agents, and use this information for predicting the outcomes of future situations.",
                "In the rest of this paper we will propose an argumentation framework and show how it can be used both for learning and for solving problems in a collaborative way. 3.",
                "MULTI-AGENT CBR SYSTEMS A Multi-Agent Case Based Reasoning System (MAC) M = {(A1, C1), ..., (An, Cn)} is a multi-agent system composed of A = {Ai, ..., An}, a set of CBR agents, where each agent Ai ∈ A possesses an individual case base Ci.",
                "Each individual agent Ai in a MAC is completely autonomous and each agent Ai has access only to its individual and private case base Ci.",
                "A case base Ci = {c1, ..., cm} is a collection of cases.",
                "Agents in a MAC system are able to individually solve problems, but they can also collaborate with other agents to solve problems.",
                "In this framework, we will restrict ourselves to analytical tasks, i.e. tasks like classification, where the solution of a problem is achieved by selecting a solution class from an enumerated set of solution classes.",
                "In the following we will note the set of all the solution classes by S = {S1, ..., SK }.",
                "Therefore, a case c = P, S is a tuple containing a case description P and a solution class S ∈ S. In the following, we will use the terms problem and case description indistinctly.",
                "Moreover, we will use the dot notation to refer to elements inside a tuple; e.g., to refer to the solution class of a case c, we will write c.S.",
                "Therefore, we say a group of agents perform joint deliberation, when they collaborate to find a joint solution by means of an argumentation process.",
                "However, in order to do so, an agent has to be able to justify its prediction to the other agents (i.e. generate an argument for its predicted solution that can be examined and critiqued by the other agents).",
                "The next section addresses this issue. 3.1 Justified Predictions Both expert systems and CBR systems may have an explanation component [14] in charge of justifying why the system has provided a specific answer to the user.",
                "The line of reasoning of the system can then be examined by a human expert, thus increasing the reliability of the system.",
                "Most of the existing work on explanation generation focuses on generating explanations to be provided to the user.",
                "However, in our approach we use explanations (or justifications) as a tool for improving communication and coordination among agents.",
                "We are interested in justifications since they can be used as arguments.",
                "For that purpose, we will benefit from the ability of some machine learning methods to provide justifications.",
                "A justification built by a CBR method after determining that the solution of a particular problem P was Sk is a description that contains the relevant information from the problem P that the CBR method has considered to predict Sk as the solution of P. In particular, CBR methods work by retrieving similar cases to the problem at hand, and then reusing their solutions for the current problem, expecting that since the problem and the cases are similar, the solutions will also be similar.",
                "Thus, if a CBR method has retrieved a set of cases C1, ..., Cn to solve a particular problem P the justification built will contain the relevant information from the problem P that made the CBR system retrieve that particular set of cases, i.e. it will contain the relevant information that P and C1, ..., Cn have in common.",
                "For example, Figure 1 shows a justification build by a CBR system for a toy problem (in the following sections we will show justifications for real problems).",
                "In the figure, a problem has two attributes (Traffic_light, and Cars_passing), the retrieval mechanism of the CBR system notices that by considering only the attribute Traffic_light, it can retrieve two cases that predict the same solution: wait.",
                "Thus, since only this attribute has been used, it is the only one appearing in the justification.",
                "The values of the rest of attributes are irrelevant, since whatever their value the solution class would have been the same. 976 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Problem Traffic_light: red Cars_passing: no Case 1 Traffic_light: red Cars_passing: no Solution: wait Case 3 Traffic_light: red Cars_passing: yes Solution: wait Case 4 Traffic_light: green Cars_passing: yes Solution: wait Case 2 Traffic_light: green Cars_passing: no Solution: cross Retrieved cases Solution: wait Justification Traffic_light: red Figure 1: An example of justification generation in a CBR system.",
                "Notice that, since the only relevant feature to decide is Traffic_light (the only one used to retrieve cases), it is the only one appearing in the justification.",
                "In general, the meaning of a justification is that all (or most of) the cases in the case base of an agent that satisfy the justification (i.e. all the cases that are subsumed by the justification) belong to the predicted solution class.",
                "In the rest of the paper, we will use to denote the subsumption relation.",
                "In our work, we use LID [2], a CBR method capable of building symbolic justifications such as the one exemplified in Figure 1.",
                "When an agent provides a justification for a prediction, the agent generates a justified prediction: DEFINITION 3.1.",
                "A Justified Prediction is a tuple J = A, P, S, D where agent A considers S the correct solution for problem P, and that prediction is justified a symbolic description D such that J.D J.P. Justifications can have many uses for CBR systems [8, 9].",
                "In this paper, we are going to use justifications as arguments, in order to allow learning agents to engage in argumentation processes. 4.",
                "ARGUMENTS AND COUNTERARGUMENTS For our purposes an argument α generated by an agent A is composed of a statement S and some evidence D supporting S as correct.",
                "In the remainder of this section we will see how this general definition of argument can be instantiated in specific kind of arguments that the agents can generate.",
                "In the context of MAC systems, agents argue about predictions for new problems and can provide two kinds of information: a) specific cases P, S , and b) justified predictions: A, P, S, D .",
                "Using this information, we can define three types of arguments: justified predictions, counterarguments, and counterexamples.",
                "A justified prediction α is generated by an agent Ai to argue that Ai believes that the correct solution for a given problem P is α.S, and the evidence provided is the justification α.D.",
                "In the example depicted in Figure 1, an agent Ai may generate the argument α = Ai, P, Wait, (Traffic_light = red) , meaning that the agent Ai believes that the correct solution for P is Wait because the attribute Traffic_light equals red.",
                "A counterargument β is an argument offered in opposition to another argument α.",
                "In our framework, a counterargument consists of a justified prediction Aj, P, S , D generated by an agent Aj with the intention to rebut an argument α generated by another agent Ai, that endorses a solution class S different from that of α.S for the problem at hand and justifies this with a justification D .",
                "In the example in Figure 1, if an agent generates the argument α = Ai, P, Walk, (Cars_passing = no) , an agent that thinks that the correct solution is Wait might answer with the counterargument β = Aj, P, Wait, (Cars_passing = no ∧ Traffic_light = red) , meaning that, although there are no cars passing, the traffic light is red, and the street cannot be crossed.",
                "A counterexample c is a case that contradicts an argument α.",
                "Thus a counterexample is also a counterargument, one that states that a specific argument α is not always true, and the evidence provided is the case c. Specifically, for a case c to be a counterexample of an argument α, the following conditions have to be met: α.D c and α.S = c.S, i.e. the case must satisfy the justification α.D and the solution of c must be different than the predicted by α.",
                "By exchanging arguments and counterarguments (including counterexamples), agents can argue about the correct solution of a given problem, i.e. they can engage a joint deliberation process.",
                "However, in order to do so, they need a specific interaction protocol, a preference relation between contradicting arguments, and a decision policy to generate counterarguments (including counterexamples).",
                "In the following sections we will present these elements. 5.",
                "PREFERENCE RELATION A specific argument provided by an agent might not be consistent with the information known to other agents (or even to some of the information known by the agent that has generated the justification due to noise in training data).",
                "For that reason, we are going to define a preference relation over contradicting justified predictions based on cases.",
                "Basically, we will define a confidence measure for each justified prediction (that takes into account the cases owned by each agent), and the justified prediction with the highest confidence will be the preferred one.",
                "The idea behind case-based confidence is to count how many of the cases in an individual case base endorse a justified prediction, and how many of them are counterexamples of it.",
                "The more the endorsing cases, the higher the confidence; and the more the counterexamples, the lower the confidence.",
                "Specifically, to assess the confidence of a justified prediction α, an agent obtains the set of cases in its individual case base that are subsumed by α.D.",
                "With them, an agent Ai obtains the Y (aye) and N (nay) values: • Y Ai α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by the justification α.D that belong to the solution class α.S, • NAi α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by justification α.D that do not belong to that solution class.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 977 + + + + + +  - - + Figure 2: Confidence of arguments is evaluated by contrasting them against the case bases of the agents.",
                "An agent estimates the confidence of an argument as: CAi (α) = Y Ai α 1 + Y Ai α + NAi α i.e. the confidence on a justified prediction is the number of endorsing cases divided by the number of endorsing cases plus counterexamples.",
                "Notice that we add 1 to the denominator, this is to avoid giving excessively high confidences to justified predictions whose confidence has been computed using a small number of cases.",
                "Notice that this correction follows the same idea than the Laplace correction to estimate probabilities.",
                "Figure 2 illustrates the individual evaluation of the confidence of an argument, in particular, three endorsing cases and one counterexample are found in the case base of agents Ai, giving an estimated confidence of 0.6 Moreover, we can also define the joint confidence of an argument α as the confidence computed using the cases present in the case bases of all the agents in the group: C(α) = i Y Ai α 1 + i Y Ai α + NAi α Notice that, to collaboratively compute the joint confidence, the agents only have to make public the aye and nay values locally computed for a given argument.",
                "In our framework, agents use this joint confidence as the preference relation: a justified prediction α is preferred over another one β if C(α) ≥ C(β). 6.",
                "GENERATION OF ARGUMENTS In our framework, arguments are generated by the agents from cases, using learning methods.",
                "Any learning method able to provide a justified prediction can be used to generate arguments.",
                "For instance, decision trees and LID [2] are suitable learning methods.",
                "Specifically, in the experiments reported in this paper agents use LID.",
                "Thus, when an agent wants to generate an argument endorsing that a specific solution class is the correct solution for a problem P, it generates a justified prediction as explained in Section 3.1.",
                "For instance, Figure 3 shows a real justification generated by LID after solving a problem P in the domain of marine sponges identification.",
                "In particular, Figure 3 shows how when an agent receives a new problem to solve (in this case, a new sponge to determine its order), the agent uses LID to generate an argument (consisting on a justified prediction) using the cases in the case base of the agent.",
                "The justification shown in Figure 3 can be interpreted saying that the predicted solution is hadromerida because the smooth form of the megascleres of the spiculate skeleton of the sponge is of type tylostyle, the spikulate skeleton of the sponge has no uniform length, and there is no gemmules in the external features of the sponge.",
                "Thus, the argument generated will be α = A1, P, hadromerida, D1 . 6.1 Generation of Counterarguments As previously stated, agents may try to rebut arguments by generating counterargument or by finding counterexamples.",
                "Let us explain how they can be generated.",
                "An agent Ai wants to generate a counterargument β to rebut an argument α when α is in contradiction with the local case base of Ai.",
                "Moreover, while generating such counterargument β, Ai expects that β is preferred over α.",
                "For that purpose, we will present a specific policy to generate counterarguments based on the specificity criterion [10].",
                "The specificity criterion is widely used in deductive frameworks for argumentation, and states that between two conflicting arguments, the most specific should be preferred since it is, in principle, more informed.",
                "Thus, counterarguments generated based on the specificity criterion are expected to be preferable (since they are more informed) to the arguments they try to rebut.",
                "However, there is no guarantee that such counterarguments will always win, since, as we have stated in Section 5, agents in our framework use a preference relation based on joint confidence.",
                "Moreover, one may think that it would be better that the agents generate counterarguments based on the joint confidence preference relation; however it is not obvious how to generate counterarguments based on joint confidence in an efficient way, since collaboration is required in order to evaluate joint confidence.",
                "Thus, the agent generating the counterargument should constantly communicate with the other agents at each step of the induction algorithm used to generate counterarguments (presently one of our future research lines).",
                "Thus, in our framework, when an agent wants to generate a counterargument β to an argument α, β has to be more specific than α (i.e. α.D < β.D).",
                "The generation of counterarguments using the specificity criterion imposes some restrictions over the learning method, although LID or ID3 can be easily adapted for this task.",
                "For instance, LID is an algorithm that generates a description starting from scratch and heuristically adding features to that term.",
                "Thus, at every step, the description is made more specific than in the previous step, and the number of cases that are subsumed by that description is reduced.",
                "When the description covers only (or almost only) cases of a single solution class LID terminates and predicts that solution class.",
                "To generate a counterargument to an argument α LID just has to use as starting point the description α.D instead of starting from scratch.",
                "In this way, the justification provided by LID will always be subsumed by α.D, and thus the resulting counterargument will be more specific than α.",
                "However, notice that LID may sometimes not be able to generate counterarguments, since LID may not be able to specialize the description α.D any further, or because the agent Ai has no case inCi that is subsumed by α.D.",
                "Figure 4 shows how an agent A2 that disagreed with the argument shown in Figure 3, generates a counterargument using LID.",
                "Moreover, Figure 4 shows the generation of a counterargument β1 2 for the argument α0 1 (in Figure 3) that is a specialization of α0 1. 978 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Solution: hadromerida Justification: D1 Sponge Spikulate skeleton External features External features Gemmules: no Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Case Base of A1 LID New sponge P Figure 3: Example of a real justification generated by LID in the marine sponges data set.",
                "Specifically, in our experiments, when an agent Ai wants to rebut an argument α, uses the following policy: 1.",
                "Agent Ai uses LID to try to find a counterargument β more specific than α; if found, β is sent to the other agent as a counterargument of α. 2.",
                "If not found, then Ai searches for a counterexample c ∈ Ci of α.",
                "If a case c is found, then c is sent to the other agent as a counterexample of α. 3.",
                "If no counterexamples are found, then Ai cannot rebut the argument α. 7.",
                "ARGUMENTATION-BASED MULTI-AGENT LEARNING The interaction protocol of AMAL allows a group of agents A1, ..., An to deliberate about the correct solution of a problem P by means of an argumentation process.",
                "If the argumentation process arrives to a consensual solution, the joint deliberation ends; otherwise a weighted vote is used to determine the joint solution.",
                "Moreover, AMAL also allows the agents to learn from the counterexamples received from other agents.",
                "The AMAL protocol consists on a series of rounds.",
                "In the initial round, each agent states which is its individual prediction for P. Then, at each round an agent can try to rebut the prediction made by any of the other agents.",
                "The protocol uses a token passing mechanism so that agents (one at a time) can send counterarguments or counterexamples if they disagree with the prediction made by any other agent.",
                "Specifically, each agent is allowed to send one counterargument or counterexample each time he gets the token (notice that this restriction is just to simplify the protocol, and that it does not restrict the number of counterargument an agent can sent, since they can be delayed for subsequent rounds).",
                "When an agent receives a counterargument or counterexample, it informs the other agents if it accepts the counterargument (and changes its prediction) or not.",
                "Moreover, agents have also the opportunity to answer to counterarguments when they receive the token, by trying to generate a counterargument to the counterargument.",
                "When all the agents have had the token once, the token returns to the first agent, and so on.",
                "If at any time in the protocol, all the agents agree or during the last n rounds no agent has generated any counterargument, the protocol ends.",
                "Moreover, if at the end of the argumentation the agents have not reached an agreement, then a voting mechanism that uses the confidence of each prediction as weights is used to decide the final solution (Thus, AMAL follows the same mechanism as human committees, first each individual member of a committee exposes his arguments and discuses those of the other members (joint deliberation), and if no consensus is reached, then a voting mechanism is required).",
                "At each iteration, agents can use the following performatives: • assert(α): the justified prediction held during the next round will be α.",
                "An agent can only hold a single prediction at each round, thus is multiple asserts are send, only the last one is considered as the currently held prediction. • rebut(β, α): the agent has found a counterargument β to the prediction α.",
                "We will define Ht = αt 1, ..., αt n as the predictions that each of the n agents hold at a round t. Moreover, we will also define contradict(αt i) = {α ∈ Ht|α.S = αt i.S} as the set of contradicting arguments for an agent Ai in a round t, i.e. the set of arguments at round t that support a different solution class than αt i.",
                "The protocol is initiated because one of the agents receives a problem P to be solved.",
                "After that, the agent informs all the other agents about the problem P to solve, and the protocol starts: 1.",
                "At round t = 0, each one of the agents individually solves P, and builds a justified prediction using its own CBR method.",
                "Then, each agent Ai sends the performative assert(α0 i ) to the other agents.",
                "Thus, the agents know H0 = α0 i , ..., α0 n .",
                "Once all the predictions have been sent the token is given to the first agent A1. 2.",
                "At each round t (other than 0), the agents check whether their arguments in Ht agree.",
                "If they do, the protocol moves to step 5.",
                "Moreover, if during the last n rounds no agent has sent any counterexample or counterargument, the protocol also moves to step 5.",
                "Otherwise, the agent Ai owner of the token tries to generate a counterargument for each of the opposing arguments in contradict(αt i) ⊆ Ht (see Section 6.1).",
                "Then, the counterargument βt i against the prediction αt j with the lowest confidence C(αt j) is selected (since αt j is the prediction more likely to be successfully rebutted). • If βt i is a counterargument, then, Ai locally compares αt i with βt i by assessing their confidence against its individual case base Ci (see Section 5) (notice that Ai is comparing its previous argument with the counterargument that Ai itself has just generated and that is about The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 979 Sponge Spikulate skeleton External features External features Gemmules: no Growing: Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Growing Grow: massive Case Base of A2 LID Solution: astrophorida Justification: D2 Figure 4: Generation of a counterargument using LID in the sponges data set. to send to Aj).",
                "If CAi (βt i ) > CAi (αt i), then Ai considers that βt i is stronger than its previous argument, changes its argument to βt i by sending assert(βt i ) to the rest of the agents (the intuition behind this is that since a counterargument is also an argument, Ai checks if the newly counterargument is a better argument than the one he was previously holding) and rebut(βt i , αt j) to Aj.",
                "Otherwise (i.e.",
                "CAi (βt i ) ≤ CAi (αt i)), Ai will send only rebut(βt i , αt j) to Aj.",
                "In any of the two situations the protocol moves to step 3. • If βt i is a counterexample c, then Ai sends rebut(c, αt j) to Aj.",
                "The protocol moves to step 4. • If Ai cannot generate any counterargument or counterexample, the token is sent to the next agent, a new round t + 1 starts, and the protocol moves to state 2. 3.",
                "The agent Aj that has received the counterargument βt i , locally compares it against its own argument, αt j, by locally assessing their confidence.",
                "If CAj (βt i ) > CAj (αt j), then Aj will accept the counterargument as stronger than its own argument, and it will send assert(βt i ) to the other agents.",
                "Otherwise (i.e.",
                "CAj (βt i ) ≤ CAj (αt j)), Aj will not accept the counterargument, and will inform the other agents accordingly.",
                "Any of the two situations start a new round t + 1, Ai sends the token to the next agent, and the protocol moves back to state 2. 4.",
                "The agent Aj that has received the counterexample c retains it into its case base and generates a new argument αt+1 j that takes into account c, and informs the rest of the agents by sending assert(αt+1 j ) to all of them.",
                "Then, Ai sends the token to the next agent, a new round t + 1 starts, and the protocol moves back to step 2. 5.",
                "The protocol ends yielding a joint prediction, as follows: if the arguments in Ht agree then their prediction is the joint prediction, otherwise a voting mechanism is used to decide the joint prediction.",
                "The voting mechanism uses the joint confidence measure as the voting weights, as follows: S = arg max Sk∈S αi∈Ht|αi.S=Sk C(αi) Moreover, in order to avoid infinite iterations, if an agent sends twice the same argument or counterargument to the same agent, the message is not considered. 8.",
                "EXEMPLIFICATION Let us consider a system composed of three agents A1, A2 and A3.",
                "One of the agents, A1 receives a problem P to solve, and decides to use AMAL to solve it.",
                "For that reason, invites A2 and A3 to take part in the argumentation process.",
                "They accept the invitation, and the argumentation protocol starts.",
                "Initially, each agent generates its individual prediction for P, and broadcasts it to the other agents.",
                "Thus, all of them can compute H0 = α0 1, α0 2, α0 3 .",
                "In particular, in this example: • α0 1 = A1, P, hadromerida, D1 • α0 2 = A2, P, astrophorida, D2 • α0 3 = A3, P, axinellida, D3 A1 starts owning the token and tries to generate counterarguments for α0 2 and α0 3, but does not succeed, however it has one counterexample c13 for α0 3.",
                "Thus, A1 sends the the message rebut( c13, α0 3) to A3.",
                "A3 incorporates c13 into its case base and tries to solve the problem P again, now taking c13 into consideration.",
                "A3 comes up with the justified prediction α1 3 = A3, P, hadromerida, D4 , and broadcasts it to the rest of the agents with the message assert(α1 3).",
                "Thus, all of them know the new H1 = α0 1, α0 2, α1 3 .",
                "Round 1 starts and A2 gets the token.",
                "A2 tries to generate counterarguments for α0 1 and α1 3 and only succeeds to generate a counterargument β1 2 = A2, P, astrophorida, D5 against α1 3.",
                "The counterargument is sent to A3 with the message rebut(β1 2 , α1 3).",
                "Agent A3 receives the counterargument and assesses its local confidence.",
                "The result is that the individual confidence of the counterargument β1 2 is lower than the local confidence of α1 3.",
                "Therefore, A3 does not accept the counterargument, and thus H2 = α0 1, α0 2, α1 3 .",
                "Round 2 starts and A3 gets the token.",
                "A3 generates a counterargument β2 3 = A3, P, hadromerida, D6 for α0 2 and sends it to A2 with the message rebut(β2 3 , α0 2).",
                "Agent A2 receives the counterargument and assesses its local confidence.",
                "The result is that the local confidence of the counterargument β2 3 is higher than the local confidence of α0 2.",
                "Therefore, A2 accepts the counterargument and informs the rest of the agents with the message assert(β2 3 ).",
                "After that, H3 = α0 1, β2 3 , α1 3 .",
                "At Round 3, since all the agents agree (all the justified predictions in H3 predict hadromerida as the solution class) The protocol ends, and A1 (the agent that received the problem) considers hadromerida as the joint solution for the problem P. 9.",
                "EXPERIMENTAL EVALUATION 980 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) SPONGE 75 77 79 81 83 85 87 89 91 2 3 4 5 AMAL Voting Individual SOYBEAN 55 60 65 70 75 80 85 90 2 3 4 5 AMAL Voting Individual Figure 5: Individual and joint accuracy for 2 to 5 agents.",
                "In this section we empirically evaluate the AMAL argumentation framework.",
                "We have made experiments in two different data sets: soybean (from the UCI machine learning repository) and sponge (a relational data set).",
                "The soybean data set has 307 examples and 19 solution classes, while the sponge data set has 280 examples and 3 solution classes.",
                "In an experimental run, the data set is divided in 2 sets: the training set and the test set.",
                "The training set examples are distributed among 5 different agents without replication, i.e. there is no example shared by two agents.",
                "In the testing stage, problems in the test set arrive randomly to one of the agents, and their goal is to predict the correct solution.",
                "The experiments are designed to test two hypotheses: (H1) that argumentation is a useful framework for joint deliberation and can improve over other typical methods such as voting; and (H2) that <br>learning from communication</br> improves the individual performance of a learning agent participating in an argumentation process.",
                "Moreover, we also expect that the improvement achieved from argumentation will increase as the number of agents participating in the argumentation increases (since more information will be taken into account).",
                "Concerning H1 (argumentation is a useful framework for joint deliberation), we ran 4 experiments, using 2, 3, 4, and 5 agents respectively (in all experiments each agent has a 20% of the training data, since the training is always distributed among 5 agents).",
                "Figure 5 shows the result of those experiments in the sponge and soybean data sets.",
                "Classification accuracy is plotted in the vertical axis, and in the horizontal axis the number of agents that took part in the argumentation processes is shown.",
                "For each number of agents, three bars are shown: individual, Voting, and AMAL.",
                "The individual bar shows the average accuracy of individual agents predictions; the voting bar shows the average accuracy of the joint prediction achieved by voting but without any argumentation; and finally the AMAL bar shows the average accuracy of the joint prediction using argumentation.",
                "The results shown are the average of 5 10-fold cross validation runs.",
                "Figure 5 shows that collaboration (voting and AMAL) outperforms individual problem solving.",
                "Moreover, as we expected, the accuracy improves as more agents collaborate, since more information is taken into account.",
                "We can also see that AMAL always outperforms standard voting, proving that joint decisions are based on better information as provided by the argumentation process.",
                "For instance, the joint accuracy for 2 agents in the sponge data set is of 87.57% for AMAL and 86.57% for voting (while individual accuracy is just 80.07%).",
                "Moreover, the improvement achieved by AMAL over Voting is even larger in the soybean data set.",
                "The reason is that the soybean data set is more difficult (in the sense that agents need more data to produce good predictions).",
                "These experimental results show that AMAL effectively exploits the opportunity for improvement: the accuracy is higher only because more agents have changed their opinion during argumentation (otherwise they would achieve the same result as Voting).",
                "Concerning H2 (<br>learning from communication</br> in argumentation processes improves individual prediction ), we ran the following experiment: initially, we distributed a 25% of the training set among the five agents; after that, the rest of the cases in the training set is sent to the agents one by one; when an agent receives a new training case, it has several options: the agent can discard it, the agent can retain it, or the agent can use it for engaging an argumentation process.",
                "Figure 6 shows the result of that experiment for the two data sets.",
                "Figure 6 contains three plots, where NL (not learning) shows accuracy of an agent with no learning at all; L (learning), shows the evolution of the individual classification accuracy when agents learn by retaining the training cases they individually receive (notice that when all the training cases have been retained at 100%, the accuracy should be equal to that of Figure 5 for individual agents); and finally LFC (<br>learning from communication</br>) shows the evolution of the individual classification accuracy of learning agents that also learn by retaining those counterexamples received during argumentation (i.e. they learn both from training examples and counterexamples).",
                "Figure 6 shows that if an agent Ai learns also from communication, Ai can significantly improve its individual performance with just a small number of additional cases (those selected as relevant counterexamples for Ai during argumentation).",
                "For instance, in the soybean data set, individual agents have achieved an accuracy of 70.62% when they also learn from communication versus an accuracy of 59.93% when they only learn from their individual experience.",
                "The number of cases learnt from communication depends on the properties of the data set: in the sponges data set, agents have retained only very few additional cases, and significantly improved individual accuracy; namely they retain 59.96 cases in average (compared to the 50.4 cases retained if they do not learn from communication).",
                "In the soybean data set more counterexamples are learnt to significantly improve individual accuracy, namely they retain 87.16 cases in average (compared to 55.27 cases retained if they do not learn from communication).",
                "Finally, the fact that both data sets show a significant improvement points out the adaptive nature of the argumentation-based approach to <br>learning from communication</br>: the useful cases are selected as counterexamples (and no more than those needed), and they have the intended effect. 10.",
                "RELATED WORK Concerning CBR in a multi-agent setting, the first research was on negotiated case retrieval [11] among groups of agents.",
                "Our work on multi-agent case-based learning started in 1999 [6]; later Mc Ginty and Smyth [7] presented a multi-agent collaborative CBR approach (CCBR) for planning.",
                "Finally, another interesting approach is multi-case-base reasoning (MCBR) [5], that deals with The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 981 SPONGE 60 65 70 75 80 85 25% 40% 55% 70% 85% 100% LFC L NL SOYBEAN 20 30 40 50 60 70 80 90 25% 40% 55% 70% 85% 100% LFC L NL Figure 6: <br>learning from communication</br> resulting from argumentation in a system composed of 5 agents. distributed systems where there are several case bases available for the same task and addresses the problems of cross-case base adaptation.",
                "The main difference is that our MAC approach is a way to distribute the Reuse process of CBR (using a voting system) while Retrieve is performed individually by each agent; the other multiagent CBR approaches, however, focus on distributing the Retrieve process.",
                "Research on MAS argumentation focus on several issues like a) logics, protocols and languages that support argumentation, b) argument selection and c) argument interpretation.",
                "Approaches for logic and languages that support argumentation include defeasible logic [4] and BDI models [13].",
                "Although argument selection is a key aspect of automated argumentation (see [12] and [13]), most research has been focused on preference relations among arguments.",
                "In our framework we have addressed both argument selection and preference relations using a case-based approach. 11.",
                "CONCLUSIONS AND FUTURE WORK In this paper we have presented an argumentation-based framework for multi-agent learning.",
                "Specifically, we have presented AMAL, a framework that allows a group of learning agents to argue about the solution of a given problem and we have shown how the learning capabilities can be used to generate arguments and counterarguments.",
                "The experimental evaluation shows that the increased amount of information provided to the agents by the argumentation process increases their predictive accuracy, and specially when an adequate number of agents take part in the argumentation.",
                "The main contributions of this work are: a) an argumentation framework for learning agents; b) a case-based preference relation over arguments, based on computing an overall confidence estimation of arguments; c) a case-based policy to generate counterarguments and select counterexamples; and d) an argumentation-based approach for <br>learning from communication</br>.",
                "Finally, in the experiments presented here a learning agent would retain all counterexamples submitted by the other agent; however, this is a very simple case retention policy, and we will like to experiment with more informed policies - with the goal that individual learning agents could significantly improve using only a small set of cases proposed by other agents.",
                "Finally, our approach is focused on lazy learning, and future works aims at incorporating eager inductive learning inside the argumentative framework for <br>learning from communication</br>. 12.",
                "REFERENCES [1] Agnar Aamodt and Enric Plaza.",
                "Case-based reasoning: Foundational issues, methodological variations, and system approaches.",
                "Artificial Intelligence Communications, 7(1):39-59, 1994. [2] E. Armengol and E. Plaza.",
                "Lazy induction of descriptions for relational case-based learning.",
                "In ECML2001, pages 13-24, 2001. [3] Gerhard Brewka.",
                "Dynamic argument systems: A formal model of argumentation processes based on situation calculus.",
                "Journal of Logic and Computation, 11(2):257-282, 2001. [4] Carlos I. Chesñevar and Guillermo R. Simari.",
                "Formalizing Defeasible Argumentation using Labelled Deductive Systems.",
                "Journal of Computer Science & Technology, 1(4):18-33, 2000. [5] D. Leake and R. Sooriamurthi.",
                "Automatically selecting strategies for multi-case-base reasoning.",
                "In S. Craw and A. Preece, editors, ECCBR2002, pages 204-219, Berlin, 2002.",
                "Springer Verlag. [6] Francisco J. Martín, Enric Plaza, and Josep-Lluis Arcos.",
                "Knowledge and experience reuse through communications among competent (peer) agents.",
                "International Journal of Software Engineering and Knowledge Engineering, 9(3):319-341, 1999. [7] Lorraine McGinty and Barry Smyth.",
                "Collaborative case-based reasoning: Applications in personalized route planning.",
                "In I. Watson and Q. Yang, editors, ICCBR, number 2080 in LNAI, pages 362-376.",
                "Springer-Verlag, 2001. [8] Santi Ontañón and Enric Plaza.",
                "Justification-based multiagent learning.",
                "In ICML2003, pages 576-583.",
                "Morgan Kaufmann, 2003. [9] Enric Plaza, Eva Armengol, and Santiago Ontañón.",
                "The explanatory power of symbolic similarity in case-based reasoning.",
                "Artificial Intelligence Review, 24(2):145-161, 2005. [10] David Poole.",
                "On the comparison of theories: Preferring the most specific explanation.",
                "In IJCAI-85, pages 144-147, 1985. [11] M V Nagendra Prassad, Victor R Lesser, and Susan Lander.",
                "Retrieval and reasoning in distributed case bases.",
                "Technical report, UMass Computer Science Department, 1995. [12] K. Sycara S. Kraus and A. Evenchik.",
                "Reaching agreements through argumentation: a logical model and implementation.",
                "Artificial Intelligence Journal, 104:1-69, 1998. [13] N. R. Jennings S. Parsons, C. Sierra.",
                "Agents that reason and negotiate by arguing.",
                "Journal of Logic and Computation, 8:261-292, 1998. [14] Bruce A. Wooley.",
                "Explanation component of software systems.",
                "ACM CrossRoads, 5.1, 1998. 982 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "Learning and Joint Deliberation through Argumentation in Multi-Agent Systems Santi Ontañón CCL, Cognitive Computing Lab Georgia Institute of Technology Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Artificial Intelligence Research Institute CSIC, Spanish Council for Scientific Research Campus UAB, 08193 Bellaterra, Catalonia (Spain) enric@iiia.csic.es ABSTRACT In this paper we will present an argumentation framework for learning agents (AMAL) designed for two purposes: (1) for joint deliberation, and (2) for <br>learning from communication</br>.",
                "For <br>learning from communication</br>, an agent engages into arguing with other agents in order to contrast its individual hypotheses and receive counterexamples; the argumentation process improves their learning scope and individual performance.",
                "In this paper we will present an argumentation framework for learning agents, and show that it can be used for two purposes: (1) joint deliberation, and (2) <br>learning from communication</br>.",
                "In this paper we will propose AMAL, an argumentation framework for learning agents, and will also also show how AMAL can be used both for <br>learning from communication</br> and for solving problems in a collaborative way: • Agents can solve problems in a collaborative way via engaging an argumentation process about the prediction for the situation at hand.",
                "The experiments are designed to test two hypotheses: (H1) that argumentation is a useful framework for joint deliberation and can improve over other typical methods such as voting; and (H2) that <br>learning from communication</br> improves the individual performance of a learning agent participating in an argumentation process."
            ],
            "translated_annotated_samples": [
                "Aprendizaje y deliberación conjunta a través de la argumentación en sistemas multiagente. Santi Ontañón CCL, Laboratorio de Computación Cognitiva, Instituto de Tecnología de Georgia, Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Instituto de Investigación en Inteligencia Artificial, CSIC, Consejo Superior de Investigaciones Científicas, Campus UAB, 08193 Bellaterra, Cataluña (España) enric@iiia.csic.es RESUMEN En este artículo presentaremos un marco de argumentación para agentes de aprendizaje (AMAL) diseñado con dos propósitos: (1) para deliberación conjunta, y (2) para <br>aprender a través de la comunicación</br>.",
                "Para <br>aprender a través de la comunicación</br>, un agente se involucra en discusiones con otros agentes para contrastar sus hipótesis individuales y recibir contraejemplos; el proceso de argumentación mejora su alcance de aprendizaje y rendimiento individual.",
                "En este artículo presentaremos un marco de argumentación para agentes de aprendizaje, y mostraremos que puede ser utilizado para dos propósitos: (1) deliberación conjunta, y (2) <br>aprendizaje a partir de la comunicación</br>.",
                "En este artículo propondremos AMAL, un marco de argumentación para agentes de aprendizaje, y también mostraremos cómo AMAL puede ser utilizado tanto para <br>aprender a través de la comunicación</br> como para resolver problemas de manera colaborativa: • Los agentes pueden resolver problemas de manera colaborativa al participar en un proceso de argumentación sobre la predicción para la situación actual.",
                "Los experimentos están diseñados para probar dos hipótesis: (H1) que la argumentación es un marco útil para la deliberación conjunta y puede mejorar sobre otros métodos típicos como la votación; y (H2) que el <br>aprendizaje a través de la comunicación</br> mejora el rendimiento individual de un agente de aprendizaje que participa en un proceso de argumentación."
            ],
            "translated_text": "Aprendizaje y deliberación conjunta a través de la argumentación en sistemas multiagente. Santi Ontañón CCL, Laboratorio de Computación Cognitiva, Instituto de Tecnología de Georgia, Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Instituto de Investigación en Inteligencia Artificial, CSIC, Consejo Superior de Investigaciones Científicas, Campus UAB, 08193 Bellaterra, Cataluña (España) enric@iiia.csic.es RESUMEN En este artículo presentaremos un marco de argumentación para agentes de aprendizaje (AMAL) diseñado con dos propósitos: (1) para deliberación conjunta, y (2) para <br>aprender a través de la comunicación</br>. El marco AMAL se basa completamente en el aprendizaje a partir de ejemplos: la relación de preferencia de argumentos, la política de generación de argumentos y la política de generación de contraargumentos son técnicas basadas en casos. Para unirse en la deliberación, los agentes de aprendizaje comparten su experiencia formando un comité para decidir sobre alguna decisión conjunta. Experimentalmente demostramos que la argumentación entre comités de agentes mejora tanto el rendimiento individual como el conjunto. Para <br>aprender a través de la comunicación</br>, un agente se involucra en discusiones con otros agentes para contrastar sus hipótesis individuales y recibir contraejemplos; el proceso de argumentación mejora su alcance de aprendizaje y rendimiento individual. Categorías y Descriptores de Asignaturas I.2.6 [Inteligencia Artificial]: Aprendizaje; I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente, Agentes Inteligentes 1. Los marcos de argumentación para sistemas multiagente pueden ser utilizados para diferentes propósitos como la deliberación conjunta, la persuasión, la negociación y la resolución de conflictos. En este artículo presentaremos un marco de argumentación para agentes de aprendizaje, y mostraremos que puede ser utilizado para dos propósitos: (1) deliberación conjunta, y (2) <br>aprendizaje a partir de la comunicación</br>. La deliberación conjunta basada en argumentos implica discutir sobre el resultado de una situación particular o la acción apropiada a tomar en una situación específica. Los agentes de aprendizaje son capaces de aprender de la experiencia, en el sentido de que ejemplos pasados (situaciones y sus resultados) se utilizan para predecir el resultado para la situación actual. Sin embargo, dado que la experiencia de los agentes individuales puede ser limitada, el conocimiento individual y la precisión de la predicción también son limitados. Por lo tanto, los agentes de aprendizaje que son capaces de argumentar sus predicciones individuales con otros agentes pueden alcanzar una mayor precisión en las predicciones después de dicho proceso de argumentación. La mayoría de los marcos de argumentación existentes para sistemas multiagente se basan en lógica deductiva u otra formalización de lógica deductiva diseñada específicamente para respaldar la argumentación, como la lógica por defecto [3]. Por lo general, un argumento se ve como una declaración lógica, mientras que un contraargumento es un argumento ofrecido en oposición a otro argumento; los agentes utilizan una relación de preferencia para resolver argumentos conflictivos. Sin embargo, los marcos de argumentación basados en lógica asumen agentes con conocimiento predefinido y relación de preferencia. En este artículo, nos enfocamos en un marco de Aprendizaje Multiagente basado en Argumentación (AMAL) donde tanto el conocimiento como la relación de preferencia se aprenden a partir de la experiencia. Por lo tanto, consideramos un escenario con agentes que (1) trabajan en el mismo dominio utilizando una ontología compartida, (2) son capaces de aprender a partir de ejemplos y (3) se comunican utilizando un marco argumentativo. Tener capacidades de aprendizaje permite a los agentes utilizar de manera efectiva una forma específica de contraargumento, a saber, el uso de contraejemplos. Los contraejemplos ofrecen la posibilidad de que los agentes aprendan durante el proceso de argumentación. Además, los agentes de aprendizaje permiten técnicas que utilizan la experiencia aprendida para generar argumentos y contraargumentos adecuados. Específicamente, necesitaremos abordar dos problemas: (1) cómo definir una técnica para generar argumentos y contraargumentos a partir de ejemplos, y (2) cómo definir una relación de preferencia sobre dos argumentos en conflicto que han sido inducidos a partir de ejemplos. Este documento presenta un enfoque basado en casos para abordar ambas cuestiones. Los agentes utilizan el razonamiento basado en casos (CBR) [1] para aprender de casos pasados (donde un caso es una situación y su resultado) con el fin de predecir el resultado de una nueva situación. Proponemos un protocolo de argumentación dentro del marco de AMAL que apoya a los agentes en llegar a una predicción conjunta sobre una situación o problema específico; además, el razonamiento necesario para respaldar el proceso de argumentación también se basará en casos. En particular, presentamos dos medidas basadas en casos, una para generar los argumentos y contraargumentos adecuados para una situación particular y otra para determinar la relación de preferencia entre los argumentos. Finalmente, evaluamos (1) si la argumentación entre agentes de aprendizaje puede producir una predicción conjunta que mejore el rendimiento de aprendizaje individual y (2) si el aprendizaje a partir de los contraejemplos transmitidos durante el proceso de argumentación aumenta el rendimiento individual con precisamente aquellos casos que se utilizan mientras discuten entre ellos. El documento está estructurado de la siguiente manera. La sección 2 discute la relación entre la argumentación, la colaboración y el aprendizaje. Luego, la Sección 3 introduce nuestro marco de trabajo de CBR multiagente (MAC) y el concepto de predicción justificada. Después de eso, la Sección 4 define formalmente nuestro marco de argumentación. Las secciones 5 y 6 presentan nuestra relación de preferencia basada en casos y las políticas de generación de argumentos, respectivamente. Más adelante, la Sección 7 presenta el protocolo de argumentación en nuestro marco AMAL. Después de eso, la Sección 8 presenta una ejemplificación del marco de argumentación. Finalmente, la Sección 9 presenta una evaluación empírica de nuestras dos hipótesis principales. El artículo concluye con secciones de trabajo relacionado y conclusiones. 2. ARGUMENTACIÓN, COLABORACIÓN Y APRENDIZAJE Tanto el aprendizaje como la colaboración son formas en las que un agente puede mejorar el rendimiento individual. De hecho, existe un claro paralelismo entre el aprendizaje y la colaboración en sistemas multiagentes, ya que ambos son formas en las que los agentes pueden lidiar con sus deficiencias. Mostremos cuáles son las principales motivaciones que un agente puede tener para aprender o colaborar. • Motivaciones para aprender: - Aumentar la calidad de la predicción, - Aumentar la eficiencia, - Ampliar el rango de problemas resolubles. • Motivaciones para colaborar: - Aumentar la calidad de la predicción, - Aumentar la eficiencia, - Ampliar el rango de problemas resolubles, - Ampliar el rango de recursos accesibles. Al observar las listas de motivación anteriores, podemos ver fácilmente que el aprendizaje y la colaboración están muy relacionados en los sistemas multiagente. De hecho, con la excepción del último elemento en la lista de motivaciones para colaborar, son dos extremos de un continuo de estrategias para mejorar el rendimiento. Un agente puede optar por aumentar el rendimiento mediante el aprendizaje, la colaboración o encontrando un punto intermedio que combine el aprendizaje y la colaboración para mejorar el rendimiento. En este artículo propondremos AMAL, un marco de argumentación para agentes de aprendizaje, y también mostraremos cómo AMAL puede ser utilizado tanto para <br>aprender a través de la comunicación</br> como para resolver problemas de manera colaborativa: • Los agentes pueden resolver problemas de manera colaborativa al participar en un proceso de argumentación sobre la predicción para la situación actual. Utilizando esta colaboración, la predicción puede realizarse de una manera más informada, ya que se ha tenido en cuenta la información conocida por varios agentes. Los agentes también pueden aprender de la comunicación con otros agentes al participar en un proceso de argumentación. Los agentes que participan en estos procesos de argumentación pueden aprender de los argumentos y contraejemplos recibidos de otros agentes, y utilizar esta información para predecir los resultados de situaciones futuras. En el resto de este documento propondremos un marco de argumentación y mostraremos cómo puede ser utilizado tanto para el aprendizaje como para la resolución de problemas de manera colaborativa. SISTEMAS CBR MULTIAGENTE Un Sistema de Razonamiento Basado en Casos Multiagente (MAC) M = {(A1, C1), ..., (An, Cn)} es un sistema multiagente compuesto por A = {Ai, ..., An}, un conjunto de agentes CBR, donde cada agente Ai ∈ A posee una base de casos individual Ci. Cada agente individual Ai en un MAC es completamente autónomo y cada agente Ai solo tiene acceso a su base de casos individual y privada Ci. Un conjunto de casos Ci = {c1, ..., cm} es una colección de casos. Los agentes en un sistema MAC pueden resolver problemas de forma individual, pero también pueden colaborar con otros agentes para resolver problemas. En este marco, nos limitaremos a tareas analíticas, es decir, tareas como la clasificación, donde la solución de un problema se logra seleccionando una clase de solución de un conjunto enumerado de clases de solución. En lo siguiente, notaremos el conjunto de todas las clases de soluciones por S = {S1, ..., SK}. Por lo tanto, un caso c = P, S es una tupla que contiene una descripción del caso P y una clase de solución S ∈ S. En lo siguiente, utilizaremos indistintamente los términos problema y descripción del caso. Además, utilizaremos la notación de punto para referirnos a elementos dentro de una tupla; por ejemplo, para referirnos a la clase de solución de un caso c, escribiremos c.S. Por lo tanto, decimos que un grupo de agentes realiza una deliberación conjunta cuando colaboran para encontrar una solución conjunta mediante un proceso de argumentación. Sin embargo, para hacerlo, un agente debe ser capaz de justificar su predicción a los otros agentes (es decir, generar un argumento para su solución predicha que pueda ser examinado y criticado por los otros agentes). La siguiente sección aborda este tema. 3.1 Predicciones Justificadas Tanto los sistemas expertos como los sistemas CBR pueden tener un componente de explicación [14] encargado de justificar por qué el sistema ha proporcionado una respuesta específica al usuario. La línea de razonamiento del sistema puede ser examinada por un experto humano, aumentando así la fiabilidad del sistema. La mayoría de los trabajos existentes sobre generación de explicaciones se centran en generar explicaciones para ser proporcionadas al usuario. Sin embargo, en nuestro enfoque utilizamos explicaciones (o justificaciones) como una herramienta para mejorar la comunicación y coordinación entre agentes. Estamos interesados en las justificaciones ya que pueden ser utilizadas como argumentos. Para ese propósito, nos beneficiaremos de la capacidad de algunos métodos de aprendizaje automático para proporcionar justificaciones. Una justificación construida por un método CBR después de determinar que la solución de un problema particular P fue Sk es una descripción que contiene la información relevante del problema P que el método CBR ha considerado para predecir Sk como la solución de P. En particular, los métodos CBR funcionan recuperando casos similares al problema en cuestión, y luego reutilizando sus soluciones para el problema actual, esperando que dado que el problema y los casos son similares, las soluciones también serán similares. Por lo tanto, si un método CBR ha recuperado un conjunto de casos C1, ..., Cn para resolver un problema particular P, la justificación construida contendrá la información relevante del problema P que hizo que el sistema CBR recuperara ese conjunto particular de casos, es decir, contendrá la información relevante que P y C1, ..., Cn tienen en común. Por ejemplo, la Figura 1 muestra una justificación construida por un sistema CBR para un problema de juguete (en las siguientes secciones mostraremos justificaciones para problemas reales). En la figura, un problema tiene dos atributos (Semáforo y Coches_pasando), el mecanismo de recuperación del sistema CBR nota que al considerar solo el atributo Semáforo, puede recuperar dos casos que predicen la misma solución: esperar. Por lo tanto, dado que solo se ha utilizado este atributo, es el único que aparece en la justificación. Los valores del resto de atributos son irrelevantes, ya que, sea cual sea su valor, la clase de solución habría sido la misma. 976 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Problema Semáforo: rojo Pasan coches: no Caso 1 Semáforo: rojo Pasan coches: no Solución: esperar Caso 3 Semáforo: rojo Pasan coches: sí Solución: esperar Caso 4 Semáforo: verde Pasan coches: sí Solución: esperar Caso 2 Semáforo: verde Pasan coches: no Solución: cruzar Casos recuperados Solución: esperar Justificación Semáforo: rojo Figura 1: Un ejemplo de generación de justificación en un sistema de RCB. Observa que, dado que la única característica relevante para decidir es Traffic_light (la única utilizada para recuperar casos), es la única que aparece en la justificación. En general, el significado de una justificación es que todos (o la mayoría) de los casos en la base de casos de un agente que satisfacen la justificación (es decir, todos los casos que están subsumidos por la justificación) pertenecen a la clase de solución predicha. En el resto del documento, usaremos para denotar la relación de subsumpción. En nuestro trabajo, utilizamos LID [2], un método CBR capaz de construir justificaciones simbólicas como la ejemplificada en la Figura 1. Cuando un agente proporciona una justificación para una predicción, el agente genera una predicción justificada: DEFINICIÓN 3.1. Una Predicción Justificada es una tupla J = A, P, S, D donde el agente A considera que S es la solución correcta para el problema P, y esa predicción está justificada por una descripción simbólica D tal que J.D J.P. Las justificaciones pueden tener muchos usos para los sistemas de RCB [8, 9]. En este documento, vamos a utilizar justificaciones como argumentos, con el fin de permitir que los agentes de aprendizaje participen en procesos de argumentación. 4. ARGUMENTOS Y CONTRAARGUMENTOS Para nuestros propósitos, un argumento α generado por un agente A está compuesto por una afirmación S y alguna evidencia D que respalda a S como correcta. En el resto de esta sección veremos cómo esta definición general de argumento puede ser concretada en un tipo específico de argumentos que los agentes pueden generar. En el contexto de los sistemas MAC, los agentes discuten sobre predicciones para nuevos problemas y pueden proporcionar dos tipos de información: a) casos específicos P, S, y b) predicciones justificadas: A, P, S, D. Usando esta información, podemos definir tres tipos de argumentos: predicciones justificadas, contraargumentos y contraejemplos. Una predicción justificada α es generada por un agente Ai para argumentar que Ai cree que la solución correcta para un problema dado P es α.S, y la evidencia proporcionada es la justificación α.D. En el ejemplo representado en la Figura 1, un agente Ai puede generar el argumento α = Ai, P, Esperar, (Semáforo = rojo), lo que significa que el agente Ai cree que la solución correcta para P es Esperar porque el atributo Semáforo es igual a rojo. Un contraargumento β es un argumento ofrecido en oposición a otro argumento α. En nuestro marco de trabajo, un contraargumento consiste en una predicción justificada Aj, P, S, D generada por un agente Aj con la intención de refutar un argumento α generado por otro agente Ai, que respalda una clase de solución S diferente a la de α.S para el problema en cuestión y justifica esto con una justificación D. En el ejemplo de la Figura 1, si un agente genera el argumento α = Ai, P, Caminar, (Pasando_autos = no), un agente que piensa que la solución correcta es Esperar podría responder con el contraargumento β = Aj, P, Esperar, (Pasando_autos = no ∧ Semáforo = rojo), lo que significa que, aunque no hay autos pasando, el semáforo está en rojo y la calle no se puede cruzar. Un contraejemplo c es un caso que contradice un argumento α. Por lo tanto, un contraejemplo también es un contraargumento, uno que afirma que un argumento específico α no siempre es verdadero, y la evidencia proporcionada es el caso c. Específicamente, para que un caso c sea un contraejemplo de un argumento α, se deben cumplir las siguientes condiciones: α.D c y α.S = c.S, es decir, el caso debe satisfacer la justificación α.D y la solución de c debe ser diferente a la predicha por α. Al intercambiar argumentos y contraargumentos (incluyendo contraejemplos), los agentes pueden discutir sobre la solución correcta de un problema dado, es decir, pueden participar en un proceso de deliberación conjunto. Sin embargo, para hacerlo, necesitan un protocolo de interacción específico, una relación de preferencia entre argumentos contradictorios y una política de decisión para generar contraargumentos (incluidos contraejemplos). En las siguientes secciones presentaremos estos elementos. 5. RELACIÓN DE PREFERENCIA Un argumento específico proporcionado por un agente podría no ser consistente con la información conocida por otros agentes (o incluso con parte de la información conocida por el agente que ha generado la justificación debido al ruido en los datos de entrenamiento). Por esa razón, vamos a definir una relación de preferencia sobre predicciones justificadas contradictorias basadas en casos. Básicamente, definiremos una medida de confianza para cada predicción justificada (que tenga en cuenta los casos poseídos por cada agente), y la predicción justificada con la mayor confianza será la preferida. La idea detrás de la confianza basada en casos es contar cuántos de los casos en una base de casos individual respaldan una predicción justificada, y cuántos de ellos son ejemplos en contra de ella. Cuanto más casos de respaldo, mayor es la confianza; y cuanto más contrajemplos, menor es la confianza. Específicamente, para evaluar la confianza de una predicción justificada α, un agente obtiene el conjunto de casos en su base de casos individuales que están subsumidos por α.D. Con ellos, un agente Ai obtiene los valores Y (sí) y N (no): • Y Ai α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| es el número de casos en la base de casos del agente subsumidos por la justificación α.D que pertenecen a la clase de solución α.S, • NAi α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| es el número de casos en la base de casos del agente subsumidos por la justificación α.D que no pertenecen a esa clase de solución. El Sexto Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 977 + + + + + +  - - + Figura 2: La confianza de los argumentos se evalúa contrastándolos con las bases de casos de los agentes. Un agente estima la confianza de un argumento como: CAi (α) = Y Ai α 1 + Y Ai α + NAi α es decir, la confianza en una predicción justificada es el número de casos que la respaldan dividido por el número de casos que la respaldan más los contraejemplos. Observa que sumamos 1 al denominador, esto es para evitar otorgar confianzas excesivamente altas a predicciones justificadas cuya confianza se ha calculado utilizando un número pequeño de casos. Ten en cuenta que esta corrección sigue la misma idea que la corrección de Laplace para estimar probabilidades. La Figura 2 ilustra la evaluación individual de la confianza de un argumento, en particular, se encuentran tres casos de respaldo y un contraejemplo en la base de casos de los agentes Ai, lo que da una confianza estimada de 0.6. Además, también podemos definir la confianza conjunta de un argumento α como la confianza calculada utilizando los casos presentes en las bases de casos de todos los agentes en el grupo: C(α) = Π Ai α / (1 + Σ Ai α + NAi α). Nótese que, para calcular conjuntamente la confianza conjunta, los agentes solo tienen que hacer públicos los valores de aprobación y rechazo calculados localmente para un argumento dado. En nuestro marco de trabajo, los agentes utilizan esta confianza conjunta como la relación de preferencia: una predicción justificada α se prefiere sobre otra β si C(α) ≥ C(β). GENERACIÓN DE ARGUMENTOS En nuestro marco de trabajo, los argumentos son generados por los agentes a partir de casos, utilizando métodos de aprendizaje. Cualquier método de aprendizaje capaz de proporcionar una predicción justificada puede ser utilizado para generar argumentos. Por ejemplo, los árboles de decisión y LID [2] son métodos de aprendizaje adecuados. Específicamente, en los experimentos reportados en este artículo, los agentes utilizan LID. Por lo tanto, cuando un agente desea generar un argumento que respalde que una clase de solución específica es la solución correcta para un problema P, genera una predicción justificada como se explica en la Sección 3.1. Por ejemplo, la Figura 3 muestra una justificación real generada por LID después de resolver un problema P en el dominio de la identificación de esponjas marinas. En particular, la Figura 3 muestra cómo cuando un agente recibe un nuevo problema para resolver (en este caso, un nuevo esponja para determinar su orden), el agente utiliza LID para generar un argumento (que consiste en una predicción justificada) utilizando los casos en la base de casos del agente. La justificación mostrada en la Figura 3 se puede interpretar diciendo que la solución predicha es hadromerida porque la forma lisa de las megascleras del esqueleto espicular de la esponja es de tipo tilostilo, el esqueleto espicular de la esponja no tiene una longitud uniforme y no hay gemas en las características externas de la esponja. Por lo tanto, el argumento generado será α = A1, P, hadromerida, D1. 6.1 Generación de Contraargumentos Como se mencionó anteriormente, los agentes pueden intentar refutar argumentos generando contraargumentos o encontrando contraejemplos. Permítanos explicar cómo pueden ser generados. Un agente Ai quiere generar un contraargumento β para rebatir un argumento α cuando α está en contradicción con la base de casos locales de Ai. Además, al generar dicho contraargumento β, Ai espera que β sea preferido sobre α. Con ese propósito, presentaremos una política específica para generar contraargumentos basados en el criterio de especificidad [10]. El criterio de especificidad se utiliza ampliamente en marcos deductivos para la argumentación, y establece que entre dos argumentos en conflicto, se debe preferir el más específico ya que, en principio, está más informado. Por lo tanto, se espera que los contraargumentos generados basados en el criterio de especificidad sean preferibles (ya que están más informados) que los argumentos que intentan refutar. Sin embargo, no hay garantía de que tales contraargumentos siempre ganen, ya que, como hemos indicado en la Sección 5, los agentes en nuestro marco utilizan una relación de preferencia basada en la confianza conjunta. Además, uno podría pensar que sería mejor que los agentes generaran contraargumentos basados en la relación de preferencia de confianza conjunta; sin embargo, no es obvio cómo generar contraargumentos basados en la confianza conjunta de manera eficiente, ya que se requiere colaboración para evaluar la confianza conjunta. Por lo tanto, el agente que genera el contraargumento debe comunicarse constantemente con los otros agentes en cada paso del algoritmo de inducción utilizado para generar contraargumentos (actualmente una de nuestras líneas de investigación futuras). Por lo tanto, en nuestro marco de trabajo, cuando un agente quiere generar un contraargumento β a un argumento α, β tiene que ser más específico que α (es decir, α.D < β.D). La generación de contraargumentos utilizando el criterio de especificidad impone algunas restricciones sobre el método de aprendizaje, aunque LID o ID3 pueden adaptarse fácilmente para esta tarea. Por ejemplo, LID es un algoritmo que genera una descripción desde cero y añade heurísticamente características a ese término. Por lo tanto, en cada paso, la descripción se vuelve más específica que en el paso anterior, y el número de casos que se incluyen en esa descripción se reduce. Cuando la descripción cubre solo (o casi solo) casos de una única clase de solución, LID termina y predice esa clase de solución. Para generar un contraargumento a un argumento α, LID solo tiene que usar como punto de partida la descripción α.D en lugar de empezar desde cero. De esta manera, la justificación proporcionada por LID siempre estará subsumida por α.D, y por lo tanto, el contraargumento resultante será más específico que α. Sin embargo, hay que tener en cuenta que LID a veces puede no ser capaz de generar contraargumentos, ya que LID puede no ser capaz de especializar aún más la descripción α.D, o porque el agente Ai no tiene ningún caso en Ci que esté subsumido por α.D. La Figura 4 muestra cómo un agente A2 que no estaba de acuerdo con el argumento mostrado en la Figura 3, genera un contraargumento utilizando LID. Además, la Figura 4 muestra la generación de un contraargumento β1 2 para el argumento α0 1 (en la Figura 3) que es una especialización de α0 1. 978 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Solución: hadromerida Justificación: D1 Esponja Esqueleto espiculado Características externas Características externas Gemmulas: no Esqueleto espiculado Megascleres Longitud uniforme: no Megascleres Forma lisa: tilostilo Caso Base de A1 LID Nueva esponja P Figura 3: Ejemplo de una justificación real generada por LID en el conjunto de datos de esponjas marinas. Específicamente, en nuestros experimentos, cuando un agente Ai quiere refutar un argumento α, utiliza la siguiente política: 1. El Agente Ai utiliza LID para intentar encontrar un contraargumento β más específico que α; si se encuentra, β se envía al otro agente como un contraargumento de α. 2. Si no se encuentra, entonces Ai busca un contraejemplo c ∈ Ci de α. Si se encuentra un caso c, entonces c se envía al otro agente como un contraejemplo de α. Si no se encuentran contraejemplos, entonces Ai no puede refutar el argumento α. 7. APRENDIZAJE MULTIAGENTE BASADO EN ARGUMENTACIÓN El protocolo de interacción de AMAL permite que un grupo de agentes A1, ..., An deliberen sobre la solución correcta de un problema P mediante un proceso de argumentación. Si el proceso de argumentación llega a una solución consensuada, la deliberación conjunta termina; de lo contrario, se utiliza una votación ponderada para determinar la solución conjunta. Además, AMAL también permite a los agentes aprender de los contraejemplos recibidos de otros agentes. El protocolo AMAL consiste en una serie de rondas. En la ronda inicial, cada agente declara cuál es su predicción individual para P. Luego, en cada ronda, un agente puede intentar refutar la predicción hecha por cualquiera de los otros agentes. El protocolo utiliza un mecanismo de paso de token para que los agentes (uno a la vez) puedan enviar contraargumentos o contraejemplos si no están de acuerdo con la predicción hecha por cualquier otro agente. Específicamente, a cada agente se le permite enviar un contraargumento o contraejemplo cada vez que recibe el token (nota que esta restricción es solo para simplificar el protocolo, y que no restringe la cantidad de contraargumentos que un agente puede enviar, ya que pueden ser retrasados para rondas posteriores). Cuando un agente recibe un contraargumento o contraejemplo, informa a los otros agentes si acepta el contraargumento (y cambia su predicción) o no. Además, los agentes también tienen la oportunidad de responder a los contraargumentos cuando reciben el token, intentando generar un contraargumento al contraargumento. Cuando todos los agentes hayan tenido el token una vez, el token regresa al primer agente, y así sucesivamente. Si en algún momento del protocolo, todos los agentes están de acuerdo o durante las últimas n rondas ningún agente ha generado ninguna contra-argumentación, el protocolo termina. Además, si al final de la argumentación los agentes no han llegado a un acuerdo, entonces se utiliza un mecanismo de votación que utiliza la confianza de cada predicción como pesos para decidir la solución final (Así, AMAL sigue el mismo mecanismo que los comités humanos, primero cada miembro individual de un comité expone sus argumentos y discute los de los otros miembros (deliberación conjunta), y si no se alcanza un consenso, entonces se requiere un mecanismo de votación). En cada iteración, los agentes pueden usar los siguientes performativos: • afirmar(α): la predicción justificada que se mantendrá durante la próxima ronda será α. Un agente solo puede tener una única predicción en cada ronda, por lo tanto, si se envían múltiples afirmaciones, solo se considera la última como la predicción actualmente mantenida. • rebut(β, α): el agente ha encontrado un contraargumento β a la predicción α. Definiremos Ht = αt 1, ..., αt n como las predicciones que cada uno de los n agentes tiene en una ronda t. Además, también definiremos contradict(αt i) = {α ∈ Ht|α.S = αt i.S} como el conjunto de argumentos contradictorios para un agente Ai en una ronda t, es decir, el conjunto de argumentos en la ronda t que respaldan una clase de solución diferente a la de αt i. El protocolo se inicia porque uno de los agentes recibe un problema P que debe ser resuelto. Después de eso, el agente informa a todos los demás agentes sobre el problema P a resolver, y el protocolo comienza: 1. En la ronda t = 0, cada uno de los agentes resuelve individualmente P y construye una predicción justificada utilizando su propio método de RBC. Entonces, cada agente Ai envía el assertivo performativo assert(α0 i) a los otros agentes. Por lo tanto, los agentes conocen H0 = α0 i , ..., α0 n. Una vez que todas las predicciones hayan sido enviadas, el token se le da al primer agente A1. 2. En cada ronda t (distinta de 0), los agentes verifican si sus argumentos en Ht coinciden. Si lo hacen, el protocolo avanza al paso 5. Además, si durante las últimas n rondas ningún agente ha enviado ningún contraejemplo o contraargumento, el protocolo también avanza al paso 5. De lo contrario, el agente Ai propietario del token intenta generar un contraargumento para cada uno de los argumentos opuestos en contradict(αt i) ⊆ Ht (ver Sección 6.1). Entonces, se selecciona el contraargumento βt i contra la predicción αt j con la menor confianza C(αt j) (ya que αt j es la predicción más probable de ser refutada con éxito). • Si βt i es un contraargumento, entonces, Ai compara localmente αt i con βt i evaluando su confianza contra su base de casos individuales Ci (ver Sección 5) (nótese que Ai está comparando su argumento previo con el contraargumento que Ai mismo acaba de generar y que trata sobre The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 979 Esponja Esqueleto espiculado Características externas Características externas Gemas: no Crecimiento: Esqueleto espiculado Megascleres Longitud uniforme: no Megascleres Forma lisa: tilostilo Crecimiento Crecimiento: masivo Caso Base de A2 LID Solución: astrophorida Justificación: D2 Figura 4: Generación de un contraargumento utilizando LID en el conjunto de datos de esponjas. para enviar a Aj. Si CAi (βt i) > CAi (αt i), entonces Ai considera que βt i es más fuerte que su argumento anterior, cambia su argumento a βt i enviando assert(βt i) al resto de los agentes (la intuición detrás de esto es que dado que un contraargumento también es un argumento, Ai verifica si el nuevo contraargumento es un mejor argumento que el que sostenía anteriormente) y rebut(βt i, αt j) a Aj. De lo contrario (es decir, Si CAi (βt i ) ≤ CAi (αt i), Ai solo enviará rebut(βt i , αt j) a Aj. En cualquiera de las dos situaciones, el protocolo avanza al paso 3. • Si βt i es un contraejemplo c, entonces Ai envía una refutación(c, αt j) a Aj. El protocolo avanza al paso 4. • Si Ai no puede generar ningún contraargumento o contraejemplo, el token se envía al siguiente agente, comienza una nueva ronda t + 1, y el protocolo avanza al estado 2. 3. El agente Aj que ha recibido el contraargumento βt i, lo compara localmente con su propio argumento, αt j, evaluando localmente su confianza. Si CAj (βt i ) > CAj (αt j), entonces Aj aceptará el contraargumento como más fuerte que su propio argumento, y enviará assert(βt i ) a los otros agentes. De lo contrario (es decir, Si CAj (βt i ) ≤ CAj (αt j)), Aj no aceptará el contraargumento e informará a los otros agentes en consecuencia. Cualquiera de las dos situaciones inicia una nueva ronda t + 1, Ai envía el token al siguiente agente y el protocolo vuelve al estado 2. 4. El agente Aj que ha recibido el contraejemplo c lo retiene en su base de casos y genera un nuevo argumento αt+1 j que tiene en cuenta c, e informa al resto de los agentes enviando assert(αt+1 j) a todos ellos. Entonces, Ai envía el token al siguiente agente, comienza una nueva ronda t + 1 y el protocolo vuelve al paso 2. 5. El protocolo finaliza generando una predicción conjunta, de la siguiente manera: si los argumentos en Ht están de acuerdo, entonces su predicción es la predicción conjunta; de lo contrario, se utiliza un mecanismo de votación para decidir la predicción conjunta. El mecanismo de votación utiliza la medida de confianza conjunta como los pesos de votación, de la siguiente manera: S = arg max Sk∈S αi∈Ht|αi.S=Sk C(αi) Además, para evitar iteraciones infinitas, si un agente envía dos veces el mismo argumento o contraargumento al mismo agente, el mensaje no se considera. 8. EJEMPLIFICACIÓN Consideremos un sistema compuesto por tres agentes A1, A2 y A3. Uno de los agentes, A1, recibe un problema P para resolver y decide usar AMAL para resolverlo. Por esa razón, invita a A2 y A3 a participar en el proceso de argumentación. Aceptan la invitación y comienza el protocolo de argumentación. Inicialmente, cada agente genera su predicción individual para P y la transmite a los otros agentes. Por lo tanto, todos ellos pueden calcular H0 = α0 1, α0 2, α0 3. En particular, en este ejemplo: • α0 1 = A1, P, hadromerida, D1 • α0 2 = A2, P, astrophorida, D2 • α0 3 = A3, P, axinellida, D3 A1 comienza poseyendo el token e intenta generar argumentos en contra de α0 2 y α0 3, pero no tiene éxito; sin embargo, tiene un contraejemplo c13 para α0 3. Por lo tanto, A1 envía el mensaje rebut(c13, α03) a A3. A3 incorpora c13 en su base de casos e intenta resolver el problema P nuevamente, ahora teniendo en cuenta c13. A3 elabora la predicción justificada α1 3 = A3, P, hadromerida, D4, y la transmite al resto de los agentes con el mensaje assert(α1 3). Por lo tanto, todos ellos conocen el nuevo H1 = α0 1, α0 2, α1 3. La ronda 1 comienza y A2 obtiene el token. A2 intenta generar contraargumentos para α0 1 y α1 3 y solo logra generar un contraargumento β1 2 = A2, P, astrophorida, D5 contra α1 3. El contraargumento se envía a A3 con el mensaje rebut(β1 2 , α1 3). El agente A3 recibe el contraargumento y evalúa su confianza local. El resultado es que la confianza individual del contraargumento β1 2 es menor que la confianza local de α1 3. Por lo tanto, A3 no acepta el contraargumento, y así H2 = α0 1, α0 2, α1 3. La ronda 2 comienza y A3 recibe el token. A3 genera un contraargumento β2 3 = A3, P, hadromerida, D6 para α0 2 y lo envía a A2 con el mensaje refutar(β2 3 , α0 2). El agente A2 recibe el contraargumento y evalúa su confianza local. El resultado es que la confianza local del contraargumento β2 3 es mayor que la confianza local de α0 2. Por lo tanto, A2 acepta el contraargumento e informa al resto de los agentes con el mensaje assert(β2 3). Después de eso, H3 = α0 1, β2 3 , α1 3. En la Ronda 3, dado que todos los agentes están de acuerdo (todas las predicciones justificadas en H3 predicen hadromerida como la clase de solución), el protocolo finaliza, y A1 (el agente que recibió el problema) considera hadromerida como la solución conjunta para el problema P. 9. EVALUACIÓN EXPERIMENTAL 980 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) SPONGE 75 77 79 81 83 85 87 89 91 2 3 4 5 AMAL Votación Individual SOYBEAN 55 60 65 70 75 80 85 90 2 3 4 5 AMAL Votación Individual Figura 5: Precisión individual y conjunta para 2 a 5 agentes. En esta sección evaluamos empíricamente el marco de argumentación AMAL. Hemos realizado experimentos en dos conjuntos de datos diferentes: soja (del repositorio de aprendizaje automático de UCI) y esponja (un conjunto de datos relacional). El conjunto de datos de soja tiene 307 ejemplos y 19 clases de solución, mientras que el conjunto de datos de esponja tiene 280 ejemplos y 3 clases de solución. En una ejecución experimental, el conjunto de datos se divide en 2 conjuntos: el conjunto de entrenamiento y el conjunto de prueba. Los ejemplos del conjunto de entrenamiento están distribuidos entre 5 agentes diferentes sin replicación, es decir, no hay ningún ejemplo compartido por dos agentes. En la etapa de pruebas, los problemas en el conjunto de pruebas llegan aleatoriamente a uno de los agentes, y su objetivo es predecir la solución correcta. Los experimentos están diseñados para probar dos hipótesis: (H1) que la argumentación es un marco útil para la deliberación conjunta y puede mejorar sobre otros métodos típicos como la votación; y (H2) que el <br>aprendizaje a través de la comunicación</br> mejora el rendimiento individual de un agente de aprendizaje que participa en un proceso de argumentación. ",
            "candidates": [],
            "error": [
                [
                    "aprender a través de la comunicación",
                    "aprender a través de la comunicación",
                    "aprendizaje a partir de la comunicación",
                    "aprender a través de la comunicación",
                    "aprendizaje a través de la comunicación"
                ]
            ]
        },
        "joint deliberation": {
            "translated_key": "deliberación conjunta",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning and <br>joint deliberation</br> through Argumentation in Multi-Agent Systems Santi Ontañón CCL, Cognitive Computing Lab Georgia Institute of Technology Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Artificial Intelligence Research Institute CSIC, Spanish Council for Scientific Research Campus UAB, 08193 Bellaterra, Catalonia (Spain) enric@iiia.csic.es ABSTRACT In this paper we will present an argumentation framework for learning agents (AMAL) designed for two purposes: (1) for <br>joint deliberation</br>, and (2) for learning from communication.",
                "The AMAL framework is completely based on learning from examples: the argument preference relation, the argument generation policy, and the counterargument generation policy are case-based techniques.",
                "For join deliberation, learning agents share their experience by forming a committee to decide upon some joint decision.",
                "We experimentally show that the argumentation among committees of agents improves both the individual and joint performance.",
                "For learning from communication, an agent engages into arguing with other agents in order to contrast its individual hypotheses and receive counterexamples; the argumentation process improves their learning scope and individual performance.",
                "Categories and Subject Descriptors I.2.6 [Artificial Intelligence]: Learning; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems, Intelligent Agents 1.",
                "INTRODUCTION Argumentation frameworks for multi-agent systems can be used for different purposes like <br>joint deliberation</br>, persuasion, negotiation, and conflict resolution.",
                "In this paper we will present an argumentation framework for learning agents, and show that it can be used for two purposes: (1) <br>joint deliberation</br>, and (2) learning from communication.",
                "Argumentation-based <br>joint deliberation</br> involves discussion over the outcome of a particular situation or the appropriate course of action for a particular situation.",
                "Learning agents are capable of learning from experience, in the sense that past examples (situations and their outcomes) are used to predict the outcome for the situation at hand.",
                "However, since individual agents experience may be limited, individual knowledge and prediction accuracy is also limited.",
                "Thus, learning agents that are capable of arguing their individual predictions with other agents may reach better prediction accuracy after such an argumentation process.",
                "Most existing argumentation frameworks for multi-agent systems are based on deductive logic or some other deductive logic formalism specifically designed to support argumentation, such as default logic [3]).",
                "Usually, an argument is seen as a logical statement, while a counterargument is an argument offered in opposition to another argument [4, 13]; agents use a preference relation to resolve conflicting arguments.",
                "However, logic-based argumentation frameworks assume agents with preloaded knowledge and preference relation.",
                "In this paper, we focus on an Argumentation-based Multi-Agent Learning (AMAL) framework where both knowledge and preference relation are learned from experience.",
                "Thus, we consider a scenario with agents that (1) work in the same domain using a shared ontology, (2) are capable of learning from examples, and (3) communicate using an argumentative framework.",
                "Having learning capabilities allows agents effectively use a specific form of counterargument, namely the use of counterexamples.",
                "Counterexamples offer the possibility of agents learning during the argumentation process.",
                "Moreover, learning agents allow techniques that use learnt experience to generate adequate arguments and counterarguments.",
                "Specifically, we will need to address two issues: (1) how to define a technique to generate arguments and counterarguments from examples, and (2)how to define a preference relation over two conflicting arguments that have been induced from examples.",
                "This paper presents a case-based approach to address both issues.",
                "The agents use case-based reasoning (CBR) [1] to learn from past cases (where a case is a situation and its outcome) in order to predict the outcome of a new situation.",
                "We propose an argumentation protocol inside the AMAL framework at supports agents in reaching a joint prediction over a specific situation or problem - moreover, the reasoning needed to support the argumentation process will also be based on cases.",
                "In particular, we present two case-based measures, one for generating the arguments and counterarguments adequate to a particular situation and another for determining preference relation among arguments.",
                "Finally, we evaluate (1) if argumentation between learning agents can produce a joint prediction that improves over individual learning performance and (2) if learning from the counterexamples conveyed during the argumentation process increases the individual performance with precisely those cases being used while arguing among them.",
                "The paper is structured as follows.",
                "Section 2 discusses the relation among argumentation, collaboration and learning.",
                "Then Section 3 introduces our multi-agent CBR (MAC) framework and the notion of justified prediction.",
                "After that, Section 4 formally defines our argumentation framework.",
                "Sections 5 and 6 present our case-based preference relation and argument generation policies respectively.",
                "Later, Section 7 presents the argumentation protocol in our AMAL framework.",
                "After that, Section 8 presents an exemplification of the argumentation framework.",
                "Finally, Section 9 presents an empirical evaluation of our two main hypotheses.",
                "The paper closes with related work and conclusions sections. 2.",
                "ARGUMENTATION,COLLABORATION AND LEARNING Both learning and collaboration are ways in which an agent can improve individual performance.",
                "In fact, there is a clear parallelism between learning and collaboration in multi-agent systems, since both are ways in which agents can deal with their shortcomings.",
                "Let us show which are the main motivations that an agent can have to learn or to collaborate. • Motivations to learn: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems. • Motivations to collaborate: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems, - Increase the range of accessible resources.",
                "Looking at the above lists of motivation, we can easily see that learning and collaboration are very related in multi-agent systems.",
                "In fact, with the exception of the last item in the motivations to collaborate list, they are two extremes of a continuum of strategies to improve performance.",
                "An agent may choose to increase performance by learning, by collaborating, or by finding an intermediate point that combines learning and collaboration in order to improve performance.",
                "In this paper we will propose AMAL, an argumentation framework for learning agents, and will also also show how AMAL can be used both for learning from communication and for solving problems in a collaborative way: • Agents can solve problems in a collaborative way via engaging an argumentation process about the prediction for the situation at hand.",
                "Using this collaboration, the prediction can be done in a more informed way, since the information known by several agents has been taken into account. • Agents can also learn from communication with other agents by engaging an argumentation process.",
                "Agents that engage in such argumentation processes can learn from the arguments and counterexamples received from other agents, and use this information for predicting the outcomes of future situations.",
                "In the rest of this paper we will propose an argumentation framework and show how it can be used both for learning and for solving problems in a collaborative way. 3.",
                "MULTI-AGENT CBR SYSTEMS A Multi-Agent Case Based Reasoning System (MAC) M = {(A1, C1), ..., (An, Cn)} is a multi-agent system composed of A = {Ai, ..., An}, a set of CBR agents, where each agent Ai ∈ A possesses an individual case base Ci.",
                "Each individual agent Ai in a MAC is completely autonomous and each agent Ai has access only to its individual and private case base Ci.",
                "A case base Ci = {c1, ..., cm} is a collection of cases.",
                "Agents in a MAC system are able to individually solve problems, but they can also collaborate with other agents to solve problems.",
                "In this framework, we will restrict ourselves to analytical tasks, i.e. tasks like classification, where the solution of a problem is achieved by selecting a solution class from an enumerated set of solution classes.",
                "In the following we will note the set of all the solution classes by S = {S1, ..., SK }.",
                "Therefore, a case c = P, S is a tuple containing a case description P and a solution class S ∈ S. In the following, we will use the terms problem and case description indistinctly.",
                "Moreover, we will use the dot notation to refer to elements inside a tuple; e.g., to refer to the solution class of a case c, we will write c.S.",
                "Therefore, we say a group of agents perform <br>joint deliberation</br>, when they collaborate to find a joint solution by means of an argumentation process.",
                "However, in order to do so, an agent has to be able to justify its prediction to the other agents (i.e. generate an argument for its predicted solution that can be examined and critiqued by the other agents).",
                "The next section addresses this issue. 3.1 Justified Predictions Both expert systems and CBR systems may have an explanation component [14] in charge of justifying why the system has provided a specific answer to the user.",
                "The line of reasoning of the system can then be examined by a human expert, thus increasing the reliability of the system.",
                "Most of the existing work on explanation generation focuses on generating explanations to be provided to the user.",
                "However, in our approach we use explanations (or justifications) as a tool for improving communication and coordination among agents.",
                "We are interested in justifications since they can be used as arguments.",
                "For that purpose, we will benefit from the ability of some machine learning methods to provide justifications.",
                "A justification built by a CBR method after determining that the solution of a particular problem P was Sk is a description that contains the relevant information from the problem P that the CBR method has considered to predict Sk as the solution of P. In particular, CBR methods work by retrieving similar cases to the problem at hand, and then reusing their solutions for the current problem, expecting that since the problem and the cases are similar, the solutions will also be similar.",
                "Thus, if a CBR method has retrieved a set of cases C1, ..., Cn to solve a particular problem P the justification built will contain the relevant information from the problem P that made the CBR system retrieve that particular set of cases, i.e. it will contain the relevant information that P and C1, ..., Cn have in common.",
                "For example, Figure 1 shows a justification build by a CBR system for a toy problem (in the following sections we will show justifications for real problems).",
                "In the figure, a problem has two attributes (Traffic_light, and Cars_passing), the retrieval mechanism of the CBR system notices that by considering only the attribute Traffic_light, it can retrieve two cases that predict the same solution: wait.",
                "Thus, since only this attribute has been used, it is the only one appearing in the justification.",
                "The values of the rest of attributes are irrelevant, since whatever their value the solution class would have been the same. 976 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Problem Traffic_light: red Cars_passing: no Case 1 Traffic_light: red Cars_passing: no Solution: wait Case 3 Traffic_light: red Cars_passing: yes Solution: wait Case 4 Traffic_light: green Cars_passing: yes Solution: wait Case 2 Traffic_light: green Cars_passing: no Solution: cross Retrieved cases Solution: wait Justification Traffic_light: red Figure 1: An example of justification generation in a CBR system.",
                "Notice that, since the only relevant feature to decide is Traffic_light (the only one used to retrieve cases), it is the only one appearing in the justification.",
                "In general, the meaning of a justification is that all (or most of) the cases in the case base of an agent that satisfy the justification (i.e. all the cases that are subsumed by the justification) belong to the predicted solution class.",
                "In the rest of the paper, we will use to denote the subsumption relation.",
                "In our work, we use LID [2], a CBR method capable of building symbolic justifications such as the one exemplified in Figure 1.",
                "When an agent provides a justification for a prediction, the agent generates a justified prediction: DEFINITION 3.1.",
                "A Justified Prediction is a tuple J = A, P, S, D where agent A considers S the correct solution for problem P, and that prediction is justified a symbolic description D such that J.D J.P. Justifications can have many uses for CBR systems [8, 9].",
                "In this paper, we are going to use justifications as arguments, in order to allow learning agents to engage in argumentation processes. 4.",
                "ARGUMENTS AND COUNTERARGUMENTS For our purposes an argument α generated by an agent A is composed of a statement S and some evidence D supporting S as correct.",
                "In the remainder of this section we will see how this general definition of argument can be instantiated in specific kind of arguments that the agents can generate.",
                "In the context of MAC systems, agents argue about predictions for new problems and can provide two kinds of information: a) specific cases P, S , and b) justified predictions: A, P, S, D .",
                "Using this information, we can define three types of arguments: justified predictions, counterarguments, and counterexamples.",
                "A justified prediction α is generated by an agent Ai to argue that Ai believes that the correct solution for a given problem P is α.S, and the evidence provided is the justification α.D.",
                "In the example depicted in Figure 1, an agent Ai may generate the argument α = Ai, P, Wait, (Traffic_light = red) , meaning that the agent Ai believes that the correct solution for P is Wait because the attribute Traffic_light equals red.",
                "A counterargument β is an argument offered in opposition to another argument α.",
                "In our framework, a counterargument consists of a justified prediction Aj, P, S , D generated by an agent Aj with the intention to rebut an argument α generated by another agent Ai, that endorses a solution class S different from that of α.S for the problem at hand and justifies this with a justification D .",
                "In the example in Figure 1, if an agent generates the argument α = Ai, P, Walk, (Cars_passing = no) , an agent that thinks that the correct solution is Wait might answer with the counterargument β = Aj, P, Wait, (Cars_passing = no ∧ Traffic_light = red) , meaning that, although there are no cars passing, the traffic light is red, and the street cannot be crossed.",
                "A counterexample c is a case that contradicts an argument α.",
                "Thus a counterexample is also a counterargument, one that states that a specific argument α is not always true, and the evidence provided is the case c. Specifically, for a case c to be a counterexample of an argument α, the following conditions have to be met: α.D c and α.S = c.S, i.e. the case must satisfy the justification α.D and the solution of c must be different than the predicted by α.",
                "By exchanging arguments and counterarguments (including counterexamples), agents can argue about the correct solution of a given problem, i.e. they can engage a <br>joint deliberation</br> process.",
                "However, in order to do so, they need a specific interaction protocol, a preference relation between contradicting arguments, and a decision policy to generate counterarguments (including counterexamples).",
                "In the following sections we will present these elements. 5.",
                "PREFERENCE RELATION A specific argument provided by an agent might not be consistent with the information known to other agents (or even to some of the information known by the agent that has generated the justification due to noise in training data).",
                "For that reason, we are going to define a preference relation over contradicting justified predictions based on cases.",
                "Basically, we will define a confidence measure for each justified prediction (that takes into account the cases owned by each agent), and the justified prediction with the highest confidence will be the preferred one.",
                "The idea behind case-based confidence is to count how many of the cases in an individual case base endorse a justified prediction, and how many of them are counterexamples of it.",
                "The more the endorsing cases, the higher the confidence; and the more the counterexamples, the lower the confidence.",
                "Specifically, to assess the confidence of a justified prediction α, an agent obtains the set of cases in its individual case base that are subsumed by α.D.",
                "With them, an agent Ai obtains the Y (aye) and N (nay) values: • Y Ai α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by the justification α.D that belong to the solution class α.S, • NAi α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by justification α.D that do not belong to that solution class.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 977 + + + + + +  - - + Figure 2: Confidence of arguments is evaluated by contrasting them against the case bases of the agents.",
                "An agent estimates the confidence of an argument as: CAi (α) = Y Ai α 1 + Y Ai α + NAi α i.e. the confidence on a justified prediction is the number of endorsing cases divided by the number of endorsing cases plus counterexamples.",
                "Notice that we add 1 to the denominator, this is to avoid giving excessively high confidences to justified predictions whose confidence has been computed using a small number of cases.",
                "Notice that this correction follows the same idea than the Laplace correction to estimate probabilities.",
                "Figure 2 illustrates the individual evaluation of the confidence of an argument, in particular, three endorsing cases and one counterexample are found in the case base of agents Ai, giving an estimated confidence of 0.6 Moreover, we can also define the joint confidence of an argument α as the confidence computed using the cases present in the case bases of all the agents in the group: C(α) = i Y Ai α 1 + i Y Ai α + NAi α Notice that, to collaboratively compute the joint confidence, the agents only have to make public the aye and nay values locally computed for a given argument.",
                "In our framework, agents use this joint confidence as the preference relation: a justified prediction α is preferred over another one β if C(α) ≥ C(β). 6.",
                "GENERATION OF ARGUMENTS In our framework, arguments are generated by the agents from cases, using learning methods.",
                "Any learning method able to provide a justified prediction can be used to generate arguments.",
                "For instance, decision trees and LID [2] are suitable learning methods.",
                "Specifically, in the experiments reported in this paper agents use LID.",
                "Thus, when an agent wants to generate an argument endorsing that a specific solution class is the correct solution for a problem P, it generates a justified prediction as explained in Section 3.1.",
                "For instance, Figure 3 shows a real justification generated by LID after solving a problem P in the domain of marine sponges identification.",
                "In particular, Figure 3 shows how when an agent receives a new problem to solve (in this case, a new sponge to determine its order), the agent uses LID to generate an argument (consisting on a justified prediction) using the cases in the case base of the agent.",
                "The justification shown in Figure 3 can be interpreted saying that the predicted solution is hadromerida because the smooth form of the megascleres of the spiculate skeleton of the sponge is of type tylostyle, the spikulate skeleton of the sponge has no uniform length, and there is no gemmules in the external features of the sponge.",
                "Thus, the argument generated will be α = A1, P, hadromerida, D1 . 6.1 Generation of Counterarguments As previously stated, agents may try to rebut arguments by generating counterargument or by finding counterexamples.",
                "Let us explain how they can be generated.",
                "An agent Ai wants to generate a counterargument β to rebut an argument α when α is in contradiction with the local case base of Ai.",
                "Moreover, while generating such counterargument β, Ai expects that β is preferred over α.",
                "For that purpose, we will present a specific policy to generate counterarguments based on the specificity criterion [10].",
                "The specificity criterion is widely used in deductive frameworks for argumentation, and states that between two conflicting arguments, the most specific should be preferred since it is, in principle, more informed.",
                "Thus, counterarguments generated based on the specificity criterion are expected to be preferable (since they are more informed) to the arguments they try to rebut.",
                "However, there is no guarantee that such counterarguments will always win, since, as we have stated in Section 5, agents in our framework use a preference relation based on joint confidence.",
                "Moreover, one may think that it would be better that the agents generate counterarguments based on the joint confidence preference relation; however it is not obvious how to generate counterarguments based on joint confidence in an efficient way, since collaboration is required in order to evaluate joint confidence.",
                "Thus, the agent generating the counterargument should constantly communicate with the other agents at each step of the induction algorithm used to generate counterarguments (presently one of our future research lines).",
                "Thus, in our framework, when an agent wants to generate a counterargument β to an argument α, β has to be more specific than α (i.e. α.D < β.D).",
                "The generation of counterarguments using the specificity criterion imposes some restrictions over the learning method, although LID or ID3 can be easily adapted for this task.",
                "For instance, LID is an algorithm that generates a description starting from scratch and heuristically adding features to that term.",
                "Thus, at every step, the description is made more specific than in the previous step, and the number of cases that are subsumed by that description is reduced.",
                "When the description covers only (or almost only) cases of a single solution class LID terminates and predicts that solution class.",
                "To generate a counterargument to an argument α LID just has to use as starting point the description α.D instead of starting from scratch.",
                "In this way, the justification provided by LID will always be subsumed by α.D, and thus the resulting counterargument will be more specific than α.",
                "However, notice that LID may sometimes not be able to generate counterarguments, since LID may not be able to specialize the description α.D any further, or because the agent Ai has no case inCi that is subsumed by α.D.",
                "Figure 4 shows how an agent A2 that disagreed with the argument shown in Figure 3, generates a counterargument using LID.",
                "Moreover, Figure 4 shows the generation of a counterargument β1 2 for the argument α0 1 (in Figure 3) that is a specialization of α0 1. 978 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Solution: hadromerida Justification: D1 Sponge Spikulate skeleton External features External features Gemmules: no Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Case Base of A1 LID New sponge P Figure 3: Example of a real justification generated by LID in the marine sponges data set.",
                "Specifically, in our experiments, when an agent Ai wants to rebut an argument α, uses the following policy: 1.",
                "Agent Ai uses LID to try to find a counterargument β more specific than α; if found, β is sent to the other agent as a counterargument of α. 2.",
                "If not found, then Ai searches for a counterexample c ∈ Ci of α.",
                "If a case c is found, then c is sent to the other agent as a counterexample of α. 3.",
                "If no counterexamples are found, then Ai cannot rebut the argument α. 7.",
                "ARGUMENTATION-BASED MULTI-AGENT LEARNING The interaction protocol of AMAL allows a group of agents A1, ..., An to deliberate about the correct solution of a problem P by means of an argumentation process.",
                "If the argumentation process arrives to a consensual solution, the <br>joint deliberation</br> ends; otherwise a weighted vote is used to determine the joint solution.",
                "Moreover, AMAL also allows the agents to learn from the counterexamples received from other agents.",
                "The AMAL protocol consists on a series of rounds.",
                "In the initial round, each agent states which is its individual prediction for P. Then, at each round an agent can try to rebut the prediction made by any of the other agents.",
                "The protocol uses a token passing mechanism so that agents (one at a time) can send counterarguments or counterexamples if they disagree with the prediction made by any other agent.",
                "Specifically, each agent is allowed to send one counterargument or counterexample each time he gets the token (notice that this restriction is just to simplify the protocol, and that it does not restrict the number of counterargument an agent can sent, since they can be delayed for subsequent rounds).",
                "When an agent receives a counterargument or counterexample, it informs the other agents if it accepts the counterargument (and changes its prediction) or not.",
                "Moreover, agents have also the opportunity to answer to counterarguments when they receive the token, by trying to generate a counterargument to the counterargument.",
                "When all the agents have had the token once, the token returns to the first agent, and so on.",
                "If at any time in the protocol, all the agents agree or during the last n rounds no agent has generated any counterargument, the protocol ends.",
                "Moreover, if at the end of the argumentation the agents have not reached an agreement, then a voting mechanism that uses the confidence of each prediction as weights is used to decide the final solution (Thus, AMAL follows the same mechanism as human committees, first each individual member of a committee exposes his arguments and discuses those of the other members (<br>joint deliberation</br>), and if no consensus is reached, then a voting mechanism is required).",
                "At each iteration, agents can use the following performatives: • assert(α): the justified prediction held during the next round will be α.",
                "An agent can only hold a single prediction at each round, thus is multiple asserts are send, only the last one is considered as the currently held prediction. • rebut(β, α): the agent has found a counterargument β to the prediction α.",
                "We will define Ht = αt 1, ..., αt n as the predictions that each of the n agents hold at a round t. Moreover, we will also define contradict(αt i) = {α ∈ Ht|α.S = αt i.S} as the set of contradicting arguments for an agent Ai in a round t, i.e. the set of arguments at round t that support a different solution class than αt i.",
                "The protocol is initiated because one of the agents receives a problem P to be solved.",
                "After that, the agent informs all the other agents about the problem P to solve, and the protocol starts: 1.",
                "At round t = 0, each one of the agents individually solves P, and builds a justified prediction using its own CBR method.",
                "Then, each agent Ai sends the performative assert(α0 i ) to the other agents.",
                "Thus, the agents know H0 = α0 i , ..., α0 n .",
                "Once all the predictions have been sent the token is given to the first agent A1. 2.",
                "At each round t (other than 0), the agents check whether their arguments in Ht agree.",
                "If they do, the protocol moves to step 5.",
                "Moreover, if during the last n rounds no agent has sent any counterexample or counterargument, the protocol also moves to step 5.",
                "Otherwise, the agent Ai owner of the token tries to generate a counterargument for each of the opposing arguments in contradict(αt i) ⊆ Ht (see Section 6.1).",
                "Then, the counterargument βt i against the prediction αt j with the lowest confidence C(αt j) is selected (since αt j is the prediction more likely to be successfully rebutted). • If βt i is a counterargument, then, Ai locally compares αt i with βt i by assessing their confidence against its individual case base Ci (see Section 5) (notice that Ai is comparing its previous argument with the counterargument that Ai itself has just generated and that is about The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 979 Sponge Spikulate skeleton External features External features Gemmules: no Growing: Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Growing Grow: massive Case Base of A2 LID Solution: astrophorida Justification: D2 Figure 4: Generation of a counterargument using LID in the sponges data set. to send to Aj).",
                "If CAi (βt i ) > CAi (αt i), then Ai considers that βt i is stronger than its previous argument, changes its argument to βt i by sending assert(βt i ) to the rest of the agents (the intuition behind this is that since a counterargument is also an argument, Ai checks if the newly counterargument is a better argument than the one he was previously holding) and rebut(βt i , αt j) to Aj.",
                "Otherwise (i.e.",
                "CAi (βt i ) ≤ CAi (αt i)), Ai will send only rebut(βt i , αt j) to Aj.",
                "In any of the two situations the protocol moves to step 3. • If βt i is a counterexample c, then Ai sends rebut(c, αt j) to Aj.",
                "The protocol moves to step 4. • If Ai cannot generate any counterargument or counterexample, the token is sent to the next agent, a new round t + 1 starts, and the protocol moves to state 2. 3.",
                "The agent Aj that has received the counterargument βt i , locally compares it against its own argument, αt j, by locally assessing their confidence.",
                "If CAj (βt i ) > CAj (αt j), then Aj will accept the counterargument as stronger than its own argument, and it will send assert(βt i ) to the other agents.",
                "Otherwise (i.e.",
                "CAj (βt i ) ≤ CAj (αt j)), Aj will not accept the counterargument, and will inform the other agents accordingly.",
                "Any of the two situations start a new round t + 1, Ai sends the token to the next agent, and the protocol moves back to state 2. 4.",
                "The agent Aj that has received the counterexample c retains it into its case base and generates a new argument αt+1 j that takes into account c, and informs the rest of the agents by sending assert(αt+1 j ) to all of them.",
                "Then, Ai sends the token to the next agent, a new round t + 1 starts, and the protocol moves back to step 2. 5.",
                "The protocol ends yielding a joint prediction, as follows: if the arguments in Ht agree then their prediction is the joint prediction, otherwise a voting mechanism is used to decide the joint prediction.",
                "The voting mechanism uses the joint confidence measure as the voting weights, as follows: S = arg max Sk∈S αi∈Ht|αi.S=Sk C(αi) Moreover, in order to avoid infinite iterations, if an agent sends twice the same argument or counterargument to the same agent, the message is not considered. 8.",
                "EXEMPLIFICATION Let us consider a system composed of three agents A1, A2 and A3.",
                "One of the agents, A1 receives a problem P to solve, and decides to use AMAL to solve it.",
                "For that reason, invites A2 and A3 to take part in the argumentation process.",
                "They accept the invitation, and the argumentation protocol starts.",
                "Initially, each agent generates its individual prediction for P, and broadcasts it to the other agents.",
                "Thus, all of them can compute H0 = α0 1, α0 2, α0 3 .",
                "In particular, in this example: • α0 1 = A1, P, hadromerida, D1 • α0 2 = A2, P, astrophorida, D2 • α0 3 = A3, P, axinellida, D3 A1 starts owning the token and tries to generate counterarguments for α0 2 and α0 3, but does not succeed, however it has one counterexample c13 for α0 3.",
                "Thus, A1 sends the the message rebut( c13, α0 3) to A3.",
                "A3 incorporates c13 into its case base and tries to solve the problem P again, now taking c13 into consideration.",
                "A3 comes up with the justified prediction α1 3 = A3, P, hadromerida, D4 , and broadcasts it to the rest of the agents with the message assert(α1 3).",
                "Thus, all of them know the new H1 = α0 1, α0 2, α1 3 .",
                "Round 1 starts and A2 gets the token.",
                "A2 tries to generate counterarguments for α0 1 and α1 3 and only succeeds to generate a counterargument β1 2 = A2, P, astrophorida, D5 against α1 3.",
                "The counterargument is sent to A3 with the message rebut(β1 2 , α1 3).",
                "Agent A3 receives the counterargument and assesses its local confidence.",
                "The result is that the individual confidence of the counterargument β1 2 is lower than the local confidence of α1 3.",
                "Therefore, A3 does not accept the counterargument, and thus H2 = α0 1, α0 2, α1 3 .",
                "Round 2 starts and A3 gets the token.",
                "A3 generates a counterargument β2 3 = A3, P, hadromerida, D6 for α0 2 and sends it to A2 with the message rebut(β2 3 , α0 2).",
                "Agent A2 receives the counterargument and assesses its local confidence.",
                "The result is that the local confidence of the counterargument β2 3 is higher than the local confidence of α0 2.",
                "Therefore, A2 accepts the counterargument and informs the rest of the agents with the message assert(β2 3 ).",
                "After that, H3 = α0 1, β2 3 , α1 3 .",
                "At Round 3, since all the agents agree (all the justified predictions in H3 predict hadromerida as the solution class) The protocol ends, and A1 (the agent that received the problem) considers hadromerida as the joint solution for the problem P. 9.",
                "EXPERIMENTAL EVALUATION 980 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) SPONGE 75 77 79 81 83 85 87 89 91 2 3 4 5 AMAL Voting Individual SOYBEAN 55 60 65 70 75 80 85 90 2 3 4 5 AMAL Voting Individual Figure 5: Individual and joint accuracy for 2 to 5 agents.",
                "In this section we empirically evaluate the AMAL argumentation framework.",
                "We have made experiments in two different data sets: soybean (from the UCI machine learning repository) and sponge (a relational data set).",
                "The soybean data set has 307 examples and 19 solution classes, while the sponge data set has 280 examples and 3 solution classes.",
                "In an experimental run, the data set is divided in 2 sets: the training set and the test set.",
                "The training set examples are distributed among 5 different agents without replication, i.e. there is no example shared by two agents.",
                "In the testing stage, problems in the test set arrive randomly to one of the agents, and their goal is to predict the correct solution.",
                "The experiments are designed to test two hypotheses: (H1) that argumentation is a useful framework for <br>joint deliberation</br> and can improve over other typical methods such as voting; and (H2) that learning from communication improves the individual performance of a learning agent participating in an argumentation process.",
                "Moreover, we also expect that the improvement achieved from argumentation will increase as the number of agents participating in the argumentation increases (since more information will be taken into account).",
                "Concerning H1 (argumentation is a useful framework for <br>joint deliberation</br>), we ran 4 experiments, using 2, 3, 4, and 5 agents respectively (in all experiments each agent has a 20% of the training data, since the training is always distributed among 5 agents).",
                "Figure 5 shows the result of those experiments in the sponge and soybean data sets.",
                "Classification accuracy is plotted in the vertical axis, and in the horizontal axis the number of agents that took part in the argumentation processes is shown.",
                "For each number of agents, three bars are shown: individual, Voting, and AMAL.",
                "The individual bar shows the average accuracy of individual agents predictions; the voting bar shows the average accuracy of the joint prediction achieved by voting but without any argumentation; and finally the AMAL bar shows the average accuracy of the joint prediction using argumentation.",
                "The results shown are the average of 5 10-fold cross validation runs.",
                "Figure 5 shows that collaboration (voting and AMAL) outperforms individual problem solving.",
                "Moreover, as we expected, the accuracy improves as more agents collaborate, since more information is taken into account.",
                "We can also see that AMAL always outperforms standard voting, proving that joint decisions are based on better information as provided by the argumentation process.",
                "For instance, the joint accuracy for 2 agents in the sponge data set is of 87.57% for AMAL and 86.57% for voting (while individual accuracy is just 80.07%).",
                "Moreover, the improvement achieved by AMAL over Voting is even larger in the soybean data set.",
                "The reason is that the soybean data set is more difficult (in the sense that agents need more data to produce good predictions).",
                "These experimental results show that AMAL effectively exploits the opportunity for improvement: the accuracy is higher only because more agents have changed their opinion during argumentation (otherwise they would achieve the same result as Voting).",
                "Concerning H2 (learning from communication in argumentation processes improves individual prediction ), we ran the following experiment: initially, we distributed a 25% of the training set among the five agents; after that, the rest of the cases in the training set is sent to the agents one by one; when an agent receives a new training case, it has several options: the agent can discard it, the agent can retain it, or the agent can use it for engaging an argumentation process.",
                "Figure 6 shows the result of that experiment for the two data sets.",
                "Figure 6 contains three plots, where NL (not learning) shows accuracy of an agent with no learning at all; L (learning), shows the evolution of the individual classification accuracy when agents learn by retaining the training cases they individually receive (notice that when all the training cases have been retained at 100%, the accuracy should be equal to that of Figure 5 for individual agents); and finally LFC (learning from communication) shows the evolution of the individual classification accuracy of learning agents that also learn by retaining those counterexamples received during argumentation (i.e. they learn both from training examples and counterexamples).",
                "Figure 6 shows that if an agent Ai learns also from communication, Ai can significantly improve its individual performance with just a small number of additional cases (those selected as relevant counterexamples for Ai during argumentation).",
                "For instance, in the soybean data set, individual agents have achieved an accuracy of 70.62% when they also learn from communication versus an accuracy of 59.93% when they only learn from their individual experience.",
                "The number of cases learnt from communication depends on the properties of the data set: in the sponges data set, agents have retained only very few additional cases, and significantly improved individual accuracy; namely they retain 59.96 cases in average (compared to the 50.4 cases retained if they do not learn from communication).",
                "In the soybean data set more counterexamples are learnt to significantly improve individual accuracy, namely they retain 87.16 cases in average (compared to 55.27 cases retained if they do not learn from communication).",
                "Finally, the fact that both data sets show a significant improvement points out the adaptive nature of the argumentation-based approach to learning from communication: the useful cases are selected as counterexamples (and no more than those needed), and they have the intended effect. 10.",
                "RELATED WORK Concerning CBR in a multi-agent setting, the first research was on negotiated case retrieval [11] among groups of agents.",
                "Our work on multi-agent case-based learning started in 1999 [6]; later Mc Ginty and Smyth [7] presented a multi-agent collaborative CBR approach (CCBR) for planning.",
                "Finally, another interesting approach is multi-case-base reasoning (MCBR) [5], that deals with The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 981 SPONGE 60 65 70 75 80 85 25% 40% 55% 70% 85% 100% LFC L NL SOYBEAN 20 30 40 50 60 70 80 90 25% 40% 55% 70% 85% 100% LFC L NL Figure 6: Learning from communication resulting from argumentation in a system composed of 5 agents. distributed systems where there are several case bases available for the same task and addresses the problems of cross-case base adaptation.",
                "The main difference is that our MAC approach is a way to distribute the Reuse process of CBR (using a voting system) while Retrieve is performed individually by each agent; the other multiagent CBR approaches, however, focus on distributing the Retrieve process.",
                "Research on MAS argumentation focus on several issues like a) logics, protocols and languages that support argumentation, b) argument selection and c) argument interpretation.",
                "Approaches for logic and languages that support argumentation include defeasible logic [4] and BDI models [13].",
                "Although argument selection is a key aspect of automated argumentation (see [12] and [13]), most research has been focused on preference relations among arguments.",
                "In our framework we have addressed both argument selection and preference relations using a case-based approach. 11.",
                "CONCLUSIONS AND FUTURE WORK In this paper we have presented an argumentation-based framework for multi-agent learning.",
                "Specifically, we have presented AMAL, a framework that allows a group of learning agents to argue about the solution of a given problem and we have shown how the learning capabilities can be used to generate arguments and counterarguments.",
                "The experimental evaluation shows that the increased amount of information provided to the agents by the argumentation process increases their predictive accuracy, and specially when an adequate number of agents take part in the argumentation.",
                "The main contributions of this work are: a) an argumentation framework for learning agents; b) a case-based preference relation over arguments, based on computing an overall confidence estimation of arguments; c) a case-based policy to generate counterarguments and select counterexamples; and d) an argumentation-based approach for learning from communication.",
                "Finally, in the experiments presented here a learning agent would retain all counterexamples submitted by the other agent; however, this is a very simple case retention policy, and we will like to experiment with more informed policies - with the goal that individual learning agents could significantly improve using only a small set of cases proposed by other agents.",
                "Finally, our approach is focused on lazy learning, and future works aims at incorporating eager inductive learning inside the argumentative framework for learning from communication. 12.",
                "REFERENCES [1] Agnar Aamodt and Enric Plaza.",
                "Case-based reasoning: Foundational issues, methodological variations, and system approaches.",
                "Artificial Intelligence Communications, 7(1):39-59, 1994. [2] E. Armengol and E. Plaza.",
                "Lazy induction of descriptions for relational case-based learning.",
                "In ECML2001, pages 13-24, 2001. [3] Gerhard Brewka.",
                "Dynamic argument systems: A formal model of argumentation processes based on situation calculus.",
                "Journal of Logic and Computation, 11(2):257-282, 2001. [4] Carlos I. Chesñevar and Guillermo R. Simari.",
                "Formalizing Defeasible Argumentation using Labelled Deductive Systems.",
                "Journal of Computer Science & Technology, 1(4):18-33, 2000. [5] D. Leake and R. Sooriamurthi.",
                "Automatically selecting strategies for multi-case-base reasoning.",
                "In S. Craw and A. Preece, editors, ECCBR2002, pages 204-219, Berlin, 2002.",
                "Springer Verlag. [6] Francisco J. Martín, Enric Plaza, and Josep-Lluis Arcos.",
                "Knowledge and experience reuse through communications among competent (peer) agents.",
                "International Journal of Software Engineering and Knowledge Engineering, 9(3):319-341, 1999. [7] Lorraine McGinty and Barry Smyth.",
                "Collaborative case-based reasoning: Applications in personalized route planning.",
                "In I. Watson and Q. Yang, editors, ICCBR, number 2080 in LNAI, pages 362-376.",
                "Springer-Verlag, 2001. [8] Santi Ontañón and Enric Plaza.",
                "Justification-based multiagent learning.",
                "In ICML2003, pages 576-583.",
                "Morgan Kaufmann, 2003. [9] Enric Plaza, Eva Armengol, and Santiago Ontañón.",
                "The explanatory power of symbolic similarity in case-based reasoning.",
                "Artificial Intelligence Review, 24(2):145-161, 2005. [10] David Poole.",
                "On the comparison of theories: Preferring the most specific explanation.",
                "In IJCAI-85, pages 144-147, 1985. [11] M V Nagendra Prassad, Victor R Lesser, and Susan Lander.",
                "Retrieval and reasoning in distributed case bases.",
                "Technical report, UMass Computer Science Department, 1995. [12] K. Sycara S. Kraus and A. Evenchik.",
                "Reaching agreements through argumentation: a logical model and implementation.",
                "Artificial Intelligence Journal, 104:1-69, 1998. [13] N. R. Jennings S. Parsons, C. Sierra.",
                "Agents that reason and negotiate by arguing.",
                "Journal of Logic and Computation, 8:261-292, 1998. [14] Bruce A. Wooley.",
                "Explanation component of software systems.",
                "ACM CrossRoads, 5.1, 1998. 982 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "Learning and <br>joint deliberation</br> through Argumentation in Multi-Agent Systems Santi Ontañón CCL, Cognitive Computing Lab Georgia Institute of Technology Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Artificial Intelligence Research Institute CSIC, Spanish Council for Scientific Research Campus UAB, 08193 Bellaterra, Catalonia (Spain) enric@iiia.csic.es ABSTRACT In this paper we will present an argumentation framework for learning agents (AMAL) designed for two purposes: (1) for <br>joint deliberation</br>, and (2) for learning from communication.",
                "INTRODUCTION Argumentation frameworks for multi-agent systems can be used for different purposes like <br>joint deliberation</br>, persuasion, negotiation, and conflict resolution.",
                "In this paper we will present an argumentation framework for learning agents, and show that it can be used for two purposes: (1) <br>joint deliberation</br>, and (2) learning from communication.",
                "Argumentation-based <br>joint deliberation</br> involves discussion over the outcome of a particular situation or the appropriate course of action for a particular situation.",
                "Therefore, we say a group of agents perform <br>joint deliberation</br>, when they collaborate to find a joint solution by means of an argumentation process."
            ],
            "translated_annotated_samples": [
                "Aprendizaje y <br>deliberación conjunta</br> a través de la argumentación en sistemas multiagente. Santi Ontañón CCL, Laboratorio de Computación Cognitiva, Instituto de Tecnología de Georgia, Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Instituto de Investigación en Inteligencia Artificial, CSIC, Consejo Superior de Investigaciones Científicas, Campus UAB, 08193 Bellaterra, Cataluña (España) enric@iiia.csic.es RESUMEN En este artículo presentaremos un marco de argumentación para agentes de aprendizaje (AMAL) diseñado con dos propósitos: (1) para <br>deliberación conjunta</br>, y (2) para aprender a través de la comunicación.",
                "Los marcos de argumentación para sistemas multiagente pueden ser utilizados para diferentes propósitos como la <br>deliberación conjunta</br>, la persuasión, la negociación y la resolución de conflictos.",
                "En este artículo presentaremos un marco de argumentación para agentes de aprendizaje, y mostraremos que puede ser utilizado para dos propósitos: (1) <br>deliberación conjunta</br>, y (2) aprendizaje a partir de la comunicación.",
                "La <br>deliberación conjunta</br> basada en argumentos implica discutir sobre el resultado de una situación particular o la acción apropiada a tomar en una situación específica.",
                "Por lo tanto, decimos que un grupo de agentes realiza una <br>deliberación conjunta</br> cuando colaboran para encontrar una solución conjunta mediante un proceso de argumentación."
            ],
            "translated_text": "Aprendizaje y <br>deliberación conjunta</br> a través de la argumentación en sistemas multiagente. Santi Ontañón CCL, Laboratorio de Computación Cognitiva, Instituto de Tecnología de Georgia, Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Instituto de Investigación en Inteligencia Artificial, CSIC, Consejo Superior de Investigaciones Científicas, Campus UAB, 08193 Bellaterra, Cataluña (España) enric@iiia.csic.es RESUMEN En este artículo presentaremos un marco de argumentación para agentes de aprendizaje (AMAL) diseñado con dos propósitos: (1) para <br>deliberación conjunta</br>, y (2) para aprender a través de la comunicación. El marco AMAL se basa completamente en el aprendizaje a partir de ejemplos: la relación de preferencia de argumentos, la política de generación de argumentos y la política de generación de contraargumentos son técnicas basadas en casos. Para unirse en la deliberación, los agentes de aprendizaje comparten su experiencia formando un comité para decidir sobre alguna decisión conjunta. Experimentalmente demostramos que la argumentación entre comités de agentes mejora tanto el rendimiento individual como el conjunto. Para aprender a través de la comunicación, un agente se involucra en discusiones con otros agentes para contrastar sus hipótesis individuales y recibir contraejemplos; el proceso de argumentación mejora su alcance de aprendizaje y rendimiento individual. Categorías y Descriptores de Asignaturas I.2.6 [Inteligencia Artificial]: Aprendizaje; I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente, Agentes Inteligentes 1. Los marcos de argumentación para sistemas multiagente pueden ser utilizados para diferentes propósitos como la <br>deliberación conjunta</br>, la persuasión, la negociación y la resolución de conflictos. En este artículo presentaremos un marco de argumentación para agentes de aprendizaje, y mostraremos que puede ser utilizado para dos propósitos: (1) <br>deliberación conjunta</br>, y (2) aprendizaje a partir de la comunicación. La <br>deliberación conjunta</br> basada en argumentos implica discutir sobre el resultado de una situación particular o la acción apropiada a tomar en una situación específica. Los agentes de aprendizaje son capaces de aprender de la experiencia, en el sentido de que ejemplos pasados (situaciones y sus resultados) se utilizan para predecir el resultado para la situación actual. Sin embargo, dado que la experiencia de los agentes individuales puede ser limitada, el conocimiento individual y la precisión de la predicción también son limitados. Por lo tanto, los agentes de aprendizaje que son capaces de argumentar sus predicciones individuales con otros agentes pueden alcanzar una mayor precisión en las predicciones después de dicho proceso de argumentación. La mayoría de los marcos de argumentación existentes para sistemas multiagente se basan en lógica deductiva u otra formalización de lógica deductiva diseñada específicamente para respaldar la argumentación, como la lógica por defecto [3]. Por lo general, un argumento se ve como una declaración lógica, mientras que un contraargumento es un argumento ofrecido en oposición a otro argumento; los agentes utilizan una relación de preferencia para resolver argumentos conflictivos. Sin embargo, los marcos de argumentación basados en lógica asumen agentes con conocimiento predefinido y relación de preferencia. En este artículo, nos enfocamos en un marco de Aprendizaje Multiagente basado en Argumentación (AMAL) donde tanto el conocimiento como la relación de preferencia se aprenden a partir de la experiencia. Por lo tanto, consideramos un escenario con agentes que (1) trabajan en el mismo dominio utilizando una ontología compartida, (2) son capaces de aprender a partir de ejemplos y (3) se comunican utilizando un marco argumentativo. Tener capacidades de aprendizaje permite a los agentes utilizar de manera efectiva una forma específica de contraargumento, a saber, el uso de contraejemplos. Los contraejemplos ofrecen la posibilidad de que los agentes aprendan durante el proceso de argumentación. Además, los agentes de aprendizaje permiten técnicas que utilizan la experiencia aprendida para generar argumentos y contraargumentos adecuados. Específicamente, necesitaremos abordar dos problemas: (1) cómo definir una técnica para generar argumentos y contraargumentos a partir de ejemplos, y (2) cómo definir una relación de preferencia sobre dos argumentos en conflicto que han sido inducidos a partir de ejemplos. Este documento presenta un enfoque basado en casos para abordar ambas cuestiones. Los agentes utilizan el razonamiento basado en casos (CBR) [1] para aprender de casos pasados (donde un caso es una situación y su resultado) con el fin de predecir el resultado de una nueva situación. Proponemos un protocolo de argumentación dentro del marco de AMAL que apoya a los agentes en llegar a una predicción conjunta sobre una situación o problema específico; además, el razonamiento necesario para respaldar el proceso de argumentación también se basará en casos. En particular, presentamos dos medidas basadas en casos, una para generar los argumentos y contraargumentos adecuados para una situación particular y otra para determinar la relación de preferencia entre los argumentos. Finalmente, evaluamos (1) si la argumentación entre agentes de aprendizaje puede producir una predicción conjunta que mejore el rendimiento de aprendizaje individual y (2) si el aprendizaje a partir de los contraejemplos transmitidos durante el proceso de argumentación aumenta el rendimiento individual con precisamente aquellos casos que se utilizan mientras discuten entre ellos. El documento está estructurado de la siguiente manera. La sección 2 discute la relación entre la argumentación, la colaboración y el aprendizaje. Luego, la Sección 3 introduce nuestro marco de trabajo de CBR multiagente (MAC) y el concepto de predicción justificada. Después de eso, la Sección 4 define formalmente nuestro marco de argumentación. Las secciones 5 y 6 presentan nuestra relación de preferencia basada en casos y las políticas de generación de argumentos, respectivamente. Más adelante, la Sección 7 presenta el protocolo de argumentación en nuestro marco AMAL. Después de eso, la Sección 8 presenta una ejemplificación del marco de argumentación. Finalmente, la Sección 9 presenta una evaluación empírica de nuestras dos hipótesis principales. El artículo concluye con secciones de trabajo relacionado y conclusiones. 2. ARGUMENTACIÓN, COLABORACIÓN Y APRENDIZAJE Tanto el aprendizaje como la colaboración son formas en las que un agente puede mejorar el rendimiento individual. De hecho, existe un claro paralelismo entre el aprendizaje y la colaboración en sistemas multiagentes, ya que ambos son formas en las que los agentes pueden lidiar con sus deficiencias. Mostremos cuáles son las principales motivaciones que un agente puede tener para aprender o colaborar. • Motivaciones para aprender: - Aumentar la calidad de la predicción, - Aumentar la eficiencia, - Ampliar el rango de problemas resolubles. • Motivaciones para colaborar: - Aumentar la calidad de la predicción, - Aumentar la eficiencia, - Ampliar el rango de problemas resolubles, - Ampliar el rango de recursos accesibles. Al observar las listas de motivación anteriores, podemos ver fácilmente que el aprendizaje y la colaboración están muy relacionados en los sistemas multiagente. De hecho, con la excepción del último elemento en la lista de motivaciones para colaborar, son dos extremos de un continuo de estrategias para mejorar el rendimiento. Un agente puede optar por aumentar el rendimiento mediante el aprendizaje, la colaboración o encontrando un punto intermedio que combine el aprendizaje y la colaboración para mejorar el rendimiento. En este artículo propondremos AMAL, un marco de argumentación para agentes de aprendizaje, y también mostraremos cómo AMAL puede ser utilizado tanto para aprender a través de la comunicación como para resolver problemas de manera colaborativa: • Los agentes pueden resolver problemas de manera colaborativa al participar en un proceso de argumentación sobre la predicción para la situación actual. Utilizando esta colaboración, la predicción puede realizarse de una manera más informada, ya que se ha tenido en cuenta la información conocida por varios agentes. Los agentes también pueden aprender de la comunicación con otros agentes al participar en un proceso de argumentación. Los agentes que participan en estos procesos de argumentación pueden aprender de los argumentos y contraejemplos recibidos de otros agentes, y utilizar esta información para predecir los resultados de situaciones futuras. En el resto de este documento propondremos un marco de argumentación y mostraremos cómo puede ser utilizado tanto para el aprendizaje como para la resolución de problemas de manera colaborativa. SISTEMAS CBR MULTIAGENTE Un Sistema de Razonamiento Basado en Casos Multiagente (MAC) M = {(A1, C1), ..., (An, Cn)} es un sistema multiagente compuesto por A = {Ai, ..., An}, un conjunto de agentes CBR, donde cada agente Ai ∈ A posee una base de casos individual Ci. Cada agente individual Ai en un MAC es completamente autónomo y cada agente Ai solo tiene acceso a su base de casos individual y privada Ci. Un conjunto de casos Ci = {c1, ..., cm} es una colección de casos. Los agentes en un sistema MAC pueden resolver problemas de forma individual, pero también pueden colaborar con otros agentes para resolver problemas. En este marco, nos limitaremos a tareas analíticas, es decir, tareas como la clasificación, donde la solución de un problema se logra seleccionando una clase de solución de un conjunto enumerado de clases de solución. En lo siguiente, notaremos el conjunto de todas las clases de soluciones por S = {S1, ..., SK}. Por lo tanto, un caso c = P, S es una tupla que contiene una descripción del caso P y una clase de solución S ∈ S. En lo siguiente, utilizaremos indistintamente los términos problema y descripción del caso. Además, utilizaremos la notación de punto para referirnos a elementos dentro de una tupla; por ejemplo, para referirnos a la clase de solución de un caso c, escribiremos c.S. Por lo tanto, decimos que un grupo de agentes realiza una <br>deliberación conjunta</br> cuando colaboran para encontrar una solución conjunta mediante un proceso de argumentación. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "argumentation protocol": {
            "translated_key": "protocolo de argumentación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning and Joint Deliberation through Argumentation in Multi-Agent Systems Santi Ontañón CCL, Cognitive Computing Lab Georgia Institute of Technology Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Artificial Intelligence Research Institute CSIC, Spanish Council for Scientific Research Campus UAB, 08193 Bellaterra, Catalonia (Spain) enric@iiia.csic.es ABSTRACT In this paper we will present an argumentation framework for learning agents (AMAL) designed for two purposes: (1) for joint deliberation, and (2) for learning from communication.",
                "The AMAL framework is completely based on learning from examples: the argument preference relation, the argument generation policy, and the counterargument generation policy are case-based techniques.",
                "For join deliberation, learning agents share their experience by forming a committee to decide upon some joint decision.",
                "We experimentally show that the argumentation among committees of agents improves both the individual and joint performance.",
                "For learning from communication, an agent engages into arguing with other agents in order to contrast its individual hypotheses and receive counterexamples; the argumentation process improves their learning scope and individual performance.",
                "Categories and Subject Descriptors I.2.6 [Artificial Intelligence]: Learning; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems, Intelligent Agents 1.",
                "INTRODUCTION Argumentation frameworks for multi-agent systems can be used for different purposes like joint deliberation, persuasion, negotiation, and conflict resolution.",
                "In this paper we will present an argumentation framework for learning agents, and show that it can be used for two purposes: (1) joint deliberation, and (2) learning from communication.",
                "Argumentation-based joint deliberation involves discussion over the outcome of a particular situation or the appropriate course of action for a particular situation.",
                "Learning agents are capable of learning from experience, in the sense that past examples (situations and their outcomes) are used to predict the outcome for the situation at hand.",
                "However, since individual agents experience may be limited, individual knowledge and prediction accuracy is also limited.",
                "Thus, learning agents that are capable of arguing their individual predictions with other agents may reach better prediction accuracy after such an argumentation process.",
                "Most existing argumentation frameworks for multi-agent systems are based on deductive logic or some other deductive logic formalism specifically designed to support argumentation, such as default logic [3]).",
                "Usually, an argument is seen as a logical statement, while a counterargument is an argument offered in opposition to another argument [4, 13]; agents use a preference relation to resolve conflicting arguments.",
                "However, logic-based argumentation frameworks assume agents with preloaded knowledge and preference relation.",
                "In this paper, we focus on an Argumentation-based Multi-Agent Learning (AMAL) framework where both knowledge and preference relation are learned from experience.",
                "Thus, we consider a scenario with agents that (1) work in the same domain using a shared ontology, (2) are capable of learning from examples, and (3) communicate using an argumentative framework.",
                "Having learning capabilities allows agents effectively use a specific form of counterargument, namely the use of counterexamples.",
                "Counterexamples offer the possibility of agents learning during the argumentation process.",
                "Moreover, learning agents allow techniques that use learnt experience to generate adequate arguments and counterarguments.",
                "Specifically, we will need to address two issues: (1) how to define a technique to generate arguments and counterarguments from examples, and (2)how to define a preference relation over two conflicting arguments that have been induced from examples.",
                "This paper presents a case-based approach to address both issues.",
                "The agents use case-based reasoning (CBR) [1] to learn from past cases (where a case is a situation and its outcome) in order to predict the outcome of a new situation.",
                "We propose an <br>argumentation protocol</br> inside the AMAL framework at supports agents in reaching a joint prediction over a specific situation or problem - moreover, the reasoning needed to support the argumentation process will also be based on cases.",
                "In particular, we present two case-based measures, one for generating the arguments and counterarguments adequate to a particular situation and another for determining preference relation among arguments.",
                "Finally, we evaluate (1) if argumentation between learning agents can produce a joint prediction that improves over individual learning performance and (2) if learning from the counterexamples conveyed during the argumentation process increases the individual performance with precisely those cases being used while arguing among them.",
                "The paper is structured as follows.",
                "Section 2 discusses the relation among argumentation, collaboration and learning.",
                "Then Section 3 introduces our multi-agent CBR (MAC) framework and the notion of justified prediction.",
                "After that, Section 4 formally defines our argumentation framework.",
                "Sections 5 and 6 present our case-based preference relation and argument generation policies respectively.",
                "Later, Section 7 presents the <br>argumentation protocol</br> in our AMAL framework.",
                "After that, Section 8 presents an exemplification of the argumentation framework.",
                "Finally, Section 9 presents an empirical evaluation of our two main hypotheses.",
                "The paper closes with related work and conclusions sections. 2.",
                "ARGUMENTATION,COLLABORATION AND LEARNING Both learning and collaboration are ways in which an agent can improve individual performance.",
                "In fact, there is a clear parallelism between learning and collaboration in multi-agent systems, since both are ways in which agents can deal with their shortcomings.",
                "Let us show which are the main motivations that an agent can have to learn or to collaborate. • Motivations to learn: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems. • Motivations to collaborate: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems, - Increase the range of accessible resources.",
                "Looking at the above lists of motivation, we can easily see that learning and collaboration are very related in multi-agent systems.",
                "In fact, with the exception of the last item in the motivations to collaborate list, they are two extremes of a continuum of strategies to improve performance.",
                "An agent may choose to increase performance by learning, by collaborating, or by finding an intermediate point that combines learning and collaboration in order to improve performance.",
                "In this paper we will propose AMAL, an argumentation framework for learning agents, and will also also show how AMAL can be used both for learning from communication and for solving problems in a collaborative way: • Agents can solve problems in a collaborative way via engaging an argumentation process about the prediction for the situation at hand.",
                "Using this collaboration, the prediction can be done in a more informed way, since the information known by several agents has been taken into account. • Agents can also learn from communication with other agents by engaging an argumentation process.",
                "Agents that engage in such argumentation processes can learn from the arguments and counterexamples received from other agents, and use this information for predicting the outcomes of future situations.",
                "In the rest of this paper we will propose an argumentation framework and show how it can be used both for learning and for solving problems in a collaborative way. 3.",
                "MULTI-AGENT CBR SYSTEMS A Multi-Agent Case Based Reasoning System (MAC) M = {(A1, C1), ..., (An, Cn)} is a multi-agent system composed of A = {Ai, ..., An}, a set of CBR agents, where each agent Ai ∈ A possesses an individual case base Ci.",
                "Each individual agent Ai in a MAC is completely autonomous and each agent Ai has access only to its individual and private case base Ci.",
                "A case base Ci = {c1, ..., cm} is a collection of cases.",
                "Agents in a MAC system are able to individually solve problems, but they can also collaborate with other agents to solve problems.",
                "In this framework, we will restrict ourselves to analytical tasks, i.e. tasks like classification, where the solution of a problem is achieved by selecting a solution class from an enumerated set of solution classes.",
                "In the following we will note the set of all the solution classes by S = {S1, ..., SK }.",
                "Therefore, a case c = P, S is a tuple containing a case description P and a solution class S ∈ S. In the following, we will use the terms problem and case description indistinctly.",
                "Moreover, we will use the dot notation to refer to elements inside a tuple; e.g., to refer to the solution class of a case c, we will write c.S.",
                "Therefore, we say a group of agents perform joint deliberation, when they collaborate to find a joint solution by means of an argumentation process.",
                "However, in order to do so, an agent has to be able to justify its prediction to the other agents (i.e. generate an argument for its predicted solution that can be examined and critiqued by the other agents).",
                "The next section addresses this issue. 3.1 Justified Predictions Both expert systems and CBR systems may have an explanation component [14] in charge of justifying why the system has provided a specific answer to the user.",
                "The line of reasoning of the system can then be examined by a human expert, thus increasing the reliability of the system.",
                "Most of the existing work on explanation generation focuses on generating explanations to be provided to the user.",
                "However, in our approach we use explanations (or justifications) as a tool for improving communication and coordination among agents.",
                "We are interested in justifications since they can be used as arguments.",
                "For that purpose, we will benefit from the ability of some machine learning methods to provide justifications.",
                "A justification built by a CBR method after determining that the solution of a particular problem P was Sk is a description that contains the relevant information from the problem P that the CBR method has considered to predict Sk as the solution of P. In particular, CBR methods work by retrieving similar cases to the problem at hand, and then reusing their solutions for the current problem, expecting that since the problem and the cases are similar, the solutions will also be similar.",
                "Thus, if a CBR method has retrieved a set of cases C1, ..., Cn to solve a particular problem P the justification built will contain the relevant information from the problem P that made the CBR system retrieve that particular set of cases, i.e. it will contain the relevant information that P and C1, ..., Cn have in common.",
                "For example, Figure 1 shows a justification build by a CBR system for a toy problem (in the following sections we will show justifications for real problems).",
                "In the figure, a problem has two attributes (Traffic_light, and Cars_passing), the retrieval mechanism of the CBR system notices that by considering only the attribute Traffic_light, it can retrieve two cases that predict the same solution: wait.",
                "Thus, since only this attribute has been used, it is the only one appearing in the justification.",
                "The values of the rest of attributes are irrelevant, since whatever their value the solution class would have been the same. 976 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Problem Traffic_light: red Cars_passing: no Case 1 Traffic_light: red Cars_passing: no Solution: wait Case 3 Traffic_light: red Cars_passing: yes Solution: wait Case 4 Traffic_light: green Cars_passing: yes Solution: wait Case 2 Traffic_light: green Cars_passing: no Solution: cross Retrieved cases Solution: wait Justification Traffic_light: red Figure 1: An example of justification generation in a CBR system.",
                "Notice that, since the only relevant feature to decide is Traffic_light (the only one used to retrieve cases), it is the only one appearing in the justification.",
                "In general, the meaning of a justification is that all (or most of) the cases in the case base of an agent that satisfy the justification (i.e. all the cases that are subsumed by the justification) belong to the predicted solution class.",
                "In the rest of the paper, we will use to denote the subsumption relation.",
                "In our work, we use LID [2], a CBR method capable of building symbolic justifications such as the one exemplified in Figure 1.",
                "When an agent provides a justification for a prediction, the agent generates a justified prediction: DEFINITION 3.1.",
                "A Justified Prediction is a tuple J = A, P, S, D where agent A considers S the correct solution for problem P, and that prediction is justified a symbolic description D such that J.D J.P. Justifications can have many uses for CBR systems [8, 9].",
                "In this paper, we are going to use justifications as arguments, in order to allow learning agents to engage in argumentation processes. 4.",
                "ARGUMENTS AND COUNTERARGUMENTS For our purposes an argument α generated by an agent A is composed of a statement S and some evidence D supporting S as correct.",
                "In the remainder of this section we will see how this general definition of argument can be instantiated in specific kind of arguments that the agents can generate.",
                "In the context of MAC systems, agents argue about predictions for new problems and can provide two kinds of information: a) specific cases P, S , and b) justified predictions: A, P, S, D .",
                "Using this information, we can define three types of arguments: justified predictions, counterarguments, and counterexamples.",
                "A justified prediction α is generated by an agent Ai to argue that Ai believes that the correct solution for a given problem P is α.S, and the evidence provided is the justification α.D.",
                "In the example depicted in Figure 1, an agent Ai may generate the argument α = Ai, P, Wait, (Traffic_light = red) , meaning that the agent Ai believes that the correct solution for P is Wait because the attribute Traffic_light equals red.",
                "A counterargument β is an argument offered in opposition to another argument α.",
                "In our framework, a counterargument consists of a justified prediction Aj, P, S , D generated by an agent Aj with the intention to rebut an argument α generated by another agent Ai, that endorses a solution class S different from that of α.S for the problem at hand and justifies this with a justification D .",
                "In the example in Figure 1, if an agent generates the argument α = Ai, P, Walk, (Cars_passing = no) , an agent that thinks that the correct solution is Wait might answer with the counterargument β = Aj, P, Wait, (Cars_passing = no ∧ Traffic_light = red) , meaning that, although there are no cars passing, the traffic light is red, and the street cannot be crossed.",
                "A counterexample c is a case that contradicts an argument α.",
                "Thus a counterexample is also a counterargument, one that states that a specific argument α is not always true, and the evidence provided is the case c. Specifically, for a case c to be a counterexample of an argument α, the following conditions have to be met: α.D c and α.S = c.S, i.e. the case must satisfy the justification α.D and the solution of c must be different than the predicted by α.",
                "By exchanging arguments and counterarguments (including counterexamples), agents can argue about the correct solution of a given problem, i.e. they can engage a joint deliberation process.",
                "However, in order to do so, they need a specific interaction protocol, a preference relation between contradicting arguments, and a decision policy to generate counterarguments (including counterexamples).",
                "In the following sections we will present these elements. 5.",
                "PREFERENCE RELATION A specific argument provided by an agent might not be consistent with the information known to other agents (or even to some of the information known by the agent that has generated the justification due to noise in training data).",
                "For that reason, we are going to define a preference relation over contradicting justified predictions based on cases.",
                "Basically, we will define a confidence measure for each justified prediction (that takes into account the cases owned by each agent), and the justified prediction with the highest confidence will be the preferred one.",
                "The idea behind case-based confidence is to count how many of the cases in an individual case base endorse a justified prediction, and how many of them are counterexamples of it.",
                "The more the endorsing cases, the higher the confidence; and the more the counterexamples, the lower the confidence.",
                "Specifically, to assess the confidence of a justified prediction α, an agent obtains the set of cases in its individual case base that are subsumed by α.D.",
                "With them, an agent Ai obtains the Y (aye) and N (nay) values: • Y Ai α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by the justification α.D that belong to the solution class α.S, • NAi α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by justification α.D that do not belong to that solution class.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 977 + + + + + +  - - + Figure 2: Confidence of arguments is evaluated by contrasting them against the case bases of the agents.",
                "An agent estimates the confidence of an argument as: CAi (α) = Y Ai α 1 + Y Ai α + NAi α i.e. the confidence on a justified prediction is the number of endorsing cases divided by the number of endorsing cases plus counterexamples.",
                "Notice that we add 1 to the denominator, this is to avoid giving excessively high confidences to justified predictions whose confidence has been computed using a small number of cases.",
                "Notice that this correction follows the same idea than the Laplace correction to estimate probabilities.",
                "Figure 2 illustrates the individual evaluation of the confidence of an argument, in particular, three endorsing cases and one counterexample are found in the case base of agents Ai, giving an estimated confidence of 0.6 Moreover, we can also define the joint confidence of an argument α as the confidence computed using the cases present in the case bases of all the agents in the group: C(α) = i Y Ai α 1 + i Y Ai α + NAi α Notice that, to collaboratively compute the joint confidence, the agents only have to make public the aye and nay values locally computed for a given argument.",
                "In our framework, agents use this joint confidence as the preference relation: a justified prediction α is preferred over another one β if C(α) ≥ C(β). 6.",
                "GENERATION OF ARGUMENTS In our framework, arguments are generated by the agents from cases, using learning methods.",
                "Any learning method able to provide a justified prediction can be used to generate arguments.",
                "For instance, decision trees and LID [2] are suitable learning methods.",
                "Specifically, in the experiments reported in this paper agents use LID.",
                "Thus, when an agent wants to generate an argument endorsing that a specific solution class is the correct solution for a problem P, it generates a justified prediction as explained in Section 3.1.",
                "For instance, Figure 3 shows a real justification generated by LID after solving a problem P in the domain of marine sponges identification.",
                "In particular, Figure 3 shows how when an agent receives a new problem to solve (in this case, a new sponge to determine its order), the agent uses LID to generate an argument (consisting on a justified prediction) using the cases in the case base of the agent.",
                "The justification shown in Figure 3 can be interpreted saying that the predicted solution is hadromerida because the smooth form of the megascleres of the spiculate skeleton of the sponge is of type tylostyle, the spikulate skeleton of the sponge has no uniform length, and there is no gemmules in the external features of the sponge.",
                "Thus, the argument generated will be α = A1, P, hadromerida, D1 . 6.1 Generation of Counterarguments As previously stated, agents may try to rebut arguments by generating counterargument or by finding counterexamples.",
                "Let us explain how they can be generated.",
                "An agent Ai wants to generate a counterargument β to rebut an argument α when α is in contradiction with the local case base of Ai.",
                "Moreover, while generating such counterargument β, Ai expects that β is preferred over α.",
                "For that purpose, we will present a specific policy to generate counterarguments based on the specificity criterion [10].",
                "The specificity criterion is widely used in deductive frameworks for argumentation, and states that between two conflicting arguments, the most specific should be preferred since it is, in principle, more informed.",
                "Thus, counterarguments generated based on the specificity criterion are expected to be preferable (since they are more informed) to the arguments they try to rebut.",
                "However, there is no guarantee that such counterarguments will always win, since, as we have stated in Section 5, agents in our framework use a preference relation based on joint confidence.",
                "Moreover, one may think that it would be better that the agents generate counterarguments based on the joint confidence preference relation; however it is not obvious how to generate counterarguments based on joint confidence in an efficient way, since collaboration is required in order to evaluate joint confidence.",
                "Thus, the agent generating the counterargument should constantly communicate with the other agents at each step of the induction algorithm used to generate counterarguments (presently one of our future research lines).",
                "Thus, in our framework, when an agent wants to generate a counterargument β to an argument α, β has to be more specific than α (i.e. α.D < β.D).",
                "The generation of counterarguments using the specificity criterion imposes some restrictions over the learning method, although LID or ID3 can be easily adapted for this task.",
                "For instance, LID is an algorithm that generates a description starting from scratch and heuristically adding features to that term.",
                "Thus, at every step, the description is made more specific than in the previous step, and the number of cases that are subsumed by that description is reduced.",
                "When the description covers only (or almost only) cases of a single solution class LID terminates and predicts that solution class.",
                "To generate a counterargument to an argument α LID just has to use as starting point the description α.D instead of starting from scratch.",
                "In this way, the justification provided by LID will always be subsumed by α.D, and thus the resulting counterargument will be more specific than α.",
                "However, notice that LID may sometimes not be able to generate counterarguments, since LID may not be able to specialize the description α.D any further, or because the agent Ai has no case inCi that is subsumed by α.D.",
                "Figure 4 shows how an agent A2 that disagreed with the argument shown in Figure 3, generates a counterargument using LID.",
                "Moreover, Figure 4 shows the generation of a counterargument β1 2 for the argument α0 1 (in Figure 3) that is a specialization of α0 1. 978 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Solution: hadromerida Justification: D1 Sponge Spikulate skeleton External features External features Gemmules: no Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Case Base of A1 LID New sponge P Figure 3: Example of a real justification generated by LID in the marine sponges data set.",
                "Specifically, in our experiments, when an agent Ai wants to rebut an argument α, uses the following policy: 1.",
                "Agent Ai uses LID to try to find a counterargument β more specific than α; if found, β is sent to the other agent as a counterargument of α. 2.",
                "If not found, then Ai searches for a counterexample c ∈ Ci of α.",
                "If a case c is found, then c is sent to the other agent as a counterexample of α. 3.",
                "If no counterexamples are found, then Ai cannot rebut the argument α. 7.",
                "ARGUMENTATION-BASED MULTI-AGENT LEARNING The interaction protocol of AMAL allows a group of agents A1, ..., An to deliberate about the correct solution of a problem P by means of an argumentation process.",
                "If the argumentation process arrives to a consensual solution, the joint deliberation ends; otherwise a weighted vote is used to determine the joint solution.",
                "Moreover, AMAL also allows the agents to learn from the counterexamples received from other agents.",
                "The AMAL protocol consists on a series of rounds.",
                "In the initial round, each agent states which is its individual prediction for P. Then, at each round an agent can try to rebut the prediction made by any of the other agents.",
                "The protocol uses a token passing mechanism so that agents (one at a time) can send counterarguments or counterexamples if they disagree with the prediction made by any other agent.",
                "Specifically, each agent is allowed to send one counterargument or counterexample each time he gets the token (notice that this restriction is just to simplify the protocol, and that it does not restrict the number of counterargument an agent can sent, since they can be delayed for subsequent rounds).",
                "When an agent receives a counterargument or counterexample, it informs the other agents if it accepts the counterargument (and changes its prediction) or not.",
                "Moreover, agents have also the opportunity to answer to counterarguments when they receive the token, by trying to generate a counterargument to the counterargument.",
                "When all the agents have had the token once, the token returns to the first agent, and so on.",
                "If at any time in the protocol, all the agents agree or during the last n rounds no agent has generated any counterargument, the protocol ends.",
                "Moreover, if at the end of the argumentation the agents have not reached an agreement, then a voting mechanism that uses the confidence of each prediction as weights is used to decide the final solution (Thus, AMAL follows the same mechanism as human committees, first each individual member of a committee exposes his arguments and discuses those of the other members (joint deliberation), and if no consensus is reached, then a voting mechanism is required).",
                "At each iteration, agents can use the following performatives: • assert(α): the justified prediction held during the next round will be α.",
                "An agent can only hold a single prediction at each round, thus is multiple asserts are send, only the last one is considered as the currently held prediction. • rebut(β, α): the agent has found a counterargument β to the prediction α.",
                "We will define Ht = αt 1, ..., αt n as the predictions that each of the n agents hold at a round t. Moreover, we will also define contradict(αt i) = {α ∈ Ht|α.S = αt i.S} as the set of contradicting arguments for an agent Ai in a round t, i.e. the set of arguments at round t that support a different solution class than αt i.",
                "The protocol is initiated because one of the agents receives a problem P to be solved.",
                "After that, the agent informs all the other agents about the problem P to solve, and the protocol starts: 1.",
                "At round t = 0, each one of the agents individually solves P, and builds a justified prediction using its own CBR method.",
                "Then, each agent Ai sends the performative assert(α0 i ) to the other agents.",
                "Thus, the agents know H0 = α0 i , ..., α0 n .",
                "Once all the predictions have been sent the token is given to the first agent A1. 2.",
                "At each round t (other than 0), the agents check whether their arguments in Ht agree.",
                "If they do, the protocol moves to step 5.",
                "Moreover, if during the last n rounds no agent has sent any counterexample or counterargument, the protocol also moves to step 5.",
                "Otherwise, the agent Ai owner of the token tries to generate a counterargument for each of the opposing arguments in contradict(αt i) ⊆ Ht (see Section 6.1).",
                "Then, the counterargument βt i against the prediction αt j with the lowest confidence C(αt j) is selected (since αt j is the prediction more likely to be successfully rebutted). • If βt i is a counterargument, then, Ai locally compares αt i with βt i by assessing their confidence against its individual case base Ci (see Section 5) (notice that Ai is comparing its previous argument with the counterargument that Ai itself has just generated and that is about The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 979 Sponge Spikulate skeleton External features External features Gemmules: no Growing: Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Growing Grow: massive Case Base of A2 LID Solution: astrophorida Justification: D2 Figure 4: Generation of a counterargument using LID in the sponges data set. to send to Aj).",
                "If CAi (βt i ) > CAi (αt i), then Ai considers that βt i is stronger than its previous argument, changes its argument to βt i by sending assert(βt i ) to the rest of the agents (the intuition behind this is that since a counterargument is also an argument, Ai checks if the newly counterargument is a better argument than the one he was previously holding) and rebut(βt i , αt j) to Aj.",
                "Otherwise (i.e.",
                "CAi (βt i ) ≤ CAi (αt i)), Ai will send only rebut(βt i , αt j) to Aj.",
                "In any of the two situations the protocol moves to step 3. • If βt i is a counterexample c, then Ai sends rebut(c, αt j) to Aj.",
                "The protocol moves to step 4. • If Ai cannot generate any counterargument or counterexample, the token is sent to the next agent, a new round t + 1 starts, and the protocol moves to state 2. 3.",
                "The agent Aj that has received the counterargument βt i , locally compares it against its own argument, αt j, by locally assessing their confidence.",
                "If CAj (βt i ) > CAj (αt j), then Aj will accept the counterargument as stronger than its own argument, and it will send assert(βt i ) to the other agents.",
                "Otherwise (i.e.",
                "CAj (βt i ) ≤ CAj (αt j)), Aj will not accept the counterargument, and will inform the other agents accordingly.",
                "Any of the two situations start a new round t + 1, Ai sends the token to the next agent, and the protocol moves back to state 2. 4.",
                "The agent Aj that has received the counterexample c retains it into its case base and generates a new argument αt+1 j that takes into account c, and informs the rest of the agents by sending assert(αt+1 j ) to all of them.",
                "Then, Ai sends the token to the next agent, a new round t + 1 starts, and the protocol moves back to step 2. 5.",
                "The protocol ends yielding a joint prediction, as follows: if the arguments in Ht agree then their prediction is the joint prediction, otherwise a voting mechanism is used to decide the joint prediction.",
                "The voting mechanism uses the joint confidence measure as the voting weights, as follows: S = arg max Sk∈S αi∈Ht|αi.S=Sk C(αi) Moreover, in order to avoid infinite iterations, if an agent sends twice the same argument or counterargument to the same agent, the message is not considered. 8.",
                "EXEMPLIFICATION Let us consider a system composed of three agents A1, A2 and A3.",
                "One of the agents, A1 receives a problem P to solve, and decides to use AMAL to solve it.",
                "For that reason, invites A2 and A3 to take part in the argumentation process.",
                "They accept the invitation, and the <br>argumentation protocol</br> starts.",
                "Initially, each agent generates its individual prediction for P, and broadcasts it to the other agents.",
                "Thus, all of them can compute H0 = α0 1, α0 2, α0 3 .",
                "In particular, in this example: • α0 1 = A1, P, hadromerida, D1 • α0 2 = A2, P, astrophorida, D2 • α0 3 = A3, P, axinellida, D3 A1 starts owning the token and tries to generate counterarguments for α0 2 and α0 3, but does not succeed, however it has one counterexample c13 for α0 3.",
                "Thus, A1 sends the the message rebut( c13, α0 3) to A3.",
                "A3 incorporates c13 into its case base and tries to solve the problem P again, now taking c13 into consideration.",
                "A3 comes up with the justified prediction α1 3 = A3, P, hadromerida, D4 , and broadcasts it to the rest of the agents with the message assert(α1 3).",
                "Thus, all of them know the new H1 = α0 1, α0 2, α1 3 .",
                "Round 1 starts and A2 gets the token.",
                "A2 tries to generate counterarguments for α0 1 and α1 3 and only succeeds to generate a counterargument β1 2 = A2, P, astrophorida, D5 against α1 3.",
                "The counterargument is sent to A3 with the message rebut(β1 2 , α1 3).",
                "Agent A3 receives the counterargument and assesses its local confidence.",
                "The result is that the individual confidence of the counterargument β1 2 is lower than the local confidence of α1 3.",
                "Therefore, A3 does not accept the counterargument, and thus H2 = α0 1, α0 2, α1 3 .",
                "Round 2 starts and A3 gets the token.",
                "A3 generates a counterargument β2 3 = A3, P, hadromerida, D6 for α0 2 and sends it to A2 with the message rebut(β2 3 , α0 2).",
                "Agent A2 receives the counterargument and assesses its local confidence.",
                "The result is that the local confidence of the counterargument β2 3 is higher than the local confidence of α0 2.",
                "Therefore, A2 accepts the counterargument and informs the rest of the agents with the message assert(β2 3 ).",
                "After that, H3 = α0 1, β2 3 , α1 3 .",
                "At Round 3, since all the agents agree (all the justified predictions in H3 predict hadromerida as the solution class) The protocol ends, and A1 (the agent that received the problem) considers hadromerida as the joint solution for the problem P. 9.",
                "EXPERIMENTAL EVALUATION 980 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) SPONGE 75 77 79 81 83 85 87 89 91 2 3 4 5 AMAL Voting Individual SOYBEAN 55 60 65 70 75 80 85 90 2 3 4 5 AMAL Voting Individual Figure 5: Individual and joint accuracy for 2 to 5 agents.",
                "In this section we empirically evaluate the AMAL argumentation framework.",
                "We have made experiments in two different data sets: soybean (from the UCI machine learning repository) and sponge (a relational data set).",
                "The soybean data set has 307 examples and 19 solution classes, while the sponge data set has 280 examples and 3 solution classes.",
                "In an experimental run, the data set is divided in 2 sets: the training set and the test set.",
                "The training set examples are distributed among 5 different agents without replication, i.e. there is no example shared by two agents.",
                "In the testing stage, problems in the test set arrive randomly to one of the agents, and their goal is to predict the correct solution.",
                "The experiments are designed to test two hypotheses: (H1) that argumentation is a useful framework for joint deliberation and can improve over other typical methods such as voting; and (H2) that learning from communication improves the individual performance of a learning agent participating in an argumentation process.",
                "Moreover, we also expect that the improvement achieved from argumentation will increase as the number of agents participating in the argumentation increases (since more information will be taken into account).",
                "Concerning H1 (argumentation is a useful framework for joint deliberation), we ran 4 experiments, using 2, 3, 4, and 5 agents respectively (in all experiments each agent has a 20% of the training data, since the training is always distributed among 5 agents).",
                "Figure 5 shows the result of those experiments in the sponge and soybean data sets.",
                "Classification accuracy is plotted in the vertical axis, and in the horizontal axis the number of agents that took part in the argumentation processes is shown.",
                "For each number of agents, three bars are shown: individual, Voting, and AMAL.",
                "The individual bar shows the average accuracy of individual agents predictions; the voting bar shows the average accuracy of the joint prediction achieved by voting but without any argumentation; and finally the AMAL bar shows the average accuracy of the joint prediction using argumentation.",
                "The results shown are the average of 5 10-fold cross validation runs.",
                "Figure 5 shows that collaboration (voting and AMAL) outperforms individual problem solving.",
                "Moreover, as we expected, the accuracy improves as more agents collaborate, since more information is taken into account.",
                "We can also see that AMAL always outperforms standard voting, proving that joint decisions are based on better information as provided by the argumentation process.",
                "For instance, the joint accuracy for 2 agents in the sponge data set is of 87.57% for AMAL and 86.57% for voting (while individual accuracy is just 80.07%).",
                "Moreover, the improvement achieved by AMAL over Voting is even larger in the soybean data set.",
                "The reason is that the soybean data set is more difficult (in the sense that agents need more data to produce good predictions).",
                "These experimental results show that AMAL effectively exploits the opportunity for improvement: the accuracy is higher only because more agents have changed their opinion during argumentation (otherwise they would achieve the same result as Voting).",
                "Concerning H2 (learning from communication in argumentation processes improves individual prediction ), we ran the following experiment: initially, we distributed a 25% of the training set among the five agents; after that, the rest of the cases in the training set is sent to the agents one by one; when an agent receives a new training case, it has several options: the agent can discard it, the agent can retain it, or the agent can use it for engaging an argumentation process.",
                "Figure 6 shows the result of that experiment for the two data sets.",
                "Figure 6 contains three plots, where NL (not learning) shows accuracy of an agent with no learning at all; L (learning), shows the evolution of the individual classification accuracy when agents learn by retaining the training cases they individually receive (notice that when all the training cases have been retained at 100%, the accuracy should be equal to that of Figure 5 for individual agents); and finally LFC (learning from communication) shows the evolution of the individual classification accuracy of learning agents that also learn by retaining those counterexamples received during argumentation (i.e. they learn both from training examples and counterexamples).",
                "Figure 6 shows that if an agent Ai learns also from communication, Ai can significantly improve its individual performance with just a small number of additional cases (those selected as relevant counterexamples for Ai during argumentation).",
                "For instance, in the soybean data set, individual agents have achieved an accuracy of 70.62% when they also learn from communication versus an accuracy of 59.93% when they only learn from their individual experience.",
                "The number of cases learnt from communication depends on the properties of the data set: in the sponges data set, agents have retained only very few additional cases, and significantly improved individual accuracy; namely they retain 59.96 cases in average (compared to the 50.4 cases retained if they do not learn from communication).",
                "In the soybean data set more counterexamples are learnt to significantly improve individual accuracy, namely they retain 87.16 cases in average (compared to 55.27 cases retained if they do not learn from communication).",
                "Finally, the fact that both data sets show a significant improvement points out the adaptive nature of the argumentation-based approach to learning from communication: the useful cases are selected as counterexamples (and no more than those needed), and they have the intended effect. 10.",
                "RELATED WORK Concerning CBR in a multi-agent setting, the first research was on negotiated case retrieval [11] among groups of agents.",
                "Our work on multi-agent case-based learning started in 1999 [6]; later Mc Ginty and Smyth [7] presented a multi-agent collaborative CBR approach (CCBR) for planning.",
                "Finally, another interesting approach is multi-case-base reasoning (MCBR) [5], that deals with The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 981 SPONGE 60 65 70 75 80 85 25% 40% 55% 70% 85% 100% LFC L NL SOYBEAN 20 30 40 50 60 70 80 90 25% 40% 55% 70% 85% 100% LFC L NL Figure 6: Learning from communication resulting from argumentation in a system composed of 5 agents. distributed systems where there are several case bases available for the same task and addresses the problems of cross-case base adaptation.",
                "The main difference is that our MAC approach is a way to distribute the Reuse process of CBR (using a voting system) while Retrieve is performed individually by each agent; the other multiagent CBR approaches, however, focus on distributing the Retrieve process.",
                "Research on MAS argumentation focus on several issues like a) logics, protocols and languages that support argumentation, b) argument selection and c) argument interpretation.",
                "Approaches for logic and languages that support argumentation include defeasible logic [4] and BDI models [13].",
                "Although argument selection is a key aspect of automated argumentation (see [12] and [13]), most research has been focused on preference relations among arguments.",
                "In our framework we have addressed both argument selection and preference relations using a case-based approach. 11.",
                "CONCLUSIONS AND FUTURE WORK In this paper we have presented an argumentation-based framework for multi-agent learning.",
                "Specifically, we have presented AMAL, a framework that allows a group of learning agents to argue about the solution of a given problem and we have shown how the learning capabilities can be used to generate arguments and counterarguments.",
                "The experimental evaluation shows that the increased amount of information provided to the agents by the argumentation process increases their predictive accuracy, and specially when an adequate number of agents take part in the argumentation.",
                "The main contributions of this work are: a) an argumentation framework for learning agents; b) a case-based preference relation over arguments, based on computing an overall confidence estimation of arguments; c) a case-based policy to generate counterarguments and select counterexamples; and d) an argumentation-based approach for learning from communication.",
                "Finally, in the experiments presented here a learning agent would retain all counterexamples submitted by the other agent; however, this is a very simple case retention policy, and we will like to experiment with more informed policies - with the goal that individual learning agents could significantly improve using only a small set of cases proposed by other agents.",
                "Finally, our approach is focused on lazy learning, and future works aims at incorporating eager inductive learning inside the argumentative framework for learning from communication. 12.",
                "REFERENCES [1] Agnar Aamodt and Enric Plaza.",
                "Case-based reasoning: Foundational issues, methodological variations, and system approaches.",
                "Artificial Intelligence Communications, 7(1):39-59, 1994. [2] E. Armengol and E. Plaza.",
                "Lazy induction of descriptions for relational case-based learning.",
                "In ECML2001, pages 13-24, 2001. [3] Gerhard Brewka.",
                "Dynamic argument systems: A formal model of argumentation processes based on situation calculus.",
                "Journal of Logic and Computation, 11(2):257-282, 2001. [4] Carlos I. Chesñevar and Guillermo R. Simari.",
                "Formalizing Defeasible Argumentation using Labelled Deductive Systems.",
                "Journal of Computer Science & Technology, 1(4):18-33, 2000. [5] D. Leake and R. Sooriamurthi.",
                "Automatically selecting strategies for multi-case-base reasoning.",
                "In S. Craw and A. Preece, editors, ECCBR2002, pages 204-219, Berlin, 2002.",
                "Springer Verlag. [6] Francisco J. Martín, Enric Plaza, and Josep-Lluis Arcos.",
                "Knowledge and experience reuse through communications among competent (peer) agents.",
                "International Journal of Software Engineering and Knowledge Engineering, 9(3):319-341, 1999. [7] Lorraine McGinty and Barry Smyth.",
                "Collaborative case-based reasoning: Applications in personalized route planning.",
                "In I. Watson and Q. Yang, editors, ICCBR, number 2080 in LNAI, pages 362-376.",
                "Springer-Verlag, 2001. [8] Santi Ontañón and Enric Plaza.",
                "Justification-based multiagent learning.",
                "In ICML2003, pages 576-583.",
                "Morgan Kaufmann, 2003. [9] Enric Plaza, Eva Armengol, and Santiago Ontañón.",
                "The explanatory power of symbolic similarity in case-based reasoning.",
                "Artificial Intelligence Review, 24(2):145-161, 2005. [10] David Poole.",
                "On the comparison of theories: Preferring the most specific explanation.",
                "In IJCAI-85, pages 144-147, 1985. [11] M V Nagendra Prassad, Victor R Lesser, and Susan Lander.",
                "Retrieval and reasoning in distributed case bases.",
                "Technical report, UMass Computer Science Department, 1995. [12] K. Sycara S. Kraus and A. Evenchik.",
                "Reaching agreements through argumentation: a logical model and implementation.",
                "Artificial Intelligence Journal, 104:1-69, 1998. [13] N. R. Jennings S. Parsons, C. Sierra.",
                "Agents that reason and negotiate by arguing.",
                "Journal of Logic and Computation, 8:261-292, 1998. [14] Bruce A. Wooley.",
                "Explanation component of software systems.",
                "ACM CrossRoads, 5.1, 1998. 982 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "We propose an <br>argumentation protocol</br> inside the AMAL framework at supports agents in reaching a joint prediction over a specific situation or problem - moreover, the reasoning needed to support the argumentation process will also be based on cases.",
                "Later, Section 7 presents the <br>argumentation protocol</br> in our AMAL framework.",
                "They accept the invitation, and the <br>argumentation protocol</br> starts."
            ],
            "translated_annotated_samples": [
                "Proponemos un <br>protocolo de argumentación</br> dentro del marco de AMAL que apoya a los agentes en llegar a una predicción conjunta sobre una situación o problema específico; además, el razonamiento necesario para respaldar el proceso de argumentación también se basará en casos.",
                "Más adelante, la Sección 7 presenta el <br>protocolo de argumentación</br> en nuestro marco AMAL.",
                "Aceptan la invitación y comienza el <br>protocolo de argumentación</br>."
            ],
            "translated_text": "Aprendizaje y deliberación conjunta a través de la argumentación en sistemas multiagente. Santi Ontañón CCL, Laboratorio de Computación Cognitiva, Instituto de Tecnología de Georgia, Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Instituto de Investigación en Inteligencia Artificial, CSIC, Consejo Superior de Investigaciones Científicas, Campus UAB, 08193 Bellaterra, Cataluña (España) enric@iiia.csic.es RESUMEN En este artículo presentaremos un marco de argumentación para agentes de aprendizaje (AMAL) diseñado con dos propósitos: (1) para deliberación conjunta, y (2) para aprender a través de la comunicación. El marco AMAL se basa completamente en el aprendizaje a partir de ejemplos: la relación de preferencia de argumentos, la política de generación de argumentos y la política de generación de contraargumentos son técnicas basadas en casos. Para unirse en la deliberación, los agentes de aprendizaje comparten su experiencia formando un comité para decidir sobre alguna decisión conjunta. Experimentalmente demostramos que la argumentación entre comités de agentes mejora tanto el rendimiento individual como el conjunto. Para aprender a través de la comunicación, un agente se involucra en discusiones con otros agentes para contrastar sus hipótesis individuales y recibir contraejemplos; el proceso de argumentación mejora su alcance de aprendizaje y rendimiento individual. Categorías y Descriptores de Asignaturas I.2.6 [Inteligencia Artificial]: Aprendizaje; I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente, Agentes Inteligentes 1. Los marcos de argumentación para sistemas multiagente pueden ser utilizados para diferentes propósitos como la deliberación conjunta, la persuasión, la negociación y la resolución de conflictos. En este artículo presentaremos un marco de argumentación para agentes de aprendizaje, y mostraremos que puede ser utilizado para dos propósitos: (1) deliberación conjunta, y (2) aprendizaje a partir de la comunicación. La deliberación conjunta basada en argumentos implica discutir sobre el resultado de una situación particular o la acción apropiada a tomar en una situación específica. Los agentes de aprendizaje son capaces de aprender de la experiencia, en el sentido de que ejemplos pasados (situaciones y sus resultados) se utilizan para predecir el resultado para la situación actual. Sin embargo, dado que la experiencia de los agentes individuales puede ser limitada, el conocimiento individual y la precisión de la predicción también son limitados. Por lo tanto, los agentes de aprendizaje que son capaces de argumentar sus predicciones individuales con otros agentes pueden alcanzar una mayor precisión en las predicciones después de dicho proceso de argumentación. La mayoría de los marcos de argumentación existentes para sistemas multiagente se basan en lógica deductiva u otra formalización de lógica deductiva diseñada específicamente para respaldar la argumentación, como la lógica por defecto [3]. Por lo general, un argumento se ve como una declaración lógica, mientras que un contraargumento es un argumento ofrecido en oposición a otro argumento; los agentes utilizan una relación de preferencia para resolver argumentos conflictivos. Sin embargo, los marcos de argumentación basados en lógica asumen agentes con conocimiento predefinido y relación de preferencia. En este artículo, nos enfocamos en un marco de Aprendizaje Multiagente basado en Argumentación (AMAL) donde tanto el conocimiento como la relación de preferencia se aprenden a partir de la experiencia. Por lo tanto, consideramos un escenario con agentes que (1) trabajan en el mismo dominio utilizando una ontología compartida, (2) son capaces de aprender a partir de ejemplos y (3) se comunican utilizando un marco argumentativo. Tener capacidades de aprendizaje permite a los agentes utilizar de manera efectiva una forma específica de contraargumento, a saber, el uso de contraejemplos. Los contraejemplos ofrecen la posibilidad de que los agentes aprendan durante el proceso de argumentación. Además, los agentes de aprendizaje permiten técnicas que utilizan la experiencia aprendida para generar argumentos y contraargumentos adecuados. Específicamente, necesitaremos abordar dos problemas: (1) cómo definir una técnica para generar argumentos y contraargumentos a partir de ejemplos, y (2) cómo definir una relación de preferencia sobre dos argumentos en conflicto que han sido inducidos a partir de ejemplos. Este documento presenta un enfoque basado en casos para abordar ambas cuestiones. Los agentes utilizan el razonamiento basado en casos (CBR) [1] para aprender de casos pasados (donde un caso es una situación y su resultado) con el fin de predecir el resultado de una nueva situación. Proponemos un <br>protocolo de argumentación</br> dentro del marco de AMAL que apoya a los agentes en llegar a una predicción conjunta sobre una situación o problema específico; además, el razonamiento necesario para respaldar el proceso de argumentación también se basará en casos. En particular, presentamos dos medidas basadas en casos, una para generar los argumentos y contraargumentos adecuados para una situación particular y otra para determinar la relación de preferencia entre los argumentos. Finalmente, evaluamos (1) si la argumentación entre agentes de aprendizaje puede producir una predicción conjunta que mejore el rendimiento de aprendizaje individual y (2) si el aprendizaje a partir de los contraejemplos transmitidos durante el proceso de argumentación aumenta el rendimiento individual con precisamente aquellos casos que se utilizan mientras discuten entre ellos. El documento está estructurado de la siguiente manera. La sección 2 discute la relación entre la argumentación, la colaboración y el aprendizaje. Luego, la Sección 3 introduce nuestro marco de trabajo de CBR multiagente (MAC) y el concepto de predicción justificada. Después de eso, la Sección 4 define formalmente nuestro marco de argumentación. Las secciones 5 y 6 presentan nuestra relación de preferencia basada en casos y las políticas de generación de argumentos, respectivamente. Más adelante, la Sección 7 presenta el <br>protocolo de argumentación</br> en nuestro marco AMAL. Después de eso, la Sección 8 presenta una ejemplificación del marco de argumentación. Finalmente, la Sección 9 presenta una evaluación empírica de nuestras dos hipótesis principales. El artículo concluye con secciones de trabajo relacionado y conclusiones. 2. ARGUMENTACIÓN, COLABORACIÓN Y APRENDIZAJE Tanto el aprendizaje como la colaboración son formas en las que un agente puede mejorar el rendimiento individual. De hecho, existe un claro paralelismo entre el aprendizaje y la colaboración en sistemas multiagentes, ya que ambos son formas en las que los agentes pueden lidiar con sus deficiencias. Mostremos cuáles son las principales motivaciones que un agente puede tener para aprender o colaborar. • Motivaciones para aprender: - Aumentar la calidad de la predicción, - Aumentar la eficiencia, - Ampliar el rango de problemas resolubles. • Motivaciones para colaborar: - Aumentar la calidad de la predicción, - Aumentar la eficiencia, - Ampliar el rango de problemas resolubles, - Ampliar el rango de recursos accesibles. Al observar las listas de motivación anteriores, podemos ver fácilmente que el aprendizaje y la colaboración están muy relacionados en los sistemas multiagente. De hecho, con la excepción del último elemento en la lista de motivaciones para colaborar, son dos extremos de un continuo de estrategias para mejorar el rendimiento. Un agente puede optar por aumentar el rendimiento mediante el aprendizaje, la colaboración o encontrando un punto intermedio que combine el aprendizaje y la colaboración para mejorar el rendimiento. En este artículo propondremos AMAL, un marco de argumentación para agentes de aprendizaje, y también mostraremos cómo AMAL puede ser utilizado tanto para aprender a través de la comunicación como para resolver problemas de manera colaborativa: • Los agentes pueden resolver problemas de manera colaborativa al participar en un proceso de argumentación sobre la predicción para la situación actual. Utilizando esta colaboración, la predicción puede realizarse de una manera más informada, ya que se ha tenido en cuenta la información conocida por varios agentes. Los agentes también pueden aprender de la comunicación con otros agentes al participar en un proceso de argumentación. Los agentes que participan en estos procesos de argumentación pueden aprender de los argumentos y contraejemplos recibidos de otros agentes, y utilizar esta información para predecir los resultados de situaciones futuras. En el resto de este documento propondremos un marco de argumentación y mostraremos cómo puede ser utilizado tanto para el aprendizaje como para la resolución de problemas de manera colaborativa. SISTEMAS CBR MULTIAGENTE Un Sistema de Razonamiento Basado en Casos Multiagente (MAC) M = {(A1, C1), ..., (An, Cn)} es un sistema multiagente compuesto por A = {Ai, ..., An}, un conjunto de agentes CBR, donde cada agente Ai ∈ A posee una base de casos individual Ci. Cada agente individual Ai en un MAC es completamente autónomo y cada agente Ai solo tiene acceso a su base de casos individual y privada Ci. Un conjunto de casos Ci = {c1, ..., cm} es una colección de casos. Los agentes en un sistema MAC pueden resolver problemas de forma individual, pero también pueden colaborar con otros agentes para resolver problemas. En este marco, nos limitaremos a tareas analíticas, es decir, tareas como la clasificación, donde la solución de un problema se logra seleccionando una clase de solución de un conjunto enumerado de clases de solución. En lo siguiente, notaremos el conjunto de todas las clases de soluciones por S = {S1, ..., SK}. Por lo tanto, un caso c = P, S es una tupla que contiene una descripción del caso P y una clase de solución S ∈ S. En lo siguiente, utilizaremos indistintamente los términos problema y descripción del caso. Además, utilizaremos la notación de punto para referirnos a elementos dentro de una tupla; por ejemplo, para referirnos a la clase de solución de un caso c, escribiremos c.S. Por lo tanto, decimos que un grupo de agentes realiza una deliberación conjunta cuando colaboran para encontrar una solución conjunta mediante un proceso de argumentación. Sin embargo, para hacerlo, un agente debe ser capaz de justificar su predicción a los otros agentes (es decir, generar un argumento para su solución predicha que pueda ser examinado y criticado por los otros agentes). La siguiente sección aborda este tema. 3.1 Predicciones Justificadas Tanto los sistemas expertos como los sistemas CBR pueden tener un componente de explicación [14] encargado de justificar por qué el sistema ha proporcionado una respuesta específica al usuario. La línea de razonamiento del sistema puede ser examinada por un experto humano, aumentando así la fiabilidad del sistema. La mayoría de los trabajos existentes sobre generación de explicaciones se centran en generar explicaciones para ser proporcionadas al usuario. Sin embargo, en nuestro enfoque utilizamos explicaciones (o justificaciones) como una herramienta para mejorar la comunicación y coordinación entre agentes. Estamos interesados en las justificaciones ya que pueden ser utilizadas como argumentos. Para ese propósito, nos beneficiaremos de la capacidad de algunos métodos de aprendizaje automático para proporcionar justificaciones. Una justificación construida por un método CBR después de determinar que la solución de un problema particular P fue Sk es una descripción que contiene la información relevante del problema P que el método CBR ha considerado para predecir Sk como la solución de P. En particular, los métodos CBR funcionan recuperando casos similares al problema en cuestión, y luego reutilizando sus soluciones para el problema actual, esperando que dado que el problema y los casos son similares, las soluciones también serán similares. Por lo tanto, si un método CBR ha recuperado un conjunto de casos C1, ..., Cn para resolver un problema particular P, la justificación construida contendrá la información relevante del problema P que hizo que el sistema CBR recuperara ese conjunto particular de casos, es decir, contendrá la información relevante que P y C1, ..., Cn tienen en común. Por ejemplo, la Figura 1 muestra una justificación construida por un sistema CBR para un problema de juguete (en las siguientes secciones mostraremos justificaciones para problemas reales). En la figura, un problema tiene dos atributos (Semáforo y Coches_pasando), el mecanismo de recuperación del sistema CBR nota que al considerar solo el atributo Semáforo, puede recuperar dos casos que predicen la misma solución: esperar. Por lo tanto, dado que solo se ha utilizado este atributo, es el único que aparece en la justificación. Los valores del resto de atributos son irrelevantes, ya que, sea cual sea su valor, la clase de solución habría sido la misma. 976 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Problema Semáforo: rojo Pasan coches: no Caso 1 Semáforo: rojo Pasan coches: no Solución: esperar Caso 3 Semáforo: rojo Pasan coches: sí Solución: esperar Caso 4 Semáforo: verde Pasan coches: sí Solución: esperar Caso 2 Semáforo: verde Pasan coches: no Solución: cruzar Casos recuperados Solución: esperar Justificación Semáforo: rojo Figura 1: Un ejemplo de generación de justificación en un sistema de RCB. Observa que, dado que la única característica relevante para decidir es Traffic_light (la única utilizada para recuperar casos), es la única que aparece en la justificación. En general, el significado de una justificación es que todos (o la mayoría) de los casos en la base de casos de un agente que satisfacen la justificación (es decir, todos los casos que están subsumidos por la justificación) pertenecen a la clase de solución predicha. En el resto del documento, usaremos para denotar la relación de subsumpción. En nuestro trabajo, utilizamos LID [2], un método CBR capaz de construir justificaciones simbólicas como la ejemplificada en la Figura 1. Cuando un agente proporciona una justificación para una predicción, el agente genera una predicción justificada: DEFINICIÓN 3.1. Una Predicción Justificada es una tupla J = A, P, S, D donde el agente A considera que S es la solución correcta para el problema P, y esa predicción está justificada por una descripción simbólica D tal que J.D J.P. Las justificaciones pueden tener muchos usos para los sistemas de RCB [8, 9]. En este documento, vamos a utilizar justificaciones como argumentos, con el fin de permitir que los agentes de aprendizaje participen en procesos de argumentación. 4. ARGUMENTOS Y CONTRAARGUMENTOS Para nuestros propósitos, un argumento α generado por un agente A está compuesto por una afirmación S y alguna evidencia D que respalda a S como correcta. En el resto de esta sección veremos cómo esta definición general de argumento puede ser concretada en un tipo específico de argumentos que los agentes pueden generar. En el contexto de los sistemas MAC, los agentes discuten sobre predicciones para nuevos problemas y pueden proporcionar dos tipos de información: a) casos específicos P, S, y b) predicciones justificadas: A, P, S, D. Usando esta información, podemos definir tres tipos de argumentos: predicciones justificadas, contraargumentos y contraejemplos. Una predicción justificada α es generada por un agente Ai para argumentar que Ai cree que la solución correcta para un problema dado P es α.S, y la evidencia proporcionada es la justificación α.D. En el ejemplo representado en la Figura 1, un agente Ai puede generar el argumento α = Ai, P, Esperar, (Semáforo = rojo), lo que significa que el agente Ai cree que la solución correcta para P es Esperar porque el atributo Semáforo es igual a rojo. Un contraargumento β es un argumento ofrecido en oposición a otro argumento α. En nuestro marco de trabajo, un contraargumento consiste en una predicción justificada Aj, P, S, D generada por un agente Aj con la intención de refutar un argumento α generado por otro agente Ai, que respalda una clase de solución S diferente a la de α.S para el problema en cuestión y justifica esto con una justificación D. En el ejemplo de la Figura 1, si un agente genera el argumento α = Ai, P, Caminar, (Pasando_autos = no), un agente que piensa que la solución correcta es Esperar podría responder con el contraargumento β = Aj, P, Esperar, (Pasando_autos = no ∧ Semáforo = rojo), lo que significa que, aunque no hay autos pasando, el semáforo está en rojo y la calle no se puede cruzar. Un contraejemplo c es un caso que contradice un argumento α. Por lo tanto, un contraejemplo también es un contraargumento, uno que afirma que un argumento específico α no siempre es verdadero, y la evidencia proporcionada es el caso c. Específicamente, para que un caso c sea un contraejemplo de un argumento α, se deben cumplir las siguientes condiciones: α.D c y α.S = c.S, es decir, el caso debe satisfacer la justificación α.D y la solución de c debe ser diferente a la predicha por α. Al intercambiar argumentos y contraargumentos (incluyendo contraejemplos), los agentes pueden discutir sobre la solución correcta de un problema dado, es decir, pueden participar en un proceso de deliberación conjunto. Sin embargo, para hacerlo, necesitan un protocolo de interacción específico, una relación de preferencia entre argumentos contradictorios y una política de decisión para generar contraargumentos (incluidos contraejemplos). En las siguientes secciones presentaremos estos elementos. 5. RELACIÓN DE PREFERENCIA Un argumento específico proporcionado por un agente podría no ser consistente con la información conocida por otros agentes (o incluso con parte de la información conocida por el agente que ha generado la justificación debido al ruido en los datos de entrenamiento). Por esa razón, vamos a definir una relación de preferencia sobre predicciones justificadas contradictorias basadas en casos. Básicamente, definiremos una medida de confianza para cada predicción justificada (que tenga en cuenta los casos poseídos por cada agente), y la predicción justificada con la mayor confianza será la preferida. La idea detrás de la confianza basada en casos es contar cuántos de los casos en una base de casos individual respaldan una predicción justificada, y cuántos de ellos son ejemplos en contra de ella. Cuanto más casos de respaldo, mayor es la confianza; y cuanto más contrajemplos, menor es la confianza. Específicamente, para evaluar la confianza de una predicción justificada α, un agente obtiene el conjunto de casos en su base de casos individuales que están subsumidos por α.D. Con ellos, un agente Ai obtiene los valores Y (sí) y N (no): • Y Ai α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| es el número de casos en la base de casos del agente subsumidos por la justificación α.D que pertenecen a la clase de solución α.S, • NAi α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| es el número de casos en la base de casos del agente subsumidos por la justificación α.D que no pertenecen a esa clase de solución. El Sexto Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 977 + + + + + +  - - + Figura 2: La confianza de los argumentos se evalúa contrastándolos con las bases de casos de los agentes. Un agente estima la confianza de un argumento como: CAi (α) = Y Ai α 1 + Y Ai α + NAi α es decir, la confianza en una predicción justificada es el número de casos que la respaldan dividido por el número de casos que la respaldan más los contraejemplos. Observa que sumamos 1 al denominador, esto es para evitar otorgar confianzas excesivamente altas a predicciones justificadas cuya confianza se ha calculado utilizando un número pequeño de casos. Ten en cuenta que esta corrección sigue la misma idea que la corrección de Laplace para estimar probabilidades. La Figura 2 ilustra la evaluación individual de la confianza de un argumento, en particular, se encuentran tres casos de respaldo y un contraejemplo en la base de casos de los agentes Ai, lo que da una confianza estimada de 0.6. Además, también podemos definir la confianza conjunta de un argumento α como la confianza calculada utilizando los casos presentes en las bases de casos de todos los agentes en el grupo: C(α) = Π Ai α / (1 + Σ Ai α + NAi α). Nótese que, para calcular conjuntamente la confianza conjunta, los agentes solo tienen que hacer públicos los valores de aprobación y rechazo calculados localmente para un argumento dado. En nuestro marco de trabajo, los agentes utilizan esta confianza conjunta como la relación de preferencia: una predicción justificada α se prefiere sobre otra β si C(α) ≥ C(β). GENERACIÓN DE ARGUMENTOS En nuestro marco de trabajo, los argumentos son generados por los agentes a partir de casos, utilizando métodos de aprendizaje. Cualquier método de aprendizaje capaz de proporcionar una predicción justificada puede ser utilizado para generar argumentos. Por ejemplo, los árboles de decisión y LID [2] son métodos de aprendizaje adecuados. Específicamente, en los experimentos reportados en este artículo, los agentes utilizan LID. Por lo tanto, cuando un agente desea generar un argumento que respalde que una clase de solución específica es la solución correcta para un problema P, genera una predicción justificada como se explica en la Sección 3.1. Por ejemplo, la Figura 3 muestra una justificación real generada por LID después de resolver un problema P en el dominio de la identificación de esponjas marinas. En particular, la Figura 3 muestra cómo cuando un agente recibe un nuevo problema para resolver (en este caso, un nuevo esponja para determinar su orden), el agente utiliza LID para generar un argumento (que consiste en una predicción justificada) utilizando los casos en la base de casos del agente. La justificación mostrada en la Figura 3 se puede interpretar diciendo que la solución predicha es hadromerida porque la forma lisa de las megascleras del esqueleto espicular de la esponja es de tipo tilostilo, el esqueleto espicular de la esponja no tiene una longitud uniforme y no hay gemas en las características externas de la esponja. Por lo tanto, el argumento generado será α = A1, P, hadromerida, D1. 6.1 Generación de Contraargumentos Como se mencionó anteriormente, los agentes pueden intentar refutar argumentos generando contraargumentos o encontrando contraejemplos. Permítanos explicar cómo pueden ser generados. Un agente Ai quiere generar un contraargumento β para rebatir un argumento α cuando α está en contradicción con la base de casos locales de Ai. Además, al generar dicho contraargumento β, Ai espera que β sea preferido sobre α. Con ese propósito, presentaremos una política específica para generar contraargumentos basados en el criterio de especificidad [10]. El criterio de especificidad se utiliza ampliamente en marcos deductivos para la argumentación, y establece que entre dos argumentos en conflicto, se debe preferir el más específico ya que, en principio, está más informado. Por lo tanto, se espera que los contraargumentos generados basados en el criterio de especificidad sean preferibles (ya que están más informados) que los argumentos que intentan refutar. Sin embargo, no hay garantía de que tales contraargumentos siempre ganen, ya que, como hemos indicado en la Sección 5, los agentes en nuestro marco utilizan una relación de preferencia basada en la confianza conjunta. Además, uno podría pensar que sería mejor que los agentes generaran contraargumentos basados en la relación de preferencia de confianza conjunta; sin embargo, no es obvio cómo generar contraargumentos basados en la confianza conjunta de manera eficiente, ya que se requiere colaboración para evaluar la confianza conjunta. Por lo tanto, el agente que genera el contraargumento debe comunicarse constantemente con los otros agentes en cada paso del algoritmo de inducción utilizado para generar contraargumentos (actualmente una de nuestras líneas de investigación futuras). Por lo tanto, en nuestro marco de trabajo, cuando un agente quiere generar un contraargumento β a un argumento α, β tiene que ser más específico que α (es decir, α.D < β.D). La generación de contraargumentos utilizando el criterio de especificidad impone algunas restricciones sobre el método de aprendizaje, aunque LID o ID3 pueden adaptarse fácilmente para esta tarea. Por ejemplo, LID es un algoritmo que genera una descripción desde cero y añade heurísticamente características a ese término. Por lo tanto, en cada paso, la descripción se vuelve más específica que en el paso anterior, y el número de casos que se incluyen en esa descripción se reduce. Cuando la descripción cubre solo (o casi solo) casos de una única clase de solución, LID termina y predice esa clase de solución. Para generar un contraargumento a un argumento α, LID solo tiene que usar como punto de partida la descripción α.D en lugar de empezar desde cero. De esta manera, la justificación proporcionada por LID siempre estará subsumida por α.D, y por lo tanto, el contraargumento resultante será más específico que α. Sin embargo, hay que tener en cuenta que LID a veces puede no ser capaz de generar contraargumentos, ya que LID puede no ser capaz de especializar aún más la descripción α.D, o porque el agente Ai no tiene ningún caso en Ci que esté subsumido por α.D. La Figura 4 muestra cómo un agente A2 que no estaba de acuerdo con el argumento mostrado en la Figura 3, genera un contraargumento utilizando LID. Además, la Figura 4 muestra la generación de un contraargumento β1 2 para el argumento α0 1 (en la Figura 3) que es una especialización de α0 1. 978 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Solución: hadromerida Justificación: D1 Esponja Esqueleto espiculado Características externas Características externas Gemmulas: no Esqueleto espiculado Megascleres Longitud uniforme: no Megascleres Forma lisa: tilostilo Caso Base de A1 LID Nueva esponja P Figura 3: Ejemplo de una justificación real generada por LID en el conjunto de datos de esponjas marinas. Específicamente, en nuestros experimentos, cuando un agente Ai quiere refutar un argumento α, utiliza la siguiente política: 1. El Agente Ai utiliza LID para intentar encontrar un contraargumento β más específico que α; si se encuentra, β se envía al otro agente como un contraargumento de α. 2. Si no se encuentra, entonces Ai busca un contraejemplo c ∈ Ci de α. Si se encuentra un caso c, entonces c se envía al otro agente como un contraejemplo de α. Si no se encuentran contraejemplos, entonces Ai no puede refutar el argumento α. 7. APRENDIZAJE MULTIAGENTE BASADO EN ARGUMENTACIÓN El protocolo de interacción de AMAL permite que un grupo de agentes A1, ..., An deliberen sobre la solución correcta de un problema P mediante un proceso de argumentación. Si el proceso de argumentación llega a una solución consensuada, la deliberación conjunta termina; de lo contrario, se utiliza una votación ponderada para determinar la solución conjunta. Además, AMAL también permite a los agentes aprender de los contraejemplos recibidos de otros agentes. El protocolo AMAL consiste en una serie de rondas. En la ronda inicial, cada agente declara cuál es su predicción individual para P. Luego, en cada ronda, un agente puede intentar refutar la predicción hecha por cualquiera de los otros agentes. El protocolo utiliza un mecanismo de paso de token para que los agentes (uno a la vez) puedan enviar contraargumentos o contraejemplos si no están de acuerdo con la predicción hecha por cualquier otro agente. Específicamente, a cada agente se le permite enviar un contraargumento o contraejemplo cada vez que recibe el token (nota que esta restricción es solo para simplificar el protocolo, y que no restringe la cantidad de contraargumentos que un agente puede enviar, ya que pueden ser retrasados para rondas posteriores). Cuando un agente recibe un contraargumento o contraejemplo, informa a los otros agentes si acepta el contraargumento (y cambia su predicción) o no. Además, los agentes también tienen la oportunidad de responder a los contraargumentos cuando reciben el token, intentando generar un contraargumento al contraargumento. Cuando todos los agentes hayan tenido el token una vez, el token regresa al primer agente, y así sucesivamente. Si en algún momento del protocolo, todos los agentes están de acuerdo o durante las últimas n rondas ningún agente ha generado ninguna contra-argumentación, el protocolo termina. Además, si al final de la argumentación los agentes no han llegado a un acuerdo, entonces se utiliza un mecanismo de votación que utiliza la confianza de cada predicción como pesos para decidir la solución final (Así, AMAL sigue el mismo mecanismo que los comités humanos, primero cada miembro individual de un comité expone sus argumentos y discute los de los otros miembros (deliberación conjunta), y si no se alcanza un consenso, entonces se requiere un mecanismo de votación). En cada iteración, los agentes pueden usar los siguientes performativos: • afirmar(α): la predicción justificada que se mantendrá durante la próxima ronda será α. Un agente solo puede tener una única predicción en cada ronda, por lo tanto, si se envían múltiples afirmaciones, solo se considera la última como la predicción actualmente mantenida. • rebut(β, α): el agente ha encontrado un contraargumento β a la predicción α. Definiremos Ht = αt 1, ..., αt n como las predicciones que cada uno de los n agentes tiene en una ronda t. Además, también definiremos contradict(αt i) = {α ∈ Ht|α.S = αt i.S} como el conjunto de argumentos contradictorios para un agente Ai en una ronda t, es decir, el conjunto de argumentos en la ronda t que respaldan una clase de solución diferente a la de αt i. El protocolo se inicia porque uno de los agentes recibe un problema P que debe ser resuelto. Después de eso, el agente informa a todos los demás agentes sobre el problema P a resolver, y el protocolo comienza: 1. En la ronda t = 0, cada uno de los agentes resuelve individualmente P y construye una predicción justificada utilizando su propio método de RBC. Entonces, cada agente Ai envía el assertivo performativo assert(α0 i) a los otros agentes. Por lo tanto, los agentes conocen H0 = α0 i , ..., α0 n. Una vez que todas las predicciones hayan sido enviadas, el token se le da al primer agente A1. 2. En cada ronda t (distinta de 0), los agentes verifican si sus argumentos en Ht coinciden. Si lo hacen, el protocolo avanza al paso 5. Además, si durante las últimas n rondas ningún agente ha enviado ningún contraejemplo o contraargumento, el protocolo también avanza al paso 5. De lo contrario, el agente Ai propietario del token intenta generar un contraargumento para cada uno de los argumentos opuestos en contradict(αt i) ⊆ Ht (ver Sección 6.1). Entonces, se selecciona el contraargumento βt i contra la predicción αt j con la menor confianza C(αt j) (ya que αt j es la predicción más probable de ser refutada con éxito). • Si βt i es un contraargumento, entonces, Ai compara localmente αt i con βt i evaluando su confianza contra su base de casos individuales Ci (ver Sección 5) (nótese que Ai está comparando su argumento previo con el contraargumento que Ai mismo acaba de generar y que trata sobre The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 979 Esponja Esqueleto espiculado Características externas Características externas Gemas: no Crecimiento: Esqueleto espiculado Megascleres Longitud uniforme: no Megascleres Forma lisa: tilostilo Crecimiento Crecimiento: masivo Caso Base de A2 LID Solución: astrophorida Justificación: D2 Figura 4: Generación de un contraargumento utilizando LID en el conjunto de datos de esponjas. para enviar a Aj. Si CAi (βt i) > CAi (αt i), entonces Ai considera que βt i es más fuerte que su argumento anterior, cambia su argumento a βt i enviando assert(βt i) al resto de los agentes (la intuición detrás de esto es que dado que un contraargumento también es un argumento, Ai verifica si el nuevo contraargumento es un mejor argumento que el que sostenía anteriormente) y rebut(βt i, αt j) a Aj. De lo contrario (es decir, Si CAi (βt i ) ≤ CAi (αt i), Ai solo enviará rebut(βt i , αt j) a Aj. En cualquiera de las dos situaciones, el protocolo avanza al paso 3. • Si βt i es un contraejemplo c, entonces Ai envía una refutación(c, αt j) a Aj. El protocolo avanza al paso 4. • Si Ai no puede generar ningún contraargumento o contraejemplo, el token se envía al siguiente agente, comienza una nueva ronda t + 1, y el protocolo avanza al estado 2. 3. El agente Aj que ha recibido el contraargumento βt i, lo compara localmente con su propio argumento, αt j, evaluando localmente su confianza. Si CAj (βt i ) > CAj (αt j), entonces Aj aceptará el contraargumento como más fuerte que su propio argumento, y enviará assert(βt i ) a los otros agentes. De lo contrario (es decir, Si CAj (βt i ) ≤ CAj (αt j)), Aj no aceptará el contraargumento e informará a los otros agentes en consecuencia. Cualquiera de las dos situaciones inicia una nueva ronda t + 1, Ai envía el token al siguiente agente y el protocolo vuelve al estado 2. 4. El agente Aj que ha recibido el contraejemplo c lo retiene en su base de casos y genera un nuevo argumento αt+1 j que tiene en cuenta c, e informa al resto de los agentes enviando assert(αt+1 j) a todos ellos. Entonces, Ai envía el token al siguiente agente, comienza una nueva ronda t + 1 y el protocolo vuelve al paso 2. 5. El protocolo finaliza generando una predicción conjunta, de la siguiente manera: si los argumentos en Ht están de acuerdo, entonces su predicción es la predicción conjunta; de lo contrario, se utiliza un mecanismo de votación para decidir la predicción conjunta. El mecanismo de votación utiliza la medida de confianza conjunta como los pesos de votación, de la siguiente manera: S = arg max Sk∈S αi∈Ht|αi.S=Sk C(αi) Además, para evitar iteraciones infinitas, si un agente envía dos veces el mismo argumento o contraargumento al mismo agente, el mensaje no se considera. 8. EJEMPLIFICACIÓN Consideremos un sistema compuesto por tres agentes A1, A2 y A3. Uno de los agentes, A1, recibe un problema P para resolver y decide usar AMAL para resolverlo. Por esa razón, invita a A2 y A3 a participar en el proceso de argumentación. Aceptan la invitación y comienza el <br>protocolo de argumentación</br>. Inicialmente, cada agente genera su predicción individual para P y la transmite a los otros agentes. Por lo tanto, todos ellos pueden calcular H0 = α0 1, α0 2, α0 3. En particular, en este ejemplo: • α0 1 = A1, P, hadromerida, D1 • α0 2 = A2, P, astrophorida, D2 • α0 3 = A3, P, axinellida, D3 A1 comienza poseyendo el token e intenta generar argumentos en contra de α0 2 y α0 3, pero no tiene éxito; sin embargo, tiene un contraejemplo c13 para α0 3. Por lo tanto, A1 envía el mensaje rebut(c13, α03) a A3. A3 incorpora c13 en su base de casos e intenta resolver el problema P nuevamente, ahora teniendo en cuenta c13. A3 elabora la predicción justificada α1 3 = A3, P, hadromerida, D4, y la transmite al resto de los agentes con el mensaje assert(α1 3). Por lo tanto, todos ellos conocen el nuevo H1 = α0 1, α0 2, α1 3. La ronda 1 comienza y A2 obtiene el token. A2 intenta generar contraargumentos para α0 1 y α1 3 y solo logra generar un contraargumento β1 2 = A2, P, astrophorida, D5 contra α1 3. El contraargumento se envía a A3 con el mensaje rebut(β1 2 , α1 3). El agente A3 recibe el contraargumento y evalúa su confianza local. El resultado es que la confianza individual del contraargumento β1 2 es menor que la confianza local de α1 3. Por lo tanto, A3 no acepta el contraargumento, y así H2 = α0 1, α0 2, α1 3. La ronda 2 comienza y A3 recibe el token. A3 genera un contraargumento β2 3 = A3, P, hadromerida, D6 para α0 2 y lo envía a A2 con el mensaje refutar(β2 3 , α0 2). El agente A2 recibe el contraargumento y evalúa su confianza local. El resultado es que la confianza local del contraargumento β2 3 es mayor que la confianza local de α0 2. Por lo tanto, A2 acepta el contraargumento e informa al resto de los agentes con el mensaje assert(β2 3). Después de eso, H3 = α0 1, β2 3 , α1 3. En la Ronda 3, dado que todos los agentes están de acuerdo (todas las predicciones justificadas en H3 predicen hadromerida como la clase de solución), el protocolo finaliza, y A1 (el agente que recibió el problema) considera hadromerida como la solución conjunta para el problema P. 9. EVALUACIÓN EXPERIMENTAL 980 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) SPONGE 75 77 79 81 83 85 87 89 91 2 3 4 5 AMAL Votación Individual SOYBEAN 55 60 65 70 75 80 85 90 2 3 4 5 AMAL Votación Individual Figura 5: Precisión individual y conjunta para 2 a 5 agentes. En esta sección evaluamos empíricamente el marco de argumentación AMAL. Hemos realizado experimentos en dos conjuntos de datos diferentes: soja (del repositorio de aprendizaje automático de UCI) y esponja (un conjunto de datos relacional). El conjunto de datos de soja tiene 307 ejemplos y 19 clases de solución, mientras que el conjunto de datos de esponja tiene 280 ejemplos y 3 clases de solución. En una ejecución experimental, el conjunto de datos se divide en 2 conjuntos: el conjunto de entrenamiento y el conjunto de prueba. Los ejemplos del conjunto de entrenamiento están distribuidos entre 5 agentes diferentes sin replicación, es decir, no hay ningún ejemplo compartido por dos agentes. En la etapa de pruebas, los problemas en el conjunto de pruebas llegan aleatoriamente a uno de los agentes, y su objetivo es predecir la solución correcta. Los experimentos están diseñados para probar dos hipótesis: (H1) que la argumentación es un marco útil para la deliberación conjunta y puede mejorar sobre otros métodos típicos como la votación; y (H2) que el aprendizaje a través de la comunicación mejora el rendimiento individual de un agente de aprendizaje que participa en un proceso de argumentación. Además, también esperamos que la mejora lograda a través de la argumentación aumente a medida que aumente el número de agentes que participan en la argumentación (ya que se tomará en cuenta más información). En relación con H1 (la argumentación es un marco útil para la deliberación conjunta), realizamos 4 experimentos, utilizando 2, 3, 4 y 5 agentes respectivamente (en todos los experimentos cada agente tiene un 20% de los datos de entrenamiento, ya que el entrenamiento siempre se distribuye entre 5 agentes). La Figura 5 muestra el resultado de esos experimentos en los conjuntos de datos de esponjas y soja. La precisión de la clasificación se representa en el eje vertical, y en el eje horizontal se muestra el número de agentes que participaron en los procesos de argumentación. Para cada número de agentes, se muestran tres barras: individual, Voting y AMAL. La barra individual muestra la precisión promedio de las predicciones de los agentes individuales; la barra de votación muestra la precisión promedio de la predicción conjunta lograda mediante votación pero sin argumentación; y finalmente la barra de AMAL muestra la precisión promedio de la predicción conjunta utilizando argumentación. Los resultados mostrados son el promedio de 5 ejecuciones de validación cruzada de 10 pliegues. La Figura 5 muestra que la colaboración (votación y AMAL) supera el rendimiento de la resolución individual de problemas. Además, como esperábamos, la precisión mejora a medida que más agentes colaboran, ya que se tiene en cuenta más información. También podemos ver que AMAL siempre supera al voto estándar, demostrando que las decisiones conjuntas se basan en una mejor información proporcionada por el proceso de argumentación. Por ejemplo, la precisión conjunta para 2 agentes en el conjunto de datos de esponjas es del 87.57% para AMAL y del 86.57% para la votación (mientras que la precisión individual es solo del 80.07%). Además, la mejora lograda por AMAL sobre Voting es aún mayor en el conjunto de datos de soja. La razón es que el conjunto de datos de soja es más difícil (en el sentido de que los agentes necesitan más datos para producir buenas predicciones). Estos resultados experimentales muestran que AMAL aprovecha de manera efectiva la oportunidad de mejora: la precisión es mayor solo porque más agentes han cambiado su opinión durante la argumentación (de lo contrario, lograrían el mismo resultado que la Votación). En relación con H2 (el aprendizaje a partir de la comunicación en procesos de argumentación mejora la predicción individual), llevamos a cabo el siguiente experimento: inicialmente, distribuimos un 25% del conjunto de entrenamiento entre los cinco agentes; después, el resto de los casos en el conjunto de entrenamiento se envía a los agentes uno por uno; cuando un agente recibe un nuevo caso de entrenamiento, tiene varias opciones: el agente puede descartarlo, el agente puede retenerlo, o el agente puede usarlo para participar en un proceso de argumentación. La Figura 6 muestra el resultado de ese experimento para los dos conjuntos de datos. La Figura 6 contiene tres gráficos, donde NL (no aprendizaje) muestra la precisión de un agente sin aprendizaje alguno; L (aprendizaje) muestra la evolución de la precisión de clasificación individual cuando los agentes aprenden reteniendo los casos de entrenamiento que reciben individualmente (notar que cuando todos los casos de entrenamiento han sido retenidos al 100%, la precisión debería ser igual a la de la Figura 5 para los agentes individuales); y finalmente LFC (aprendizaje a partir de la comunicación) muestra la evolución de la precisión de clasificación individual de los agentes de aprendizaje que también aprenden reteniendo los contraejemplos recibidos durante la argumentación (es decir, aprenden tanto de ejemplos de entrenamiento como de contraejemplos). La Figura 6 muestra que si un agente Ai también aprende de la comunicación, Ai puede mejorar significativamente su rendimiento individual con solo un pequeño número de casos adicionales (seleccionados como contraejemplos relevantes para Ai durante la argumentación). Por ejemplo, en el conjunto de datos de soja, los agentes individuales han logrado una precisión del 70.62% cuando también aprenden de la comunicación, en comparación con una precisión del 59.93% cuando solo aprenden de su experiencia individual. El número de casos aprendidos a través de la comunicación depende de las propiedades del conjunto de datos: en el conjunto de datos de esponjas, los agentes solo han retenido muy pocos casos adicionales y han mejorado significativamente la precisión individual; es decir, retienen en promedio 59.96 casos (en comparación con los 50.4 casos retenidos si no aprenden de la comunicación). En el conjunto de datos de soja se aprenden más contraejemplos para mejorar significativamente la precisión individual, es decir, retienen en promedio 87.16 casos (en comparación con 55.27 casos retenidos si no aprenden de la comunicación). Finalmente, el hecho de que ambos conjuntos de datos muestren una mejora significativa señala la naturaleza adaptativa del enfoque basado en la argumentación para el aprendizaje a partir de la comunicación: los casos útiles se seleccionan como contraejemplos (y no más de los necesarios), y tienen el efecto deseado. 10. TRABAJO RELACIONADO En cuanto al RCB en un entorno multiagente, la primera investigación se centró en la recuperación de casos negociada [11] entre grupos de agentes. Nuestro trabajo sobre el aprendizaje basado en casos multiagente comenzó en 1999; más tarde Mc Ginty y Smyth presentaron un enfoque colaborativo de CBR multiagente (CCBR) para la planificación. Finalmente, otro enfoque interesante es el razonamiento de múltiples bases de casos (MCBR) [5], que trata sobre The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 981 ESPONJA 60 65 70 75 80 85 25% 40% 55% 70% 85% 100% LFC L NL SOJA 20 30 40 50 60 70 80 90 25% 40% 55% 70% 85% 100% LFC L NL Figura 6: Aprendizaje a partir de la comunicación resultante de la argumentación en un sistema compuesto por 5 agentes. sistemas distribuidos donde hay varias bases de casos disponibles para la misma tarea y aborda los problemas de adaptación entre bases de casos. La principal diferencia es que nuestro enfoque MAC es una forma de distribuir el proceso de Reutilización de CBR (utilizando un sistema de votación) mientras que la Recuperación se realiza individualmente por cada agente; sin embargo, los otros enfoques de CBR multiagente se centran en distribuir el proceso de Recuperación. La investigación sobre la argumentación en MAS se centra en varios temas como a) lógicas, protocolos y lenguajes que respaldan la argumentación, b) selección de argumentos y c) interpretación de argumentos. Los enfoques para la lógica y los lenguajes que respaldan la argumentación incluyen la lógica no definitiva [4] y los modelos BDI [13]. Aunque la selección de argumentos es un aspecto clave de la argumentación automatizada (ver [12] y [13]), la mayoría de las investigaciones se han centrado en las relaciones de preferencia entre argumentos. En nuestro marco de trabajo hemos abordado tanto la selección de argumentos como las relaciones de preferencia utilizando un enfoque basado en casos. 11. CONCLUSIONES Y TRABAJO FUTURO En este artículo hemos presentado un marco basado en argumentación para el aprendizaje multiagente. Específicamente, hemos presentado AMAL, un marco que permite a un grupo de agentes de aprendizaje discutir sobre la solución de un problema dado y hemos demostrado cómo las capacidades de aprendizaje pueden ser utilizadas para generar argumentos y contraargumentos. La evaluación experimental muestra que la cantidad aumentada de información proporcionada a los agentes por el proceso de argumentación aumenta su precisión predictiva, especialmente cuando un número adecuado de agentes participa en la argumentación. Las principales contribuciones de este trabajo son: a) un marco de argumentación para agentes de aprendizaje; b) una relación de preferencia basada en casos sobre argumentos, basada en el cálculo de una estimación general de confianza de los argumentos; c) una política basada en casos para generar contraargumentos y seleccionar contraejemplos; y d) un enfoque basado en la argumentación para aprender a partir de la comunicación. Finalmente, en los experimentos presentados aquí, un agente de aprendizaje retendría todos los contraejemplos presentados por el otro agente; sin embargo, esta es una política de retención de casos muy simple, y nos gustaría experimentar con políticas más informadas, con el objetivo de que los agentes de aprendizaje individuales puedan mejorar significativamente utilizando solo un pequeño conjunto de casos propuestos por otros agentes. Finalmente, nuestro enfoque se centra en el aprendizaje perezoso, y los trabajos futuros tienen como objetivo incorporar el aprendizaje inductivo ansioso dentro del marco argumentativo para el aprendizaje a partir de la comunicación. 12. REFERENCIAS [1] Agnar Aamodt y Enric Plaza. Razonamiento basado en casos: Aspectos fundamentales, variaciones metodológicas y enfoques de sistemas. Comunicaciones de Inteligencia Artificial, 7(1):39-59, 1994. [2] E. Armengol y E. Plaza. Inducción perezosa de descripciones para el aprendizaje basado en casos relacionales. En ECML2001, páginas 13-24, 2001. [3] Gerhard Brewka. Sistemas de argumentación dinámica: Un modelo formal de procesos de argumentación basado en el cálculo de situaciones. Revista de Lógica y Computación, 11(2):257-282, 2001. [4] Carlos I. Chesñevar y Guillermo R. Simari. Formalizando la argumentación no definitiva utilizando sistemas deductivos etiquetados. Revista de Ciencias de la Computación y Tecnología, 1(4):18-33, 2000. [5] D. Leake y R. Sooriamurthi. Seleccionando estrategias automáticamente para el razonamiento multi-base de casos. En S. Craw y A. Preece, editores, ECCBR2002, páginas 204-219, Berlín, 2002. Springer Verlag. [6] Francisco J. Martín, Enric Plaza y Josep-Lluis Arcos. Reutilización de conocimiento y experiencia a través de comunicaciones entre agentes competentes (pares). Revista Internacional de Ingeniería de Software e Ingeniería del Conocimiento, 9(3):319-341, 1999. [7] Lorraine McGinty y Barry Smyth. Razonamiento basado en casos colaborativo: Aplicaciones en la planificación de rutas personalizadas. En I. Watson y Q. Yang, editores, ICCBR, número 2080 en LNAI, páginas 362-376. Springer-Verlag, 2001. [8] Santi Ontañón y Enric Plaza. Aprendizaje multiagente basado en justificaciones. En ICML2003, páginas 576-583. Morgan Kaufmann, 2003. [9] Enric Plaza, Eva Armengol y Santiago Ontañón. El poder explicativo de la similitud simbólica en el razonamiento basado en casos. Revisión de Inteligencia Artificial, 24(2):145-161, 2005. [10] David Poole. En la comparación de teorías: prefiriendo la explicación más específica. En IJCAI-85, páginas 144-147, 1985. [11] M V Nagendra Prassad, Victor R Lesser y Susan Lander. Recuperación y razonamiento en bases de casos distribuidas. Informe técnico, Departamento de Ciencias de la Computación de UMass, 1995. [12] K. Sycara S. Kraus y A. Evenchik. Alcanzando acuerdos a través de la argumentación: un modelo lógico y su implementación. Revista de Inteligencia Artificial, 104:1-69, 1998. [13] N. R. Jennings, S. Parsons, C. Sierra. Agentes que razonan y negocian mediante argumentos. Revista de Lógica y Computación, 8:261-292, 1998. [14] Bruce A. Wooley. Componente de explicación de sistemas de software. ACM CrossRoads, 5.1, 1998. 982 The Sixth Intl. \n\nACM CrossRoads, 5.1, 1998. 982 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "collaboration": {
            "translated_key": "colaboración",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning and Joint Deliberation through Argumentation in Multi-Agent Systems Santi Ontañón CCL, Cognitive Computing Lab Georgia Institute of Technology Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Artificial Intelligence Research Institute CSIC, Spanish Council for Scientific Research Campus UAB, 08193 Bellaterra, Catalonia (Spain) enric@iiia.csic.es ABSTRACT In this paper we will present an argumentation framework for learning agents (AMAL) designed for two purposes: (1) for joint deliberation, and (2) for learning from communication.",
                "The AMAL framework is completely based on learning from examples: the argument preference relation, the argument generation policy, and the counterargument generation policy are case-based techniques.",
                "For join deliberation, learning agents share their experience by forming a committee to decide upon some joint decision.",
                "We experimentally show that the argumentation among committees of agents improves both the individual and joint performance.",
                "For learning from communication, an agent engages into arguing with other agents in order to contrast its individual hypotheses and receive counterexamples; the argumentation process improves their learning scope and individual performance.",
                "Categories and Subject Descriptors I.2.6 [Artificial Intelligence]: Learning; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems, Intelligent Agents 1.",
                "INTRODUCTION Argumentation frameworks for multi-agent systems can be used for different purposes like joint deliberation, persuasion, negotiation, and conflict resolution.",
                "In this paper we will present an argumentation framework for learning agents, and show that it can be used for two purposes: (1) joint deliberation, and (2) learning from communication.",
                "Argumentation-based joint deliberation involves discussion over the outcome of a particular situation or the appropriate course of action for a particular situation.",
                "Learning agents are capable of learning from experience, in the sense that past examples (situations and their outcomes) are used to predict the outcome for the situation at hand.",
                "However, since individual agents experience may be limited, individual knowledge and prediction accuracy is also limited.",
                "Thus, learning agents that are capable of arguing their individual predictions with other agents may reach better prediction accuracy after such an argumentation process.",
                "Most existing argumentation frameworks for multi-agent systems are based on deductive logic or some other deductive logic formalism specifically designed to support argumentation, such as default logic [3]).",
                "Usually, an argument is seen as a logical statement, while a counterargument is an argument offered in opposition to another argument [4, 13]; agents use a preference relation to resolve conflicting arguments.",
                "However, logic-based argumentation frameworks assume agents with preloaded knowledge and preference relation.",
                "In this paper, we focus on an Argumentation-based Multi-Agent Learning (AMAL) framework where both knowledge and preference relation are learned from experience.",
                "Thus, we consider a scenario with agents that (1) work in the same domain using a shared ontology, (2) are capable of learning from examples, and (3) communicate using an argumentative framework.",
                "Having learning capabilities allows agents effectively use a specific form of counterargument, namely the use of counterexamples.",
                "Counterexamples offer the possibility of agents learning during the argumentation process.",
                "Moreover, learning agents allow techniques that use learnt experience to generate adequate arguments and counterarguments.",
                "Specifically, we will need to address two issues: (1) how to define a technique to generate arguments and counterarguments from examples, and (2)how to define a preference relation over two conflicting arguments that have been induced from examples.",
                "This paper presents a case-based approach to address both issues.",
                "The agents use case-based reasoning (CBR) [1] to learn from past cases (where a case is a situation and its outcome) in order to predict the outcome of a new situation.",
                "We propose an argumentation protocol inside the AMAL framework at supports agents in reaching a joint prediction over a specific situation or problem - moreover, the reasoning needed to support the argumentation process will also be based on cases.",
                "In particular, we present two case-based measures, one for generating the arguments and counterarguments adequate to a particular situation and another for determining preference relation among arguments.",
                "Finally, we evaluate (1) if argumentation between learning agents can produce a joint prediction that improves over individual learning performance and (2) if learning from the counterexamples conveyed during the argumentation process increases the individual performance with precisely those cases being used while arguing among them.",
                "The paper is structured as follows.",
                "Section 2 discusses the relation among argumentation, <br>collaboration</br> and learning.",
                "Then Section 3 introduces our multi-agent CBR (MAC) framework and the notion of justified prediction.",
                "After that, Section 4 formally defines our argumentation framework.",
                "Sections 5 and 6 present our case-based preference relation and argument generation policies respectively.",
                "Later, Section 7 presents the argumentation protocol in our AMAL framework.",
                "After that, Section 8 presents an exemplification of the argumentation framework.",
                "Finally, Section 9 presents an empirical evaluation of our two main hypotheses.",
                "The paper closes with related work and conclusions sections. 2.",
                "ARGUMENTATION,<br>collaboration</br> AND LEARNING Both learning and <br>collaboration</br> are ways in which an agent can improve individual performance.",
                "In fact, there is a clear parallelism between learning and <br>collaboration</br> in multi-agent systems, since both are ways in which agents can deal with their shortcomings.",
                "Let us show which are the main motivations that an agent can have to learn or to collaborate. • Motivations to learn: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems. • Motivations to collaborate: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems, - Increase the range of accessible resources.",
                "Looking at the above lists of motivation, we can easily see that learning and <br>collaboration</br> are very related in multi-agent systems.",
                "In fact, with the exception of the last item in the motivations to collaborate list, they are two extremes of a continuum of strategies to improve performance.",
                "An agent may choose to increase performance by learning, by collaborating, or by finding an intermediate point that combines learning and <br>collaboration</br> in order to improve performance.",
                "In this paper we will propose AMAL, an argumentation framework for learning agents, and will also also show how AMAL can be used both for learning from communication and for solving problems in a collaborative way: • Agents can solve problems in a collaborative way via engaging an argumentation process about the prediction for the situation at hand.",
                "Using this <br>collaboration</br>, the prediction can be done in a more informed way, since the information known by several agents has been taken into account. • Agents can also learn from communication with other agents by engaging an argumentation process.",
                "Agents that engage in such argumentation processes can learn from the arguments and counterexamples received from other agents, and use this information for predicting the outcomes of future situations.",
                "In the rest of this paper we will propose an argumentation framework and show how it can be used both for learning and for solving problems in a collaborative way. 3.",
                "MULTI-AGENT CBR SYSTEMS A Multi-Agent Case Based Reasoning System (MAC) M = {(A1, C1), ..., (An, Cn)} is a multi-agent system composed of A = {Ai, ..., An}, a set of CBR agents, where each agent Ai ∈ A possesses an individual case base Ci.",
                "Each individual agent Ai in a MAC is completely autonomous and each agent Ai has access only to its individual and private case base Ci.",
                "A case base Ci = {c1, ..., cm} is a collection of cases.",
                "Agents in a MAC system are able to individually solve problems, but they can also collaborate with other agents to solve problems.",
                "In this framework, we will restrict ourselves to analytical tasks, i.e. tasks like classification, where the solution of a problem is achieved by selecting a solution class from an enumerated set of solution classes.",
                "In the following we will note the set of all the solution classes by S = {S1, ..., SK }.",
                "Therefore, a case c = P, S is a tuple containing a case description P and a solution class S ∈ S. In the following, we will use the terms problem and case description indistinctly.",
                "Moreover, we will use the dot notation to refer to elements inside a tuple; e.g., to refer to the solution class of a case c, we will write c.S.",
                "Therefore, we say a group of agents perform joint deliberation, when they collaborate to find a joint solution by means of an argumentation process.",
                "However, in order to do so, an agent has to be able to justify its prediction to the other agents (i.e. generate an argument for its predicted solution that can be examined and critiqued by the other agents).",
                "The next section addresses this issue. 3.1 Justified Predictions Both expert systems and CBR systems may have an explanation component [14] in charge of justifying why the system has provided a specific answer to the user.",
                "The line of reasoning of the system can then be examined by a human expert, thus increasing the reliability of the system.",
                "Most of the existing work on explanation generation focuses on generating explanations to be provided to the user.",
                "However, in our approach we use explanations (or justifications) as a tool for improving communication and coordination among agents.",
                "We are interested in justifications since they can be used as arguments.",
                "For that purpose, we will benefit from the ability of some machine learning methods to provide justifications.",
                "A justification built by a CBR method after determining that the solution of a particular problem P was Sk is a description that contains the relevant information from the problem P that the CBR method has considered to predict Sk as the solution of P. In particular, CBR methods work by retrieving similar cases to the problem at hand, and then reusing their solutions for the current problem, expecting that since the problem and the cases are similar, the solutions will also be similar.",
                "Thus, if a CBR method has retrieved a set of cases C1, ..., Cn to solve a particular problem P the justification built will contain the relevant information from the problem P that made the CBR system retrieve that particular set of cases, i.e. it will contain the relevant information that P and C1, ..., Cn have in common.",
                "For example, Figure 1 shows a justification build by a CBR system for a toy problem (in the following sections we will show justifications for real problems).",
                "In the figure, a problem has two attributes (Traffic_light, and Cars_passing), the retrieval mechanism of the CBR system notices that by considering only the attribute Traffic_light, it can retrieve two cases that predict the same solution: wait.",
                "Thus, since only this attribute has been used, it is the only one appearing in the justification.",
                "The values of the rest of attributes are irrelevant, since whatever their value the solution class would have been the same. 976 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Problem Traffic_light: red Cars_passing: no Case 1 Traffic_light: red Cars_passing: no Solution: wait Case 3 Traffic_light: red Cars_passing: yes Solution: wait Case 4 Traffic_light: green Cars_passing: yes Solution: wait Case 2 Traffic_light: green Cars_passing: no Solution: cross Retrieved cases Solution: wait Justification Traffic_light: red Figure 1: An example of justification generation in a CBR system.",
                "Notice that, since the only relevant feature to decide is Traffic_light (the only one used to retrieve cases), it is the only one appearing in the justification.",
                "In general, the meaning of a justification is that all (or most of) the cases in the case base of an agent that satisfy the justification (i.e. all the cases that are subsumed by the justification) belong to the predicted solution class.",
                "In the rest of the paper, we will use to denote the subsumption relation.",
                "In our work, we use LID [2], a CBR method capable of building symbolic justifications such as the one exemplified in Figure 1.",
                "When an agent provides a justification for a prediction, the agent generates a justified prediction: DEFINITION 3.1.",
                "A Justified Prediction is a tuple J = A, P, S, D where agent A considers S the correct solution for problem P, and that prediction is justified a symbolic description D such that J.D J.P. Justifications can have many uses for CBR systems [8, 9].",
                "In this paper, we are going to use justifications as arguments, in order to allow learning agents to engage in argumentation processes. 4.",
                "ARGUMENTS AND COUNTERARGUMENTS For our purposes an argument α generated by an agent A is composed of a statement S and some evidence D supporting S as correct.",
                "In the remainder of this section we will see how this general definition of argument can be instantiated in specific kind of arguments that the agents can generate.",
                "In the context of MAC systems, agents argue about predictions for new problems and can provide two kinds of information: a) specific cases P, S , and b) justified predictions: A, P, S, D .",
                "Using this information, we can define three types of arguments: justified predictions, counterarguments, and counterexamples.",
                "A justified prediction α is generated by an agent Ai to argue that Ai believes that the correct solution for a given problem P is α.S, and the evidence provided is the justification α.D.",
                "In the example depicted in Figure 1, an agent Ai may generate the argument α = Ai, P, Wait, (Traffic_light = red) , meaning that the agent Ai believes that the correct solution for P is Wait because the attribute Traffic_light equals red.",
                "A counterargument β is an argument offered in opposition to another argument α.",
                "In our framework, a counterargument consists of a justified prediction Aj, P, S , D generated by an agent Aj with the intention to rebut an argument α generated by another agent Ai, that endorses a solution class S different from that of α.S for the problem at hand and justifies this with a justification D .",
                "In the example in Figure 1, if an agent generates the argument α = Ai, P, Walk, (Cars_passing = no) , an agent that thinks that the correct solution is Wait might answer with the counterargument β = Aj, P, Wait, (Cars_passing = no ∧ Traffic_light = red) , meaning that, although there are no cars passing, the traffic light is red, and the street cannot be crossed.",
                "A counterexample c is a case that contradicts an argument α.",
                "Thus a counterexample is also a counterargument, one that states that a specific argument α is not always true, and the evidence provided is the case c. Specifically, for a case c to be a counterexample of an argument α, the following conditions have to be met: α.D c and α.S = c.S, i.e. the case must satisfy the justification α.D and the solution of c must be different than the predicted by α.",
                "By exchanging arguments and counterarguments (including counterexamples), agents can argue about the correct solution of a given problem, i.e. they can engage a joint deliberation process.",
                "However, in order to do so, they need a specific interaction protocol, a preference relation between contradicting arguments, and a decision policy to generate counterarguments (including counterexamples).",
                "In the following sections we will present these elements. 5.",
                "PREFERENCE RELATION A specific argument provided by an agent might not be consistent with the information known to other agents (or even to some of the information known by the agent that has generated the justification due to noise in training data).",
                "For that reason, we are going to define a preference relation over contradicting justified predictions based on cases.",
                "Basically, we will define a confidence measure for each justified prediction (that takes into account the cases owned by each agent), and the justified prediction with the highest confidence will be the preferred one.",
                "The idea behind case-based confidence is to count how many of the cases in an individual case base endorse a justified prediction, and how many of them are counterexamples of it.",
                "The more the endorsing cases, the higher the confidence; and the more the counterexamples, the lower the confidence.",
                "Specifically, to assess the confidence of a justified prediction α, an agent obtains the set of cases in its individual case base that are subsumed by α.D.",
                "With them, an agent Ai obtains the Y (aye) and N (nay) values: • Y Ai α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by the justification α.D that belong to the solution class α.S, • NAi α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by justification α.D that do not belong to that solution class.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 977 + + + + + +  - - + Figure 2: Confidence of arguments is evaluated by contrasting them against the case bases of the agents.",
                "An agent estimates the confidence of an argument as: CAi (α) = Y Ai α 1 + Y Ai α + NAi α i.e. the confidence on a justified prediction is the number of endorsing cases divided by the number of endorsing cases plus counterexamples.",
                "Notice that we add 1 to the denominator, this is to avoid giving excessively high confidences to justified predictions whose confidence has been computed using a small number of cases.",
                "Notice that this correction follows the same idea than the Laplace correction to estimate probabilities.",
                "Figure 2 illustrates the individual evaluation of the confidence of an argument, in particular, three endorsing cases and one counterexample are found in the case base of agents Ai, giving an estimated confidence of 0.6 Moreover, we can also define the joint confidence of an argument α as the confidence computed using the cases present in the case bases of all the agents in the group: C(α) = i Y Ai α 1 + i Y Ai α + NAi α Notice that, to collaboratively compute the joint confidence, the agents only have to make public the aye and nay values locally computed for a given argument.",
                "In our framework, agents use this joint confidence as the preference relation: a justified prediction α is preferred over another one β if C(α) ≥ C(β). 6.",
                "GENERATION OF ARGUMENTS In our framework, arguments are generated by the agents from cases, using learning methods.",
                "Any learning method able to provide a justified prediction can be used to generate arguments.",
                "For instance, decision trees and LID [2] are suitable learning methods.",
                "Specifically, in the experiments reported in this paper agents use LID.",
                "Thus, when an agent wants to generate an argument endorsing that a specific solution class is the correct solution for a problem P, it generates a justified prediction as explained in Section 3.1.",
                "For instance, Figure 3 shows a real justification generated by LID after solving a problem P in the domain of marine sponges identification.",
                "In particular, Figure 3 shows how when an agent receives a new problem to solve (in this case, a new sponge to determine its order), the agent uses LID to generate an argument (consisting on a justified prediction) using the cases in the case base of the agent.",
                "The justification shown in Figure 3 can be interpreted saying that the predicted solution is hadromerida because the smooth form of the megascleres of the spiculate skeleton of the sponge is of type tylostyle, the spikulate skeleton of the sponge has no uniform length, and there is no gemmules in the external features of the sponge.",
                "Thus, the argument generated will be α = A1, P, hadromerida, D1 . 6.1 Generation of Counterarguments As previously stated, agents may try to rebut arguments by generating counterargument or by finding counterexamples.",
                "Let us explain how they can be generated.",
                "An agent Ai wants to generate a counterargument β to rebut an argument α when α is in contradiction with the local case base of Ai.",
                "Moreover, while generating such counterargument β, Ai expects that β is preferred over α.",
                "For that purpose, we will present a specific policy to generate counterarguments based on the specificity criterion [10].",
                "The specificity criterion is widely used in deductive frameworks for argumentation, and states that between two conflicting arguments, the most specific should be preferred since it is, in principle, more informed.",
                "Thus, counterarguments generated based on the specificity criterion are expected to be preferable (since they are more informed) to the arguments they try to rebut.",
                "However, there is no guarantee that such counterarguments will always win, since, as we have stated in Section 5, agents in our framework use a preference relation based on joint confidence.",
                "Moreover, one may think that it would be better that the agents generate counterarguments based on the joint confidence preference relation; however it is not obvious how to generate counterarguments based on joint confidence in an efficient way, since <br>collaboration</br> is required in order to evaluate joint confidence.",
                "Thus, the agent generating the counterargument should constantly communicate with the other agents at each step of the induction algorithm used to generate counterarguments (presently one of our future research lines).",
                "Thus, in our framework, when an agent wants to generate a counterargument β to an argument α, β has to be more specific than α (i.e. α.D < β.D).",
                "The generation of counterarguments using the specificity criterion imposes some restrictions over the learning method, although LID or ID3 can be easily adapted for this task.",
                "For instance, LID is an algorithm that generates a description starting from scratch and heuristically adding features to that term.",
                "Thus, at every step, the description is made more specific than in the previous step, and the number of cases that are subsumed by that description is reduced.",
                "When the description covers only (or almost only) cases of a single solution class LID terminates and predicts that solution class.",
                "To generate a counterargument to an argument α LID just has to use as starting point the description α.D instead of starting from scratch.",
                "In this way, the justification provided by LID will always be subsumed by α.D, and thus the resulting counterargument will be more specific than α.",
                "However, notice that LID may sometimes not be able to generate counterarguments, since LID may not be able to specialize the description α.D any further, or because the agent Ai has no case inCi that is subsumed by α.D.",
                "Figure 4 shows how an agent A2 that disagreed with the argument shown in Figure 3, generates a counterargument using LID.",
                "Moreover, Figure 4 shows the generation of a counterargument β1 2 for the argument α0 1 (in Figure 3) that is a specialization of α0 1. 978 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Solution: hadromerida Justification: D1 Sponge Spikulate skeleton External features External features Gemmules: no Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Case Base of A1 LID New sponge P Figure 3: Example of a real justification generated by LID in the marine sponges data set.",
                "Specifically, in our experiments, when an agent Ai wants to rebut an argument α, uses the following policy: 1.",
                "Agent Ai uses LID to try to find a counterargument β more specific than α; if found, β is sent to the other agent as a counterargument of α. 2.",
                "If not found, then Ai searches for a counterexample c ∈ Ci of α.",
                "If a case c is found, then c is sent to the other agent as a counterexample of α. 3.",
                "If no counterexamples are found, then Ai cannot rebut the argument α. 7.",
                "ARGUMENTATION-BASED MULTI-AGENT LEARNING The interaction protocol of AMAL allows a group of agents A1, ..., An to deliberate about the correct solution of a problem P by means of an argumentation process.",
                "If the argumentation process arrives to a consensual solution, the joint deliberation ends; otherwise a weighted vote is used to determine the joint solution.",
                "Moreover, AMAL also allows the agents to learn from the counterexamples received from other agents.",
                "The AMAL protocol consists on a series of rounds.",
                "In the initial round, each agent states which is its individual prediction for P. Then, at each round an agent can try to rebut the prediction made by any of the other agents.",
                "The protocol uses a token passing mechanism so that agents (one at a time) can send counterarguments or counterexamples if they disagree with the prediction made by any other agent.",
                "Specifically, each agent is allowed to send one counterargument or counterexample each time he gets the token (notice that this restriction is just to simplify the protocol, and that it does not restrict the number of counterargument an agent can sent, since they can be delayed for subsequent rounds).",
                "When an agent receives a counterargument or counterexample, it informs the other agents if it accepts the counterargument (and changes its prediction) or not.",
                "Moreover, agents have also the opportunity to answer to counterarguments when they receive the token, by trying to generate a counterargument to the counterargument.",
                "When all the agents have had the token once, the token returns to the first agent, and so on.",
                "If at any time in the protocol, all the agents agree or during the last n rounds no agent has generated any counterargument, the protocol ends.",
                "Moreover, if at the end of the argumentation the agents have not reached an agreement, then a voting mechanism that uses the confidence of each prediction as weights is used to decide the final solution (Thus, AMAL follows the same mechanism as human committees, first each individual member of a committee exposes his arguments and discuses those of the other members (joint deliberation), and if no consensus is reached, then a voting mechanism is required).",
                "At each iteration, agents can use the following performatives: • assert(α): the justified prediction held during the next round will be α.",
                "An agent can only hold a single prediction at each round, thus is multiple asserts are send, only the last one is considered as the currently held prediction. • rebut(β, α): the agent has found a counterargument β to the prediction α.",
                "We will define Ht = αt 1, ..., αt n as the predictions that each of the n agents hold at a round t. Moreover, we will also define contradict(αt i) = {α ∈ Ht|α.S = αt i.S} as the set of contradicting arguments for an agent Ai in a round t, i.e. the set of arguments at round t that support a different solution class than αt i.",
                "The protocol is initiated because one of the agents receives a problem P to be solved.",
                "After that, the agent informs all the other agents about the problem P to solve, and the protocol starts: 1.",
                "At round t = 0, each one of the agents individually solves P, and builds a justified prediction using its own CBR method.",
                "Then, each agent Ai sends the performative assert(α0 i ) to the other agents.",
                "Thus, the agents know H0 = α0 i , ..., α0 n .",
                "Once all the predictions have been sent the token is given to the first agent A1. 2.",
                "At each round t (other than 0), the agents check whether their arguments in Ht agree.",
                "If they do, the protocol moves to step 5.",
                "Moreover, if during the last n rounds no agent has sent any counterexample or counterargument, the protocol also moves to step 5.",
                "Otherwise, the agent Ai owner of the token tries to generate a counterargument for each of the opposing arguments in contradict(αt i) ⊆ Ht (see Section 6.1).",
                "Then, the counterargument βt i against the prediction αt j with the lowest confidence C(αt j) is selected (since αt j is the prediction more likely to be successfully rebutted). • If βt i is a counterargument, then, Ai locally compares αt i with βt i by assessing their confidence against its individual case base Ci (see Section 5) (notice that Ai is comparing its previous argument with the counterargument that Ai itself has just generated and that is about The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 979 Sponge Spikulate skeleton External features External features Gemmules: no Growing: Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Growing Grow: massive Case Base of A2 LID Solution: astrophorida Justification: D2 Figure 4: Generation of a counterargument using LID in the sponges data set. to send to Aj).",
                "If CAi (βt i ) > CAi (αt i), then Ai considers that βt i is stronger than its previous argument, changes its argument to βt i by sending assert(βt i ) to the rest of the agents (the intuition behind this is that since a counterargument is also an argument, Ai checks if the newly counterargument is a better argument than the one he was previously holding) and rebut(βt i , αt j) to Aj.",
                "Otherwise (i.e.",
                "CAi (βt i ) ≤ CAi (αt i)), Ai will send only rebut(βt i , αt j) to Aj.",
                "In any of the two situations the protocol moves to step 3. • If βt i is a counterexample c, then Ai sends rebut(c, αt j) to Aj.",
                "The protocol moves to step 4. • If Ai cannot generate any counterargument or counterexample, the token is sent to the next agent, a new round t + 1 starts, and the protocol moves to state 2. 3.",
                "The agent Aj that has received the counterargument βt i , locally compares it against its own argument, αt j, by locally assessing their confidence.",
                "If CAj (βt i ) > CAj (αt j), then Aj will accept the counterargument as stronger than its own argument, and it will send assert(βt i ) to the other agents.",
                "Otherwise (i.e.",
                "CAj (βt i ) ≤ CAj (αt j)), Aj will not accept the counterargument, and will inform the other agents accordingly.",
                "Any of the two situations start a new round t + 1, Ai sends the token to the next agent, and the protocol moves back to state 2. 4.",
                "The agent Aj that has received the counterexample c retains it into its case base and generates a new argument αt+1 j that takes into account c, and informs the rest of the agents by sending assert(αt+1 j ) to all of them.",
                "Then, Ai sends the token to the next agent, a new round t + 1 starts, and the protocol moves back to step 2. 5.",
                "The protocol ends yielding a joint prediction, as follows: if the arguments in Ht agree then their prediction is the joint prediction, otherwise a voting mechanism is used to decide the joint prediction.",
                "The voting mechanism uses the joint confidence measure as the voting weights, as follows: S = arg max Sk∈S αi∈Ht|αi.S=Sk C(αi) Moreover, in order to avoid infinite iterations, if an agent sends twice the same argument or counterargument to the same agent, the message is not considered. 8.",
                "EXEMPLIFICATION Let us consider a system composed of three agents A1, A2 and A3.",
                "One of the agents, A1 receives a problem P to solve, and decides to use AMAL to solve it.",
                "For that reason, invites A2 and A3 to take part in the argumentation process.",
                "They accept the invitation, and the argumentation protocol starts.",
                "Initially, each agent generates its individual prediction for P, and broadcasts it to the other agents.",
                "Thus, all of them can compute H0 = α0 1, α0 2, α0 3 .",
                "In particular, in this example: • α0 1 = A1, P, hadromerida, D1 • α0 2 = A2, P, astrophorida, D2 • α0 3 = A3, P, axinellida, D3 A1 starts owning the token and tries to generate counterarguments for α0 2 and α0 3, but does not succeed, however it has one counterexample c13 for α0 3.",
                "Thus, A1 sends the the message rebut( c13, α0 3) to A3.",
                "A3 incorporates c13 into its case base and tries to solve the problem P again, now taking c13 into consideration.",
                "A3 comes up with the justified prediction α1 3 = A3, P, hadromerida, D4 , and broadcasts it to the rest of the agents with the message assert(α1 3).",
                "Thus, all of them know the new H1 = α0 1, α0 2, α1 3 .",
                "Round 1 starts and A2 gets the token.",
                "A2 tries to generate counterarguments for α0 1 and α1 3 and only succeeds to generate a counterargument β1 2 = A2, P, astrophorida, D5 against α1 3.",
                "The counterargument is sent to A3 with the message rebut(β1 2 , α1 3).",
                "Agent A3 receives the counterargument and assesses its local confidence.",
                "The result is that the individual confidence of the counterargument β1 2 is lower than the local confidence of α1 3.",
                "Therefore, A3 does not accept the counterargument, and thus H2 = α0 1, α0 2, α1 3 .",
                "Round 2 starts and A3 gets the token.",
                "A3 generates a counterargument β2 3 = A3, P, hadromerida, D6 for α0 2 and sends it to A2 with the message rebut(β2 3 , α0 2).",
                "Agent A2 receives the counterargument and assesses its local confidence.",
                "The result is that the local confidence of the counterargument β2 3 is higher than the local confidence of α0 2.",
                "Therefore, A2 accepts the counterargument and informs the rest of the agents with the message assert(β2 3 ).",
                "After that, H3 = α0 1, β2 3 , α1 3 .",
                "At Round 3, since all the agents agree (all the justified predictions in H3 predict hadromerida as the solution class) The protocol ends, and A1 (the agent that received the problem) considers hadromerida as the joint solution for the problem P. 9.",
                "EXPERIMENTAL EVALUATION 980 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) SPONGE 75 77 79 81 83 85 87 89 91 2 3 4 5 AMAL Voting Individual SOYBEAN 55 60 65 70 75 80 85 90 2 3 4 5 AMAL Voting Individual Figure 5: Individual and joint accuracy for 2 to 5 agents.",
                "In this section we empirically evaluate the AMAL argumentation framework.",
                "We have made experiments in two different data sets: soybean (from the UCI machine learning repository) and sponge (a relational data set).",
                "The soybean data set has 307 examples and 19 solution classes, while the sponge data set has 280 examples and 3 solution classes.",
                "In an experimental run, the data set is divided in 2 sets: the training set and the test set.",
                "The training set examples are distributed among 5 different agents without replication, i.e. there is no example shared by two agents.",
                "In the testing stage, problems in the test set arrive randomly to one of the agents, and their goal is to predict the correct solution.",
                "The experiments are designed to test two hypotheses: (H1) that argumentation is a useful framework for joint deliberation and can improve over other typical methods such as voting; and (H2) that learning from communication improves the individual performance of a learning agent participating in an argumentation process.",
                "Moreover, we also expect that the improvement achieved from argumentation will increase as the number of agents participating in the argumentation increases (since more information will be taken into account).",
                "Concerning H1 (argumentation is a useful framework for joint deliberation), we ran 4 experiments, using 2, 3, 4, and 5 agents respectively (in all experiments each agent has a 20% of the training data, since the training is always distributed among 5 agents).",
                "Figure 5 shows the result of those experiments in the sponge and soybean data sets.",
                "Classification accuracy is plotted in the vertical axis, and in the horizontal axis the number of agents that took part in the argumentation processes is shown.",
                "For each number of agents, three bars are shown: individual, Voting, and AMAL.",
                "The individual bar shows the average accuracy of individual agents predictions; the voting bar shows the average accuracy of the joint prediction achieved by voting but without any argumentation; and finally the AMAL bar shows the average accuracy of the joint prediction using argumentation.",
                "The results shown are the average of 5 10-fold cross validation runs.",
                "Figure 5 shows that <br>collaboration</br> (voting and AMAL) outperforms individual problem solving.",
                "Moreover, as we expected, the accuracy improves as more agents collaborate, since more information is taken into account.",
                "We can also see that AMAL always outperforms standard voting, proving that joint decisions are based on better information as provided by the argumentation process.",
                "For instance, the joint accuracy for 2 agents in the sponge data set is of 87.57% for AMAL and 86.57% for voting (while individual accuracy is just 80.07%).",
                "Moreover, the improvement achieved by AMAL over Voting is even larger in the soybean data set.",
                "The reason is that the soybean data set is more difficult (in the sense that agents need more data to produce good predictions).",
                "These experimental results show that AMAL effectively exploits the opportunity for improvement: the accuracy is higher only because more agents have changed their opinion during argumentation (otherwise they would achieve the same result as Voting).",
                "Concerning H2 (learning from communication in argumentation processes improves individual prediction ), we ran the following experiment: initially, we distributed a 25% of the training set among the five agents; after that, the rest of the cases in the training set is sent to the agents one by one; when an agent receives a new training case, it has several options: the agent can discard it, the agent can retain it, or the agent can use it for engaging an argumentation process.",
                "Figure 6 shows the result of that experiment for the two data sets.",
                "Figure 6 contains three plots, where NL (not learning) shows accuracy of an agent with no learning at all; L (learning), shows the evolution of the individual classification accuracy when agents learn by retaining the training cases they individually receive (notice that when all the training cases have been retained at 100%, the accuracy should be equal to that of Figure 5 for individual agents); and finally LFC (learning from communication) shows the evolution of the individual classification accuracy of learning agents that also learn by retaining those counterexamples received during argumentation (i.e. they learn both from training examples and counterexamples).",
                "Figure 6 shows that if an agent Ai learns also from communication, Ai can significantly improve its individual performance with just a small number of additional cases (those selected as relevant counterexamples for Ai during argumentation).",
                "For instance, in the soybean data set, individual agents have achieved an accuracy of 70.62% when they also learn from communication versus an accuracy of 59.93% when they only learn from their individual experience.",
                "The number of cases learnt from communication depends on the properties of the data set: in the sponges data set, agents have retained only very few additional cases, and significantly improved individual accuracy; namely they retain 59.96 cases in average (compared to the 50.4 cases retained if they do not learn from communication).",
                "In the soybean data set more counterexamples are learnt to significantly improve individual accuracy, namely they retain 87.16 cases in average (compared to 55.27 cases retained if they do not learn from communication).",
                "Finally, the fact that both data sets show a significant improvement points out the adaptive nature of the argumentation-based approach to learning from communication: the useful cases are selected as counterexamples (and no more than those needed), and they have the intended effect. 10.",
                "RELATED WORK Concerning CBR in a multi-agent setting, the first research was on negotiated case retrieval [11] among groups of agents.",
                "Our work on multi-agent case-based learning started in 1999 [6]; later Mc Ginty and Smyth [7] presented a multi-agent collaborative CBR approach (CCBR) for planning.",
                "Finally, another interesting approach is multi-case-base reasoning (MCBR) [5], that deals with The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 981 SPONGE 60 65 70 75 80 85 25% 40% 55% 70% 85% 100% LFC L NL SOYBEAN 20 30 40 50 60 70 80 90 25% 40% 55% 70% 85% 100% LFC L NL Figure 6: Learning from communication resulting from argumentation in a system composed of 5 agents. distributed systems where there are several case bases available for the same task and addresses the problems of cross-case base adaptation.",
                "The main difference is that our MAC approach is a way to distribute the Reuse process of CBR (using a voting system) while Retrieve is performed individually by each agent; the other multiagent CBR approaches, however, focus on distributing the Retrieve process.",
                "Research on MAS argumentation focus on several issues like a) logics, protocols and languages that support argumentation, b) argument selection and c) argument interpretation.",
                "Approaches for logic and languages that support argumentation include defeasible logic [4] and BDI models [13].",
                "Although argument selection is a key aspect of automated argumentation (see [12] and [13]), most research has been focused on preference relations among arguments.",
                "In our framework we have addressed both argument selection and preference relations using a case-based approach. 11.",
                "CONCLUSIONS AND FUTURE WORK In this paper we have presented an argumentation-based framework for multi-agent learning.",
                "Specifically, we have presented AMAL, a framework that allows a group of learning agents to argue about the solution of a given problem and we have shown how the learning capabilities can be used to generate arguments and counterarguments.",
                "The experimental evaluation shows that the increased amount of information provided to the agents by the argumentation process increases their predictive accuracy, and specially when an adequate number of agents take part in the argumentation.",
                "The main contributions of this work are: a) an argumentation framework for learning agents; b) a case-based preference relation over arguments, based on computing an overall confidence estimation of arguments; c) a case-based policy to generate counterarguments and select counterexamples; and d) an argumentation-based approach for learning from communication.",
                "Finally, in the experiments presented here a learning agent would retain all counterexamples submitted by the other agent; however, this is a very simple case retention policy, and we will like to experiment with more informed policies - with the goal that individual learning agents could significantly improve using only a small set of cases proposed by other agents.",
                "Finally, our approach is focused on lazy learning, and future works aims at incorporating eager inductive learning inside the argumentative framework for learning from communication. 12.",
                "REFERENCES [1] Agnar Aamodt and Enric Plaza.",
                "Case-based reasoning: Foundational issues, methodological variations, and system approaches.",
                "Artificial Intelligence Communications, 7(1):39-59, 1994. [2] E. Armengol and E. Plaza.",
                "Lazy induction of descriptions for relational case-based learning.",
                "In ECML2001, pages 13-24, 2001. [3] Gerhard Brewka.",
                "Dynamic argument systems: A formal model of argumentation processes based on situation calculus.",
                "Journal of Logic and Computation, 11(2):257-282, 2001. [4] Carlos I. Chesñevar and Guillermo R. Simari.",
                "Formalizing Defeasible Argumentation using Labelled Deductive Systems.",
                "Journal of Computer Science & Technology, 1(4):18-33, 2000. [5] D. Leake and R. Sooriamurthi.",
                "Automatically selecting strategies for multi-case-base reasoning.",
                "In S. Craw and A. Preece, editors, ECCBR2002, pages 204-219, Berlin, 2002.",
                "Springer Verlag. [6] Francisco J. Martín, Enric Plaza, and Josep-Lluis Arcos.",
                "Knowledge and experience reuse through communications among competent (peer) agents.",
                "International Journal of Software Engineering and Knowledge Engineering, 9(3):319-341, 1999. [7] Lorraine McGinty and Barry Smyth.",
                "Collaborative case-based reasoning: Applications in personalized route planning.",
                "In I. Watson and Q. Yang, editors, ICCBR, number 2080 in LNAI, pages 362-376.",
                "Springer-Verlag, 2001. [8] Santi Ontañón and Enric Plaza.",
                "Justification-based multiagent learning.",
                "In ICML2003, pages 576-583.",
                "Morgan Kaufmann, 2003. [9] Enric Plaza, Eva Armengol, and Santiago Ontañón.",
                "The explanatory power of symbolic similarity in case-based reasoning.",
                "Artificial Intelligence Review, 24(2):145-161, 2005. [10] David Poole.",
                "On the comparison of theories: Preferring the most specific explanation.",
                "In IJCAI-85, pages 144-147, 1985. [11] M V Nagendra Prassad, Victor R Lesser, and Susan Lander.",
                "Retrieval and reasoning in distributed case bases.",
                "Technical report, UMass Computer Science Department, 1995. [12] K. Sycara S. Kraus and A. Evenchik.",
                "Reaching agreements through argumentation: a logical model and implementation.",
                "Artificial Intelligence Journal, 104:1-69, 1998. [13] N. R. Jennings S. Parsons, C. Sierra.",
                "Agents that reason and negotiate by arguing.",
                "Journal of Logic and Computation, 8:261-292, 1998. [14] Bruce A. Wooley.",
                "Explanation component of software systems.",
                "ACM CrossRoads, 5.1, 1998. 982 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "Section 2 discusses the relation among argumentation, <br>collaboration</br> and learning.",
                "ARGUMENTATION,<br>collaboration</br> AND LEARNING Both learning and <br>collaboration</br> are ways in which an agent can improve individual performance.",
                "In fact, there is a clear parallelism between learning and <br>collaboration</br> in multi-agent systems, since both are ways in which agents can deal with their shortcomings.",
                "Looking at the above lists of motivation, we can easily see that learning and <br>collaboration</br> are very related in multi-agent systems.",
                "An agent may choose to increase performance by learning, by collaborating, or by finding an intermediate point that combines learning and <br>collaboration</br> in order to improve performance."
            ],
            "translated_annotated_samples": [
                "La sección 2 discute la relación entre la argumentación, la <br>colaboración</br> y el aprendizaje.",
                "ARGUMENTACIÓN, COLABORACIÓN Y APRENDIZAJE Tanto el aprendizaje como la <br>colaboración</br> son formas en las que un agente puede mejorar el rendimiento individual.",
                "De hecho, existe un claro paralelismo entre el aprendizaje y la <br>colaboración</br> en sistemas multiagentes, ya que ambos son formas en las que los agentes pueden lidiar con sus deficiencias.",
                "Al observar las listas de motivación anteriores, podemos ver fácilmente que el aprendizaje y la <br>colaboración</br> están muy relacionados en los sistemas multiagente.",
                "Un agente puede optar por aumentar el rendimiento mediante el aprendizaje, la <br>colaboración</br> o encontrando un punto intermedio que combine el aprendizaje y la <br>colaboración</br> para mejorar el rendimiento."
            ],
            "translated_text": "Aprendizaje y deliberación conjunta a través de la argumentación en sistemas multiagente. Santi Ontañón CCL, Laboratorio de Computación Cognitiva, Instituto de Tecnología de Georgia, Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Instituto de Investigación en Inteligencia Artificial, CSIC, Consejo Superior de Investigaciones Científicas, Campus UAB, 08193 Bellaterra, Cataluña (España) enric@iiia.csic.es RESUMEN En este artículo presentaremos un marco de argumentación para agentes de aprendizaje (AMAL) diseñado con dos propósitos: (1) para deliberación conjunta, y (2) para aprender a través de la comunicación. El marco AMAL se basa completamente en el aprendizaje a partir de ejemplos: la relación de preferencia de argumentos, la política de generación de argumentos y la política de generación de contraargumentos son técnicas basadas en casos. Para unirse en la deliberación, los agentes de aprendizaje comparten su experiencia formando un comité para decidir sobre alguna decisión conjunta. Experimentalmente demostramos que la argumentación entre comités de agentes mejora tanto el rendimiento individual como el conjunto. Para aprender a través de la comunicación, un agente se involucra en discusiones con otros agentes para contrastar sus hipótesis individuales y recibir contraejemplos; el proceso de argumentación mejora su alcance de aprendizaje y rendimiento individual. Categorías y Descriptores de Asignaturas I.2.6 [Inteligencia Artificial]: Aprendizaje; I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente, Agentes Inteligentes 1. Los marcos de argumentación para sistemas multiagente pueden ser utilizados para diferentes propósitos como la deliberación conjunta, la persuasión, la negociación y la resolución de conflictos. En este artículo presentaremos un marco de argumentación para agentes de aprendizaje, y mostraremos que puede ser utilizado para dos propósitos: (1) deliberación conjunta, y (2) aprendizaje a partir de la comunicación. La deliberación conjunta basada en argumentos implica discutir sobre el resultado de una situación particular o la acción apropiada a tomar en una situación específica. Los agentes de aprendizaje son capaces de aprender de la experiencia, en el sentido de que ejemplos pasados (situaciones y sus resultados) se utilizan para predecir el resultado para la situación actual. Sin embargo, dado que la experiencia de los agentes individuales puede ser limitada, el conocimiento individual y la precisión de la predicción también son limitados. Por lo tanto, los agentes de aprendizaje que son capaces de argumentar sus predicciones individuales con otros agentes pueden alcanzar una mayor precisión en las predicciones después de dicho proceso de argumentación. La mayoría de los marcos de argumentación existentes para sistemas multiagente se basan en lógica deductiva u otra formalización de lógica deductiva diseñada específicamente para respaldar la argumentación, como la lógica por defecto [3]. Por lo general, un argumento se ve como una declaración lógica, mientras que un contraargumento es un argumento ofrecido en oposición a otro argumento; los agentes utilizan una relación de preferencia para resolver argumentos conflictivos. Sin embargo, los marcos de argumentación basados en lógica asumen agentes con conocimiento predefinido y relación de preferencia. En este artículo, nos enfocamos en un marco de Aprendizaje Multiagente basado en Argumentación (AMAL) donde tanto el conocimiento como la relación de preferencia se aprenden a partir de la experiencia. Por lo tanto, consideramos un escenario con agentes que (1) trabajan en el mismo dominio utilizando una ontología compartida, (2) son capaces de aprender a partir de ejemplos y (3) se comunican utilizando un marco argumentativo. Tener capacidades de aprendizaje permite a los agentes utilizar de manera efectiva una forma específica de contraargumento, a saber, el uso de contraejemplos. Los contraejemplos ofrecen la posibilidad de que los agentes aprendan durante el proceso de argumentación. Además, los agentes de aprendizaje permiten técnicas que utilizan la experiencia aprendida para generar argumentos y contraargumentos adecuados. Específicamente, necesitaremos abordar dos problemas: (1) cómo definir una técnica para generar argumentos y contraargumentos a partir de ejemplos, y (2) cómo definir una relación de preferencia sobre dos argumentos en conflicto que han sido inducidos a partir de ejemplos. Este documento presenta un enfoque basado en casos para abordar ambas cuestiones. Los agentes utilizan el razonamiento basado en casos (CBR) [1] para aprender de casos pasados (donde un caso es una situación y su resultado) con el fin de predecir el resultado de una nueva situación. Proponemos un protocolo de argumentación dentro del marco de AMAL que apoya a los agentes en llegar a una predicción conjunta sobre una situación o problema específico; además, el razonamiento necesario para respaldar el proceso de argumentación también se basará en casos. En particular, presentamos dos medidas basadas en casos, una para generar los argumentos y contraargumentos adecuados para una situación particular y otra para determinar la relación de preferencia entre los argumentos. Finalmente, evaluamos (1) si la argumentación entre agentes de aprendizaje puede producir una predicción conjunta que mejore el rendimiento de aprendizaje individual y (2) si el aprendizaje a partir de los contraejemplos transmitidos durante el proceso de argumentación aumenta el rendimiento individual con precisamente aquellos casos que se utilizan mientras discuten entre ellos. El documento está estructurado de la siguiente manera. La sección 2 discute la relación entre la argumentación, la <br>colaboración</br> y el aprendizaje. Luego, la Sección 3 introduce nuestro marco de trabajo de CBR multiagente (MAC) y el concepto de predicción justificada. Después de eso, la Sección 4 define formalmente nuestro marco de argumentación. Las secciones 5 y 6 presentan nuestra relación de preferencia basada en casos y las políticas de generación de argumentos, respectivamente. Más adelante, la Sección 7 presenta el protocolo de argumentación en nuestro marco AMAL. Después de eso, la Sección 8 presenta una ejemplificación del marco de argumentación. Finalmente, la Sección 9 presenta una evaluación empírica de nuestras dos hipótesis principales. El artículo concluye con secciones de trabajo relacionado y conclusiones. 2. ARGUMENTACIÓN, COLABORACIÓN Y APRENDIZAJE Tanto el aprendizaje como la <br>colaboración</br> son formas en las que un agente puede mejorar el rendimiento individual. De hecho, existe un claro paralelismo entre el aprendizaje y la <br>colaboración</br> en sistemas multiagentes, ya que ambos son formas en las que los agentes pueden lidiar con sus deficiencias. Mostremos cuáles son las principales motivaciones que un agente puede tener para aprender o colaborar. • Motivaciones para aprender: - Aumentar la calidad de la predicción, - Aumentar la eficiencia, - Ampliar el rango de problemas resolubles. • Motivaciones para colaborar: - Aumentar la calidad de la predicción, - Aumentar la eficiencia, - Ampliar el rango de problemas resolubles, - Ampliar el rango de recursos accesibles. Al observar las listas de motivación anteriores, podemos ver fácilmente que el aprendizaje y la <br>colaboración</br> están muy relacionados en los sistemas multiagente. De hecho, con la excepción del último elemento en la lista de motivaciones para colaborar, son dos extremos de un continuo de estrategias para mejorar el rendimiento. Un agente puede optar por aumentar el rendimiento mediante el aprendizaje, la <br>colaboración</br> o encontrando un punto intermedio que combine el aprendizaje y la <br>colaboración</br> para mejorar el rendimiento. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "group": {
            "translated_key": "grupo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning and Joint Deliberation through Argumentation in Multi-Agent Systems Santi Ontañón CCL, Cognitive Computing Lab Georgia Institute of Technology Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Artificial Intelligence Research Institute CSIC, Spanish Council for Scientific Research Campus UAB, 08193 Bellaterra, Catalonia (Spain) enric@iiia.csic.es ABSTRACT In this paper we will present an argumentation framework for learning agents (AMAL) designed for two purposes: (1) for joint deliberation, and (2) for learning from communication.",
                "The AMAL framework is completely based on learning from examples: the argument preference relation, the argument generation policy, and the counterargument generation policy are case-based techniques.",
                "For join deliberation, learning agents share their experience by forming a committee to decide upon some joint decision.",
                "We experimentally show that the argumentation among committees of agents improves both the individual and joint performance.",
                "For learning from communication, an agent engages into arguing with other agents in order to contrast its individual hypotheses and receive counterexamples; the argumentation process improves their learning scope and individual performance.",
                "Categories and Subject Descriptors I.2.6 [Artificial Intelligence]: Learning; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems, Intelligent Agents 1.",
                "INTRODUCTION Argumentation frameworks for multi-agent systems can be used for different purposes like joint deliberation, persuasion, negotiation, and conflict resolution.",
                "In this paper we will present an argumentation framework for learning agents, and show that it can be used for two purposes: (1) joint deliberation, and (2) learning from communication.",
                "Argumentation-based joint deliberation involves discussion over the outcome of a particular situation or the appropriate course of action for a particular situation.",
                "Learning agents are capable of learning from experience, in the sense that past examples (situations and their outcomes) are used to predict the outcome for the situation at hand.",
                "However, since individual agents experience may be limited, individual knowledge and prediction accuracy is also limited.",
                "Thus, learning agents that are capable of arguing their individual predictions with other agents may reach better prediction accuracy after such an argumentation process.",
                "Most existing argumentation frameworks for multi-agent systems are based on deductive logic or some other deductive logic formalism specifically designed to support argumentation, such as default logic [3]).",
                "Usually, an argument is seen as a logical statement, while a counterargument is an argument offered in opposition to another argument [4, 13]; agents use a preference relation to resolve conflicting arguments.",
                "However, logic-based argumentation frameworks assume agents with preloaded knowledge and preference relation.",
                "In this paper, we focus on an Argumentation-based Multi-Agent Learning (AMAL) framework where both knowledge and preference relation are learned from experience.",
                "Thus, we consider a scenario with agents that (1) work in the same domain using a shared ontology, (2) are capable of learning from examples, and (3) communicate using an argumentative framework.",
                "Having learning capabilities allows agents effectively use a specific form of counterargument, namely the use of counterexamples.",
                "Counterexamples offer the possibility of agents learning during the argumentation process.",
                "Moreover, learning agents allow techniques that use learnt experience to generate adequate arguments and counterarguments.",
                "Specifically, we will need to address two issues: (1) how to define a technique to generate arguments and counterarguments from examples, and (2)how to define a preference relation over two conflicting arguments that have been induced from examples.",
                "This paper presents a case-based approach to address both issues.",
                "The agents use case-based reasoning (CBR) [1] to learn from past cases (where a case is a situation and its outcome) in order to predict the outcome of a new situation.",
                "We propose an argumentation protocol inside the AMAL framework at supports agents in reaching a joint prediction over a specific situation or problem - moreover, the reasoning needed to support the argumentation process will also be based on cases.",
                "In particular, we present two case-based measures, one for generating the arguments and counterarguments adequate to a particular situation and another for determining preference relation among arguments.",
                "Finally, we evaluate (1) if argumentation between learning agents can produce a joint prediction that improves over individual learning performance and (2) if learning from the counterexamples conveyed during the argumentation process increases the individual performance with precisely those cases being used while arguing among them.",
                "The paper is structured as follows.",
                "Section 2 discusses the relation among argumentation, collaboration and learning.",
                "Then Section 3 introduces our multi-agent CBR (MAC) framework and the notion of justified prediction.",
                "After that, Section 4 formally defines our argumentation framework.",
                "Sections 5 and 6 present our case-based preference relation and argument generation policies respectively.",
                "Later, Section 7 presents the argumentation protocol in our AMAL framework.",
                "After that, Section 8 presents an exemplification of the argumentation framework.",
                "Finally, Section 9 presents an empirical evaluation of our two main hypotheses.",
                "The paper closes with related work and conclusions sections. 2.",
                "ARGUMENTATION,COLLABORATION AND LEARNING Both learning and collaboration are ways in which an agent can improve individual performance.",
                "In fact, there is a clear parallelism between learning and collaboration in multi-agent systems, since both are ways in which agents can deal with their shortcomings.",
                "Let us show which are the main motivations that an agent can have to learn or to collaborate. • Motivations to learn: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems. • Motivations to collaborate: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems, - Increase the range of accessible resources.",
                "Looking at the above lists of motivation, we can easily see that learning and collaboration are very related in multi-agent systems.",
                "In fact, with the exception of the last item in the motivations to collaborate list, they are two extremes of a continuum of strategies to improve performance.",
                "An agent may choose to increase performance by learning, by collaborating, or by finding an intermediate point that combines learning and collaboration in order to improve performance.",
                "In this paper we will propose AMAL, an argumentation framework for learning agents, and will also also show how AMAL can be used both for learning from communication and for solving problems in a collaborative way: • Agents can solve problems in a collaborative way via engaging an argumentation process about the prediction for the situation at hand.",
                "Using this collaboration, the prediction can be done in a more informed way, since the information known by several agents has been taken into account. • Agents can also learn from communication with other agents by engaging an argumentation process.",
                "Agents that engage in such argumentation processes can learn from the arguments and counterexamples received from other agents, and use this information for predicting the outcomes of future situations.",
                "In the rest of this paper we will propose an argumentation framework and show how it can be used both for learning and for solving problems in a collaborative way. 3.",
                "MULTI-AGENT CBR SYSTEMS A Multi-Agent Case Based Reasoning System (MAC) M = {(A1, C1), ..., (An, Cn)} is a multi-agent system composed of A = {Ai, ..., An}, a set of CBR agents, where each agent Ai ∈ A possesses an individual case base Ci.",
                "Each individual agent Ai in a MAC is completely autonomous and each agent Ai has access only to its individual and private case base Ci.",
                "A case base Ci = {c1, ..., cm} is a collection of cases.",
                "Agents in a MAC system are able to individually solve problems, but they can also collaborate with other agents to solve problems.",
                "In this framework, we will restrict ourselves to analytical tasks, i.e. tasks like classification, where the solution of a problem is achieved by selecting a solution class from an enumerated set of solution classes.",
                "In the following we will note the set of all the solution classes by S = {S1, ..., SK }.",
                "Therefore, a case c = P, S is a tuple containing a case description P and a solution class S ∈ S. In the following, we will use the terms problem and case description indistinctly.",
                "Moreover, we will use the dot notation to refer to elements inside a tuple; e.g., to refer to the solution class of a case c, we will write c.S.",
                "Therefore, we say a <br>group</br> of agents perform joint deliberation, when they collaborate to find a joint solution by means of an argumentation process.",
                "However, in order to do so, an agent has to be able to justify its prediction to the other agents (i.e. generate an argument for its predicted solution that can be examined and critiqued by the other agents).",
                "The next section addresses this issue. 3.1 Justified Predictions Both expert systems and CBR systems may have an explanation component [14] in charge of justifying why the system has provided a specific answer to the user.",
                "The line of reasoning of the system can then be examined by a human expert, thus increasing the reliability of the system.",
                "Most of the existing work on explanation generation focuses on generating explanations to be provided to the user.",
                "However, in our approach we use explanations (or justifications) as a tool for improving communication and coordination among agents.",
                "We are interested in justifications since they can be used as arguments.",
                "For that purpose, we will benefit from the ability of some machine learning methods to provide justifications.",
                "A justification built by a CBR method after determining that the solution of a particular problem P was Sk is a description that contains the relevant information from the problem P that the CBR method has considered to predict Sk as the solution of P. In particular, CBR methods work by retrieving similar cases to the problem at hand, and then reusing their solutions for the current problem, expecting that since the problem and the cases are similar, the solutions will also be similar.",
                "Thus, if a CBR method has retrieved a set of cases C1, ..., Cn to solve a particular problem P the justification built will contain the relevant information from the problem P that made the CBR system retrieve that particular set of cases, i.e. it will contain the relevant information that P and C1, ..., Cn have in common.",
                "For example, Figure 1 shows a justification build by a CBR system for a toy problem (in the following sections we will show justifications for real problems).",
                "In the figure, a problem has two attributes (Traffic_light, and Cars_passing), the retrieval mechanism of the CBR system notices that by considering only the attribute Traffic_light, it can retrieve two cases that predict the same solution: wait.",
                "Thus, since only this attribute has been used, it is the only one appearing in the justification.",
                "The values of the rest of attributes are irrelevant, since whatever their value the solution class would have been the same. 976 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Problem Traffic_light: red Cars_passing: no Case 1 Traffic_light: red Cars_passing: no Solution: wait Case 3 Traffic_light: red Cars_passing: yes Solution: wait Case 4 Traffic_light: green Cars_passing: yes Solution: wait Case 2 Traffic_light: green Cars_passing: no Solution: cross Retrieved cases Solution: wait Justification Traffic_light: red Figure 1: An example of justification generation in a CBR system.",
                "Notice that, since the only relevant feature to decide is Traffic_light (the only one used to retrieve cases), it is the only one appearing in the justification.",
                "In general, the meaning of a justification is that all (or most of) the cases in the case base of an agent that satisfy the justification (i.e. all the cases that are subsumed by the justification) belong to the predicted solution class.",
                "In the rest of the paper, we will use to denote the subsumption relation.",
                "In our work, we use LID [2], a CBR method capable of building symbolic justifications such as the one exemplified in Figure 1.",
                "When an agent provides a justification for a prediction, the agent generates a justified prediction: DEFINITION 3.1.",
                "A Justified Prediction is a tuple J = A, P, S, D where agent A considers S the correct solution for problem P, and that prediction is justified a symbolic description D such that J.D J.P. Justifications can have many uses for CBR systems [8, 9].",
                "In this paper, we are going to use justifications as arguments, in order to allow learning agents to engage in argumentation processes. 4.",
                "ARGUMENTS AND COUNTERARGUMENTS For our purposes an argument α generated by an agent A is composed of a statement S and some evidence D supporting S as correct.",
                "In the remainder of this section we will see how this general definition of argument can be instantiated in specific kind of arguments that the agents can generate.",
                "In the context of MAC systems, agents argue about predictions for new problems and can provide two kinds of information: a) specific cases P, S , and b) justified predictions: A, P, S, D .",
                "Using this information, we can define three types of arguments: justified predictions, counterarguments, and counterexamples.",
                "A justified prediction α is generated by an agent Ai to argue that Ai believes that the correct solution for a given problem P is α.S, and the evidence provided is the justification α.D.",
                "In the example depicted in Figure 1, an agent Ai may generate the argument α = Ai, P, Wait, (Traffic_light = red) , meaning that the agent Ai believes that the correct solution for P is Wait because the attribute Traffic_light equals red.",
                "A counterargument β is an argument offered in opposition to another argument α.",
                "In our framework, a counterargument consists of a justified prediction Aj, P, S , D generated by an agent Aj with the intention to rebut an argument α generated by another agent Ai, that endorses a solution class S different from that of α.S for the problem at hand and justifies this with a justification D .",
                "In the example in Figure 1, if an agent generates the argument α = Ai, P, Walk, (Cars_passing = no) , an agent that thinks that the correct solution is Wait might answer with the counterargument β = Aj, P, Wait, (Cars_passing = no ∧ Traffic_light = red) , meaning that, although there are no cars passing, the traffic light is red, and the street cannot be crossed.",
                "A counterexample c is a case that contradicts an argument α.",
                "Thus a counterexample is also a counterargument, one that states that a specific argument α is not always true, and the evidence provided is the case c. Specifically, for a case c to be a counterexample of an argument α, the following conditions have to be met: α.D c and α.S = c.S, i.e. the case must satisfy the justification α.D and the solution of c must be different than the predicted by α.",
                "By exchanging arguments and counterarguments (including counterexamples), agents can argue about the correct solution of a given problem, i.e. they can engage a joint deliberation process.",
                "However, in order to do so, they need a specific interaction protocol, a preference relation between contradicting arguments, and a decision policy to generate counterarguments (including counterexamples).",
                "In the following sections we will present these elements. 5.",
                "PREFERENCE RELATION A specific argument provided by an agent might not be consistent with the information known to other agents (or even to some of the information known by the agent that has generated the justification due to noise in training data).",
                "For that reason, we are going to define a preference relation over contradicting justified predictions based on cases.",
                "Basically, we will define a confidence measure for each justified prediction (that takes into account the cases owned by each agent), and the justified prediction with the highest confidence will be the preferred one.",
                "The idea behind case-based confidence is to count how many of the cases in an individual case base endorse a justified prediction, and how many of them are counterexamples of it.",
                "The more the endorsing cases, the higher the confidence; and the more the counterexamples, the lower the confidence.",
                "Specifically, to assess the confidence of a justified prediction α, an agent obtains the set of cases in its individual case base that are subsumed by α.D.",
                "With them, an agent Ai obtains the Y (aye) and N (nay) values: • Y Ai α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by the justification α.D that belong to the solution class α.S, • NAi α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by justification α.D that do not belong to that solution class.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 977 + + + + + +  - - + Figure 2: Confidence of arguments is evaluated by contrasting them against the case bases of the agents.",
                "An agent estimates the confidence of an argument as: CAi (α) = Y Ai α 1 + Y Ai α + NAi α i.e. the confidence on a justified prediction is the number of endorsing cases divided by the number of endorsing cases plus counterexamples.",
                "Notice that we add 1 to the denominator, this is to avoid giving excessively high confidences to justified predictions whose confidence has been computed using a small number of cases.",
                "Notice that this correction follows the same idea than the Laplace correction to estimate probabilities.",
                "Figure 2 illustrates the individual evaluation of the confidence of an argument, in particular, three endorsing cases and one counterexample are found in the case base of agents Ai, giving an estimated confidence of 0.6 Moreover, we can also define the joint confidence of an argument α as the confidence computed using the cases present in the case bases of all the agents in the <br>group</br>: C(α) = i Y Ai α 1 + i Y Ai α + NAi α Notice that, to collaboratively compute the joint confidence, the agents only have to make public the aye and nay values locally computed for a given argument.",
                "In our framework, agents use this joint confidence as the preference relation: a justified prediction α is preferred over another one β if C(α) ≥ C(β). 6.",
                "GENERATION OF ARGUMENTS In our framework, arguments are generated by the agents from cases, using learning methods.",
                "Any learning method able to provide a justified prediction can be used to generate arguments.",
                "For instance, decision trees and LID [2] are suitable learning methods.",
                "Specifically, in the experiments reported in this paper agents use LID.",
                "Thus, when an agent wants to generate an argument endorsing that a specific solution class is the correct solution for a problem P, it generates a justified prediction as explained in Section 3.1.",
                "For instance, Figure 3 shows a real justification generated by LID after solving a problem P in the domain of marine sponges identification.",
                "In particular, Figure 3 shows how when an agent receives a new problem to solve (in this case, a new sponge to determine its order), the agent uses LID to generate an argument (consisting on a justified prediction) using the cases in the case base of the agent.",
                "The justification shown in Figure 3 can be interpreted saying that the predicted solution is hadromerida because the smooth form of the megascleres of the spiculate skeleton of the sponge is of type tylostyle, the spikulate skeleton of the sponge has no uniform length, and there is no gemmules in the external features of the sponge.",
                "Thus, the argument generated will be α = A1, P, hadromerida, D1 . 6.1 Generation of Counterarguments As previously stated, agents may try to rebut arguments by generating counterargument or by finding counterexamples.",
                "Let us explain how they can be generated.",
                "An agent Ai wants to generate a counterargument β to rebut an argument α when α is in contradiction with the local case base of Ai.",
                "Moreover, while generating such counterargument β, Ai expects that β is preferred over α.",
                "For that purpose, we will present a specific policy to generate counterarguments based on the specificity criterion [10].",
                "The specificity criterion is widely used in deductive frameworks for argumentation, and states that between two conflicting arguments, the most specific should be preferred since it is, in principle, more informed.",
                "Thus, counterarguments generated based on the specificity criterion are expected to be preferable (since they are more informed) to the arguments they try to rebut.",
                "However, there is no guarantee that such counterarguments will always win, since, as we have stated in Section 5, agents in our framework use a preference relation based on joint confidence.",
                "Moreover, one may think that it would be better that the agents generate counterarguments based on the joint confidence preference relation; however it is not obvious how to generate counterarguments based on joint confidence in an efficient way, since collaboration is required in order to evaluate joint confidence.",
                "Thus, the agent generating the counterargument should constantly communicate with the other agents at each step of the induction algorithm used to generate counterarguments (presently one of our future research lines).",
                "Thus, in our framework, when an agent wants to generate a counterargument β to an argument α, β has to be more specific than α (i.e. α.D < β.D).",
                "The generation of counterarguments using the specificity criterion imposes some restrictions over the learning method, although LID or ID3 can be easily adapted for this task.",
                "For instance, LID is an algorithm that generates a description starting from scratch and heuristically adding features to that term.",
                "Thus, at every step, the description is made more specific than in the previous step, and the number of cases that are subsumed by that description is reduced.",
                "When the description covers only (or almost only) cases of a single solution class LID terminates and predicts that solution class.",
                "To generate a counterargument to an argument α LID just has to use as starting point the description α.D instead of starting from scratch.",
                "In this way, the justification provided by LID will always be subsumed by α.D, and thus the resulting counterargument will be more specific than α.",
                "However, notice that LID may sometimes not be able to generate counterarguments, since LID may not be able to specialize the description α.D any further, or because the agent Ai has no case inCi that is subsumed by α.D.",
                "Figure 4 shows how an agent A2 that disagreed with the argument shown in Figure 3, generates a counterargument using LID.",
                "Moreover, Figure 4 shows the generation of a counterargument β1 2 for the argument α0 1 (in Figure 3) that is a specialization of α0 1. 978 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Solution: hadromerida Justification: D1 Sponge Spikulate skeleton External features External features Gemmules: no Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Case Base of A1 LID New sponge P Figure 3: Example of a real justification generated by LID in the marine sponges data set.",
                "Specifically, in our experiments, when an agent Ai wants to rebut an argument α, uses the following policy: 1.",
                "Agent Ai uses LID to try to find a counterargument β more specific than α; if found, β is sent to the other agent as a counterargument of α. 2.",
                "If not found, then Ai searches for a counterexample c ∈ Ci of α.",
                "If a case c is found, then c is sent to the other agent as a counterexample of α. 3.",
                "If no counterexamples are found, then Ai cannot rebut the argument α. 7.",
                "ARGUMENTATION-BASED MULTI-AGENT LEARNING The interaction protocol of AMAL allows a <br>group</br> of agents A1, ..., An to deliberate about the correct solution of a problem P by means of an argumentation process.",
                "If the argumentation process arrives to a consensual solution, the joint deliberation ends; otherwise a weighted vote is used to determine the joint solution.",
                "Moreover, AMAL also allows the agents to learn from the counterexamples received from other agents.",
                "The AMAL protocol consists on a series of rounds.",
                "In the initial round, each agent states which is its individual prediction for P. Then, at each round an agent can try to rebut the prediction made by any of the other agents.",
                "The protocol uses a token passing mechanism so that agents (one at a time) can send counterarguments or counterexamples if they disagree with the prediction made by any other agent.",
                "Specifically, each agent is allowed to send one counterargument or counterexample each time he gets the token (notice that this restriction is just to simplify the protocol, and that it does not restrict the number of counterargument an agent can sent, since they can be delayed for subsequent rounds).",
                "When an agent receives a counterargument or counterexample, it informs the other agents if it accepts the counterargument (and changes its prediction) or not.",
                "Moreover, agents have also the opportunity to answer to counterarguments when they receive the token, by trying to generate a counterargument to the counterargument.",
                "When all the agents have had the token once, the token returns to the first agent, and so on.",
                "If at any time in the protocol, all the agents agree or during the last n rounds no agent has generated any counterargument, the protocol ends.",
                "Moreover, if at the end of the argumentation the agents have not reached an agreement, then a voting mechanism that uses the confidence of each prediction as weights is used to decide the final solution (Thus, AMAL follows the same mechanism as human committees, first each individual member of a committee exposes his arguments and discuses those of the other members (joint deliberation), and if no consensus is reached, then a voting mechanism is required).",
                "At each iteration, agents can use the following performatives: • assert(α): the justified prediction held during the next round will be α.",
                "An agent can only hold a single prediction at each round, thus is multiple asserts are send, only the last one is considered as the currently held prediction. • rebut(β, α): the agent has found a counterargument β to the prediction α.",
                "We will define Ht = αt 1, ..., αt n as the predictions that each of the n agents hold at a round t. Moreover, we will also define contradict(αt i) = {α ∈ Ht|α.S = αt i.S} as the set of contradicting arguments for an agent Ai in a round t, i.e. the set of arguments at round t that support a different solution class than αt i.",
                "The protocol is initiated because one of the agents receives a problem P to be solved.",
                "After that, the agent informs all the other agents about the problem P to solve, and the protocol starts: 1.",
                "At round t = 0, each one of the agents individually solves P, and builds a justified prediction using its own CBR method.",
                "Then, each agent Ai sends the performative assert(α0 i ) to the other agents.",
                "Thus, the agents know H0 = α0 i , ..., α0 n .",
                "Once all the predictions have been sent the token is given to the first agent A1. 2.",
                "At each round t (other than 0), the agents check whether their arguments in Ht agree.",
                "If they do, the protocol moves to step 5.",
                "Moreover, if during the last n rounds no agent has sent any counterexample or counterargument, the protocol also moves to step 5.",
                "Otherwise, the agent Ai owner of the token tries to generate a counterargument for each of the opposing arguments in contradict(αt i) ⊆ Ht (see Section 6.1).",
                "Then, the counterargument βt i against the prediction αt j with the lowest confidence C(αt j) is selected (since αt j is the prediction more likely to be successfully rebutted). • If βt i is a counterargument, then, Ai locally compares αt i with βt i by assessing their confidence against its individual case base Ci (see Section 5) (notice that Ai is comparing its previous argument with the counterargument that Ai itself has just generated and that is about The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 979 Sponge Spikulate skeleton External features External features Gemmules: no Growing: Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Growing Grow: massive Case Base of A2 LID Solution: astrophorida Justification: D2 Figure 4: Generation of a counterargument using LID in the sponges data set. to send to Aj).",
                "If CAi (βt i ) > CAi (αt i), then Ai considers that βt i is stronger than its previous argument, changes its argument to βt i by sending assert(βt i ) to the rest of the agents (the intuition behind this is that since a counterargument is also an argument, Ai checks if the newly counterargument is a better argument than the one he was previously holding) and rebut(βt i , αt j) to Aj.",
                "Otherwise (i.e.",
                "CAi (βt i ) ≤ CAi (αt i)), Ai will send only rebut(βt i , αt j) to Aj.",
                "In any of the two situations the protocol moves to step 3. • If βt i is a counterexample c, then Ai sends rebut(c, αt j) to Aj.",
                "The protocol moves to step 4. • If Ai cannot generate any counterargument or counterexample, the token is sent to the next agent, a new round t + 1 starts, and the protocol moves to state 2. 3.",
                "The agent Aj that has received the counterargument βt i , locally compares it against its own argument, αt j, by locally assessing their confidence.",
                "If CAj (βt i ) > CAj (αt j), then Aj will accept the counterargument as stronger than its own argument, and it will send assert(βt i ) to the other agents.",
                "Otherwise (i.e.",
                "CAj (βt i ) ≤ CAj (αt j)), Aj will not accept the counterargument, and will inform the other agents accordingly.",
                "Any of the two situations start a new round t + 1, Ai sends the token to the next agent, and the protocol moves back to state 2. 4.",
                "The agent Aj that has received the counterexample c retains it into its case base and generates a new argument αt+1 j that takes into account c, and informs the rest of the agents by sending assert(αt+1 j ) to all of them.",
                "Then, Ai sends the token to the next agent, a new round t + 1 starts, and the protocol moves back to step 2. 5.",
                "The protocol ends yielding a joint prediction, as follows: if the arguments in Ht agree then their prediction is the joint prediction, otherwise a voting mechanism is used to decide the joint prediction.",
                "The voting mechanism uses the joint confidence measure as the voting weights, as follows: S = arg max Sk∈S αi∈Ht|αi.S=Sk C(αi) Moreover, in order to avoid infinite iterations, if an agent sends twice the same argument or counterargument to the same agent, the message is not considered. 8.",
                "EXEMPLIFICATION Let us consider a system composed of three agents A1, A2 and A3.",
                "One of the agents, A1 receives a problem P to solve, and decides to use AMAL to solve it.",
                "For that reason, invites A2 and A3 to take part in the argumentation process.",
                "They accept the invitation, and the argumentation protocol starts.",
                "Initially, each agent generates its individual prediction for P, and broadcasts it to the other agents.",
                "Thus, all of them can compute H0 = α0 1, α0 2, α0 3 .",
                "In particular, in this example: • α0 1 = A1, P, hadromerida, D1 • α0 2 = A2, P, astrophorida, D2 • α0 3 = A3, P, axinellida, D3 A1 starts owning the token and tries to generate counterarguments for α0 2 and α0 3, but does not succeed, however it has one counterexample c13 for α0 3.",
                "Thus, A1 sends the the message rebut( c13, α0 3) to A3.",
                "A3 incorporates c13 into its case base and tries to solve the problem P again, now taking c13 into consideration.",
                "A3 comes up with the justified prediction α1 3 = A3, P, hadromerida, D4 , and broadcasts it to the rest of the agents with the message assert(α1 3).",
                "Thus, all of them know the new H1 = α0 1, α0 2, α1 3 .",
                "Round 1 starts and A2 gets the token.",
                "A2 tries to generate counterarguments for α0 1 and α1 3 and only succeeds to generate a counterargument β1 2 = A2, P, astrophorida, D5 against α1 3.",
                "The counterargument is sent to A3 with the message rebut(β1 2 , α1 3).",
                "Agent A3 receives the counterargument and assesses its local confidence.",
                "The result is that the individual confidence of the counterargument β1 2 is lower than the local confidence of α1 3.",
                "Therefore, A3 does not accept the counterargument, and thus H2 = α0 1, α0 2, α1 3 .",
                "Round 2 starts and A3 gets the token.",
                "A3 generates a counterargument β2 3 = A3, P, hadromerida, D6 for α0 2 and sends it to A2 with the message rebut(β2 3 , α0 2).",
                "Agent A2 receives the counterargument and assesses its local confidence.",
                "The result is that the local confidence of the counterargument β2 3 is higher than the local confidence of α0 2.",
                "Therefore, A2 accepts the counterargument and informs the rest of the agents with the message assert(β2 3 ).",
                "After that, H3 = α0 1, β2 3 , α1 3 .",
                "At Round 3, since all the agents agree (all the justified predictions in H3 predict hadromerida as the solution class) The protocol ends, and A1 (the agent that received the problem) considers hadromerida as the joint solution for the problem P. 9.",
                "EXPERIMENTAL EVALUATION 980 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) SPONGE 75 77 79 81 83 85 87 89 91 2 3 4 5 AMAL Voting Individual SOYBEAN 55 60 65 70 75 80 85 90 2 3 4 5 AMAL Voting Individual Figure 5: Individual and joint accuracy for 2 to 5 agents.",
                "In this section we empirically evaluate the AMAL argumentation framework.",
                "We have made experiments in two different data sets: soybean (from the UCI machine learning repository) and sponge (a relational data set).",
                "The soybean data set has 307 examples and 19 solution classes, while the sponge data set has 280 examples and 3 solution classes.",
                "In an experimental run, the data set is divided in 2 sets: the training set and the test set.",
                "The training set examples are distributed among 5 different agents without replication, i.e. there is no example shared by two agents.",
                "In the testing stage, problems in the test set arrive randomly to one of the agents, and their goal is to predict the correct solution.",
                "The experiments are designed to test two hypotheses: (H1) that argumentation is a useful framework for joint deliberation and can improve over other typical methods such as voting; and (H2) that learning from communication improves the individual performance of a learning agent participating in an argumentation process.",
                "Moreover, we also expect that the improvement achieved from argumentation will increase as the number of agents participating in the argumentation increases (since more information will be taken into account).",
                "Concerning H1 (argumentation is a useful framework for joint deliberation), we ran 4 experiments, using 2, 3, 4, and 5 agents respectively (in all experiments each agent has a 20% of the training data, since the training is always distributed among 5 agents).",
                "Figure 5 shows the result of those experiments in the sponge and soybean data sets.",
                "Classification accuracy is plotted in the vertical axis, and in the horizontal axis the number of agents that took part in the argumentation processes is shown.",
                "For each number of agents, three bars are shown: individual, Voting, and AMAL.",
                "The individual bar shows the average accuracy of individual agents predictions; the voting bar shows the average accuracy of the joint prediction achieved by voting but without any argumentation; and finally the AMAL bar shows the average accuracy of the joint prediction using argumentation.",
                "The results shown are the average of 5 10-fold cross validation runs.",
                "Figure 5 shows that collaboration (voting and AMAL) outperforms individual problem solving.",
                "Moreover, as we expected, the accuracy improves as more agents collaborate, since more information is taken into account.",
                "We can also see that AMAL always outperforms standard voting, proving that joint decisions are based on better information as provided by the argumentation process.",
                "For instance, the joint accuracy for 2 agents in the sponge data set is of 87.57% for AMAL and 86.57% for voting (while individual accuracy is just 80.07%).",
                "Moreover, the improvement achieved by AMAL over Voting is even larger in the soybean data set.",
                "The reason is that the soybean data set is more difficult (in the sense that agents need more data to produce good predictions).",
                "These experimental results show that AMAL effectively exploits the opportunity for improvement: the accuracy is higher only because more agents have changed their opinion during argumentation (otherwise they would achieve the same result as Voting).",
                "Concerning H2 (learning from communication in argumentation processes improves individual prediction ), we ran the following experiment: initially, we distributed a 25% of the training set among the five agents; after that, the rest of the cases in the training set is sent to the agents one by one; when an agent receives a new training case, it has several options: the agent can discard it, the agent can retain it, or the agent can use it for engaging an argumentation process.",
                "Figure 6 shows the result of that experiment for the two data sets.",
                "Figure 6 contains three plots, where NL (not learning) shows accuracy of an agent with no learning at all; L (learning), shows the evolution of the individual classification accuracy when agents learn by retaining the training cases they individually receive (notice that when all the training cases have been retained at 100%, the accuracy should be equal to that of Figure 5 for individual agents); and finally LFC (learning from communication) shows the evolution of the individual classification accuracy of learning agents that also learn by retaining those counterexamples received during argumentation (i.e. they learn both from training examples and counterexamples).",
                "Figure 6 shows that if an agent Ai learns also from communication, Ai can significantly improve its individual performance with just a small number of additional cases (those selected as relevant counterexamples for Ai during argumentation).",
                "For instance, in the soybean data set, individual agents have achieved an accuracy of 70.62% when they also learn from communication versus an accuracy of 59.93% when they only learn from their individual experience.",
                "The number of cases learnt from communication depends on the properties of the data set: in the sponges data set, agents have retained only very few additional cases, and significantly improved individual accuracy; namely they retain 59.96 cases in average (compared to the 50.4 cases retained if they do not learn from communication).",
                "In the soybean data set more counterexamples are learnt to significantly improve individual accuracy, namely they retain 87.16 cases in average (compared to 55.27 cases retained if they do not learn from communication).",
                "Finally, the fact that both data sets show a significant improvement points out the adaptive nature of the argumentation-based approach to learning from communication: the useful cases are selected as counterexamples (and no more than those needed), and they have the intended effect. 10.",
                "RELATED WORK Concerning CBR in a multi-agent setting, the first research was on negotiated case retrieval [11] among groups of agents.",
                "Our work on multi-agent case-based learning started in 1999 [6]; later Mc Ginty and Smyth [7] presented a multi-agent collaborative CBR approach (CCBR) for planning.",
                "Finally, another interesting approach is multi-case-base reasoning (MCBR) [5], that deals with The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 981 SPONGE 60 65 70 75 80 85 25% 40% 55% 70% 85% 100% LFC L NL SOYBEAN 20 30 40 50 60 70 80 90 25% 40% 55% 70% 85% 100% LFC L NL Figure 6: Learning from communication resulting from argumentation in a system composed of 5 agents. distributed systems where there are several case bases available for the same task and addresses the problems of cross-case base adaptation.",
                "The main difference is that our MAC approach is a way to distribute the Reuse process of CBR (using a voting system) while Retrieve is performed individually by each agent; the other multiagent CBR approaches, however, focus on distributing the Retrieve process.",
                "Research on MAS argumentation focus on several issues like a) logics, protocols and languages that support argumentation, b) argument selection and c) argument interpretation.",
                "Approaches for logic and languages that support argumentation include defeasible logic [4] and BDI models [13].",
                "Although argument selection is a key aspect of automated argumentation (see [12] and [13]), most research has been focused on preference relations among arguments.",
                "In our framework we have addressed both argument selection and preference relations using a case-based approach. 11.",
                "CONCLUSIONS AND FUTURE WORK In this paper we have presented an argumentation-based framework for multi-agent learning.",
                "Specifically, we have presented AMAL, a framework that allows a <br>group</br> of learning agents to argue about the solution of a given problem and we have shown how the learning capabilities can be used to generate arguments and counterarguments.",
                "The experimental evaluation shows that the increased amount of information provided to the agents by the argumentation process increases their predictive accuracy, and specially when an adequate number of agents take part in the argumentation.",
                "The main contributions of this work are: a) an argumentation framework for learning agents; b) a case-based preference relation over arguments, based on computing an overall confidence estimation of arguments; c) a case-based policy to generate counterarguments and select counterexamples; and d) an argumentation-based approach for learning from communication.",
                "Finally, in the experiments presented here a learning agent would retain all counterexamples submitted by the other agent; however, this is a very simple case retention policy, and we will like to experiment with more informed policies - with the goal that individual learning agents could significantly improve using only a small set of cases proposed by other agents.",
                "Finally, our approach is focused on lazy learning, and future works aims at incorporating eager inductive learning inside the argumentative framework for learning from communication. 12.",
                "REFERENCES [1] Agnar Aamodt and Enric Plaza.",
                "Case-based reasoning: Foundational issues, methodological variations, and system approaches.",
                "Artificial Intelligence Communications, 7(1):39-59, 1994. [2] E. Armengol and E. Plaza.",
                "Lazy induction of descriptions for relational case-based learning.",
                "In ECML2001, pages 13-24, 2001. [3] Gerhard Brewka.",
                "Dynamic argument systems: A formal model of argumentation processes based on situation calculus.",
                "Journal of Logic and Computation, 11(2):257-282, 2001. [4] Carlos I. Chesñevar and Guillermo R. Simari.",
                "Formalizing Defeasible Argumentation using Labelled Deductive Systems.",
                "Journal of Computer Science & Technology, 1(4):18-33, 2000. [5] D. Leake and R. Sooriamurthi.",
                "Automatically selecting strategies for multi-case-base reasoning.",
                "In S. Craw and A. Preece, editors, ECCBR2002, pages 204-219, Berlin, 2002.",
                "Springer Verlag. [6] Francisco J. Martín, Enric Plaza, and Josep-Lluis Arcos.",
                "Knowledge and experience reuse through communications among competent (peer) agents.",
                "International Journal of Software Engineering and Knowledge Engineering, 9(3):319-341, 1999. [7] Lorraine McGinty and Barry Smyth.",
                "Collaborative case-based reasoning: Applications in personalized route planning.",
                "In I. Watson and Q. Yang, editors, ICCBR, number 2080 in LNAI, pages 362-376.",
                "Springer-Verlag, 2001. [8] Santi Ontañón and Enric Plaza.",
                "Justification-based multiagent learning.",
                "In ICML2003, pages 576-583.",
                "Morgan Kaufmann, 2003. [9] Enric Plaza, Eva Armengol, and Santiago Ontañón.",
                "The explanatory power of symbolic similarity in case-based reasoning.",
                "Artificial Intelligence Review, 24(2):145-161, 2005. [10] David Poole.",
                "On the comparison of theories: Preferring the most specific explanation.",
                "In IJCAI-85, pages 144-147, 1985. [11] M V Nagendra Prassad, Victor R Lesser, and Susan Lander.",
                "Retrieval and reasoning in distributed case bases.",
                "Technical report, UMass Computer Science Department, 1995. [12] K. Sycara S. Kraus and A. Evenchik.",
                "Reaching agreements through argumentation: a logical model and implementation.",
                "Artificial Intelligence Journal, 104:1-69, 1998. [13] N. R. Jennings S. Parsons, C. Sierra.",
                "Agents that reason and negotiate by arguing.",
                "Journal of Logic and Computation, 8:261-292, 1998. [14] Bruce A. Wooley.",
                "Explanation component of software systems.",
                "ACM CrossRoads, 5.1, 1998. 982 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "Therefore, we say a <br>group</br> of agents perform joint deliberation, when they collaborate to find a joint solution by means of an argumentation process.",
                "Figure 2 illustrates the individual evaluation of the confidence of an argument, in particular, three endorsing cases and one counterexample are found in the case base of agents Ai, giving an estimated confidence of 0.6 Moreover, we can also define the joint confidence of an argument α as the confidence computed using the cases present in the case bases of all the agents in the <br>group</br>: C(α) = i Y Ai α 1 + i Y Ai α + NAi α Notice that, to collaboratively compute the joint confidence, the agents only have to make public the aye and nay values locally computed for a given argument.",
                "ARGUMENTATION-BASED MULTI-AGENT LEARNING The interaction protocol of AMAL allows a <br>group</br> of agents A1, ..., An to deliberate about the correct solution of a problem P by means of an argumentation process.",
                "Specifically, we have presented AMAL, a framework that allows a <br>group</br> of learning agents to argue about the solution of a given problem and we have shown how the learning capabilities can be used to generate arguments and counterarguments."
            ],
            "translated_annotated_samples": [
                "Por lo tanto, decimos que un <br>grupo</br> de agentes realiza una deliberación conjunta cuando colaboran para encontrar una solución conjunta mediante un proceso de argumentación.",
                "La Figura 2 ilustra la evaluación individual de la confianza de un argumento, en particular, se encuentran tres casos de respaldo y un contraejemplo en la base de casos de los agentes Ai, lo que da una confianza estimada de 0.6. Además, también podemos definir la confianza conjunta de un argumento α como la confianza calculada utilizando los casos presentes en las bases de casos de todos los agentes en el <br>grupo</br>: C(α) = Π Ai α / (1 + Σ Ai α + NAi α). Nótese que, para calcular conjuntamente la confianza conjunta, los agentes solo tienen que hacer públicos los valores de aprobación y rechazo calculados localmente para un argumento dado.",
                "APRENDIZAJE MULTIAGENTE BASADO EN ARGUMENTACIÓN El protocolo de interacción de AMAL permite que un <br>grupo</br> de agentes A1, ..., An deliberen sobre la solución correcta de un problema P mediante un proceso de argumentación.",
                "Específicamente, hemos presentado AMAL, un marco que permite a un <br>grupo</br> de agentes de aprendizaje discutir sobre la solución de un problema dado y hemos demostrado cómo las capacidades de aprendizaje pueden ser utilizadas para generar argumentos y contraargumentos."
            ],
            "translated_text": "Aprendizaje y deliberación conjunta a través de la argumentación en sistemas multiagente. Santi Ontañón CCL, Laboratorio de Computación Cognitiva, Instituto de Tecnología de Georgia, Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Instituto de Investigación en Inteligencia Artificial, CSIC, Consejo Superior de Investigaciones Científicas, Campus UAB, 08193 Bellaterra, Cataluña (España) enric@iiia.csic.es RESUMEN En este artículo presentaremos un marco de argumentación para agentes de aprendizaje (AMAL) diseñado con dos propósitos: (1) para deliberación conjunta, y (2) para aprender a través de la comunicación. El marco AMAL se basa completamente en el aprendizaje a partir de ejemplos: la relación de preferencia de argumentos, la política de generación de argumentos y la política de generación de contraargumentos son técnicas basadas en casos. Para unirse en la deliberación, los agentes de aprendizaje comparten su experiencia formando un comité para decidir sobre alguna decisión conjunta. Experimentalmente demostramos que la argumentación entre comités de agentes mejora tanto el rendimiento individual como el conjunto. Para aprender a través de la comunicación, un agente se involucra en discusiones con otros agentes para contrastar sus hipótesis individuales y recibir contraejemplos; el proceso de argumentación mejora su alcance de aprendizaje y rendimiento individual. Categorías y Descriptores de Asignaturas I.2.6 [Inteligencia Artificial]: Aprendizaje; I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente, Agentes Inteligentes 1. Los marcos de argumentación para sistemas multiagente pueden ser utilizados para diferentes propósitos como la deliberación conjunta, la persuasión, la negociación y la resolución de conflictos. En este artículo presentaremos un marco de argumentación para agentes de aprendizaje, y mostraremos que puede ser utilizado para dos propósitos: (1) deliberación conjunta, y (2) aprendizaje a partir de la comunicación. La deliberación conjunta basada en argumentos implica discutir sobre el resultado de una situación particular o la acción apropiada a tomar en una situación específica. Los agentes de aprendizaje son capaces de aprender de la experiencia, en el sentido de que ejemplos pasados (situaciones y sus resultados) se utilizan para predecir el resultado para la situación actual. Sin embargo, dado que la experiencia de los agentes individuales puede ser limitada, el conocimiento individual y la precisión de la predicción también son limitados. Por lo tanto, los agentes de aprendizaje que son capaces de argumentar sus predicciones individuales con otros agentes pueden alcanzar una mayor precisión en las predicciones después de dicho proceso de argumentación. La mayoría de los marcos de argumentación existentes para sistemas multiagente se basan en lógica deductiva u otra formalización de lógica deductiva diseñada específicamente para respaldar la argumentación, como la lógica por defecto [3]. Por lo general, un argumento se ve como una declaración lógica, mientras que un contraargumento es un argumento ofrecido en oposición a otro argumento; los agentes utilizan una relación de preferencia para resolver argumentos conflictivos. Sin embargo, los marcos de argumentación basados en lógica asumen agentes con conocimiento predefinido y relación de preferencia. En este artículo, nos enfocamos en un marco de Aprendizaje Multiagente basado en Argumentación (AMAL) donde tanto el conocimiento como la relación de preferencia se aprenden a partir de la experiencia. Por lo tanto, consideramos un escenario con agentes que (1) trabajan en el mismo dominio utilizando una ontología compartida, (2) son capaces de aprender a partir de ejemplos y (3) se comunican utilizando un marco argumentativo. Tener capacidades de aprendizaje permite a los agentes utilizar de manera efectiva una forma específica de contraargumento, a saber, el uso de contraejemplos. Los contraejemplos ofrecen la posibilidad de que los agentes aprendan durante el proceso de argumentación. Además, los agentes de aprendizaje permiten técnicas que utilizan la experiencia aprendida para generar argumentos y contraargumentos adecuados. Específicamente, necesitaremos abordar dos problemas: (1) cómo definir una técnica para generar argumentos y contraargumentos a partir de ejemplos, y (2) cómo definir una relación de preferencia sobre dos argumentos en conflicto que han sido inducidos a partir de ejemplos. Este documento presenta un enfoque basado en casos para abordar ambas cuestiones. Los agentes utilizan el razonamiento basado en casos (CBR) [1] para aprender de casos pasados (donde un caso es una situación y su resultado) con el fin de predecir el resultado de una nueva situación. Proponemos un protocolo de argumentación dentro del marco de AMAL que apoya a los agentes en llegar a una predicción conjunta sobre una situación o problema específico; además, el razonamiento necesario para respaldar el proceso de argumentación también se basará en casos. En particular, presentamos dos medidas basadas en casos, una para generar los argumentos y contraargumentos adecuados para una situación particular y otra para determinar la relación de preferencia entre los argumentos. Finalmente, evaluamos (1) si la argumentación entre agentes de aprendizaje puede producir una predicción conjunta que mejore el rendimiento de aprendizaje individual y (2) si el aprendizaje a partir de los contraejemplos transmitidos durante el proceso de argumentación aumenta el rendimiento individual con precisamente aquellos casos que se utilizan mientras discuten entre ellos. El documento está estructurado de la siguiente manera. La sección 2 discute la relación entre la argumentación, la colaboración y el aprendizaje. Luego, la Sección 3 introduce nuestro marco de trabajo de CBR multiagente (MAC) y el concepto de predicción justificada. Después de eso, la Sección 4 define formalmente nuestro marco de argumentación. Las secciones 5 y 6 presentan nuestra relación de preferencia basada en casos y las políticas de generación de argumentos, respectivamente. Más adelante, la Sección 7 presenta el protocolo de argumentación en nuestro marco AMAL. Después de eso, la Sección 8 presenta una ejemplificación del marco de argumentación. Finalmente, la Sección 9 presenta una evaluación empírica de nuestras dos hipótesis principales. El artículo concluye con secciones de trabajo relacionado y conclusiones. 2. ARGUMENTACIÓN, COLABORACIÓN Y APRENDIZAJE Tanto el aprendizaje como la colaboración son formas en las que un agente puede mejorar el rendimiento individual. De hecho, existe un claro paralelismo entre el aprendizaje y la colaboración en sistemas multiagentes, ya que ambos son formas en las que los agentes pueden lidiar con sus deficiencias. Mostremos cuáles son las principales motivaciones que un agente puede tener para aprender o colaborar. • Motivaciones para aprender: - Aumentar la calidad de la predicción, - Aumentar la eficiencia, - Ampliar el rango de problemas resolubles. • Motivaciones para colaborar: - Aumentar la calidad de la predicción, - Aumentar la eficiencia, - Ampliar el rango de problemas resolubles, - Ampliar el rango de recursos accesibles. Al observar las listas de motivación anteriores, podemos ver fácilmente que el aprendizaje y la colaboración están muy relacionados en los sistemas multiagente. De hecho, con la excepción del último elemento en la lista de motivaciones para colaborar, son dos extremos de un continuo de estrategias para mejorar el rendimiento. Un agente puede optar por aumentar el rendimiento mediante el aprendizaje, la colaboración o encontrando un punto intermedio que combine el aprendizaje y la colaboración para mejorar el rendimiento. En este artículo propondremos AMAL, un marco de argumentación para agentes de aprendizaje, y también mostraremos cómo AMAL puede ser utilizado tanto para aprender a través de la comunicación como para resolver problemas de manera colaborativa: • Los agentes pueden resolver problemas de manera colaborativa al participar en un proceso de argumentación sobre la predicción para la situación actual. Utilizando esta colaboración, la predicción puede realizarse de una manera más informada, ya que se ha tenido en cuenta la información conocida por varios agentes. Los agentes también pueden aprender de la comunicación con otros agentes al participar en un proceso de argumentación. Los agentes que participan en estos procesos de argumentación pueden aprender de los argumentos y contraejemplos recibidos de otros agentes, y utilizar esta información para predecir los resultados de situaciones futuras. En el resto de este documento propondremos un marco de argumentación y mostraremos cómo puede ser utilizado tanto para el aprendizaje como para la resolución de problemas de manera colaborativa. SISTEMAS CBR MULTIAGENTE Un Sistema de Razonamiento Basado en Casos Multiagente (MAC) M = {(A1, C1), ..., (An, Cn)} es un sistema multiagente compuesto por A = {Ai, ..., An}, un conjunto de agentes CBR, donde cada agente Ai ∈ A posee una base de casos individual Ci. Cada agente individual Ai en un MAC es completamente autónomo y cada agente Ai solo tiene acceso a su base de casos individual y privada Ci. Un conjunto de casos Ci = {c1, ..., cm} es una colección de casos. Los agentes en un sistema MAC pueden resolver problemas de forma individual, pero también pueden colaborar con otros agentes para resolver problemas. En este marco, nos limitaremos a tareas analíticas, es decir, tareas como la clasificación, donde la solución de un problema se logra seleccionando una clase de solución de un conjunto enumerado de clases de solución. En lo siguiente, notaremos el conjunto de todas las clases de soluciones por S = {S1, ..., SK}. Por lo tanto, un caso c = P, S es una tupla que contiene una descripción del caso P y una clase de solución S ∈ S. En lo siguiente, utilizaremos indistintamente los términos problema y descripción del caso. Además, utilizaremos la notación de punto para referirnos a elementos dentro de una tupla; por ejemplo, para referirnos a la clase de solución de un caso c, escribiremos c.S. Por lo tanto, decimos que un <br>grupo</br> de agentes realiza una deliberación conjunta cuando colaboran para encontrar una solución conjunta mediante un proceso de argumentación. Sin embargo, para hacerlo, un agente debe ser capaz de justificar su predicción a los otros agentes (es decir, generar un argumento para su solución predicha que pueda ser examinado y criticado por los otros agentes). La siguiente sección aborda este tema. 3.1 Predicciones Justificadas Tanto los sistemas expertos como los sistemas CBR pueden tener un componente de explicación [14] encargado de justificar por qué el sistema ha proporcionado una respuesta específica al usuario. La línea de razonamiento del sistema puede ser examinada por un experto humano, aumentando así la fiabilidad del sistema. La mayoría de los trabajos existentes sobre generación de explicaciones se centran en generar explicaciones para ser proporcionadas al usuario. Sin embargo, en nuestro enfoque utilizamos explicaciones (o justificaciones) como una herramienta para mejorar la comunicación y coordinación entre agentes. Estamos interesados en las justificaciones ya que pueden ser utilizadas como argumentos. Para ese propósito, nos beneficiaremos de la capacidad de algunos métodos de aprendizaje automático para proporcionar justificaciones. Una justificación construida por un método CBR después de determinar que la solución de un problema particular P fue Sk es una descripción que contiene la información relevante del problema P que el método CBR ha considerado para predecir Sk como la solución de P. En particular, los métodos CBR funcionan recuperando casos similares al problema en cuestión, y luego reutilizando sus soluciones para el problema actual, esperando que dado que el problema y los casos son similares, las soluciones también serán similares. Por lo tanto, si un método CBR ha recuperado un conjunto de casos C1, ..., Cn para resolver un problema particular P, la justificación construida contendrá la información relevante del problema P que hizo que el sistema CBR recuperara ese conjunto particular de casos, es decir, contendrá la información relevante que P y C1, ..., Cn tienen en común. Por ejemplo, la Figura 1 muestra una justificación construida por un sistema CBR para un problema de juguete (en las siguientes secciones mostraremos justificaciones para problemas reales). En la figura, un problema tiene dos atributos (Semáforo y Coches_pasando), el mecanismo de recuperación del sistema CBR nota que al considerar solo el atributo Semáforo, puede recuperar dos casos que predicen la misma solución: esperar. Por lo tanto, dado que solo se ha utilizado este atributo, es el único que aparece en la justificación. Los valores del resto de atributos son irrelevantes, ya que, sea cual sea su valor, la clase de solución habría sido la misma. 976 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Problema Semáforo: rojo Pasan coches: no Caso 1 Semáforo: rojo Pasan coches: no Solución: esperar Caso 3 Semáforo: rojo Pasan coches: sí Solución: esperar Caso 4 Semáforo: verde Pasan coches: sí Solución: esperar Caso 2 Semáforo: verde Pasan coches: no Solución: cruzar Casos recuperados Solución: esperar Justificación Semáforo: rojo Figura 1: Un ejemplo de generación de justificación en un sistema de RCB. Observa que, dado que la única característica relevante para decidir es Traffic_light (la única utilizada para recuperar casos), es la única que aparece en la justificación. En general, el significado de una justificación es que todos (o la mayoría) de los casos en la base de casos de un agente que satisfacen la justificación (es decir, todos los casos que están subsumidos por la justificación) pertenecen a la clase de solución predicha. En el resto del documento, usaremos para denotar la relación de subsumpción. En nuestro trabajo, utilizamos LID [2], un método CBR capaz de construir justificaciones simbólicas como la ejemplificada en la Figura 1. Cuando un agente proporciona una justificación para una predicción, el agente genera una predicción justificada: DEFINICIÓN 3.1. Una Predicción Justificada es una tupla J = A, P, S, D donde el agente A considera que S es la solución correcta para el problema P, y esa predicción está justificada por una descripción simbólica D tal que J.D J.P. Las justificaciones pueden tener muchos usos para los sistemas de RCB [8, 9]. En este documento, vamos a utilizar justificaciones como argumentos, con el fin de permitir que los agentes de aprendizaje participen en procesos de argumentación. 4. ARGUMENTOS Y CONTRAARGUMENTOS Para nuestros propósitos, un argumento α generado por un agente A está compuesto por una afirmación S y alguna evidencia D que respalda a S como correcta. En el resto de esta sección veremos cómo esta definición general de argumento puede ser concretada en un tipo específico de argumentos que los agentes pueden generar. En el contexto de los sistemas MAC, los agentes discuten sobre predicciones para nuevos problemas y pueden proporcionar dos tipos de información: a) casos específicos P, S, y b) predicciones justificadas: A, P, S, D. Usando esta información, podemos definir tres tipos de argumentos: predicciones justificadas, contraargumentos y contraejemplos. Una predicción justificada α es generada por un agente Ai para argumentar que Ai cree que la solución correcta para un problema dado P es α.S, y la evidencia proporcionada es la justificación α.D. En el ejemplo representado en la Figura 1, un agente Ai puede generar el argumento α = Ai, P, Esperar, (Semáforo = rojo), lo que significa que el agente Ai cree que la solución correcta para P es Esperar porque el atributo Semáforo es igual a rojo. Un contraargumento β es un argumento ofrecido en oposición a otro argumento α. En nuestro marco de trabajo, un contraargumento consiste en una predicción justificada Aj, P, S, D generada por un agente Aj con la intención de refutar un argumento α generado por otro agente Ai, que respalda una clase de solución S diferente a la de α.S para el problema en cuestión y justifica esto con una justificación D. En el ejemplo de la Figura 1, si un agente genera el argumento α = Ai, P, Caminar, (Pasando_autos = no), un agente que piensa que la solución correcta es Esperar podría responder con el contraargumento β = Aj, P, Esperar, (Pasando_autos = no ∧ Semáforo = rojo), lo que significa que, aunque no hay autos pasando, el semáforo está en rojo y la calle no se puede cruzar. Un contraejemplo c es un caso que contradice un argumento α. Por lo tanto, un contraejemplo también es un contraargumento, uno que afirma que un argumento específico α no siempre es verdadero, y la evidencia proporcionada es el caso c. Específicamente, para que un caso c sea un contraejemplo de un argumento α, se deben cumplir las siguientes condiciones: α.D c y α.S = c.S, es decir, el caso debe satisfacer la justificación α.D y la solución de c debe ser diferente a la predicha por α. Al intercambiar argumentos y contraargumentos (incluyendo contraejemplos), los agentes pueden discutir sobre la solución correcta de un problema dado, es decir, pueden participar en un proceso de deliberación conjunto. Sin embargo, para hacerlo, necesitan un protocolo de interacción específico, una relación de preferencia entre argumentos contradictorios y una política de decisión para generar contraargumentos (incluidos contraejemplos). En las siguientes secciones presentaremos estos elementos. 5. RELACIÓN DE PREFERENCIA Un argumento específico proporcionado por un agente podría no ser consistente con la información conocida por otros agentes (o incluso con parte de la información conocida por el agente que ha generado la justificación debido al ruido en los datos de entrenamiento). Por esa razón, vamos a definir una relación de preferencia sobre predicciones justificadas contradictorias basadas en casos. Básicamente, definiremos una medida de confianza para cada predicción justificada (que tenga en cuenta los casos poseídos por cada agente), y la predicción justificada con la mayor confianza será la preferida. La idea detrás de la confianza basada en casos es contar cuántos de los casos en una base de casos individual respaldan una predicción justificada, y cuántos de ellos son ejemplos en contra de ella. Cuanto más casos de respaldo, mayor es la confianza; y cuanto más contrajemplos, menor es la confianza. Específicamente, para evaluar la confianza de una predicción justificada α, un agente obtiene el conjunto de casos en su base de casos individuales que están subsumidos por α.D. Con ellos, un agente Ai obtiene los valores Y (sí) y N (no): • Y Ai α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| es el número de casos en la base de casos del agente subsumidos por la justificación α.D que pertenecen a la clase de solución α.S, • NAi α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| es el número de casos en la base de casos del agente subsumidos por la justificación α.D que no pertenecen a esa clase de solución. El Sexto Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 977 + + + + + +  - - + Figura 2: La confianza de los argumentos se evalúa contrastándolos con las bases de casos de los agentes. Un agente estima la confianza de un argumento como: CAi (α) = Y Ai α 1 + Y Ai α + NAi α es decir, la confianza en una predicción justificada es el número de casos que la respaldan dividido por el número de casos que la respaldan más los contraejemplos. Observa que sumamos 1 al denominador, esto es para evitar otorgar confianzas excesivamente altas a predicciones justificadas cuya confianza se ha calculado utilizando un número pequeño de casos. Ten en cuenta que esta corrección sigue la misma idea que la corrección de Laplace para estimar probabilidades. La Figura 2 ilustra la evaluación individual de la confianza de un argumento, en particular, se encuentran tres casos de respaldo y un contraejemplo en la base de casos de los agentes Ai, lo que da una confianza estimada de 0.6. Además, también podemos definir la confianza conjunta de un argumento α como la confianza calculada utilizando los casos presentes en las bases de casos de todos los agentes en el <br>grupo</br>: C(α) = Π Ai α / (1 + Σ Ai α + NAi α). Nótese que, para calcular conjuntamente la confianza conjunta, los agentes solo tienen que hacer públicos los valores de aprobación y rechazo calculados localmente para un argumento dado. En nuestro marco de trabajo, los agentes utilizan esta confianza conjunta como la relación de preferencia: una predicción justificada α se prefiere sobre otra β si C(α) ≥ C(β). GENERACIÓN DE ARGUMENTOS En nuestro marco de trabajo, los argumentos son generados por los agentes a partir de casos, utilizando métodos de aprendizaje. Cualquier método de aprendizaje capaz de proporcionar una predicción justificada puede ser utilizado para generar argumentos. Por ejemplo, los árboles de decisión y LID [2] son métodos de aprendizaje adecuados. Específicamente, en los experimentos reportados en este artículo, los agentes utilizan LID. Por lo tanto, cuando un agente desea generar un argumento que respalde que una clase de solución específica es la solución correcta para un problema P, genera una predicción justificada como se explica en la Sección 3.1. Por ejemplo, la Figura 3 muestra una justificación real generada por LID después de resolver un problema P en el dominio de la identificación de esponjas marinas. En particular, la Figura 3 muestra cómo cuando un agente recibe un nuevo problema para resolver (en este caso, un nuevo esponja para determinar su orden), el agente utiliza LID para generar un argumento (que consiste en una predicción justificada) utilizando los casos en la base de casos del agente. La justificación mostrada en la Figura 3 se puede interpretar diciendo que la solución predicha es hadromerida porque la forma lisa de las megascleras del esqueleto espicular de la esponja es de tipo tilostilo, el esqueleto espicular de la esponja no tiene una longitud uniforme y no hay gemas en las características externas de la esponja. Por lo tanto, el argumento generado será α = A1, P, hadromerida, D1. 6.1 Generación de Contraargumentos Como se mencionó anteriormente, los agentes pueden intentar refutar argumentos generando contraargumentos o encontrando contraejemplos. Permítanos explicar cómo pueden ser generados. Un agente Ai quiere generar un contraargumento β para rebatir un argumento α cuando α está en contradicción con la base de casos locales de Ai. Además, al generar dicho contraargumento β, Ai espera que β sea preferido sobre α. Con ese propósito, presentaremos una política específica para generar contraargumentos basados en el criterio de especificidad [10]. El criterio de especificidad se utiliza ampliamente en marcos deductivos para la argumentación, y establece que entre dos argumentos en conflicto, se debe preferir el más específico ya que, en principio, está más informado. Por lo tanto, se espera que los contraargumentos generados basados en el criterio de especificidad sean preferibles (ya que están más informados) que los argumentos que intentan refutar. Sin embargo, no hay garantía de que tales contraargumentos siempre ganen, ya que, como hemos indicado en la Sección 5, los agentes en nuestro marco utilizan una relación de preferencia basada en la confianza conjunta. Además, uno podría pensar que sería mejor que los agentes generaran contraargumentos basados en la relación de preferencia de confianza conjunta; sin embargo, no es obvio cómo generar contraargumentos basados en la confianza conjunta de manera eficiente, ya que se requiere colaboración para evaluar la confianza conjunta. Por lo tanto, el agente que genera el contraargumento debe comunicarse constantemente con los otros agentes en cada paso del algoritmo de inducción utilizado para generar contraargumentos (actualmente una de nuestras líneas de investigación futuras). Por lo tanto, en nuestro marco de trabajo, cuando un agente quiere generar un contraargumento β a un argumento α, β tiene que ser más específico que α (es decir, α.D < β.D). La generación de contraargumentos utilizando el criterio de especificidad impone algunas restricciones sobre el método de aprendizaje, aunque LID o ID3 pueden adaptarse fácilmente para esta tarea. Por ejemplo, LID es un algoritmo que genera una descripción desde cero y añade heurísticamente características a ese término. Por lo tanto, en cada paso, la descripción se vuelve más específica que en el paso anterior, y el número de casos que se incluyen en esa descripción se reduce. Cuando la descripción cubre solo (o casi solo) casos de una única clase de solución, LID termina y predice esa clase de solución. Para generar un contraargumento a un argumento α, LID solo tiene que usar como punto de partida la descripción α.D en lugar de empezar desde cero. De esta manera, la justificación proporcionada por LID siempre estará subsumida por α.D, y por lo tanto, el contraargumento resultante será más específico que α. Sin embargo, hay que tener en cuenta que LID a veces puede no ser capaz de generar contraargumentos, ya que LID puede no ser capaz de especializar aún más la descripción α.D, o porque el agente Ai no tiene ningún caso en Ci que esté subsumido por α.D. La Figura 4 muestra cómo un agente A2 que no estaba de acuerdo con el argumento mostrado en la Figura 3, genera un contraargumento utilizando LID. Además, la Figura 4 muestra la generación de un contraargumento β1 2 para el argumento α0 1 (en la Figura 3) que es una especialización de α0 1. 978 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Solución: hadromerida Justificación: D1 Esponja Esqueleto espiculado Características externas Características externas Gemmulas: no Esqueleto espiculado Megascleres Longitud uniforme: no Megascleres Forma lisa: tilostilo Caso Base de A1 LID Nueva esponja P Figura 3: Ejemplo de una justificación real generada por LID en el conjunto de datos de esponjas marinas. Específicamente, en nuestros experimentos, cuando un agente Ai quiere refutar un argumento α, utiliza la siguiente política: 1. El Agente Ai utiliza LID para intentar encontrar un contraargumento β más específico que α; si se encuentra, β se envía al otro agente como un contraargumento de α. 2. Si no se encuentra, entonces Ai busca un contraejemplo c ∈ Ci de α. Si se encuentra un caso c, entonces c se envía al otro agente como un contraejemplo de α. Si no se encuentran contraejemplos, entonces Ai no puede refutar el argumento α. 7. APRENDIZAJE MULTIAGENTE BASADO EN ARGUMENTACIÓN El protocolo de interacción de AMAL permite que un <br>grupo</br> de agentes A1, ..., An deliberen sobre la solución correcta de un problema P mediante un proceso de argumentación. Si el proceso de argumentación llega a una solución consensuada, la deliberación conjunta termina; de lo contrario, se utiliza una votación ponderada para determinar la solución conjunta. Además, AMAL también permite a los agentes aprender de los contraejemplos recibidos de otros agentes. El protocolo AMAL consiste en una serie de rondas. En la ronda inicial, cada agente declara cuál es su predicción individual para P. Luego, en cada ronda, un agente puede intentar refutar la predicción hecha por cualquiera de los otros agentes. El protocolo utiliza un mecanismo de paso de token para que los agentes (uno a la vez) puedan enviar contraargumentos o contraejemplos si no están de acuerdo con la predicción hecha por cualquier otro agente. Específicamente, a cada agente se le permite enviar un contraargumento o contraejemplo cada vez que recibe el token (nota que esta restricción es solo para simplificar el protocolo, y que no restringe la cantidad de contraargumentos que un agente puede enviar, ya que pueden ser retrasados para rondas posteriores). Cuando un agente recibe un contraargumento o contraejemplo, informa a los otros agentes si acepta el contraargumento (y cambia su predicción) o no. Además, los agentes también tienen la oportunidad de responder a los contraargumentos cuando reciben el token, intentando generar un contraargumento al contraargumento. Cuando todos los agentes hayan tenido el token una vez, el token regresa al primer agente, y así sucesivamente. Si en algún momento del protocolo, todos los agentes están de acuerdo o durante las últimas n rondas ningún agente ha generado ninguna contra-argumentación, el protocolo termina. Además, si al final de la argumentación los agentes no han llegado a un acuerdo, entonces se utiliza un mecanismo de votación que utiliza la confianza de cada predicción como pesos para decidir la solución final (Así, AMAL sigue el mismo mecanismo que los comités humanos, primero cada miembro individual de un comité expone sus argumentos y discute los de los otros miembros (deliberación conjunta), y si no se alcanza un consenso, entonces se requiere un mecanismo de votación). En cada iteración, los agentes pueden usar los siguientes performativos: • afirmar(α): la predicción justificada que se mantendrá durante la próxima ronda será α. Un agente solo puede tener una única predicción en cada ronda, por lo tanto, si se envían múltiples afirmaciones, solo se considera la última como la predicción actualmente mantenida. • rebut(β, α): el agente ha encontrado un contraargumento β a la predicción α. Definiremos Ht = αt 1, ..., αt n como las predicciones que cada uno de los n agentes tiene en una ronda t. Además, también definiremos contradict(αt i) = {α ∈ Ht|α.S = αt i.S} como el conjunto de argumentos contradictorios para un agente Ai en una ronda t, es decir, el conjunto de argumentos en la ronda t que respaldan una clase de solución diferente a la de αt i. El protocolo se inicia porque uno de los agentes recibe un problema P que debe ser resuelto. Después de eso, el agente informa a todos los demás agentes sobre el problema P a resolver, y el protocolo comienza: 1. En la ronda t = 0, cada uno de los agentes resuelve individualmente P y construye una predicción justificada utilizando su propio método de RBC. Entonces, cada agente Ai envía el assertivo performativo assert(α0 i) a los otros agentes. Por lo tanto, los agentes conocen H0 = α0 i , ..., α0 n. Una vez que todas las predicciones hayan sido enviadas, el token se le da al primer agente A1. 2. En cada ronda t (distinta de 0), los agentes verifican si sus argumentos en Ht coinciden. Si lo hacen, el protocolo avanza al paso 5. Además, si durante las últimas n rondas ningún agente ha enviado ningún contraejemplo o contraargumento, el protocolo también avanza al paso 5. De lo contrario, el agente Ai propietario del token intenta generar un contraargumento para cada uno de los argumentos opuestos en contradict(αt i) ⊆ Ht (ver Sección 6.1). Entonces, se selecciona el contraargumento βt i contra la predicción αt j con la menor confianza C(αt j) (ya que αt j es la predicción más probable de ser refutada con éxito). • Si βt i es un contraargumento, entonces, Ai compara localmente αt i con βt i evaluando su confianza contra su base de casos individuales Ci (ver Sección 5) (nótese que Ai está comparando su argumento previo con el contraargumento que Ai mismo acaba de generar y que trata sobre The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 979 Esponja Esqueleto espiculado Características externas Características externas Gemas: no Crecimiento: Esqueleto espiculado Megascleres Longitud uniforme: no Megascleres Forma lisa: tilostilo Crecimiento Crecimiento: masivo Caso Base de A2 LID Solución: astrophorida Justificación: D2 Figura 4: Generación de un contraargumento utilizando LID en el conjunto de datos de esponjas. para enviar a Aj. Si CAi (βt i) > CAi (αt i), entonces Ai considera que βt i es más fuerte que su argumento anterior, cambia su argumento a βt i enviando assert(βt i) al resto de los agentes (la intuición detrás de esto es que dado que un contraargumento también es un argumento, Ai verifica si el nuevo contraargumento es un mejor argumento que el que sostenía anteriormente) y rebut(βt i, αt j) a Aj. De lo contrario (es decir, Si CAi (βt i ) ≤ CAi (αt i), Ai solo enviará rebut(βt i , αt j) a Aj. En cualquiera de las dos situaciones, el protocolo avanza al paso 3. • Si βt i es un contraejemplo c, entonces Ai envía una refutación(c, αt j) a Aj. El protocolo avanza al paso 4. • Si Ai no puede generar ningún contraargumento o contraejemplo, el token se envía al siguiente agente, comienza una nueva ronda t + 1, y el protocolo avanza al estado 2. 3. El agente Aj que ha recibido el contraargumento βt i, lo compara localmente con su propio argumento, αt j, evaluando localmente su confianza. Si CAj (βt i ) > CAj (αt j), entonces Aj aceptará el contraargumento como más fuerte que su propio argumento, y enviará assert(βt i ) a los otros agentes. De lo contrario (es decir, Si CAj (βt i ) ≤ CAj (αt j)), Aj no aceptará el contraargumento e informará a los otros agentes en consecuencia. Cualquiera de las dos situaciones inicia una nueva ronda t + 1, Ai envía el token al siguiente agente y el protocolo vuelve al estado 2. 4. El agente Aj que ha recibido el contraejemplo c lo retiene en su base de casos y genera un nuevo argumento αt+1 j que tiene en cuenta c, e informa al resto de los agentes enviando assert(αt+1 j) a todos ellos. Entonces, Ai envía el token al siguiente agente, comienza una nueva ronda t + 1 y el protocolo vuelve al paso 2. 5. El protocolo finaliza generando una predicción conjunta, de la siguiente manera: si los argumentos en Ht están de acuerdo, entonces su predicción es la predicción conjunta; de lo contrario, se utiliza un mecanismo de votación para decidir la predicción conjunta. El mecanismo de votación utiliza la medida de confianza conjunta como los pesos de votación, de la siguiente manera: S = arg max Sk∈S αi∈Ht|αi.S=Sk C(αi) Además, para evitar iteraciones infinitas, si un agente envía dos veces el mismo argumento o contraargumento al mismo agente, el mensaje no se considera. 8. EJEMPLIFICACIÓN Consideremos un sistema compuesto por tres agentes A1, A2 y A3. Uno de los agentes, A1, recibe un problema P para resolver y decide usar AMAL para resolverlo. Por esa razón, invita a A2 y A3 a participar en el proceso de argumentación. Aceptan la invitación y comienza el protocolo de argumentación. Inicialmente, cada agente genera su predicción individual para P y la transmite a los otros agentes. Por lo tanto, todos ellos pueden calcular H0 = α0 1, α0 2, α0 3. En particular, en este ejemplo: • α0 1 = A1, P, hadromerida, D1 • α0 2 = A2, P, astrophorida, D2 • α0 3 = A3, P, axinellida, D3 A1 comienza poseyendo el token e intenta generar argumentos en contra de α0 2 y α0 3, pero no tiene éxito; sin embargo, tiene un contraejemplo c13 para α0 3. Por lo tanto, A1 envía el mensaje rebut(c13, α03) a A3. A3 incorpora c13 en su base de casos e intenta resolver el problema P nuevamente, ahora teniendo en cuenta c13. A3 elabora la predicción justificada α1 3 = A3, P, hadromerida, D4, y la transmite al resto de los agentes con el mensaje assert(α1 3). Por lo tanto, todos ellos conocen el nuevo H1 = α0 1, α0 2, α1 3. La ronda 1 comienza y A2 obtiene el token. A2 intenta generar contraargumentos para α0 1 y α1 3 y solo logra generar un contraargumento β1 2 = A2, P, astrophorida, D5 contra α1 3. El contraargumento se envía a A3 con el mensaje rebut(β1 2 , α1 3). El agente A3 recibe el contraargumento y evalúa su confianza local. El resultado es que la confianza individual del contraargumento β1 2 es menor que la confianza local de α1 3. Por lo tanto, A3 no acepta el contraargumento, y así H2 = α0 1, α0 2, α1 3. La ronda 2 comienza y A3 recibe el token. A3 genera un contraargumento β2 3 = A3, P, hadromerida, D6 para α0 2 y lo envía a A2 con el mensaje refutar(β2 3 , α0 2). El agente A2 recibe el contraargumento y evalúa su confianza local. El resultado es que la confianza local del contraargumento β2 3 es mayor que la confianza local de α0 2. Por lo tanto, A2 acepta el contraargumento e informa al resto de los agentes con el mensaje assert(β2 3). Después de eso, H3 = α0 1, β2 3 , α1 3. En la Ronda 3, dado que todos los agentes están de acuerdo (todas las predicciones justificadas en H3 predicen hadromerida como la clase de solución), el protocolo finaliza, y A1 (el agente que recibió el problema) considera hadromerida como la solución conjunta para el problema P. 9. EVALUACIÓN EXPERIMENTAL 980 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) SPONGE 75 77 79 81 83 85 87 89 91 2 3 4 5 AMAL Votación Individual SOYBEAN 55 60 65 70 75 80 85 90 2 3 4 5 AMAL Votación Individual Figura 5: Precisión individual y conjunta para 2 a 5 agentes. En esta sección evaluamos empíricamente el marco de argumentación AMAL. Hemos realizado experimentos en dos conjuntos de datos diferentes: soja (del repositorio de aprendizaje automático de UCI) y esponja (un conjunto de datos relacional). El conjunto de datos de soja tiene 307 ejemplos y 19 clases de solución, mientras que el conjunto de datos de esponja tiene 280 ejemplos y 3 clases de solución. En una ejecución experimental, el conjunto de datos se divide en 2 conjuntos: el conjunto de entrenamiento y el conjunto de prueba. Los ejemplos del conjunto de entrenamiento están distribuidos entre 5 agentes diferentes sin replicación, es decir, no hay ningún ejemplo compartido por dos agentes. En la etapa de pruebas, los problemas en el conjunto de pruebas llegan aleatoriamente a uno de los agentes, y su objetivo es predecir la solución correcta. Los experimentos están diseñados para probar dos hipótesis: (H1) que la argumentación es un marco útil para la deliberación conjunta y puede mejorar sobre otros métodos típicos como la votación; y (H2) que el aprendizaje a través de la comunicación mejora el rendimiento individual de un agente de aprendizaje que participa en un proceso de argumentación. Además, también esperamos que la mejora lograda a través de la argumentación aumente a medida que aumente el número de agentes que participan en la argumentación (ya que se tomará en cuenta más información). En relación con H1 (la argumentación es un marco útil para la deliberación conjunta), realizamos 4 experimentos, utilizando 2, 3, 4 y 5 agentes respectivamente (en todos los experimentos cada agente tiene un 20% de los datos de entrenamiento, ya que el entrenamiento siempre se distribuye entre 5 agentes). La Figura 5 muestra el resultado de esos experimentos en los conjuntos de datos de esponjas y soja. La precisión de la clasificación se representa en el eje vertical, y en el eje horizontal se muestra el número de agentes que participaron en los procesos de argumentación. Para cada número de agentes, se muestran tres barras: individual, Voting y AMAL. La barra individual muestra la precisión promedio de las predicciones de los agentes individuales; la barra de votación muestra la precisión promedio de la predicción conjunta lograda mediante votación pero sin argumentación; y finalmente la barra de AMAL muestra la precisión promedio de la predicción conjunta utilizando argumentación. Los resultados mostrados son el promedio de 5 ejecuciones de validación cruzada de 10 pliegues. La Figura 5 muestra que la colaboración (votación y AMAL) supera el rendimiento de la resolución individual de problemas. Además, como esperábamos, la precisión mejora a medida que más agentes colaboran, ya que se tiene en cuenta más información. También podemos ver que AMAL siempre supera al voto estándar, demostrando que las decisiones conjuntas se basan en una mejor información proporcionada por el proceso de argumentación. Por ejemplo, la precisión conjunta para 2 agentes en el conjunto de datos de esponjas es del 87.57% para AMAL y del 86.57% para la votación (mientras que la precisión individual es solo del 80.07%). Además, la mejora lograda por AMAL sobre Voting es aún mayor en el conjunto de datos de soja. La razón es que el conjunto de datos de soja es más difícil (en el sentido de que los agentes necesitan más datos para producir buenas predicciones). Estos resultados experimentales muestran que AMAL aprovecha de manera efectiva la oportunidad de mejora: la precisión es mayor solo porque más agentes han cambiado su opinión durante la argumentación (de lo contrario, lograrían el mismo resultado que la Votación). En relación con H2 (el aprendizaje a partir de la comunicación en procesos de argumentación mejora la predicción individual), llevamos a cabo el siguiente experimento: inicialmente, distribuimos un 25% del conjunto de entrenamiento entre los cinco agentes; después, el resto de los casos en el conjunto de entrenamiento se envía a los agentes uno por uno; cuando un agente recibe un nuevo caso de entrenamiento, tiene varias opciones: el agente puede descartarlo, el agente puede retenerlo, o el agente puede usarlo para participar en un proceso de argumentación. La Figura 6 muestra el resultado de ese experimento para los dos conjuntos de datos. La Figura 6 contiene tres gráficos, donde NL (no aprendizaje) muestra la precisión de un agente sin aprendizaje alguno; L (aprendizaje) muestra la evolución de la precisión de clasificación individual cuando los agentes aprenden reteniendo los casos de entrenamiento que reciben individualmente (notar que cuando todos los casos de entrenamiento han sido retenidos al 100%, la precisión debería ser igual a la de la Figura 5 para los agentes individuales); y finalmente LFC (aprendizaje a partir de la comunicación) muestra la evolución de la precisión de clasificación individual de los agentes de aprendizaje que también aprenden reteniendo los contraejemplos recibidos durante la argumentación (es decir, aprenden tanto de ejemplos de entrenamiento como de contraejemplos). La Figura 6 muestra que si un agente Ai también aprende de la comunicación, Ai puede mejorar significativamente su rendimiento individual con solo un pequeño número de casos adicionales (seleccionados como contraejemplos relevantes para Ai durante la argumentación). Por ejemplo, en el conjunto de datos de soja, los agentes individuales han logrado una precisión del 70.62% cuando también aprenden de la comunicación, en comparación con una precisión del 59.93% cuando solo aprenden de su experiencia individual. El número de casos aprendidos a través de la comunicación depende de las propiedades del conjunto de datos: en el conjunto de datos de esponjas, los agentes solo han retenido muy pocos casos adicionales y han mejorado significativamente la precisión individual; es decir, retienen en promedio 59.96 casos (en comparación con los 50.4 casos retenidos si no aprenden de la comunicación). En el conjunto de datos de soja se aprenden más contraejemplos para mejorar significativamente la precisión individual, es decir, retienen en promedio 87.16 casos (en comparación con 55.27 casos retenidos si no aprenden de la comunicación). Finalmente, el hecho de que ambos conjuntos de datos muestren una mejora significativa señala la naturaleza adaptativa del enfoque basado en la argumentación para el aprendizaje a partir de la comunicación: los casos útiles se seleccionan como contraejemplos (y no más de los necesarios), y tienen el efecto deseado. 10. TRABAJO RELACIONADO En cuanto al RCB en un entorno multiagente, la primera investigación se centró en la recuperación de casos negociada [11] entre grupos de agentes. Nuestro trabajo sobre el aprendizaje basado en casos multiagente comenzó en 1999; más tarde Mc Ginty y Smyth presentaron un enfoque colaborativo de CBR multiagente (CCBR) para la planificación. Finalmente, otro enfoque interesante es el razonamiento de múltiples bases de casos (MCBR) [5], que trata sobre The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 981 ESPONJA 60 65 70 75 80 85 25% 40% 55% 70% 85% 100% LFC L NL SOJA 20 30 40 50 60 70 80 90 25% 40% 55% 70% 85% 100% LFC L NL Figura 6: Aprendizaje a partir de la comunicación resultante de la argumentación en un sistema compuesto por 5 agentes. sistemas distribuidos donde hay varias bases de casos disponibles para la misma tarea y aborda los problemas de adaptación entre bases de casos. La principal diferencia es que nuestro enfoque MAC es una forma de distribuir el proceso de Reutilización de CBR (utilizando un sistema de votación) mientras que la Recuperación se realiza individualmente por cada agente; sin embargo, los otros enfoques de CBR multiagente se centran en distribuir el proceso de Recuperación. La investigación sobre la argumentación en MAS se centra en varios temas como a) lógicas, protocolos y lenguajes que respaldan la argumentación, b) selección de argumentos y c) interpretación de argumentos. Los enfoques para la lógica y los lenguajes que respaldan la argumentación incluyen la lógica no definitiva [4] y los modelos BDI [13]. Aunque la selección de argumentos es un aspecto clave de la argumentación automatizada (ver [12] y [13]), la mayoría de las investigaciones se han centrado en las relaciones de preferencia entre argumentos. En nuestro marco de trabajo hemos abordado tanto la selección de argumentos como las relaciones de preferencia utilizando un enfoque basado en casos. 11. CONCLUSIONES Y TRABAJO FUTURO En este artículo hemos presentado un marco basado en argumentación para el aprendizaje multiagente. Específicamente, hemos presentado AMAL, un marco que permite a un <br>grupo</br> de agentes de aprendizaje discutir sobre la solución de un problema dado y hemos demostrado cómo las capacidades de aprendizaje pueden ser utilizadas para generar argumentos y contraargumentos. La evaluación experimental muestra que la cantidad aumentada de información proporcionada a los agentes por el proceso de argumentación aumenta su precisión predictiva, especialmente cuando un número adecuado de agentes participa en la argumentación. Las principales contribuciones de este trabajo son: a) un marco de argumentación para agentes de aprendizaje; b) una relación de preferencia basada en casos sobre argumentos, basada en el cálculo de una estimación general de confianza de los argumentos; c) una política basada en casos para generar contraargumentos y seleccionar contraejemplos; y d) un enfoque basado en la argumentación para aprender a partir de la comunicación. Finalmente, en los experimentos presentados aquí, un agente de aprendizaje retendría todos los contraejemplos presentados por el otro agente; sin embargo, esta es una política de retención de casos muy simple, y nos gustaría experimentar con políticas más informadas, con el objetivo de que los agentes de aprendizaje individuales puedan mejorar significativamente utilizando solo un pequeño conjunto de casos propuestos por otros agentes. Finalmente, nuestro enfoque se centra en el aprendizaje perezoso, y los trabajos futuros tienen como objetivo incorporar el aprendizaje inductivo ansioso dentro del marco argumentativo para el aprendizaje a partir de la comunicación. 12. REFERENCIAS [1] Agnar Aamodt y Enric Plaza. Razonamiento basado en casos: Aspectos fundamentales, variaciones metodológicas y enfoques de sistemas. Comunicaciones de Inteligencia Artificial, 7(1):39-59, 1994. [2] E. Armengol y E. Plaza. Inducción perezosa de descripciones para el aprendizaje basado en casos relacionales. En ECML2001, páginas 13-24, 2001. [3] Gerhard Brewka. Sistemas de argumentación dinámica: Un modelo formal de procesos de argumentación basado en el cálculo de situaciones. Revista de Lógica y Computación, 11(2):257-282, 2001. [4] Carlos I. Chesñevar y Guillermo R. Simari. Formalizando la argumentación no definitiva utilizando sistemas deductivos etiquetados. Revista de Ciencias de la Computación y Tecnología, 1(4):18-33, 2000. [5] D. Leake y R. Sooriamurthi. Seleccionando estrategias automáticamente para el razonamiento multi-base de casos. En S. Craw y A. Preece, editores, ECCBR2002, páginas 204-219, Berlín, 2002. Springer Verlag. [6] Francisco J. Martín, Enric Plaza y Josep-Lluis Arcos. Reutilización de conocimiento y experiencia a través de comunicaciones entre agentes competentes (pares). Revista Internacional de Ingeniería de Software e Ingeniería del Conocimiento, 9(3):319-341, 1999. [7] Lorraine McGinty y Barry Smyth. Razonamiento basado en casos colaborativo: Aplicaciones en la planificación de rutas personalizadas. En I. Watson y Q. Yang, editores, ICCBR, número 2080 en LNAI, páginas 362-376. Springer-Verlag, 2001. [8] Santi Ontañón y Enric Plaza. Aprendizaje multiagente basado en justificaciones. En ICML2003, páginas 576-583. Morgan Kaufmann, 2003. [9] Enric Plaza, Eva Armengol y Santiago Ontañón. El poder explicativo de la similitud simbólica en el razonamiento basado en casos. Revisión de Inteligencia Artificial, 24(2):145-161, 2005. [10] David Poole. En la comparación de teorías: prefiriendo la explicación más específica. En IJCAI-85, páginas 144-147, 1985. [11] M V Nagendra Prassad, Victor R Lesser y Susan Lander. Recuperación y razonamiento en bases de casos distribuidas. Informe técnico, Departamento de Ciencias de la Computación de UMass, 1995. [12] K. Sycara S. Kraus y A. Evenchik. Alcanzando acuerdos a través de la argumentación: un modelo lógico y su implementación. Revista de Inteligencia Artificial, 104:1-69, 1998. [13] N. R. Jennings, S. Parsons, C. Sierra. Agentes que razonan y negocian mediante argumentos. Revista de Lógica y Computación, 8:261-292, 1998. [14] Bruce A. Wooley. Componente de explicación de sistemas de software. ACM CrossRoads, 5.1, 1998. 982 The Sixth Intl. \n\nACM CrossRoads, 5.1, 1998. 982 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "predictive accuracy": {
            "translated_key": "precisión predictiva",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning and Joint Deliberation through Argumentation in Multi-Agent Systems Santi Ontañón CCL, Cognitive Computing Lab Georgia Institute of Technology Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Artificial Intelligence Research Institute CSIC, Spanish Council for Scientific Research Campus UAB, 08193 Bellaterra, Catalonia (Spain) enric@iiia.csic.es ABSTRACT In this paper we will present an argumentation framework for learning agents (AMAL) designed for two purposes: (1) for joint deliberation, and (2) for learning from communication.",
                "The AMAL framework is completely based on learning from examples: the argument preference relation, the argument generation policy, and the counterargument generation policy are case-based techniques.",
                "For join deliberation, learning agents share their experience by forming a committee to decide upon some joint decision.",
                "We experimentally show that the argumentation among committees of agents improves both the individual and joint performance.",
                "For learning from communication, an agent engages into arguing with other agents in order to contrast its individual hypotheses and receive counterexamples; the argumentation process improves their learning scope and individual performance.",
                "Categories and Subject Descriptors I.2.6 [Artificial Intelligence]: Learning; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems, Intelligent Agents 1.",
                "INTRODUCTION Argumentation frameworks for multi-agent systems can be used for different purposes like joint deliberation, persuasion, negotiation, and conflict resolution.",
                "In this paper we will present an argumentation framework for learning agents, and show that it can be used for two purposes: (1) joint deliberation, and (2) learning from communication.",
                "Argumentation-based joint deliberation involves discussion over the outcome of a particular situation or the appropriate course of action for a particular situation.",
                "Learning agents are capable of learning from experience, in the sense that past examples (situations and their outcomes) are used to predict the outcome for the situation at hand.",
                "However, since individual agents experience may be limited, individual knowledge and prediction accuracy is also limited.",
                "Thus, learning agents that are capable of arguing their individual predictions with other agents may reach better prediction accuracy after such an argumentation process.",
                "Most existing argumentation frameworks for multi-agent systems are based on deductive logic or some other deductive logic formalism specifically designed to support argumentation, such as default logic [3]).",
                "Usually, an argument is seen as a logical statement, while a counterargument is an argument offered in opposition to another argument [4, 13]; agents use a preference relation to resolve conflicting arguments.",
                "However, logic-based argumentation frameworks assume agents with preloaded knowledge and preference relation.",
                "In this paper, we focus on an Argumentation-based Multi-Agent Learning (AMAL) framework where both knowledge and preference relation are learned from experience.",
                "Thus, we consider a scenario with agents that (1) work in the same domain using a shared ontology, (2) are capable of learning from examples, and (3) communicate using an argumentative framework.",
                "Having learning capabilities allows agents effectively use a specific form of counterargument, namely the use of counterexamples.",
                "Counterexamples offer the possibility of agents learning during the argumentation process.",
                "Moreover, learning agents allow techniques that use learnt experience to generate adequate arguments and counterarguments.",
                "Specifically, we will need to address two issues: (1) how to define a technique to generate arguments and counterarguments from examples, and (2)how to define a preference relation over two conflicting arguments that have been induced from examples.",
                "This paper presents a case-based approach to address both issues.",
                "The agents use case-based reasoning (CBR) [1] to learn from past cases (where a case is a situation and its outcome) in order to predict the outcome of a new situation.",
                "We propose an argumentation protocol inside the AMAL framework at supports agents in reaching a joint prediction over a specific situation or problem - moreover, the reasoning needed to support the argumentation process will also be based on cases.",
                "In particular, we present two case-based measures, one for generating the arguments and counterarguments adequate to a particular situation and another for determining preference relation among arguments.",
                "Finally, we evaluate (1) if argumentation between learning agents can produce a joint prediction that improves over individual learning performance and (2) if learning from the counterexamples conveyed during the argumentation process increases the individual performance with precisely those cases being used while arguing among them.",
                "The paper is structured as follows.",
                "Section 2 discusses the relation among argumentation, collaboration and learning.",
                "Then Section 3 introduces our multi-agent CBR (MAC) framework and the notion of justified prediction.",
                "After that, Section 4 formally defines our argumentation framework.",
                "Sections 5 and 6 present our case-based preference relation and argument generation policies respectively.",
                "Later, Section 7 presents the argumentation protocol in our AMAL framework.",
                "After that, Section 8 presents an exemplification of the argumentation framework.",
                "Finally, Section 9 presents an empirical evaluation of our two main hypotheses.",
                "The paper closes with related work and conclusions sections. 2.",
                "ARGUMENTATION,COLLABORATION AND LEARNING Both learning and collaboration are ways in which an agent can improve individual performance.",
                "In fact, there is a clear parallelism between learning and collaboration in multi-agent systems, since both are ways in which agents can deal with their shortcomings.",
                "Let us show which are the main motivations that an agent can have to learn or to collaborate. • Motivations to learn: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems. • Motivations to collaborate: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems, - Increase the range of accessible resources.",
                "Looking at the above lists of motivation, we can easily see that learning and collaboration are very related in multi-agent systems.",
                "In fact, with the exception of the last item in the motivations to collaborate list, they are two extremes of a continuum of strategies to improve performance.",
                "An agent may choose to increase performance by learning, by collaborating, or by finding an intermediate point that combines learning and collaboration in order to improve performance.",
                "In this paper we will propose AMAL, an argumentation framework for learning agents, and will also also show how AMAL can be used both for learning from communication and for solving problems in a collaborative way: • Agents can solve problems in a collaborative way via engaging an argumentation process about the prediction for the situation at hand.",
                "Using this collaboration, the prediction can be done in a more informed way, since the information known by several agents has been taken into account. • Agents can also learn from communication with other agents by engaging an argumentation process.",
                "Agents that engage in such argumentation processes can learn from the arguments and counterexamples received from other agents, and use this information for predicting the outcomes of future situations.",
                "In the rest of this paper we will propose an argumentation framework and show how it can be used both for learning and for solving problems in a collaborative way. 3.",
                "MULTI-AGENT CBR SYSTEMS A Multi-Agent Case Based Reasoning System (MAC) M = {(A1, C1), ..., (An, Cn)} is a multi-agent system composed of A = {Ai, ..., An}, a set of CBR agents, where each agent Ai ∈ A possesses an individual case base Ci.",
                "Each individual agent Ai in a MAC is completely autonomous and each agent Ai has access only to its individual and private case base Ci.",
                "A case base Ci = {c1, ..., cm} is a collection of cases.",
                "Agents in a MAC system are able to individually solve problems, but they can also collaborate with other agents to solve problems.",
                "In this framework, we will restrict ourselves to analytical tasks, i.e. tasks like classification, where the solution of a problem is achieved by selecting a solution class from an enumerated set of solution classes.",
                "In the following we will note the set of all the solution classes by S = {S1, ..., SK }.",
                "Therefore, a case c = P, S is a tuple containing a case description P and a solution class S ∈ S. In the following, we will use the terms problem and case description indistinctly.",
                "Moreover, we will use the dot notation to refer to elements inside a tuple; e.g., to refer to the solution class of a case c, we will write c.S.",
                "Therefore, we say a group of agents perform joint deliberation, when they collaborate to find a joint solution by means of an argumentation process.",
                "However, in order to do so, an agent has to be able to justify its prediction to the other agents (i.e. generate an argument for its predicted solution that can be examined and critiqued by the other agents).",
                "The next section addresses this issue. 3.1 Justified Predictions Both expert systems and CBR systems may have an explanation component [14] in charge of justifying why the system has provided a specific answer to the user.",
                "The line of reasoning of the system can then be examined by a human expert, thus increasing the reliability of the system.",
                "Most of the existing work on explanation generation focuses on generating explanations to be provided to the user.",
                "However, in our approach we use explanations (or justifications) as a tool for improving communication and coordination among agents.",
                "We are interested in justifications since they can be used as arguments.",
                "For that purpose, we will benefit from the ability of some machine learning methods to provide justifications.",
                "A justification built by a CBR method after determining that the solution of a particular problem P was Sk is a description that contains the relevant information from the problem P that the CBR method has considered to predict Sk as the solution of P. In particular, CBR methods work by retrieving similar cases to the problem at hand, and then reusing their solutions for the current problem, expecting that since the problem and the cases are similar, the solutions will also be similar.",
                "Thus, if a CBR method has retrieved a set of cases C1, ..., Cn to solve a particular problem P the justification built will contain the relevant information from the problem P that made the CBR system retrieve that particular set of cases, i.e. it will contain the relevant information that P and C1, ..., Cn have in common.",
                "For example, Figure 1 shows a justification build by a CBR system for a toy problem (in the following sections we will show justifications for real problems).",
                "In the figure, a problem has two attributes (Traffic_light, and Cars_passing), the retrieval mechanism of the CBR system notices that by considering only the attribute Traffic_light, it can retrieve two cases that predict the same solution: wait.",
                "Thus, since only this attribute has been used, it is the only one appearing in the justification.",
                "The values of the rest of attributes are irrelevant, since whatever their value the solution class would have been the same. 976 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Problem Traffic_light: red Cars_passing: no Case 1 Traffic_light: red Cars_passing: no Solution: wait Case 3 Traffic_light: red Cars_passing: yes Solution: wait Case 4 Traffic_light: green Cars_passing: yes Solution: wait Case 2 Traffic_light: green Cars_passing: no Solution: cross Retrieved cases Solution: wait Justification Traffic_light: red Figure 1: An example of justification generation in a CBR system.",
                "Notice that, since the only relevant feature to decide is Traffic_light (the only one used to retrieve cases), it is the only one appearing in the justification.",
                "In general, the meaning of a justification is that all (or most of) the cases in the case base of an agent that satisfy the justification (i.e. all the cases that are subsumed by the justification) belong to the predicted solution class.",
                "In the rest of the paper, we will use to denote the subsumption relation.",
                "In our work, we use LID [2], a CBR method capable of building symbolic justifications such as the one exemplified in Figure 1.",
                "When an agent provides a justification for a prediction, the agent generates a justified prediction: DEFINITION 3.1.",
                "A Justified Prediction is a tuple J = A, P, S, D where agent A considers S the correct solution for problem P, and that prediction is justified a symbolic description D such that J.D J.P. Justifications can have many uses for CBR systems [8, 9].",
                "In this paper, we are going to use justifications as arguments, in order to allow learning agents to engage in argumentation processes. 4.",
                "ARGUMENTS AND COUNTERARGUMENTS For our purposes an argument α generated by an agent A is composed of a statement S and some evidence D supporting S as correct.",
                "In the remainder of this section we will see how this general definition of argument can be instantiated in specific kind of arguments that the agents can generate.",
                "In the context of MAC systems, agents argue about predictions for new problems and can provide two kinds of information: a) specific cases P, S , and b) justified predictions: A, P, S, D .",
                "Using this information, we can define three types of arguments: justified predictions, counterarguments, and counterexamples.",
                "A justified prediction α is generated by an agent Ai to argue that Ai believes that the correct solution for a given problem P is α.S, and the evidence provided is the justification α.D.",
                "In the example depicted in Figure 1, an agent Ai may generate the argument α = Ai, P, Wait, (Traffic_light = red) , meaning that the agent Ai believes that the correct solution for P is Wait because the attribute Traffic_light equals red.",
                "A counterargument β is an argument offered in opposition to another argument α.",
                "In our framework, a counterargument consists of a justified prediction Aj, P, S , D generated by an agent Aj with the intention to rebut an argument α generated by another agent Ai, that endorses a solution class S different from that of α.S for the problem at hand and justifies this with a justification D .",
                "In the example in Figure 1, if an agent generates the argument α = Ai, P, Walk, (Cars_passing = no) , an agent that thinks that the correct solution is Wait might answer with the counterargument β = Aj, P, Wait, (Cars_passing = no ∧ Traffic_light = red) , meaning that, although there are no cars passing, the traffic light is red, and the street cannot be crossed.",
                "A counterexample c is a case that contradicts an argument α.",
                "Thus a counterexample is also a counterargument, one that states that a specific argument α is not always true, and the evidence provided is the case c. Specifically, for a case c to be a counterexample of an argument α, the following conditions have to be met: α.D c and α.S = c.S, i.e. the case must satisfy the justification α.D and the solution of c must be different than the predicted by α.",
                "By exchanging arguments and counterarguments (including counterexamples), agents can argue about the correct solution of a given problem, i.e. they can engage a joint deliberation process.",
                "However, in order to do so, they need a specific interaction protocol, a preference relation between contradicting arguments, and a decision policy to generate counterarguments (including counterexamples).",
                "In the following sections we will present these elements. 5.",
                "PREFERENCE RELATION A specific argument provided by an agent might not be consistent with the information known to other agents (or even to some of the information known by the agent that has generated the justification due to noise in training data).",
                "For that reason, we are going to define a preference relation over contradicting justified predictions based on cases.",
                "Basically, we will define a confidence measure for each justified prediction (that takes into account the cases owned by each agent), and the justified prediction with the highest confidence will be the preferred one.",
                "The idea behind case-based confidence is to count how many of the cases in an individual case base endorse a justified prediction, and how many of them are counterexamples of it.",
                "The more the endorsing cases, the higher the confidence; and the more the counterexamples, the lower the confidence.",
                "Specifically, to assess the confidence of a justified prediction α, an agent obtains the set of cases in its individual case base that are subsumed by α.D.",
                "With them, an agent Ai obtains the Y (aye) and N (nay) values: • Y Ai α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by the justification α.D that belong to the solution class α.S, • NAi α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by justification α.D that do not belong to that solution class.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 977 + + + + + +  - - + Figure 2: Confidence of arguments is evaluated by contrasting them against the case bases of the agents.",
                "An agent estimates the confidence of an argument as: CAi (α) = Y Ai α 1 + Y Ai α + NAi α i.e. the confidence on a justified prediction is the number of endorsing cases divided by the number of endorsing cases plus counterexamples.",
                "Notice that we add 1 to the denominator, this is to avoid giving excessively high confidences to justified predictions whose confidence has been computed using a small number of cases.",
                "Notice that this correction follows the same idea than the Laplace correction to estimate probabilities.",
                "Figure 2 illustrates the individual evaluation of the confidence of an argument, in particular, three endorsing cases and one counterexample are found in the case base of agents Ai, giving an estimated confidence of 0.6 Moreover, we can also define the joint confidence of an argument α as the confidence computed using the cases present in the case bases of all the agents in the group: C(α) = i Y Ai α 1 + i Y Ai α + NAi α Notice that, to collaboratively compute the joint confidence, the agents only have to make public the aye and nay values locally computed for a given argument.",
                "In our framework, agents use this joint confidence as the preference relation: a justified prediction α is preferred over another one β if C(α) ≥ C(β). 6.",
                "GENERATION OF ARGUMENTS In our framework, arguments are generated by the agents from cases, using learning methods.",
                "Any learning method able to provide a justified prediction can be used to generate arguments.",
                "For instance, decision trees and LID [2] are suitable learning methods.",
                "Specifically, in the experiments reported in this paper agents use LID.",
                "Thus, when an agent wants to generate an argument endorsing that a specific solution class is the correct solution for a problem P, it generates a justified prediction as explained in Section 3.1.",
                "For instance, Figure 3 shows a real justification generated by LID after solving a problem P in the domain of marine sponges identification.",
                "In particular, Figure 3 shows how when an agent receives a new problem to solve (in this case, a new sponge to determine its order), the agent uses LID to generate an argument (consisting on a justified prediction) using the cases in the case base of the agent.",
                "The justification shown in Figure 3 can be interpreted saying that the predicted solution is hadromerida because the smooth form of the megascleres of the spiculate skeleton of the sponge is of type tylostyle, the spikulate skeleton of the sponge has no uniform length, and there is no gemmules in the external features of the sponge.",
                "Thus, the argument generated will be α = A1, P, hadromerida, D1 . 6.1 Generation of Counterarguments As previously stated, agents may try to rebut arguments by generating counterargument or by finding counterexamples.",
                "Let us explain how they can be generated.",
                "An agent Ai wants to generate a counterargument β to rebut an argument α when α is in contradiction with the local case base of Ai.",
                "Moreover, while generating such counterargument β, Ai expects that β is preferred over α.",
                "For that purpose, we will present a specific policy to generate counterarguments based on the specificity criterion [10].",
                "The specificity criterion is widely used in deductive frameworks for argumentation, and states that between two conflicting arguments, the most specific should be preferred since it is, in principle, more informed.",
                "Thus, counterarguments generated based on the specificity criterion are expected to be preferable (since they are more informed) to the arguments they try to rebut.",
                "However, there is no guarantee that such counterarguments will always win, since, as we have stated in Section 5, agents in our framework use a preference relation based on joint confidence.",
                "Moreover, one may think that it would be better that the agents generate counterarguments based on the joint confidence preference relation; however it is not obvious how to generate counterarguments based on joint confidence in an efficient way, since collaboration is required in order to evaluate joint confidence.",
                "Thus, the agent generating the counterargument should constantly communicate with the other agents at each step of the induction algorithm used to generate counterarguments (presently one of our future research lines).",
                "Thus, in our framework, when an agent wants to generate a counterargument β to an argument α, β has to be more specific than α (i.e. α.D < β.D).",
                "The generation of counterarguments using the specificity criterion imposes some restrictions over the learning method, although LID or ID3 can be easily adapted for this task.",
                "For instance, LID is an algorithm that generates a description starting from scratch and heuristically adding features to that term.",
                "Thus, at every step, the description is made more specific than in the previous step, and the number of cases that are subsumed by that description is reduced.",
                "When the description covers only (or almost only) cases of a single solution class LID terminates and predicts that solution class.",
                "To generate a counterargument to an argument α LID just has to use as starting point the description α.D instead of starting from scratch.",
                "In this way, the justification provided by LID will always be subsumed by α.D, and thus the resulting counterargument will be more specific than α.",
                "However, notice that LID may sometimes not be able to generate counterarguments, since LID may not be able to specialize the description α.D any further, or because the agent Ai has no case inCi that is subsumed by α.D.",
                "Figure 4 shows how an agent A2 that disagreed with the argument shown in Figure 3, generates a counterargument using LID.",
                "Moreover, Figure 4 shows the generation of a counterargument β1 2 for the argument α0 1 (in Figure 3) that is a specialization of α0 1. 978 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Solution: hadromerida Justification: D1 Sponge Spikulate skeleton External features External features Gemmules: no Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Case Base of A1 LID New sponge P Figure 3: Example of a real justification generated by LID in the marine sponges data set.",
                "Specifically, in our experiments, when an agent Ai wants to rebut an argument α, uses the following policy: 1.",
                "Agent Ai uses LID to try to find a counterargument β more specific than α; if found, β is sent to the other agent as a counterargument of α. 2.",
                "If not found, then Ai searches for a counterexample c ∈ Ci of α.",
                "If a case c is found, then c is sent to the other agent as a counterexample of α. 3.",
                "If no counterexamples are found, then Ai cannot rebut the argument α. 7.",
                "ARGUMENTATION-BASED MULTI-AGENT LEARNING The interaction protocol of AMAL allows a group of agents A1, ..., An to deliberate about the correct solution of a problem P by means of an argumentation process.",
                "If the argumentation process arrives to a consensual solution, the joint deliberation ends; otherwise a weighted vote is used to determine the joint solution.",
                "Moreover, AMAL also allows the agents to learn from the counterexamples received from other agents.",
                "The AMAL protocol consists on a series of rounds.",
                "In the initial round, each agent states which is its individual prediction for P. Then, at each round an agent can try to rebut the prediction made by any of the other agents.",
                "The protocol uses a token passing mechanism so that agents (one at a time) can send counterarguments or counterexamples if they disagree with the prediction made by any other agent.",
                "Specifically, each agent is allowed to send one counterargument or counterexample each time he gets the token (notice that this restriction is just to simplify the protocol, and that it does not restrict the number of counterargument an agent can sent, since they can be delayed for subsequent rounds).",
                "When an agent receives a counterargument or counterexample, it informs the other agents if it accepts the counterargument (and changes its prediction) or not.",
                "Moreover, agents have also the opportunity to answer to counterarguments when they receive the token, by trying to generate a counterargument to the counterargument.",
                "When all the agents have had the token once, the token returns to the first agent, and so on.",
                "If at any time in the protocol, all the agents agree or during the last n rounds no agent has generated any counterargument, the protocol ends.",
                "Moreover, if at the end of the argumentation the agents have not reached an agreement, then a voting mechanism that uses the confidence of each prediction as weights is used to decide the final solution (Thus, AMAL follows the same mechanism as human committees, first each individual member of a committee exposes his arguments and discuses those of the other members (joint deliberation), and if no consensus is reached, then a voting mechanism is required).",
                "At each iteration, agents can use the following performatives: • assert(α): the justified prediction held during the next round will be α.",
                "An agent can only hold a single prediction at each round, thus is multiple asserts are send, only the last one is considered as the currently held prediction. • rebut(β, α): the agent has found a counterargument β to the prediction α.",
                "We will define Ht = αt 1, ..., αt n as the predictions that each of the n agents hold at a round t. Moreover, we will also define contradict(αt i) = {α ∈ Ht|α.S = αt i.S} as the set of contradicting arguments for an agent Ai in a round t, i.e. the set of arguments at round t that support a different solution class than αt i.",
                "The protocol is initiated because one of the agents receives a problem P to be solved.",
                "After that, the agent informs all the other agents about the problem P to solve, and the protocol starts: 1.",
                "At round t = 0, each one of the agents individually solves P, and builds a justified prediction using its own CBR method.",
                "Then, each agent Ai sends the performative assert(α0 i ) to the other agents.",
                "Thus, the agents know H0 = α0 i , ..., α0 n .",
                "Once all the predictions have been sent the token is given to the first agent A1. 2.",
                "At each round t (other than 0), the agents check whether their arguments in Ht agree.",
                "If they do, the protocol moves to step 5.",
                "Moreover, if during the last n rounds no agent has sent any counterexample or counterargument, the protocol also moves to step 5.",
                "Otherwise, the agent Ai owner of the token tries to generate a counterargument for each of the opposing arguments in contradict(αt i) ⊆ Ht (see Section 6.1).",
                "Then, the counterargument βt i against the prediction αt j with the lowest confidence C(αt j) is selected (since αt j is the prediction more likely to be successfully rebutted). • If βt i is a counterargument, then, Ai locally compares αt i with βt i by assessing their confidence against its individual case base Ci (see Section 5) (notice that Ai is comparing its previous argument with the counterargument that Ai itself has just generated and that is about The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 979 Sponge Spikulate skeleton External features External features Gemmules: no Growing: Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Growing Grow: massive Case Base of A2 LID Solution: astrophorida Justification: D2 Figure 4: Generation of a counterargument using LID in the sponges data set. to send to Aj).",
                "If CAi (βt i ) > CAi (αt i), then Ai considers that βt i is stronger than its previous argument, changes its argument to βt i by sending assert(βt i ) to the rest of the agents (the intuition behind this is that since a counterargument is also an argument, Ai checks if the newly counterargument is a better argument than the one he was previously holding) and rebut(βt i , αt j) to Aj.",
                "Otherwise (i.e.",
                "CAi (βt i ) ≤ CAi (αt i)), Ai will send only rebut(βt i , αt j) to Aj.",
                "In any of the two situations the protocol moves to step 3. • If βt i is a counterexample c, then Ai sends rebut(c, αt j) to Aj.",
                "The protocol moves to step 4. • If Ai cannot generate any counterargument or counterexample, the token is sent to the next agent, a new round t + 1 starts, and the protocol moves to state 2. 3.",
                "The agent Aj that has received the counterargument βt i , locally compares it against its own argument, αt j, by locally assessing their confidence.",
                "If CAj (βt i ) > CAj (αt j), then Aj will accept the counterargument as stronger than its own argument, and it will send assert(βt i ) to the other agents.",
                "Otherwise (i.e.",
                "CAj (βt i ) ≤ CAj (αt j)), Aj will not accept the counterargument, and will inform the other agents accordingly.",
                "Any of the two situations start a new round t + 1, Ai sends the token to the next agent, and the protocol moves back to state 2. 4.",
                "The agent Aj that has received the counterexample c retains it into its case base and generates a new argument αt+1 j that takes into account c, and informs the rest of the agents by sending assert(αt+1 j ) to all of them.",
                "Then, Ai sends the token to the next agent, a new round t + 1 starts, and the protocol moves back to step 2. 5.",
                "The protocol ends yielding a joint prediction, as follows: if the arguments in Ht agree then their prediction is the joint prediction, otherwise a voting mechanism is used to decide the joint prediction.",
                "The voting mechanism uses the joint confidence measure as the voting weights, as follows: S = arg max Sk∈S αi∈Ht|αi.S=Sk C(αi) Moreover, in order to avoid infinite iterations, if an agent sends twice the same argument or counterargument to the same agent, the message is not considered. 8.",
                "EXEMPLIFICATION Let us consider a system composed of three agents A1, A2 and A3.",
                "One of the agents, A1 receives a problem P to solve, and decides to use AMAL to solve it.",
                "For that reason, invites A2 and A3 to take part in the argumentation process.",
                "They accept the invitation, and the argumentation protocol starts.",
                "Initially, each agent generates its individual prediction for P, and broadcasts it to the other agents.",
                "Thus, all of them can compute H0 = α0 1, α0 2, α0 3 .",
                "In particular, in this example: • α0 1 = A1, P, hadromerida, D1 • α0 2 = A2, P, astrophorida, D2 • α0 3 = A3, P, axinellida, D3 A1 starts owning the token and tries to generate counterarguments for α0 2 and α0 3, but does not succeed, however it has one counterexample c13 for α0 3.",
                "Thus, A1 sends the the message rebut( c13, α0 3) to A3.",
                "A3 incorporates c13 into its case base and tries to solve the problem P again, now taking c13 into consideration.",
                "A3 comes up with the justified prediction α1 3 = A3, P, hadromerida, D4 , and broadcasts it to the rest of the agents with the message assert(α1 3).",
                "Thus, all of them know the new H1 = α0 1, α0 2, α1 3 .",
                "Round 1 starts and A2 gets the token.",
                "A2 tries to generate counterarguments for α0 1 and α1 3 and only succeeds to generate a counterargument β1 2 = A2, P, astrophorida, D5 against α1 3.",
                "The counterargument is sent to A3 with the message rebut(β1 2 , α1 3).",
                "Agent A3 receives the counterargument and assesses its local confidence.",
                "The result is that the individual confidence of the counterargument β1 2 is lower than the local confidence of α1 3.",
                "Therefore, A3 does not accept the counterargument, and thus H2 = α0 1, α0 2, α1 3 .",
                "Round 2 starts and A3 gets the token.",
                "A3 generates a counterargument β2 3 = A3, P, hadromerida, D6 for α0 2 and sends it to A2 with the message rebut(β2 3 , α0 2).",
                "Agent A2 receives the counterargument and assesses its local confidence.",
                "The result is that the local confidence of the counterargument β2 3 is higher than the local confidence of α0 2.",
                "Therefore, A2 accepts the counterargument and informs the rest of the agents with the message assert(β2 3 ).",
                "After that, H3 = α0 1, β2 3 , α1 3 .",
                "At Round 3, since all the agents agree (all the justified predictions in H3 predict hadromerida as the solution class) The protocol ends, and A1 (the agent that received the problem) considers hadromerida as the joint solution for the problem P. 9.",
                "EXPERIMENTAL EVALUATION 980 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) SPONGE 75 77 79 81 83 85 87 89 91 2 3 4 5 AMAL Voting Individual SOYBEAN 55 60 65 70 75 80 85 90 2 3 4 5 AMAL Voting Individual Figure 5: Individual and joint accuracy for 2 to 5 agents.",
                "In this section we empirically evaluate the AMAL argumentation framework.",
                "We have made experiments in two different data sets: soybean (from the UCI machine learning repository) and sponge (a relational data set).",
                "The soybean data set has 307 examples and 19 solution classes, while the sponge data set has 280 examples and 3 solution classes.",
                "In an experimental run, the data set is divided in 2 sets: the training set and the test set.",
                "The training set examples are distributed among 5 different agents without replication, i.e. there is no example shared by two agents.",
                "In the testing stage, problems in the test set arrive randomly to one of the agents, and their goal is to predict the correct solution.",
                "The experiments are designed to test two hypotheses: (H1) that argumentation is a useful framework for joint deliberation and can improve over other typical methods such as voting; and (H2) that learning from communication improves the individual performance of a learning agent participating in an argumentation process.",
                "Moreover, we also expect that the improvement achieved from argumentation will increase as the number of agents participating in the argumentation increases (since more information will be taken into account).",
                "Concerning H1 (argumentation is a useful framework for joint deliberation), we ran 4 experiments, using 2, 3, 4, and 5 agents respectively (in all experiments each agent has a 20% of the training data, since the training is always distributed among 5 agents).",
                "Figure 5 shows the result of those experiments in the sponge and soybean data sets.",
                "Classification accuracy is plotted in the vertical axis, and in the horizontal axis the number of agents that took part in the argumentation processes is shown.",
                "For each number of agents, three bars are shown: individual, Voting, and AMAL.",
                "The individual bar shows the average accuracy of individual agents predictions; the voting bar shows the average accuracy of the joint prediction achieved by voting but without any argumentation; and finally the AMAL bar shows the average accuracy of the joint prediction using argumentation.",
                "The results shown are the average of 5 10-fold cross validation runs.",
                "Figure 5 shows that collaboration (voting and AMAL) outperforms individual problem solving.",
                "Moreover, as we expected, the accuracy improves as more agents collaborate, since more information is taken into account.",
                "We can also see that AMAL always outperforms standard voting, proving that joint decisions are based on better information as provided by the argumentation process.",
                "For instance, the joint accuracy for 2 agents in the sponge data set is of 87.57% for AMAL and 86.57% for voting (while individual accuracy is just 80.07%).",
                "Moreover, the improvement achieved by AMAL over Voting is even larger in the soybean data set.",
                "The reason is that the soybean data set is more difficult (in the sense that agents need more data to produce good predictions).",
                "These experimental results show that AMAL effectively exploits the opportunity for improvement: the accuracy is higher only because more agents have changed their opinion during argumentation (otherwise they would achieve the same result as Voting).",
                "Concerning H2 (learning from communication in argumentation processes improves individual prediction ), we ran the following experiment: initially, we distributed a 25% of the training set among the five agents; after that, the rest of the cases in the training set is sent to the agents one by one; when an agent receives a new training case, it has several options: the agent can discard it, the agent can retain it, or the agent can use it for engaging an argumentation process.",
                "Figure 6 shows the result of that experiment for the two data sets.",
                "Figure 6 contains three plots, where NL (not learning) shows accuracy of an agent with no learning at all; L (learning), shows the evolution of the individual classification accuracy when agents learn by retaining the training cases they individually receive (notice that when all the training cases have been retained at 100%, the accuracy should be equal to that of Figure 5 for individual agents); and finally LFC (learning from communication) shows the evolution of the individual classification accuracy of learning agents that also learn by retaining those counterexamples received during argumentation (i.e. they learn both from training examples and counterexamples).",
                "Figure 6 shows that if an agent Ai learns also from communication, Ai can significantly improve its individual performance with just a small number of additional cases (those selected as relevant counterexamples for Ai during argumentation).",
                "For instance, in the soybean data set, individual agents have achieved an accuracy of 70.62% when they also learn from communication versus an accuracy of 59.93% when they only learn from their individual experience.",
                "The number of cases learnt from communication depends on the properties of the data set: in the sponges data set, agents have retained only very few additional cases, and significantly improved individual accuracy; namely they retain 59.96 cases in average (compared to the 50.4 cases retained if they do not learn from communication).",
                "In the soybean data set more counterexamples are learnt to significantly improve individual accuracy, namely they retain 87.16 cases in average (compared to 55.27 cases retained if they do not learn from communication).",
                "Finally, the fact that both data sets show a significant improvement points out the adaptive nature of the argumentation-based approach to learning from communication: the useful cases are selected as counterexamples (and no more than those needed), and they have the intended effect. 10.",
                "RELATED WORK Concerning CBR in a multi-agent setting, the first research was on negotiated case retrieval [11] among groups of agents.",
                "Our work on multi-agent case-based learning started in 1999 [6]; later Mc Ginty and Smyth [7] presented a multi-agent collaborative CBR approach (CCBR) for planning.",
                "Finally, another interesting approach is multi-case-base reasoning (MCBR) [5], that deals with The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 981 SPONGE 60 65 70 75 80 85 25% 40% 55% 70% 85% 100% LFC L NL SOYBEAN 20 30 40 50 60 70 80 90 25% 40% 55% 70% 85% 100% LFC L NL Figure 6: Learning from communication resulting from argumentation in a system composed of 5 agents. distributed systems where there are several case bases available for the same task and addresses the problems of cross-case base adaptation.",
                "The main difference is that our MAC approach is a way to distribute the Reuse process of CBR (using a voting system) while Retrieve is performed individually by each agent; the other multiagent CBR approaches, however, focus on distributing the Retrieve process.",
                "Research on MAS argumentation focus on several issues like a) logics, protocols and languages that support argumentation, b) argument selection and c) argument interpretation.",
                "Approaches for logic and languages that support argumentation include defeasible logic [4] and BDI models [13].",
                "Although argument selection is a key aspect of automated argumentation (see [12] and [13]), most research has been focused on preference relations among arguments.",
                "In our framework we have addressed both argument selection and preference relations using a case-based approach. 11.",
                "CONCLUSIONS AND FUTURE WORK In this paper we have presented an argumentation-based framework for multi-agent learning.",
                "Specifically, we have presented AMAL, a framework that allows a group of learning agents to argue about the solution of a given problem and we have shown how the learning capabilities can be used to generate arguments and counterarguments.",
                "The experimental evaluation shows that the increased amount of information provided to the agents by the argumentation process increases their <br>predictive accuracy</br>, and specially when an adequate number of agents take part in the argumentation.",
                "The main contributions of this work are: a) an argumentation framework for learning agents; b) a case-based preference relation over arguments, based on computing an overall confidence estimation of arguments; c) a case-based policy to generate counterarguments and select counterexamples; and d) an argumentation-based approach for learning from communication.",
                "Finally, in the experiments presented here a learning agent would retain all counterexamples submitted by the other agent; however, this is a very simple case retention policy, and we will like to experiment with more informed policies - with the goal that individual learning agents could significantly improve using only a small set of cases proposed by other agents.",
                "Finally, our approach is focused on lazy learning, and future works aims at incorporating eager inductive learning inside the argumentative framework for learning from communication. 12.",
                "REFERENCES [1] Agnar Aamodt and Enric Plaza.",
                "Case-based reasoning: Foundational issues, methodological variations, and system approaches.",
                "Artificial Intelligence Communications, 7(1):39-59, 1994. [2] E. Armengol and E. Plaza.",
                "Lazy induction of descriptions for relational case-based learning.",
                "In ECML2001, pages 13-24, 2001. [3] Gerhard Brewka.",
                "Dynamic argument systems: A formal model of argumentation processes based on situation calculus.",
                "Journal of Logic and Computation, 11(2):257-282, 2001. [4] Carlos I. Chesñevar and Guillermo R. Simari.",
                "Formalizing Defeasible Argumentation using Labelled Deductive Systems.",
                "Journal of Computer Science & Technology, 1(4):18-33, 2000. [5] D. Leake and R. Sooriamurthi.",
                "Automatically selecting strategies for multi-case-base reasoning.",
                "In S. Craw and A. Preece, editors, ECCBR2002, pages 204-219, Berlin, 2002.",
                "Springer Verlag. [6] Francisco J. Martín, Enric Plaza, and Josep-Lluis Arcos.",
                "Knowledge and experience reuse through communications among competent (peer) agents.",
                "International Journal of Software Engineering and Knowledge Engineering, 9(3):319-341, 1999. [7] Lorraine McGinty and Barry Smyth.",
                "Collaborative case-based reasoning: Applications in personalized route planning.",
                "In I. Watson and Q. Yang, editors, ICCBR, number 2080 in LNAI, pages 362-376.",
                "Springer-Verlag, 2001. [8] Santi Ontañón and Enric Plaza.",
                "Justification-based multiagent learning.",
                "In ICML2003, pages 576-583.",
                "Morgan Kaufmann, 2003. [9] Enric Plaza, Eva Armengol, and Santiago Ontañón.",
                "The explanatory power of symbolic similarity in case-based reasoning.",
                "Artificial Intelligence Review, 24(2):145-161, 2005. [10] David Poole.",
                "On the comparison of theories: Preferring the most specific explanation.",
                "In IJCAI-85, pages 144-147, 1985. [11] M V Nagendra Prassad, Victor R Lesser, and Susan Lander.",
                "Retrieval and reasoning in distributed case bases.",
                "Technical report, UMass Computer Science Department, 1995. [12] K. Sycara S. Kraus and A. Evenchik.",
                "Reaching agreements through argumentation: a logical model and implementation.",
                "Artificial Intelligence Journal, 104:1-69, 1998. [13] N. R. Jennings S. Parsons, C. Sierra.",
                "Agents that reason and negotiate by arguing.",
                "Journal of Logic and Computation, 8:261-292, 1998. [14] Bruce A. Wooley.",
                "Explanation component of software systems.",
                "ACM CrossRoads, 5.1, 1998. 982 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "The experimental evaluation shows that the increased amount of information provided to the agents by the argumentation process increases their <br>predictive accuracy</br>, and specially when an adequate number of agents take part in the argumentation."
            ],
            "translated_annotated_samples": [
                "La evaluación experimental muestra que la cantidad aumentada de información proporcionada a los agentes por el proceso de argumentación aumenta su <br>precisión predictiva</br>, especialmente cuando un número adecuado de agentes participa en la argumentación."
            ],
            "translated_text": "Aprendizaje y deliberación conjunta a través de la argumentación en sistemas multiagente. Santi Ontañón CCL, Laboratorio de Computación Cognitiva, Instituto de Tecnología de Georgia, Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Instituto de Investigación en Inteligencia Artificial, CSIC, Consejo Superior de Investigaciones Científicas, Campus UAB, 08193 Bellaterra, Cataluña (España) enric@iiia.csic.es RESUMEN En este artículo presentaremos un marco de argumentación para agentes de aprendizaje (AMAL) diseñado con dos propósitos: (1) para deliberación conjunta, y (2) para aprender a través de la comunicación. El marco AMAL se basa completamente en el aprendizaje a partir de ejemplos: la relación de preferencia de argumentos, la política de generación de argumentos y la política de generación de contraargumentos son técnicas basadas en casos. Para unirse en la deliberación, los agentes de aprendizaje comparten su experiencia formando un comité para decidir sobre alguna decisión conjunta. Experimentalmente demostramos que la argumentación entre comités de agentes mejora tanto el rendimiento individual como el conjunto. Para aprender a través de la comunicación, un agente se involucra en discusiones con otros agentes para contrastar sus hipótesis individuales y recibir contraejemplos; el proceso de argumentación mejora su alcance de aprendizaje y rendimiento individual. Categorías y Descriptores de Asignaturas I.2.6 [Inteligencia Artificial]: Aprendizaje; I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente, Agentes Inteligentes 1. Los marcos de argumentación para sistemas multiagente pueden ser utilizados para diferentes propósitos como la deliberación conjunta, la persuasión, la negociación y la resolución de conflictos. En este artículo presentaremos un marco de argumentación para agentes de aprendizaje, y mostraremos que puede ser utilizado para dos propósitos: (1) deliberación conjunta, y (2) aprendizaje a partir de la comunicación. La deliberación conjunta basada en argumentos implica discutir sobre el resultado de una situación particular o la acción apropiada a tomar en una situación específica. Los agentes de aprendizaje son capaces de aprender de la experiencia, en el sentido de que ejemplos pasados (situaciones y sus resultados) se utilizan para predecir el resultado para la situación actual. Sin embargo, dado que la experiencia de los agentes individuales puede ser limitada, el conocimiento individual y la precisión de la predicción también son limitados. Por lo tanto, los agentes de aprendizaje que son capaces de argumentar sus predicciones individuales con otros agentes pueden alcanzar una mayor precisión en las predicciones después de dicho proceso de argumentación. La mayoría de los marcos de argumentación existentes para sistemas multiagente se basan en lógica deductiva u otra formalización de lógica deductiva diseñada específicamente para respaldar la argumentación, como la lógica por defecto [3]. Por lo general, un argumento se ve como una declaración lógica, mientras que un contraargumento es un argumento ofrecido en oposición a otro argumento; los agentes utilizan una relación de preferencia para resolver argumentos conflictivos. Sin embargo, los marcos de argumentación basados en lógica asumen agentes con conocimiento predefinido y relación de preferencia. En este artículo, nos enfocamos en un marco de Aprendizaje Multiagente basado en Argumentación (AMAL) donde tanto el conocimiento como la relación de preferencia se aprenden a partir de la experiencia. Por lo tanto, consideramos un escenario con agentes que (1) trabajan en el mismo dominio utilizando una ontología compartida, (2) son capaces de aprender a partir de ejemplos y (3) se comunican utilizando un marco argumentativo. Tener capacidades de aprendizaje permite a los agentes utilizar de manera efectiva una forma específica de contraargumento, a saber, el uso de contraejemplos. Los contraejemplos ofrecen la posibilidad de que los agentes aprendan durante el proceso de argumentación. Además, los agentes de aprendizaje permiten técnicas que utilizan la experiencia aprendida para generar argumentos y contraargumentos adecuados. Específicamente, necesitaremos abordar dos problemas: (1) cómo definir una técnica para generar argumentos y contraargumentos a partir de ejemplos, y (2) cómo definir una relación de preferencia sobre dos argumentos en conflicto que han sido inducidos a partir de ejemplos. Este documento presenta un enfoque basado en casos para abordar ambas cuestiones. Los agentes utilizan el razonamiento basado en casos (CBR) [1] para aprender de casos pasados (donde un caso es una situación y su resultado) con el fin de predecir el resultado de una nueva situación. Proponemos un protocolo de argumentación dentro del marco de AMAL que apoya a los agentes en llegar a una predicción conjunta sobre una situación o problema específico; además, el razonamiento necesario para respaldar el proceso de argumentación también se basará en casos. En particular, presentamos dos medidas basadas en casos, una para generar los argumentos y contraargumentos adecuados para una situación particular y otra para determinar la relación de preferencia entre los argumentos. Finalmente, evaluamos (1) si la argumentación entre agentes de aprendizaje puede producir una predicción conjunta que mejore el rendimiento de aprendizaje individual y (2) si el aprendizaje a partir de los contraejemplos transmitidos durante el proceso de argumentación aumenta el rendimiento individual con precisamente aquellos casos que se utilizan mientras discuten entre ellos. El documento está estructurado de la siguiente manera. La sección 2 discute la relación entre la argumentación, la colaboración y el aprendizaje. Luego, la Sección 3 introduce nuestro marco de trabajo de CBR multiagente (MAC) y el concepto de predicción justificada. Después de eso, la Sección 4 define formalmente nuestro marco de argumentación. Las secciones 5 y 6 presentan nuestra relación de preferencia basada en casos y las políticas de generación de argumentos, respectivamente. Más adelante, la Sección 7 presenta el protocolo de argumentación en nuestro marco AMAL. Después de eso, la Sección 8 presenta una ejemplificación del marco de argumentación. Finalmente, la Sección 9 presenta una evaluación empírica de nuestras dos hipótesis principales. El artículo concluye con secciones de trabajo relacionado y conclusiones. 2. ARGUMENTACIÓN, COLABORACIÓN Y APRENDIZAJE Tanto el aprendizaje como la colaboración son formas en las que un agente puede mejorar el rendimiento individual. De hecho, existe un claro paralelismo entre el aprendizaje y la colaboración en sistemas multiagentes, ya que ambos son formas en las que los agentes pueden lidiar con sus deficiencias. Mostremos cuáles son las principales motivaciones que un agente puede tener para aprender o colaborar. • Motivaciones para aprender: - Aumentar la calidad de la predicción, - Aumentar la eficiencia, - Ampliar el rango de problemas resolubles. • Motivaciones para colaborar: - Aumentar la calidad de la predicción, - Aumentar la eficiencia, - Ampliar el rango de problemas resolubles, - Ampliar el rango de recursos accesibles. Al observar las listas de motivación anteriores, podemos ver fácilmente que el aprendizaje y la colaboración están muy relacionados en los sistemas multiagente. De hecho, con la excepción del último elemento en la lista de motivaciones para colaborar, son dos extremos de un continuo de estrategias para mejorar el rendimiento. Un agente puede optar por aumentar el rendimiento mediante el aprendizaje, la colaboración o encontrando un punto intermedio que combine el aprendizaje y la colaboración para mejorar el rendimiento. En este artículo propondremos AMAL, un marco de argumentación para agentes de aprendizaje, y también mostraremos cómo AMAL puede ser utilizado tanto para aprender a través de la comunicación como para resolver problemas de manera colaborativa: • Los agentes pueden resolver problemas de manera colaborativa al participar en un proceso de argumentación sobre la predicción para la situación actual. Utilizando esta colaboración, la predicción puede realizarse de una manera más informada, ya que se ha tenido en cuenta la información conocida por varios agentes. Los agentes también pueden aprender de la comunicación con otros agentes al participar en un proceso de argumentación. Los agentes que participan en estos procesos de argumentación pueden aprender de los argumentos y contraejemplos recibidos de otros agentes, y utilizar esta información para predecir los resultados de situaciones futuras. En el resto de este documento propondremos un marco de argumentación y mostraremos cómo puede ser utilizado tanto para el aprendizaje como para la resolución de problemas de manera colaborativa. SISTEMAS CBR MULTIAGENTE Un Sistema de Razonamiento Basado en Casos Multiagente (MAC) M = {(A1, C1), ..., (An, Cn)} es un sistema multiagente compuesto por A = {Ai, ..., An}, un conjunto de agentes CBR, donde cada agente Ai ∈ A posee una base de casos individual Ci. Cada agente individual Ai en un MAC es completamente autónomo y cada agente Ai solo tiene acceso a su base de casos individual y privada Ci. Un conjunto de casos Ci = {c1, ..., cm} es una colección de casos. Los agentes en un sistema MAC pueden resolver problemas de forma individual, pero también pueden colaborar con otros agentes para resolver problemas. En este marco, nos limitaremos a tareas analíticas, es decir, tareas como la clasificación, donde la solución de un problema se logra seleccionando una clase de solución de un conjunto enumerado de clases de solución. En lo siguiente, notaremos el conjunto de todas las clases de soluciones por S = {S1, ..., SK}. Por lo tanto, un caso c = P, S es una tupla que contiene una descripción del caso P y una clase de solución S ∈ S. En lo siguiente, utilizaremos indistintamente los términos problema y descripción del caso. Además, utilizaremos la notación de punto para referirnos a elementos dentro de una tupla; por ejemplo, para referirnos a la clase de solución de un caso c, escribiremos c.S. Por lo tanto, decimos que un grupo de agentes realiza una deliberación conjunta cuando colaboran para encontrar una solución conjunta mediante un proceso de argumentación. Sin embargo, para hacerlo, un agente debe ser capaz de justificar su predicción a los otros agentes (es decir, generar un argumento para su solución predicha que pueda ser examinado y criticado por los otros agentes). La siguiente sección aborda este tema. 3.1 Predicciones Justificadas Tanto los sistemas expertos como los sistemas CBR pueden tener un componente de explicación [14] encargado de justificar por qué el sistema ha proporcionado una respuesta específica al usuario. La línea de razonamiento del sistema puede ser examinada por un experto humano, aumentando así la fiabilidad del sistema. La mayoría de los trabajos existentes sobre generación de explicaciones se centran en generar explicaciones para ser proporcionadas al usuario. Sin embargo, en nuestro enfoque utilizamos explicaciones (o justificaciones) como una herramienta para mejorar la comunicación y coordinación entre agentes. Estamos interesados en las justificaciones ya que pueden ser utilizadas como argumentos. Para ese propósito, nos beneficiaremos de la capacidad de algunos métodos de aprendizaje automático para proporcionar justificaciones. Una justificación construida por un método CBR después de determinar que la solución de un problema particular P fue Sk es una descripción que contiene la información relevante del problema P que el método CBR ha considerado para predecir Sk como la solución de P. En particular, los métodos CBR funcionan recuperando casos similares al problema en cuestión, y luego reutilizando sus soluciones para el problema actual, esperando que dado que el problema y los casos son similares, las soluciones también serán similares. Por lo tanto, si un método CBR ha recuperado un conjunto de casos C1, ..., Cn para resolver un problema particular P, la justificación construida contendrá la información relevante del problema P que hizo que el sistema CBR recuperara ese conjunto particular de casos, es decir, contendrá la información relevante que P y C1, ..., Cn tienen en común. Por ejemplo, la Figura 1 muestra una justificación construida por un sistema CBR para un problema de juguete (en las siguientes secciones mostraremos justificaciones para problemas reales). En la figura, un problema tiene dos atributos (Semáforo y Coches_pasando), el mecanismo de recuperación del sistema CBR nota que al considerar solo el atributo Semáforo, puede recuperar dos casos que predicen la misma solución: esperar. Por lo tanto, dado que solo se ha utilizado este atributo, es el único que aparece en la justificación. Los valores del resto de atributos son irrelevantes, ya que, sea cual sea su valor, la clase de solución habría sido la misma. 976 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Problema Semáforo: rojo Pasan coches: no Caso 1 Semáforo: rojo Pasan coches: no Solución: esperar Caso 3 Semáforo: rojo Pasan coches: sí Solución: esperar Caso 4 Semáforo: verde Pasan coches: sí Solución: esperar Caso 2 Semáforo: verde Pasan coches: no Solución: cruzar Casos recuperados Solución: esperar Justificación Semáforo: rojo Figura 1: Un ejemplo de generación de justificación en un sistema de RCB. Observa que, dado que la única característica relevante para decidir es Traffic_light (la única utilizada para recuperar casos), es la única que aparece en la justificación. En general, el significado de una justificación es que todos (o la mayoría) de los casos en la base de casos de un agente que satisfacen la justificación (es decir, todos los casos que están subsumidos por la justificación) pertenecen a la clase de solución predicha. En el resto del documento, usaremos para denotar la relación de subsumpción. En nuestro trabajo, utilizamos LID [2], un método CBR capaz de construir justificaciones simbólicas como la ejemplificada en la Figura 1. Cuando un agente proporciona una justificación para una predicción, el agente genera una predicción justificada: DEFINICIÓN 3.1. Una Predicción Justificada es una tupla J = A, P, S, D donde el agente A considera que S es la solución correcta para el problema P, y esa predicción está justificada por una descripción simbólica D tal que J.D J.P. Las justificaciones pueden tener muchos usos para los sistemas de RCB [8, 9]. En este documento, vamos a utilizar justificaciones como argumentos, con el fin de permitir que los agentes de aprendizaje participen en procesos de argumentación. 4. ARGUMENTOS Y CONTRAARGUMENTOS Para nuestros propósitos, un argumento α generado por un agente A está compuesto por una afirmación S y alguna evidencia D que respalda a S como correcta. En el resto de esta sección veremos cómo esta definición general de argumento puede ser concretada en un tipo específico de argumentos que los agentes pueden generar. En el contexto de los sistemas MAC, los agentes discuten sobre predicciones para nuevos problemas y pueden proporcionar dos tipos de información: a) casos específicos P, S, y b) predicciones justificadas: A, P, S, D. Usando esta información, podemos definir tres tipos de argumentos: predicciones justificadas, contraargumentos y contraejemplos. Una predicción justificada α es generada por un agente Ai para argumentar que Ai cree que la solución correcta para un problema dado P es α.S, y la evidencia proporcionada es la justificación α.D. En el ejemplo representado en la Figura 1, un agente Ai puede generar el argumento α = Ai, P, Esperar, (Semáforo = rojo), lo que significa que el agente Ai cree que la solución correcta para P es Esperar porque el atributo Semáforo es igual a rojo. Un contraargumento β es un argumento ofrecido en oposición a otro argumento α. En nuestro marco de trabajo, un contraargumento consiste en una predicción justificada Aj, P, S, D generada por un agente Aj con la intención de refutar un argumento α generado por otro agente Ai, que respalda una clase de solución S diferente a la de α.S para el problema en cuestión y justifica esto con una justificación D. En el ejemplo de la Figura 1, si un agente genera el argumento α = Ai, P, Caminar, (Pasando_autos = no), un agente que piensa que la solución correcta es Esperar podría responder con el contraargumento β = Aj, P, Esperar, (Pasando_autos = no ∧ Semáforo = rojo), lo que significa que, aunque no hay autos pasando, el semáforo está en rojo y la calle no se puede cruzar. Un contraejemplo c es un caso que contradice un argumento α. Por lo tanto, un contraejemplo también es un contraargumento, uno que afirma que un argumento específico α no siempre es verdadero, y la evidencia proporcionada es el caso c. Específicamente, para que un caso c sea un contraejemplo de un argumento α, se deben cumplir las siguientes condiciones: α.D c y α.S = c.S, es decir, el caso debe satisfacer la justificación α.D y la solución de c debe ser diferente a la predicha por α. Al intercambiar argumentos y contraargumentos (incluyendo contraejemplos), los agentes pueden discutir sobre la solución correcta de un problema dado, es decir, pueden participar en un proceso de deliberación conjunto. Sin embargo, para hacerlo, necesitan un protocolo de interacción específico, una relación de preferencia entre argumentos contradictorios y una política de decisión para generar contraargumentos (incluidos contraejemplos). En las siguientes secciones presentaremos estos elementos. 5. RELACIÓN DE PREFERENCIA Un argumento específico proporcionado por un agente podría no ser consistente con la información conocida por otros agentes (o incluso con parte de la información conocida por el agente que ha generado la justificación debido al ruido en los datos de entrenamiento). Por esa razón, vamos a definir una relación de preferencia sobre predicciones justificadas contradictorias basadas en casos. Básicamente, definiremos una medida de confianza para cada predicción justificada (que tenga en cuenta los casos poseídos por cada agente), y la predicción justificada con la mayor confianza será la preferida. La idea detrás de la confianza basada en casos es contar cuántos de los casos en una base de casos individual respaldan una predicción justificada, y cuántos de ellos son ejemplos en contra de ella. Cuanto más casos de respaldo, mayor es la confianza; y cuanto más contrajemplos, menor es la confianza. Específicamente, para evaluar la confianza de una predicción justificada α, un agente obtiene el conjunto de casos en su base de casos individuales que están subsumidos por α.D. Con ellos, un agente Ai obtiene los valores Y (sí) y N (no): • Y Ai α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| es el número de casos en la base de casos del agente subsumidos por la justificación α.D que pertenecen a la clase de solución α.S, • NAi α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| es el número de casos en la base de casos del agente subsumidos por la justificación α.D que no pertenecen a esa clase de solución. El Sexto Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 977 + + + + + +  - - + Figura 2: La confianza de los argumentos se evalúa contrastándolos con las bases de casos de los agentes. Un agente estima la confianza de un argumento como: CAi (α) = Y Ai α 1 + Y Ai α + NAi α es decir, la confianza en una predicción justificada es el número de casos que la respaldan dividido por el número de casos que la respaldan más los contraejemplos. Observa que sumamos 1 al denominador, esto es para evitar otorgar confianzas excesivamente altas a predicciones justificadas cuya confianza se ha calculado utilizando un número pequeño de casos. Ten en cuenta que esta corrección sigue la misma idea que la corrección de Laplace para estimar probabilidades. La Figura 2 ilustra la evaluación individual de la confianza de un argumento, en particular, se encuentran tres casos de respaldo y un contraejemplo en la base de casos de los agentes Ai, lo que da una confianza estimada de 0.6. Además, también podemos definir la confianza conjunta de un argumento α como la confianza calculada utilizando los casos presentes en las bases de casos de todos los agentes en el grupo: C(α) = Π Ai α / (1 + Σ Ai α + NAi α). Nótese que, para calcular conjuntamente la confianza conjunta, los agentes solo tienen que hacer públicos los valores de aprobación y rechazo calculados localmente para un argumento dado. En nuestro marco de trabajo, los agentes utilizan esta confianza conjunta como la relación de preferencia: una predicción justificada α se prefiere sobre otra β si C(α) ≥ C(β). GENERACIÓN DE ARGUMENTOS En nuestro marco de trabajo, los argumentos son generados por los agentes a partir de casos, utilizando métodos de aprendizaje. Cualquier método de aprendizaje capaz de proporcionar una predicción justificada puede ser utilizado para generar argumentos. Por ejemplo, los árboles de decisión y LID [2] son métodos de aprendizaje adecuados. Específicamente, en los experimentos reportados en este artículo, los agentes utilizan LID. Por lo tanto, cuando un agente desea generar un argumento que respalde que una clase de solución específica es la solución correcta para un problema P, genera una predicción justificada como se explica en la Sección 3.1. Por ejemplo, la Figura 3 muestra una justificación real generada por LID después de resolver un problema P en el dominio de la identificación de esponjas marinas. En particular, la Figura 3 muestra cómo cuando un agente recibe un nuevo problema para resolver (en este caso, un nuevo esponja para determinar su orden), el agente utiliza LID para generar un argumento (que consiste en una predicción justificada) utilizando los casos en la base de casos del agente. La justificación mostrada en la Figura 3 se puede interpretar diciendo que la solución predicha es hadromerida porque la forma lisa de las megascleras del esqueleto espicular de la esponja es de tipo tilostilo, el esqueleto espicular de la esponja no tiene una longitud uniforme y no hay gemas en las características externas de la esponja. Por lo tanto, el argumento generado será α = A1, P, hadromerida, D1. 6.1 Generación de Contraargumentos Como se mencionó anteriormente, los agentes pueden intentar refutar argumentos generando contraargumentos o encontrando contraejemplos. Permítanos explicar cómo pueden ser generados. Un agente Ai quiere generar un contraargumento β para rebatir un argumento α cuando α está en contradicción con la base de casos locales de Ai. Además, al generar dicho contraargumento β, Ai espera que β sea preferido sobre α. Con ese propósito, presentaremos una política específica para generar contraargumentos basados en el criterio de especificidad [10]. El criterio de especificidad se utiliza ampliamente en marcos deductivos para la argumentación, y establece que entre dos argumentos en conflicto, se debe preferir el más específico ya que, en principio, está más informado. Por lo tanto, se espera que los contraargumentos generados basados en el criterio de especificidad sean preferibles (ya que están más informados) que los argumentos que intentan refutar. Sin embargo, no hay garantía de que tales contraargumentos siempre ganen, ya que, como hemos indicado en la Sección 5, los agentes en nuestro marco utilizan una relación de preferencia basada en la confianza conjunta. Además, uno podría pensar que sería mejor que los agentes generaran contraargumentos basados en la relación de preferencia de confianza conjunta; sin embargo, no es obvio cómo generar contraargumentos basados en la confianza conjunta de manera eficiente, ya que se requiere colaboración para evaluar la confianza conjunta. Por lo tanto, el agente que genera el contraargumento debe comunicarse constantemente con los otros agentes en cada paso del algoritmo de inducción utilizado para generar contraargumentos (actualmente una de nuestras líneas de investigación futuras). Por lo tanto, en nuestro marco de trabajo, cuando un agente quiere generar un contraargumento β a un argumento α, β tiene que ser más específico que α (es decir, α.D < β.D). La generación de contraargumentos utilizando el criterio de especificidad impone algunas restricciones sobre el método de aprendizaje, aunque LID o ID3 pueden adaptarse fácilmente para esta tarea. Por ejemplo, LID es un algoritmo que genera una descripción desde cero y añade heurísticamente características a ese término. Por lo tanto, en cada paso, la descripción se vuelve más específica que en el paso anterior, y el número de casos que se incluyen en esa descripción se reduce. Cuando la descripción cubre solo (o casi solo) casos de una única clase de solución, LID termina y predice esa clase de solución. Para generar un contraargumento a un argumento α, LID solo tiene que usar como punto de partida la descripción α.D en lugar de empezar desde cero. De esta manera, la justificación proporcionada por LID siempre estará subsumida por α.D, y por lo tanto, el contraargumento resultante será más específico que α. Sin embargo, hay que tener en cuenta que LID a veces puede no ser capaz de generar contraargumentos, ya que LID puede no ser capaz de especializar aún más la descripción α.D, o porque el agente Ai no tiene ningún caso en Ci que esté subsumido por α.D. La Figura 4 muestra cómo un agente A2 que no estaba de acuerdo con el argumento mostrado en la Figura 3, genera un contraargumento utilizando LID. Además, la Figura 4 muestra la generación de un contraargumento β1 2 para el argumento α0 1 (en la Figura 3) que es una especialización de α0 1. 978 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Solución: hadromerida Justificación: D1 Esponja Esqueleto espiculado Características externas Características externas Gemmulas: no Esqueleto espiculado Megascleres Longitud uniforme: no Megascleres Forma lisa: tilostilo Caso Base de A1 LID Nueva esponja P Figura 3: Ejemplo de una justificación real generada por LID en el conjunto de datos de esponjas marinas. Específicamente, en nuestros experimentos, cuando un agente Ai quiere refutar un argumento α, utiliza la siguiente política: 1. El Agente Ai utiliza LID para intentar encontrar un contraargumento β más específico que α; si se encuentra, β se envía al otro agente como un contraargumento de α. 2. Si no se encuentra, entonces Ai busca un contraejemplo c ∈ Ci de α. Si se encuentra un caso c, entonces c se envía al otro agente como un contraejemplo de α. Si no se encuentran contraejemplos, entonces Ai no puede refutar el argumento α. 7. APRENDIZAJE MULTIAGENTE BASADO EN ARGUMENTACIÓN El protocolo de interacción de AMAL permite que un grupo de agentes A1, ..., An deliberen sobre la solución correcta de un problema P mediante un proceso de argumentación. Si el proceso de argumentación llega a una solución consensuada, la deliberación conjunta termina; de lo contrario, se utiliza una votación ponderada para determinar la solución conjunta. Además, AMAL también permite a los agentes aprender de los contraejemplos recibidos de otros agentes. El protocolo AMAL consiste en una serie de rondas. En la ronda inicial, cada agente declara cuál es su predicción individual para P. Luego, en cada ronda, un agente puede intentar refutar la predicción hecha por cualquiera de los otros agentes. El protocolo utiliza un mecanismo de paso de token para que los agentes (uno a la vez) puedan enviar contraargumentos o contraejemplos si no están de acuerdo con la predicción hecha por cualquier otro agente. Específicamente, a cada agente se le permite enviar un contraargumento o contraejemplo cada vez que recibe el token (nota que esta restricción es solo para simplificar el protocolo, y que no restringe la cantidad de contraargumentos que un agente puede enviar, ya que pueden ser retrasados para rondas posteriores). Cuando un agente recibe un contraargumento o contraejemplo, informa a los otros agentes si acepta el contraargumento (y cambia su predicción) o no. Además, los agentes también tienen la oportunidad de responder a los contraargumentos cuando reciben el token, intentando generar un contraargumento al contraargumento. Cuando todos los agentes hayan tenido el token una vez, el token regresa al primer agente, y así sucesivamente. Si en algún momento del protocolo, todos los agentes están de acuerdo o durante las últimas n rondas ningún agente ha generado ninguna contra-argumentación, el protocolo termina. Además, si al final de la argumentación los agentes no han llegado a un acuerdo, entonces se utiliza un mecanismo de votación que utiliza la confianza de cada predicción como pesos para decidir la solución final (Así, AMAL sigue el mismo mecanismo que los comités humanos, primero cada miembro individual de un comité expone sus argumentos y discute los de los otros miembros (deliberación conjunta), y si no se alcanza un consenso, entonces se requiere un mecanismo de votación). En cada iteración, los agentes pueden usar los siguientes performativos: • afirmar(α): la predicción justificada que se mantendrá durante la próxima ronda será α. Un agente solo puede tener una única predicción en cada ronda, por lo tanto, si se envían múltiples afirmaciones, solo se considera la última como la predicción actualmente mantenida. • rebut(β, α): el agente ha encontrado un contraargumento β a la predicción α. Definiremos Ht = αt 1, ..., αt n como las predicciones que cada uno de los n agentes tiene en una ronda t. Además, también definiremos contradict(αt i) = {α ∈ Ht|α.S = αt i.S} como el conjunto de argumentos contradictorios para un agente Ai en una ronda t, es decir, el conjunto de argumentos en la ronda t que respaldan una clase de solución diferente a la de αt i. El protocolo se inicia porque uno de los agentes recibe un problema P que debe ser resuelto. Después de eso, el agente informa a todos los demás agentes sobre el problema P a resolver, y el protocolo comienza: 1. En la ronda t = 0, cada uno de los agentes resuelve individualmente P y construye una predicción justificada utilizando su propio método de RBC. Entonces, cada agente Ai envía el assertivo performativo assert(α0 i) a los otros agentes. Por lo tanto, los agentes conocen H0 = α0 i , ..., α0 n. Una vez que todas las predicciones hayan sido enviadas, el token se le da al primer agente A1. 2. En cada ronda t (distinta de 0), los agentes verifican si sus argumentos en Ht coinciden. Si lo hacen, el protocolo avanza al paso 5. Además, si durante las últimas n rondas ningún agente ha enviado ningún contraejemplo o contraargumento, el protocolo también avanza al paso 5. De lo contrario, el agente Ai propietario del token intenta generar un contraargumento para cada uno de los argumentos opuestos en contradict(αt i) ⊆ Ht (ver Sección 6.1). Entonces, se selecciona el contraargumento βt i contra la predicción αt j con la menor confianza C(αt j) (ya que αt j es la predicción más probable de ser refutada con éxito). • Si βt i es un contraargumento, entonces, Ai compara localmente αt i con βt i evaluando su confianza contra su base de casos individuales Ci (ver Sección 5) (nótese que Ai está comparando su argumento previo con el contraargumento que Ai mismo acaba de generar y que trata sobre The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 979 Esponja Esqueleto espiculado Características externas Características externas Gemas: no Crecimiento: Esqueleto espiculado Megascleres Longitud uniforme: no Megascleres Forma lisa: tilostilo Crecimiento Crecimiento: masivo Caso Base de A2 LID Solución: astrophorida Justificación: D2 Figura 4: Generación de un contraargumento utilizando LID en el conjunto de datos de esponjas. para enviar a Aj. Si CAi (βt i) > CAi (αt i), entonces Ai considera que βt i es más fuerte que su argumento anterior, cambia su argumento a βt i enviando assert(βt i) al resto de los agentes (la intuición detrás de esto es que dado que un contraargumento también es un argumento, Ai verifica si el nuevo contraargumento es un mejor argumento que el que sostenía anteriormente) y rebut(βt i, αt j) a Aj. De lo contrario (es decir, Si CAi (βt i ) ≤ CAi (αt i), Ai solo enviará rebut(βt i , αt j) a Aj. En cualquiera de las dos situaciones, el protocolo avanza al paso 3. • Si βt i es un contraejemplo c, entonces Ai envía una refutación(c, αt j) a Aj. El protocolo avanza al paso 4. • Si Ai no puede generar ningún contraargumento o contraejemplo, el token se envía al siguiente agente, comienza una nueva ronda t + 1, y el protocolo avanza al estado 2. 3. El agente Aj que ha recibido el contraargumento βt i, lo compara localmente con su propio argumento, αt j, evaluando localmente su confianza. Si CAj (βt i ) > CAj (αt j), entonces Aj aceptará el contraargumento como más fuerte que su propio argumento, y enviará assert(βt i ) a los otros agentes. De lo contrario (es decir, Si CAj (βt i ) ≤ CAj (αt j)), Aj no aceptará el contraargumento e informará a los otros agentes en consecuencia. Cualquiera de las dos situaciones inicia una nueva ronda t + 1, Ai envía el token al siguiente agente y el protocolo vuelve al estado 2. 4. El agente Aj que ha recibido el contraejemplo c lo retiene en su base de casos y genera un nuevo argumento αt+1 j que tiene en cuenta c, e informa al resto de los agentes enviando assert(αt+1 j) a todos ellos. Entonces, Ai envía el token al siguiente agente, comienza una nueva ronda t + 1 y el protocolo vuelve al paso 2. 5. El protocolo finaliza generando una predicción conjunta, de la siguiente manera: si los argumentos en Ht están de acuerdo, entonces su predicción es la predicción conjunta; de lo contrario, se utiliza un mecanismo de votación para decidir la predicción conjunta. El mecanismo de votación utiliza la medida de confianza conjunta como los pesos de votación, de la siguiente manera: S = arg max Sk∈S αi∈Ht|αi.S=Sk C(αi) Además, para evitar iteraciones infinitas, si un agente envía dos veces el mismo argumento o contraargumento al mismo agente, el mensaje no se considera. 8. EJEMPLIFICACIÓN Consideremos un sistema compuesto por tres agentes A1, A2 y A3. Uno de los agentes, A1, recibe un problema P para resolver y decide usar AMAL para resolverlo. Por esa razón, invita a A2 y A3 a participar en el proceso de argumentación. Aceptan la invitación y comienza el protocolo de argumentación. Inicialmente, cada agente genera su predicción individual para P y la transmite a los otros agentes. Por lo tanto, todos ellos pueden calcular H0 = α0 1, α0 2, α0 3. En particular, en este ejemplo: • α0 1 = A1, P, hadromerida, D1 • α0 2 = A2, P, astrophorida, D2 • α0 3 = A3, P, axinellida, D3 A1 comienza poseyendo el token e intenta generar argumentos en contra de α0 2 y α0 3, pero no tiene éxito; sin embargo, tiene un contraejemplo c13 para α0 3. Por lo tanto, A1 envía el mensaje rebut(c13, α03) a A3. A3 incorpora c13 en su base de casos e intenta resolver el problema P nuevamente, ahora teniendo en cuenta c13. A3 elabora la predicción justificada α1 3 = A3, P, hadromerida, D4, y la transmite al resto de los agentes con el mensaje assert(α1 3). Por lo tanto, todos ellos conocen el nuevo H1 = α0 1, α0 2, α1 3. La ronda 1 comienza y A2 obtiene el token. A2 intenta generar contraargumentos para α0 1 y α1 3 y solo logra generar un contraargumento β1 2 = A2, P, astrophorida, D5 contra α1 3. El contraargumento se envía a A3 con el mensaje rebut(β1 2 , α1 3). El agente A3 recibe el contraargumento y evalúa su confianza local. El resultado es que la confianza individual del contraargumento β1 2 es menor que la confianza local de α1 3. Por lo tanto, A3 no acepta el contraargumento, y así H2 = α0 1, α0 2, α1 3. La ronda 2 comienza y A3 recibe el token. A3 genera un contraargumento β2 3 = A3, P, hadromerida, D6 para α0 2 y lo envía a A2 con el mensaje refutar(β2 3 , α0 2). El agente A2 recibe el contraargumento y evalúa su confianza local. El resultado es que la confianza local del contraargumento β2 3 es mayor que la confianza local de α0 2. Por lo tanto, A2 acepta el contraargumento e informa al resto de los agentes con el mensaje assert(β2 3). Después de eso, H3 = α0 1, β2 3 , α1 3. En la Ronda 3, dado que todos los agentes están de acuerdo (todas las predicciones justificadas en H3 predicen hadromerida como la clase de solución), el protocolo finaliza, y A1 (el agente que recibió el problema) considera hadromerida como la solución conjunta para el problema P. 9. EVALUACIÓN EXPERIMENTAL 980 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) SPONGE 75 77 79 81 83 85 87 89 91 2 3 4 5 AMAL Votación Individual SOYBEAN 55 60 65 70 75 80 85 90 2 3 4 5 AMAL Votación Individual Figura 5: Precisión individual y conjunta para 2 a 5 agentes. En esta sección evaluamos empíricamente el marco de argumentación AMAL. Hemos realizado experimentos en dos conjuntos de datos diferentes: soja (del repositorio de aprendizaje automático de UCI) y esponja (un conjunto de datos relacional). El conjunto de datos de soja tiene 307 ejemplos y 19 clases de solución, mientras que el conjunto de datos de esponja tiene 280 ejemplos y 3 clases de solución. En una ejecución experimental, el conjunto de datos se divide en 2 conjuntos: el conjunto de entrenamiento y el conjunto de prueba. Los ejemplos del conjunto de entrenamiento están distribuidos entre 5 agentes diferentes sin replicación, es decir, no hay ningún ejemplo compartido por dos agentes. En la etapa de pruebas, los problemas en el conjunto de pruebas llegan aleatoriamente a uno de los agentes, y su objetivo es predecir la solución correcta. Los experimentos están diseñados para probar dos hipótesis: (H1) que la argumentación es un marco útil para la deliberación conjunta y puede mejorar sobre otros métodos típicos como la votación; y (H2) que el aprendizaje a través de la comunicación mejora el rendimiento individual de un agente de aprendizaje que participa en un proceso de argumentación. Además, también esperamos que la mejora lograda a través de la argumentación aumente a medida que aumente el número de agentes que participan en la argumentación (ya que se tomará en cuenta más información). En relación con H1 (la argumentación es un marco útil para la deliberación conjunta), realizamos 4 experimentos, utilizando 2, 3, 4 y 5 agentes respectivamente (en todos los experimentos cada agente tiene un 20% de los datos de entrenamiento, ya que el entrenamiento siempre se distribuye entre 5 agentes). La Figura 5 muestra el resultado de esos experimentos en los conjuntos de datos de esponjas y soja. La precisión de la clasificación se representa en el eje vertical, y en el eje horizontal se muestra el número de agentes que participaron en los procesos de argumentación. Para cada número de agentes, se muestran tres barras: individual, Voting y AMAL. La barra individual muestra la precisión promedio de las predicciones de los agentes individuales; la barra de votación muestra la precisión promedio de la predicción conjunta lograda mediante votación pero sin argumentación; y finalmente la barra de AMAL muestra la precisión promedio de la predicción conjunta utilizando argumentación. Los resultados mostrados son el promedio de 5 ejecuciones de validación cruzada de 10 pliegues. La Figura 5 muestra que la colaboración (votación y AMAL) supera el rendimiento de la resolución individual de problemas. Además, como esperábamos, la precisión mejora a medida que más agentes colaboran, ya que se tiene en cuenta más información. También podemos ver que AMAL siempre supera al voto estándar, demostrando que las decisiones conjuntas se basan en una mejor información proporcionada por el proceso de argumentación. Por ejemplo, la precisión conjunta para 2 agentes en el conjunto de datos de esponjas es del 87.57% para AMAL y del 86.57% para la votación (mientras que la precisión individual es solo del 80.07%). Además, la mejora lograda por AMAL sobre Voting es aún mayor en el conjunto de datos de soja. La razón es que el conjunto de datos de soja es más difícil (en el sentido de que los agentes necesitan más datos para producir buenas predicciones). Estos resultados experimentales muestran que AMAL aprovecha de manera efectiva la oportunidad de mejora: la precisión es mayor solo porque más agentes han cambiado su opinión durante la argumentación (de lo contrario, lograrían el mismo resultado que la Votación). En relación con H2 (el aprendizaje a partir de la comunicación en procesos de argumentación mejora la predicción individual), llevamos a cabo el siguiente experimento: inicialmente, distribuimos un 25% del conjunto de entrenamiento entre los cinco agentes; después, el resto de los casos en el conjunto de entrenamiento se envía a los agentes uno por uno; cuando un agente recibe un nuevo caso de entrenamiento, tiene varias opciones: el agente puede descartarlo, el agente puede retenerlo, o el agente puede usarlo para participar en un proceso de argumentación. La Figura 6 muestra el resultado de ese experimento para los dos conjuntos de datos. La Figura 6 contiene tres gráficos, donde NL (no aprendizaje) muestra la precisión de un agente sin aprendizaje alguno; L (aprendizaje) muestra la evolución de la precisión de clasificación individual cuando los agentes aprenden reteniendo los casos de entrenamiento que reciben individualmente (notar que cuando todos los casos de entrenamiento han sido retenidos al 100%, la precisión debería ser igual a la de la Figura 5 para los agentes individuales); y finalmente LFC (aprendizaje a partir de la comunicación) muestra la evolución de la precisión de clasificación individual de los agentes de aprendizaje que también aprenden reteniendo los contraejemplos recibidos durante la argumentación (es decir, aprenden tanto de ejemplos de entrenamiento como de contraejemplos). La Figura 6 muestra que si un agente Ai también aprende de la comunicación, Ai puede mejorar significativamente su rendimiento individual con solo un pequeño número de casos adicionales (seleccionados como contraejemplos relevantes para Ai durante la argumentación). Por ejemplo, en el conjunto de datos de soja, los agentes individuales han logrado una precisión del 70.62% cuando también aprenden de la comunicación, en comparación con una precisión del 59.93% cuando solo aprenden de su experiencia individual. El número de casos aprendidos a través de la comunicación depende de las propiedades del conjunto de datos: en el conjunto de datos de esponjas, los agentes solo han retenido muy pocos casos adicionales y han mejorado significativamente la precisión individual; es decir, retienen en promedio 59.96 casos (en comparación con los 50.4 casos retenidos si no aprenden de la comunicación). En el conjunto de datos de soja se aprenden más contraejemplos para mejorar significativamente la precisión individual, es decir, retienen en promedio 87.16 casos (en comparación con 55.27 casos retenidos si no aprenden de la comunicación). Finalmente, el hecho de que ambos conjuntos de datos muestren una mejora significativa señala la naturaleza adaptativa del enfoque basado en la argumentación para el aprendizaje a partir de la comunicación: los casos útiles se seleccionan como contraejemplos (y no más de los necesarios), y tienen el efecto deseado. 10. TRABAJO RELACIONADO En cuanto al RCB en un entorno multiagente, la primera investigación se centró en la recuperación de casos negociada [11] entre grupos de agentes. Nuestro trabajo sobre el aprendizaje basado en casos multiagente comenzó en 1999; más tarde Mc Ginty y Smyth presentaron un enfoque colaborativo de CBR multiagente (CCBR) para la planificación. Finalmente, otro enfoque interesante es el razonamiento de múltiples bases de casos (MCBR) [5], que trata sobre The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 981 ESPONJA 60 65 70 75 80 85 25% 40% 55% 70% 85% 100% LFC L NL SOJA 20 30 40 50 60 70 80 90 25% 40% 55% 70% 85% 100% LFC L NL Figura 6: Aprendizaje a partir de la comunicación resultante de la argumentación en un sistema compuesto por 5 agentes. sistemas distribuidos donde hay varias bases de casos disponibles para la misma tarea y aborda los problemas de adaptación entre bases de casos. La principal diferencia es que nuestro enfoque MAC es una forma de distribuir el proceso de Reutilización de CBR (utilizando un sistema de votación) mientras que la Recuperación se realiza individualmente por cada agente; sin embargo, los otros enfoques de CBR multiagente se centran en distribuir el proceso de Recuperación. La investigación sobre la argumentación en MAS se centra en varios temas como a) lógicas, protocolos y lenguajes que respaldan la argumentación, b) selección de argumentos y c) interpretación de argumentos. Los enfoques para la lógica y los lenguajes que respaldan la argumentación incluyen la lógica no definitiva [4] y los modelos BDI [13]. Aunque la selección de argumentos es un aspecto clave de la argumentación automatizada (ver [12] y [13]), la mayoría de las investigaciones se han centrado en las relaciones de preferencia entre argumentos. En nuestro marco de trabajo hemos abordado tanto la selección de argumentos como las relaciones de preferencia utilizando un enfoque basado en casos. 11. CONCLUSIONES Y TRABAJO FUTURO En este artículo hemos presentado un marco basado en argumentación para el aprendizaje multiagente. Específicamente, hemos presentado AMAL, un marco que permite a un grupo de agentes de aprendizaje discutir sobre la solución de un problema dado y hemos demostrado cómo las capacidades de aprendizaje pueden ser utilizadas para generar argumentos y contraargumentos. La evaluación experimental muestra que la cantidad aumentada de información proporcionada a los agentes por el proceso de argumentación aumenta su <br>precisión predictiva</br>, especialmente cuando un número adecuado de agentes participa en la argumentación. Las principales contribuciones de este trabajo son: a) un marco de argumentación para agentes de aprendizaje; b) una relación de preferencia basada en casos sobre argumentos, basada en el cálculo de una estimación general de confianza de los argumentos; c) una política basada en casos para generar contraargumentos y seleccionar contraejemplos; y d) un enfoque basado en la argumentación para aprender a partir de la comunicación. Finalmente, en los experimentos presentados aquí, un agente de aprendizaje retendría todos los contraejemplos presentados por el otro agente; sin embargo, esta es una política de retención de casos muy simple, y nos gustaría experimentar con políticas más informadas, con el objetivo de que los agentes de aprendizaje individuales puedan mejorar significativamente utilizando solo un pequeño conjunto de casos propuestos por otros agentes. Finalmente, nuestro enfoque se centra en el aprendizaje perezoso, y los trabajos futuros tienen como objetivo incorporar el aprendizaje inductivo ansioso dentro del marco argumentativo para el aprendizaje a partir de la comunicación. 12. REFERENCIAS [1] Agnar Aamodt y Enric Plaza. Razonamiento basado en casos: Aspectos fundamentales, variaciones metodológicas y enfoques de sistemas. Comunicaciones de Inteligencia Artificial, 7(1):39-59, 1994. [2] E. Armengol y E. Plaza. Inducción perezosa de descripciones para el aprendizaje basado en casos relacionales. En ECML2001, páginas 13-24, 2001. [3] Gerhard Brewka. Sistemas de argumentación dinámica: Un modelo formal de procesos de argumentación basado en el cálculo de situaciones. Revista de Lógica y Computación, 11(2):257-282, 2001. [4] Carlos I. Chesñevar y Guillermo R. Simari. Formalizando la argumentación no definitiva utilizando sistemas deductivos etiquetados. Revista de Ciencias de la Computación y Tecnología, 1(4):18-33, 2000. [5] D. Leake y R. Sooriamurthi. Seleccionando estrategias automáticamente para el razonamiento multi-base de casos. En S. Craw y A. Preece, editores, ECCBR2002, páginas 204-219, Berlín, 2002. Springer Verlag. [6] Francisco J. Martín, Enric Plaza y Josep-Lluis Arcos. Reutilización de conocimiento y experiencia a través de comunicaciones entre agentes competentes (pares). Revista Internacional de Ingeniería de Software e Ingeniería del Conocimiento, 9(3):319-341, 1999. [7] Lorraine McGinty y Barry Smyth. Razonamiento basado en casos colaborativo: Aplicaciones en la planificación de rutas personalizadas. En I. Watson y Q. Yang, editores, ICCBR, número 2080 en LNAI, páginas 362-376. Springer-Verlag, 2001. [8] Santi Ontañón y Enric Plaza. Aprendizaje multiagente basado en justificaciones. En ICML2003, páginas 576-583. Morgan Kaufmann, 2003. [9] Enric Plaza, Eva Armengol y Santiago Ontañón. El poder explicativo de la similitud simbólica en el razonamiento basado en casos. Revisión de Inteligencia Artificial, 24(2):145-161, 2005. [10] David Poole. En la comparación de teorías: prefiriendo la explicación más específica. En IJCAI-85, páginas 144-147, 1985. [11] M V Nagendra Prassad, Victor R Lesser y Susan Lander. Recuperación y razonamiento en bases de casos distribuidas. Informe técnico, Departamento de Ciencias de la Computación de UMass, 1995. [12] K. Sycara S. Kraus y A. Evenchik. Alcanzando acuerdos a través de la argumentación: un modelo lógico y su implementación. Revista de Inteligencia Artificial, 104:1-69, 1998. [13] N. R. Jennings, S. Parsons, C. Sierra. Agentes que razonan y negocian mediante argumentos. Revista de Lógica y Computación, 8:261-292, 1998. [14] Bruce A. Wooley. Componente de explicación de sistemas de software. ACM CrossRoads, 5.1, 1998. 982 The Sixth Intl. \n\nACM CrossRoads, 5.1, 1998. 982 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "case-based policy": {
            "translated_key": "política basada en casos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning and Joint Deliberation through Argumentation in Multi-Agent Systems Santi Ontañón CCL, Cognitive Computing Lab Georgia Institute of Technology Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Artificial Intelligence Research Institute CSIC, Spanish Council for Scientific Research Campus UAB, 08193 Bellaterra, Catalonia (Spain) enric@iiia.csic.es ABSTRACT In this paper we will present an argumentation framework for learning agents (AMAL) designed for two purposes: (1) for joint deliberation, and (2) for learning from communication.",
                "The AMAL framework is completely based on learning from examples: the argument preference relation, the argument generation policy, and the counterargument generation policy are case-based techniques.",
                "For join deliberation, learning agents share their experience by forming a committee to decide upon some joint decision.",
                "We experimentally show that the argumentation among committees of agents improves both the individual and joint performance.",
                "For learning from communication, an agent engages into arguing with other agents in order to contrast its individual hypotheses and receive counterexamples; the argumentation process improves their learning scope and individual performance.",
                "Categories and Subject Descriptors I.2.6 [Artificial Intelligence]: Learning; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems, Intelligent Agents 1.",
                "INTRODUCTION Argumentation frameworks for multi-agent systems can be used for different purposes like joint deliberation, persuasion, negotiation, and conflict resolution.",
                "In this paper we will present an argumentation framework for learning agents, and show that it can be used for two purposes: (1) joint deliberation, and (2) learning from communication.",
                "Argumentation-based joint deliberation involves discussion over the outcome of a particular situation or the appropriate course of action for a particular situation.",
                "Learning agents are capable of learning from experience, in the sense that past examples (situations and their outcomes) are used to predict the outcome for the situation at hand.",
                "However, since individual agents experience may be limited, individual knowledge and prediction accuracy is also limited.",
                "Thus, learning agents that are capable of arguing their individual predictions with other agents may reach better prediction accuracy after such an argumentation process.",
                "Most existing argumentation frameworks for multi-agent systems are based on deductive logic or some other deductive logic formalism specifically designed to support argumentation, such as default logic [3]).",
                "Usually, an argument is seen as a logical statement, while a counterargument is an argument offered in opposition to another argument [4, 13]; agents use a preference relation to resolve conflicting arguments.",
                "However, logic-based argumentation frameworks assume agents with preloaded knowledge and preference relation.",
                "In this paper, we focus on an Argumentation-based Multi-Agent Learning (AMAL) framework where both knowledge and preference relation are learned from experience.",
                "Thus, we consider a scenario with agents that (1) work in the same domain using a shared ontology, (2) are capable of learning from examples, and (3) communicate using an argumentative framework.",
                "Having learning capabilities allows agents effectively use a specific form of counterargument, namely the use of counterexamples.",
                "Counterexamples offer the possibility of agents learning during the argumentation process.",
                "Moreover, learning agents allow techniques that use learnt experience to generate adequate arguments and counterarguments.",
                "Specifically, we will need to address two issues: (1) how to define a technique to generate arguments and counterarguments from examples, and (2)how to define a preference relation over two conflicting arguments that have been induced from examples.",
                "This paper presents a case-based approach to address both issues.",
                "The agents use case-based reasoning (CBR) [1] to learn from past cases (where a case is a situation and its outcome) in order to predict the outcome of a new situation.",
                "We propose an argumentation protocol inside the AMAL framework at supports agents in reaching a joint prediction over a specific situation or problem - moreover, the reasoning needed to support the argumentation process will also be based on cases.",
                "In particular, we present two case-based measures, one for generating the arguments and counterarguments adequate to a particular situation and another for determining preference relation among arguments.",
                "Finally, we evaluate (1) if argumentation between learning agents can produce a joint prediction that improves over individual learning performance and (2) if learning from the counterexamples conveyed during the argumentation process increases the individual performance with precisely those cases being used while arguing among them.",
                "The paper is structured as follows.",
                "Section 2 discusses the relation among argumentation, collaboration and learning.",
                "Then Section 3 introduces our multi-agent CBR (MAC) framework and the notion of justified prediction.",
                "After that, Section 4 formally defines our argumentation framework.",
                "Sections 5 and 6 present our case-based preference relation and argument generation policies respectively.",
                "Later, Section 7 presents the argumentation protocol in our AMAL framework.",
                "After that, Section 8 presents an exemplification of the argumentation framework.",
                "Finally, Section 9 presents an empirical evaluation of our two main hypotheses.",
                "The paper closes with related work and conclusions sections. 2.",
                "ARGUMENTATION,COLLABORATION AND LEARNING Both learning and collaboration are ways in which an agent can improve individual performance.",
                "In fact, there is a clear parallelism between learning and collaboration in multi-agent systems, since both are ways in which agents can deal with their shortcomings.",
                "Let us show which are the main motivations that an agent can have to learn or to collaborate. • Motivations to learn: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems. • Motivations to collaborate: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems, - Increase the range of accessible resources.",
                "Looking at the above lists of motivation, we can easily see that learning and collaboration are very related in multi-agent systems.",
                "In fact, with the exception of the last item in the motivations to collaborate list, they are two extremes of a continuum of strategies to improve performance.",
                "An agent may choose to increase performance by learning, by collaborating, or by finding an intermediate point that combines learning and collaboration in order to improve performance.",
                "In this paper we will propose AMAL, an argumentation framework for learning agents, and will also also show how AMAL can be used both for learning from communication and for solving problems in a collaborative way: • Agents can solve problems in a collaborative way via engaging an argumentation process about the prediction for the situation at hand.",
                "Using this collaboration, the prediction can be done in a more informed way, since the information known by several agents has been taken into account. • Agents can also learn from communication with other agents by engaging an argumentation process.",
                "Agents that engage in such argumentation processes can learn from the arguments and counterexamples received from other agents, and use this information for predicting the outcomes of future situations.",
                "In the rest of this paper we will propose an argumentation framework and show how it can be used both for learning and for solving problems in a collaborative way. 3.",
                "MULTI-AGENT CBR SYSTEMS A Multi-Agent Case Based Reasoning System (MAC) M = {(A1, C1), ..., (An, Cn)} is a multi-agent system composed of A = {Ai, ..., An}, a set of CBR agents, where each agent Ai ∈ A possesses an individual case base Ci.",
                "Each individual agent Ai in a MAC is completely autonomous and each agent Ai has access only to its individual and private case base Ci.",
                "A case base Ci = {c1, ..., cm} is a collection of cases.",
                "Agents in a MAC system are able to individually solve problems, but they can also collaborate with other agents to solve problems.",
                "In this framework, we will restrict ourselves to analytical tasks, i.e. tasks like classification, where the solution of a problem is achieved by selecting a solution class from an enumerated set of solution classes.",
                "In the following we will note the set of all the solution classes by S = {S1, ..., SK }.",
                "Therefore, a case c = P, S is a tuple containing a case description P and a solution class S ∈ S. In the following, we will use the terms problem and case description indistinctly.",
                "Moreover, we will use the dot notation to refer to elements inside a tuple; e.g., to refer to the solution class of a case c, we will write c.S.",
                "Therefore, we say a group of agents perform joint deliberation, when they collaborate to find a joint solution by means of an argumentation process.",
                "However, in order to do so, an agent has to be able to justify its prediction to the other agents (i.e. generate an argument for its predicted solution that can be examined and critiqued by the other agents).",
                "The next section addresses this issue. 3.1 Justified Predictions Both expert systems and CBR systems may have an explanation component [14] in charge of justifying why the system has provided a specific answer to the user.",
                "The line of reasoning of the system can then be examined by a human expert, thus increasing the reliability of the system.",
                "Most of the existing work on explanation generation focuses on generating explanations to be provided to the user.",
                "However, in our approach we use explanations (or justifications) as a tool for improving communication and coordination among agents.",
                "We are interested in justifications since they can be used as arguments.",
                "For that purpose, we will benefit from the ability of some machine learning methods to provide justifications.",
                "A justification built by a CBR method after determining that the solution of a particular problem P was Sk is a description that contains the relevant information from the problem P that the CBR method has considered to predict Sk as the solution of P. In particular, CBR methods work by retrieving similar cases to the problem at hand, and then reusing their solutions for the current problem, expecting that since the problem and the cases are similar, the solutions will also be similar.",
                "Thus, if a CBR method has retrieved a set of cases C1, ..., Cn to solve a particular problem P the justification built will contain the relevant information from the problem P that made the CBR system retrieve that particular set of cases, i.e. it will contain the relevant information that P and C1, ..., Cn have in common.",
                "For example, Figure 1 shows a justification build by a CBR system for a toy problem (in the following sections we will show justifications for real problems).",
                "In the figure, a problem has two attributes (Traffic_light, and Cars_passing), the retrieval mechanism of the CBR system notices that by considering only the attribute Traffic_light, it can retrieve two cases that predict the same solution: wait.",
                "Thus, since only this attribute has been used, it is the only one appearing in the justification.",
                "The values of the rest of attributes are irrelevant, since whatever their value the solution class would have been the same. 976 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Problem Traffic_light: red Cars_passing: no Case 1 Traffic_light: red Cars_passing: no Solution: wait Case 3 Traffic_light: red Cars_passing: yes Solution: wait Case 4 Traffic_light: green Cars_passing: yes Solution: wait Case 2 Traffic_light: green Cars_passing: no Solution: cross Retrieved cases Solution: wait Justification Traffic_light: red Figure 1: An example of justification generation in a CBR system.",
                "Notice that, since the only relevant feature to decide is Traffic_light (the only one used to retrieve cases), it is the only one appearing in the justification.",
                "In general, the meaning of a justification is that all (or most of) the cases in the case base of an agent that satisfy the justification (i.e. all the cases that are subsumed by the justification) belong to the predicted solution class.",
                "In the rest of the paper, we will use to denote the subsumption relation.",
                "In our work, we use LID [2], a CBR method capable of building symbolic justifications such as the one exemplified in Figure 1.",
                "When an agent provides a justification for a prediction, the agent generates a justified prediction: DEFINITION 3.1.",
                "A Justified Prediction is a tuple J = A, P, S, D where agent A considers S the correct solution for problem P, and that prediction is justified a symbolic description D such that J.D J.P. Justifications can have many uses for CBR systems [8, 9].",
                "In this paper, we are going to use justifications as arguments, in order to allow learning agents to engage in argumentation processes. 4.",
                "ARGUMENTS AND COUNTERARGUMENTS For our purposes an argument α generated by an agent A is composed of a statement S and some evidence D supporting S as correct.",
                "In the remainder of this section we will see how this general definition of argument can be instantiated in specific kind of arguments that the agents can generate.",
                "In the context of MAC systems, agents argue about predictions for new problems and can provide two kinds of information: a) specific cases P, S , and b) justified predictions: A, P, S, D .",
                "Using this information, we can define three types of arguments: justified predictions, counterarguments, and counterexamples.",
                "A justified prediction α is generated by an agent Ai to argue that Ai believes that the correct solution for a given problem P is α.S, and the evidence provided is the justification α.D.",
                "In the example depicted in Figure 1, an agent Ai may generate the argument α = Ai, P, Wait, (Traffic_light = red) , meaning that the agent Ai believes that the correct solution for P is Wait because the attribute Traffic_light equals red.",
                "A counterargument β is an argument offered in opposition to another argument α.",
                "In our framework, a counterargument consists of a justified prediction Aj, P, S , D generated by an agent Aj with the intention to rebut an argument α generated by another agent Ai, that endorses a solution class S different from that of α.S for the problem at hand and justifies this with a justification D .",
                "In the example in Figure 1, if an agent generates the argument α = Ai, P, Walk, (Cars_passing = no) , an agent that thinks that the correct solution is Wait might answer with the counterargument β = Aj, P, Wait, (Cars_passing = no ∧ Traffic_light = red) , meaning that, although there are no cars passing, the traffic light is red, and the street cannot be crossed.",
                "A counterexample c is a case that contradicts an argument α.",
                "Thus a counterexample is also a counterargument, one that states that a specific argument α is not always true, and the evidence provided is the case c. Specifically, for a case c to be a counterexample of an argument α, the following conditions have to be met: α.D c and α.S = c.S, i.e. the case must satisfy the justification α.D and the solution of c must be different than the predicted by α.",
                "By exchanging arguments and counterarguments (including counterexamples), agents can argue about the correct solution of a given problem, i.e. they can engage a joint deliberation process.",
                "However, in order to do so, they need a specific interaction protocol, a preference relation between contradicting arguments, and a decision policy to generate counterarguments (including counterexamples).",
                "In the following sections we will present these elements. 5.",
                "PREFERENCE RELATION A specific argument provided by an agent might not be consistent with the information known to other agents (or even to some of the information known by the agent that has generated the justification due to noise in training data).",
                "For that reason, we are going to define a preference relation over contradicting justified predictions based on cases.",
                "Basically, we will define a confidence measure for each justified prediction (that takes into account the cases owned by each agent), and the justified prediction with the highest confidence will be the preferred one.",
                "The idea behind case-based confidence is to count how many of the cases in an individual case base endorse a justified prediction, and how many of them are counterexamples of it.",
                "The more the endorsing cases, the higher the confidence; and the more the counterexamples, the lower the confidence.",
                "Specifically, to assess the confidence of a justified prediction α, an agent obtains the set of cases in its individual case base that are subsumed by α.D.",
                "With them, an agent Ai obtains the Y (aye) and N (nay) values: • Y Ai α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by the justification α.D that belong to the solution class α.S, • NAi α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by justification α.D that do not belong to that solution class.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 977 + + + + + +  - - + Figure 2: Confidence of arguments is evaluated by contrasting them against the case bases of the agents.",
                "An agent estimates the confidence of an argument as: CAi (α) = Y Ai α 1 + Y Ai α + NAi α i.e. the confidence on a justified prediction is the number of endorsing cases divided by the number of endorsing cases plus counterexamples.",
                "Notice that we add 1 to the denominator, this is to avoid giving excessively high confidences to justified predictions whose confidence has been computed using a small number of cases.",
                "Notice that this correction follows the same idea than the Laplace correction to estimate probabilities.",
                "Figure 2 illustrates the individual evaluation of the confidence of an argument, in particular, three endorsing cases and one counterexample are found in the case base of agents Ai, giving an estimated confidence of 0.6 Moreover, we can also define the joint confidence of an argument α as the confidence computed using the cases present in the case bases of all the agents in the group: C(α) = i Y Ai α 1 + i Y Ai α + NAi α Notice that, to collaboratively compute the joint confidence, the agents only have to make public the aye and nay values locally computed for a given argument.",
                "In our framework, agents use this joint confidence as the preference relation: a justified prediction α is preferred over another one β if C(α) ≥ C(β). 6.",
                "GENERATION OF ARGUMENTS In our framework, arguments are generated by the agents from cases, using learning methods.",
                "Any learning method able to provide a justified prediction can be used to generate arguments.",
                "For instance, decision trees and LID [2] are suitable learning methods.",
                "Specifically, in the experiments reported in this paper agents use LID.",
                "Thus, when an agent wants to generate an argument endorsing that a specific solution class is the correct solution for a problem P, it generates a justified prediction as explained in Section 3.1.",
                "For instance, Figure 3 shows a real justification generated by LID after solving a problem P in the domain of marine sponges identification.",
                "In particular, Figure 3 shows how when an agent receives a new problem to solve (in this case, a new sponge to determine its order), the agent uses LID to generate an argument (consisting on a justified prediction) using the cases in the case base of the agent.",
                "The justification shown in Figure 3 can be interpreted saying that the predicted solution is hadromerida because the smooth form of the megascleres of the spiculate skeleton of the sponge is of type tylostyle, the spikulate skeleton of the sponge has no uniform length, and there is no gemmules in the external features of the sponge.",
                "Thus, the argument generated will be α = A1, P, hadromerida, D1 . 6.1 Generation of Counterarguments As previously stated, agents may try to rebut arguments by generating counterargument or by finding counterexamples.",
                "Let us explain how they can be generated.",
                "An agent Ai wants to generate a counterargument β to rebut an argument α when α is in contradiction with the local case base of Ai.",
                "Moreover, while generating such counterargument β, Ai expects that β is preferred over α.",
                "For that purpose, we will present a specific policy to generate counterarguments based on the specificity criterion [10].",
                "The specificity criterion is widely used in deductive frameworks for argumentation, and states that between two conflicting arguments, the most specific should be preferred since it is, in principle, more informed.",
                "Thus, counterarguments generated based on the specificity criterion are expected to be preferable (since they are more informed) to the arguments they try to rebut.",
                "However, there is no guarantee that such counterarguments will always win, since, as we have stated in Section 5, agents in our framework use a preference relation based on joint confidence.",
                "Moreover, one may think that it would be better that the agents generate counterarguments based on the joint confidence preference relation; however it is not obvious how to generate counterarguments based on joint confidence in an efficient way, since collaboration is required in order to evaluate joint confidence.",
                "Thus, the agent generating the counterargument should constantly communicate with the other agents at each step of the induction algorithm used to generate counterarguments (presently one of our future research lines).",
                "Thus, in our framework, when an agent wants to generate a counterargument β to an argument α, β has to be more specific than α (i.e. α.D < β.D).",
                "The generation of counterarguments using the specificity criterion imposes some restrictions over the learning method, although LID or ID3 can be easily adapted for this task.",
                "For instance, LID is an algorithm that generates a description starting from scratch and heuristically adding features to that term.",
                "Thus, at every step, the description is made more specific than in the previous step, and the number of cases that are subsumed by that description is reduced.",
                "When the description covers only (or almost only) cases of a single solution class LID terminates and predicts that solution class.",
                "To generate a counterargument to an argument α LID just has to use as starting point the description α.D instead of starting from scratch.",
                "In this way, the justification provided by LID will always be subsumed by α.D, and thus the resulting counterargument will be more specific than α.",
                "However, notice that LID may sometimes not be able to generate counterarguments, since LID may not be able to specialize the description α.D any further, or because the agent Ai has no case inCi that is subsumed by α.D.",
                "Figure 4 shows how an agent A2 that disagreed with the argument shown in Figure 3, generates a counterargument using LID.",
                "Moreover, Figure 4 shows the generation of a counterargument β1 2 for the argument α0 1 (in Figure 3) that is a specialization of α0 1. 978 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Solution: hadromerida Justification: D1 Sponge Spikulate skeleton External features External features Gemmules: no Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Case Base of A1 LID New sponge P Figure 3: Example of a real justification generated by LID in the marine sponges data set.",
                "Specifically, in our experiments, when an agent Ai wants to rebut an argument α, uses the following policy: 1.",
                "Agent Ai uses LID to try to find a counterargument β more specific than α; if found, β is sent to the other agent as a counterargument of α. 2.",
                "If not found, then Ai searches for a counterexample c ∈ Ci of α.",
                "If a case c is found, then c is sent to the other agent as a counterexample of α. 3.",
                "If no counterexamples are found, then Ai cannot rebut the argument α. 7.",
                "ARGUMENTATION-BASED MULTI-AGENT LEARNING The interaction protocol of AMAL allows a group of agents A1, ..., An to deliberate about the correct solution of a problem P by means of an argumentation process.",
                "If the argumentation process arrives to a consensual solution, the joint deliberation ends; otherwise a weighted vote is used to determine the joint solution.",
                "Moreover, AMAL also allows the agents to learn from the counterexamples received from other agents.",
                "The AMAL protocol consists on a series of rounds.",
                "In the initial round, each agent states which is its individual prediction for P. Then, at each round an agent can try to rebut the prediction made by any of the other agents.",
                "The protocol uses a token passing mechanism so that agents (one at a time) can send counterarguments or counterexamples if they disagree with the prediction made by any other agent.",
                "Specifically, each agent is allowed to send one counterargument or counterexample each time he gets the token (notice that this restriction is just to simplify the protocol, and that it does not restrict the number of counterargument an agent can sent, since they can be delayed for subsequent rounds).",
                "When an agent receives a counterargument or counterexample, it informs the other agents if it accepts the counterargument (and changes its prediction) or not.",
                "Moreover, agents have also the opportunity to answer to counterarguments when they receive the token, by trying to generate a counterargument to the counterargument.",
                "When all the agents have had the token once, the token returns to the first agent, and so on.",
                "If at any time in the protocol, all the agents agree or during the last n rounds no agent has generated any counterargument, the protocol ends.",
                "Moreover, if at the end of the argumentation the agents have not reached an agreement, then a voting mechanism that uses the confidence of each prediction as weights is used to decide the final solution (Thus, AMAL follows the same mechanism as human committees, first each individual member of a committee exposes his arguments and discuses those of the other members (joint deliberation), and if no consensus is reached, then a voting mechanism is required).",
                "At each iteration, agents can use the following performatives: • assert(α): the justified prediction held during the next round will be α.",
                "An agent can only hold a single prediction at each round, thus is multiple asserts are send, only the last one is considered as the currently held prediction. • rebut(β, α): the agent has found a counterargument β to the prediction α.",
                "We will define Ht = αt 1, ..., αt n as the predictions that each of the n agents hold at a round t. Moreover, we will also define contradict(αt i) = {α ∈ Ht|α.S = αt i.S} as the set of contradicting arguments for an agent Ai in a round t, i.e. the set of arguments at round t that support a different solution class than αt i.",
                "The protocol is initiated because one of the agents receives a problem P to be solved.",
                "After that, the agent informs all the other agents about the problem P to solve, and the protocol starts: 1.",
                "At round t = 0, each one of the agents individually solves P, and builds a justified prediction using its own CBR method.",
                "Then, each agent Ai sends the performative assert(α0 i ) to the other agents.",
                "Thus, the agents know H0 = α0 i , ..., α0 n .",
                "Once all the predictions have been sent the token is given to the first agent A1. 2.",
                "At each round t (other than 0), the agents check whether their arguments in Ht agree.",
                "If they do, the protocol moves to step 5.",
                "Moreover, if during the last n rounds no agent has sent any counterexample or counterargument, the protocol also moves to step 5.",
                "Otherwise, the agent Ai owner of the token tries to generate a counterargument for each of the opposing arguments in contradict(αt i) ⊆ Ht (see Section 6.1).",
                "Then, the counterargument βt i against the prediction αt j with the lowest confidence C(αt j) is selected (since αt j is the prediction more likely to be successfully rebutted). • If βt i is a counterargument, then, Ai locally compares αt i with βt i by assessing their confidence against its individual case base Ci (see Section 5) (notice that Ai is comparing its previous argument with the counterargument that Ai itself has just generated and that is about The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 979 Sponge Spikulate skeleton External features External features Gemmules: no Growing: Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Growing Grow: massive Case Base of A2 LID Solution: astrophorida Justification: D2 Figure 4: Generation of a counterargument using LID in the sponges data set. to send to Aj).",
                "If CAi (βt i ) > CAi (αt i), then Ai considers that βt i is stronger than its previous argument, changes its argument to βt i by sending assert(βt i ) to the rest of the agents (the intuition behind this is that since a counterargument is also an argument, Ai checks if the newly counterargument is a better argument than the one he was previously holding) and rebut(βt i , αt j) to Aj.",
                "Otherwise (i.e.",
                "CAi (βt i ) ≤ CAi (αt i)), Ai will send only rebut(βt i , αt j) to Aj.",
                "In any of the two situations the protocol moves to step 3. • If βt i is a counterexample c, then Ai sends rebut(c, αt j) to Aj.",
                "The protocol moves to step 4. • If Ai cannot generate any counterargument or counterexample, the token is sent to the next agent, a new round t + 1 starts, and the protocol moves to state 2. 3.",
                "The agent Aj that has received the counterargument βt i , locally compares it against its own argument, αt j, by locally assessing their confidence.",
                "If CAj (βt i ) > CAj (αt j), then Aj will accept the counterargument as stronger than its own argument, and it will send assert(βt i ) to the other agents.",
                "Otherwise (i.e.",
                "CAj (βt i ) ≤ CAj (αt j)), Aj will not accept the counterargument, and will inform the other agents accordingly.",
                "Any of the two situations start a new round t + 1, Ai sends the token to the next agent, and the protocol moves back to state 2. 4.",
                "The agent Aj that has received the counterexample c retains it into its case base and generates a new argument αt+1 j that takes into account c, and informs the rest of the agents by sending assert(αt+1 j ) to all of them.",
                "Then, Ai sends the token to the next agent, a new round t + 1 starts, and the protocol moves back to step 2. 5.",
                "The protocol ends yielding a joint prediction, as follows: if the arguments in Ht agree then their prediction is the joint prediction, otherwise a voting mechanism is used to decide the joint prediction.",
                "The voting mechanism uses the joint confidence measure as the voting weights, as follows: S = arg max Sk∈S αi∈Ht|αi.S=Sk C(αi) Moreover, in order to avoid infinite iterations, if an agent sends twice the same argument or counterargument to the same agent, the message is not considered. 8.",
                "EXEMPLIFICATION Let us consider a system composed of three agents A1, A2 and A3.",
                "One of the agents, A1 receives a problem P to solve, and decides to use AMAL to solve it.",
                "For that reason, invites A2 and A3 to take part in the argumentation process.",
                "They accept the invitation, and the argumentation protocol starts.",
                "Initially, each agent generates its individual prediction for P, and broadcasts it to the other agents.",
                "Thus, all of them can compute H0 = α0 1, α0 2, α0 3 .",
                "In particular, in this example: • α0 1 = A1, P, hadromerida, D1 • α0 2 = A2, P, astrophorida, D2 • α0 3 = A3, P, axinellida, D3 A1 starts owning the token and tries to generate counterarguments for α0 2 and α0 3, but does not succeed, however it has one counterexample c13 for α0 3.",
                "Thus, A1 sends the the message rebut( c13, α0 3) to A3.",
                "A3 incorporates c13 into its case base and tries to solve the problem P again, now taking c13 into consideration.",
                "A3 comes up with the justified prediction α1 3 = A3, P, hadromerida, D4 , and broadcasts it to the rest of the agents with the message assert(α1 3).",
                "Thus, all of them know the new H1 = α0 1, α0 2, α1 3 .",
                "Round 1 starts and A2 gets the token.",
                "A2 tries to generate counterarguments for α0 1 and α1 3 and only succeeds to generate a counterargument β1 2 = A2, P, astrophorida, D5 against α1 3.",
                "The counterargument is sent to A3 with the message rebut(β1 2 , α1 3).",
                "Agent A3 receives the counterargument and assesses its local confidence.",
                "The result is that the individual confidence of the counterargument β1 2 is lower than the local confidence of α1 3.",
                "Therefore, A3 does not accept the counterargument, and thus H2 = α0 1, α0 2, α1 3 .",
                "Round 2 starts and A3 gets the token.",
                "A3 generates a counterargument β2 3 = A3, P, hadromerida, D6 for α0 2 and sends it to A2 with the message rebut(β2 3 , α0 2).",
                "Agent A2 receives the counterargument and assesses its local confidence.",
                "The result is that the local confidence of the counterargument β2 3 is higher than the local confidence of α0 2.",
                "Therefore, A2 accepts the counterargument and informs the rest of the agents with the message assert(β2 3 ).",
                "After that, H3 = α0 1, β2 3 , α1 3 .",
                "At Round 3, since all the agents agree (all the justified predictions in H3 predict hadromerida as the solution class) The protocol ends, and A1 (the agent that received the problem) considers hadromerida as the joint solution for the problem P. 9.",
                "EXPERIMENTAL EVALUATION 980 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) SPONGE 75 77 79 81 83 85 87 89 91 2 3 4 5 AMAL Voting Individual SOYBEAN 55 60 65 70 75 80 85 90 2 3 4 5 AMAL Voting Individual Figure 5: Individual and joint accuracy for 2 to 5 agents.",
                "In this section we empirically evaluate the AMAL argumentation framework.",
                "We have made experiments in two different data sets: soybean (from the UCI machine learning repository) and sponge (a relational data set).",
                "The soybean data set has 307 examples and 19 solution classes, while the sponge data set has 280 examples and 3 solution classes.",
                "In an experimental run, the data set is divided in 2 sets: the training set and the test set.",
                "The training set examples are distributed among 5 different agents without replication, i.e. there is no example shared by two agents.",
                "In the testing stage, problems in the test set arrive randomly to one of the agents, and their goal is to predict the correct solution.",
                "The experiments are designed to test two hypotheses: (H1) that argumentation is a useful framework for joint deliberation and can improve over other typical methods such as voting; and (H2) that learning from communication improves the individual performance of a learning agent participating in an argumentation process.",
                "Moreover, we also expect that the improvement achieved from argumentation will increase as the number of agents participating in the argumentation increases (since more information will be taken into account).",
                "Concerning H1 (argumentation is a useful framework for joint deliberation), we ran 4 experiments, using 2, 3, 4, and 5 agents respectively (in all experiments each agent has a 20% of the training data, since the training is always distributed among 5 agents).",
                "Figure 5 shows the result of those experiments in the sponge and soybean data sets.",
                "Classification accuracy is plotted in the vertical axis, and in the horizontal axis the number of agents that took part in the argumentation processes is shown.",
                "For each number of agents, three bars are shown: individual, Voting, and AMAL.",
                "The individual bar shows the average accuracy of individual agents predictions; the voting bar shows the average accuracy of the joint prediction achieved by voting but without any argumentation; and finally the AMAL bar shows the average accuracy of the joint prediction using argumentation.",
                "The results shown are the average of 5 10-fold cross validation runs.",
                "Figure 5 shows that collaboration (voting and AMAL) outperforms individual problem solving.",
                "Moreover, as we expected, the accuracy improves as more agents collaborate, since more information is taken into account.",
                "We can also see that AMAL always outperforms standard voting, proving that joint decisions are based on better information as provided by the argumentation process.",
                "For instance, the joint accuracy for 2 agents in the sponge data set is of 87.57% for AMAL and 86.57% for voting (while individual accuracy is just 80.07%).",
                "Moreover, the improvement achieved by AMAL over Voting is even larger in the soybean data set.",
                "The reason is that the soybean data set is more difficult (in the sense that agents need more data to produce good predictions).",
                "These experimental results show that AMAL effectively exploits the opportunity for improvement: the accuracy is higher only because more agents have changed their opinion during argumentation (otherwise they would achieve the same result as Voting).",
                "Concerning H2 (learning from communication in argumentation processes improves individual prediction ), we ran the following experiment: initially, we distributed a 25% of the training set among the five agents; after that, the rest of the cases in the training set is sent to the agents one by one; when an agent receives a new training case, it has several options: the agent can discard it, the agent can retain it, or the agent can use it for engaging an argumentation process.",
                "Figure 6 shows the result of that experiment for the two data sets.",
                "Figure 6 contains three plots, where NL (not learning) shows accuracy of an agent with no learning at all; L (learning), shows the evolution of the individual classification accuracy when agents learn by retaining the training cases they individually receive (notice that when all the training cases have been retained at 100%, the accuracy should be equal to that of Figure 5 for individual agents); and finally LFC (learning from communication) shows the evolution of the individual classification accuracy of learning agents that also learn by retaining those counterexamples received during argumentation (i.e. they learn both from training examples and counterexamples).",
                "Figure 6 shows that if an agent Ai learns also from communication, Ai can significantly improve its individual performance with just a small number of additional cases (those selected as relevant counterexamples for Ai during argumentation).",
                "For instance, in the soybean data set, individual agents have achieved an accuracy of 70.62% when they also learn from communication versus an accuracy of 59.93% when they only learn from their individual experience.",
                "The number of cases learnt from communication depends on the properties of the data set: in the sponges data set, agents have retained only very few additional cases, and significantly improved individual accuracy; namely they retain 59.96 cases in average (compared to the 50.4 cases retained if they do not learn from communication).",
                "In the soybean data set more counterexamples are learnt to significantly improve individual accuracy, namely they retain 87.16 cases in average (compared to 55.27 cases retained if they do not learn from communication).",
                "Finally, the fact that both data sets show a significant improvement points out the adaptive nature of the argumentation-based approach to learning from communication: the useful cases are selected as counterexamples (and no more than those needed), and they have the intended effect. 10.",
                "RELATED WORK Concerning CBR in a multi-agent setting, the first research was on negotiated case retrieval [11] among groups of agents.",
                "Our work on multi-agent case-based learning started in 1999 [6]; later Mc Ginty and Smyth [7] presented a multi-agent collaborative CBR approach (CCBR) for planning.",
                "Finally, another interesting approach is multi-case-base reasoning (MCBR) [5], that deals with The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 981 SPONGE 60 65 70 75 80 85 25% 40% 55% 70% 85% 100% LFC L NL SOYBEAN 20 30 40 50 60 70 80 90 25% 40% 55% 70% 85% 100% LFC L NL Figure 6: Learning from communication resulting from argumentation in a system composed of 5 agents. distributed systems where there are several case bases available for the same task and addresses the problems of cross-case base adaptation.",
                "The main difference is that our MAC approach is a way to distribute the Reuse process of CBR (using a voting system) while Retrieve is performed individually by each agent; the other multiagent CBR approaches, however, focus on distributing the Retrieve process.",
                "Research on MAS argumentation focus on several issues like a) logics, protocols and languages that support argumentation, b) argument selection and c) argument interpretation.",
                "Approaches for logic and languages that support argumentation include defeasible logic [4] and BDI models [13].",
                "Although argument selection is a key aspect of automated argumentation (see [12] and [13]), most research has been focused on preference relations among arguments.",
                "In our framework we have addressed both argument selection and preference relations using a case-based approach. 11.",
                "CONCLUSIONS AND FUTURE WORK In this paper we have presented an argumentation-based framework for multi-agent learning.",
                "Specifically, we have presented AMAL, a framework that allows a group of learning agents to argue about the solution of a given problem and we have shown how the learning capabilities can be used to generate arguments and counterarguments.",
                "The experimental evaluation shows that the increased amount of information provided to the agents by the argumentation process increases their predictive accuracy, and specially when an adequate number of agents take part in the argumentation.",
                "The main contributions of this work are: a) an argumentation framework for learning agents; b) a case-based preference relation over arguments, based on computing an overall confidence estimation of arguments; c) a <br>case-based policy</br> to generate counterarguments and select counterexamples; and d) an argumentation-based approach for learning from communication.",
                "Finally, in the experiments presented here a learning agent would retain all counterexamples submitted by the other agent; however, this is a very simple case retention policy, and we will like to experiment with more informed policies - with the goal that individual learning agents could significantly improve using only a small set of cases proposed by other agents.",
                "Finally, our approach is focused on lazy learning, and future works aims at incorporating eager inductive learning inside the argumentative framework for learning from communication. 12.",
                "REFERENCES [1] Agnar Aamodt and Enric Plaza.",
                "Case-based reasoning: Foundational issues, methodological variations, and system approaches.",
                "Artificial Intelligence Communications, 7(1):39-59, 1994. [2] E. Armengol and E. Plaza.",
                "Lazy induction of descriptions for relational case-based learning.",
                "In ECML2001, pages 13-24, 2001. [3] Gerhard Brewka.",
                "Dynamic argument systems: A formal model of argumentation processes based on situation calculus.",
                "Journal of Logic and Computation, 11(2):257-282, 2001. [4] Carlos I. Chesñevar and Guillermo R. Simari.",
                "Formalizing Defeasible Argumentation using Labelled Deductive Systems.",
                "Journal of Computer Science & Technology, 1(4):18-33, 2000. [5] D. Leake and R. Sooriamurthi.",
                "Automatically selecting strategies for multi-case-base reasoning.",
                "In S. Craw and A. Preece, editors, ECCBR2002, pages 204-219, Berlin, 2002.",
                "Springer Verlag. [6] Francisco J. Martín, Enric Plaza, and Josep-Lluis Arcos.",
                "Knowledge and experience reuse through communications among competent (peer) agents.",
                "International Journal of Software Engineering and Knowledge Engineering, 9(3):319-341, 1999. [7] Lorraine McGinty and Barry Smyth.",
                "Collaborative case-based reasoning: Applications in personalized route planning.",
                "In I. Watson and Q. Yang, editors, ICCBR, number 2080 in LNAI, pages 362-376.",
                "Springer-Verlag, 2001. [8] Santi Ontañón and Enric Plaza.",
                "Justification-based multiagent learning.",
                "In ICML2003, pages 576-583.",
                "Morgan Kaufmann, 2003. [9] Enric Plaza, Eva Armengol, and Santiago Ontañón.",
                "The explanatory power of symbolic similarity in case-based reasoning.",
                "Artificial Intelligence Review, 24(2):145-161, 2005. [10] David Poole.",
                "On the comparison of theories: Preferring the most specific explanation.",
                "In IJCAI-85, pages 144-147, 1985. [11] M V Nagendra Prassad, Victor R Lesser, and Susan Lander.",
                "Retrieval and reasoning in distributed case bases.",
                "Technical report, UMass Computer Science Department, 1995. [12] K. Sycara S. Kraus and A. Evenchik.",
                "Reaching agreements through argumentation: a logical model and implementation.",
                "Artificial Intelligence Journal, 104:1-69, 1998. [13] N. R. Jennings S. Parsons, C. Sierra.",
                "Agents that reason and negotiate by arguing.",
                "Journal of Logic and Computation, 8:261-292, 1998. [14] Bruce A. Wooley.",
                "Explanation component of software systems.",
                "ACM CrossRoads, 5.1, 1998. 982 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "The main contributions of this work are: a) an argumentation framework for learning agents; b) a case-based preference relation over arguments, based on computing an overall confidence estimation of arguments; c) a <br>case-based policy</br> to generate counterarguments and select counterexamples; and d) an argumentation-based approach for learning from communication."
            ],
            "translated_annotated_samples": [
                "Las principales contribuciones de este trabajo son: a) un marco de argumentación para agentes de aprendizaje; b) una relación de preferencia basada en casos sobre argumentos, basada en el cálculo de una estimación general de confianza de los argumentos; c) una <br>política basada en casos</br> para generar contraargumentos y seleccionar contraejemplos; y d) un enfoque basado en la argumentación para aprender a partir de la comunicación."
            ],
            "translated_text": "Aprendizaje y deliberación conjunta a través de la argumentación en sistemas multiagente. Santi Ontañón CCL, Laboratorio de Computación Cognitiva, Instituto de Tecnología de Georgia, Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Instituto de Investigación en Inteligencia Artificial, CSIC, Consejo Superior de Investigaciones Científicas, Campus UAB, 08193 Bellaterra, Cataluña (España) enric@iiia.csic.es RESUMEN En este artículo presentaremos un marco de argumentación para agentes de aprendizaje (AMAL) diseñado con dos propósitos: (1) para deliberación conjunta, y (2) para aprender a través de la comunicación. El marco AMAL se basa completamente en el aprendizaje a partir de ejemplos: la relación de preferencia de argumentos, la política de generación de argumentos y la política de generación de contraargumentos son técnicas basadas en casos. Para unirse en la deliberación, los agentes de aprendizaje comparten su experiencia formando un comité para decidir sobre alguna decisión conjunta. Experimentalmente demostramos que la argumentación entre comités de agentes mejora tanto el rendimiento individual como el conjunto. Para aprender a través de la comunicación, un agente se involucra en discusiones con otros agentes para contrastar sus hipótesis individuales y recibir contraejemplos; el proceso de argumentación mejora su alcance de aprendizaje y rendimiento individual. Categorías y Descriptores de Asignaturas I.2.6 [Inteligencia Artificial]: Aprendizaje; I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente, Agentes Inteligentes 1. Los marcos de argumentación para sistemas multiagente pueden ser utilizados para diferentes propósitos como la deliberación conjunta, la persuasión, la negociación y la resolución de conflictos. En este artículo presentaremos un marco de argumentación para agentes de aprendizaje, y mostraremos que puede ser utilizado para dos propósitos: (1) deliberación conjunta, y (2) aprendizaje a partir de la comunicación. La deliberación conjunta basada en argumentos implica discutir sobre el resultado de una situación particular o la acción apropiada a tomar en una situación específica. Los agentes de aprendizaje son capaces de aprender de la experiencia, en el sentido de que ejemplos pasados (situaciones y sus resultados) se utilizan para predecir el resultado para la situación actual. Sin embargo, dado que la experiencia de los agentes individuales puede ser limitada, el conocimiento individual y la precisión de la predicción también son limitados. Por lo tanto, los agentes de aprendizaje que son capaces de argumentar sus predicciones individuales con otros agentes pueden alcanzar una mayor precisión en las predicciones después de dicho proceso de argumentación. La mayoría de los marcos de argumentación existentes para sistemas multiagente se basan en lógica deductiva u otra formalización de lógica deductiva diseñada específicamente para respaldar la argumentación, como la lógica por defecto [3]. Por lo general, un argumento se ve como una declaración lógica, mientras que un contraargumento es un argumento ofrecido en oposición a otro argumento; los agentes utilizan una relación de preferencia para resolver argumentos conflictivos. Sin embargo, los marcos de argumentación basados en lógica asumen agentes con conocimiento predefinido y relación de preferencia. En este artículo, nos enfocamos en un marco de Aprendizaje Multiagente basado en Argumentación (AMAL) donde tanto el conocimiento como la relación de preferencia se aprenden a partir de la experiencia. Por lo tanto, consideramos un escenario con agentes que (1) trabajan en el mismo dominio utilizando una ontología compartida, (2) son capaces de aprender a partir de ejemplos y (3) se comunican utilizando un marco argumentativo. Tener capacidades de aprendizaje permite a los agentes utilizar de manera efectiva una forma específica de contraargumento, a saber, el uso de contraejemplos. Los contraejemplos ofrecen la posibilidad de que los agentes aprendan durante el proceso de argumentación. Además, los agentes de aprendizaje permiten técnicas que utilizan la experiencia aprendida para generar argumentos y contraargumentos adecuados. Específicamente, necesitaremos abordar dos problemas: (1) cómo definir una técnica para generar argumentos y contraargumentos a partir de ejemplos, y (2) cómo definir una relación de preferencia sobre dos argumentos en conflicto que han sido inducidos a partir de ejemplos. Este documento presenta un enfoque basado en casos para abordar ambas cuestiones. Los agentes utilizan el razonamiento basado en casos (CBR) [1] para aprender de casos pasados (donde un caso es una situación y su resultado) con el fin de predecir el resultado de una nueva situación. Proponemos un protocolo de argumentación dentro del marco de AMAL que apoya a los agentes en llegar a una predicción conjunta sobre una situación o problema específico; además, el razonamiento necesario para respaldar el proceso de argumentación también se basará en casos. En particular, presentamos dos medidas basadas en casos, una para generar los argumentos y contraargumentos adecuados para una situación particular y otra para determinar la relación de preferencia entre los argumentos. Finalmente, evaluamos (1) si la argumentación entre agentes de aprendizaje puede producir una predicción conjunta que mejore el rendimiento de aprendizaje individual y (2) si el aprendizaje a partir de los contraejemplos transmitidos durante el proceso de argumentación aumenta el rendimiento individual con precisamente aquellos casos que se utilizan mientras discuten entre ellos. El documento está estructurado de la siguiente manera. La sección 2 discute la relación entre la argumentación, la colaboración y el aprendizaje. Luego, la Sección 3 introduce nuestro marco de trabajo de CBR multiagente (MAC) y el concepto de predicción justificada. Después de eso, la Sección 4 define formalmente nuestro marco de argumentación. Las secciones 5 y 6 presentan nuestra relación de preferencia basada en casos y las políticas de generación de argumentos, respectivamente. Más adelante, la Sección 7 presenta el protocolo de argumentación en nuestro marco AMAL. Después de eso, la Sección 8 presenta una ejemplificación del marco de argumentación. Finalmente, la Sección 9 presenta una evaluación empírica de nuestras dos hipótesis principales. El artículo concluye con secciones de trabajo relacionado y conclusiones. 2. ARGUMENTACIÓN, COLABORACIÓN Y APRENDIZAJE Tanto el aprendizaje como la colaboración son formas en las que un agente puede mejorar el rendimiento individual. De hecho, existe un claro paralelismo entre el aprendizaje y la colaboración en sistemas multiagentes, ya que ambos son formas en las que los agentes pueden lidiar con sus deficiencias. Mostremos cuáles son las principales motivaciones que un agente puede tener para aprender o colaborar. • Motivaciones para aprender: - Aumentar la calidad de la predicción, - Aumentar la eficiencia, - Ampliar el rango de problemas resolubles. • Motivaciones para colaborar: - Aumentar la calidad de la predicción, - Aumentar la eficiencia, - Ampliar el rango de problemas resolubles, - Ampliar el rango de recursos accesibles. Al observar las listas de motivación anteriores, podemos ver fácilmente que el aprendizaje y la colaboración están muy relacionados en los sistemas multiagente. De hecho, con la excepción del último elemento en la lista de motivaciones para colaborar, son dos extremos de un continuo de estrategias para mejorar el rendimiento. Un agente puede optar por aumentar el rendimiento mediante el aprendizaje, la colaboración o encontrando un punto intermedio que combine el aprendizaje y la colaboración para mejorar el rendimiento. En este artículo propondremos AMAL, un marco de argumentación para agentes de aprendizaje, y también mostraremos cómo AMAL puede ser utilizado tanto para aprender a través de la comunicación como para resolver problemas de manera colaborativa: • Los agentes pueden resolver problemas de manera colaborativa al participar en un proceso de argumentación sobre la predicción para la situación actual. Utilizando esta colaboración, la predicción puede realizarse de una manera más informada, ya que se ha tenido en cuenta la información conocida por varios agentes. Los agentes también pueden aprender de la comunicación con otros agentes al participar en un proceso de argumentación. Los agentes que participan en estos procesos de argumentación pueden aprender de los argumentos y contraejemplos recibidos de otros agentes, y utilizar esta información para predecir los resultados de situaciones futuras. En el resto de este documento propondremos un marco de argumentación y mostraremos cómo puede ser utilizado tanto para el aprendizaje como para la resolución de problemas de manera colaborativa. SISTEMAS CBR MULTIAGENTE Un Sistema de Razonamiento Basado en Casos Multiagente (MAC) M = {(A1, C1), ..., (An, Cn)} es un sistema multiagente compuesto por A = {Ai, ..., An}, un conjunto de agentes CBR, donde cada agente Ai ∈ A posee una base de casos individual Ci. Cada agente individual Ai en un MAC es completamente autónomo y cada agente Ai solo tiene acceso a su base de casos individual y privada Ci. Un conjunto de casos Ci = {c1, ..., cm} es una colección de casos. Los agentes en un sistema MAC pueden resolver problemas de forma individual, pero también pueden colaborar con otros agentes para resolver problemas. En este marco, nos limitaremos a tareas analíticas, es decir, tareas como la clasificación, donde la solución de un problema se logra seleccionando una clase de solución de un conjunto enumerado de clases de solución. En lo siguiente, notaremos el conjunto de todas las clases de soluciones por S = {S1, ..., SK}. Por lo tanto, un caso c = P, S es una tupla que contiene una descripción del caso P y una clase de solución S ∈ S. En lo siguiente, utilizaremos indistintamente los términos problema y descripción del caso. Además, utilizaremos la notación de punto para referirnos a elementos dentro de una tupla; por ejemplo, para referirnos a la clase de solución de un caso c, escribiremos c.S. Por lo tanto, decimos que un grupo de agentes realiza una deliberación conjunta cuando colaboran para encontrar una solución conjunta mediante un proceso de argumentación. Sin embargo, para hacerlo, un agente debe ser capaz de justificar su predicción a los otros agentes (es decir, generar un argumento para su solución predicha que pueda ser examinado y criticado por los otros agentes). La siguiente sección aborda este tema. 3.1 Predicciones Justificadas Tanto los sistemas expertos como los sistemas CBR pueden tener un componente de explicación [14] encargado de justificar por qué el sistema ha proporcionado una respuesta específica al usuario. La línea de razonamiento del sistema puede ser examinada por un experto humano, aumentando así la fiabilidad del sistema. La mayoría de los trabajos existentes sobre generación de explicaciones se centran en generar explicaciones para ser proporcionadas al usuario. Sin embargo, en nuestro enfoque utilizamos explicaciones (o justificaciones) como una herramienta para mejorar la comunicación y coordinación entre agentes. Estamos interesados en las justificaciones ya que pueden ser utilizadas como argumentos. Para ese propósito, nos beneficiaremos de la capacidad de algunos métodos de aprendizaje automático para proporcionar justificaciones. Una justificación construida por un método CBR después de determinar que la solución de un problema particular P fue Sk es una descripción que contiene la información relevante del problema P que el método CBR ha considerado para predecir Sk como la solución de P. En particular, los métodos CBR funcionan recuperando casos similares al problema en cuestión, y luego reutilizando sus soluciones para el problema actual, esperando que dado que el problema y los casos son similares, las soluciones también serán similares. Por lo tanto, si un método CBR ha recuperado un conjunto de casos C1, ..., Cn para resolver un problema particular P, la justificación construida contendrá la información relevante del problema P que hizo que el sistema CBR recuperara ese conjunto particular de casos, es decir, contendrá la información relevante que P y C1, ..., Cn tienen en común. Por ejemplo, la Figura 1 muestra una justificación construida por un sistema CBR para un problema de juguete (en las siguientes secciones mostraremos justificaciones para problemas reales). En la figura, un problema tiene dos atributos (Semáforo y Coches_pasando), el mecanismo de recuperación del sistema CBR nota que al considerar solo el atributo Semáforo, puede recuperar dos casos que predicen la misma solución: esperar. Por lo tanto, dado que solo se ha utilizado este atributo, es el único que aparece en la justificación. Los valores del resto de atributos son irrelevantes, ya que, sea cual sea su valor, la clase de solución habría sido la misma. 976 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Problema Semáforo: rojo Pasan coches: no Caso 1 Semáforo: rojo Pasan coches: no Solución: esperar Caso 3 Semáforo: rojo Pasan coches: sí Solución: esperar Caso 4 Semáforo: verde Pasan coches: sí Solución: esperar Caso 2 Semáforo: verde Pasan coches: no Solución: cruzar Casos recuperados Solución: esperar Justificación Semáforo: rojo Figura 1: Un ejemplo de generación de justificación en un sistema de RCB. Observa que, dado que la única característica relevante para decidir es Traffic_light (la única utilizada para recuperar casos), es la única que aparece en la justificación. En general, el significado de una justificación es que todos (o la mayoría) de los casos en la base de casos de un agente que satisfacen la justificación (es decir, todos los casos que están subsumidos por la justificación) pertenecen a la clase de solución predicha. En el resto del documento, usaremos para denotar la relación de subsumpción. En nuestro trabajo, utilizamos LID [2], un método CBR capaz de construir justificaciones simbólicas como la ejemplificada en la Figura 1. Cuando un agente proporciona una justificación para una predicción, el agente genera una predicción justificada: DEFINICIÓN 3.1. Una Predicción Justificada es una tupla J = A, P, S, D donde el agente A considera que S es la solución correcta para el problema P, y esa predicción está justificada por una descripción simbólica D tal que J.D J.P. Las justificaciones pueden tener muchos usos para los sistemas de RCB [8, 9]. En este documento, vamos a utilizar justificaciones como argumentos, con el fin de permitir que los agentes de aprendizaje participen en procesos de argumentación. 4. ARGUMENTOS Y CONTRAARGUMENTOS Para nuestros propósitos, un argumento α generado por un agente A está compuesto por una afirmación S y alguna evidencia D que respalda a S como correcta. En el resto de esta sección veremos cómo esta definición general de argumento puede ser concretada en un tipo específico de argumentos que los agentes pueden generar. En el contexto de los sistemas MAC, los agentes discuten sobre predicciones para nuevos problemas y pueden proporcionar dos tipos de información: a) casos específicos P, S, y b) predicciones justificadas: A, P, S, D. Usando esta información, podemos definir tres tipos de argumentos: predicciones justificadas, contraargumentos y contraejemplos. Una predicción justificada α es generada por un agente Ai para argumentar que Ai cree que la solución correcta para un problema dado P es α.S, y la evidencia proporcionada es la justificación α.D. En el ejemplo representado en la Figura 1, un agente Ai puede generar el argumento α = Ai, P, Esperar, (Semáforo = rojo), lo que significa que el agente Ai cree que la solución correcta para P es Esperar porque el atributo Semáforo es igual a rojo. Un contraargumento β es un argumento ofrecido en oposición a otro argumento α. En nuestro marco de trabajo, un contraargumento consiste en una predicción justificada Aj, P, S, D generada por un agente Aj con la intención de refutar un argumento α generado por otro agente Ai, que respalda una clase de solución S diferente a la de α.S para el problema en cuestión y justifica esto con una justificación D. En el ejemplo de la Figura 1, si un agente genera el argumento α = Ai, P, Caminar, (Pasando_autos = no), un agente que piensa que la solución correcta es Esperar podría responder con el contraargumento β = Aj, P, Esperar, (Pasando_autos = no ∧ Semáforo = rojo), lo que significa que, aunque no hay autos pasando, el semáforo está en rojo y la calle no se puede cruzar. Un contraejemplo c es un caso que contradice un argumento α. Por lo tanto, un contraejemplo también es un contraargumento, uno que afirma que un argumento específico α no siempre es verdadero, y la evidencia proporcionada es el caso c. Específicamente, para que un caso c sea un contraejemplo de un argumento α, se deben cumplir las siguientes condiciones: α.D c y α.S = c.S, es decir, el caso debe satisfacer la justificación α.D y la solución de c debe ser diferente a la predicha por α. Al intercambiar argumentos y contraargumentos (incluyendo contraejemplos), los agentes pueden discutir sobre la solución correcta de un problema dado, es decir, pueden participar en un proceso de deliberación conjunto. Sin embargo, para hacerlo, necesitan un protocolo de interacción específico, una relación de preferencia entre argumentos contradictorios y una política de decisión para generar contraargumentos (incluidos contraejemplos). En las siguientes secciones presentaremos estos elementos. 5. RELACIÓN DE PREFERENCIA Un argumento específico proporcionado por un agente podría no ser consistente con la información conocida por otros agentes (o incluso con parte de la información conocida por el agente que ha generado la justificación debido al ruido en los datos de entrenamiento). Por esa razón, vamos a definir una relación de preferencia sobre predicciones justificadas contradictorias basadas en casos. Básicamente, definiremos una medida de confianza para cada predicción justificada (que tenga en cuenta los casos poseídos por cada agente), y la predicción justificada con la mayor confianza será la preferida. La idea detrás de la confianza basada en casos es contar cuántos de los casos en una base de casos individual respaldan una predicción justificada, y cuántos de ellos son ejemplos en contra de ella. Cuanto más casos de respaldo, mayor es la confianza; y cuanto más contrajemplos, menor es la confianza. Específicamente, para evaluar la confianza de una predicción justificada α, un agente obtiene el conjunto de casos en su base de casos individuales que están subsumidos por α.D. Con ellos, un agente Ai obtiene los valores Y (sí) y N (no): • Y Ai α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| es el número de casos en la base de casos del agente subsumidos por la justificación α.D que pertenecen a la clase de solución α.S, • NAi α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| es el número de casos en la base de casos del agente subsumidos por la justificación α.D que no pertenecen a esa clase de solución. El Sexto Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 977 + + + + + +  - - + Figura 2: La confianza de los argumentos se evalúa contrastándolos con las bases de casos de los agentes. Un agente estima la confianza de un argumento como: CAi (α) = Y Ai α 1 + Y Ai α + NAi α es decir, la confianza en una predicción justificada es el número de casos que la respaldan dividido por el número de casos que la respaldan más los contraejemplos. Observa que sumamos 1 al denominador, esto es para evitar otorgar confianzas excesivamente altas a predicciones justificadas cuya confianza se ha calculado utilizando un número pequeño de casos. Ten en cuenta que esta corrección sigue la misma idea que la corrección de Laplace para estimar probabilidades. La Figura 2 ilustra la evaluación individual de la confianza de un argumento, en particular, se encuentran tres casos de respaldo y un contraejemplo en la base de casos de los agentes Ai, lo que da una confianza estimada de 0.6. Además, también podemos definir la confianza conjunta de un argumento α como la confianza calculada utilizando los casos presentes en las bases de casos de todos los agentes en el grupo: C(α) = Π Ai α / (1 + Σ Ai α + NAi α). Nótese que, para calcular conjuntamente la confianza conjunta, los agentes solo tienen que hacer públicos los valores de aprobación y rechazo calculados localmente para un argumento dado. En nuestro marco de trabajo, los agentes utilizan esta confianza conjunta como la relación de preferencia: una predicción justificada α se prefiere sobre otra β si C(α) ≥ C(β). GENERACIÓN DE ARGUMENTOS En nuestro marco de trabajo, los argumentos son generados por los agentes a partir de casos, utilizando métodos de aprendizaje. Cualquier método de aprendizaje capaz de proporcionar una predicción justificada puede ser utilizado para generar argumentos. Por ejemplo, los árboles de decisión y LID [2] son métodos de aprendizaje adecuados. Específicamente, en los experimentos reportados en este artículo, los agentes utilizan LID. Por lo tanto, cuando un agente desea generar un argumento que respalde que una clase de solución específica es la solución correcta para un problema P, genera una predicción justificada como se explica en la Sección 3.1. Por ejemplo, la Figura 3 muestra una justificación real generada por LID después de resolver un problema P en el dominio de la identificación de esponjas marinas. En particular, la Figura 3 muestra cómo cuando un agente recibe un nuevo problema para resolver (en este caso, un nuevo esponja para determinar su orden), el agente utiliza LID para generar un argumento (que consiste en una predicción justificada) utilizando los casos en la base de casos del agente. La justificación mostrada en la Figura 3 se puede interpretar diciendo que la solución predicha es hadromerida porque la forma lisa de las megascleras del esqueleto espicular de la esponja es de tipo tilostilo, el esqueleto espicular de la esponja no tiene una longitud uniforme y no hay gemas en las características externas de la esponja. Por lo tanto, el argumento generado será α = A1, P, hadromerida, D1. 6.1 Generación de Contraargumentos Como se mencionó anteriormente, los agentes pueden intentar refutar argumentos generando contraargumentos o encontrando contraejemplos. Permítanos explicar cómo pueden ser generados. Un agente Ai quiere generar un contraargumento β para rebatir un argumento α cuando α está en contradicción con la base de casos locales de Ai. Además, al generar dicho contraargumento β, Ai espera que β sea preferido sobre α. Con ese propósito, presentaremos una política específica para generar contraargumentos basados en el criterio de especificidad [10]. El criterio de especificidad se utiliza ampliamente en marcos deductivos para la argumentación, y establece que entre dos argumentos en conflicto, se debe preferir el más específico ya que, en principio, está más informado. Por lo tanto, se espera que los contraargumentos generados basados en el criterio de especificidad sean preferibles (ya que están más informados) que los argumentos que intentan refutar. Sin embargo, no hay garantía de que tales contraargumentos siempre ganen, ya que, como hemos indicado en la Sección 5, los agentes en nuestro marco utilizan una relación de preferencia basada en la confianza conjunta. Además, uno podría pensar que sería mejor que los agentes generaran contraargumentos basados en la relación de preferencia de confianza conjunta; sin embargo, no es obvio cómo generar contraargumentos basados en la confianza conjunta de manera eficiente, ya que se requiere colaboración para evaluar la confianza conjunta. Por lo tanto, el agente que genera el contraargumento debe comunicarse constantemente con los otros agentes en cada paso del algoritmo de inducción utilizado para generar contraargumentos (actualmente una de nuestras líneas de investigación futuras). Por lo tanto, en nuestro marco de trabajo, cuando un agente quiere generar un contraargumento β a un argumento α, β tiene que ser más específico que α (es decir, α.D < β.D). La generación de contraargumentos utilizando el criterio de especificidad impone algunas restricciones sobre el método de aprendizaje, aunque LID o ID3 pueden adaptarse fácilmente para esta tarea. Por ejemplo, LID es un algoritmo que genera una descripción desde cero y añade heurísticamente características a ese término. Por lo tanto, en cada paso, la descripción se vuelve más específica que en el paso anterior, y el número de casos que se incluyen en esa descripción se reduce. Cuando la descripción cubre solo (o casi solo) casos de una única clase de solución, LID termina y predice esa clase de solución. Para generar un contraargumento a un argumento α, LID solo tiene que usar como punto de partida la descripción α.D en lugar de empezar desde cero. De esta manera, la justificación proporcionada por LID siempre estará subsumida por α.D, y por lo tanto, el contraargumento resultante será más específico que α. Sin embargo, hay que tener en cuenta que LID a veces puede no ser capaz de generar contraargumentos, ya que LID puede no ser capaz de especializar aún más la descripción α.D, o porque el agente Ai no tiene ningún caso en Ci que esté subsumido por α.D. La Figura 4 muestra cómo un agente A2 que no estaba de acuerdo con el argumento mostrado en la Figura 3, genera un contraargumento utilizando LID. Además, la Figura 4 muestra la generación de un contraargumento β1 2 para el argumento α0 1 (en la Figura 3) que es una especialización de α0 1. 978 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Solución: hadromerida Justificación: D1 Esponja Esqueleto espiculado Características externas Características externas Gemmulas: no Esqueleto espiculado Megascleres Longitud uniforme: no Megascleres Forma lisa: tilostilo Caso Base de A1 LID Nueva esponja P Figura 3: Ejemplo de una justificación real generada por LID en el conjunto de datos de esponjas marinas. Específicamente, en nuestros experimentos, cuando un agente Ai quiere refutar un argumento α, utiliza la siguiente política: 1. El Agente Ai utiliza LID para intentar encontrar un contraargumento β más específico que α; si se encuentra, β se envía al otro agente como un contraargumento de α. 2. Si no se encuentra, entonces Ai busca un contraejemplo c ∈ Ci de α. Si se encuentra un caso c, entonces c se envía al otro agente como un contraejemplo de α. Si no se encuentran contraejemplos, entonces Ai no puede refutar el argumento α. 7. APRENDIZAJE MULTIAGENTE BASADO EN ARGUMENTACIÓN El protocolo de interacción de AMAL permite que un grupo de agentes A1, ..., An deliberen sobre la solución correcta de un problema P mediante un proceso de argumentación. Si el proceso de argumentación llega a una solución consensuada, la deliberación conjunta termina; de lo contrario, se utiliza una votación ponderada para determinar la solución conjunta. Además, AMAL también permite a los agentes aprender de los contraejemplos recibidos de otros agentes. El protocolo AMAL consiste en una serie de rondas. En la ronda inicial, cada agente declara cuál es su predicción individual para P. Luego, en cada ronda, un agente puede intentar refutar la predicción hecha por cualquiera de los otros agentes. El protocolo utiliza un mecanismo de paso de token para que los agentes (uno a la vez) puedan enviar contraargumentos o contraejemplos si no están de acuerdo con la predicción hecha por cualquier otro agente. Específicamente, a cada agente se le permite enviar un contraargumento o contraejemplo cada vez que recibe el token (nota que esta restricción es solo para simplificar el protocolo, y que no restringe la cantidad de contraargumentos que un agente puede enviar, ya que pueden ser retrasados para rondas posteriores). Cuando un agente recibe un contraargumento o contraejemplo, informa a los otros agentes si acepta el contraargumento (y cambia su predicción) o no. Además, los agentes también tienen la oportunidad de responder a los contraargumentos cuando reciben el token, intentando generar un contraargumento al contraargumento. Cuando todos los agentes hayan tenido el token una vez, el token regresa al primer agente, y así sucesivamente. Si en algún momento del protocolo, todos los agentes están de acuerdo o durante las últimas n rondas ningún agente ha generado ninguna contra-argumentación, el protocolo termina. Además, si al final de la argumentación los agentes no han llegado a un acuerdo, entonces se utiliza un mecanismo de votación que utiliza la confianza de cada predicción como pesos para decidir la solución final (Así, AMAL sigue el mismo mecanismo que los comités humanos, primero cada miembro individual de un comité expone sus argumentos y discute los de los otros miembros (deliberación conjunta), y si no se alcanza un consenso, entonces se requiere un mecanismo de votación). En cada iteración, los agentes pueden usar los siguientes performativos: • afirmar(α): la predicción justificada que se mantendrá durante la próxima ronda será α. Un agente solo puede tener una única predicción en cada ronda, por lo tanto, si se envían múltiples afirmaciones, solo se considera la última como la predicción actualmente mantenida. • rebut(β, α): el agente ha encontrado un contraargumento β a la predicción α. Definiremos Ht = αt 1, ..., αt n como las predicciones que cada uno de los n agentes tiene en una ronda t. Además, también definiremos contradict(αt i) = {α ∈ Ht|α.S = αt i.S} como el conjunto de argumentos contradictorios para un agente Ai en una ronda t, es decir, el conjunto de argumentos en la ronda t que respaldan una clase de solución diferente a la de αt i. El protocolo se inicia porque uno de los agentes recibe un problema P que debe ser resuelto. Después de eso, el agente informa a todos los demás agentes sobre el problema P a resolver, y el protocolo comienza: 1. En la ronda t = 0, cada uno de los agentes resuelve individualmente P y construye una predicción justificada utilizando su propio método de RBC. Entonces, cada agente Ai envía el assertivo performativo assert(α0 i) a los otros agentes. Por lo tanto, los agentes conocen H0 = α0 i , ..., α0 n. Una vez que todas las predicciones hayan sido enviadas, el token se le da al primer agente A1. 2. En cada ronda t (distinta de 0), los agentes verifican si sus argumentos en Ht coinciden. Si lo hacen, el protocolo avanza al paso 5. Además, si durante las últimas n rondas ningún agente ha enviado ningún contraejemplo o contraargumento, el protocolo también avanza al paso 5. De lo contrario, el agente Ai propietario del token intenta generar un contraargumento para cada uno de los argumentos opuestos en contradict(αt i) ⊆ Ht (ver Sección 6.1). Entonces, se selecciona el contraargumento βt i contra la predicción αt j con la menor confianza C(αt j) (ya que αt j es la predicción más probable de ser refutada con éxito). • Si βt i es un contraargumento, entonces, Ai compara localmente αt i con βt i evaluando su confianza contra su base de casos individuales Ci (ver Sección 5) (nótese que Ai está comparando su argumento previo con el contraargumento que Ai mismo acaba de generar y que trata sobre The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 979 Esponja Esqueleto espiculado Características externas Características externas Gemas: no Crecimiento: Esqueleto espiculado Megascleres Longitud uniforme: no Megascleres Forma lisa: tilostilo Crecimiento Crecimiento: masivo Caso Base de A2 LID Solución: astrophorida Justificación: D2 Figura 4: Generación de un contraargumento utilizando LID en el conjunto de datos de esponjas. para enviar a Aj. Si CAi (βt i) > CAi (αt i), entonces Ai considera que βt i es más fuerte que su argumento anterior, cambia su argumento a βt i enviando assert(βt i) al resto de los agentes (la intuición detrás de esto es que dado que un contraargumento también es un argumento, Ai verifica si el nuevo contraargumento es un mejor argumento que el que sostenía anteriormente) y rebut(βt i, αt j) a Aj. De lo contrario (es decir, Si CAi (βt i ) ≤ CAi (αt i), Ai solo enviará rebut(βt i , αt j) a Aj. En cualquiera de las dos situaciones, el protocolo avanza al paso 3. • Si βt i es un contraejemplo c, entonces Ai envía una refutación(c, αt j) a Aj. El protocolo avanza al paso 4. • Si Ai no puede generar ningún contraargumento o contraejemplo, el token se envía al siguiente agente, comienza una nueva ronda t + 1, y el protocolo avanza al estado 2. 3. El agente Aj que ha recibido el contraargumento βt i, lo compara localmente con su propio argumento, αt j, evaluando localmente su confianza. Si CAj (βt i ) > CAj (αt j), entonces Aj aceptará el contraargumento como más fuerte que su propio argumento, y enviará assert(βt i ) a los otros agentes. De lo contrario (es decir, Si CAj (βt i ) ≤ CAj (αt j)), Aj no aceptará el contraargumento e informará a los otros agentes en consecuencia. Cualquiera de las dos situaciones inicia una nueva ronda t + 1, Ai envía el token al siguiente agente y el protocolo vuelve al estado 2. 4. El agente Aj que ha recibido el contraejemplo c lo retiene en su base de casos y genera un nuevo argumento αt+1 j que tiene en cuenta c, e informa al resto de los agentes enviando assert(αt+1 j) a todos ellos. Entonces, Ai envía el token al siguiente agente, comienza una nueva ronda t + 1 y el protocolo vuelve al paso 2. 5. El protocolo finaliza generando una predicción conjunta, de la siguiente manera: si los argumentos en Ht están de acuerdo, entonces su predicción es la predicción conjunta; de lo contrario, se utiliza un mecanismo de votación para decidir la predicción conjunta. El mecanismo de votación utiliza la medida de confianza conjunta como los pesos de votación, de la siguiente manera: S = arg max Sk∈S αi∈Ht|αi.S=Sk C(αi) Además, para evitar iteraciones infinitas, si un agente envía dos veces el mismo argumento o contraargumento al mismo agente, el mensaje no se considera. 8. EJEMPLIFICACIÓN Consideremos un sistema compuesto por tres agentes A1, A2 y A3. Uno de los agentes, A1, recibe un problema P para resolver y decide usar AMAL para resolverlo. Por esa razón, invita a A2 y A3 a participar en el proceso de argumentación. Aceptan la invitación y comienza el protocolo de argumentación. Inicialmente, cada agente genera su predicción individual para P y la transmite a los otros agentes. Por lo tanto, todos ellos pueden calcular H0 = α0 1, α0 2, α0 3. En particular, en este ejemplo: • α0 1 = A1, P, hadromerida, D1 • α0 2 = A2, P, astrophorida, D2 • α0 3 = A3, P, axinellida, D3 A1 comienza poseyendo el token e intenta generar argumentos en contra de α0 2 y α0 3, pero no tiene éxito; sin embargo, tiene un contraejemplo c13 para α0 3. Por lo tanto, A1 envía el mensaje rebut(c13, α03) a A3. A3 incorpora c13 en su base de casos e intenta resolver el problema P nuevamente, ahora teniendo en cuenta c13. A3 elabora la predicción justificada α1 3 = A3, P, hadromerida, D4, y la transmite al resto de los agentes con el mensaje assert(α1 3). Por lo tanto, todos ellos conocen el nuevo H1 = α0 1, α0 2, α1 3. La ronda 1 comienza y A2 obtiene el token. A2 intenta generar contraargumentos para α0 1 y α1 3 y solo logra generar un contraargumento β1 2 = A2, P, astrophorida, D5 contra α1 3. El contraargumento se envía a A3 con el mensaje rebut(β1 2 , α1 3). El agente A3 recibe el contraargumento y evalúa su confianza local. El resultado es que la confianza individual del contraargumento β1 2 es menor que la confianza local de α1 3. Por lo tanto, A3 no acepta el contraargumento, y así H2 = α0 1, α0 2, α1 3. La ronda 2 comienza y A3 recibe el token. A3 genera un contraargumento β2 3 = A3, P, hadromerida, D6 para α0 2 y lo envía a A2 con el mensaje refutar(β2 3 , α0 2). El agente A2 recibe el contraargumento y evalúa su confianza local. El resultado es que la confianza local del contraargumento β2 3 es mayor que la confianza local de α0 2. Por lo tanto, A2 acepta el contraargumento e informa al resto de los agentes con el mensaje assert(β2 3). Después de eso, H3 = α0 1, β2 3 , α1 3. En la Ronda 3, dado que todos los agentes están de acuerdo (todas las predicciones justificadas en H3 predicen hadromerida como la clase de solución), el protocolo finaliza, y A1 (el agente que recibió el problema) considera hadromerida como la solución conjunta para el problema P. 9. EVALUACIÓN EXPERIMENTAL 980 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) SPONGE 75 77 79 81 83 85 87 89 91 2 3 4 5 AMAL Votación Individual SOYBEAN 55 60 65 70 75 80 85 90 2 3 4 5 AMAL Votación Individual Figura 5: Precisión individual y conjunta para 2 a 5 agentes. En esta sección evaluamos empíricamente el marco de argumentación AMAL. Hemos realizado experimentos en dos conjuntos de datos diferentes: soja (del repositorio de aprendizaje automático de UCI) y esponja (un conjunto de datos relacional). El conjunto de datos de soja tiene 307 ejemplos y 19 clases de solución, mientras que el conjunto de datos de esponja tiene 280 ejemplos y 3 clases de solución. En una ejecución experimental, el conjunto de datos se divide en 2 conjuntos: el conjunto de entrenamiento y el conjunto de prueba. Los ejemplos del conjunto de entrenamiento están distribuidos entre 5 agentes diferentes sin replicación, es decir, no hay ningún ejemplo compartido por dos agentes. En la etapa de pruebas, los problemas en el conjunto de pruebas llegan aleatoriamente a uno de los agentes, y su objetivo es predecir la solución correcta. Los experimentos están diseñados para probar dos hipótesis: (H1) que la argumentación es un marco útil para la deliberación conjunta y puede mejorar sobre otros métodos típicos como la votación; y (H2) que el aprendizaje a través de la comunicación mejora el rendimiento individual de un agente de aprendizaje que participa en un proceso de argumentación. Además, también esperamos que la mejora lograda a través de la argumentación aumente a medida que aumente el número de agentes que participan en la argumentación (ya que se tomará en cuenta más información). En relación con H1 (la argumentación es un marco útil para la deliberación conjunta), realizamos 4 experimentos, utilizando 2, 3, 4 y 5 agentes respectivamente (en todos los experimentos cada agente tiene un 20% de los datos de entrenamiento, ya que el entrenamiento siempre se distribuye entre 5 agentes). La Figura 5 muestra el resultado de esos experimentos en los conjuntos de datos de esponjas y soja. La precisión de la clasificación se representa en el eje vertical, y en el eje horizontal se muestra el número de agentes que participaron en los procesos de argumentación. Para cada número de agentes, se muestran tres barras: individual, Voting y AMAL. La barra individual muestra la precisión promedio de las predicciones de los agentes individuales; la barra de votación muestra la precisión promedio de la predicción conjunta lograda mediante votación pero sin argumentación; y finalmente la barra de AMAL muestra la precisión promedio de la predicción conjunta utilizando argumentación. Los resultados mostrados son el promedio de 5 ejecuciones de validación cruzada de 10 pliegues. La Figura 5 muestra que la colaboración (votación y AMAL) supera el rendimiento de la resolución individual de problemas. Además, como esperábamos, la precisión mejora a medida que más agentes colaboran, ya que se tiene en cuenta más información. También podemos ver que AMAL siempre supera al voto estándar, demostrando que las decisiones conjuntas se basan en una mejor información proporcionada por el proceso de argumentación. Por ejemplo, la precisión conjunta para 2 agentes en el conjunto de datos de esponjas es del 87.57% para AMAL y del 86.57% para la votación (mientras que la precisión individual es solo del 80.07%). Además, la mejora lograda por AMAL sobre Voting es aún mayor en el conjunto de datos de soja. La razón es que el conjunto de datos de soja es más difícil (en el sentido de que los agentes necesitan más datos para producir buenas predicciones). Estos resultados experimentales muestran que AMAL aprovecha de manera efectiva la oportunidad de mejora: la precisión es mayor solo porque más agentes han cambiado su opinión durante la argumentación (de lo contrario, lograrían el mismo resultado que la Votación). En relación con H2 (el aprendizaje a partir de la comunicación en procesos de argumentación mejora la predicción individual), llevamos a cabo el siguiente experimento: inicialmente, distribuimos un 25% del conjunto de entrenamiento entre los cinco agentes; después, el resto de los casos en el conjunto de entrenamiento se envía a los agentes uno por uno; cuando un agente recibe un nuevo caso de entrenamiento, tiene varias opciones: el agente puede descartarlo, el agente puede retenerlo, o el agente puede usarlo para participar en un proceso de argumentación. La Figura 6 muestra el resultado de ese experimento para los dos conjuntos de datos. La Figura 6 contiene tres gráficos, donde NL (no aprendizaje) muestra la precisión de un agente sin aprendizaje alguno; L (aprendizaje) muestra la evolución de la precisión de clasificación individual cuando los agentes aprenden reteniendo los casos de entrenamiento que reciben individualmente (notar que cuando todos los casos de entrenamiento han sido retenidos al 100%, la precisión debería ser igual a la de la Figura 5 para los agentes individuales); y finalmente LFC (aprendizaje a partir de la comunicación) muestra la evolución de la precisión de clasificación individual de los agentes de aprendizaje que también aprenden reteniendo los contraejemplos recibidos durante la argumentación (es decir, aprenden tanto de ejemplos de entrenamiento como de contraejemplos). La Figura 6 muestra que si un agente Ai también aprende de la comunicación, Ai puede mejorar significativamente su rendimiento individual con solo un pequeño número de casos adicionales (seleccionados como contraejemplos relevantes para Ai durante la argumentación). Por ejemplo, en el conjunto de datos de soja, los agentes individuales han logrado una precisión del 70.62% cuando también aprenden de la comunicación, en comparación con una precisión del 59.93% cuando solo aprenden de su experiencia individual. El número de casos aprendidos a través de la comunicación depende de las propiedades del conjunto de datos: en el conjunto de datos de esponjas, los agentes solo han retenido muy pocos casos adicionales y han mejorado significativamente la precisión individual; es decir, retienen en promedio 59.96 casos (en comparación con los 50.4 casos retenidos si no aprenden de la comunicación). En el conjunto de datos de soja se aprenden más contraejemplos para mejorar significativamente la precisión individual, es decir, retienen en promedio 87.16 casos (en comparación con 55.27 casos retenidos si no aprenden de la comunicación). Finalmente, el hecho de que ambos conjuntos de datos muestren una mejora significativa señala la naturaleza adaptativa del enfoque basado en la argumentación para el aprendizaje a partir de la comunicación: los casos útiles se seleccionan como contraejemplos (y no más de los necesarios), y tienen el efecto deseado. 10. TRABAJO RELACIONADO En cuanto al RCB en un entorno multiagente, la primera investigación se centró en la recuperación de casos negociada [11] entre grupos de agentes. Nuestro trabajo sobre el aprendizaje basado en casos multiagente comenzó en 1999; más tarde Mc Ginty y Smyth presentaron un enfoque colaborativo de CBR multiagente (CCBR) para la planificación. Finalmente, otro enfoque interesante es el razonamiento de múltiples bases de casos (MCBR) [5], que trata sobre The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 981 ESPONJA 60 65 70 75 80 85 25% 40% 55% 70% 85% 100% LFC L NL SOJA 20 30 40 50 60 70 80 90 25% 40% 55% 70% 85% 100% LFC L NL Figura 6: Aprendizaje a partir de la comunicación resultante de la argumentación en un sistema compuesto por 5 agentes. sistemas distribuidos donde hay varias bases de casos disponibles para la misma tarea y aborda los problemas de adaptación entre bases de casos. La principal diferencia es que nuestro enfoque MAC es una forma de distribuir el proceso de Reutilización de CBR (utilizando un sistema de votación) mientras que la Recuperación se realiza individualmente por cada agente; sin embargo, los otros enfoques de CBR multiagente se centran en distribuir el proceso de Recuperación. La investigación sobre la argumentación en MAS se centra en varios temas como a) lógicas, protocolos y lenguajes que respaldan la argumentación, b) selección de argumentos y c) interpretación de argumentos. Los enfoques para la lógica y los lenguajes que respaldan la argumentación incluyen la lógica no definitiva [4] y los modelos BDI [13]. Aunque la selección de argumentos es un aspecto clave de la argumentación automatizada (ver [12] y [13]), la mayoría de las investigaciones se han centrado en las relaciones de preferencia entre argumentos. En nuestro marco de trabajo hemos abordado tanto la selección de argumentos como las relaciones de preferencia utilizando un enfoque basado en casos. 11. CONCLUSIONES Y TRABAJO FUTURO En este artículo hemos presentado un marco basado en argumentación para el aprendizaje multiagente. Específicamente, hemos presentado AMAL, un marco que permite a un grupo de agentes de aprendizaje discutir sobre la solución de un problema dado y hemos demostrado cómo las capacidades de aprendizaje pueden ser utilizadas para generar argumentos y contraargumentos. La evaluación experimental muestra que la cantidad aumentada de información proporcionada a los agentes por el proceso de argumentación aumenta su precisión predictiva, especialmente cuando un número adecuado de agentes participa en la argumentación. Las principales contribuciones de este trabajo son: a) un marco de argumentación para agentes de aprendizaje; b) una relación de preferencia basada en casos sobre argumentos, basada en el cálculo de una estimación general de confianza de los argumentos; c) una <br>política basada en casos</br> para generar contraargumentos y seleccionar contraejemplos; y d) un enfoque basado en la argumentación para aprender a partir de la comunicación. Finalmente, en los experimentos presentados aquí, un agente de aprendizaje retendría todos los contraejemplos presentados por el otro agente; sin embargo, esta es una política de retención de casos muy simple, y nos gustaría experimentar con políticas más informadas, con el objetivo de que los agentes de aprendizaje individuales puedan mejorar significativamente utilizando solo un pequeño conjunto de casos propuestos por otros agentes. Finalmente, nuestro enfoque se centra en el aprendizaje perezoso, y los trabajos futuros tienen como objetivo incorporar el aprendizaje inductivo ansioso dentro del marco argumentativo para el aprendizaje a partir de la comunicación. 12. REFERENCIAS [1] Agnar Aamodt y Enric Plaza. Razonamiento basado en casos: Aspectos fundamentales, variaciones metodológicas y enfoques de sistemas. Comunicaciones de Inteligencia Artificial, 7(1):39-59, 1994. [2] E. Armengol y E. Plaza. Inducción perezosa de descripciones para el aprendizaje basado en casos relacionales. En ECML2001, páginas 13-24, 2001. [3] Gerhard Brewka. Sistemas de argumentación dinámica: Un modelo formal de procesos de argumentación basado en el cálculo de situaciones. Revista de Lógica y Computación, 11(2):257-282, 2001. [4] Carlos I. Chesñevar y Guillermo R. Simari. Formalizando la argumentación no definitiva utilizando sistemas deductivos etiquetados. Revista de Ciencias de la Computación y Tecnología, 1(4):18-33, 2000. [5] D. Leake y R. Sooriamurthi. Seleccionando estrategias automáticamente para el razonamiento multi-base de casos. En S. Craw y A. Preece, editores, ECCBR2002, páginas 204-219, Berlín, 2002. Springer Verlag. [6] Francisco J. Martín, Enric Plaza y Josep-Lluis Arcos. Reutilización de conocimiento y experiencia a través de comunicaciones entre agentes competentes (pares). Revista Internacional de Ingeniería de Software e Ingeniería del Conocimiento, 9(3):319-341, 1999. [7] Lorraine McGinty y Barry Smyth. Razonamiento basado en casos colaborativo: Aplicaciones en la planificación de rutas personalizadas. En I. Watson y Q. Yang, editores, ICCBR, número 2080 en LNAI, páginas 362-376. Springer-Verlag, 2001. [8] Santi Ontañón y Enric Plaza. Aprendizaje multiagente basado en justificaciones. En ICML2003, páginas 576-583. Morgan Kaufmann, 2003. [9] Enric Plaza, Eva Armengol y Santiago Ontañón. El poder explicativo de la similitud simbólica en el razonamiento basado en casos. Revisión de Inteligencia Artificial, 24(2):145-161, 2005. [10] David Poole. En la comparación de teorías: prefiriendo la explicación más específica. En IJCAI-85, páginas 144-147, 1985. [11] M V Nagendra Prassad, Victor R Lesser y Susan Lander. Recuperación y razonamiento en bases de casos distribuidas. Informe técnico, Departamento de Ciencias de la Computación de UMass, 1995. [12] K. Sycara S. Kraus y A. Evenchik. Alcanzando acuerdos a través de la argumentación: un modelo lógico y su implementación. Revista de Inteligencia Artificial, 104:1-69, 1998. [13] N. R. Jennings, S. Parsons, C. Sierra. Agentes que razonan y negocian mediante argumentos. Revista de Lógica y Computación, 8:261-292, 1998. [14] Bruce A. Wooley. Componente de explicación de sistemas de software. ACM CrossRoads, 5.1, 1998. 982 The Sixth Intl. \n\nACM CrossRoads, 5.1, 1998. 982 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "multi-agent learn": {
            "translated_key": "aprendizaje multiagente",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning and Joint Deliberation through Argumentation in Multi-Agent Systems Santi Ontañón CCL, Cognitive Computing Lab Georgia Institute of Technology Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Artificial Intelligence Research Institute CSIC, Spanish Council for Scientific Research Campus UAB, 08193 Bellaterra, Catalonia (Spain) enric@iiia.csic.es ABSTRACT In this paper we will present an argumentation framework for learning agents (AMAL) designed for two purposes: (1) for joint deliberation, and (2) for learning from communication.",
                "The AMAL framework is completely based on learning from examples: the argument preference relation, the argument generation policy, and the counterargument generation policy are case-based techniques.",
                "For join deliberation, learning agents share their experience by forming a committee to decide upon some joint decision.",
                "We experimentally show that the argumentation among committees of agents improves both the individual and joint performance.",
                "For learning from communication, an agent engages into arguing with other agents in order to contrast its individual hypotheses and receive counterexamples; the argumentation process improves their learning scope and individual performance.",
                "Categories and Subject Descriptors I.2.6 [Artificial Intelligence]: Learning; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems, Intelligent Agents 1.",
                "INTRODUCTION Argumentation frameworks for multi-agent systems can be used for different purposes like joint deliberation, persuasion, negotiation, and conflict resolution.",
                "In this paper we will present an argumentation framework for learning agents, and show that it can be used for two purposes: (1) joint deliberation, and (2) learning from communication.",
                "Argumentation-based joint deliberation involves discussion over the outcome of a particular situation or the appropriate course of action for a particular situation.",
                "Learning agents are capable of learning from experience, in the sense that past examples (situations and their outcomes) are used to predict the outcome for the situation at hand.",
                "However, since individual agents experience may be limited, individual knowledge and prediction accuracy is also limited.",
                "Thus, learning agents that are capable of arguing their individual predictions with other agents may reach better prediction accuracy after such an argumentation process.",
                "Most existing argumentation frameworks for multi-agent systems are based on deductive logic or some other deductive logic formalism specifically designed to support argumentation, such as default logic [3]).",
                "Usually, an argument is seen as a logical statement, while a counterargument is an argument offered in opposition to another argument [4, 13]; agents use a preference relation to resolve conflicting arguments.",
                "However, logic-based argumentation frameworks assume agents with preloaded knowledge and preference relation.",
                "In this paper, we focus on an Argumentation-based Multi-Agent Learning (AMAL) framework where both knowledge and preference relation are learned from experience.",
                "Thus, we consider a scenario with agents that (1) work in the same domain using a shared ontology, (2) are capable of learning from examples, and (3) communicate using an argumentative framework.",
                "Having learning capabilities allows agents effectively use a specific form of counterargument, namely the use of counterexamples.",
                "Counterexamples offer the possibility of agents learning during the argumentation process.",
                "Moreover, learning agents allow techniques that use learnt experience to generate adequate arguments and counterarguments.",
                "Specifically, we will need to address two issues: (1) how to define a technique to generate arguments and counterarguments from examples, and (2)how to define a preference relation over two conflicting arguments that have been induced from examples.",
                "This paper presents a case-based approach to address both issues.",
                "The agents use case-based reasoning (CBR) [1] to learn from past cases (where a case is a situation and its outcome) in order to predict the outcome of a new situation.",
                "We propose an argumentation protocol inside the AMAL framework at supports agents in reaching a joint prediction over a specific situation or problem - moreover, the reasoning needed to support the argumentation process will also be based on cases.",
                "In particular, we present two case-based measures, one for generating the arguments and counterarguments adequate to a particular situation and another for determining preference relation among arguments.",
                "Finally, we evaluate (1) if argumentation between learning agents can produce a joint prediction that improves over individual learning performance and (2) if learning from the counterexamples conveyed during the argumentation process increases the individual performance with precisely those cases being used while arguing among them.",
                "The paper is structured as follows.",
                "Section 2 discusses the relation among argumentation, collaboration and learning.",
                "Then Section 3 introduces our multi-agent CBR (MAC) framework and the notion of justified prediction.",
                "After that, Section 4 formally defines our argumentation framework.",
                "Sections 5 and 6 present our case-based preference relation and argument generation policies respectively.",
                "Later, Section 7 presents the argumentation protocol in our AMAL framework.",
                "After that, Section 8 presents an exemplification of the argumentation framework.",
                "Finally, Section 9 presents an empirical evaluation of our two main hypotheses.",
                "The paper closes with related work and conclusions sections. 2.",
                "ARGUMENTATION,COLLABORATION AND LEARNING Both learning and collaboration are ways in which an agent can improve individual performance.",
                "In fact, there is a clear parallelism between learning and collaboration in multi-agent systems, since both are ways in which agents can deal with their shortcomings.",
                "Let us show which are the main motivations that an agent can have to learn or to collaborate. • Motivations to learn: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems. • Motivations to collaborate: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems, - Increase the range of accessible resources.",
                "Looking at the above lists of motivation, we can easily see that learning and collaboration are very related in multi-agent systems.",
                "In fact, with the exception of the last item in the motivations to collaborate list, they are two extremes of a continuum of strategies to improve performance.",
                "An agent may choose to increase performance by learning, by collaborating, or by finding an intermediate point that combines learning and collaboration in order to improve performance.",
                "In this paper we will propose AMAL, an argumentation framework for learning agents, and will also also show how AMAL can be used both for learning from communication and for solving problems in a collaborative way: • Agents can solve problems in a collaborative way via engaging an argumentation process about the prediction for the situation at hand.",
                "Using this collaboration, the prediction can be done in a more informed way, since the information known by several agents has been taken into account. • Agents can also learn from communication with other agents by engaging an argumentation process.",
                "Agents that engage in such argumentation processes can learn from the arguments and counterexamples received from other agents, and use this information for predicting the outcomes of future situations.",
                "In the rest of this paper we will propose an argumentation framework and show how it can be used both for learning and for solving problems in a collaborative way. 3.",
                "MULTI-AGENT CBR SYSTEMS A Multi-Agent Case Based Reasoning System (MAC) M = {(A1, C1), ..., (An, Cn)} is a multi-agent system composed of A = {Ai, ..., An}, a set of CBR agents, where each agent Ai ∈ A possesses an individual case base Ci.",
                "Each individual agent Ai in a MAC is completely autonomous and each agent Ai has access only to its individual and private case base Ci.",
                "A case base Ci = {c1, ..., cm} is a collection of cases.",
                "Agents in a MAC system are able to individually solve problems, but they can also collaborate with other agents to solve problems.",
                "In this framework, we will restrict ourselves to analytical tasks, i.e. tasks like classification, where the solution of a problem is achieved by selecting a solution class from an enumerated set of solution classes.",
                "In the following we will note the set of all the solution classes by S = {S1, ..., SK }.",
                "Therefore, a case c = P, S is a tuple containing a case description P and a solution class S ∈ S. In the following, we will use the terms problem and case description indistinctly.",
                "Moreover, we will use the dot notation to refer to elements inside a tuple; e.g., to refer to the solution class of a case c, we will write c.S.",
                "Therefore, we say a group of agents perform joint deliberation, when they collaborate to find a joint solution by means of an argumentation process.",
                "However, in order to do so, an agent has to be able to justify its prediction to the other agents (i.e. generate an argument for its predicted solution that can be examined and critiqued by the other agents).",
                "The next section addresses this issue. 3.1 Justified Predictions Both expert systems and CBR systems may have an explanation component [14] in charge of justifying why the system has provided a specific answer to the user.",
                "The line of reasoning of the system can then be examined by a human expert, thus increasing the reliability of the system.",
                "Most of the existing work on explanation generation focuses on generating explanations to be provided to the user.",
                "However, in our approach we use explanations (or justifications) as a tool for improving communication and coordination among agents.",
                "We are interested in justifications since they can be used as arguments.",
                "For that purpose, we will benefit from the ability of some machine learning methods to provide justifications.",
                "A justification built by a CBR method after determining that the solution of a particular problem P was Sk is a description that contains the relevant information from the problem P that the CBR method has considered to predict Sk as the solution of P. In particular, CBR methods work by retrieving similar cases to the problem at hand, and then reusing their solutions for the current problem, expecting that since the problem and the cases are similar, the solutions will also be similar.",
                "Thus, if a CBR method has retrieved a set of cases C1, ..., Cn to solve a particular problem P the justification built will contain the relevant information from the problem P that made the CBR system retrieve that particular set of cases, i.e. it will contain the relevant information that P and C1, ..., Cn have in common.",
                "For example, Figure 1 shows a justification build by a CBR system for a toy problem (in the following sections we will show justifications for real problems).",
                "In the figure, a problem has two attributes (Traffic_light, and Cars_passing), the retrieval mechanism of the CBR system notices that by considering only the attribute Traffic_light, it can retrieve two cases that predict the same solution: wait.",
                "Thus, since only this attribute has been used, it is the only one appearing in the justification.",
                "The values of the rest of attributes are irrelevant, since whatever their value the solution class would have been the same. 976 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Problem Traffic_light: red Cars_passing: no Case 1 Traffic_light: red Cars_passing: no Solution: wait Case 3 Traffic_light: red Cars_passing: yes Solution: wait Case 4 Traffic_light: green Cars_passing: yes Solution: wait Case 2 Traffic_light: green Cars_passing: no Solution: cross Retrieved cases Solution: wait Justification Traffic_light: red Figure 1: An example of justification generation in a CBR system.",
                "Notice that, since the only relevant feature to decide is Traffic_light (the only one used to retrieve cases), it is the only one appearing in the justification.",
                "In general, the meaning of a justification is that all (or most of) the cases in the case base of an agent that satisfy the justification (i.e. all the cases that are subsumed by the justification) belong to the predicted solution class.",
                "In the rest of the paper, we will use to denote the subsumption relation.",
                "In our work, we use LID [2], a CBR method capable of building symbolic justifications such as the one exemplified in Figure 1.",
                "When an agent provides a justification for a prediction, the agent generates a justified prediction: DEFINITION 3.1.",
                "A Justified Prediction is a tuple J = A, P, S, D where agent A considers S the correct solution for problem P, and that prediction is justified a symbolic description D such that J.D J.P. Justifications can have many uses for CBR systems [8, 9].",
                "In this paper, we are going to use justifications as arguments, in order to allow learning agents to engage in argumentation processes. 4.",
                "ARGUMENTS AND COUNTERARGUMENTS For our purposes an argument α generated by an agent A is composed of a statement S and some evidence D supporting S as correct.",
                "In the remainder of this section we will see how this general definition of argument can be instantiated in specific kind of arguments that the agents can generate.",
                "In the context of MAC systems, agents argue about predictions for new problems and can provide two kinds of information: a) specific cases P, S , and b) justified predictions: A, P, S, D .",
                "Using this information, we can define three types of arguments: justified predictions, counterarguments, and counterexamples.",
                "A justified prediction α is generated by an agent Ai to argue that Ai believes that the correct solution for a given problem P is α.S, and the evidence provided is the justification α.D.",
                "In the example depicted in Figure 1, an agent Ai may generate the argument α = Ai, P, Wait, (Traffic_light = red) , meaning that the agent Ai believes that the correct solution for P is Wait because the attribute Traffic_light equals red.",
                "A counterargument β is an argument offered in opposition to another argument α.",
                "In our framework, a counterargument consists of a justified prediction Aj, P, S , D generated by an agent Aj with the intention to rebut an argument α generated by another agent Ai, that endorses a solution class S different from that of α.S for the problem at hand and justifies this with a justification D .",
                "In the example in Figure 1, if an agent generates the argument α = Ai, P, Walk, (Cars_passing = no) , an agent that thinks that the correct solution is Wait might answer with the counterargument β = Aj, P, Wait, (Cars_passing = no ∧ Traffic_light = red) , meaning that, although there are no cars passing, the traffic light is red, and the street cannot be crossed.",
                "A counterexample c is a case that contradicts an argument α.",
                "Thus a counterexample is also a counterargument, one that states that a specific argument α is not always true, and the evidence provided is the case c. Specifically, for a case c to be a counterexample of an argument α, the following conditions have to be met: α.D c and α.S = c.S, i.e. the case must satisfy the justification α.D and the solution of c must be different than the predicted by α.",
                "By exchanging arguments and counterarguments (including counterexamples), agents can argue about the correct solution of a given problem, i.e. they can engage a joint deliberation process.",
                "However, in order to do so, they need a specific interaction protocol, a preference relation between contradicting arguments, and a decision policy to generate counterarguments (including counterexamples).",
                "In the following sections we will present these elements. 5.",
                "PREFERENCE RELATION A specific argument provided by an agent might not be consistent with the information known to other agents (or even to some of the information known by the agent that has generated the justification due to noise in training data).",
                "For that reason, we are going to define a preference relation over contradicting justified predictions based on cases.",
                "Basically, we will define a confidence measure for each justified prediction (that takes into account the cases owned by each agent), and the justified prediction with the highest confidence will be the preferred one.",
                "The idea behind case-based confidence is to count how many of the cases in an individual case base endorse a justified prediction, and how many of them are counterexamples of it.",
                "The more the endorsing cases, the higher the confidence; and the more the counterexamples, the lower the confidence.",
                "Specifically, to assess the confidence of a justified prediction α, an agent obtains the set of cases in its individual case base that are subsumed by α.D.",
                "With them, an agent Ai obtains the Y (aye) and N (nay) values: • Y Ai α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by the justification α.D that belong to the solution class α.S, • NAi α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by justification α.D that do not belong to that solution class.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 977 + + + + + +  - - + Figure 2: Confidence of arguments is evaluated by contrasting them against the case bases of the agents.",
                "An agent estimates the confidence of an argument as: CAi (α) = Y Ai α 1 + Y Ai α + NAi α i.e. the confidence on a justified prediction is the number of endorsing cases divided by the number of endorsing cases plus counterexamples.",
                "Notice that we add 1 to the denominator, this is to avoid giving excessively high confidences to justified predictions whose confidence has been computed using a small number of cases.",
                "Notice that this correction follows the same idea than the Laplace correction to estimate probabilities.",
                "Figure 2 illustrates the individual evaluation of the confidence of an argument, in particular, three endorsing cases and one counterexample are found in the case base of agents Ai, giving an estimated confidence of 0.6 Moreover, we can also define the joint confidence of an argument α as the confidence computed using the cases present in the case bases of all the agents in the group: C(α) = i Y Ai α 1 + i Y Ai α + NAi α Notice that, to collaboratively compute the joint confidence, the agents only have to make public the aye and nay values locally computed for a given argument.",
                "In our framework, agents use this joint confidence as the preference relation: a justified prediction α is preferred over another one β if C(α) ≥ C(β). 6.",
                "GENERATION OF ARGUMENTS In our framework, arguments are generated by the agents from cases, using learning methods.",
                "Any learning method able to provide a justified prediction can be used to generate arguments.",
                "For instance, decision trees and LID [2] are suitable learning methods.",
                "Specifically, in the experiments reported in this paper agents use LID.",
                "Thus, when an agent wants to generate an argument endorsing that a specific solution class is the correct solution for a problem P, it generates a justified prediction as explained in Section 3.1.",
                "For instance, Figure 3 shows a real justification generated by LID after solving a problem P in the domain of marine sponges identification.",
                "In particular, Figure 3 shows how when an agent receives a new problem to solve (in this case, a new sponge to determine its order), the agent uses LID to generate an argument (consisting on a justified prediction) using the cases in the case base of the agent.",
                "The justification shown in Figure 3 can be interpreted saying that the predicted solution is hadromerida because the smooth form of the megascleres of the spiculate skeleton of the sponge is of type tylostyle, the spikulate skeleton of the sponge has no uniform length, and there is no gemmules in the external features of the sponge.",
                "Thus, the argument generated will be α = A1, P, hadromerida, D1 . 6.1 Generation of Counterarguments As previously stated, agents may try to rebut arguments by generating counterargument or by finding counterexamples.",
                "Let us explain how they can be generated.",
                "An agent Ai wants to generate a counterargument β to rebut an argument α when α is in contradiction with the local case base of Ai.",
                "Moreover, while generating such counterargument β, Ai expects that β is preferred over α.",
                "For that purpose, we will present a specific policy to generate counterarguments based on the specificity criterion [10].",
                "The specificity criterion is widely used in deductive frameworks for argumentation, and states that between two conflicting arguments, the most specific should be preferred since it is, in principle, more informed.",
                "Thus, counterarguments generated based on the specificity criterion are expected to be preferable (since they are more informed) to the arguments they try to rebut.",
                "However, there is no guarantee that such counterarguments will always win, since, as we have stated in Section 5, agents in our framework use a preference relation based on joint confidence.",
                "Moreover, one may think that it would be better that the agents generate counterarguments based on the joint confidence preference relation; however it is not obvious how to generate counterarguments based on joint confidence in an efficient way, since collaboration is required in order to evaluate joint confidence.",
                "Thus, the agent generating the counterargument should constantly communicate with the other agents at each step of the induction algorithm used to generate counterarguments (presently one of our future research lines).",
                "Thus, in our framework, when an agent wants to generate a counterargument β to an argument α, β has to be more specific than α (i.e. α.D < β.D).",
                "The generation of counterarguments using the specificity criterion imposes some restrictions over the learning method, although LID or ID3 can be easily adapted for this task.",
                "For instance, LID is an algorithm that generates a description starting from scratch and heuristically adding features to that term.",
                "Thus, at every step, the description is made more specific than in the previous step, and the number of cases that are subsumed by that description is reduced.",
                "When the description covers only (or almost only) cases of a single solution class LID terminates and predicts that solution class.",
                "To generate a counterargument to an argument α LID just has to use as starting point the description α.D instead of starting from scratch.",
                "In this way, the justification provided by LID will always be subsumed by α.D, and thus the resulting counterargument will be more specific than α.",
                "However, notice that LID may sometimes not be able to generate counterarguments, since LID may not be able to specialize the description α.D any further, or because the agent Ai has no case inCi that is subsumed by α.D.",
                "Figure 4 shows how an agent A2 that disagreed with the argument shown in Figure 3, generates a counterargument using LID.",
                "Moreover, Figure 4 shows the generation of a counterargument β1 2 for the argument α0 1 (in Figure 3) that is a specialization of α0 1. 978 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Solution: hadromerida Justification: D1 Sponge Spikulate skeleton External features External features Gemmules: no Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Case Base of A1 LID New sponge P Figure 3: Example of a real justification generated by LID in the marine sponges data set.",
                "Specifically, in our experiments, when an agent Ai wants to rebut an argument α, uses the following policy: 1.",
                "Agent Ai uses LID to try to find a counterargument β more specific than α; if found, β is sent to the other agent as a counterargument of α. 2.",
                "If not found, then Ai searches for a counterexample c ∈ Ci of α.",
                "If a case c is found, then c is sent to the other agent as a counterexample of α. 3.",
                "If no counterexamples are found, then Ai cannot rebut the argument α. 7.",
                "ARGUMENTATION-BASED MULTI-AGENT LEARNING The interaction protocol of AMAL allows a group of agents A1, ..., An to deliberate about the correct solution of a problem P by means of an argumentation process.",
                "If the argumentation process arrives to a consensual solution, the joint deliberation ends; otherwise a weighted vote is used to determine the joint solution.",
                "Moreover, AMAL also allows the agents to learn from the counterexamples received from other agents.",
                "The AMAL protocol consists on a series of rounds.",
                "In the initial round, each agent states which is its individual prediction for P. Then, at each round an agent can try to rebut the prediction made by any of the other agents.",
                "The protocol uses a token passing mechanism so that agents (one at a time) can send counterarguments or counterexamples if they disagree with the prediction made by any other agent.",
                "Specifically, each agent is allowed to send one counterargument or counterexample each time he gets the token (notice that this restriction is just to simplify the protocol, and that it does not restrict the number of counterargument an agent can sent, since they can be delayed for subsequent rounds).",
                "When an agent receives a counterargument or counterexample, it informs the other agents if it accepts the counterargument (and changes its prediction) or not.",
                "Moreover, agents have also the opportunity to answer to counterarguments when they receive the token, by trying to generate a counterargument to the counterargument.",
                "When all the agents have had the token once, the token returns to the first agent, and so on.",
                "If at any time in the protocol, all the agents agree or during the last n rounds no agent has generated any counterargument, the protocol ends.",
                "Moreover, if at the end of the argumentation the agents have not reached an agreement, then a voting mechanism that uses the confidence of each prediction as weights is used to decide the final solution (Thus, AMAL follows the same mechanism as human committees, first each individual member of a committee exposes his arguments and discuses those of the other members (joint deliberation), and if no consensus is reached, then a voting mechanism is required).",
                "At each iteration, agents can use the following performatives: • assert(α): the justified prediction held during the next round will be α.",
                "An agent can only hold a single prediction at each round, thus is multiple asserts are send, only the last one is considered as the currently held prediction. • rebut(β, α): the agent has found a counterargument β to the prediction α.",
                "We will define Ht = αt 1, ..., αt n as the predictions that each of the n agents hold at a round t. Moreover, we will also define contradict(αt i) = {α ∈ Ht|α.S = αt i.S} as the set of contradicting arguments for an agent Ai in a round t, i.e. the set of arguments at round t that support a different solution class than αt i.",
                "The protocol is initiated because one of the agents receives a problem P to be solved.",
                "After that, the agent informs all the other agents about the problem P to solve, and the protocol starts: 1.",
                "At round t = 0, each one of the agents individually solves P, and builds a justified prediction using its own CBR method.",
                "Then, each agent Ai sends the performative assert(α0 i ) to the other agents.",
                "Thus, the agents know H0 = α0 i , ..., α0 n .",
                "Once all the predictions have been sent the token is given to the first agent A1. 2.",
                "At each round t (other than 0), the agents check whether their arguments in Ht agree.",
                "If they do, the protocol moves to step 5.",
                "Moreover, if during the last n rounds no agent has sent any counterexample or counterargument, the protocol also moves to step 5.",
                "Otherwise, the agent Ai owner of the token tries to generate a counterargument for each of the opposing arguments in contradict(αt i) ⊆ Ht (see Section 6.1).",
                "Then, the counterargument βt i against the prediction αt j with the lowest confidence C(αt j) is selected (since αt j is the prediction more likely to be successfully rebutted). • If βt i is a counterargument, then, Ai locally compares αt i with βt i by assessing their confidence against its individual case base Ci (see Section 5) (notice that Ai is comparing its previous argument with the counterargument that Ai itself has just generated and that is about The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 979 Sponge Spikulate skeleton External features External features Gemmules: no Growing: Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Growing Grow: massive Case Base of A2 LID Solution: astrophorida Justification: D2 Figure 4: Generation of a counterargument using LID in the sponges data set. to send to Aj).",
                "If CAi (βt i ) > CAi (αt i), then Ai considers that βt i is stronger than its previous argument, changes its argument to βt i by sending assert(βt i ) to the rest of the agents (the intuition behind this is that since a counterargument is also an argument, Ai checks if the newly counterargument is a better argument than the one he was previously holding) and rebut(βt i , αt j) to Aj.",
                "Otherwise (i.e.",
                "CAi (βt i ) ≤ CAi (αt i)), Ai will send only rebut(βt i , αt j) to Aj.",
                "In any of the two situations the protocol moves to step 3. • If βt i is a counterexample c, then Ai sends rebut(c, αt j) to Aj.",
                "The protocol moves to step 4. • If Ai cannot generate any counterargument or counterexample, the token is sent to the next agent, a new round t + 1 starts, and the protocol moves to state 2. 3.",
                "The agent Aj that has received the counterargument βt i , locally compares it against its own argument, αt j, by locally assessing their confidence.",
                "If CAj (βt i ) > CAj (αt j), then Aj will accept the counterargument as stronger than its own argument, and it will send assert(βt i ) to the other agents.",
                "Otherwise (i.e.",
                "CAj (βt i ) ≤ CAj (αt j)), Aj will not accept the counterargument, and will inform the other agents accordingly.",
                "Any of the two situations start a new round t + 1, Ai sends the token to the next agent, and the protocol moves back to state 2. 4.",
                "The agent Aj that has received the counterexample c retains it into its case base and generates a new argument αt+1 j that takes into account c, and informs the rest of the agents by sending assert(αt+1 j ) to all of them.",
                "Then, Ai sends the token to the next agent, a new round t + 1 starts, and the protocol moves back to step 2. 5.",
                "The protocol ends yielding a joint prediction, as follows: if the arguments in Ht agree then their prediction is the joint prediction, otherwise a voting mechanism is used to decide the joint prediction.",
                "The voting mechanism uses the joint confidence measure as the voting weights, as follows: S = arg max Sk∈S αi∈Ht|αi.S=Sk C(αi) Moreover, in order to avoid infinite iterations, if an agent sends twice the same argument or counterargument to the same agent, the message is not considered. 8.",
                "EXEMPLIFICATION Let us consider a system composed of three agents A1, A2 and A3.",
                "One of the agents, A1 receives a problem P to solve, and decides to use AMAL to solve it.",
                "For that reason, invites A2 and A3 to take part in the argumentation process.",
                "They accept the invitation, and the argumentation protocol starts.",
                "Initially, each agent generates its individual prediction for P, and broadcasts it to the other agents.",
                "Thus, all of them can compute H0 = α0 1, α0 2, α0 3 .",
                "In particular, in this example: • α0 1 = A1, P, hadromerida, D1 • α0 2 = A2, P, astrophorida, D2 • α0 3 = A3, P, axinellida, D3 A1 starts owning the token and tries to generate counterarguments for α0 2 and α0 3, but does not succeed, however it has one counterexample c13 for α0 3.",
                "Thus, A1 sends the the message rebut( c13, α0 3) to A3.",
                "A3 incorporates c13 into its case base and tries to solve the problem P again, now taking c13 into consideration.",
                "A3 comes up with the justified prediction α1 3 = A3, P, hadromerida, D4 , and broadcasts it to the rest of the agents with the message assert(α1 3).",
                "Thus, all of them know the new H1 = α0 1, α0 2, α1 3 .",
                "Round 1 starts and A2 gets the token.",
                "A2 tries to generate counterarguments for α0 1 and α1 3 and only succeeds to generate a counterargument β1 2 = A2, P, astrophorida, D5 against α1 3.",
                "The counterargument is sent to A3 with the message rebut(β1 2 , α1 3).",
                "Agent A3 receives the counterargument and assesses its local confidence.",
                "The result is that the individual confidence of the counterargument β1 2 is lower than the local confidence of α1 3.",
                "Therefore, A3 does not accept the counterargument, and thus H2 = α0 1, α0 2, α1 3 .",
                "Round 2 starts and A3 gets the token.",
                "A3 generates a counterargument β2 3 = A3, P, hadromerida, D6 for α0 2 and sends it to A2 with the message rebut(β2 3 , α0 2).",
                "Agent A2 receives the counterargument and assesses its local confidence.",
                "The result is that the local confidence of the counterargument β2 3 is higher than the local confidence of α0 2.",
                "Therefore, A2 accepts the counterargument and informs the rest of the agents with the message assert(β2 3 ).",
                "After that, H3 = α0 1, β2 3 , α1 3 .",
                "At Round 3, since all the agents agree (all the justified predictions in H3 predict hadromerida as the solution class) The protocol ends, and A1 (the agent that received the problem) considers hadromerida as the joint solution for the problem P. 9.",
                "EXPERIMENTAL EVALUATION 980 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) SPONGE 75 77 79 81 83 85 87 89 91 2 3 4 5 AMAL Voting Individual SOYBEAN 55 60 65 70 75 80 85 90 2 3 4 5 AMAL Voting Individual Figure 5: Individual and joint accuracy for 2 to 5 agents.",
                "In this section we empirically evaluate the AMAL argumentation framework.",
                "We have made experiments in two different data sets: soybean (from the UCI machine learning repository) and sponge (a relational data set).",
                "The soybean data set has 307 examples and 19 solution classes, while the sponge data set has 280 examples and 3 solution classes.",
                "In an experimental run, the data set is divided in 2 sets: the training set and the test set.",
                "The training set examples are distributed among 5 different agents without replication, i.e. there is no example shared by two agents.",
                "In the testing stage, problems in the test set arrive randomly to one of the agents, and their goal is to predict the correct solution.",
                "The experiments are designed to test two hypotheses: (H1) that argumentation is a useful framework for joint deliberation and can improve over other typical methods such as voting; and (H2) that learning from communication improves the individual performance of a learning agent participating in an argumentation process.",
                "Moreover, we also expect that the improvement achieved from argumentation will increase as the number of agents participating in the argumentation increases (since more information will be taken into account).",
                "Concerning H1 (argumentation is a useful framework for joint deliberation), we ran 4 experiments, using 2, 3, 4, and 5 agents respectively (in all experiments each agent has a 20% of the training data, since the training is always distributed among 5 agents).",
                "Figure 5 shows the result of those experiments in the sponge and soybean data sets.",
                "Classification accuracy is plotted in the vertical axis, and in the horizontal axis the number of agents that took part in the argumentation processes is shown.",
                "For each number of agents, three bars are shown: individual, Voting, and AMAL.",
                "The individual bar shows the average accuracy of individual agents predictions; the voting bar shows the average accuracy of the joint prediction achieved by voting but without any argumentation; and finally the AMAL bar shows the average accuracy of the joint prediction using argumentation.",
                "The results shown are the average of 5 10-fold cross validation runs.",
                "Figure 5 shows that collaboration (voting and AMAL) outperforms individual problem solving.",
                "Moreover, as we expected, the accuracy improves as more agents collaborate, since more information is taken into account.",
                "We can also see that AMAL always outperforms standard voting, proving that joint decisions are based on better information as provided by the argumentation process.",
                "For instance, the joint accuracy for 2 agents in the sponge data set is of 87.57% for AMAL and 86.57% for voting (while individual accuracy is just 80.07%).",
                "Moreover, the improvement achieved by AMAL over Voting is even larger in the soybean data set.",
                "The reason is that the soybean data set is more difficult (in the sense that agents need more data to produce good predictions).",
                "These experimental results show that AMAL effectively exploits the opportunity for improvement: the accuracy is higher only because more agents have changed their opinion during argumentation (otherwise they would achieve the same result as Voting).",
                "Concerning H2 (learning from communication in argumentation processes improves individual prediction ), we ran the following experiment: initially, we distributed a 25% of the training set among the five agents; after that, the rest of the cases in the training set is sent to the agents one by one; when an agent receives a new training case, it has several options: the agent can discard it, the agent can retain it, or the agent can use it for engaging an argumentation process.",
                "Figure 6 shows the result of that experiment for the two data sets.",
                "Figure 6 contains three plots, where NL (not learning) shows accuracy of an agent with no learning at all; L (learning), shows the evolution of the individual classification accuracy when agents learn by retaining the training cases they individually receive (notice that when all the training cases have been retained at 100%, the accuracy should be equal to that of Figure 5 for individual agents); and finally LFC (learning from communication) shows the evolution of the individual classification accuracy of learning agents that also learn by retaining those counterexamples received during argumentation (i.e. they learn both from training examples and counterexamples).",
                "Figure 6 shows that if an agent Ai learns also from communication, Ai can significantly improve its individual performance with just a small number of additional cases (those selected as relevant counterexamples for Ai during argumentation).",
                "For instance, in the soybean data set, individual agents have achieved an accuracy of 70.62% when they also learn from communication versus an accuracy of 59.93% when they only learn from their individual experience.",
                "The number of cases learnt from communication depends on the properties of the data set: in the sponges data set, agents have retained only very few additional cases, and significantly improved individual accuracy; namely they retain 59.96 cases in average (compared to the 50.4 cases retained if they do not learn from communication).",
                "In the soybean data set more counterexamples are learnt to significantly improve individual accuracy, namely they retain 87.16 cases in average (compared to 55.27 cases retained if they do not learn from communication).",
                "Finally, the fact that both data sets show a significant improvement points out the adaptive nature of the argumentation-based approach to learning from communication: the useful cases are selected as counterexamples (and no more than those needed), and they have the intended effect. 10.",
                "RELATED WORK Concerning CBR in a multi-agent setting, the first research was on negotiated case retrieval [11] among groups of agents.",
                "Our work on multi-agent case-based learning started in 1999 [6]; later Mc Ginty and Smyth [7] presented a multi-agent collaborative CBR approach (CCBR) for planning.",
                "Finally, another interesting approach is multi-case-base reasoning (MCBR) [5], that deals with The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 981 SPONGE 60 65 70 75 80 85 25% 40% 55% 70% 85% 100% LFC L NL SOYBEAN 20 30 40 50 60 70 80 90 25% 40% 55% 70% 85% 100% LFC L NL Figure 6: Learning from communication resulting from argumentation in a system composed of 5 agents. distributed systems where there are several case bases available for the same task and addresses the problems of cross-case base adaptation.",
                "The main difference is that our MAC approach is a way to distribute the Reuse process of CBR (using a voting system) while Retrieve is performed individually by each agent; the other multiagent CBR approaches, however, focus on distributing the Retrieve process.",
                "Research on MAS argumentation focus on several issues like a) logics, protocols and languages that support argumentation, b) argument selection and c) argument interpretation.",
                "Approaches for logic and languages that support argumentation include defeasible logic [4] and BDI models [13].",
                "Although argument selection is a key aspect of automated argumentation (see [12] and [13]), most research has been focused on preference relations among arguments.",
                "In our framework we have addressed both argument selection and preference relations using a case-based approach. 11.",
                "CONCLUSIONS AND FUTURE WORK In this paper we have presented an argumentation-based framework for <br>multi-agent learn</br>ing.",
                "Specifically, we have presented AMAL, a framework that allows a group of learning agents to argue about the solution of a given problem and we have shown how the learning capabilities can be used to generate arguments and counterarguments.",
                "The experimental evaluation shows that the increased amount of information provided to the agents by the argumentation process increases their predictive accuracy, and specially when an adequate number of agents take part in the argumentation.",
                "The main contributions of this work are: a) an argumentation framework for learning agents; b) a case-based preference relation over arguments, based on computing an overall confidence estimation of arguments; c) a case-based policy to generate counterarguments and select counterexamples; and d) an argumentation-based approach for learning from communication.",
                "Finally, in the experiments presented here a learning agent would retain all counterexamples submitted by the other agent; however, this is a very simple case retention policy, and we will like to experiment with more informed policies - with the goal that individual learning agents could significantly improve using only a small set of cases proposed by other agents.",
                "Finally, our approach is focused on lazy learning, and future works aims at incorporating eager inductive learning inside the argumentative framework for learning from communication. 12.",
                "REFERENCES [1] Agnar Aamodt and Enric Plaza.",
                "Case-based reasoning: Foundational issues, methodological variations, and system approaches.",
                "Artificial Intelligence Communications, 7(1):39-59, 1994. [2] E. Armengol and E. Plaza.",
                "Lazy induction of descriptions for relational case-based learning.",
                "In ECML2001, pages 13-24, 2001. [3] Gerhard Brewka.",
                "Dynamic argument systems: A formal model of argumentation processes based on situation calculus.",
                "Journal of Logic and Computation, 11(2):257-282, 2001. [4] Carlos I. Chesñevar and Guillermo R. Simari.",
                "Formalizing Defeasible Argumentation using Labelled Deductive Systems.",
                "Journal of Computer Science & Technology, 1(4):18-33, 2000. [5] D. Leake and R. Sooriamurthi.",
                "Automatically selecting strategies for multi-case-base reasoning.",
                "In S. Craw and A. Preece, editors, ECCBR2002, pages 204-219, Berlin, 2002.",
                "Springer Verlag. [6] Francisco J. Martín, Enric Plaza, and Josep-Lluis Arcos.",
                "Knowledge and experience reuse through communications among competent (peer) agents.",
                "International Journal of Software Engineering and Knowledge Engineering, 9(3):319-341, 1999. [7] Lorraine McGinty and Barry Smyth.",
                "Collaborative case-based reasoning: Applications in personalized route planning.",
                "In I. Watson and Q. Yang, editors, ICCBR, number 2080 in LNAI, pages 362-376.",
                "Springer-Verlag, 2001. [8] Santi Ontañón and Enric Plaza.",
                "Justification-based multiagent learning.",
                "In ICML2003, pages 576-583.",
                "Morgan Kaufmann, 2003. [9] Enric Plaza, Eva Armengol, and Santiago Ontañón.",
                "The explanatory power of symbolic similarity in case-based reasoning.",
                "Artificial Intelligence Review, 24(2):145-161, 2005. [10] David Poole.",
                "On the comparison of theories: Preferring the most specific explanation.",
                "In IJCAI-85, pages 144-147, 1985. [11] M V Nagendra Prassad, Victor R Lesser, and Susan Lander.",
                "Retrieval and reasoning in distributed case bases.",
                "Technical report, UMass Computer Science Department, 1995. [12] K. Sycara S. Kraus and A. Evenchik.",
                "Reaching agreements through argumentation: a logical model and implementation.",
                "Artificial Intelligence Journal, 104:1-69, 1998. [13] N. R. Jennings S. Parsons, C. Sierra.",
                "Agents that reason and negotiate by arguing.",
                "Journal of Logic and Computation, 8:261-292, 1998. [14] Bruce A. Wooley.",
                "Explanation component of software systems.",
                "ACM CrossRoads, 5.1, 1998. 982 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "CONCLUSIONS AND FUTURE WORK In this paper we have presented an argumentation-based framework for <br>multi-agent learn</br>ing."
            ],
            "translated_annotated_samples": [
                "CONCLUSIONES Y TRABAJO FUTURO En este artículo hemos presentado un marco basado en argumentación para el <br>aprendizaje multiagente</br>."
            ],
            "translated_text": "Aprendizaje y deliberación conjunta a través de la argumentación en sistemas multiagente. Santi Ontañón CCL, Laboratorio de Computación Cognitiva, Instituto de Tecnología de Georgia, Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Instituto de Investigación en Inteligencia Artificial, CSIC, Consejo Superior de Investigaciones Científicas, Campus UAB, 08193 Bellaterra, Cataluña (España) enric@iiia.csic.es RESUMEN En este artículo presentaremos un marco de argumentación para agentes de aprendizaje (AMAL) diseñado con dos propósitos: (1) para deliberación conjunta, y (2) para aprender a través de la comunicación. El marco AMAL se basa completamente en el aprendizaje a partir de ejemplos: la relación de preferencia de argumentos, la política de generación de argumentos y la política de generación de contraargumentos son técnicas basadas en casos. Para unirse en la deliberación, los agentes de aprendizaje comparten su experiencia formando un comité para decidir sobre alguna decisión conjunta. Experimentalmente demostramos que la argumentación entre comités de agentes mejora tanto el rendimiento individual como el conjunto. Para aprender a través de la comunicación, un agente se involucra en discusiones con otros agentes para contrastar sus hipótesis individuales y recibir contraejemplos; el proceso de argumentación mejora su alcance de aprendizaje y rendimiento individual. Categorías y Descriptores de Asignaturas I.2.6 [Inteligencia Artificial]: Aprendizaje; I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente, Agentes Inteligentes 1. Los marcos de argumentación para sistemas multiagente pueden ser utilizados para diferentes propósitos como la deliberación conjunta, la persuasión, la negociación y la resolución de conflictos. En este artículo presentaremos un marco de argumentación para agentes de aprendizaje, y mostraremos que puede ser utilizado para dos propósitos: (1) deliberación conjunta, y (2) aprendizaje a partir de la comunicación. La deliberación conjunta basada en argumentos implica discutir sobre el resultado de una situación particular o la acción apropiada a tomar en una situación específica. Los agentes de aprendizaje son capaces de aprender de la experiencia, en el sentido de que ejemplos pasados (situaciones y sus resultados) se utilizan para predecir el resultado para la situación actual. Sin embargo, dado que la experiencia de los agentes individuales puede ser limitada, el conocimiento individual y la precisión de la predicción también son limitados. Por lo tanto, los agentes de aprendizaje que son capaces de argumentar sus predicciones individuales con otros agentes pueden alcanzar una mayor precisión en las predicciones después de dicho proceso de argumentación. La mayoría de los marcos de argumentación existentes para sistemas multiagente se basan en lógica deductiva u otra formalización de lógica deductiva diseñada específicamente para respaldar la argumentación, como la lógica por defecto [3]. Por lo general, un argumento se ve como una declaración lógica, mientras que un contraargumento es un argumento ofrecido en oposición a otro argumento; los agentes utilizan una relación de preferencia para resolver argumentos conflictivos. Sin embargo, los marcos de argumentación basados en lógica asumen agentes con conocimiento predefinido y relación de preferencia. En este artículo, nos enfocamos en un marco de Aprendizaje Multiagente basado en Argumentación (AMAL) donde tanto el conocimiento como la relación de preferencia se aprenden a partir de la experiencia. Por lo tanto, consideramos un escenario con agentes que (1) trabajan en el mismo dominio utilizando una ontología compartida, (2) son capaces de aprender a partir de ejemplos y (3) se comunican utilizando un marco argumentativo. Tener capacidades de aprendizaje permite a los agentes utilizar de manera efectiva una forma específica de contraargumento, a saber, el uso de contraejemplos. Los contraejemplos ofrecen la posibilidad de que los agentes aprendan durante el proceso de argumentación. Además, los agentes de aprendizaje permiten técnicas que utilizan la experiencia aprendida para generar argumentos y contraargumentos adecuados. Específicamente, necesitaremos abordar dos problemas: (1) cómo definir una técnica para generar argumentos y contraargumentos a partir de ejemplos, y (2) cómo definir una relación de preferencia sobre dos argumentos en conflicto que han sido inducidos a partir de ejemplos. Este documento presenta un enfoque basado en casos para abordar ambas cuestiones. Los agentes utilizan el razonamiento basado en casos (CBR) [1] para aprender de casos pasados (donde un caso es una situación y su resultado) con el fin de predecir el resultado de una nueva situación. Proponemos un protocolo de argumentación dentro del marco de AMAL que apoya a los agentes en llegar a una predicción conjunta sobre una situación o problema específico; además, el razonamiento necesario para respaldar el proceso de argumentación también se basará en casos. En particular, presentamos dos medidas basadas en casos, una para generar los argumentos y contraargumentos adecuados para una situación particular y otra para determinar la relación de preferencia entre los argumentos. Finalmente, evaluamos (1) si la argumentación entre agentes de aprendizaje puede producir una predicción conjunta que mejore el rendimiento de aprendizaje individual y (2) si el aprendizaje a partir de los contraejemplos transmitidos durante el proceso de argumentación aumenta el rendimiento individual con precisamente aquellos casos que se utilizan mientras discuten entre ellos. El documento está estructurado de la siguiente manera. La sección 2 discute la relación entre la argumentación, la colaboración y el aprendizaje. Luego, la Sección 3 introduce nuestro marco de trabajo de CBR multiagente (MAC) y el concepto de predicción justificada. Después de eso, la Sección 4 define formalmente nuestro marco de argumentación. Las secciones 5 y 6 presentan nuestra relación de preferencia basada en casos y las políticas de generación de argumentos, respectivamente. Más adelante, la Sección 7 presenta el protocolo de argumentación en nuestro marco AMAL. Después de eso, la Sección 8 presenta una ejemplificación del marco de argumentación. Finalmente, la Sección 9 presenta una evaluación empírica de nuestras dos hipótesis principales. El artículo concluye con secciones de trabajo relacionado y conclusiones. 2. ARGUMENTACIÓN, COLABORACIÓN Y APRENDIZAJE Tanto el aprendizaje como la colaboración son formas en las que un agente puede mejorar el rendimiento individual. De hecho, existe un claro paralelismo entre el aprendizaje y la colaboración en sistemas multiagentes, ya que ambos son formas en las que los agentes pueden lidiar con sus deficiencias. Mostremos cuáles son las principales motivaciones que un agente puede tener para aprender o colaborar. • Motivaciones para aprender: - Aumentar la calidad de la predicción, - Aumentar la eficiencia, - Ampliar el rango de problemas resolubles. • Motivaciones para colaborar: - Aumentar la calidad de la predicción, - Aumentar la eficiencia, - Ampliar el rango de problemas resolubles, - Ampliar el rango de recursos accesibles. Al observar las listas de motivación anteriores, podemos ver fácilmente que el aprendizaje y la colaboración están muy relacionados en los sistemas multiagente. De hecho, con la excepción del último elemento en la lista de motivaciones para colaborar, son dos extremos de un continuo de estrategias para mejorar el rendimiento. Un agente puede optar por aumentar el rendimiento mediante el aprendizaje, la colaboración o encontrando un punto intermedio que combine el aprendizaje y la colaboración para mejorar el rendimiento. En este artículo propondremos AMAL, un marco de argumentación para agentes de aprendizaje, y también mostraremos cómo AMAL puede ser utilizado tanto para aprender a través de la comunicación como para resolver problemas de manera colaborativa: • Los agentes pueden resolver problemas de manera colaborativa al participar en un proceso de argumentación sobre la predicción para la situación actual. Utilizando esta colaboración, la predicción puede realizarse de una manera más informada, ya que se ha tenido en cuenta la información conocida por varios agentes. Los agentes también pueden aprender de la comunicación con otros agentes al participar en un proceso de argumentación. Los agentes que participan en estos procesos de argumentación pueden aprender de los argumentos y contraejemplos recibidos de otros agentes, y utilizar esta información para predecir los resultados de situaciones futuras. En el resto de este documento propondremos un marco de argumentación y mostraremos cómo puede ser utilizado tanto para el aprendizaje como para la resolución de problemas de manera colaborativa. SISTEMAS CBR MULTIAGENTE Un Sistema de Razonamiento Basado en Casos Multiagente (MAC) M = {(A1, C1), ..., (An, Cn)} es un sistema multiagente compuesto por A = {Ai, ..., An}, un conjunto de agentes CBR, donde cada agente Ai ∈ A posee una base de casos individual Ci. Cada agente individual Ai en un MAC es completamente autónomo y cada agente Ai solo tiene acceso a su base de casos individual y privada Ci. Un conjunto de casos Ci = {c1, ..., cm} es una colección de casos. Los agentes en un sistema MAC pueden resolver problemas de forma individual, pero también pueden colaborar con otros agentes para resolver problemas. En este marco, nos limitaremos a tareas analíticas, es decir, tareas como la clasificación, donde la solución de un problema se logra seleccionando una clase de solución de un conjunto enumerado de clases de solución. En lo siguiente, notaremos el conjunto de todas las clases de soluciones por S = {S1, ..., SK}. Por lo tanto, un caso c = P, S es una tupla que contiene una descripción del caso P y una clase de solución S ∈ S. En lo siguiente, utilizaremos indistintamente los términos problema y descripción del caso. Además, utilizaremos la notación de punto para referirnos a elementos dentro de una tupla; por ejemplo, para referirnos a la clase de solución de un caso c, escribiremos c.S. Por lo tanto, decimos que un grupo de agentes realiza una deliberación conjunta cuando colaboran para encontrar una solución conjunta mediante un proceso de argumentación. Sin embargo, para hacerlo, un agente debe ser capaz de justificar su predicción a los otros agentes (es decir, generar un argumento para su solución predicha que pueda ser examinado y criticado por los otros agentes). La siguiente sección aborda este tema. 3.1 Predicciones Justificadas Tanto los sistemas expertos como los sistemas CBR pueden tener un componente de explicación [14] encargado de justificar por qué el sistema ha proporcionado una respuesta específica al usuario. La línea de razonamiento del sistema puede ser examinada por un experto humano, aumentando así la fiabilidad del sistema. La mayoría de los trabajos existentes sobre generación de explicaciones se centran en generar explicaciones para ser proporcionadas al usuario. Sin embargo, en nuestro enfoque utilizamos explicaciones (o justificaciones) como una herramienta para mejorar la comunicación y coordinación entre agentes. Estamos interesados en las justificaciones ya que pueden ser utilizadas como argumentos. Para ese propósito, nos beneficiaremos de la capacidad de algunos métodos de aprendizaje automático para proporcionar justificaciones. Una justificación construida por un método CBR después de determinar que la solución de un problema particular P fue Sk es una descripción que contiene la información relevante del problema P que el método CBR ha considerado para predecir Sk como la solución de P. En particular, los métodos CBR funcionan recuperando casos similares al problema en cuestión, y luego reutilizando sus soluciones para el problema actual, esperando que dado que el problema y los casos son similares, las soluciones también serán similares. Por lo tanto, si un método CBR ha recuperado un conjunto de casos C1, ..., Cn para resolver un problema particular P, la justificación construida contendrá la información relevante del problema P que hizo que el sistema CBR recuperara ese conjunto particular de casos, es decir, contendrá la información relevante que P y C1, ..., Cn tienen en común. Por ejemplo, la Figura 1 muestra una justificación construida por un sistema CBR para un problema de juguete (en las siguientes secciones mostraremos justificaciones para problemas reales). En la figura, un problema tiene dos atributos (Semáforo y Coches_pasando), el mecanismo de recuperación del sistema CBR nota que al considerar solo el atributo Semáforo, puede recuperar dos casos que predicen la misma solución: esperar. Por lo tanto, dado que solo se ha utilizado este atributo, es el único que aparece en la justificación. Los valores del resto de atributos son irrelevantes, ya que, sea cual sea su valor, la clase de solución habría sido la misma. 976 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Problema Semáforo: rojo Pasan coches: no Caso 1 Semáforo: rojo Pasan coches: no Solución: esperar Caso 3 Semáforo: rojo Pasan coches: sí Solución: esperar Caso 4 Semáforo: verde Pasan coches: sí Solución: esperar Caso 2 Semáforo: verde Pasan coches: no Solución: cruzar Casos recuperados Solución: esperar Justificación Semáforo: rojo Figura 1: Un ejemplo de generación de justificación en un sistema de RCB. Observa que, dado que la única característica relevante para decidir es Traffic_light (la única utilizada para recuperar casos), es la única que aparece en la justificación. En general, el significado de una justificación es que todos (o la mayoría) de los casos en la base de casos de un agente que satisfacen la justificación (es decir, todos los casos que están subsumidos por la justificación) pertenecen a la clase de solución predicha. En el resto del documento, usaremos para denotar la relación de subsumpción. En nuestro trabajo, utilizamos LID [2], un método CBR capaz de construir justificaciones simbólicas como la ejemplificada en la Figura 1. Cuando un agente proporciona una justificación para una predicción, el agente genera una predicción justificada: DEFINICIÓN 3.1. Una Predicción Justificada es una tupla J = A, P, S, D donde el agente A considera que S es la solución correcta para el problema P, y esa predicción está justificada por una descripción simbólica D tal que J.D J.P. Las justificaciones pueden tener muchos usos para los sistemas de RCB [8, 9]. En este documento, vamos a utilizar justificaciones como argumentos, con el fin de permitir que los agentes de aprendizaje participen en procesos de argumentación. 4. ARGUMENTOS Y CONTRAARGUMENTOS Para nuestros propósitos, un argumento α generado por un agente A está compuesto por una afirmación S y alguna evidencia D que respalda a S como correcta. En el resto de esta sección veremos cómo esta definición general de argumento puede ser concretada en un tipo específico de argumentos que los agentes pueden generar. En el contexto de los sistemas MAC, los agentes discuten sobre predicciones para nuevos problemas y pueden proporcionar dos tipos de información: a) casos específicos P, S, y b) predicciones justificadas: A, P, S, D. Usando esta información, podemos definir tres tipos de argumentos: predicciones justificadas, contraargumentos y contraejemplos. Una predicción justificada α es generada por un agente Ai para argumentar que Ai cree que la solución correcta para un problema dado P es α.S, y la evidencia proporcionada es la justificación α.D. En el ejemplo representado en la Figura 1, un agente Ai puede generar el argumento α = Ai, P, Esperar, (Semáforo = rojo), lo que significa que el agente Ai cree que la solución correcta para P es Esperar porque el atributo Semáforo es igual a rojo. Un contraargumento β es un argumento ofrecido en oposición a otro argumento α. En nuestro marco de trabajo, un contraargumento consiste en una predicción justificada Aj, P, S, D generada por un agente Aj con la intención de refutar un argumento α generado por otro agente Ai, que respalda una clase de solución S diferente a la de α.S para el problema en cuestión y justifica esto con una justificación D. En el ejemplo de la Figura 1, si un agente genera el argumento α = Ai, P, Caminar, (Pasando_autos = no), un agente que piensa que la solución correcta es Esperar podría responder con el contraargumento β = Aj, P, Esperar, (Pasando_autos = no ∧ Semáforo = rojo), lo que significa que, aunque no hay autos pasando, el semáforo está en rojo y la calle no se puede cruzar. Un contraejemplo c es un caso que contradice un argumento α. Por lo tanto, un contraejemplo también es un contraargumento, uno que afirma que un argumento específico α no siempre es verdadero, y la evidencia proporcionada es el caso c. Específicamente, para que un caso c sea un contraejemplo de un argumento α, se deben cumplir las siguientes condiciones: α.D c y α.S = c.S, es decir, el caso debe satisfacer la justificación α.D y la solución de c debe ser diferente a la predicha por α. Al intercambiar argumentos y contraargumentos (incluyendo contraejemplos), los agentes pueden discutir sobre la solución correcta de un problema dado, es decir, pueden participar en un proceso de deliberación conjunto. Sin embargo, para hacerlo, necesitan un protocolo de interacción específico, una relación de preferencia entre argumentos contradictorios y una política de decisión para generar contraargumentos (incluidos contraejemplos). En las siguientes secciones presentaremos estos elementos. 5. RELACIÓN DE PREFERENCIA Un argumento específico proporcionado por un agente podría no ser consistente con la información conocida por otros agentes (o incluso con parte de la información conocida por el agente que ha generado la justificación debido al ruido en los datos de entrenamiento). Por esa razón, vamos a definir una relación de preferencia sobre predicciones justificadas contradictorias basadas en casos. Básicamente, definiremos una medida de confianza para cada predicción justificada (que tenga en cuenta los casos poseídos por cada agente), y la predicción justificada con la mayor confianza será la preferida. La idea detrás de la confianza basada en casos es contar cuántos de los casos en una base de casos individual respaldan una predicción justificada, y cuántos de ellos son ejemplos en contra de ella. Cuanto más casos de respaldo, mayor es la confianza; y cuanto más contrajemplos, menor es la confianza. Específicamente, para evaluar la confianza de una predicción justificada α, un agente obtiene el conjunto de casos en su base de casos individuales que están subsumidos por α.D. Con ellos, un agente Ai obtiene los valores Y (sí) y N (no): • Y Ai α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| es el número de casos en la base de casos del agente subsumidos por la justificación α.D que pertenecen a la clase de solución α.S, • NAi α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| es el número de casos en la base de casos del agente subsumidos por la justificación α.D que no pertenecen a esa clase de solución. El Sexto Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 977 + + + + + +  - - + Figura 2: La confianza de los argumentos se evalúa contrastándolos con las bases de casos de los agentes. Un agente estima la confianza de un argumento como: CAi (α) = Y Ai α 1 + Y Ai α + NAi α es decir, la confianza en una predicción justificada es el número de casos que la respaldan dividido por el número de casos que la respaldan más los contraejemplos. Observa que sumamos 1 al denominador, esto es para evitar otorgar confianzas excesivamente altas a predicciones justificadas cuya confianza se ha calculado utilizando un número pequeño de casos. Ten en cuenta que esta corrección sigue la misma idea que la corrección de Laplace para estimar probabilidades. La Figura 2 ilustra la evaluación individual de la confianza de un argumento, en particular, se encuentran tres casos de respaldo y un contraejemplo en la base de casos de los agentes Ai, lo que da una confianza estimada de 0.6. Además, también podemos definir la confianza conjunta de un argumento α como la confianza calculada utilizando los casos presentes en las bases de casos de todos los agentes en el grupo: C(α) = Π Ai α / (1 + Σ Ai α + NAi α). Nótese que, para calcular conjuntamente la confianza conjunta, los agentes solo tienen que hacer públicos los valores de aprobación y rechazo calculados localmente para un argumento dado. En nuestro marco de trabajo, los agentes utilizan esta confianza conjunta como la relación de preferencia: una predicción justificada α se prefiere sobre otra β si C(α) ≥ C(β). GENERACIÓN DE ARGUMENTOS En nuestro marco de trabajo, los argumentos son generados por los agentes a partir de casos, utilizando métodos de aprendizaje. Cualquier método de aprendizaje capaz de proporcionar una predicción justificada puede ser utilizado para generar argumentos. Por ejemplo, los árboles de decisión y LID [2] son métodos de aprendizaje adecuados. Específicamente, en los experimentos reportados en este artículo, los agentes utilizan LID. Por lo tanto, cuando un agente desea generar un argumento que respalde que una clase de solución específica es la solución correcta para un problema P, genera una predicción justificada como se explica en la Sección 3.1. Por ejemplo, la Figura 3 muestra una justificación real generada por LID después de resolver un problema P en el dominio de la identificación de esponjas marinas. En particular, la Figura 3 muestra cómo cuando un agente recibe un nuevo problema para resolver (en este caso, un nuevo esponja para determinar su orden), el agente utiliza LID para generar un argumento (que consiste en una predicción justificada) utilizando los casos en la base de casos del agente. La justificación mostrada en la Figura 3 se puede interpretar diciendo que la solución predicha es hadromerida porque la forma lisa de las megascleras del esqueleto espicular de la esponja es de tipo tilostilo, el esqueleto espicular de la esponja no tiene una longitud uniforme y no hay gemas en las características externas de la esponja. Por lo tanto, el argumento generado será α = A1, P, hadromerida, D1. 6.1 Generación de Contraargumentos Como se mencionó anteriormente, los agentes pueden intentar refutar argumentos generando contraargumentos o encontrando contraejemplos. Permítanos explicar cómo pueden ser generados. Un agente Ai quiere generar un contraargumento β para rebatir un argumento α cuando α está en contradicción con la base de casos locales de Ai. Además, al generar dicho contraargumento β, Ai espera que β sea preferido sobre α. Con ese propósito, presentaremos una política específica para generar contraargumentos basados en el criterio de especificidad [10]. El criterio de especificidad se utiliza ampliamente en marcos deductivos para la argumentación, y establece que entre dos argumentos en conflicto, se debe preferir el más específico ya que, en principio, está más informado. Por lo tanto, se espera que los contraargumentos generados basados en el criterio de especificidad sean preferibles (ya que están más informados) que los argumentos que intentan refutar. Sin embargo, no hay garantía de que tales contraargumentos siempre ganen, ya que, como hemos indicado en la Sección 5, los agentes en nuestro marco utilizan una relación de preferencia basada en la confianza conjunta. Además, uno podría pensar que sería mejor que los agentes generaran contraargumentos basados en la relación de preferencia de confianza conjunta; sin embargo, no es obvio cómo generar contraargumentos basados en la confianza conjunta de manera eficiente, ya que se requiere colaboración para evaluar la confianza conjunta. Por lo tanto, el agente que genera el contraargumento debe comunicarse constantemente con los otros agentes en cada paso del algoritmo de inducción utilizado para generar contraargumentos (actualmente una de nuestras líneas de investigación futuras). Por lo tanto, en nuestro marco de trabajo, cuando un agente quiere generar un contraargumento β a un argumento α, β tiene que ser más específico que α (es decir, α.D < β.D). La generación de contraargumentos utilizando el criterio de especificidad impone algunas restricciones sobre el método de aprendizaje, aunque LID o ID3 pueden adaptarse fácilmente para esta tarea. Por ejemplo, LID es un algoritmo que genera una descripción desde cero y añade heurísticamente características a ese término. Por lo tanto, en cada paso, la descripción se vuelve más específica que en el paso anterior, y el número de casos que se incluyen en esa descripción se reduce. Cuando la descripción cubre solo (o casi solo) casos de una única clase de solución, LID termina y predice esa clase de solución. Para generar un contraargumento a un argumento α, LID solo tiene que usar como punto de partida la descripción α.D en lugar de empezar desde cero. De esta manera, la justificación proporcionada por LID siempre estará subsumida por α.D, y por lo tanto, el contraargumento resultante será más específico que α. Sin embargo, hay que tener en cuenta que LID a veces puede no ser capaz de generar contraargumentos, ya que LID puede no ser capaz de especializar aún más la descripción α.D, o porque el agente Ai no tiene ningún caso en Ci que esté subsumido por α.D. La Figura 4 muestra cómo un agente A2 que no estaba de acuerdo con el argumento mostrado en la Figura 3, genera un contraargumento utilizando LID. Además, la Figura 4 muestra la generación de un contraargumento β1 2 para el argumento α0 1 (en la Figura 3) que es una especialización de α0 1. 978 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Solución: hadromerida Justificación: D1 Esponja Esqueleto espiculado Características externas Características externas Gemmulas: no Esqueleto espiculado Megascleres Longitud uniforme: no Megascleres Forma lisa: tilostilo Caso Base de A1 LID Nueva esponja P Figura 3: Ejemplo de una justificación real generada por LID en el conjunto de datos de esponjas marinas. Específicamente, en nuestros experimentos, cuando un agente Ai quiere refutar un argumento α, utiliza la siguiente política: 1. El Agente Ai utiliza LID para intentar encontrar un contraargumento β más específico que α; si se encuentra, β se envía al otro agente como un contraargumento de α. 2. Si no se encuentra, entonces Ai busca un contraejemplo c ∈ Ci de α. Si se encuentra un caso c, entonces c se envía al otro agente como un contraejemplo de α. Si no se encuentran contraejemplos, entonces Ai no puede refutar el argumento α. 7. APRENDIZAJE MULTIAGENTE BASADO EN ARGUMENTACIÓN El protocolo de interacción de AMAL permite que un grupo de agentes A1, ..., An deliberen sobre la solución correcta de un problema P mediante un proceso de argumentación. Si el proceso de argumentación llega a una solución consensuada, la deliberación conjunta termina; de lo contrario, se utiliza una votación ponderada para determinar la solución conjunta. Además, AMAL también permite a los agentes aprender de los contraejemplos recibidos de otros agentes. El protocolo AMAL consiste en una serie de rondas. En la ronda inicial, cada agente declara cuál es su predicción individual para P. Luego, en cada ronda, un agente puede intentar refutar la predicción hecha por cualquiera de los otros agentes. El protocolo utiliza un mecanismo de paso de token para que los agentes (uno a la vez) puedan enviar contraargumentos o contraejemplos si no están de acuerdo con la predicción hecha por cualquier otro agente. Específicamente, a cada agente se le permite enviar un contraargumento o contraejemplo cada vez que recibe el token (nota que esta restricción es solo para simplificar el protocolo, y que no restringe la cantidad de contraargumentos que un agente puede enviar, ya que pueden ser retrasados para rondas posteriores). Cuando un agente recibe un contraargumento o contraejemplo, informa a los otros agentes si acepta el contraargumento (y cambia su predicción) o no. Además, los agentes también tienen la oportunidad de responder a los contraargumentos cuando reciben el token, intentando generar un contraargumento al contraargumento. Cuando todos los agentes hayan tenido el token una vez, el token regresa al primer agente, y así sucesivamente. Si en algún momento del protocolo, todos los agentes están de acuerdo o durante las últimas n rondas ningún agente ha generado ninguna contra-argumentación, el protocolo termina. Además, si al final de la argumentación los agentes no han llegado a un acuerdo, entonces se utiliza un mecanismo de votación que utiliza la confianza de cada predicción como pesos para decidir la solución final (Así, AMAL sigue el mismo mecanismo que los comités humanos, primero cada miembro individual de un comité expone sus argumentos y discute los de los otros miembros (deliberación conjunta), y si no se alcanza un consenso, entonces se requiere un mecanismo de votación). En cada iteración, los agentes pueden usar los siguientes performativos: • afirmar(α): la predicción justificada que se mantendrá durante la próxima ronda será α. Un agente solo puede tener una única predicción en cada ronda, por lo tanto, si se envían múltiples afirmaciones, solo se considera la última como la predicción actualmente mantenida. • rebut(β, α): el agente ha encontrado un contraargumento β a la predicción α. Definiremos Ht = αt 1, ..., αt n como las predicciones que cada uno de los n agentes tiene en una ronda t. Además, también definiremos contradict(αt i) = {α ∈ Ht|α.S = αt i.S} como el conjunto de argumentos contradictorios para un agente Ai en una ronda t, es decir, el conjunto de argumentos en la ronda t que respaldan una clase de solución diferente a la de αt i. El protocolo se inicia porque uno de los agentes recibe un problema P que debe ser resuelto. Después de eso, el agente informa a todos los demás agentes sobre el problema P a resolver, y el protocolo comienza: 1. En la ronda t = 0, cada uno de los agentes resuelve individualmente P y construye una predicción justificada utilizando su propio método de RBC. Entonces, cada agente Ai envía el assertivo performativo assert(α0 i) a los otros agentes. Por lo tanto, los agentes conocen H0 = α0 i , ..., α0 n. Una vez que todas las predicciones hayan sido enviadas, el token se le da al primer agente A1. 2. En cada ronda t (distinta de 0), los agentes verifican si sus argumentos en Ht coinciden. Si lo hacen, el protocolo avanza al paso 5. Además, si durante las últimas n rondas ningún agente ha enviado ningún contraejemplo o contraargumento, el protocolo también avanza al paso 5. De lo contrario, el agente Ai propietario del token intenta generar un contraargumento para cada uno de los argumentos opuestos en contradict(αt i) ⊆ Ht (ver Sección 6.1). Entonces, se selecciona el contraargumento βt i contra la predicción αt j con la menor confianza C(αt j) (ya que αt j es la predicción más probable de ser refutada con éxito). • Si βt i es un contraargumento, entonces, Ai compara localmente αt i con βt i evaluando su confianza contra su base de casos individuales Ci (ver Sección 5) (nótese que Ai está comparando su argumento previo con el contraargumento que Ai mismo acaba de generar y que trata sobre The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 979 Esponja Esqueleto espiculado Características externas Características externas Gemas: no Crecimiento: Esqueleto espiculado Megascleres Longitud uniforme: no Megascleres Forma lisa: tilostilo Crecimiento Crecimiento: masivo Caso Base de A2 LID Solución: astrophorida Justificación: D2 Figura 4: Generación de un contraargumento utilizando LID en el conjunto de datos de esponjas. para enviar a Aj. Si CAi (βt i) > CAi (αt i), entonces Ai considera que βt i es más fuerte que su argumento anterior, cambia su argumento a βt i enviando assert(βt i) al resto de los agentes (la intuición detrás de esto es que dado que un contraargumento también es un argumento, Ai verifica si el nuevo contraargumento es un mejor argumento que el que sostenía anteriormente) y rebut(βt i, αt j) a Aj. De lo contrario (es decir, Si CAi (βt i ) ≤ CAi (αt i), Ai solo enviará rebut(βt i , αt j) a Aj. En cualquiera de las dos situaciones, el protocolo avanza al paso 3. • Si βt i es un contraejemplo c, entonces Ai envía una refutación(c, αt j) a Aj. El protocolo avanza al paso 4. • Si Ai no puede generar ningún contraargumento o contraejemplo, el token se envía al siguiente agente, comienza una nueva ronda t + 1, y el protocolo avanza al estado 2. 3. El agente Aj que ha recibido el contraargumento βt i, lo compara localmente con su propio argumento, αt j, evaluando localmente su confianza. Si CAj (βt i ) > CAj (αt j), entonces Aj aceptará el contraargumento como más fuerte que su propio argumento, y enviará assert(βt i ) a los otros agentes. De lo contrario (es decir, Si CAj (βt i ) ≤ CAj (αt j)), Aj no aceptará el contraargumento e informará a los otros agentes en consecuencia. Cualquiera de las dos situaciones inicia una nueva ronda t + 1, Ai envía el token al siguiente agente y el protocolo vuelve al estado 2. 4. El agente Aj que ha recibido el contraejemplo c lo retiene en su base de casos y genera un nuevo argumento αt+1 j que tiene en cuenta c, e informa al resto de los agentes enviando assert(αt+1 j) a todos ellos. Entonces, Ai envía el token al siguiente agente, comienza una nueva ronda t + 1 y el protocolo vuelve al paso 2. 5. El protocolo finaliza generando una predicción conjunta, de la siguiente manera: si los argumentos en Ht están de acuerdo, entonces su predicción es la predicción conjunta; de lo contrario, se utiliza un mecanismo de votación para decidir la predicción conjunta. El mecanismo de votación utiliza la medida de confianza conjunta como los pesos de votación, de la siguiente manera: S = arg max Sk∈S αi∈Ht|αi.S=Sk C(αi) Además, para evitar iteraciones infinitas, si un agente envía dos veces el mismo argumento o contraargumento al mismo agente, el mensaje no se considera. 8. EJEMPLIFICACIÓN Consideremos un sistema compuesto por tres agentes A1, A2 y A3. Uno de los agentes, A1, recibe un problema P para resolver y decide usar AMAL para resolverlo. Por esa razón, invita a A2 y A3 a participar en el proceso de argumentación. Aceptan la invitación y comienza el protocolo de argumentación. Inicialmente, cada agente genera su predicción individual para P y la transmite a los otros agentes. Por lo tanto, todos ellos pueden calcular H0 = α0 1, α0 2, α0 3. En particular, en este ejemplo: • α0 1 = A1, P, hadromerida, D1 • α0 2 = A2, P, astrophorida, D2 • α0 3 = A3, P, axinellida, D3 A1 comienza poseyendo el token e intenta generar argumentos en contra de α0 2 y α0 3, pero no tiene éxito; sin embargo, tiene un contraejemplo c13 para α0 3. Por lo tanto, A1 envía el mensaje rebut(c13, α03) a A3. A3 incorpora c13 en su base de casos e intenta resolver el problema P nuevamente, ahora teniendo en cuenta c13. A3 elabora la predicción justificada α1 3 = A3, P, hadromerida, D4, y la transmite al resto de los agentes con el mensaje assert(α1 3). Por lo tanto, todos ellos conocen el nuevo H1 = α0 1, α0 2, α1 3. La ronda 1 comienza y A2 obtiene el token. A2 intenta generar contraargumentos para α0 1 y α1 3 y solo logra generar un contraargumento β1 2 = A2, P, astrophorida, D5 contra α1 3. El contraargumento se envía a A3 con el mensaje rebut(β1 2 , α1 3). El agente A3 recibe el contraargumento y evalúa su confianza local. El resultado es que la confianza individual del contraargumento β1 2 es menor que la confianza local de α1 3. Por lo tanto, A3 no acepta el contraargumento, y así H2 = α0 1, α0 2, α1 3. La ronda 2 comienza y A3 recibe el token. A3 genera un contraargumento β2 3 = A3, P, hadromerida, D6 para α0 2 y lo envía a A2 con el mensaje refutar(β2 3 , α0 2). El agente A2 recibe el contraargumento y evalúa su confianza local. El resultado es que la confianza local del contraargumento β2 3 es mayor que la confianza local de α0 2. Por lo tanto, A2 acepta el contraargumento e informa al resto de los agentes con el mensaje assert(β2 3). Después de eso, H3 = α0 1, β2 3 , α1 3. En la Ronda 3, dado que todos los agentes están de acuerdo (todas las predicciones justificadas en H3 predicen hadromerida como la clase de solución), el protocolo finaliza, y A1 (el agente que recibió el problema) considera hadromerida como la solución conjunta para el problema P. 9. EVALUACIÓN EXPERIMENTAL 980 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) SPONGE 75 77 79 81 83 85 87 89 91 2 3 4 5 AMAL Votación Individual SOYBEAN 55 60 65 70 75 80 85 90 2 3 4 5 AMAL Votación Individual Figura 5: Precisión individual y conjunta para 2 a 5 agentes. En esta sección evaluamos empíricamente el marco de argumentación AMAL. Hemos realizado experimentos en dos conjuntos de datos diferentes: soja (del repositorio de aprendizaje automático de UCI) y esponja (un conjunto de datos relacional). El conjunto de datos de soja tiene 307 ejemplos y 19 clases de solución, mientras que el conjunto de datos de esponja tiene 280 ejemplos y 3 clases de solución. En una ejecución experimental, el conjunto de datos se divide en 2 conjuntos: el conjunto de entrenamiento y el conjunto de prueba. Los ejemplos del conjunto de entrenamiento están distribuidos entre 5 agentes diferentes sin replicación, es decir, no hay ningún ejemplo compartido por dos agentes. En la etapa de pruebas, los problemas en el conjunto de pruebas llegan aleatoriamente a uno de los agentes, y su objetivo es predecir la solución correcta. Los experimentos están diseñados para probar dos hipótesis: (H1) que la argumentación es un marco útil para la deliberación conjunta y puede mejorar sobre otros métodos típicos como la votación; y (H2) que el aprendizaje a través de la comunicación mejora el rendimiento individual de un agente de aprendizaje que participa en un proceso de argumentación. Además, también esperamos que la mejora lograda a través de la argumentación aumente a medida que aumente el número de agentes que participan en la argumentación (ya que se tomará en cuenta más información). En relación con H1 (la argumentación es un marco útil para la deliberación conjunta), realizamos 4 experimentos, utilizando 2, 3, 4 y 5 agentes respectivamente (en todos los experimentos cada agente tiene un 20% de los datos de entrenamiento, ya que el entrenamiento siempre se distribuye entre 5 agentes). La Figura 5 muestra el resultado de esos experimentos en los conjuntos de datos de esponjas y soja. La precisión de la clasificación se representa en el eje vertical, y en el eje horizontal se muestra el número de agentes que participaron en los procesos de argumentación. Para cada número de agentes, se muestran tres barras: individual, Voting y AMAL. La barra individual muestra la precisión promedio de las predicciones de los agentes individuales; la barra de votación muestra la precisión promedio de la predicción conjunta lograda mediante votación pero sin argumentación; y finalmente la barra de AMAL muestra la precisión promedio de la predicción conjunta utilizando argumentación. Los resultados mostrados son el promedio de 5 ejecuciones de validación cruzada de 10 pliegues. La Figura 5 muestra que la colaboración (votación y AMAL) supera el rendimiento de la resolución individual de problemas. Además, como esperábamos, la precisión mejora a medida que más agentes colaboran, ya que se tiene en cuenta más información. También podemos ver que AMAL siempre supera al voto estándar, demostrando que las decisiones conjuntas se basan en una mejor información proporcionada por el proceso de argumentación. Por ejemplo, la precisión conjunta para 2 agentes en el conjunto de datos de esponjas es del 87.57% para AMAL y del 86.57% para la votación (mientras que la precisión individual es solo del 80.07%). Además, la mejora lograda por AMAL sobre Voting es aún mayor en el conjunto de datos de soja. La razón es que el conjunto de datos de soja es más difícil (en el sentido de que los agentes necesitan más datos para producir buenas predicciones). Estos resultados experimentales muestran que AMAL aprovecha de manera efectiva la oportunidad de mejora: la precisión es mayor solo porque más agentes han cambiado su opinión durante la argumentación (de lo contrario, lograrían el mismo resultado que la Votación). En relación con H2 (el aprendizaje a partir de la comunicación en procesos de argumentación mejora la predicción individual), llevamos a cabo el siguiente experimento: inicialmente, distribuimos un 25% del conjunto de entrenamiento entre los cinco agentes; después, el resto de los casos en el conjunto de entrenamiento se envía a los agentes uno por uno; cuando un agente recibe un nuevo caso de entrenamiento, tiene varias opciones: el agente puede descartarlo, el agente puede retenerlo, o el agente puede usarlo para participar en un proceso de argumentación. La Figura 6 muestra el resultado de ese experimento para los dos conjuntos de datos. La Figura 6 contiene tres gráficos, donde NL (no aprendizaje) muestra la precisión de un agente sin aprendizaje alguno; L (aprendizaje) muestra la evolución de la precisión de clasificación individual cuando los agentes aprenden reteniendo los casos de entrenamiento que reciben individualmente (notar que cuando todos los casos de entrenamiento han sido retenidos al 100%, la precisión debería ser igual a la de la Figura 5 para los agentes individuales); y finalmente LFC (aprendizaje a partir de la comunicación) muestra la evolución de la precisión de clasificación individual de los agentes de aprendizaje que también aprenden reteniendo los contraejemplos recibidos durante la argumentación (es decir, aprenden tanto de ejemplos de entrenamiento como de contraejemplos). La Figura 6 muestra que si un agente Ai también aprende de la comunicación, Ai puede mejorar significativamente su rendimiento individual con solo un pequeño número de casos adicionales (seleccionados como contraejemplos relevantes para Ai durante la argumentación). Por ejemplo, en el conjunto de datos de soja, los agentes individuales han logrado una precisión del 70.62% cuando también aprenden de la comunicación, en comparación con una precisión del 59.93% cuando solo aprenden de su experiencia individual. El número de casos aprendidos a través de la comunicación depende de las propiedades del conjunto de datos: en el conjunto de datos de esponjas, los agentes solo han retenido muy pocos casos adicionales y han mejorado significativamente la precisión individual; es decir, retienen en promedio 59.96 casos (en comparación con los 50.4 casos retenidos si no aprenden de la comunicación). En el conjunto de datos de soja se aprenden más contraejemplos para mejorar significativamente la precisión individual, es decir, retienen en promedio 87.16 casos (en comparación con 55.27 casos retenidos si no aprenden de la comunicación). Finalmente, el hecho de que ambos conjuntos de datos muestren una mejora significativa señala la naturaleza adaptativa del enfoque basado en la argumentación para el aprendizaje a partir de la comunicación: los casos útiles se seleccionan como contraejemplos (y no más de los necesarios), y tienen el efecto deseado. 10. TRABAJO RELACIONADO En cuanto al RCB en un entorno multiagente, la primera investigación se centró en la recuperación de casos negociada [11] entre grupos de agentes. Nuestro trabajo sobre el aprendizaje basado en casos multiagente comenzó en 1999; más tarde Mc Ginty y Smyth presentaron un enfoque colaborativo de CBR multiagente (CCBR) para la planificación. Finalmente, otro enfoque interesante es el razonamiento de múltiples bases de casos (MCBR) [5], que trata sobre The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 981 ESPONJA 60 65 70 75 80 85 25% 40% 55% 70% 85% 100% LFC L NL SOJA 20 30 40 50 60 70 80 90 25% 40% 55% 70% 85% 100% LFC L NL Figura 6: Aprendizaje a partir de la comunicación resultante de la argumentación en un sistema compuesto por 5 agentes. sistemas distribuidos donde hay varias bases de casos disponibles para la misma tarea y aborda los problemas de adaptación entre bases de casos. La principal diferencia es que nuestro enfoque MAC es una forma de distribuir el proceso de Reutilización de CBR (utilizando un sistema de votación) mientras que la Recuperación se realiza individualmente por cada agente; sin embargo, los otros enfoques de CBR multiagente se centran en distribuir el proceso de Recuperación. La investigación sobre la argumentación en MAS se centra en varios temas como a) lógicas, protocolos y lenguajes que respaldan la argumentación, b) selección de argumentos y c) interpretación de argumentos. Los enfoques para la lógica y los lenguajes que respaldan la argumentación incluyen la lógica no definitiva [4] y los modelos BDI [13]. Aunque la selección de argumentos es un aspecto clave de la argumentación automatizada (ver [12] y [13]), la mayoría de las investigaciones se han centrado en las relaciones de preferencia entre argumentos. En nuestro marco de trabajo hemos abordado tanto la selección de argumentos como las relaciones de preferencia utilizando un enfoque basado en casos. 11. CONCLUSIONES Y TRABAJO FUTURO En este artículo hemos presentado un marco basado en argumentación para el <br>aprendizaje multiagente</br>. Específicamente, hemos presentado AMAL, un marco que permite a un grupo de agentes de aprendizaje discutir sobre la solución de un problema dado y hemos demostrado cómo las capacidades de aprendizaje pueden ser utilizadas para generar argumentos y contraargumentos. La evaluación experimental muestra que la cantidad aumentada de información proporcionada a los agentes por el proceso de argumentación aumenta su precisión predictiva, especialmente cuando un número adecuado de agentes participa en la argumentación. Las principales contribuciones de este trabajo son: a) un marco de argumentación para agentes de aprendizaje; b) una relación de preferencia basada en casos sobre argumentos, basada en el cálculo de una estimación general de confianza de los argumentos; c) una política basada en casos para generar contraargumentos y seleccionar contraejemplos; y d) un enfoque basado en la argumentación para aprender a partir de la comunicación. Finalmente, en los experimentos presentados aquí, un agente de aprendizaje retendría todos los contraejemplos presentados por el otro agente; sin embargo, esta es una política de retención de casos muy simple, y nos gustaría experimentar con políticas más informadas, con el objetivo de que los agentes de aprendizaje individuales puedan mejorar significativamente utilizando solo un pequeño conjunto de casos propuestos por otros agentes. Finalmente, nuestro enfoque se centra en el aprendizaje perezoso, y los trabajos futuros tienen como objetivo incorporar el aprendizaje inductivo ansioso dentro del marco argumentativo para el aprendizaje a partir de la comunicación. 12. REFERENCIAS [1] Agnar Aamodt y Enric Plaza. Razonamiento basado en casos: Aspectos fundamentales, variaciones metodológicas y enfoques de sistemas. Comunicaciones de Inteligencia Artificial, 7(1):39-59, 1994. [2] E. Armengol y E. Plaza. Inducción perezosa de descripciones para el aprendizaje basado en casos relacionales. En ECML2001, páginas 13-24, 2001. [3] Gerhard Brewka. Sistemas de argumentación dinámica: Un modelo formal de procesos de argumentación basado en el cálculo de situaciones. Revista de Lógica y Computación, 11(2):257-282, 2001. [4] Carlos I. Chesñevar y Guillermo R. Simari. Formalizando la argumentación no definitiva utilizando sistemas deductivos etiquetados. Revista de Ciencias de la Computación y Tecnología, 1(4):18-33, 2000. [5] D. Leake y R. Sooriamurthi. Seleccionando estrategias automáticamente para el razonamiento multi-base de casos. En S. Craw y A. Preece, editores, ECCBR2002, páginas 204-219, Berlín, 2002. Springer Verlag. [6] Francisco J. Martín, Enric Plaza y Josep-Lluis Arcos. Reutilización de conocimiento y experiencia a través de comunicaciones entre agentes competentes (pares). Revista Internacional de Ingeniería de Software e Ingeniería del Conocimiento, 9(3):319-341, 1999. [7] Lorraine McGinty y Barry Smyth. Razonamiento basado en casos colaborativo: Aplicaciones en la planificación de rutas personalizadas. En I. Watson y Q. Yang, editores, ICCBR, número 2080 en LNAI, páginas 362-376. Springer-Verlag, 2001. [8] Santi Ontañón y Enric Plaza. Aprendizaje multiagente basado en justificaciones. En ICML2003, páginas 576-583. Morgan Kaufmann, 2003. [9] Enric Plaza, Eva Armengol y Santiago Ontañón. El poder explicativo de la similitud simbólica en el razonamiento basado en casos. Revisión de Inteligencia Artificial, 24(2):145-161, 2005. [10] David Poole. En la comparación de teorías: prefiriendo la explicación más específica. En IJCAI-85, páginas 144-147, 1985. [11] M V Nagendra Prassad, Victor R Lesser y Susan Lander. Recuperación y razonamiento en bases de casos distribuidas. Informe técnico, Departamento de Ciencias de la Computación de UMass, 1995. [12] K. Sycara S. Kraus y A. Evenchik. Alcanzando acuerdos a través de la argumentación: un modelo lógico y su implementación. Revista de Inteligencia Artificial, 104:1-69, 1998. [13] N. R. Jennings, S. Parsons, C. Sierra. Agentes que razonan y negocian mediante argumentos. Revista de Lógica y Computación, 8:261-292, 1998. [14] Bruce A. Wooley. Componente de explicación de sistemas de software. ACM CrossRoads, 5.1, 1998. 982 The Sixth Intl. \n\nACM CrossRoads, 5.1, 1998. 982 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "argumentation": {
            "translated_key": "argumentación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning and Joint Deliberation through <br>argumentation</br> in Multi-Agent Systems Santi Ontañón CCL, Cognitive Computing Lab Georgia Institute of Technology Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Artificial Intelligence Research Institute CSIC, Spanish Council for Scientific Research Campus UAB, 08193 Bellaterra, Catalonia (Spain) enric@iiia.csic.es ABSTRACT In this paper we will present an <br>argumentation</br> framework for learning agents (AMAL) designed for two purposes: (1) for joint deliberation, and (2) for learning from communication.",
                "The AMAL framework is completely based on learning from examples: the argument preference relation, the argument generation policy, and the counterargument generation policy are case-based techniques.",
                "For join deliberation, learning agents share their experience by forming a committee to decide upon some joint decision.",
                "We experimentally show that the <br>argumentation</br> among committees of agents improves both the individual and joint performance.",
                "For learning from communication, an agent engages into arguing with other agents in order to contrast its individual hypotheses and receive counterexamples; the <br>argumentation</br> process improves their learning scope and individual performance.",
                "Categories and Subject Descriptors I.2.6 [Artificial Intelligence]: Learning; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems, Intelligent Agents 1.",
                "INTRODUCTION <br>argumentation</br> frameworks for multi-agent systems can be used for different purposes like joint deliberation, persuasion, negotiation, and conflict resolution.",
                "In this paper we will present an <br>argumentation</br> framework for learning agents, and show that it can be used for two purposes: (1) joint deliberation, and (2) learning from communication.",
                "<br>argumentation</br>-based joint deliberation involves discussion over the outcome of a particular situation or the appropriate course of action for a particular situation.",
                "Learning agents are capable of learning from experience, in the sense that past examples (situations and their outcomes) are used to predict the outcome for the situation at hand.",
                "However, since individual agents experience may be limited, individual knowledge and prediction accuracy is also limited.",
                "Thus, learning agents that are capable of arguing their individual predictions with other agents may reach better prediction accuracy after such an <br>argumentation</br> process.",
                "Most existing <br>argumentation</br> frameworks for multi-agent systems are based on deductive logic or some other deductive logic formalism specifically designed to support <br>argumentation</br>, such as default logic [3]).",
                "Usually, an argument is seen as a logical statement, while a counterargument is an argument offered in opposition to another argument [4, 13]; agents use a preference relation to resolve conflicting arguments.",
                "However, logic-based <br>argumentation</br> frameworks assume agents with preloaded knowledge and preference relation.",
                "In this paper, we focus on an <br>argumentation</br>-based Multi-Agent Learning (AMAL) framework where both knowledge and preference relation are learned from experience.",
                "Thus, we consider a scenario with agents that (1) work in the same domain using a shared ontology, (2) are capable of learning from examples, and (3) communicate using an argumentative framework.",
                "Having learning capabilities allows agents effectively use a specific form of counterargument, namely the use of counterexamples.",
                "Counterexamples offer the possibility of agents learning during the <br>argumentation</br> process.",
                "Moreover, learning agents allow techniques that use learnt experience to generate adequate arguments and counterarguments.",
                "Specifically, we will need to address two issues: (1) how to define a technique to generate arguments and counterarguments from examples, and (2)how to define a preference relation over two conflicting arguments that have been induced from examples.",
                "This paper presents a case-based approach to address both issues.",
                "The agents use case-based reasoning (CBR) [1] to learn from past cases (where a case is a situation and its outcome) in order to predict the outcome of a new situation.",
                "We propose an <br>argumentation</br> protocol inside the AMAL framework at supports agents in reaching a joint prediction over a specific situation or problem - moreover, the reasoning needed to support the <br>argumentation</br> process will also be based on cases.",
                "In particular, we present two case-based measures, one for generating the arguments and counterarguments adequate to a particular situation and another for determining preference relation among arguments.",
                "Finally, we evaluate (1) if <br>argumentation</br> between learning agents can produce a joint prediction that improves over individual learning performance and (2) if learning from the counterexamples conveyed during the <br>argumentation</br> process increases the individual performance with precisely those cases being used while arguing among them.",
                "The paper is structured as follows.",
                "Section 2 discusses the relation among <br>argumentation</br>, collaboration and learning.",
                "Then Section 3 introduces our multi-agent CBR (MAC) framework and the notion of justified prediction.",
                "After that, Section 4 formally defines our <br>argumentation</br> framework.",
                "Sections 5 and 6 present our case-based preference relation and argument generation policies respectively.",
                "Later, Section 7 presents the <br>argumentation</br> protocol in our AMAL framework.",
                "After that, Section 8 presents an exemplification of the <br>argumentation</br> framework.",
                "Finally, Section 9 presents an empirical evaluation of our two main hypotheses.",
                "The paper closes with related work and conclusions sections. 2.",
                "<br>argumentation</br>COLLABORATION AND LEARNING Both learning and collaboration are ways in which an agent can improve individual performance.",
                "In fact, there is a clear parallelism between learning and collaboration in multi-agent systems, since both are ways in which agents can deal with their shortcomings.",
                "Let us show which are the main motivations that an agent can have to learn or to collaborate. • Motivations to learn: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems. • Motivations to collaborate: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems, - Increase the range of accessible resources.",
                "Looking at the above lists of motivation, we can easily see that learning and collaboration are very related in multi-agent systems.",
                "In fact, with the exception of the last item in the motivations to collaborate list, they are two extremes of a continuum of strategies to improve performance.",
                "An agent may choose to increase performance by learning, by collaborating, or by finding an intermediate point that combines learning and collaboration in order to improve performance.",
                "In this paper we will propose AMAL, an <br>argumentation</br> framework for learning agents, and will also also show how AMAL can be used both for learning from communication and for solving problems in a collaborative way: • Agents can solve problems in a collaborative way via engaging an <br>argumentation</br> process about the prediction for the situation at hand.",
                "Using this collaboration, the prediction can be done in a more informed way, since the information known by several agents has been taken into account. • Agents can also learn from communication with other agents by engaging an <br>argumentation</br> process.",
                "Agents that engage in such <br>argumentation</br> processes can learn from the arguments and counterexamples received from other agents, and use this information for predicting the outcomes of future situations.",
                "In the rest of this paper we will propose an <br>argumentation</br> framework and show how it can be used both for learning and for solving problems in a collaborative way. 3.",
                "MULTI-AGENT CBR SYSTEMS A Multi-Agent Case Based Reasoning System (MAC) M = {(A1, C1), ..., (An, Cn)} is a multi-agent system composed of A = {Ai, ..., An}, a set of CBR agents, where each agent Ai ∈ A possesses an individual case base Ci.",
                "Each individual agent Ai in a MAC is completely autonomous and each agent Ai has access only to its individual and private case base Ci.",
                "A case base Ci = {c1, ..., cm} is a collection of cases.",
                "Agents in a MAC system are able to individually solve problems, but they can also collaborate with other agents to solve problems.",
                "In this framework, we will restrict ourselves to analytical tasks, i.e. tasks like classification, where the solution of a problem is achieved by selecting a solution class from an enumerated set of solution classes.",
                "In the following we will note the set of all the solution classes by S = {S1, ..., SK }.",
                "Therefore, a case c = P, S is a tuple containing a case description P and a solution class S ∈ S. In the following, we will use the terms problem and case description indistinctly.",
                "Moreover, we will use the dot notation to refer to elements inside a tuple; e.g., to refer to the solution class of a case c, we will write c.S.",
                "Therefore, we say a group of agents perform joint deliberation, when they collaborate to find a joint solution by means of an <br>argumentation</br> process.",
                "However, in order to do so, an agent has to be able to justify its prediction to the other agents (i.e. generate an argument for its predicted solution that can be examined and critiqued by the other agents).",
                "The next section addresses this issue. 3.1 Justified Predictions Both expert systems and CBR systems may have an explanation component [14] in charge of justifying why the system has provided a specific answer to the user.",
                "The line of reasoning of the system can then be examined by a human expert, thus increasing the reliability of the system.",
                "Most of the existing work on explanation generation focuses on generating explanations to be provided to the user.",
                "However, in our approach we use explanations (or justifications) as a tool for improving communication and coordination among agents.",
                "We are interested in justifications since they can be used as arguments.",
                "For that purpose, we will benefit from the ability of some machine learning methods to provide justifications.",
                "A justification built by a CBR method after determining that the solution of a particular problem P was Sk is a description that contains the relevant information from the problem P that the CBR method has considered to predict Sk as the solution of P. In particular, CBR methods work by retrieving similar cases to the problem at hand, and then reusing their solutions for the current problem, expecting that since the problem and the cases are similar, the solutions will also be similar.",
                "Thus, if a CBR method has retrieved a set of cases C1, ..., Cn to solve a particular problem P the justification built will contain the relevant information from the problem P that made the CBR system retrieve that particular set of cases, i.e. it will contain the relevant information that P and C1, ..., Cn have in common.",
                "For example, Figure 1 shows a justification build by a CBR system for a toy problem (in the following sections we will show justifications for real problems).",
                "In the figure, a problem has two attributes (Traffic_light, and Cars_passing), the retrieval mechanism of the CBR system notices that by considering only the attribute Traffic_light, it can retrieve two cases that predict the same solution: wait.",
                "Thus, since only this attribute has been used, it is the only one appearing in the justification.",
                "The values of the rest of attributes are irrelevant, since whatever their value the solution class would have been the same. 976 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Problem Traffic_light: red Cars_passing: no Case 1 Traffic_light: red Cars_passing: no Solution: wait Case 3 Traffic_light: red Cars_passing: yes Solution: wait Case 4 Traffic_light: green Cars_passing: yes Solution: wait Case 2 Traffic_light: green Cars_passing: no Solution: cross Retrieved cases Solution: wait Justification Traffic_light: red Figure 1: An example of justification generation in a CBR system.",
                "Notice that, since the only relevant feature to decide is Traffic_light (the only one used to retrieve cases), it is the only one appearing in the justification.",
                "In general, the meaning of a justification is that all (or most of) the cases in the case base of an agent that satisfy the justification (i.e. all the cases that are subsumed by the justification) belong to the predicted solution class.",
                "In the rest of the paper, we will use to denote the subsumption relation.",
                "In our work, we use LID [2], a CBR method capable of building symbolic justifications such as the one exemplified in Figure 1.",
                "When an agent provides a justification for a prediction, the agent generates a justified prediction: DEFINITION 3.1.",
                "A Justified Prediction is a tuple J = A, P, S, D where agent A considers S the correct solution for problem P, and that prediction is justified a symbolic description D such that J.D J.P. Justifications can have many uses for CBR systems [8, 9].",
                "In this paper, we are going to use justifications as arguments, in order to allow learning agents to engage in <br>argumentation</br> processes. 4.",
                "ARGUMENTS AND COUNTERARGUMENTS For our purposes an argument α generated by an agent A is composed of a statement S and some evidence D supporting S as correct.",
                "In the remainder of this section we will see how this general definition of argument can be instantiated in specific kind of arguments that the agents can generate.",
                "In the context of MAC systems, agents argue about predictions for new problems and can provide two kinds of information: a) specific cases P, S , and b) justified predictions: A, P, S, D .",
                "Using this information, we can define three types of arguments: justified predictions, counterarguments, and counterexamples.",
                "A justified prediction α is generated by an agent Ai to argue that Ai believes that the correct solution for a given problem P is α.S, and the evidence provided is the justification α.D.",
                "In the example depicted in Figure 1, an agent Ai may generate the argument α = Ai, P, Wait, (Traffic_light = red) , meaning that the agent Ai believes that the correct solution for P is Wait because the attribute Traffic_light equals red.",
                "A counterargument β is an argument offered in opposition to another argument α.",
                "In our framework, a counterargument consists of a justified prediction Aj, P, S , D generated by an agent Aj with the intention to rebut an argument α generated by another agent Ai, that endorses a solution class S different from that of α.S for the problem at hand and justifies this with a justification D .",
                "In the example in Figure 1, if an agent generates the argument α = Ai, P, Walk, (Cars_passing = no) , an agent that thinks that the correct solution is Wait might answer with the counterargument β = Aj, P, Wait, (Cars_passing = no ∧ Traffic_light = red) , meaning that, although there are no cars passing, the traffic light is red, and the street cannot be crossed.",
                "A counterexample c is a case that contradicts an argument α.",
                "Thus a counterexample is also a counterargument, one that states that a specific argument α is not always true, and the evidence provided is the case c. Specifically, for a case c to be a counterexample of an argument α, the following conditions have to be met: α.D c and α.S = c.S, i.e. the case must satisfy the justification α.D and the solution of c must be different than the predicted by α.",
                "By exchanging arguments and counterarguments (including counterexamples), agents can argue about the correct solution of a given problem, i.e. they can engage a joint deliberation process.",
                "However, in order to do so, they need a specific interaction protocol, a preference relation between contradicting arguments, and a decision policy to generate counterarguments (including counterexamples).",
                "In the following sections we will present these elements. 5.",
                "PREFERENCE RELATION A specific argument provided by an agent might not be consistent with the information known to other agents (or even to some of the information known by the agent that has generated the justification due to noise in training data).",
                "For that reason, we are going to define a preference relation over contradicting justified predictions based on cases.",
                "Basically, we will define a confidence measure for each justified prediction (that takes into account the cases owned by each agent), and the justified prediction with the highest confidence will be the preferred one.",
                "The idea behind case-based confidence is to count how many of the cases in an individual case base endorse a justified prediction, and how many of them are counterexamples of it.",
                "The more the endorsing cases, the higher the confidence; and the more the counterexamples, the lower the confidence.",
                "Specifically, to assess the confidence of a justified prediction α, an agent obtains the set of cases in its individual case base that are subsumed by α.D.",
                "With them, an agent Ai obtains the Y (aye) and N (nay) values: • Y Ai α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by the justification α.D that belong to the solution class α.S, • NAi α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by justification α.D that do not belong to that solution class.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 977 + + + + + +  - - + Figure 2: Confidence of arguments is evaluated by contrasting them against the case bases of the agents.",
                "An agent estimates the confidence of an argument as: CAi (α) = Y Ai α 1 + Y Ai α + NAi α i.e. the confidence on a justified prediction is the number of endorsing cases divided by the number of endorsing cases plus counterexamples.",
                "Notice that we add 1 to the denominator, this is to avoid giving excessively high confidences to justified predictions whose confidence has been computed using a small number of cases.",
                "Notice that this correction follows the same idea than the Laplace correction to estimate probabilities.",
                "Figure 2 illustrates the individual evaluation of the confidence of an argument, in particular, three endorsing cases and one counterexample are found in the case base of agents Ai, giving an estimated confidence of 0.6 Moreover, we can also define the joint confidence of an argument α as the confidence computed using the cases present in the case bases of all the agents in the group: C(α) = i Y Ai α 1 + i Y Ai α + NAi α Notice that, to collaboratively compute the joint confidence, the agents only have to make public the aye and nay values locally computed for a given argument.",
                "In our framework, agents use this joint confidence as the preference relation: a justified prediction α is preferred over another one β if C(α) ≥ C(β). 6.",
                "GENERATION OF ARGUMENTS In our framework, arguments are generated by the agents from cases, using learning methods.",
                "Any learning method able to provide a justified prediction can be used to generate arguments.",
                "For instance, decision trees and LID [2] are suitable learning methods.",
                "Specifically, in the experiments reported in this paper agents use LID.",
                "Thus, when an agent wants to generate an argument endorsing that a specific solution class is the correct solution for a problem P, it generates a justified prediction as explained in Section 3.1.",
                "For instance, Figure 3 shows a real justification generated by LID after solving a problem P in the domain of marine sponges identification.",
                "In particular, Figure 3 shows how when an agent receives a new problem to solve (in this case, a new sponge to determine its order), the agent uses LID to generate an argument (consisting on a justified prediction) using the cases in the case base of the agent.",
                "The justification shown in Figure 3 can be interpreted saying that the predicted solution is hadromerida because the smooth form of the megascleres of the spiculate skeleton of the sponge is of type tylostyle, the spikulate skeleton of the sponge has no uniform length, and there is no gemmules in the external features of the sponge.",
                "Thus, the argument generated will be α = A1, P, hadromerida, D1 . 6.1 Generation of Counterarguments As previously stated, agents may try to rebut arguments by generating counterargument or by finding counterexamples.",
                "Let us explain how they can be generated.",
                "An agent Ai wants to generate a counterargument β to rebut an argument α when α is in contradiction with the local case base of Ai.",
                "Moreover, while generating such counterargument β, Ai expects that β is preferred over α.",
                "For that purpose, we will present a specific policy to generate counterarguments based on the specificity criterion [10].",
                "The specificity criterion is widely used in deductive frameworks for <br>argumentation</br>, and states that between two conflicting arguments, the most specific should be preferred since it is, in principle, more informed.",
                "Thus, counterarguments generated based on the specificity criterion are expected to be preferable (since they are more informed) to the arguments they try to rebut.",
                "However, there is no guarantee that such counterarguments will always win, since, as we have stated in Section 5, agents in our framework use a preference relation based on joint confidence.",
                "Moreover, one may think that it would be better that the agents generate counterarguments based on the joint confidence preference relation; however it is not obvious how to generate counterarguments based on joint confidence in an efficient way, since collaboration is required in order to evaluate joint confidence.",
                "Thus, the agent generating the counterargument should constantly communicate with the other agents at each step of the induction algorithm used to generate counterarguments (presently one of our future research lines).",
                "Thus, in our framework, when an agent wants to generate a counterargument β to an argument α, β has to be more specific than α (i.e. α.D < β.D).",
                "The generation of counterarguments using the specificity criterion imposes some restrictions over the learning method, although LID or ID3 can be easily adapted for this task.",
                "For instance, LID is an algorithm that generates a description starting from scratch and heuristically adding features to that term.",
                "Thus, at every step, the description is made more specific than in the previous step, and the number of cases that are subsumed by that description is reduced.",
                "When the description covers only (or almost only) cases of a single solution class LID terminates and predicts that solution class.",
                "To generate a counterargument to an argument α LID just has to use as starting point the description α.D instead of starting from scratch.",
                "In this way, the justification provided by LID will always be subsumed by α.D, and thus the resulting counterargument will be more specific than α.",
                "However, notice that LID may sometimes not be able to generate counterarguments, since LID may not be able to specialize the description α.D any further, or because the agent Ai has no case inCi that is subsumed by α.D.",
                "Figure 4 shows how an agent A2 that disagreed with the argument shown in Figure 3, generates a counterargument using LID.",
                "Moreover, Figure 4 shows the generation of a counterargument β1 2 for the argument α0 1 (in Figure 3) that is a specialization of α0 1. 978 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Solution: hadromerida Justification: D1 Sponge Spikulate skeleton External features External features Gemmules: no Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Case Base of A1 LID New sponge P Figure 3: Example of a real justification generated by LID in the marine sponges data set.",
                "Specifically, in our experiments, when an agent Ai wants to rebut an argument α, uses the following policy: 1.",
                "Agent Ai uses LID to try to find a counterargument β more specific than α; if found, β is sent to the other agent as a counterargument of α. 2.",
                "If not found, then Ai searches for a counterexample c ∈ Ci of α.",
                "If a case c is found, then c is sent to the other agent as a counterexample of α. 3.",
                "If no counterexamples are found, then Ai cannot rebut the argument α. 7.",
                "<br>argumentation</br>-BASED MULTI-AGENT LEARNING The interaction protocol of AMAL allows a group of agents A1, ..., An to deliberate about the correct solution of a problem P by means of an <br>argumentation</br> process.",
                "If the <br>argumentation</br> process arrives to a consensual solution, the joint deliberation ends; otherwise a weighted vote is used to determine the joint solution.",
                "Moreover, AMAL also allows the agents to learn from the counterexamples received from other agents.",
                "The AMAL protocol consists on a series of rounds.",
                "In the initial round, each agent states which is its individual prediction for P. Then, at each round an agent can try to rebut the prediction made by any of the other agents.",
                "The protocol uses a token passing mechanism so that agents (one at a time) can send counterarguments or counterexamples if they disagree with the prediction made by any other agent.",
                "Specifically, each agent is allowed to send one counterargument or counterexample each time he gets the token (notice that this restriction is just to simplify the protocol, and that it does not restrict the number of counterargument an agent can sent, since they can be delayed for subsequent rounds).",
                "When an agent receives a counterargument or counterexample, it informs the other agents if it accepts the counterargument (and changes its prediction) or not.",
                "Moreover, agents have also the opportunity to answer to counterarguments when they receive the token, by trying to generate a counterargument to the counterargument.",
                "When all the agents have had the token once, the token returns to the first agent, and so on.",
                "If at any time in the protocol, all the agents agree or during the last n rounds no agent has generated any counterargument, the protocol ends.",
                "Moreover, if at the end of the <br>argumentation</br> the agents have not reached an agreement, then a voting mechanism that uses the confidence of each prediction as weights is used to decide the final solution (Thus, AMAL follows the same mechanism as human committees, first each individual member of a committee exposes his arguments and discuses those of the other members (joint deliberation), and if no consensus is reached, then a voting mechanism is required).",
                "At each iteration, agents can use the following performatives: • assert(α): the justified prediction held during the next round will be α.",
                "An agent can only hold a single prediction at each round, thus is multiple asserts are send, only the last one is considered as the currently held prediction. • rebut(β, α): the agent has found a counterargument β to the prediction α.",
                "We will define Ht = αt 1, ..., αt n as the predictions that each of the n agents hold at a round t. Moreover, we will also define contradict(αt i) = {α ∈ Ht|α.S = αt i.S} as the set of contradicting arguments for an agent Ai in a round t, i.e. the set of arguments at round t that support a different solution class than αt i.",
                "The protocol is initiated because one of the agents receives a problem P to be solved.",
                "After that, the agent informs all the other agents about the problem P to solve, and the protocol starts: 1.",
                "At round t = 0, each one of the agents individually solves P, and builds a justified prediction using its own CBR method.",
                "Then, each agent Ai sends the performative assert(α0 i ) to the other agents.",
                "Thus, the agents know H0 = α0 i , ..., α0 n .",
                "Once all the predictions have been sent the token is given to the first agent A1. 2.",
                "At each round t (other than 0), the agents check whether their arguments in Ht agree.",
                "If they do, the protocol moves to step 5.",
                "Moreover, if during the last n rounds no agent has sent any counterexample or counterargument, the protocol also moves to step 5.",
                "Otherwise, the agent Ai owner of the token tries to generate a counterargument for each of the opposing arguments in contradict(αt i) ⊆ Ht (see Section 6.1).",
                "Then, the counterargument βt i against the prediction αt j with the lowest confidence C(αt j) is selected (since αt j is the prediction more likely to be successfully rebutted). • If βt i is a counterargument, then, Ai locally compares αt i with βt i by assessing their confidence against its individual case base Ci (see Section 5) (notice that Ai is comparing its previous argument with the counterargument that Ai itself has just generated and that is about The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 979 Sponge Spikulate skeleton External features External features Gemmules: no Growing: Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Growing Grow: massive Case Base of A2 LID Solution: astrophorida Justification: D2 Figure 4: Generation of a counterargument using LID in the sponges data set. to send to Aj).",
                "If CAi (βt i ) > CAi (αt i), then Ai considers that βt i is stronger than its previous argument, changes its argument to βt i by sending assert(βt i ) to the rest of the agents (the intuition behind this is that since a counterargument is also an argument, Ai checks if the newly counterargument is a better argument than the one he was previously holding) and rebut(βt i , αt j) to Aj.",
                "Otherwise (i.e.",
                "CAi (βt i ) ≤ CAi (αt i)), Ai will send only rebut(βt i , αt j) to Aj.",
                "In any of the two situations the protocol moves to step 3. • If βt i is a counterexample c, then Ai sends rebut(c, αt j) to Aj.",
                "The protocol moves to step 4. • If Ai cannot generate any counterargument or counterexample, the token is sent to the next agent, a new round t + 1 starts, and the protocol moves to state 2. 3.",
                "The agent Aj that has received the counterargument βt i , locally compares it against its own argument, αt j, by locally assessing their confidence.",
                "If CAj (βt i ) > CAj (αt j), then Aj will accept the counterargument as stronger than its own argument, and it will send assert(βt i ) to the other agents.",
                "Otherwise (i.e.",
                "CAj (βt i ) ≤ CAj (αt j)), Aj will not accept the counterargument, and will inform the other agents accordingly.",
                "Any of the two situations start a new round t + 1, Ai sends the token to the next agent, and the protocol moves back to state 2. 4.",
                "The agent Aj that has received the counterexample c retains it into its case base and generates a new argument αt+1 j that takes into account c, and informs the rest of the agents by sending assert(αt+1 j ) to all of them.",
                "Then, Ai sends the token to the next agent, a new round t + 1 starts, and the protocol moves back to step 2. 5.",
                "The protocol ends yielding a joint prediction, as follows: if the arguments in Ht agree then their prediction is the joint prediction, otherwise a voting mechanism is used to decide the joint prediction.",
                "The voting mechanism uses the joint confidence measure as the voting weights, as follows: S = arg max Sk∈S αi∈Ht|αi.S=Sk C(αi) Moreover, in order to avoid infinite iterations, if an agent sends twice the same argument or counterargument to the same agent, the message is not considered. 8.",
                "EXEMPLIFICATION Let us consider a system composed of three agents A1, A2 and A3.",
                "One of the agents, A1 receives a problem P to solve, and decides to use AMAL to solve it.",
                "For that reason, invites A2 and A3 to take part in the <br>argumentation</br> process.",
                "They accept the invitation, and the <br>argumentation</br> protocol starts.",
                "Initially, each agent generates its individual prediction for P, and broadcasts it to the other agents.",
                "Thus, all of them can compute H0 = α0 1, α0 2, α0 3 .",
                "In particular, in this example: • α0 1 = A1, P, hadromerida, D1 • α0 2 = A2, P, astrophorida, D2 • α0 3 = A3, P, axinellida, D3 A1 starts owning the token and tries to generate counterarguments for α0 2 and α0 3, but does not succeed, however it has one counterexample c13 for α0 3.",
                "Thus, A1 sends the the message rebut( c13, α0 3) to A3.",
                "A3 incorporates c13 into its case base and tries to solve the problem P again, now taking c13 into consideration.",
                "A3 comes up with the justified prediction α1 3 = A3, P, hadromerida, D4 , and broadcasts it to the rest of the agents with the message assert(α1 3).",
                "Thus, all of them know the new H1 = α0 1, α0 2, α1 3 .",
                "Round 1 starts and A2 gets the token.",
                "A2 tries to generate counterarguments for α0 1 and α1 3 and only succeeds to generate a counterargument β1 2 = A2, P, astrophorida, D5 against α1 3.",
                "The counterargument is sent to A3 with the message rebut(β1 2 , α1 3).",
                "Agent A3 receives the counterargument and assesses its local confidence.",
                "The result is that the individual confidence of the counterargument β1 2 is lower than the local confidence of α1 3.",
                "Therefore, A3 does not accept the counterargument, and thus H2 = α0 1, α0 2, α1 3 .",
                "Round 2 starts and A3 gets the token.",
                "A3 generates a counterargument β2 3 = A3, P, hadromerida, D6 for α0 2 and sends it to A2 with the message rebut(β2 3 , α0 2).",
                "Agent A2 receives the counterargument and assesses its local confidence.",
                "The result is that the local confidence of the counterargument β2 3 is higher than the local confidence of α0 2.",
                "Therefore, A2 accepts the counterargument and informs the rest of the agents with the message assert(β2 3 ).",
                "After that, H3 = α0 1, β2 3 , α1 3 .",
                "At Round 3, since all the agents agree (all the justified predictions in H3 predict hadromerida as the solution class) The protocol ends, and A1 (the agent that received the problem) considers hadromerida as the joint solution for the problem P. 9.",
                "EXPERIMENTAL EVALUATION 980 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) SPONGE 75 77 79 81 83 85 87 89 91 2 3 4 5 AMAL Voting Individual SOYBEAN 55 60 65 70 75 80 85 90 2 3 4 5 AMAL Voting Individual Figure 5: Individual and joint accuracy for 2 to 5 agents.",
                "In this section we empirically evaluate the AMAL <br>argumentation</br> framework.",
                "We have made experiments in two different data sets: soybean (from the UCI machine learning repository) and sponge (a relational data set).",
                "The soybean data set has 307 examples and 19 solution classes, while the sponge data set has 280 examples and 3 solution classes.",
                "In an experimental run, the data set is divided in 2 sets: the training set and the test set.",
                "The training set examples are distributed among 5 different agents without replication, i.e. there is no example shared by two agents.",
                "In the testing stage, problems in the test set arrive randomly to one of the agents, and their goal is to predict the correct solution.",
                "The experiments are designed to test two hypotheses: (H1) that <br>argumentation</br> is a useful framework for joint deliberation and can improve over other typical methods such as voting; and (H2) that learning from communication improves the individual performance of a learning agent participating in an <br>argumentation</br> process.",
                "Moreover, we also expect that the improvement achieved from <br>argumentation</br> will increase as the number of agents participating in the <br>argumentation</br> increases (since more information will be taken into account).",
                "Concerning H1 (<br>argumentation</br> is a useful framework for joint deliberation), we ran 4 experiments, using 2, 3, 4, and 5 agents respectively (in all experiments each agent has a 20% of the training data, since the training is always distributed among 5 agents).",
                "Figure 5 shows the result of those experiments in the sponge and soybean data sets.",
                "Classification accuracy is plotted in the vertical axis, and in the horizontal axis the number of agents that took part in the <br>argumentation</br> processes is shown.",
                "For each number of agents, three bars are shown: individual, Voting, and AMAL.",
                "The individual bar shows the average accuracy of individual agents predictions; the voting bar shows the average accuracy of the joint prediction achieved by voting but without any <br>argumentation</br>; and finally the AMAL bar shows the average accuracy of the joint prediction using <br>argumentation</br>.",
                "The results shown are the average of 5 10-fold cross validation runs.",
                "Figure 5 shows that collaboration (voting and AMAL) outperforms individual problem solving.",
                "Moreover, as we expected, the accuracy improves as more agents collaborate, since more information is taken into account.",
                "We can also see that AMAL always outperforms standard voting, proving that joint decisions are based on better information as provided by the <br>argumentation</br> process.",
                "For instance, the joint accuracy for 2 agents in the sponge data set is of 87.57% for AMAL and 86.57% for voting (while individual accuracy is just 80.07%).",
                "Moreover, the improvement achieved by AMAL over Voting is even larger in the soybean data set.",
                "The reason is that the soybean data set is more difficult (in the sense that agents need more data to produce good predictions).",
                "These experimental results show that AMAL effectively exploits the opportunity for improvement: the accuracy is higher only because more agents have changed their opinion during <br>argumentation</br> (otherwise they would achieve the same result as Voting).",
                "Concerning H2 (learning from communication in <br>argumentation</br> processes improves individual prediction ), we ran the following experiment: initially, we distributed a 25% of the training set among the five agents; after that, the rest of the cases in the training set is sent to the agents one by one; when an agent receives a new training case, it has several options: the agent can discard it, the agent can retain it, or the agent can use it for engaging an <br>argumentation</br> process.",
                "Figure 6 shows the result of that experiment for the two data sets.",
                "Figure 6 contains three plots, where NL (not learning) shows accuracy of an agent with no learning at all; L (learning), shows the evolution of the individual classification accuracy when agents learn by retaining the training cases they individually receive (notice that when all the training cases have been retained at 100%, the accuracy should be equal to that of Figure 5 for individual agents); and finally LFC (learning from communication) shows the evolution of the individual classification accuracy of learning agents that also learn by retaining those counterexamples received during <br>argumentation</br> (i.e. they learn both from training examples and counterexamples).",
                "Figure 6 shows that if an agent Ai learns also from communication, Ai can significantly improve its individual performance with just a small number of additional cases (those selected as relevant counterexamples for Ai during <br>argumentation</br>).",
                "For instance, in the soybean data set, individual agents have achieved an accuracy of 70.62% when they also learn from communication versus an accuracy of 59.93% when they only learn from their individual experience.",
                "The number of cases learnt from communication depends on the properties of the data set: in the sponges data set, agents have retained only very few additional cases, and significantly improved individual accuracy; namely they retain 59.96 cases in average (compared to the 50.4 cases retained if they do not learn from communication).",
                "In the soybean data set more counterexamples are learnt to significantly improve individual accuracy, namely they retain 87.16 cases in average (compared to 55.27 cases retained if they do not learn from communication).",
                "Finally, the fact that both data sets show a significant improvement points out the adaptive nature of the <br>argumentation</br>-based approach to learning from communication: the useful cases are selected as counterexamples (and no more than those needed), and they have the intended effect. 10.",
                "RELATED WORK Concerning CBR in a multi-agent setting, the first research was on negotiated case retrieval [11] among groups of agents.",
                "Our work on multi-agent case-based learning started in 1999 [6]; later Mc Ginty and Smyth [7] presented a multi-agent collaborative CBR approach (CCBR) for planning.",
                "Finally, another interesting approach is multi-case-base reasoning (MCBR) [5], that deals with The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 981 SPONGE 60 65 70 75 80 85 25% 40% 55% 70% 85% 100% LFC L NL SOYBEAN 20 30 40 50 60 70 80 90 25% 40% 55% 70% 85% 100% LFC L NL Figure 6: Learning from communication resulting from <br>argumentation</br> in a system composed of 5 agents. distributed systems where there are several case bases available for the same task and addresses the problems of cross-case base adaptation.",
                "The main difference is that our MAC approach is a way to distribute the Reuse process of CBR (using a voting system) while Retrieve is performed individually by each agent; the other multiagent CBR approaches, however, focus on distributing the Retrieve process.",
                "Research on MAS <br>argumentation</br> focus on several issues like a) logics, protocols and languages that support <br>argumentation</br>, b) argument selection and c) argument interpretation.",
                "Approaches for logic and languages that support <br>argumentation</br> include defeasible logic [4] and BDI models [13].",
                "Although argument selection is a key aspect of automated <br>argumentation</br> (see [12] and [13]), most research has been focused on preference relations among arguments.",
                "In our framework we have addressed both argument selection and preference relations using a case-based approach. 11.",
                "CONCLUSIONS AND FUTURE WORK In this paper we have presented an <br>argumentation</br>-based framework for multi-agent learning.",
                "Specifically, we have presented AMAL, a framework that allows a group of learning agents to argue about the solution of a given problem and we have shown how the learning capabilities can be used to generate arguments and counterarguments.",
                "The experimental evaluation shows that the increased amount of information provided to the agents by the <br>argumentation</br> process increases their predictive accuracy, and specially when an adequate number of agents take part in the <br>argumentation</br>.",
                "The main contributions of this work are: a) an <br>argumentation</br> framework for learning agents; b) a case-based preference relation over arguments, based on computing an overall confidence estimation of arguments; c) a case-based policy to generate counterarguments and select counterexamples; and d) an <br>argumentation</br>-based approach for learning from communication.",
                "Finally, in the experiments presented here a learning agent would retain all counterexamples submitted by the other agent; however, this is a very simple case retention policy, and we will like to experiment with more informed policies - with the goal that individual learning agents could significantly improve using only a small set of cases proposed by other agents.",
                "Finally, our approach is focused on lazy learning, and future works aims at incorporating eager inductive learning inside the argumentative framework for learning from communication. 12.",
                "REFERENCES [1] Agnar Aamodt and Enric Plaza.",
                "Case-based reasoning: Foundational issues, methodological variations, and system approaches.",
                "Artificial Intelligence Communications, 7(1):39-59, 1994. [2] E. Armengol and E. Plaza.",
                "Lazy induction of descriptions for relational case-based learning.",
                "In ECML2001, pages 13-24, 2001. [3] Gerhard Brewka.",
                "Dynamic argument systems: A formal model of <br>argumentation</br> processes based on situation calculus.",
                "Journal of Logic and Computation, 11(2):257-282, 2001. [4] Carlos I. Chesñevar and Guillermo R. Simari.",
                "Formalizing Defeasible <br>argumentation</br> using Labelled Deductive Systems.",
                "Journal of Computer Science & Technology, 1(4):18-33, 2000. [5] D. Leake and R. Sooriamurthi.",
                "Automatically selecting strategies for multi-case-base reasoning.",
                "In S. Craw and A. Preece, editors, ECCBR2002, pages 204-219, Berlin, 2002.",
                "Springer Verlag. [6] Francisco J. Martín, Enric Plaza, and Josep-Lluis Arcos.",
                "Knowledge and experience reuse through communications among competent (peer) agents.",
                "International Journal of Software Engineering and Knowledge Engineering, 9(3):319-341, 1999. [7] Lorraine McGinty and Barry Smyth.",
                "Collaborative case-based reasoning: Applications in personalized route planning.",
                "In I. Watson and Q. Yang, editors, ICCBR, number 2080 in LNAI, pages 362-376.",
                "Springer-Verlag, 2001. [8] Santi Ontañón and Enric Plaza.",
                "Justification-based multiagent learning.",
                "In ICML2003, pages 576-583.",
                "Morgan Kaufmann, 2003. [9] Enric Plaza, Eva Armengol, and Santiago Ontañón.",
                "The explanatory power of symbolic similarity in case-based reasoning.",
                "Artificial Intelligence Review, 24(2):145-161, 2005. [10] David Poole.",
                "On the comparison of theories: Preferring the most specific explanation.",
                "In IJCAI-85, pages 144-147, 1985. [11] M V Nagendra Prassad, Victor R Lesser, and Susan Lander.",
                "Retrieval and reasoning in distributed case bases.",
                "Technical report, UMass Computer Science Department, 1995. [12] K. Sycara S. Kraus and A. Evenchik.",
                "Reaching agreements through <br>argumentation</br>: a logical model and implementation.",
                "Artificial Intelligence Journal, 104:1-69, 1998. [13] N. R. Jennings S. Parsons, C. Sierra.",
                "Agents that reason and negotiate by arguing.",
                "Journal of Logic and Computation, 8:261-292, 1998. [14] Bruce A. Wooley.",
                "Explanation component of software systems.",
                "ACM CrossRoads, 5.1, 1998. 982 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "Learning and Joint Deliberation through <br>argumentation</br> in Multi-Agent Systems Santi Ontañón CCL, Cognitive Computing Lab Georgia Institute of Technology Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Artificial Intelligence Research Institute CSIC, Spanish Council for Scientific Research Campus UAB, 08193 Bellaterra, Catalonia (Spain) enric@iiia.csic.es ABSTRACT In this paper we will present an <br>argumentation</br> framework for learning agents (AMAL) designed for two purposes: (1) for joint deliberation, and (2) for learning from communication.",
                "We experimentally show that the <br>argumentation</br> among committees of agents improves both the individual and joint performance.",
                "For learning from communication, an agent engages into arguing with other agents in order to contrast its individual hypotheses and receive counterexamples; the <br>argumentation</br> process improves their learning scope and individual performance.",
                "INTRODUCTION <br>argumentation</br> frameworks for multi-agent systems can be used for different purposes like joint deliberation, persuasion, negotiation, and conflict resolution.",
                "In this paper we will present an <br>argumentation</br> framework for learning agents, and show that it can be used for two purposes: (1) joint deliberation, and (2) learning from communication."
            ],
            "translated_annotated_samples": [
                "Aprendizaje y deliberación conjunta a través de la <br>argumentación</br> en sistemas multiagente. Santi Ontañón CCL, Laboratorio de Computación Cognitiva, Instituto de Tecnología de Georgia, Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Instituto de Investigación en Inteligencia Artificial, CSIC, Consejo Superior de Investigaciones Científicas, Campus UAB, 08193 Bellaterra, Cataluña (España) enric@iiia.csic.es RESUMEN En este artículo presentaremos un marco de <br>argumentación</br> para agentes de aprendizaje (AMAL) diseñado con dos propósitos: (1) para deliberación conjunta, y (2) para aprender a través de la comunicación.",
                "Experimentalmente demostramos que la <br>argumentación</br> entre comités de agentes mejora tanto el rendimiento individual como el conjunto.",
                "Para aprender a través de la comunicación, un agente se involucra en discusiones con otros agentes para contrastar sus hipótesis individuales y recibir contraejemplos; el proceso de <br>argumentación</br> mejora su alcance de aprendizaje y rendimiento individual.",
                "Los marcos de <br>argumentación</br> para sistemas multiagente pueden ser utilizados para diferentes propósitos como la deliberación conjunta, la persuasión, la negociación y la resolución de conflictos.",
                "En este artículo presentaremos un marco de <br>argumentación</br> para agentes de aprendizaje, y mostraremos que puede ser utilizado para dos propósitos: (1) deliberación conjunta, y (2) aprendizaje a partir de la comunicación."
            ],
            "translated_text": "Aprendizaje y deliberación conjunta a través de la <br>argumentación</br> en sistemas multiagente. Santi Ontañón CCL, Laboratorio de Computación Cognitiva, Instituto de Tecnología de Georgia, Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Instituto de Investigación en Inteligencia Artificial, CSIC, Consejo Superior de Investigaciones Científicas, Campus UAB, 08193 Bellaterra, Cataluña (España) enric@iiia.csic.es RESUMEN En este artículo presentaremos un marco de <br>argumentación</br> para agentes de aprendizaje (AMAL) diseñado con dos propósitos: (1) para deliberación conjunta, y (2) para aprender a través de la comunicación. El marco AMAL se basa completamente en el aprendizaje a partir de ejemplos: la relación de preferencia de argumentos, la política de generación de argumentos y la política de generación de contraargumentos son técnicas basadas en casos. Para unirse en la deliberación, los agentes de aprendizaje comparten su experiencia formando un comité para decidir sobre alguna decisión conjunta. Experimentalmente demostramos que la <br>argumentación</br> entre comités de agentes mejora tanto el rendimiento individual como el conjunto. Para aprender a través de la comunicación, un agente se involucra en discusiones con otros agentes para contrastar sus hipótesis individuales y recibir contraejemplos; el proceso de <br>argumentación</br> mejora su alcance de aprendizaje y rendimiento individual. Categorías y Descriptores de Asignaturas I.2.6 [Inteligencia Artificial]: Aprendizaje; I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente, Agentes Inteligentes 1. Los marcos de <br>argumentación</br> para sistemas multiagente pueden ser utilizados para diferentes propósitos como la deliberación conjunta, la persuasión, la negociación y la resolución de conflictos. En este artículo presentaremos un marco de <br>argumentación</br> para agentes de aprendizaje, y mostraremos que puede ser utilizado para dos propósitos: (1) deliberación conjunta, y (2) aprendizaje a partir de la comunicación. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "case-base reason": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning and Joint Deliberation through Argumentation in Multi-Agent Systems Santi Ontañón CCL, Cognitive Computing Lab Georgia Institute of Technology Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Artificial Intelligence Research Institute CSIC, Spanish Council for Scientific Research Campus UAB, 08193 Bellaterra, Catalonia (Spain) enric@iiia.csic.es ABSTRACT In this paper we will present an argumentation framework for learning agents (AMAL) designed for two purposes: (1) for joint deliberation, and (2) for learning from communication.",
                "The AMAL framework is completely based on learning from examples: the argument preference relation, the argument generation policy, and the counterargument generation policy are case-based techniques.",
                "For join deliberation, learning agents share their experience by forming a committee to decide upon some joint decision.",
                "We experimentally show that the argumentation among committees of agents improves both the individual and joint performance.",
                "For learning from communication, an agent engages into arguing with other agents in order to contrast its individual hypotheses and receive counterexamples; the argumentation process improves their learning scope and individual performance.",
                "Categories and Subject Descriptors I.2.6 [Artificial Intelligence]: Learning; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems, Intelligent Agents 1.",
                "INTRODUCTION Argumentation frameworks for multi-agent systems can be used for different purposes like joint deliberation, persuasion, negotiation, and conflict resolution.",
                "In this paper we will present an argumentation framework for learning agents, and show that it can be used for two purposes: (1) joint deliberation, and (2) learning from communication.",
                "Argumentation-based joint deliberation involves discussion over the outcome of a particular situation or the appropriate course of action for a particular situation.",
                "Learning agents are capable of learning from experience, in the sense that past examples (situations and their outcomes) are used to predict the outcome for the situation at hand.",
                "However, since individual agents experience may be limited, individual knowledge and prediction accuracy is also limited.",
                "Thus, learning agents that are capable of arguing their individual predictions with other agents may reach better prediction accuracy after such an argumentation process.",
                "Most existing argumentation frameworks for multi-agent systems are based on deductive logic or some other deductive logic formalism specifically designed to support argumentation, such as default logic [3]).",
                "Usually, an argument is seen as a logical statement, while a counterargument is an argument offered in opposition to another argument [4, 13]; agents use a preference relation to resolve conflicting arguments.",
                "However, logic-based argumentation frameworks assume agents with preloaded knowledge and preference relation.",
                "In this paper, we focus on an Argumentation-based Multi-Agent Learning (AMAL) framework where both knowledge and preference relation are learned from experience.",
                "Thus, we consider a scenario with agents that (1) work in the same domain using a shared ontology, (2) are capable of learning from examples, and (3) communicate using an argumentative framework.",
                "Having learning capabilities allows agents effectively use a specific form of counterargument, namely the use of counterexamples.",
                "Counterexamples offer the possibility of agents learning during the argumentation process.",
                "Moreover, learning agents allow techniques that use learnt experience to generate adequate arguments and counterarguments.",
                "Specifically, we will need to address two issues: (1) how to define a technique to generate arguments and counterarguments from examples, and (2)how to define a preference relation over two conflicting arguments that have been induced from examples.",
                "This paper presents a case-based approach to address both issues.",
                "The agents use case-based reasoning (CBR) [1] to learn from past cases (where a case is a situation and its outcome) in order to predict the outcome of a new situation.",
                "We propose an argumentation protocol inside the AMAL framework at supports agents in reaching a joint prediction over a specific situation or problem - moreover, the reasoning needed to support the argumentation process will also be based on cases.",
                "In particular, we present two case-based measures, one for generating the arguments and counterarguments adequate to a particular situation and another for determining preference relation among arguments.",
                "Finally, we evaluate (1) if argumentation between learning agents can produce a joint prediction that improves over individual learning performance and (2) if learning from the counterexamples conveyed during the argumentation process increases the individual performance with precisely those cases being used while arguing among them.",
                "The paper is structured as follows.",
                "Section 2 discusses the relation among argumentation, collaboration and learning.",
                "Then Section 3 introduces our multi-agent CBR (MAC) framework and the notion of justified prediction.",
                "After that, Section 4 formally defines our argumentation framework.",
                "Sections 5 and 6 present our case-based preference relation and argument generation policies respectively.",
                "Later, Section 7 presents the argumentation protocol in our AMAL framework.",
                "After that, Section 8 presents an exemplification of the argumentation framework.",
                "Finally, Section 9 presents an empirical evaluation of our two main hypotheses.",
                "The paper closes with related work and conclusions sections. 2.",
                "ARGUMENTATION,COLLABORATION AND LEARNING Both learning and collaboration are ways in which an agent can improve individual performance.",
                "In fact, there is a clear parallelism between learning and collaboration in multi-agent systems, since both are ways in which agents can deal with their shortcomings.",
                "Let us show which are the main motivations that an agent can have to learn or to collaborate. • Motivations to learn: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems. • Motivations to collaborate: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems, - Increase the range of accessible resources.",
                "Looking at the above lists of motivation, we can easily see that learning and collaboration are very related in multi-agent systems.",
                "In fact, with the exception of the last item in the motivations to collaborate list, they are two extremes of a continuum of strategies to improve performance.",
                "An agent may choose to increase performance by learning, by collaborating, or by finding an intermediate point that combines learning and collaboration in order to improve performance.",
                "In this paper we will propose AMAL, an argumentation framework for learning agents, and will also also show how AMAL can be used both for learning from communication and for solving problems in a collaborative way: • Agents can solve problems in a collaborative way via engaging an argumentation process about the prediction for the situation at hand.",
                "Using this collaboration, the prediction can be done in a more informed way, since the information known by several agents has been taken into account. • Agents can also learn from communication with other agents by engaging an argumentation process.",
                "Agents that engage in such argumentation processes can learn from the arguments and counterexamples received from other agents, and use this information for predicting the outcomes of future situations.",
                "In the rest of this paper we will propose an argumentation framework and show how it can be used both for learning and for solving problems in a collaborative way. 3.",
                "MULTI-AGENT CBR SYSTEMS A Multi-Agent Case Based Reasoning System (MAC) M = {(A1, C1), ..., (An, Cn)} is a multi-agent system composed of A = {Ai, ..., An}, a set of CBR agents, where each agent Ai ∈ A possesses an individual case base Ci.",
                "Each individual agent Ai in a MAC is completely autonomous and each agent Ai has access only to its individual and private case base Ci.",
                "A case base Ci = {c1, ..., cm} is a collection of cases.",
                "Agents in a MAC system are able to individually solve problems, but they can also collaborate with other agents to solve problems.",
                "In this framework, we will restrict ourselves to analytical tasks, i.e. tasks like classification, where the solution of a problem is achieved by selecting a solution class from an enumerated set of solution classes.",
                "In the following we will note the set of all the solution classes by S = {S1, ..., SK }.",
                "Therefore, a case c = P, S is a tuple containing a case description P and a solution class S ∈ S. In the following, we will use the terms problem and case description indistinctly.",
                "Moreover, we will use the dot notation to refer to elements inside a tuple; e.g., to refer to the solution class of a case c, we will write c.S.",
                "Therefore, we say a group of agents perform joint deliberation, when they collaborate to find a joint solution by means of an argumentation process.",
                "However, in order to do so, an agent has to be able to justify its prediction to the other agents (i.e. generate an argument for its predicted solution that can be examined and critiqued by the other agents).",
                "The next section addresses this issue. 3.1 Justified Predictions Both expert systems and CBR systems may have an explanation component [14] in charge of justifying why the system has provided a specific answer to the user.",
                "The line of reasoning of the system can then be examined by a human expert, thus increasing the reliability of the system.",
                "Most of the existing work on explanation generation focuses on generating explanations to be provided to the user.",
                "However, in our approach we use explanations (or justifications) as a tool for improving communication and coordination among agents.",
                "We are interested in justifications since they can be used as arguments.",
                "For that purpose, we will benefit from the ability of some machine learning methods to provide justifications.",
                "A justification built by a CBR method after determining that the solution of a particular problem P was Sk is a description that contains the relevant information from the problem P that the CBR method has considered to predict Sk as the solution of P. In particular, CBR methods work by retrieving similar cases to the problem at hand, and then reusing their solutions for the current problem, expecting that since the problem and the cases are similar, the solutions will also be similar.",
                "Thus, if a CBR method has retrieved a set of cases C1, ..., Cn to solve a particular problem P the justification built will contain the relevant information from the problem P that made the CBR system retrieve that particular set of cases, i.e. it will contain the relevant information that P and C1, ..., Cn have in common.",
                "For example, Figure 1 shows a justification build by a CBR system for a toy problem (in the following sections we will show justifications for real problems).",
                "In the figure, a problem has two attributes (Traffic_light, and Cars_passing), the retrieval mechanism of the CBR system notices that by considering only the attribute Traffic_light, it can retrieve two cases that predict the same solution: wait.",
                "Thus, since only this attribute has been used, it is the only one appearing in the justification.",
                "The values of the rest of attributes are irrelevant, since whatever their value the solution class would have been the same. 976 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Problem Traffic_light: red Cars_passing: no Case 1 Traffic_light: red Cars_passing: no Solution: wait Case 3 Traffic_light: red Cars_passing: yes Solution: wait Case 4 Traffic_light: green Cars_passing: yes Solution: wait Case 2 Traffic_light: green Cars_passing: no Solution: cross Retrieved cases Solution: wait Justification Traffic_light: red Figure 1: An example of justification generation in a CBR system.",
                "Notice that, since the only relevant feature to decide is Traffic_light (the only one used to retrieve cases), it is the only one appearing in the justification.",
                "In general, the meaning of a justification is that all (or most of) the cases in the case base of an agent that satisfy the justification (i.e. all the cases that are subsumed by the justification) belong to the predicted solution class.",
                "In the rest of the paper, we will use to denote the subsumption relation.",
                "In our work, we use LID [2], a CBR method capable of building symbolic justifications such as the one exemplified in Figure 1.",
                "When an agent provides a justification for a prediction, the agent generates a justified prediction: DEFINITION 3.1.",
                "A Justified Prediction is a tuple J = A, P, S, D where agent A considers S the correct solution for problem P, and that prediction is justified a symbolic description D such that J.D J.P. Justifications can have many uses for CBR systems [8, 9].",
                "In this paper, we are going to use justifications as arguments, in order to allow learning agents to engage in argumentation processes. 4.",
                "ARGUMENTS AND COUNTERARGUMENTS For our purposes an argument α generated by an agent A is composed of a statement S and some evidence D supporting S as correct.",
                "In the remainder of this section we will see how this general definition of argument can be instantiated in specific kind of arguments that the agents can generate.",
                "In the context of MAC systems, agents argue about predictions for new problems and can provide two kinds of information: a) specific cases P, S , and b) justified predictions: A, P, S, D .",
                "Using this information, we can define three types of arguments: justified predictions, counterarguments, and counterexamples.",
                "A justified prediction α is generated by an agent Ai to argue that Ai believes that the correct solution for a given problem P is α.S, and the evidence provided is the justification α.D.",
                "In the example depicted in Figure 1, an agent Ai may generate the argument α = Ai, P, Wait, (Traffic_light = red) , meaning that the agent Ai believes that the correct solution for P is Wait because the attribute Traffic_light equals red.",
                "A counterargument β is an argument offered in opposition to another argument α.",
                "In our framework, a counterargument consists of a justified prediction Aj, P, S , D generated by an agent Aj with the intention to rebut an argument α generated by another agent Ai, that endorses a solution class S different from that of α.S for the problem at hand and justifies this with a justification D .",
                "In the example in Figure 1, if an agent generates the argument α = Ai, P, Walk, (Cars_passing = no) , an agent that thinks that the correct solution is Wait might answer with the counterargument β = Aj, P, Wait, (Cars_passing = no ∧ Traffic_light = red) , meaning that, although there are no cars passing, the traffic light is red, and the street cannot be crossed.",
                "A counterexample c is a case that contradicts an argument α.",
                "Thus a counterexample is also a counterargument, one that states that a specific argument α is not always true, and the evidence provided is the case c. Specifically, for a case c to be a counterexample of an argument α, the following conditions have to be met: α.D c and α.S = c.S, i.e. the case must satisfy the justification α.D and the solution of c must be different than the predicted by α.",
                "By exchanging arguments and counterarguments (including counterexamples), agents can argue about the correct solution of a given problem, i.e. they can engage a joint deliberation process.",
                "However, in order to do so, they need a specific interaction protocol, a preference relation between contradicting arguments, and a decision policy to generate counterarguments (including counterexamples).",
                "In the following sections we will present these elements. 5.",
                "PREFERENCE RELATION A specific argument provided by an agent might not be consistent with the information known to other agents (or even to some of the information known by the agent that has generated the justification due to noise in training data).",
                "For that reason, we are going to define a preference relation over contradicting justified predictions based on cases.",
                "Basically, we will define a confidence measure for each justified prediction (that takes into account the cases owned by each agent), and the justified prediction with the highest confidence will be the preferred one.",
                "The idea behind case-based confidence is to count how many of the cases in an individual case base endorse a justified prediction, and how many of them are counterexamples of it.",
                "The more the endorsing cases, the higher the confidence; and the more the counterexamples, the lower the confidence.",
                "Specifically, to assess the confidence of a justified prediction α, an agent obtains the set of cases in its individual case base that are subsumed by α.D.",
                "With them, an agent Ai obtains the Y (aye) and N (nay) values: • Y Ai α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by the justification α.D that belong to the solution class α.S, • NAi α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by justification α.D that do not belong to that solution class.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 977 + + + + + +  - - + Figure 2: Confidence of arguments is evaluated by contrasting them against the case bases of the agents.",
                "An agent estimates the confidence of an argument as: CAi (α) = Y Ai α 1 + Y Ai α + NAi α i.e. the confidence on a justified prediction is the number of endorsing cases divided by the number of endorsing cases plus counterexamples.",
                "Notice that we add 1 to the denominator, this is to avoid giving excessively high confidences to justified predictions whose confidence has been computed using a small number of cases.",
                "Notice that this correction follows the same idea than the Laplace correction to estimate probabilities.",
                "Figure 2 illustrates the individual evaluation of the confidence of an argument, in particular, three endorsing cases and one counterexample are found in the case base of agents Ai, giving an estimated confidence of 0.6 Moreover, we can also define the joint confidence of an argument α as the confidence computed using the cases present in the case bases of all the agents in the group: C(α) = i Y Ai α 1 + i Y Ai α + NAi α Notice that, to collaboratively compute the joint confidence, the agents only have to make public the aye and nay values locally computed for a given argument.",
                "In our framework, agents use this joint confidence as the preference relation: a justified prediction α is preferred over another one β if C(α) ≥ C(β). 6.",
                "GENERATION OF ARGUMENTS In our framework, arguments are generated by the agents from cases, using learning methods.",
                "Any learning method able to provide a justified prediction can be used to generate arguments.",
                "For instance, decision trees and LID [2] are suitable learning methods.",
                "Specifically, in the experiments reported in this paper agents use LID.",
                "Thus, when an agent wants to generate an argument endorsing that a specific solution class is the correct solution for a problem P, it generates a justified prediction as explained in Section 3.1.",
                "For instance, Figure 3 shows a real justification generated by LID after solving a problem P in the domain of marine sponges identification.",
                "In particular, Figure 3 shows how when an agent receives a new problem to solve (in this case, a new sponge to determine its order), the agent uses LID to generate an argument (consisting on a justified prediction) using the cases in the case base of the agent.",
                "The justification shown in Figure 3 can be interpreted saying that the predicted solution is hadromerida because the smooth form of the megascleres of the spiculate skeleton of the sponge is of type tylostyle, the spikulate skeleton of the sponge has no uniform length, and there is no gemmules in the external features of the sponge.",
                "Thus, the argument generated will be α = A1, P, hadromerida, D1 . 6.1 Generation of Counterarguments As previously stated, agents may try to rebut arguments by generating counterargument or by finding counterexamples.",
                "Let us explain how they can be generated.",
                "An agent Ai wants to generate a counterargument β to rebut an argument α when α is in contradiction with the local case base of Ai.",
                "Moreover, while generating such counterargument β, Ai expects that β is preferred over α.",
                "For that purpose, we will present a specific policy to generate counterarguments based on the specificity criterion [10].",
                "The specificity criterion is widely used in deductive frameworks for argumentation, and states that between two conflicting arguments, the most specific should be preferred since it is, in principle, more informed.",
                "Thus, counterarguments generated based on the specificity criterion are expected to be preferable (since they are more informed) to the arguments they try to rebut.",
                "However, there is no guarantee that such counterarguments will always win, since, as we have stated in Section 5, agents in our framework use a preference relation based on joint confidence.",
                "Moreover, one may think that it would be better that the agents generate counterarguments based on the joint confidence preference relation; however it is not obvious how to generate counterarguments based on joint confidence in an efficient way, since collaboration is required in order to evaluate joint confidence.",
                "Thus, the agent generating the counterargument should constantly communicate with the other agents at each step of the induction algorithm used to generate counterarguments (presently one of our future research lines).",
                "Thus, in our framework, when an agent wants to generate a counterargument β to an argument α, β has to be more specific than α (i.e. α.D < β.D).",
                "The generation of counterarguments using the specificity criterion imposes some restrictions over the learning method, although LID or ID3 can be easily adapted for this task.",
                "For instance, LID is an algorithm that generates a description starting from scratch and heuristically adding features to that term.",
                "Thus, at every step, the description is made more specific than in the previous step, and the number of cases that are subsumed by that description is reduced.",
                "When the description covers only (or almost only) cases of a single solution class LID terminates and predicts that solution class.",
                "To generate a counterargument to an argument α LID just has to use as starting point the description α.D instead of starting from scratch.",
                "In this way, the justification provided by LID will always be subsumed by α.D, and thus the resulting counterargument will be more specific than α.",
                "However, notice that LID may sometimes not be able to generate counterarguments, since LID may not be able to specialize the description α.D any further, or because the agent Ai has no case inCi that is subsumed by α.D.",
                "Figure 4 shows how an agent A2 that disagreed with the argument shown in Figure 3, generates a counterargument using LID.",
                "Moreover, Figure 4 shows the generation of a counterargument β1 2 for the argument α0 1 (in Figure 3) that is a specialization of α0 1. 978 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Solution: hadromerida Justification: D1 Sponge Spikulate skeleton External features External features Gemmules: no Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Case Base of A1 LID New sponge P Figure 3: Example of a real justification generated by LID in the marine sponges data set.",
                "Specifically, in our experiments, when an agent Ai wants to rebut an argument α, uses the following policy: 1.",
                "Agent Ai uses LID to try to find a counterargument β more specific than α; if found, β is sent to the other agent as a counterargument of α. 2.",
                "If not found, then Ai searches for a counterexample c ∈ Ci of α.",
                "If a case c is found, then c is sent to the other agent as a counterexample of α. 3.",
                "If no counterexamples are found, then Ai cannot rebut the argument α. 7.",
                "ARGUMENTATION-BASED MULTI-AGENT LEARNING The interaction protocol of AMAL allows a group of agents A1, ..., An to deliberate about the correct solution of a problem P by means of an argumentation process.",
                "If the argumentation process arrives to a consensual solution, the joint deliberation ends; otherwise a weighted vote is used to determine the joint solution.",
                "Moreover, AMAL also allows the agents to learn from the counterexamples received from other agents.",
                "The AMAL protocol consists on a series of rounds.",
                "In the initial round, each agent states which is its individual prediction for P. Then, at each round an agent can try to rebut the prediction made by any of the other agents.",
                "The protocol uses a token passing mechanism so that agents (one at a time) can send counterarguments or counterexamples if they disagree with the prediction made by any other agent.",
                "Specifically, each agent is allowed to send one counterargument or counterexample each time he gets the token (notice that this restriction is just to simplify the protocol, and that it does not restrict the number of counterargument an agent can sent, since they can be delayed for subsequent rounds).",
                "When an agent receives a counterargument or counterexample, it informs the other agents if it accepts the counterargument (and changes its prediction) or not.",
                "Moreover, agents have also the opportunity to answer to counterarguments when they receive the token, by trying to generate a counterargument to the counterargument.",
                "When all the agents have had the token once, the token returns to the first agent, and so on.",
                "If at any time in the protocol, all the agents agree or during the last n rounds no agent has generated any counterargument, the protocol ends.",
                "Moreover, if at the end of the argumentation the agents have not reached an agreement, then a voting mechanism that uses the confidence of each prediction as weights is used to decide the final solution (Thus, AMAL follows the same mechanism as human committees, first each individual member of a committee exposes his arguments and discuses those of the other members (joint deliberation), and if no consensus is reached, then a voting mechanism is required).",
                "At each iteration, agents can use the following performatives: • assert(α): the justified prediction held during the next round will be α.",
                "An agent can only hold a single prediction at each round, thus is multiple asserts are send, only the last one is considered as the currently held prediction. • rebut(β, α): the agent has found a counterargument β to the prediction α.",
                "We will define Ht = αt 1, ..., αt n as the predictions that each of the n agents hold at a round t. Moreover, we will also define contradict(αt i) = {α ∈ Ht|α.S = αt i.S} as the set of contradicting arguments for an agent Ai in a round t, i.e. the set of arguments at round t that support a different solution class than αt i.",
                "The protocol is initiated because one of the agents receives a problem P to be solved.",
                "After that, the agent informs all the other agents about the problem P to solve, and the protocol starts: 1.",
                "At round t = 0, each one of the agents individually solves P, and builds a justified prediction using its own CBR method.",
                "Then, each agent Ai sends the performative assert(α0 i ) to the other agents.",
                "Thus, the agents know H0 = α0 i , ..., α0 n .",
                "Once all the predictions have been sent the token is given to the first agent A1. 2.",
                "At each round t (other than 0), the agents check whether their arguments in Ht agree.",
                "If they do, the protocol moves to step 5.",
                "Moreover, if during the last n rounds no agent has sent any counterexample or counterargument, the protocol also moves to step 5.",
                "Otherwise, the agent Ai owner of the token tries to generate a counterargument for each of the opposing arguments in contradict(αt i) ⊆ Ht (see Section 6.1).",
                "Then, the counterargument βt i against the prediction αt j with the lowest confidence C(αt j) is selected (since αt j is the prediction more likely to be successfully rebutted). • If βt i is a counterargument, then, Ai locally compares αt i with βt i by assessing their confidence against its individual case base Ci (see Section 5) (notice that Ai is comparing its previous argument with the counterargument that Ai itself has just generated and that is about The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 979 Sponge Spikulate skeleton External features External features Gemmules: no Growing: Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Growing Grow: massive Case Base of A2 LID Solution: astrophorida Justification: D2 Figure 4: Generation of a counterargument using LID in the sponges data set. to send to Aj).",
                "If CAi (βt i ) > CAi (αt i), then Ai considers that βt i is stronger than its previous argument, changes its argument to βt i by sending assert(βt i ) to the rest of the agents (the intuition behind this is that since a counterargument is also an argument, Ai checks if the newly counterargument is a better argument than the one he was previously holding) and rebut(βt i , αt j) to Aj.",
                "Otherwise (i.e.",
                "CAi (βt i ) ≤ CAi (αt i)), Ai will send only rebut(βt i , αt j) to Aj.",
                "In any of the two situations the protocol moves to step 3. • If βt i is a counterexample c, then Ai sends rebut(c, αt j) to Aj.",
                "The protocol moves to step 4. • If Ai cannot generate any counterargument or counterexample, the token is sent to the next agent, a new round t + 1 starts, and the protocol moves to state 2. 3.",
                "The agent Aj that has received the counterargument βt i , locally compares it against its own argument, αt j, by locally assessing their confidence.",
                "If CAj (βt i ) > CAj (αt j), then Aj will accept the counterargument as stronger than its own argument, and it will send assert(βt i ) to the other agents.",
                "Otherwise (i.e.",
                "CAj (βt i ) ≤ CAj (αt j)), Aj will not accept the counterargument, and will inform the other agents accordingly.",
                "Any of the two situations start a new round t + 1, Ai sends the token to the next agent, and the protocol moves back to state 2. 4.",
                "The agent Aj that has received the counterexample c retains it into its case base and generates a new argument αt+1 j that takes into account c, and informs the rest of the agents by sending assert(αt+1 j ) to all of them.",
                "Then, Ai sends the token to the next agent, a new round t + 1 starts, and the protocol moves back to step 2. 5.",
                "The protocol ends yielding a joint prediction, as follows: if the arguments in Ht agree then their prediction is the joint prediction, otherwise a voting mechanism is used to decide the joint prediction.",
                "The voting mechanism uses the joint confidence measure as the voting weights, as follows: S = arg max Sk∈S αi∈Ht|αi.S=Sk C(αi) Moreover, in order to avoid infinite iterations, if an agent sends twice the same argument or counterargument to the same agent, the message is not considered. 8.",
                "EXEMPLIFICATION Let us consider a system composed of three agents A1, A2 and A3.",
                "One of the agents, A1 receives a problem P to solve, and decides to use AMAL to solve it.",
                "For that reason, invites A2 and A3 to take part in the argumentation process.",
                "They accept the invitation, and the argumentation protocol starts.",
                "Initially, each agent generates its individual prediction for P, and broadcasts it to the other agents.",
                "Thus, all of them can compute H0 = α0 1, α0 2, α0 3 .",
                "In particular, in this example: • α0 1 = A1, P, hadromerida, D1 • α0 2 = A2, P, astrophorida, D2 • α0 3 = A3, P, axinellida, D3 A1 starts owning the token and tries to generate counterarguments for α0 2 and α0 3, but does not succeed, however it has one counterexample c13 for α0 3.",
                "Thus, A1 sends the the message rebut( c13, α0 3) to A3.",
                "A3 incorporates c13 into its case base and tries to solve the problem P again, now taking c13 into consideration.",
                "A3 comes up with the justified prediction α1 3 = A3, P, hadromerida, D4 , and broadcasts it to the rest of the agents with the message assert(α1 3).",
                "Thus, all of them know the new H1 = α0 1, α0 2, α1 3 .",
                "Round 1 starts and A2 gets the token.",
                "A2 tries to generate counterarguments for α0 1 and α1 3 and only succeeds to generate a counterargument β1 2 = A2, P, astrophorida, D5 against α1 3.",
                "The counterargument is sent to A3 with the message rebut(β1 2 , α1 3).",
                "Agent A3 receives the counterargument and assesses its local confidence.",
                "The result is that the individual confidence of the counterargument β1 2 is lower than the local confidence of α1 3.",
                "Therefore, A3 does not accept the counterargument, and thus H2 = α0 1, α0 2, α1 3 .",
                "Round 2 starts and A3 gets the token.",
                "A3 generates a counterargument β2 3 = A3, P, hadromerida, D6 for α0 2 and sends it to A2 with the message rebut(β2 3 , α0 2).",
                "Agent A2 receives the counterargument and assesses its local confidence.",
                "The result is that the local confidence of the counterargument β2 3 is higher than the local confidence of α0 2.",
                "Therefore, A2 accepts the counterargument and informs the rest of the agents with the message assert(β2 3 ).",
                "After that, H3 = α0 1, β2 3 , α1 3 .",
                "At Round 3, since all the agents agree (all the justified predictions in H3 predict hadromerida as the solution class) The protocol ends, and A1 (the agent that received the problem) considers hadromerida as the joint solution for the problem P. 9.",
                "EXPERIMENTAL EVALUATION 980 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) SPONGE 75 77 79 81 83 85 87 89 91 2 3 4 5 AMAL Voting Individual SOYBEAN 55 60 65 70 75 80 85 90 2 3 4 5 AMAL Voting Individual Figure 5: Individual and joint accuracy for 2 to 5 agents.",
                "In this section we empirically evaluate the AMAL argumentation framework.",
                "We have made experiments in two different data sets: soybean (from the UCI machine learning repository) and sponge (a relational data set).",
                "The soybean data set has 307 examples and 19 solution classes, while the sponge data set has 280 examples and 3 solution classes.",
                "In an experimental run, the data set is divided in 2 sets: the training set and the test set.",
                "The training set examples are distributed among 5 different agents without replication, i.e. there is no example shared by two agents.",
                "In the testing stage, problems in the test set arrive randomly to one of the agents, and their goal is to predict the correct solution.",
                "The experiments are designed to test two hypotheses: (H1) that argumentation is a useful framework for joint deliberation and can improve over other typical methods such as voting; and (H2) that learning from communication improves the individual performance of a learning agent participating in an argumentation process.",
                "Moreover, we also expect that the improvement achieved from argumentation will increase as the number of agents participating in the argumentation increases (since more information will be taken into account).",
                "Concerning H1 (argumentation is a useful framework for joint deliberation), we ran 4 experiments, using 2, 3, 4, and 5 agents respectively (in all experiments each agent has a 20% of the training data, since the training is always distributed among 5 agents).",
                "Figure 5 shows the result of those experiments in the sponge and soybean data sets.",
                "Classification accuracy is plotted in the vertical axis, and in the horizontal axis the number of agents that took part in the argumentation processes is shown.",
                "For each number of agents, three bars are shown: individual, Voting, and AMAL.",
                "The individual bar shows the average accuracy of individual agents predictions; the voting bar shows the average accuracy of the joint prediction achieved by voting but without any argumentation; and finally the AMAL bar shows the average accuracy of the joint prediction using argumentation.",
                "The results shown are the average of 5 10-fold cross validation runs.",
                "Figure 5 shows that collaboration (voting and AMAL) outperforms individual problem solving.",
                "Moreover, as we expected, the accuracy improves as more agents collaborate, since more information is taken into account.",
                "We can also see that AMAL always outperforms standard voting, proving that joint decisions are based on better information as provided by the argumentation process.",
                "For instance, the joint accuracy for 2 agents in the sponge data set is of 87.57% for AMAL and 86.57% for voting (while individual accuracy is just 80.07%).",
                "Moreover, the improvement achieved by AMAL over Voting is even larger in the soybean data set.",
                "The reason is that the soybean data set is more difficult (in the sense that agents need more data to produce good predictions).",
                "These experimental results show that AMAL effectively exploits the opportunity for improvement: the accuracy is higher only because more agents have changed their opinion during argumentation (otherwise they would achieve the same result as Voting).",
                "Concerning H2 (learning from communication in argumentation processes improves individual prediction ), we ran the following experiment: initially, we distributed a 25% of the training set among the five agents; after that, the rest of the cases in the training set is sent to the agents one by one; when an agent receives a new training case, it has several options: the agent can discard it, the agent can retain it, or the agent can use it for engaging an argumentation process.",
                "Figure 6 shows the result of that experiment for the two data sets.",
                "Figure 6 contains three plots, where NL (not learning) shows accuracy of an agent with no learning at all; L (learning), shows the evolution of the individual classification accuracy when agents learn by retaining the training cases they individually receive (notice that when all the training cases have been retained at 100%, the accuracy should be equal to that of Figure 5 for individual agents); and finally LFC (learning from communication) shows the evolution of the individual classification accuracy of learning agents that also learn by retaining those counterexamples received during argumentation (i.e. they learn both from training examples and counterexamples).",
                "Figure 6 shows that if an agent Ai learns also from communication, Ai can significantly improve its individual performance with just a small number of additional cases (those selected as relevant counterexamples for Ai during argumentation).",
                "For instance, in the soybean data set, individual agents have achieved an accuracy of 70.62% when they also learn from communication versus an accuracy of 59.93% when they only learn from their individual experience.",
                "The number of cases learnt from communication depends on the properties of the data set: in the sponges data set, agents have retained only very few additional cases, and significantly improved individual accuracy; namely they retain 59.96 cases in average (compared to the 50.4 cases retained if they do not learn from communication).",
                "In the soybean data set more counterexamples are learnt to significantly improve individual accuracy, namely they retain 87.16 cases in average (compared to 55.27 cases retained if they do not learn from communication).",
                "Finally, the fact that both data sets show a significant improvement points out the adaptive nature of the argumentation-based approach to learning from communication: the useful cases are selected as counterexamples (and no more than those needed), and they have the intended effect. 10.",
                "RELATED WORK Concerning CBR in a multi-agent setting, the first research was on negotiated case retrieval [11] among groups of agents.",
                "Our work on multi-agent case-based learning started in 1999 [6]; later Mc Ginty and Smyth [7] presented a multi-agent collaborative CBR approach (CCBR) for planning.",
                "Finally, another interesting approach is multi-<br>case-base reason</br>ing (MCBR) [5], that deals with The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 981 SPONGE 60 65 70 75 80 85 25% 40% 55% 70% 85% 100% LFC L NL SOYBEAN 20 30 40 50 60 70 80 90 25% 40% 55% 70% 85% 100% LFC L NL Figure 6: Learning from communication resulting from argumentation in a system composed of 5 agents. distributed systems where there are several case bases available for the same task and addresses the problems of cross-case base adaptation.",
                "The main difference is that our MAC approach is a way to distribute the Reuse process of CBR (using a voting system) while Retrieve is performed individually by each agent; the other multiagent CBR approaches, however, focus on distributing the Retrieve process.",
                "Research on MAS argumentation focus on several issues like a) logics, protocols and languages that support argumentation, b) argument selection and c) argument interpretation.",
                "Approaches for logic and languages that support argumentation include defeasible logic [4] and BDI models [13].",
                "Although argument selection is a key aspect of automated argumentation (see [12] and [13]), most research has been focused on preference relations among arguments.",
                "In our framework we have addressed both argument selection and preference relations using a case-based approach. 11.",
                "CONCLUSIONS AND FUTURE WORK In this paper we have presented an argumentation-based framework for multi-agent learning.",
                "Specifically, we have presented AMAL, a framework that allows a group of learning agents to argue about the solution of a given problem and we have shown how the learning capabilities can be used to generate arguments and counterarguments.",
                "The experimental evaluation shows that the increased amount of information provided to the agents by the argumentation process increases their predictive accuracy, and specially when an adequate number of agents take part in the argumentation.",
                "The main contributions of this work are: a) an argumentation framework for learning agents; b) a case-based preference relation over arguments, based on computing an overall confidence estimation of arguments; c) a case-based policy to generate counterarguments and select counterexamples; and d) an argumentation-based approach for learning from communication.",
                "Finally, in the experiments presented here a learning agent would retain all counterexamples submitted by the other agent; however, this is a very simple case retention policy, and we will like to experiment with more informed policies - with the goal that individual learning agents could significantly improve using only a small set of cases proposed by other agents.",
                "Finally, our approach is focused on lazy learning, and future works aims at incorporating eager inductive learning inside the argumentative framework for learning from communication. 12.",
                "REFERENCES [1] Agnar Aamodt and Enric Plaza.",
                "Case-based reasoning: Foundational issues, methodological variations, and system approaches.",
                "Artificial Intelligence Communications, 7(1):39-59, 1994. [2] E. Armengol and E. Plaza.",
                "Lazy induction of descriptions for relational case-based learning.",
                "In ECML2001, pages 13-24, 2001. [3] Gerhard Brewka.",
                "Dynamic argument systems: A formal model of argumentation processes based on situation calculus.",
                "Journal of Logic and Computation, 11(2):257-282, 2001. [4] Carlos I. Chesñevar and Guillermo R. Simari.",
                "Formalizing Defeasible Argumentation using Labelled Deductive Systems.",
                "Journal of Computer Science & Technology, 1(4):18-33, 2000. [5] D. Leake and R. Sooriamurthi.",
                "Automatically selecting strategies for multi-<br>case-base reason</br>ing.",
                "In S. Craw and A. Preece, editors, ECCBR2002, pages 204-219, Berlin, 2002.",
                "Springer Verlag. [6] Francisco J. Martín, Enric Plaza, and Josep-Lluis Arcos.",
                "Knowledge and experience reuse through communications among competent (peer) agents.",
                "International Journal of Software Engineering and Knowledge Engineering, 9(3):319-341, 1999. [7] Lorraine McGinty and Barry Smyth.",
                "Collaborative case-based reasoning: Applications in personalized route planning.",
                "In I. Watson and Q. Yang, editors, ICCBR, number 2080 in LNAI, pages 362-376.",
                "Springer-Verlag, 2001. [8] Santi Ontañón and Enric Plaza.",
                "Justification-based multiagent learning.",
                "In ICML2003, pages 576-583.",
                "Morgan Kaufmann, 2003. [9] Enric Plaza, Eva Armengol, and Santiago Ontañón.",
                "The explanatory power of symbolic similarity in case-based reasoning.",
                "Artificial Intelligence Review, 24(2):145-161, 2005. [10] David Poole.",
                "On the comparison of theories: Preferring the most specific explanation.",
                "In IJCAI-85, pages 144-147, 1985. [11] M V Nagendra Prassad, Victor R Lesser, and Susan Lander.",
                "Retrieval and reasoning in distributed case bases.",
                "Technical report, UMass Computer Science Department, 1995. [12] K. Sycara S. Kraus and A. Evenchik.",
                "Reaching agreements through argumentation: a logical model and implementation.",
                "Artificial Intelligence Journal, 104:1-69, 1998. [13] N. R. Jennings S. Parsons, C. Sierra.",
                "Agents that reason and negotiate by arguing.",
                "Journal of Logic and Computation, 8:261-292, 1998. [14] Bruce A. Wooley.",
                "Explanation component of software systems.",
                "ACM CrossRoads, 5.1, 1998. 982 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "Finally, another interesting approach is multi-<br>case-base reason</br>ing (MCBR) [5], that deals with The Sixth Intl.",
                "Automatically selecting strategies for multi-<br>case-base reason</br>ing."
            ],
            "translated_annotated_samples": [
                "Finalmente, otro enfoque interesante es el <br>razonamiento de múltiples bases de casos</br> (MCBR) [5], que trata sobre The Sixth Intl.",
                "Seleccionando estrategias automáticamente para el <br>razonamiento multi-base</br> de casos."
            ],
            "translated_text": "Aprendizaje y deliberación conjunta a través de la argumentación en sistemas multiagente. Santi Ontañón CCL, Laboratorio de Computación Cognitiva, Instituto de Tecnología de Georgia, Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Instituto de Investigación en Inteligencia Artificial, CSIC, Consejo Superior de Investigaciones Científicas, Campus UAB, 08193 Bellaterra, Cataluña (España) enric@iiia.csic.es RESUMEN En este artículo presentaremos un marco de argumentación para agentes de aprendizaje (AMAL) diseñado con dos propósitos: (1) para deliberación conjunta, y (2) para aprender a través de la comunicación. El marco AMAL se basa completamente en el aprendizaje a partir de ejemplos: la relación de preferencia de argumentos, la política de generación de argumentos y la política de generación de contraargumentos son técnicas basadas en casos. Para unirse en la deliberación, los agentes de aprendizaje comparten su experiencia formando un comité para decidir sobre alguna decisión conjunta. Experimentalmente demostramos que la argumentación entre comités de agentes mejora tanto el rendimiento individual como el conjunto. Para aprender a través de la comunicación, un agente se involucra en discusiones con otros agentes para contrastar sus hipótesis individuales y recibir contraejemplos; el proceso de argumentación mejora su alcance de aprendizaje y rendimiento individual. Categorías y Descriptores de Asignaturas I.2.6 [Inteligencia Artificial]: Aprendizaje; I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente, Agentes Inteligentes 1. Los marcos de argumentación para sistemas multiagente pueden ser utilizados para diferentes propósitos como la deliberación conjunta, la persuasión, la negociación y la resolución de conflictos. En este artículo presentaremos un marco de argumentación para agentes de aprendizaje, y mostraremos que puede ser utilizado para dos propósitos: (1) deliberación conjunta, y (2) aprendizaje a partir de la comunicación. La deliberación conjunta basada en argumentos implica discutir sobre el resultado de una situación particular o la acción apropiada a tomar en una situación específica. Los agentes de aprendizaje son capaces de aprender de la experiencia, en el sentido de que ejemplos pasados (situaciones y sus resultados) se utilizan para predecir el resultado para la situación actual. Sin embargo, dado que la experiencia de los agentes individuales puede ser limitada, el conocimiento individual y la precisión de la predicción también son limitados. Por lo tanto, los agentes de aprendizaje que son capaces de argumentar sus predicciones individuales con otros agentes pueden alcanzar una mayor precisión en las predicciones después de dicho proceso de argumentación. La mayoría de los marcos de argumentación existentes para sistemas multiagente se basan en lógica deductiva u otra formalización de lógica deductiva diseñada específicamente para respaldar la argumentación, como la lógica por defecto [3]. Por lo general, un argumento se ve como una declaración lógica, mientras que un contraargumento es un argumento ofrecido en oposición a otro argumento; los agentes utilizan una relación de preferencia para resolver argumentos conflictivos. Sin embargo, los marcos de argumentación basados en lógica asumen agentes con conocimiento predefinido y relación de preferencia. En este artículo, nos enfocamos en un marco de Aprendizaje Multiagente basado en Argumentación (AMAL) donde tanto el conocimiento como la relación de preferencia se aprenden a partir de la experiencia. Por lo tanto, consideramos un escenario con agentes que (1) trabajan en el mismo dominio utilizando una ontología compartida, (2) son capaces de aprender a partir de ejemplos y (3) se comunican utilizando un marco argumentativo. Tener capacidades de aprendizaje permite a los agentes utilizar de manera efectiva una forma específica de contraargumento, a saber, el uso de contraejemplos. Los contraejemplos ofrecen la posibilidad de que los agentes aprendan durante el proceso de argumentación. Además, los agentes de aprendizaje permiten técnicas que utilizan la experiencia aprendida para generar argumentos y contraargumentos adecuados. Específicamente, necesitaremos abordar dos problemas: (1) cómo definir una técnica para generar argumentos y contraargumentos a partir de ejemplos, y (2) cómo definir una relación de preferencia sobre dos argumentos en conflicto que han sido inducidos a partir de ejemplos. Este documento presenta un enfoque basado en casos para abordar ambas cuestiones. Los agentes utilizan el razonamiento basado en casos (CBR) [1] para aprender de casos pasados (donde un caso es una situación y su resultado) con el fin de predecir el resultado de una nueva situación. Proponemos un protocolo de argumentación dentro del marco de AMAL que apoya a los agentes en llegar a una predicción conjunta sobre una situación o problema específico; además, el razonamiento necesario para respaldar el proceso de argumentación también se basará en casos. En particular, presentamos dos medidas basadas en casos, una para generar los argumentos y contraargumentos adecuados para una situación particular y otra para determinar la relación de preferencia entre los argumentos. Finalmente, evaluamos (1) si la argumentación entre agentes de aprendizaje puede producir una predicción conjunta que mejore el rendimiento de aprendizaje individual y (2) si el aprendizaje a partir de los contraejemplos transmitidos durante el proceso de argumentación aumenta el rendimiento individual con precisamente aquellos casos que se utilizan mientras discuten entre ellos. El documento está estructurado de la siguiente manera. La sección 2 discute la relación entre la argumentación, la colaboración y el aprendizaje. Luego, la Sección 3 introduce nuestro marco de trabajo de CBR multiagente (MAC) y el concepto de predicción justificada. Después de eso, la Sección 4 define formalmente nuestro marco de argumentación. Las secciones 5 y 6 presentan nuestra relación de preferencia basada en casos y las políticas de generación de argumentos, respectivamente. Más adelante, la Sección 7 presenta el protocolo de argumentación en nuestro marco AMAL. Después de eso, la Sección 8 presenta una ejemplificación del marco de argumentación. Finalmente, la Sección 9 presenta una evaluación empírica de nuestras dos hipótesis principales. El artículo concluye con secciones de trabajo relacionado y conclusiones. 2. ARGUMENTACIÓN, COLABORACIÓN Y APRENDIZAJE Tanto el aprendizaje como la colaboración son formas en las que un agente puede mejorar el rendimiento individual. De hecho, existe un claro paralelismo entre el aprendizaje y la colaboración en sistemas multiagentes, ya que ambos son formas en las que los agentes pueden lidiar con sus deficiencias. Mostremos cuáles son las principales motivaciones que un agente puede tener para aprender o colaborar. • Motivaciones para aprender: - Aumentar la calidad de la predicción, - Aumentar la eficiencia, - Ampliar el rango de problemas resolubles. • Motivaciones para colaborar: - Aumentar la calidad de la predicción, - Aumentar la eficiencia, - Ampliar el rango de problemas resolubles, - Ampliar el rango de recursos accesibles. Al observar las listas de motivación anteriores, podemos ver fácilmente que el aprendizaje y la colaboración están muy relacionados en los sistemas multiagente. De hecho, con la excepción del último elemento en la lista de motivaciones para colaborar, son dos extremos de un continuo de estrategias para mejorar el rendimiento. Un agente puede optar por aumentar el rendimiento mediante el aprendizaje, la colaboración o encontrando un punto intermedio que combine el aprendizaje y la colaboración para mejorar el rendimiento. En este artículo propondremos AMAL, un marco de argumentación para agentes de aprendizaje, y también mostraremos cómo AMAL puede ser utilizado tanto para aprender a través de la comunicación como para resolver problemas de manera colaborativa: • Los agentes pueden resolver problemas de manera colaborativa al participar en un proceso de argumentación sobre la predicción para la situación actual. Utilizando esta colaboración, la predicción puede realizarse de una manera más informada, ya que se ha tenido en cuenta la información conocida por varios agentes. Los agentes también pueden aprender de la comunicación con otros agentes al participar en un proceso de argumentación. Los agentes que participan en estos procesos de argumentación pueden aprender de los argumentos y contraejemplos recibidos de otros agentes, y utilizar esta información para predecir los resultados de situaciones futuras. En el resto de este documento propondremos un marco de argumentación y mostraremos cómo puede ser utilizado tanto para el aprendizaje como para la resolución de problemas de manera colaborativa. SISTEMAS CBR MULTIAGENTE Un Sistema de Razonamiento Basado en Casos Multiagente (MAC) M = {(A1, C1), ..., (An, Cn)} es un sistema multiagente compuesto por A = {Ai, ..., An}, un conjunto de agentes CBR, donde cada agente Ai ∈ A posee una base de casos individual Ci. Cada agente individual Ai en un MAC es completamente autónomo y cada agente Ai solo tiene acceso a su base de casos individual y privada Ci. Un conjunto de casos Ci = {c1, ..., cm} es una colección de casos. Los agentes en un sistema MAC pueden resolver problemas de forma individual, pero también pueden colaborar con otros agentes para resolver problemas. En este marco, nos limitaremos a tareas analíticas, es decir, tareas como la clasificación, donde la solución de un problema se logra seleccionando una clase de solución de un conjunto enumerado de clases de solución. En lo siguiente, notaremos el conjunto de todas las clases de soluciones por S = {S1, ..., SK}. Por lo tanto, un caso c = P, S es una tupla que contiene una descripción del caso P y una clase de solución S ∈ S. En lo siguiente, utilizaremos indistintamente los términos problema y descripción del caso. Además, utilizaremos la notación de punto para referirnos a elementos dentro de una tupla; por ejemplo, para referirnos a la clase de solución de un caso c, escribiremos c.S. Por lo tanto, decimos que un grupo de agentes realiza una deliberación conjunta cuando colaboran para encontrar una solución conjunta mediante un proceso de argumentación. Sin embargo, para hacerlo, un agente debe ser capaz de justificar su predicción a los otros agentes (es decir, generar un argumento para su solución predicha que pueda ser examinado y criticado por los otros agentes). La siguiente sección aborda este tema. 3.1 Predicciones Justificadas Tanto los sistemas expertos como los sistemas CBR pueden tener un componente de explicación [14] encargado de justificar por qué el sistema ha proporcionado una respuesta específica al usuario. La línea de razonamiento del sistema puede ser examinada por un experto humano, aumentando así la fiabilidad del sistema. La mayoría de los trabajos existentes sobre generación de explicaciones se centran en generar explicaciones para ser proporcionadas al usuario. Sin embargo, en nuestro enfoque utilizamos explicaciones (o justificaciones) como una herramienta para mejorar la comunicación y coordinación entre agentes. Estamos interesados en las justificaciones ya que pueden ser utilizadas como argumentos. Para ese propósito, nos beneficiaremos de la capacidad de algunos métodos de aprendizaje automático para proporcionar justificaciones. Una justificación construida por un método CBR después de determinar que la solución de un problema particular P fue Sk es una descripción que contiene la información relevante del problema P que el método CBR ha considerado para predecir Sk como la solución de P. En particular, los métodos CBR funcionan recuperando casos similares al problema en cuestión, y luego reutilizando sus soluciones para el problema actual, esperando que dado que el problema y los casos son similares, las soluciones también serán similares. Por lo tanto, si un método CBR ha recuperado un conjunto de casos C1, ..., Cn para resolver un problema particular P, la justificación construida contendrá la información relevante del problema P que hizo que el sistema CBR recuperara ese conjunto particular de casos, es decir, contendrá la información relevante que P y C1, ..., Cn tienen en común. Por ejemplo, la Figura 1 muestra una justificación construida por un sistema CBR para un problema de juguete (en las siguientes secciones mostraremos justificaciones para problemas reales). En la figura, un problema tiene dos atributos (Semáforo y Coches_pasando), el mecanismo de recuperación del sistema CBR nota que al considerar solo el atributo Semáforo, puede recuperar dos casos que predicen la misma solución: esperar. Por lo tanto, dado que solo se ha utilizado este atributo, es el único que aparece en la justificación. Los valores del resto de atributos son irrelevantes, ya que, sea cual sea su valor, la clase de solución habría sido la misma. 976 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Problema Semáforo: rojo Pasan coches: no Caso 1 Semáforo: rojo Pasan coches: no Solución: esperar Caso 3 Semáforo: rojo Pasan coches: sí Solución: esperar Caso 4 Semáforo: verde Pasan coches: sí Solución: esperar Caso 2 Semáforo: verde Pasan coches: no Solución: cruzar Casos recuperados Solución: esperar Justificación Semáforo: rojo Figura 1: Un ejemplo de generación de justificación en un sistema de RCB. Observa que, dado que la única característica relevante para decidir es Traffic_light (la única utilizada para recuperar casos), es la única que aparece en la justificación. En general, el significado de una justificación es que todos (o la mayoría) de los casos en la base de casos de un agente que satisfacen la justificación (es decir, todos los casos que están subsumidos por la justificación) pertenecen a la clase de solución predicha. En el resto del documento, usaremos para denotar la relación de subsumpción. En nuestro trabajo, utilizamos LID [2], un método CBR capaz de construir justificaciones simbólicas como la ejemplificada en la Figura 1. Cuando un agente proporciona una justificación para una predicción, el agente genera una predicción justificada: DEFINICIÓN 3.1. Una Predicción Justificada es una tupla J = A, P, S, D donde el agente A considera que S es la solución correcta para el problema P, y esa predicción está justificada por una descripción simbólica D tal que J.D J.P. Las justificaciones pueden tener muchos usos para los sistemas de RCB [8, 9]. En este documento, vamos a utilizar justificaciones como argumentos, con el fin de permitir que los agentes de aprendizaje participen en procesos de argumentación. 4. ARGUMENTOS Y CONTRAARGUMENTOS Para nuestros propósitos, un argumento α generado por un agente A está compuesto por una afirmación S y alguna evidencia D que respalda a S como correcta. En el resto de esta sección veremos cómo esta definición general de argumento puede ser concretada en un tipo específico de argumentos que los agentes pueden generar. En el contexto de los sistemas MAC, los agentes discuten sobre predicciones para nuevos problemas y pueden proporcionar dos tipos de información: a) casos específicos P, S, y b) predicciones justificadas: A, P, S, D. Usando esta información, podemos definir tres tipos de argumentos: predicciones justificadas, contraargumentos y contraejemplos. Una predicción justificada α es generada por un agente Ai para argumentar que Ai cree que la solución correcta para un problema dado P es α.S, y la evidencia proporcionada es la justificación α.D. En el ejemplo representado en la Figura 1, un agente Ai puede generar el argumento α = Ai, P, Esperar, (Semáforo = rojo), lo que significa que el agente Ai cree que la solución correcta para P es Esperar porque el atributo Semáforo es igual a rojo. Un contraargumento β es un argumento ofrecido en oposición a otro argumento α. En nuestro marco de trabajo, un contraargumento consiste en una predicción justificada Aj, P, S, D generada por un agente Aj con la intención de refutar un argumento α generado por otro agente Ai, que respalda una clase de solución S diferente a la de α.S para el problema en cuestión y justifica esto con una justificación D. En el ejemplo de la Figura 1, si un agente genera el argumento α = Ai, P, Caminar, (Pasando_autos = no), un agente que piensa que la solución correcta es Esperar podría responder con el contraargumento β = Aj, P, Esperar, (Pasando_autos = no ∧ Semáforo = rojo), lo que significa que, aunque no hay autos pasando, el semáforo está en rojo y la calle no se puede cruzar. Un contraejemplo c es un caso que contradice un argumento α. Por lo tanto, un contraejemplo también es un contraargumento, uno que afirma que un argumento específico α no siempre es verdadero, y la evidencia proporcionada es el caso c. Específicamente, para que un caso c sea un contraejemplo de un argumento α, se deben cumplir las siguientes condiciones: α.D c y α.S = c.S, es decir, el caso debe satisfacer la justificación α.D y la solución de c debe ser diferente a la predicha por α. Al intercambiar argumentos y contraargumentos (incluyendo contraejemplos), los agentes pueden discutir sobre la solución correcta de un problema dado, es decir, pueden participar en un proceso de deliberación conjunto. Sin embargo, para hacerlo, necesitan un protocolo de interacción específico, una relación de preferencia entre argumentos contradictorios y una política de decisión para generar contraargumentos (incluidos contraejemplos). En las siguientes secciones presentaremos estos elementos. 5. RELACIÓN DE PREFERENCIA Un argumento específico proporcionado por un agente podría no ser consistente con la información conocida por otros agentes (o incluso con parte de la información conocida por el agente que ha generado la justificación debido al ruido en los datos de entrenamiento). Por esa razón, vamos a definir una relación de preferencia sobre predicciones justificadas contradictorias basadas en casos. Básicamente, definiremos una medida de confianza para cada predicción justificada (que tenga en cuenta los casos poseídos por cada agente), y la predicción justificada con la mayor confianza será la preferida. La idea detrás de la confianza basada en casos es contar cuántos de los casos en una base de casos individual respaldan una predicción justificada, y cuántos de ellos son ejemplos en contra de ella. Cuanto más casos de respaldo, mayor es la confianza; y cuanto más contrajemplos, menor es la confianza. Específicamente, para evaluar la confianza de una predicción justificada α, un agente obtiene el conjunto de casos en su base de casos individuales que están subsumidos por α.D. Con ellos, un agente Ai obtiene los valores Y (sí) y N (no): • Y Ai α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| es el número de casos en la base de casos del agente subsumidos por la justificación α.D que pertenecen a la clase de solución α.S, • NAi α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| es el número de casos en la base de casos del agente subsumidos por la justificación α.D que no pertenecen a esa clase de solución. El Sexto Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 977 + + + + + +  - - + Figura 2: La confianza de los argumentos se evalúa contrastándolos con las bases de casos de los agentes. Un agente estima la confianza de un argumento como: CAi (α) = Y Ai α 1 + Y Ai α + NAi α es decir, la confianza en una predicción justificada es el número de casos que la respaldan dividido por el número de casos que la respaldan más los contraejemplos. Observa que sumamos 1 al denominador, esto es para evitar otorgar confianzas excesivamente altas a predicciones justificadas cuya confianza se ha calculado utilizando un número pequeño de casos. Ten en cuenta que esta corrección sigue la misma idea que la corrección de Laplace para estimar probabilidades. La Figura 2 ilustra la evaluación individual de la confianza de un argumento, en particular, se encuentran tres casos de respaldo y un contraejemplo en la base de casos de los agentes Ai, lo que da una confianza estimada de 0.6. Además, también podemos definir la confianza conjunta de un argumento α como la confianza calculada utilizando los casos presentes en las bases de casos de todos los agentes en el grupo: C(α) = Π Ai α / (1 + Σ Ai α + NAi α). Nótese que, para calcular conjuntamente la confianza conjunta, los agentes solo tienen que hacer públicos los valores de aprobación y rechazo calculados localmente para un argumento dado. En nuestro marco de trabajo, los agentes utilizan esta confianza conjunta como la relación de preferencia: una predicción justificada α se prefiere sobre otra β si C(α) ≥ C(β). GENERACIÓN DE ARGUMENTOS En nuestro marco de trabajo, los argumentos son generados por los agentes a partir de casos, utilizando métodos de aprendizaje. Cualquier método de aprendizaje capaz de proporcionar una predicción justificada puede ser utilizado para generar argumentos. Por ejemplo, los árboles de decisión y LID [2] son métodos de aprendizaje adecuados. Específicamente, en los experimentos reportados en este artículo, los agentes utilizan LID. Por lo tanto, cuando un agente desea generar un argumento que respalde que una clase de solución específica es la solución correcta para un problema P, genera una predicción justificada como se explica en la Sección 3.1. Por ejemplo, la Figura 3 muestra una justificación real generada por LID después de resolver un problema P en el dominio de la identificación de esponjas marinas. En particular, la Figura 3 muestra cómo cuando un agente recibe un nuevo problema para resolver (en este caso, un nuevo esponja para determinar su orden), el agente utiliza LID para generar un argumento (que consiste en una predicción justificada) utilizando los casos en la base de casos del agente. La justificación mostrada en la Figura 3 se puede interpretar diciendo que la solución predicha es hadromerida porque la forma lisa de las megascleras del esqueleto espicular de la esponja es de tipo tilostilo, el esqueleto espicular de la esponja no tiene una longitud uniforme y no hay gemas en las características externas de la esponja. Por lo tanto, el argumento generado será α = A1, P, hadromerida, D1. 6.1 Generación de Contraargumentos Como se mencionó anteriormente, los agentes pueden intentar refutar argumentos generando contraargumentos o encontrando contraejemplos. Permítanos explicar cómo pueden ser generados. Un agente Ai quiere generar un contraargumento β para rebatir un argumento α cuando α está en contradicción con la base de casos locales de Ai. Además, al generar dicho contraargumento β, Ai espera que β sea preferido sobre α. Con ese propósito, presentaremos una política específica para generar contraargumentos basados en el criterio de especificidad [10]. El criterio de especificidad se utiliza ampliamente en marcos deductivos para la argumentación, y establece que entre dos argumentos en conflicto, se debe preferir el más específico ya que, en principio, está más informado. Por lo tanto, se espera que los contraargumentos generados basados en el criterio de especificidad sean preferibles (ya que están más informados) que los argumentos que intentan refutar. Sin embargo, no hay garantía de que tales contraargumentos siempre ganen, ya que, como hemos indicado en la Sección 5, los agentes en nuestro marco utilizan una relación de preferencia basada en la confianza conjunta. Además, uno podría pensar que sería mejor que los agentes generaran contraargumentos basados en la relación de preferencia de confianza conjunta; sin embargo, no es obvio cómo generar contraargumentos basados en la confianza conjunta de manera eficiente, ya que se requiere colaboración para evaluar la confianza conjunta. Por lo tanto, el agente que genera el contraargumento debe comunicarse constantemente con los otros agentes en cada paso del algoritmo de inducción utilizado para generar contraargumentos (actualmente una de nuestras líneas de investigación futuras). Por lo tanto, en nuestro marco de trabajo, cuando un agente quiere generar un contraargumento β a un argumento α, β tiene que ser más específico que α (es decir, α.D < β.D). La generación de contraargumentos utilizando el criterio de especificidad impone algunas restricciones sobre el método de aprendizaje, aunque LID o ID3 pueden adaptarse fácilmente para esta tarea. Por ejemplo, LID es un algoritmo que genera una descripción desde cero y añade heurísticamente características a ese término. Por lo tanto, en cada paso, la descripción se vuelve más específica que en el paso anterior, y el número de casos que se incluyen en esa descripción se reduce. Cuando la descripción cubre solo (o casi solo) casos de una única clase de solución, LID termina y predice esa clase de solución. Para generar un contraargumento a un argumento α, LID solo tiene que usar como punto de partida la descripción α.D en lugar de empezar desde cero. De esta manera, la justificación proporcionada por LID siempre estará subsumida por α.D, y por lo tanto, el contraargumento resultante será más específico que α. Sin embargo, hay que tener en cuenta que LID a veces puede no ser capaz de generar contraargumentos, ya que LID puede no ser capaz de especializar aún más la descripción α.D, o porque el agente Ai no tiene ningún caso en Ci que esté subsumido por α.D. La Figura 4 muestra cómo un agente A2 que no estaba de acuerdo con el argumento mostrado en la Figura 3, genera un contraargumento utilizando LID. Además, la Figura 4 muestra la generación de un contraargumento β1 2 para el argumento α0 1 (en la Figura 3) que es una especialización de α0 1. 978 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Solución: hadromerida Justificación: D1 Esponja Esqueleto espiculado Características externas Características externas Gemmulas: no Esqueleto espiculado Megascleres Longitud uniforme: no Megascleres Forma lisa: tilostilo Caso Base de A1 LID Nueva esponja P Figura 3: Ejemplo de una justificación real generada por LID en el conjunto de datos de esponjas marinas. Específicamente, en nuestros experimentos, cuando un agente Ai quiere refutar un argumento α, utiliza la siguiente política: 1. El Agente Ai utiliza LID para intentar encontrar un contraargumento β más específico que α; si se encuentra, β se envía al otro agente como un contraargumento de α. 2. Si no se encuentra, entonces Ai busca un contraejemplo c ∈ Ci de α. Si se encuentra un caso c, entonces c se envía al otro agente como un contraejemplo de α. Si no se encuentran contraejemplos, entonces Ai no puede refutar el argumento α. 7. APRENDIZAJE MULTIAGENTE BASADO EN ARGUMENTACIÓN El protocolo de interacción de AMAL permite que un grupo de agentes A1, ..., An deliberen sobre la solución correcta de un problema P mediante un proceso de argumentación. Si el proceso de argumentación llega a una solución consensuada, la deliberación conjunta termina; de lo contrario, se utiliza una votación ponderada para determinar la solución conjunta. Además, AMAL también permite a los agentes aprender de los contraejemplos recibidos de otros agentes. El protocolo AMAL consiste en una serie de rondas. En la ronda inicial, cada agente declara cuál es su predicción individual para P. Luego, en cada ronda, un agente puede intentar refutar la predicción hecha por cualquiera de los otros agentes. El protocolo utiliza un mecanismo de paso de token para que los agentes (uno a la vez) puedan enviar contraargumentos o contraejemplos si no están de acuerdo con la predicción hecha por cualquier otro agente. Específicamente, a cada agente se le permite enviar un contraargumento o contraejemplo cada vez que recibe el token (nota que esta restricción es solo para simplificar el protocolo, y que no restringe la cantidad de contraargumentos que un agente puede enviar, ya que pueden ser retrasados para rondas posteriores). Cuando un agente recibe un contraargumento o contraejemplo, informa a los otros agentes si acepta el contraargumento (y cambia su predicción) o no. Además, los agentes también tienen la oportunidad de responder a los contraargumentos cuando reciben el token, intentando generar un contraargumento al contraargumento. Cuando todos los agentes hayan tenido el token una vez, el token regresa al primer agente, y así sucesivamente. Si en algún momento del protocolo, todos los agentes están de acuerdo o durante las últimas n rondas ningún agente ha generado ninguna contra-argumentación, el protocolo termina. Además, si al final de la argumentación los agentes no han llegado a un acuerdo, entonces se utiliza un mecanismo de votación que utiliza la confianza de cada predicción como pesos para decidir la solución final (Así, AMAL sigue el mismo mecanismo que los comités humanos, primero cada miembro individual de un comité expone sus argumentos y discute los de los otros miembros (deliberación conjunta), y si no se alcanza un consenso, entonces se requiere un mecanismo de votación). En cada iteración, los agentes pueden usar los siguientes performativos: • afirmar(α): la predicción justificada que se mantendrá durante la próxima ronda será α. Un agente solo puede tener una única predicción en cada ronda, por lo tanto, si se envían múltiples afirmaciones, solo se considera la última como la predicción actualmente mantenida. • rebut(β, α): el agente ha encontrado un contraargumento β a la predicción α. Definiremos Ht = αt 1, ..., αt n como las predicciones que cada uno de los n agentes tiene en una ronda t. Además, también definiremos contradict(αt i) = {α ∈ Ht|α.S = αt i.S} como el conjunto de argumentos contradictorios para un agente Ai en una ronda t, es decir, el conjunto de argumentos en la ronda t que respaldan una clase de solución diferente a la de αt i. El protocolo se inicia porque uno de los agentes recibe un problema P que debe ser resuelto. Después de eso, el agente informa a todos los demás agentes sobre el problema P a resolver, y el protocolo comienza: 1. En la ronda t = 0, cada uno de los agentes resuelve individualmente P y construye una predicción justificada utilizando su propio método de RBC. Entonces, cada agente Ai envía el assertivo performativo assert(α0 i) a los otros agentes. Por lo tanto, los agentes conocen H0 = α0 i , ..., α0 n. Una vez que todas las predicciones hayan sido enviadas, el token se le da al primer agente A1. 2. En cada ronda t (distinta de 0), los agentes verifican si sus argumentos en Ht coinciden. Si lo hacen, el protocolo avanza al paso 5. Además, si durante las últimas n rondas ningún agente ha enviado ningún contraejemplo o contraargumento, el protocolo también avanza al paso 5. De lo contrario, el agente Ai propietario del token intenta generar un contraargumento para cada uno de los argumentos opuestos en contradict(αt i) ⊆ Ht (ver Sección 6.1). Entonces, se selecciona el contraargumento βt i contra la predicción αt j con la menor confianza C(αt j) (ya que αt j es la predicción más probable de ser refutada con éxito). • Si βt i es un contraargumento, entonces, Ai compara localmente αt i con βt i evaluando su confianza contra su base de casos individuales Ci (ver Sección 5) (nótese que Ai está comparando su argumento previo con el contraargumento que Ai mismo acaba de generar y que trata sobre The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 979 Esponja Esqueleto espiculado Características externas Características externas Gemas: no Crecimiento: Esqueleto espiculado Megascleres Longitud uniforme: no Megascleres Forma lisa: tilostilo Crecimiento Crecimiento: masivo Caso Base de A2 LID Solución: astrophorida Justificación: D2 Figura 4: Generación de un contraargumento utilizando LID en el conjunto de datos de esponjas. para enviar a Aj. Si CAi (βt i) > CAi (αt i), entonces Ai considera que βt i es más fuerte que su argumento anterior, cambia su argumento a βt i enviando assert(βt i) al resto de los agentes (la intuición detrás de esto es que dado que un contraargumento también es un argumento, Ai verifica si el nuevo contraargumento es un mejor argumento que el que sostenía anteriormente) y rebut(βt i, αt j) a Aj. De lo contrario (es decir, Si CAi (βt i ) ≤ CAi (αt i), Ai solo enviará rebut(βt i , αt j) a Aj. En cualquiera de las dos situaciones, el protocolo avanza al paso 3. • Si βt i es un contraejemplo c, entonces Ai envía una refutación(c, αt j) a Aj. El protocolo avanza al paso 4. • Si Ai no puede generar ningún contraargumento o contraejemplo, el token se envía al siguiente agente, comienza una nueva ronda t + 1, y el protocolo avanza al estado 2. 3. El agente Aj que ha recibido el contraargumento βt i, lo compara localmente con su propio argumento, αt j, evaluando localmente su confianza. Si CAj (βt i ) > CAj (αt j), entonces Aj aceptará el contraargumento como más fuerte que su propio argumento, y enviará assert(βt i ) a los otros agentes. De lo contrario (es decir, Si CAj (βt i ) ≤ CAj (αt j)), Aj no aceptará el contraargumento e informará a los otros agentes en consecuencia. Cualquiera de las dos situaciones inicia una nueva ronda t + 1, Ai envía el token al siguiente agente y el protocolo vuelve al estado 2. 4. El agente Aj que ha recibido el contraejemplo c lo retiene en su base de casos y genera un nuevo argumento αt+1 j que tiene en cuenta c, e informa al resto de los agentes enviando assert(αt+1 j) a todos ellos. Entonces, Ai envía el token al siguiente agente, comienza una nueva ronda t + 1 y el protocolo vuelve al paso 2. 5. El protocolo finaliza generando una predicción conjunta, de la siguiente manera: si los argumentos en Ht están de acuerdo, entonces su predicción es la predicción conjunta; de lo contrario, se utiliza un mecanismo de votación para decidir la predicción conjunta. El mecanismo de votación utiliza la medida de confianza conjunta como los pesos de votación, de la siguiente manera: S = arg max Sk∈S αi∈Ht|αi.S=Sk C(αi) Además, para evitar iteraciones infinitas, si un agente envía dos veces el mismo argumento o contraargumento al mismo agente, el mensaje no se considera. 8. EJEMPLIFICACIÓN Consideremos un sistema compuesto por tres agentes A1, A2 y A3. Uno de los agentes, A1, recibe un problema P para resolver y decide usar AMAL para resolverlo. Por esa razón, invita a A2 y A3 a participar en el proceso de argumentación. Aceptan la invitación y comienza el protocolo de argumentación. Inicialmente, cada agente genera su predicción individual para P y la transmite a los otros agentes. Por lo tanto, todos ellos pueden calcular H0 = α0 1, α0 2, α0 3. En particular, en este ejemplo: • α0 1 = A1, P, hadromerida, D1 • α0 2 = A2, P, astrophorida, D2 • α0 3 = A3, P, axinellida, D3 A1 comienza poseyendo el token e intenta generar argumentos en contra de α0 2 y α0 3, pero no tiene éxito; sin embargo, tiene un contraejemplo c13 para α0 3. Por lo tanto, A1 envía el mensaje rebut(c13, α03) a A3. A3 incorpora c13 en su base de casos e intenta resolver el problema P nuevamente, ahora teniendo en cuenta c13. A3 elabora la predicción justificada α1 3 = A3, P, hadromerida, D4, y la transmite al resto de los agentes con el mensaje assert(α1 3). Por lo tanto, todos ellos conocen el nuevo H1 = α0 1, α0 2, α1 3. La ronda 1 comienza y A2 obtiene el token. A2 intenta generar contraargumentos para α0 1 y α1 3 y solo logra generar un contraargumento β1 2 = A2, P, astrophorida, D5 contra α1 3. El contraargumento se envía a A3 con el mensaje rebut(β1 2 , α1 3). El agente A3 recibe el contraargumento y evalúa su confianza local. El resultado es que la confianza individual del contraargumento β1 2 es menor que la confianza local de α1 3. Por lo tanto, A3 no acepta el contraargumento, y así H2 = α0 1, α0 2, α1 3. La ronda 2 comienza y A3 recibe el token. A3 genera un contraargumento β2 3 = A3, P, hadromerida, D6 para α0 2 y lo envía a A2 con el mensaje refutar(β2 3 , α0 2). El agente A2 recibe el contraargumento y evalúa su confianza local. El resultado es que la confianza local del contraargumento β2 3 es mayor que la confianza local de α0 2. Por lo tanto, A2 acepta el contraargumento e informa al resto de los agentes con el mensaje assert(β2 3). Después de eso, H3 = α0 1, β2 3 , α1 3. En la Ronda 3, dado que todos los agentes están de acuerdo (todas las predicciones justificadas en H3 predicen hadromerida como la clase de solución), el protocolo finaliza, y A1 (el agente que recibió el problema) considera hadromerida como la solución conjunta para el problema P. 9. EVALUACIÓN EXPERIMENTAL 980 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) SPONGE 75 77 79 81 83 85 87 89 91 2 3 4 5 AMAL Votación Individual SOYBEAN 55 60 65 70 75 80 85 90 2 3 4 5 AMAL Votación Individual Figura 5: Precisión individual y conjunta para 2 a 5 agentes. En esta sección evaluamos empíricamente el marco de argumentación AMAL. Hemos realizado experimentos en dos conjuntos de datos diferentes: soja (del repositorio de aprendizaje automático de UCI) y esponja (un conjunto de datos relacional). El conjunto de datos de soja tiene 307 ejemplos y 19 clases de solución, mientras que el conjunto de datos de esponja tiene 280 ejemplos y 3 clases de solución. En una ejecución experimental, el conjunto de datos se divide en 2 conjuntos: el conjunto de entrenamiento y el conjunto de prueba. Los ejemplos del conjunto de entrenamiento están distribuidos entre 5 agentes diferentes sin replicación, es decir, no hay ningún ejemplo compartido por dos agentes. En la etapa de pruebas, los problemas en el conjunto de pruebas llegan aleatoriamente a uno de los agentes, y su objetivo es predecir la solución correcta. Los experimentos están diseñados para probar dos hipótesis: (H1) que la argumentación es un marco útil para la deliberación conjunta y puede mejorar sobre otros métodos típicos como la votación; y (H2) que el aprendizaje a través de la comunicación mejora el rendimiento individual de un agente de aprendizaje que participa en un proceso de argumentación. Además, también esperamos que la mejora lograda a través de la argumentación aumente a medida que aumente el número de agentes que participan en la argumentación (ya que se tomará en cuenta más información). En relación con H1 (la argumentación es un marco útil para la deliberación conjunta), realizamos 4 experimentos, utilizando 2, 3, 4 y 5 agentes respectivamente (en todos los experimentos cada agente tiene un 20% de los datos de entrenamiento, ya que el entrenamiento siempre se distribuye entre 5 agentes). La Figura 5 muestra el resultado de esos experimentos en los conjuntos de datos de esponjas y soja. La precisión de la clasificación se representa en el eje vertical, y en el eje horizontal se muestra el número de agentes que participaron en los procesos de argumentación. Para cada número de agentes, se muestran tres barras: individual, Voting y AMAL. La barra individual muestra la precisión promedio de las predicciones de los agentes individuales; la barra de votación muestra la precisión promedio de la predicción conjunta lograda mediante votación pero sin argumentación; y finalmente la barra de AMAL muestra la precisión promedio de la predicción conjunta utilizando argumentación. Los resultados mostrados son el promedio de 5 ejecuciones de validación cruzada de 10 pliegues. La Figura 5 muestra que la colaboración (votación y AMAL) supera el rendimiento de la resolución individual de problemas. Además, como esperábamos, la precisión mejora a medida que más agentes colaboran, ya que se tiene en cuenta más información. También podemos ver que AMAL siempre supera al voto estándar, demostrando que las decisiones conjuntas se basan en una mejor información proporcionada por el proceso de argumentación. Por ejemplo, la precisión conjunta para 2 agentes en el conjunto de datos de esponjas es del 87.57% para AMAL y del 86.57% para la votación (mientras que la precisión individual es solo del 80.07%). Además, la mejora lograda por AMAL sobre Voting es aún mayor en el conjunto de datos de soja. La razón es que el conjunto de datos de soja es más difícil (en el sentido de que los agentes necesitan más datos para producir buenas predicciones). Estos resultados experimentales muestran que AMAL aprovecha de manera efectiva la oportunidad de mejora: la precisión es mayor solo porque más agentes han cambiado su opinión durante la argumentación (de lo contrario, lograrían el mismo resultado que la Votación). En relación con H2 (el aprendizaje a partir de la comunicación en procesos de argumentación mejora la predicción individual), llevamos a cabo el siguiente experimento: inicialmente, distribuimos un 25% del conjunto de entrenamiento entre los cinco agentes; después, el resto de los casos en el conjunto de entrenamiento se envía a los agentes uno por uno; cuando un agente recibe un nuevo caso de entrenamiento, tiene varias opciones: el agente puede descartarlo, el agente puede retenerlo, o el agente puede usarlo para participar en un proceso de argumentación. La Figura 6 muestra el resultado de ese experimento para los dos conjuntos de datos. La Figura 6 contiene tres gráficos, donde NL (no aprendizaje) muestra la precisión de un agente sin aprendizaje alguno; L (aprendizaje) muestra la evolución de la precisión de clasificación individual cuando los agentes aprenden reteniendo los casos de entrenamiento que reciben individualmente (notar que cuando todos los casos de entrenamiento han sido retenidos al 100%, la precisión debería ser igual a la de la Figura 5 para los agentes individuales); y finalmente LFC (aprendizaje a partir de la comunicación) muestra la evolución de la precisión de clasificación individual de los agentes de aprendizaje que también aprenden reteniendo los contraejemplos recibidos durante la argumentación (es decir, aprenden tanto de ejemplos de entrenamiento como de contraejemplos). La Figura 6 muestra que si un agente Ai también aprende de la comunicación, Ai puede mejorar significativamente su rendimiento individual con solo un pequeño número de casos adicionales (seleccionados como contraejemplos relevantes para Ai durante la argumentación). Por ejemplo, en el conjunto de datos de soja, los agentes individuales han logrado una precisión del 70.62% cuando también aprenden de la comunicación, en comparación con una precisión del 59.93% cuando solo aprenden de su experiencia individual. El número de casos aprendidos a través de la comunicación depende de las propiedades del conjunto de datos: en el conjunto de datos de esponjas, los agentes solo han retenido muy pocos casos adicionales y han mejorado significativamente la precisión individual; es decir, retienen en promedio 59.96 casos (en comparación con los 50.4 casos retenidos si no aprenden de la comunicación). En el conjunto de datos de soja se aprenden más contraejemplos para mejorar significativamente la precisión individual, es decir, retienen en promedio 87.16 casos (en comparación con 55.27 casos retenidos si no aprenden de la comunicación). Finalmente, el hecho de que ambos conjuntos de datos muestren una mejora significativa señala la naturaleza adaptativa del enfoque basado en la argumentación para el aprendizaje a partir de la comunicación: los casos útiles se seleccionan como contraejemplos (y no más de los necesarios), y tienen el efecto deseado. 10. TRABAJO RELACIONADO En cuanto al RCB en un entorno multiagente, la primera investigación se centró en la recuperación de casos negociada [11] entre grupos de agentes. Nuestro trabajo sobre el aprendizaje basado en casos multiagente comenzó en 1999; más tarde Mc Ginty y Smyth presentaron un enfoque colaborativo de CBR multiagente (CCBR) para la planificación. Finalmente, otro enfoque interesante es el <br>razonamiento de múltiples bases de casos</br> (MCBR) [5], que trata sobre The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 981 ESPONJA 60 65 70 75 80 85 25% 40% 55% 70% 85% 100% LFC L NL SOJA 20 30 40 50 60 70 80 90 25% 40% 55% 70% 85% 100% LFC L NL Figura 6: Aprendizaje a partir de la comunicación resultante de la argumentación en un sistema compuesto por 5 agentes. sistemas distribuidos donde hay varias bases de casos disponibles para la misma tarea y aborda los problemas de adaptación entre bases de casos. La principal diferencia es que nuestro enfoque MAC es una forma de distribuir el proceso de Reutilización de CBR (utilizando un sistema de votación) mientras que la Recuperación se realiza individualmente por cada agente; sin embargo, los otros enfoques de CBR multiagente se centran en distribuir el proceso de Recuperación. La investigación sobre la argumentación en MAS se centra en varios temas como a) lógicas, protocolos y lenguajes que respaldan la argumentación, b) selección de argumentos y c) interpretación de argumentos. Los enfoques para la lógica y los lenguajes que respaldan la argumentación incluyen la lógica no definitiva [4] y los modelos BDI [13]. Aunque la selección de argumentos es un aspecto clave de la argumentación automatizada (ver [12] y [13]), la mayoría de las investigaciones se han centrado en las relaciones de preferencia entre argumentos. En nuestro marco de trabajo hemos abordado tanto la selección de argumentos como las relaciones de preferencia utilizando un enfoque basado en casos. 11. CONCLUSIONES Y TRABAJO FUTURO En este artículo hemos presentado un marco basado en argumentación para el aprendizaje multiagente. Específicamente, hemos presentado AMAL, un marco que permite a un grupo de agentes de aprendizaje discutir sobre la solución de un problema dado y hemos demostrado cómo las capacidades de aprendizaje pueden ser utilizadas para generar argumentos y contraargumentos. La evaluación experimental muestra que la cantidad aumentada de información proporcionada a los agentes por el proceso de argumentación aumenta su precisión predictiva, especialmente cuando un número adecuado de agentes participa en la argumentación. Las principales contribuciones de este trabajo son: a) un marco de argumentación para agentes de aprendizaje; b) una relación de preferencia basada en casos sobre argumentos, basada en el cálculo de una estimación general de confianza de los argumentos; c) una política basada en casos para generar contraargumentos y seleccionar contraejemplos; y d) un enfoque basado en la argumentación para aprender a partir de la comunicación. Finalmente, en los experimentos presentados aquí, un agente de aprendizaje retendría todos los contraejemplos presentados por el otro agente; sin embargo, esta es una política de retención de casos muy simple, y nos gustaría experimentar con políticas más informadas, con el objetivo de que los agentes de aprendizaje individuales puedan mejorar significativamente utilizando solo un pequeño conjunto de casos propuestos por otros agentes. Finalmente, nuestro enfoque se centra en el aprendizaje perezoso, y los trabajos futuros tienen como objetivo incorporar el aprendizaje inductivo ansioso dentro del marco argumentativo para el aprendizaje a partir de la comunicación. 12. REFERENCIAS [1] Agnar Aamodt y Enric Plaza. Razonamiento basado en casos: Aspectos fundamentales, variaciones metodológicas y enfoques de sistemas. Comunicaciones de Inteligencia Artificial, 7(1):39-59, 1994. [2] E. Armengol y E. Plaza. Inducción perezosa de descripciones para el aprendizaje basado en casos relacionales. En ECML2001, páginas 13-24, 2001. [3] Gerhard Brewka. Sistemas de argumentación dinámica: Un modelo formal de procesos de argumentación basado en el cálculo de situaciones. Revista de Lógica y Computación, 11(2):257-282, 2001. [4] Carlos I. Chesñevar y Guillermo R. Simari. Formalizando la argumentación no definitiva utilizando sistemas deductivos etiquetados. Revista de Ciencias de la Computación y Tecnología, 1(4):18-33, 2000. [5] D. Leake y R. Sooriamurthi. Seleccionando estrategias automáticamente para el <br>razonamiento multi-base</br> de casos. En S. Craw y A. Preece, editores, ECCBR2002, páginas 204-219, Berlín, 2002. Springer Verlag. [6] Francisco J. Martín, Enric Plaza y Josep-Lluis Arcos. Reutilización de conocimiento y experiencia a través de comunicaciones entre agentes competentes (pares). Revista Internacional de Ingeniería de Software e Ingeniería del Conocimiento, 9(3):319-341, 1999. [7] Lorraine McGinty y Barry Smyth. Razonamiento basado en casos colaborativo: Aplicaciones en la planificación de rutas personalizadas. En I. Watson y Q. Yang, editores, ICCBR, número 2080 en LNAI, páginas 362-376. Springer-Verlag, 2001. [8] Santi Ontañón y Enric Plaza. Aprendizaje multiagente basado en justificaciones. En ICML2003, páginas 576-583. Morgan Kaufmann, 2003. [9] Enric Plaza, Eva Armengol y Santiago Ontañón. El poder explicativo de la similitud simbólica en el razonamiento basado en casos. Revisión de Inteligencia Artificial, 24(2):145-161, 2005. [10] David Poole. En la comparación de teorías: prefiriendo la explicación más específica. En IJCAI-85, páginas 144-147, 1985. [11] M V Nagendra Prassad, Victor R Lesser y Susan Lander. Recuperación y razonamiento en bases de casos distribuidas. Informe técnico, Departamento de Ciencias de la Computación de UMass, 1995. [12] K. Sycara S. Kraus y A. Evenchik. Alcanzando acuerdos a través de la argumentación: un modelo lógico y su implementación. Revista de Inteligencia Artificial, 104:1-69, 1998. [13] N. R. Jennings, S. Parsons, C. Sierra. Agentes que razonan y negocian mediante argumentos. Revista de Lógica y Computación, 8:261-292, 1998. [14] Bruce A. Wooley. Componente de explicación de sistemas de software. ACM CrossRoads, 5.1, 1998. 982 The Sixth Intl. \n\nACM CrossRoads, 5.1, 1998. 982 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ",
            "candidates": [],
            "error": [
                [
                    "razonamiento de múltiples bases de casos",
                    "razonamiento multi-base"
                ]
            ]
        }
    }
}