{
    "id": "H-20",
    "original_text": "New Event Detection Based on Indexing-tree and Named Entity Zhang Kuo Tsinghua University Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Tsinghua University Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Tsinghua University Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn ABSTRACT New Event Detection (NED) aims at detecting from one or multiple streams of news stories that which one is reported on a new event (i.e. not reported previously). With the overwhelming volume of news available today, there is an increasing need for a NED system which is able to detect new events more efficiently and accurately. In this paper we propose a new NED model to speed up the NED task by using news indexing-tree dynamically. Moreover, based on the observation that terms of different types have different effects for NED task, two term reweighting approaches are proposed to improve NED accuracy. In the first approach, we propose to adjust term weights dynamically based on previous story clusters and in the second approach, we propose to employ statistics on training data to learn the named entity reweighting model for each class of stories. Experimental results on two Linguistic Data Consortium (LDC) datasets TDT2 and TDT3 show that the proposed model can improve both efficiency and accuracy of NED task significantly, compared to the baseline system and other existing systems. Categories and Subject Descriptors H.3.3 [Information Systems]: Information Search and Retrieval; H.4.2 [Information Systems Applications]: Types of Systemsdecision support. General Terms Algorithms, Performance, Experimentation 1. INTRODUCTION Topic Detection and Tracking (TDT) program aims to develop techniques which can effectively organize, search and structure news text materials from a variety of newswire and broadcast media [1]. New Event Detection (NED) is one of the five tasks in TDT. It is the task of online identification of the earliest report for each topic as soon as that report arrives in the sequence of documents. A Topic is defined as a seminal event or activity, along with directly related events and activities [2]. An Event is defined as something (non-trivial) happening in a certain place at a certain time [3]. For instance, when a bomb explodes in a building, the exploding is the seminal event that triggers the topic, and other stories on the same topic would be those discussing salvaging efforts, the search for perpetrators, arrests and trial and so on. Useful news information is usually buried in a mass of data generated everyday. Therefore, NED systems are very useful for people who need to detect novel information from real-time news stream. These real-life needs often occur in domains like financial markets, news analysis, and intelligence gathering. In most of state-of-the-art (currently) NED systems, each news story on hand is compared to all the previous received stories. If all the similarities between them do not exceed a threshold, then the story triggers a new event. They are usually in the form of cosine similarity or Hellinger similarity metric. The core problem of NED is to identify whether two stories are on the same topic. Obviously, these systems cannot take advantage of topic information. Further more, it is not acceptable in real applications because of the large amount of computation required in the NED process. Other systems organize previous stories into clusters (each cluster corresponds to a topic), and new story is compared to the previous clusters instead of stories. This manner can reduce comparing times significantly. Nevertheless, it has been proved that this manner is less accurate [4, 5]. This is because sometimes stories within a topic drift far away from each other, which could lead low similarity between a story and its topic. On the other hand, some proposed NED systems tried to improve accuracy by making better use of named entities [10, 11, 12, 13]. However, none of the systems have considered that terms of different types (e.g. Noun, Verb or Person name) have different effects for different classes of stories in determining whether two stories are on the same topic. For example, the names of election candidates (Person name) are very important for stories of election class; the locations (Location name) where accidents happened are important for stories of accidents class. So, in NED, there still exist following three problems to be investigated: (1) How to speed up the detection procedure while do not decrease the detection accuracy? (2) How to make good use of cluster (topic) information to improve accuracy? (3) How to obtain better news story representation by better understanding of named entities. Driven by these problems, we have proposed three approaches in this paper. (1)To make the detection procedure faster, we propose a new NED procedure based on news indexing-tree created dynamically. Story indexing-tree is created by assembling similar stories together to form news clusters in different hierarchies according to their values of similarity. Comparisons between current story and previous clusters could help find the most similar story in less comparing times. The new procedure can reduce the amount of comparing times without hurting accuracy. (2)We use the clusters of the first floor in the indexing-tree as news topics, in which term weights are adjusted dynamically according to term distribution in the clusters. In this approach, cluster (topic) information is used properly, so the problem of theme decentralization is avoided. (3)Based on observations on the statistics obtained from training data, we found that terms of different types (e.g. Noun and Verb) have different effects for different classes of stories in determining whether two stories are on the same topic. And we propose to use statistics to optimize the weights of the terms of different types in a story according to the news class that the story belongs to. On TDT3 dataset, the new NED model just uses 14.9% comparing times of the basic model, while its minimum normalized cost is 0.5012, which is 0.0797 better than the basic model, and also better than any other results previously reported for this dataset [8, 13]. The rest of the paper is organized as follows. We start off this paper by summarizing the previous work in NED in section 2. Section 3 presents the basic model for NED that most current systems use. Section 4 describes our new detection procedure based on news indexing-tree. In section 5, two term reweighting methods are proposed to improve NED accuracy. Section 6 gives our experimental data and evaluation metrics. We finally wrap up with the experimental results in Section 7, and the conclusions and future work in Section 8. 2. RELATED WORK Papka et al. proposed Single-Pass clustering on NED [6]. When a new story was encountered, it was processed immediately to extract term features and a query representation of the storys content is built up. Then it was compared with all the previous queries. If the document did not trigger any queries by exceeding a threshold, it was marked as a new event. Lam et al build up previous query representations of story clusters, each of which corresponds to a topic [7]. In this manner comparisons happen between stories and clusters. Recent years, most work focus on proposing better methods on comparison of stories and document representation. Brants et al. [8] extended a basic incremental TF-IDF model to include sourcespecific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, term reweighting based on inverse event frequencies, and segmentation of documents. Good improvements on TDT bench-marks were shown. Stokes et al. [9] utilized a combination of evidence from two distinct representations of a documents content. One of the representations was the usual free text vector, the other made use of lexical chains (created using WordNet) to build another term vector. Then the two representations are combined in a linear fashion. A marginal increase in effectiveness was achieved when the combined representation was used. Some efforts have been done on how to utilize named entities to improve NED. Yang et al. gave location named entities four times weight than other terms and named entities [10]. DOREMI research group combined semantic similarities of person names, location names and time together with textual similarity [11][12]. UMass [13] research group split document representation into two parts: named entities and non-named entities. And it was found that some classes of news could achieve better performance using named entity representation, while some other classes of news could achieve better performance using non-named entity representation. Both [10] and [13] used text categorization technique to classify news stories in advance. In [13] news stories are classified automatically at first, and then test sensitivities of names and non-name terms for NED for each class. In [10] frequent terms for each class are removed from document representation. For example, word election does not help identify different elections. In their work, effectiveness of different kinds of names (or terms with different POS) for NED in different news classes are not investigated. We use statistical analysis to reveal the fact and use it to improve NED performance. 3. BASIC MODEL In this section, we present the basic New Event Detection model which is similar to what most current systems apply. Then, we propose our new model by extending the basic model. New Event Detection systems use news story stream as input, in which stories are strictly time-ordered. Only previously received stories are available when dealing with current story. The output is a decision for whether the current story is on a new event or not and the confidence of the decision. Usually, a NED model consists of three parts: story representation, similarity calculation and detection procedure. 3.1 Story Representation Preprocessing is needed before generating story representation. For preprocessing, we tokenize words, recognize abbreviations, normalize abbreviations, add part-of-speech tags, remove stopwords included in the stop list used in InQuery [14], replace words with their stems using K-stem algorithm[15], and then generate word vector for each news story. We use incremental TF-IDF model for term weight calculation [4]. In a TF-IDF model, term frequency in a news document is weighted by the inverse document frequency, which is generated from training corpus. When a new term occurs in testing process, there are two solutions: simply ignore the new term or set df of the term as a small const (e.g. df = 1). The new term receives too low weight in the first solution (0) and too high weight in the second solution. In incremental TF-IDF model, document frequencies are updated dynamically in each time step t: 1( ) ( ) ( )t t D tdf w df w df w−= + (1) where Dt represents news story set received in time t, and dfDt(w) means the number of documents that term w occurs in, and dft(w) means the total number of documents that term w occurs in before time t. In this work, each time window includes 50 news stories. Thus, each story d received in t is represented as follows: 1 2{ ( , , ), ( , , ),..., ( , , )}nd weight d t w weight d t w weight d t w→ where n means the number of distinct terms in story d, and ( , , )weight d t w means the weight of term w in story d at time t: log( ( , ) 1) log(( 1) /( ( ) 0.5)) ( , , ) log( ( , ) 1) log(( 1) /( ( ) 0.5)) t t t t w d tf d w N df w weight d t w tf d w N df w ∈ + + + = + + +∑ (2) where Nt means the total number of news stories before time t, and tf(d,w) means how many times term w occurs in news story d. 3.2 Similarity Calculation We use Hellinger distance for the calculation of similarity between two stories, for two stories d and d at time t, their similarity is defined as follows: , ( , , ) ( , , ) * ( , , ) w d d sim d d t weight d t w weight d t w ∈ = ∑ (3) 3.3 Detection Procedure For each story d received in time step t, the value ( ) ( ) ( ) ( ( , , )) time d time d n d max sim d d t < = (4) is a score used to determine whether d is a story about a new topic and at the same time is an indication of the confidence in our decision [8]. time(d) means the publication time of story d. If the score exceeds the thresholdθ new, then there exists a sufficiently similar document, thus d is a old story, otherwise, there is no sufficiently similar previous document, thus d is an new story. 4. New NED Procedure Traditional NED systems can be classified into two main types on the aspect of detection procedure: (1) S-S type, in which the story on hand is compared to each story received previously, and use the highest similarity to determine whether current story is about a new event; (2) S-C type, in which the story on hand is compared to all previous clusters each of which representing a topic, and the highest similarity is used for final decision for current story. If the highest similarity exceeds thresholdθ new, then it is an old story, and put it into the most similar cluster; otherwise it is a new story and create a new cluster. Previous work show that the first manner is more accurate than the second one [4][5]. Since sometimes stories within a topic drift far away from each other, a story may have very low similarity with its topic. So using similarities between stories for determining new story is better than using similarities between story and clusters. Nevertheless, the first manner needs much more comparing times which means the first manner is low efficient. We propose a new detection procedure which uses comparisons with previous clusters to help find the most similar story in less comparing times, and the final new event decision is made according to the most similar story. Therefore, we can get both the accuracy of S-S type methods and the efficiency of S-C type methods. The new procedure creates a news indexing-tree dynamically, in which similar stories are put together to form a hierarchy of clusters. We index similar stories together by their common ancestor (a cluster node). Dissimilar stories are indexed in different clusters. When a story is coming, we use comparisons between the current story and previous hierarchical clusters to help find the most similar story which is useful for new event decision. After the new event decision is made, the current story is inserted to the indexing-tree for the following detection. The news indexing-tree is defined formally as follows: S-Tree = {r, NC , NS , E} where r is the root of S-Tree, NC is the set of all cluster nodes, NS is the set of all story nodes, and E is the set of all edges in S-Tree. We define a set of constraints for a S-Tree: ⅰ . , is an non-terminal node in the treeC i i N i∀ ∈ → ⅱ . , is a terminal node in the treeS i i N i∀ ∈ → ⅲ . , out degree of is at least 2C i i N i∀ ∈ → ⅳ . , is represented as the centroid of its desendantsC i i iN∀ ∈ → For a news story di, the comparison procedure and inserting procedure based on indexing-tree are defined as follows. An example is shown by Figure 1 and Figure 2. Figure 1. Comparison procedure Figure 2. Inserting procedure Comparison procedure: Step 1: compare di to all the direct child nodes of r and select λ nodes with highest similarities, e.g., C1 2 and C1 3 in Figure 1. Step 2: for each selected node in the last step, e.g. C1 2, compare di to all its direct child nodes, and select λ nodes with highest similarities, e.g. C2 2 and d8. Repeat step 2 for all non-terminal nodes. Step 3: record the terminal node with the highest similarty to di, e.g. s5, and the similarity value (0.20). Inserting di to the S-tree with r as root: Find the node n which is direct child of r in the path from r to the terminal node with highest similarity s, e.g. C1 2. If s is smaller than θ init+(h-1)δ , then add di to the tree as a direct child of r. Otherwise, if n is a terminal node, then create a cluster node instead of n, and add both n and di as its direct children; if n is an non-terminal node, then repeat this procedure and insert di to the sub-tree with n as root recursively. Here h is the length between n and the root of S-tree. The more the stories in a cluster similar to each other, the better the cluster represents the stories in it. Hence we add no constraints on the maximum of trees height and degree of a node. Therefore, we cannot give the complexity of this indexing-tree based procedure. But we will give the number of comparing times needed by the new procedure in our experiments in section7. 5. Term Reweighting Methods In this section, two term reweighting methods are proposed to improve NED accuracy. In the first method, a new way is explored for better using of cluster (topic) information. The second one finds a better way to make use of named entities based on news classification. 5.1 Term Reweighting Based on Distribution Distance TF-IDF is the most prevalent model used in information retrieval systems. The basic idea is that the fewer documents a term appears in, the more important the term is in discrimination of documents (relevant or not relevant to a query containing the term). Nevertheless, in TDT domain, we need to discriminate documents with regard to topics rather than queries. Intuitively, using cluster (topic) vectors to compare with subsequent news stories should outperform using story vectors. Unfortunately, the experimental results do not support this intuition [4][5]. Based on observation on data, we find the reason is that a news topic usually contains many directly or indirectly related events, while they all have their own sub-subjects which are usually different with each other. Take the topic described in section 1 as an example, events like the explosion and salvage have very low similarities with events about criminal trial, therefore stories about trial would have low similarity with the topic vector built on its previous events. This section focuses on how to effectively make use of topic information and at the same time avoid the problem of content decentralization. At first, we classify terms into 5 classes to help analysis the needs of the modified model: Term class A: terms that occur frequently in the whole corpus, e.g., year and people. Terms of this class should be given low weights because they do not help much for topic discrimination. Term class B: terms that occur frequently within a news category, e.g., election, storm. They are useful to distinguish two stories in different news categories. However, they cannot provide information to determine whether two stories are on the same or different topics. In another words, term election and term storm are not helpful in differentiate two election campaigns and two storm disasters. Therefore, terms of this class should be assigned lower weights. Term class C: terms that occur frequently in a topic, and infrequently in other topics, e.g., the name of a crash plane, the name of a specific hurricane. News stories that belong to different topics rarely have overlap terms in this class. The more frequently a term appears in a topic, the more important the term is for a story belonging to the topic, therefore the term should be set higher weight. Term class D: terms that appear in a topic exclusively, but not frequently. For example, the name of a fireman who did very well in a salvage action, which may appears in only two or three stories but never appeared in other topics. Terms of this type should receive more weights than in TF-IDF model. However, since they are not popular in the topic, it is not appropriate to give them too high weights. Term class E: terms with low document frequency, and appear in different topics. Terms of this class should receive lower weights. Now we analyze whether TF-IDF model can give proper weights to the five classes of terms. Obviously, terms of class A are lowly weighted in TF-IDF model, which is conformable with the requirement described above. In TF-IDF model, terms of class B are highly dependant with the number of stories in a news class. TF-IDF model cannot provide low weights if the story containing the term belongs to a relative small news class. For a term of class C, the more frequently it appears in a topic, the less weight TFIDF model gives to it. This strongly conflicts with the requirement of terms in class C. For terms of class D, TF-IDF model gives them high weights correctly. But for terms of class E, TF-IDF model gives high weights to them which are not conformable with the requirement of low weights. To sum up, terms of class B, C, E cannot be properly weighted in TF-IDF model. So, we propose a modified model to resolve this problem. When θ init andθ new are set closely, we assume that most of the stories in a first-level cluster (a direct child node of root node) are on the same topic. Therefore, we make use of a first-level cluster to capture term distribution (df for all the terms within the cluster) within the topic dynamically. KL divergence of term distribution in a first-level cluster and the whole story set is used to adjust term weights:  ( , , ) * (1 * ( || )) ( , , ) ( , , ) * (1 * ( || )) cw tw cw tw w d D weight d t w KL P P weight d t w weight d t w KL P P γ γ ∈ + = +∑ (5) where ( ) ( ) ( ) ( ) 1,cw cw c c c c df w df w p y p y N N = = − (6) ( ) ( ) ( ) ( ) 1,t t tw tw t t df w df w p y p y N N = = − (7) where dfc(w) is the number of documents containing term w within cluster C, and Nc is the number of documents in cluster C, and Nt is the total number of documents that arrive before time step t. γ is a const parameter, now is manually set 3. KL divergence is defined as follows [17]: ( ) ( || ) ( ) log ( )x p x KL P Q p x q x = ∑ (8) The basic idea is: for a story in a topic, the more a term occurs within the topic, and the less it occurs in other topics, it should be assigned higher weights. Obviously, modified model can meet all the requirements of the five term classes listed above. 5.2 Term Reweighting Based on Term Type and Story Class Previous work found that some classes of news stories could achieve good improvements by giving extra weight to named entities. But we find that terms of different types should be given different amount of extra weight for different classes of news stories. We use open-NLP1 to recognize named entity types and part-ofspeech tags for terms that appear in news stories. Named entity types include person name, organization name, location name, date, time, money and percentage, and five POSs are selected: none (NN), verb (VB), adjective (JJ), adverb (RB) and cardinal number (CD). Statistical analysis shows topic-level discriminative terms types for different classes of stories. For the sake of convenience, named entity type and part-of-speech tags are uniformly called term type in subsequent sections. Determining whether two stories are about the same topic is a basic component for NED task. So at first we use 2 χ statistic to compute correlations between terms and topics. For a term t and a topic T, a contingence table is derived: Table 1. A 2×2 Contingence Table Doc Number belong to topic T not belong to topic T include t A B not include t C D The 2 χ statistic for a specific term t with respect to topic T is defined to be [16]: 2 2 ( , ) ( ) * ( ) ( ) * ( ) * ( ) * ( ) w T A B C D AD CB A C B D A B C D χ = + + + − + + + + (9) News topics for the TDT task are further classified into 11 rules of interpretations (ROIs) 2 . The ROI can be seen as a higher level class of stories. The average correlation between a term type and a topic ROI is computed as: 2 avg 2 ( , )( ( , ) )k m m km kT R w P w TP R p w T R P χ χ ∈ ∈ ∑ ∑（ , ）= 1 1 k=1…K, m=1…M (10) where K is the number of term types (set 12 constantly in the paper). M is the number news classes (ROIs, set 11 in the paper). Pk represents the set of all terms of type k, and Rm represents the set of all topics of class m, p(t,T) means the probability that t occurs in topic T. Because of limitation of space, only parts of the term types (9 term types) and parts of news classes (8 classes) are listed in table 2 with the average correlation values between them. The statistics is derived from labeled data in TDT2 corpus. (Results in table 2 are already normalized for convenience in comparison.) The statistics in table 2 indicates the usefulness of different term types in topic discrimination with respect to different news classes. We can see that, location name is the most useful term type for three news classes: Natural Disasters, Violence or War, Finances. And for three other categories Elections, Legal/Criminal Cases, Science and Discovery, person name is the most discriminative term type. For Scandals/Hearings, date is the most important information for topic discrimination. In addition, Legal/Criminal Cases and Finance topics have higher correlation with money terms, while Science and Discovery have higher correlation with percentage terms. Non-name terms are more stable for different classes. 1 . http://opennlp.sourceforge.net/ 2 . http://projects.ldc.upenn.edu/TDT3/Guide/label.html From the analysis of table 2, it is reasonable to adjust term weight according to their term type and the news class the story belongs to. New term weights are reweighted as follows: ( ) ( ) ( ) ( ) ( , , ) * ( , , ) ( , , ) * class d D type w T class d D type w w d weight d t w weight d t w weight d t w α α ∈ = ∑ (11) where type(w) represents the type of term w, and class(d) represents the class of story d, c kα is reweighting parameter for news class c and term type k. In the work, we just simply use statistics in table 2 as the reweighting parameters. Even thought using the statistics directly may not the best choice, we do not discuss how to automatically obtain the best parameters. We will try to use machine learning techniques to obtain the best parameters in the future work. In the work, we use BoosTexter [20] to classify all stories into one of the 11 ROIs. BoosTexter is a boosting based machine learning program, which creates a series of simple rules for building a classifier for text or attribute-value data. We use term weight generated using TF-IDF model as feature for story classification. We trained the model on the 12000 judged English stories in TDT2, and classify the rest of the stories in TDT2 and all stories in TDT3. Classification results are used for term reweighting in formula (11). Since the class labels of topic-off stories are not given in TDT datasets, we cannot give the classification accuracy here. Thus we do not discuss the effects of classification accuracy to NED performance in the paper. 6. EXPERIMENTAL SETUP 6.1 Datasets We used two LDC [18] datasets TDT2 and TDT3 for our experiments. TDT2 contains news stories from January to June 1998. It contains around 54,000 stories from sources like ABC, Associated Press, CNN, New York Times, Public Radio International, Voice of America etc. Only English stories in the collection were considered. TDT3 contains approximately 31,000 English stories collected from October to December 1998. In addition to the sources used in TDT2, it also contains stories from NBC and MSNBC TV broadcasts. We used transcribed versions of the TV and radio broadcasts besides textual news. TDT2 dataset is labeled with about 100 topics, and approximately 12,000 English stories belong to at least one of these topics. TDT3 dataset is labeled with about 120 topics, and approximately 8000 English stories belong to at least one of these topics. All the topics are classified into 11 Rules of Interpretation: (1)Elections, (2)Scandals/Hearings, (3)Legal/Criminal Cases, (4)Natural Disasters, (5)Accidents, (6)Ongoing Violence or War, (7)Science and Discovery News, (8)Finance, (9)New Law, (10)Sports News, (11)MISC. News. 6.2 Evaluation Metric TDT uses a cost function CDet that combines the probabilities of missing a new story and a false alarm [19]: * * * *Det Miss Miss Target FA FA NontargetC C P P C P P= + (12) Table 2. Average correlation between term types and news classes where CMiss means the cost of missing a new story, PMiss means the probability of missing a new story, and PTarget means the probability of seeing a new story in the data; CFA means the cost of a false alarm, PFA means the probability of a false alarm, and PNontarget means the probability of seeing an old story. The cost CDet is normalized such that a perfect system scores 0 and a trivial system, which is the better one of mark all stories as new or old, scores 1: ( ( * , * ) ) Det Det Miss Target FA Nontarget C Norm C min C P C P = (13) New event detection system gives two outputs for each story. The first part is yes or no indicating whether the story triggers a new event or not. The second part is a score indicating confidence of the first decision. Confidence scores can be used to plot DET curve, i.e., curves that plot false alarm vs. miss probabilities. Minimum normalized cost can be determined if optimal threshold on the score were chosen. 7. EXPERIMENTAL RESULTS 7.1 Main Results To test the approaches proposed in the model, we implemented and tested five systems: System-1: this system is used as baseline. It is implemented based on the basic model described in section 3, i.e., using incremental TF-IDF model to generate term weights, and using Hellinger distance to compute document similarity. Similarity score normalization is also employed [8]. S-S detection procedure is used. System-2: this system is the same as system-1 except that S-C detection procedure is used. System-3: this system is the same as system-1 except that it uses the new detection procedure which is based on indexing-tree. System-4: implemented based on the approach presented in section 5.1, i.e., terms are reweighted according to the distance between term distributions in a cluster and all stories. The new detection procedure is used. System-5: implemented based on the approach presented in section 5.2, i.e., terms of different types are reweighted according to news class using trained parameters. The new detection procedure is used. The following are some other NED systems: System-6: [21] for each pair of stories, it computes three similarity values for named entity, non-named entity and all terms respectively. And employ Support Vector Machine to predict new or old using the similarity values as features. System-7: [8] it extended a basic incremental TF-IDF model to include source-specific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, etc. System-8: [13] it split document representation into two parts: named entities and non-named entities, and choose one effective part for each news class. Table 3 and table 4 show topic-weighted normalized costs and comparing times on TDT2 and TDT3 datasets respectively. Since no heldout data set for fine-tuning the threshold θ new was available for experiments on TDT2, we only report minimum normalized costs for our systems in table 3. System-5 outperforms all other systems including system-6, and it performs only 2.78e+8 comparing times in detection procedure which is only 13.4% of system-1. Table 3. NED results on TDT2 Systems Min Norm(CDet) Cmp times System-1 0.5749 2.08e+9 System-2① 0.6673 3.77e+8 System-3② 0.5765 2.81e+8 System-4② 0.5431 2.99e+8 System-5② 0.5089 2.78e+8 System-6 0.5300 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 When evaluating on the normalized costs on TDT3, we use the optimal thresholds obtained from TDT2 data set for all systems. System-2 reduces comparing times to 1.29e+9 which is just 18.3% of system-1, but at the same time it also gets a deteriorated minimum normalized cost which is 0.0499 higher than system-1. System-3 uses the new detection procedure based on news indexing-tree. It requires even less comparing times than system-2. This is because story-story comparisons usually yield greater similarities than story-cluster ones, so stories tend to be combined Location Person Date Organization Money Percentage NN JJ CD Elections 0.37 1 0.04 0.58 0.08 0.03 0.32 0.13 0.1 Scandals/Hearings 0.66 0.62 0.28 1 0.11 0.02 0.27 0.13 0.05 Legal/Criminal Cases 0.48 1 0.02 0.62 0.15 0 0.22 0.24 0.09 Natural Disasters 1 0.27 0 0.04 0.04 0 0.25 0.04 0.02 Violence or War 1 0.36 0.02 0.14 0.02 0.04 0.21 0.11 0.02 Science and Discovery 0.11 1 0.01 0.22 0.08 0.12 0.19 0.08 0.03 Finances 1 0.45 0.04 0.98 0.13 0.02 0.29 0.06 0.05 Sports 0.16 0.27 0.01 1 0.02 0 0.11 0.03 0.01 together in system-3. And system-3 is basically equivalent to system-1 in accuracy results. System-4 adjusts term weights based on the distance of term distributions between the whole corpus and cluster story set, yielding a good improvement by 0.0468 compared to system-1. The best system (system-5) has a minimum normalized cost 0.5012, which is 0.0797 better than system-1, and also better than any other results previously reported for this dataset [8, 13]. Further more, system-5 only needs 1.05e+8 comparing times which is 14.9% of system-1. Table 4. NED results on TDT3 Systems Norm(CDet) Min Norm(CDet) Cmp times System-1 0.6159 0.5809 7.04e+8 System-2① 0.6493 0.6308 1.29e+8 System-3② 0.6197 0.5868 1.03e+8 System-4② 0.5601 0.5341 1.03e+8 System-5② 0.5413 0.5012 1.05e+8 System-7 -- 0.5783 -System-8 -- 0.5229 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 Figure5 shows the five DET curves for our systems on data set TDT3. System-5 achieves the minimum cost at a false alarm rate of 0.0157 and a miss rate of 0.4310. We can observe that System4 and System-5 obtain lower miss probability at regions of low false alarm probabilities. The hypothesis is that, more weight value is transferred to key terms of topics from non-key terms. Similarity score between two stories belonging to different topics are lower than before, because their overlapping terms are usually not key terms of their topics. 7.2 Parameter selection for indexing-tree detection Figure 3 shows the minimum normalized costs obtained by system-3 on TDT3 using different parameters. Theθ init parameter is tested on six values spanning from 0.03 to 0.18. And the λ parameter is tested on four values 1, 2, 3 and 4. We can see that, whenθ init is set to 0.12, which is the closest one toθ new, the costs are lower than others. This is easy to explain, because when stories belonging to the same topic are put in a cluster, it is more reasonable for the cluster to represent the stories in it. When parameter λ is set to 3 or 4, the costs are better than other cases, but there is no much difference between 3 and 4. 0 0.05 0.1 0.15 0.2 1 2 3 4 0.5 0.6 0.7 0.8 0.9 1 θ-initλ MinCost 0.6 0.65 0.7 0.75 0.8 0.85 0.9 Figure 3. Min Cost on TDT3 (δ =0.15) 0 0.05 0.1 0.15 0.2 1 2 3 4 0 0.5 1 1.5 2 2.5 x 10 8 θ-init λ Comparingtimes 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 8 Figure 4. Comparing times on TDT3 (δ =0.15) Figure 4 gives the comparing times used by system-3 on TDT3 with the same parameters as figure 3. The comparing times are strongly dependent onθ init. Because the greaterθ init is, the less stories combined together, the more comparing times are needed for new event decision. So we useθ init =0.13,λ =3,δ =0.15 for system-3, 4, and 5. In this parameter setting, we can get both low minimum normalized costs and less comparing times. 8. CONCLUSION We have proposed a news indexing-tree based detection procedure in our model. It reduces comparing times to about one seventh of traditional method without hurting NED accuracy. We also have presented two extensions to the basic TF-IDF model. The first extension is made by adjust term weights based on term distributions between the whole corpus and a cluster story set. And the second extension to basic TF-IDF model is better use of term types (named entities types and part-of-speed) according to news categories. Our experimental results on TDT2 and TDT3 datasets show that both of the two extensions contribute significantly to improvement in accuracy. We did not consider news time information as a clue for NED task, since most of the topics last for a long time and TDT data sets only span for a relative short period (no more than 6 months). For the future work, we want to collect news set which span for a longer period from internet, and integrate time information in NED task. Since topic is a relative coarse-grained news cluster, we also want to refine cluster granularity to event-level, and identify different events and their relations within a topic. Acknowledgments This work is supported by the National Natural Science Foundation of China under Grant No. 90604025. Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsor. 9. REFERENCES [1] http://www.nist.gov/speech/tests/tdt/index.htm [2] In Topic Detection and Tracking. Event-based Information Organization. Kluwer Academic Publishers, 2002. .01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90 1 2 5 10 20 40 60 80 90 False Alarm Probability (in %) MissProbability(in%) SYSTEM1 Topic Weighted Curve SYSTEM1 Min Norm(Cost) SYSTEM2 Topic Weighted Curve SYSTEM2 Min Norm(Cost) SYSTEM3 Topic Weighted Curve SYSTEM3 Min Norm(Cost) SYSTEM4 Topic Weighted Curve SYSTEM4 Min Norm(Cost) SYSTEM5 Topic Weighted Curve SYSTEM5 Min Norm(Cost) Random Performance Figure 5. DET curves on TDT3 [3] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B.T. Archibald, and X. Liu. Learning Approaches for Detecting and Tracking News Events. In IEEE Intelligent Systems Special Issue on Applications of Intelligent Information Retrieval, volume 14 (4), 1999, 32-43. [4] Y. Yang, T. Pierce, and J. Carbonell. A Study on Retrospective and On-line Event Detection. In Proceedings of SIGIR-98, Melbourne, Australia, 1998, 28-36. [5] J. Allan, V. Lavrenko, D. Malin, and R. Swan. Detections, Bounds, and Timelines: Umass and tdt-3. In Proceedings of Topic Detection and Tracking Workshop (TDT-3), Vienna, VA, 2000, 167-174. [6] R. Papka and J. Allan. On-line New Event Detection Using Single Pass Clustering TITLE2:. Technical Report UM-CS1998-021, 1998. [7] W. Lam, H. Meng, K. Wong, and J. Yen. Using Contextual Analysis for News Event Detection. International Journal on Intelligent Systems, 2001, 525-546. [8] B. Thorsten, C. Francine, and F. Ayman. A System for New Event Detection. In Proceedings of the 26th Annual International ACM SIGIR Conference, New York, NY, USA. ACM Press. 2003, 330-337. [9] S. Nicola and C. Joe. Combining Semantic and Syntactic Document Classifiers to Improve First Story Detection. In Proceedings of the 24th Annual International ACM SIGIR Conference, New York, NY, USA. ACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell, and C. Jin. Topicconditioned Novelty Detection. In Proceedings of the 8th ACM SIGKDD International Conference, ACM Press. 2002, 688-693. [11] M. Juha, A.M. Helena, and S. Marko. Applying Semantic Classes in Event Detection and Tracking. In Proceedings of International Conference on Natural Language Processing (ICON 2002), 2002, pages 175-183. [12] M. Juha, A.M. Helena, and S. Marko. Simple Semantics in Topic Detection and Tracking. Information Retrieval, 7(3-4): 2004, 347-368. [13] K. Giridhar and J. Allan. Text Classification and Named Entities for New Event Detection. In Proceedings of the 27th Annual International ACM SIGIR Conference, New York, NY, USA. ACM Press. 2004, 297-304. [14] J. P. Callan, W. B. Croft, and S. M. Harding. The INQUERY Retrieval System. In Proceedings of DEXA-92, 3rd International Conference on Database and Expert Systems Applications, 1992, 78-83. [15] R. Krovetz. Viewing Morphology as An Inference Process. In Proceedings of ACM SIGIR93, 1993, 61-81. [16] Y. Yang and J. Pedersen. A Comparative Study on Feature Selection in Text Categorization. In J. D. H. Fisher, editor, The Fourteenth International Conference on Machine Learning (ICML97), Morgan Kaufmann, 1997, 412-420. [17] T. M. Cover, and J.A. Thomas. Elements of Information Theory. Wiley. 1991. [18] The linguistic data consortium, http://www.ldc,upenn.edu/. [19] The 2001 TDT task definition and evaluation plan, http://www.nist.gov/speech/tests/tdt/tdt2001/evalplan.htm. [20] R. E. Schapire and Y. Singer. Boostexter: A Boosting-based System for Text Categorization. In Machine Learning 39(2/3):1, Kluwer Academic Publishers, 2000, 35-168. [21] K. Giridhar and J. Allan. 2005. Using Names and Topics for New Event Detection. In Proceedings of Human Technology Conference and Conference on Empirical Methods in Natural Language, Vancouver, 2005, 121-128",
    "original_translation": "Detección de nuevos eventos basada en árbol de indexación y entidad nombrada Zhang Kuo Universidad Tsinghua Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Universidad Tsinghua Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Universidad Tsinghua Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn RESUMEN La Detección de Nuevos Eventos (NED) tiene como objetivo detectar de una o varias corrientes de noticias cuál se informa sobre un nuevo evento (es decir, no reportado previamente). Con el abrumador volumen de noticias disponible hoy en día, hay una creciente necesidad de un sistema NED que sea capaz de detectar eventos nuevos de manera más eficiente y precisa. En este artículo proponemos un nuevo modelo de NED para acelerar la tarea de NED mediante el uso dinámico de un árbol de indexación de noticias. Además, basándose en la observación de que los términos de diferentes tipos tienen diferentes efectos para la tarea de NED, se proponen dos enfoques de reponderación de términos para mejorar la precisión de NED. En el primer enfoque, proponemos ajustar dinámicamente los pesos de los términos basados en los clusters de historias previas y en el segundo enfoque, proponemos emplear estadísticas en los datos de entrenamiento para aprender el modelo de reajuste de entidades nombradas para cada clase de historias. Los resultados experimentales en dos conjuntos de datos del Consorcio de Datos Lingüísticos (LDC), TDT2 y TDT3, muestran que el modelo propuesto puede mejorar significativamente tanto la eficiencia como la precisión de la tarea de NED, en comparación con el sistema base y otros sistemas existentes. Categorías y Descriptores de Asignaturas H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información; H.4.2 [Aplicaciones de Sistemas de Información]: Tipos de Sistemas de Apoyo a Decisiones. Términos generales Algoritmos, Rendimiento, Experimentación 1. El programa de Detección y Seguimiento de Temas (TDT) tiene como objetivo desarrollar técnicas que puedan organizar, buscar y estructurar de manera efectiva materiales de texto de noticias de una variedad de agencias de noticias y medios de difusión [1]. La Detección de Eventos Nuevos (DEN) es una de las cinco tareas en TDT. Es la tarea de identificación en línea del informe más temprano para cada tema tan pronto como ese informe llegue en la secuencia de documentos. Un tema se define como un evento o actividad seminal, junto con eventos y actividades directamente relacionados [2]. Un evento se define como algo (no trivial) que sucede en un lugar y en un momento determinados [3]. Por ejemplo, cuando una bomba explota en un edificio, la explosión es el evento seminal que desencadena el tema, y otras historias sobre el mismo tema serían aquellas que discuten los esfuerzos de salvamento, la búsqueda de los perpetradores, arrestos y juicios, entre otros. La información útil de noticias suele estar enterrada en una masa de datos generados a diario. Por lo tanto, los sistemas NED son muy útiles para las personas que necesitan detectar información novedosa de un flujo de noticias en tiempo real. Estas necesidades de la vida real a menudo se presentan en ámbitos como los mercados financieros, el análisis de noticias y la recopilación de inteligencia. En la mayoría de los sistemas NED de última generación, cada noticia disponible se compara con todas las noticias previamente recibidas. Si todas las similitudes entre ellos no superan un umbral, entonces la historia desencadena un nuevo evento. Por lo general, se presentan en forma de similitud de coseno o métrica de similitud de Hellinger. El problema central de NED es identificar si dos historias tratan sobre el mismo tema. Obviamente, estos sistemas no pueden aprovechar la información del tema. Además, no es aceptable en aplicaciones reales debido a la gran cantidad de cálculos requeridos en el proceso de NED. Otros sistemas organizan historias anteriores en grupos (cada grupo corresponde a un tema), y la nueva historia se compara con los grupos anteriores en lugar de con historias. Esta forma puede reducir significativamente los tiempos de comparación. Sin embargo, se ha demostrado que este método es menos preciso [4, 5]. Esto se debe a que a veces las historias dentro de un tema se alejan mucho entre sí, lo que podría resultar en una baja similitud entre una historia y su tema. Por otro lado, algunos sistemas NED propuestos intentaron mejorar la precisión al hacer un mejor uso de las entidades nombradas [10, 11, 12, 13]. Sin embargo, ninguno de los sistemas ha considerado que los términos de diferentes tipos (por ejemplo, Los sustantivos, verbos o nombres de personas tienen diferentes efectos en diferentes clases de historias para determinar si dos historias tratan sobre el mismo tema. Por ejemplo, los nombres de los candidatos electorales (Nombre de la persona) son muy importantes para las historias de la clase de elecciones; las ubicaciones (Nombre de la ubicación) donde ocurrieron los accidentes son importantes para las historias de la clase de accidentes. Por lo tanto, en NED, todavía existen los siguientes tres problemas por investigar: (1) ¿Cómo acelerar el procedimiento de detección sin disminuir la precisión de la detección? (2) ¿Cómo aprovechar mejor la información de los grupos (temas) para mejorar la precisión? (3) ¿Cómo obtener una mejor representación de historias de noticias mediante una mejor comprensión de las entidades nombradas? Impulsados por estos problemas, hemos propuesto tres enfoques en este documento. (1) Para hacer que el procedimiento de detección sea más rápido, proponemos un nuevo procedimiento de NED basado en un árbol de indexación de noticias creado dinámicamente. El índice de historias en árbol se crea ensamblando historias similares para formar grupos de noticias en diferentes jerarquías según sus valores de similitud. Las comparaciones entre la historia actual y los grupos anteriores podrían ayudar a encontrar la historia más similar en menos tiempo de comparación. El nuevo procedimiento puede reducir la cantidad de veces de comparación sin afectar la precisión. Utilizamos los grupos del primer piso en el árbol de indexación como temas de noticias, en los cuales los pesos de los términos se ajustan dinámicamente según la distribución de términos en los grupos. En este enfoque, se utiliza adecuadamente la información de los grupos (temas), por lo que se evita el problema de la descentralización del tema. Basándonos en observaciones de las estadísticas obtenidas de los datos de entrenamiento, encontramos que los términos de diferentes tipos (por ejemplo, Los sustantivos y verbos tienen diferentes efectos para diferentes clases de historias al determinar si dos historias tratan sobre el mismo tema. Y proponemos utilizar estadísticas para optimizar los pesos de los términos de diferentes tipos en una historia de acuerdo a la clase de noticias a la que pertenece la historia. En el conjunto de datos TDT3, el nuevo modelo NED solo utiliza un 14.9% en comparación con los tiempos del modelo básico, mientras que su costo normalizado mínimo es de 0.5012, lo que es 0.0797 mejor que el modelo básico, y también mejor que cualquier otro resultado previamente reportado para este conjunto de datos [8, 13]. El resto del documento está organizado de la siguiente manera. Comenzamos este artículo resumiendo el trabajo previo en NED en la sección 2. La sección 3 presenta el modelo básico para NED que la mayoría de los sistemas actuales utilizan. La sección 4 describe nuestro nuevo procedimiento de detección basado en el índice de árbol de noticias. En la sección 5, se proponen dos métodos de reponderación de términos para mejorar la precisión de NED. La sección 6 presenta nuestros datos experimentales y métricas de evaluación. Finalmente concluimos con los resultados experimentales en la Sección 7, y las conclusiones y trabajo futuro en la Sección 8. TRABAJO RELACIONADO Papka et al. propusieron el agrupamiento de un solo paso en NED [6]. Cuando se encontraba una nueva historia, esta era procesada inmediatamente para extraer características de términos y se construía una representación de consulta del contenido de la historia. Luego se comparó con todas las consultas anteriores. Si el documento no generaba ninguna consulta al exceder un umbral, se marcaba como un nuevo evento. Lam et al construyen representaciones de consultas previas de grupos de historias, cada una de las cuales corresponde a un tema [7]. De esta manera se realizan comparaciones entre historias y grupos. En los últimos años, la mayoría de los trabajos se han centrado en proponer mejores métodos para la comparación de historias y la representación de documentos. Brants et al. [8] extendieron un modelo básico incremental de TF-IDF para incluir modelos específicos de fuente, normalización de puntajes de similitud basada en promedios específicos de documentos, normalización de puntajes de similitud basada en promedios específicos de pares de fuentes, reponderación de términos basada en frecuencias de eventos inversas y segmentación de documentos. Se mostraron buenas mejoras en los puntos de referencia de TDT. Stokes et al. [9] utilizaron una combinación de evidencia de dos representaciones distintas del contenido de un documento. Una de las representaciones era el vector de texto libre habitual, la otra hacía uso de cadenas léxicas (creadas utilizando WordNet) para construir otro vector de términos. Luego, las dos representaciones se combinan de forma lineal. Se logró un aumento marginal en la efectividad cuando se utilizó la representación combinada. Algunos esfuerzos se han realizado sobre cómo utilizar entidades nombradas para mejorar la desambiguación de entidades nombradas (NED). Yang et al. asignaron un peso cuatro veces mayor a las entidades con nombres de ubicación que a otros términos y entidades con nombres [10]. El grupo de investigación DOREMI combinó similitudes semánticas de nombres de personas, nombres de lugares y tiempo junto con similitud textual [11][12]. El grupo de investigación de UMass [13] dividió la representación del documento en dos partes: entidades nombradas y entidades no nombradas. Y se descubrió que algunas clases de noticias podían lograr un mejor rendimiento utilizando la representación de entidades nombradas, mientras que otras clases de noticias podían lograr un mejor rendimiento utilizando la representación sin entidades nombradas. Tanto [10] como [13] utilizaron la técnica de categorización de texto para clasificar previamente las noticias. En [13] las noticias se clasifican automáticamente al principio, y luego se prueban las sensibilidades de los nombres y términos que no son nombres para NED para cada clase. En [10], se eliminan los términos frecuentes de cada clase de la representación del documento. Por ejemplo, la palabra \"elección\" no ayuda a identificar diferentes elecciones. En su trabajo, no se investiga la efectividad de diferentes tipos de nombres (o términos con diferentes categorías gramaticales) para NED en diferentes clases de noticias. Utilizamos análisis estadístico para revelar el hecho y utilizarlo para mejorar el rendimiento de NED. 3. MODELO BÁSICO En esta sección, presentamos el modelo básico de Detección de Nuevos Eventos que es similar a lo que la mayoría de los sistemas actuales aplican. Luego, proponemos nuestro nuevo modelo ampliando el modelo básico. Los sistemas de detección de eventos nuevos utilizan flujos de noticias como entrada, en los cuales las historias están estrictamente ordenadas por tiempo. Solo están disponibles las historias recibidas previamente al tratar con la historia actual. La salida es una decisión sobre si la historia actual es un evento nuevo o no y la confianza de la decisión. Por lo general, un modelo NED consta de tres partes: representación de la historia, cálculo de similitud y procedimiento de detección. 3.1 Representación de la historia Se necesita preprocesamiento antes de generar la representación de la historia. Para el preprocesamiento, tokenizamos palabras, reconocemos abreviaturas, normalizamos abreviaturas, agregamos etiquetas de partes del discurso, eliminamos palabras vacías incluidas en la lista de paradas utilizada en InQuery [14], reemplazamos palabras con sus raíces utilizando el algoritmo K-stem[15], y luego generamos un vector de palabras para cada noticia. Utilizamos el modelo TF-IDF incremental para el cálculo del peso de los términos [4]. En un modelo TF-IDF, la frecuencia de término en un documento de noticias se pondera por la frecuencia inversa del documento, la cual se genera a partir del corpus de entrenamiento. Cuando surge un nuevo término en el proceso de prueba, hay dos soluciones: simplemente ignorar el nuevo término o establecer df del término como una constante pequeña (por ejemplo, df = 1). El nuevo término recibe un peso demasiado bajo en la primera solución (0) y un peso demasiado alto en la segunda solución. En el modelo TF-IDF incremental, las frecuencias de documentos se actualizan dinámicamente en cada paso de tiempo t: 1( ) ( ) ( )t t D tdf w df w df w−= + (1) donde Dt representa el conjunto de noticias recibidas en el tiempo t, y dfDt(w) significa el número de documentos en los que aparece el término w, y dft(w) significa el número total de documentos en los que aparece el término w antes del tiempo t. En este trabajo, cada ventana de tiempo incluye 50 noticias. Así, cada historia d recibida en t se representa de la siguiente manera: 1 2{ ( , , ), ( , , ),..., ( , , )}nd peso d t w peso d t w peso d t w→ donde n significa el número de términos distintos en la historia d, y ( , , )peso d t w significa el peso del término w en la historia d en el tiempo t: log( ( , ) 1) log(( 1) /( ( ) 0.5)) ( , , ) log( ( , ) 1) log(( 1) /( ( ) 0.5)) t t t t w d tf d w N df w peso d t w tf d w N df w ∈ + + + = + + +∑ (2) donde Nt significa el número total de historias de noticias antes del tiempo t, y tf(d,w) significa cuántas veces aparece el término w en la historia de noticias d. 3.2 Cálculo de Similitud Utilizamos la distancia de Hellinger para el cálculo de la similitud entre dos historias, para dos historias d y d en el tiempo t, su similitud se define de la siguiente manera: , ( , , ) ( , , ) * ( , , ) w d d sim d d t peso d t w peso d t w ∈ = ∑ (3) 3.3 Procedimiento de Detección Para cada historia d recibida en el paso de tiempo t, el valor ( ) ( ) ( ) ( ( , , )) tiempo d tiempo d n d max sim d d t < = (4) es un puntaje utilizado para determinar si d es una historia sobre un nuevo tema y al mismo tiempo es una indicación de la confianza en nuestra decisión [8]. tiempo(d) significa el tiempo de publicación de la historia d. Si el puntaje supera el umbral θ new, entonces existe un documento suficientemente similar, por lo tanto, d es una historia antigua, de lo contrario, no hay un documento previo suficientemente similar, por lo tanto, d es una historia nueva. 4. Nuevo procedimiento NED Los sistemas NED tradicionales se pueden clasificar en dos tipos principales en cuanto al procedimiento de detección: (1) tipo S-S, en el que la historia en cuestión se compara con cada historia recibida previamente, y se utiliza la mayor similitud para determinar si la historia actual trata sobre un evento nuevo; (2) tipo S-C, en el que la historia en cuestión se compara con todos los clusters anteriores, cada uno de los cuales representa un tema, y se utiliza la mayor similitud para la decisión final sobre la historia actual. Si la similitud más alta supera el umbral θ nuevo, entonces es una historia antigua y se coloca en el clúster más similar; de lo contrario, es una historia nueva y se crea un nuevo clúster. Trabajos anteriores muestran que la primera forma es más precisa que la segunda [4][5]. Dado que a veces las historias dentro de un tema se alejan mucho entre sí, una historia puede tener una similitud muy baja con su tema. Por lo tanto, utilizar similitudes entre historias para determinar una nueva historia es mejor que utilizar similitudes entre historias y grupos. Sin embargo, el primer método requiere mucho más tiempo de comparación, lo que significa que es menos eficiente. Proponemos un nuevo procedimiento de detección que utiliza comparaciones con clusters anteriores para ayudar a encontrar la historia más similar en menos tiempo de comparación, y la decisión final sobre el nuevo evento se toma de acuerdo con la historia más similar. Por lo tanto, podemos obtener tanto la precisión de los métodos de tipo S-S como la eficiencia de los métodos de tipo S-C. El nuevo procedimiento crea un árbol de indexación de noticias dinámicamente, en el cual se agrupan historias similares para formar una jerarquía de clusters. Indexamos historias similares juntas por su ancestro común (un nodo de clúster). Historias diferentes están indexadas en diferentes grupos. Cuando una historia está llegando, utilizamos comparaciones entre la historia actual y los grupos jerárquicos anteriores para ayudar a encontrar la historia más similar que es útil para la toma de decisiones sobre nuevos eventos. Después de que se tome la decisión sobre el nuevo evento, la historia actual se inserta en el árbol de indexación para la detección siguiente. El índice de noticias en forma de árbol se define formalmente de la siguiente manera: S-Tree = {r, NC, NS, E}, donde r es la raíz de S-Tree, NC es el conjunto de todos los nodos de clúster, NS es el conjunto de todos los nodos de historia, y E es el conjunto de todos los bordes en S-Tree. Definimos un conjunto de restricciones para un S-Árbol: ⅰ. , es un nodo no terminal en el árbolC i i N i∀ ∈ → ⅱ. , es un nodo terminal en el árbolS i i N i∀ ∈ → ⅲ. , el grado de salida de es al menos 2C i i N i∀ ∈ → ⅳ. , se representa como el centroide de sus descendientesC i i iN∀ ∈ → Para una noticia di, el procedimiento de comparación y el procedimiento de inserción basados en el árbol de indexación se definen de la siguiente manera. Un ejemplo se muestra en la Figura 1 y la Figura 2. Figura 1. Procedimiento de comparación Figura 2. Procedimiento de comparación de inserción: Paso 1: comparar di con todos los nodos hijos directos de r y seleccionar λ nodos con las similitudes más altas, por ejemplo, C1 2 y C1 3 en la Figura 1. Paso 2: para cada nodo seleccionado en el paso anterior, por ejemplo. C1 2, compara di con todos sus nodos hijos directos y selecciona λ nodos con las similitudes más altas, por ejemplo. C2 2 y d8. Repetir el paso 2 para todos los nodos no terminales. Paso 3: registrar el nodo terminal con la mayor similitud a di, por ejemplo, s5, y el valor de similitud (0.20). Insertando di en el S-árbol con r como raíz: Encuentra el nodo n que es hijo directo de r en el camino desde r hasta el nodo terminal con la similitud más alta s, por ejemplo. C1 2. Si s es menor que θ init + (h-1)δ, entonces agregar di al árbol como un hijo directo de r. De lo contrario, si n es un nodo terminal, crear un nodo de clúster en lugar de n, y agregar tanto n como di como sus hijos directos; si n es un nodo no terminal, repetir este procedimiento e insertar di en el subárbol con n como raíz de forma recursiva. Aquí h es la longitud entre n y la raíz del árbol S. Cuanto más similares sean las historias en un grupo, mejor representará el grupo a las historias en él. Por lo tanto, no imponemos restricciones en la altura máxima de los árboles y el grado de un nodo. Por lo tanto, no podemos dar la complejidad de este procedimiento basado en árboles de indexación. Pero proporcionaremos el número de veces de comparación necesarias por el nuevo procedimiento en nuestros experimentos en la sección 7.5. En esta sección, se proponen dos métodos de reponderación de términos para mejorar la precisión de NED. En el primer método, se explora una nueva forma de utilizar de manera más efectiva la información de los grupos (temas). El segundo encuentra una mejor manera de utilizar entidades nombradas basadas en la clasificación de noticias. 5.1 Reajuste de términos basado en la distancia de distribución TF-IDF es el modelo más prevalente utilizado en sistemas de recuperación de información. La idea básica es que cuanto menos documentos aparezca un término, más importante es el término en la discriminación de documentos (relevantes o no relevantes para una consulta que contenga el término). Sin embargo, en el dominio de TDT, necesitamos discriminar documentos en función de los temas en lugar de las consultas. Intuitivamente, el uso de vectores de clúster (tema) para comparar con historias de noticias posteriores debería tener un mejor rendimiento que el uso de vectores de historias. Desafortunadamente, los resultados experimentales no respaldan esta intuición [4][5]. Basándonos en la observación de los datos, encontramos que la razón es que un tema de noticias generalmente contiene muchos eventos directa o indirectamente relacionados, y todos ellos tienen sus propios subtemas que suelen ser diferentes entre sí. Tomemos el tema descrito en la sección 1 como ejemplo, eventos como la explosión y el rescate tienen similitudes muy bajas con eventos sobre juicios criminales, por lo tanto, las historias sobre juicios tendrían una baja similitud con el vector de tema construido en base a sus eventos previos. Esta sección se centra en cómo hacer un uso efectivo de la información del tema y al mismo tiempo evitar el problema de la descentralización del contenido. Al principio, clasificamos los términos en 5 clases para ayudar en el análisis de las necesidades del modelo modificado: Clase de términos A: términos que ocurren con frecuencia en todo el corpus, por ejemplo, año y personas. Los términos de esta clase deben recibir pesos bajos porque no ayudan mucho en la discriminación de temas. Clase B de términos: términos que ocurren con frecuencia dentro de una categoría de noticias, por ejemplo, elección, tormenta. Son útiles para distinguir dos historias en diferentes categorías de noticias. Sin embargo, no pueden proporcionar información para determinar si dos historias tratan sobre el mismo tema o temas diferentes. En otras palabras, los términos elección y tormenta no son útiles para diferenciar dos campañas electorales y dos desastres de tormenta. Por lo tanto, los términos de esta clase deberían asignarse pesos más bajos. Clase de término C: términos que ocurren con frecuencia en un tema y raramente en otros temas, por ejemplo, el nombre de un avión accidentado, el nombre de un huracán específico. Las noticias que pertenecen a diferentes temas rara vez tienen términos superpuestos en esta clase. Cuanto más frecuentemente aparezca un término en un tema, más importante es el término para una historia perteneciente al tema, por lo tanto, el término debería tener un peso más alto. Clase de término D: términos que aparecen en un tema de manera exclusiva, pero no con frecuencia. Por ejemplo, el nombre de un bombero que se desempeñó muy bien en una acción de salvamento, que puede aparecer en solo dos o tres historias pero nunca ha aparecido en otros temas. Los términos de este tipo deberían recibir más peso que en el modelo TF-IDF. Sin embargo, dado que no son populares en el tema, no es apropiado asignarles pesos demasiado altos. Clase de término E: términos con baja frecuencia en documentos y que aparecen en diferentes temas. Los términos de esta clase deberían recibir pesos más bajos. Ahora analizamos si el modelo TF-IDF puede asignar pesos adecuados a las cinco clases de términos. Obviamente, los términos de la clase A tienen un peso bajo en el modelo TF-IDF, lo cual es acorde con el requisito descrito anteriormente. En el modelo TF-IDF, los términos de la clase B dependen altamente del número de historias en una clase de noticias. El modelo TF-IDF no puede proporcionar pesos bajos si la historia que contiene el término pertenece a una clase de noticias relativamente pequeña. Para un término de clase C, cuanto más frecuentemente aparezca en un tema, menos peso le da el modelo TFIDF. Esto entra en conflicto directo con el requisito de términos en la clase C. Para los términos de la clase D, el modelo TF-IDF les asigna pesos altos de manera correcta. Pero para los términos de la clase E, el modelo TF-IDF asigna pesos altos a ellos que no son conformes con el requisito de pesos bajos. En resumen, los términos de clase B, C, E no pueden ser ponderados adecuadamente en el modelo TF-IDF. Por lo tanto, proponemos un modelo modificado para resolver este problema. Cuando θ init y θ new están configurados de cerca, asumimos que la mayoría de las historias en un clúster de primer nivel (un nodo hijo directo del nodo raíz) están en el mismo tema. Por lo tanto, utilizamos un clúster de primer nivel para capturar la distribución de términos (df para todos los términos dentro del clúster) dentro del tema de forma dinámica. La divergencia de Kullback-Leibler de la distribución de términos en un clúster de primer nivel y el conjunto completo de historias se utiliza para ajustar los pesos de los términos: ( , , ) * (1 * ( || )) ( , , ) ( , , ) * (1 * ( || )) cw tw cw tw w d D peso d t w KL P P peso d t w peso d t w KL P P γ γ ∈ + = +∑ (5) donde ( ) ( ) ( ) ( ) 1,cw cw c c c c df w df w p y p y N N = = − (6) ( ) ( ) ( ) ( ) 1,t t tw tw t t df w df w p y p y N N = = − (7) donde dfc(w) es el número de documentos que contienen el término w dentro del clúster C, y Nc es el número de documentos en el clúster C, y Nt es el número total de documentos que llegan antes del paso de tiempo t. γ es un parámetro constante, actualmente configurado manualmente en 3. La divergencia de Kullback-Leibler se define de la siguiente manera [17]: La idea básica es que, para una historia en un tema, cuanto más ocurre un término dentro del tema y menos ocurre en otros temas, se le deben asignar pesos más altos. Obviamente, el modelo modificado puede cumplir con todos los requisitos de las cinco clases de términos mencionadas anteriormente. 5.2 Reajuste de Términos Basado en el Tipo de Término y la Clase de Historia. Trabajos anteriores encontraron que algunas clases de historias de noticias podrían lograr mejoras significativas al dar un peso adicional a las entidades nombradas. Pero encontramos que los términos de diferentes tipos deben recibir una cantidad diferente de peso adicional para diferentes clases de noticias. Utilizamos open-NLP1 para reconocer tipos de entidades nombradas y etiquetas de partes del discurso para términos que aparecen en noticias. Los tipos de entidades nombradas incluyen nombre de persona, nombre de organización, nombre de ubicación, fecha, hora, dinero y porcentaje, y se seleccionan cinco partes del discurso: ninguno (NN), verbo (VB), adjetivo (JJ), adverbio (RB) y número cardinal (CD). El análisis estadístico muestra tipos de términos discriminativos a nivel de tema para diferentes clases de historias. Por conveniencia, los tipos de entidad nombrada y etiquetas de parte de la oración se llaman uniformemente tipo de término en las secciones siguientes. Determinar si dos historias tratan sobre el mismo tema es un componente básico para la tarea de NED. Por lo tanto, al principio usamos la estadística χ² para calcular las correlaciones entre términos y temas. Para un término t y un tema T, se deriva una tabla de contingencia: Tabla 1. Una tabla de contingencia de 2×2 con el número de documentos pertenecientes al tema T y no pertenecientes al tema T incluye t A B no incluye t C D. La estadística χ² para un término específico t con respecto al tema T se define como [16]: χ² = ( ) ( ) * ( ) ( ) * ( ) * ( ) * ( ) w T A B C D AD CB A C B D A B C D + + + − + + + + (9). Los temas de noticias para la tarea TDT se clasifican además en 11 reglas de interpretaciones (ROIs) 2. El ROI se puede ver como una clase de historias de nivel superior. El promedio de correlación entre un tipo de término y un ROI de tema se calcula como: 2 avg 2 ( , )( ( , ) )k m m km kT R w P w TP R p w T R P χ χ ∈ ∈ ∑ ∑（ , ）= 1 1 k=1…K, m=1…M (10) donde K es el número de tipos de términos (fijado constantemente en 12 en el artículo). M es el número de nuevas clases (ROIs, establecido en el conjunto 11 del artículo). Pk representa el conjunto de todos los términos de tipo k, y Rm representa el conjunto de todos los temas de la clase m, p(t,T) significa la probabilidad de que t ocurra en el tema T. Debido a limitaciones de espacio, solo se enumeran partes de los tipos de términos (9 tipos de términos) y partes de las clases de noticias (8 clases) en la tabla 2 con los valores promedio de correlación entre ellos. Las estadísticas se derivan de datos etiquetados en el corpus TDT2. (Los resultados en la tabla 2 ya están normalizados para mayor comodidad en la comparación). Las estadísticas en la tabla 2 indican la utilidad de diferentes tipos de términos en la discriminación de temas con respecto a diferentes clases de noticias. Podemos ver que el nombre de la ubicación es el tipo de término más útil para tres clases de noticias: Desastres Naturales, Violencia o Guerra, Finanzas. Y para otras tres categorías: Elecciones, Casos Legales/Criminales, Ciencia y Descubrimiento, el nombre de la persona es el tipo de término más discriminativo. Para Escándalos/Audiencias, la fecha es la información más importante para la discriminación de temas. Además, los casos legales/criminales y los temas financieros tienen una correlación más alta con términos relacionados con el dinero, mientras que la ciencia y el descubrimiento tienen una correlación más alta con términos de porcentaje. Los términos no nominales son más estables para diferentes clases. 1. http://opennlp.sourceforge.net/ 2. http://projects.ldc.upenn.edu/TDT3/Guide/label.html A partir del análisis de la tabla 2, es razonable ajustar el peso de los términos según su tipo de término y la clase de noticias a la que pertenece la historia. Los nuevos pesos de los términos se recalculan de la siguiente manera: ( ) ( ) ( ) ( ) ( , , ) * ( , , ) ( , , ) * clase d D tipo w T clase d D tipo w w d peso d t w peso d t w peso d t w α α ∈ = ∑ (11) donde type(w) representa el tipo del término w, y class(d) representa la clase de la historia d, c kα es el parámetro de recalibración para la clase de noticias c y el tipo de término k. En el trabajo, simplemente utilizamos las estadísticas de la tabla 2 como parámetros de recalibración. Aunque usar las estadísticas directamente puede que no sea la mejor opción, no discutimos cómo obtener automáticamente los mejores parámetros. Intentaremos utilizar técnicas de aprendizaje automático para obtener los mejores parámetros en el trabajo futuro. En el trabajo, utilizamos BoosTexter [20] para clasificar todas las historias en uno de los 11 ROIs. BoosTexter es un programa de aprendizaje automático basado en boosting, que crea una serie de reglas simples para construir un clasificador de datos de texto o atributos-valor. Utilizamos el peso del término generado utilizando el modelo TF-IDF como característica para la clasificación de historias. Entrenamos el modelo con las 12000 historias en inglés evaluadas en TDT2, y clasificamos el resto de las historias en TDT2 y todas las historias en TDT3. Los resultados de clasificación se utilizan para la reponderación de términos en la fórmula (11). Dado que las etiquetas de clase de las historias de tema apagado no se proporcionan en los conjuntos de datos de TDT, no podemos proporcionar la precisión de clasificación aquí. Por lo tanto, no discutimos los efectos de la precisión de la clasificación en el rendimiento de NED en el artículo. 6. CONFIGURACIÓN EXPERIMENTAL 6.1 Conjuntos de datos Utilizamos dos conjuntos de datos de LDC [18], TDT2 y TDT3, para nuestros experimentos. TDT2 contiene noticias desde enero hasta junio de 1998. Contiene alrededor de 54,000 historias de fuentes como ABC, Associated Press, CNN, New York Times, Public Radio International, Voice of America, etc. Solo se consideraron historias en inglés en la colección. TDT3 contiene aproximadamente 31,000 historias en inglés recopiladas de octubre a diciembre de 1998. Además de las fuentes utilizadas en TDT2, también contiene historias de las transmisiones de televisión de NBC y MSNBC. Utilizamos versiones transcritas de las emisiones de televisión y radio además de noticias textuales. El conjunto de datos TDT2 está etiquetado con alrededor de 100 temas, y aproximadamente 12,000 historias en inglés pertenecen al menos a uno de estos temas. El conjunto de datos TDT3 está etiquetado con alrededor de 120 temas, y aproximadamente 8000 historias en inglés pertenecen al menos a uno de estos temas. Todos los temas están clasificados en 11 Reglas de Interpretación: (1) Elecciones, (2) Escándalos/Audiencias, (3) Casos Legales/Criminales, (4) Desastres Naturales, (5) Accidentes, (6) Violencia o Guerra en Curso, (7) Noticias de Ciencia y Descubrimientos, (8) Finanzas, (9) Nueva Ley, (10) Noticias Deportivas, (11) Varios. Noticias. 6.2 Métrica de evaluación TDT utiliza una función de costo CDet que combina las probabilidades de perder una nueva historia y una falsa alarma [19]: * * * *Det Miss Miss Target FA FA NontargetC C P P C P P= + (12) Tabla 2. Correlación promedio entre tipos de términos y clases de noticias donde CMiss significa el costo de perder una nueva historia, PMiss significa la probabilidad de perder una nueva historia, y PTarget significa la probabilidad de ver una nueva historia en los datos; CFA significa el costo de una falsa alarma, PFA significa la probabilidad de una falsa alarma, y PNontarget significa la probabilidad de ver una historia antigua. El costo CDet se normaliza de tal manera que un sistema perfecto obtiene una puntuación de 0 y un sistema trivial, que es el mejor de marcar todas las historias como nuevas o antiguas, obtiene una puntuación de 1: ( ( * , * ) ) Det Det Miss Target FA Nontarget C Norm C min C P C P = (13) El sistema de detección de eventos nuevos proporciona dos salidas para cada historia. La primera parte es sí o no, indicando si la historia desencadena un nuevo evento o no. La segunda parte es una puntuación que indica la confianza de la primera decisión. Los puntajes de confianza se pueden utilizar para trazar la curva DET, es decir, curvas que representan la probabilidad de falsa alarma frente a la probabilidad de omisión. El costo normalizado mínimo se puede determinar si se elige el umbral óptimo en la puntuación. RESULTADOS EXPERIMENTALES 7.1 Resultados principales Para probar los enfoques propuestos en el modelo, implementamos y probamos cinco sistemas: Sistema-1: este sistema se utiliza como referencia. Se implementa basándose en el modelo básico descrito en la sección 3, es decir, utilizando el modelo TF-IDF incremental para generar pesos de términos y utilizando la distancia de Hellinger para calcular la similitud entre documentos. La normalización del puntaje de similitud también se emplea [8]. Se utiliza el procedimiento de detección S-S. Sistema-2: este sistema es igual que el sistema-1 excepto que se utiliza el procedimiento de detección S-C. Sistema-3: este sistema es igual que el sistema-1 excepto que utiliza el nuevo procedimiento de detección que se basa en un árbol de indexación. Sistema-4: implementado basado en el enfoque presentado en la sección 5.1, es decir, los términos se vuelven a ponderar según la distancia entre las distribuciones de términos en un grupo y todas las historias. Se utiliza el nuevo procedimiento de detección. Sistema-5: implementado basado en el enfoque presentado en la sección 5.2, es decir, los términos de diferentes tipos son reponderados según la clase de noticias utilizando parámetros entrenados. El nuevo procedimiento de detección se utiliza. Los siguientes son algunos otros sistemas NED: Sistema-6: [21] para cada par de historias, calcula tres valores de similitud para entidad nombrada, entidad no nombrada y todos los términos respectivamente. Y emplear Máquina de Vectores de Soporte para predecir nuevo o antiguo utilizando los valores de similitud como características. Sistema-7: [8] extendió un modelo básico incremental de TF-IDF para incluir modelos específicos de origen, normalización de puntuaciones de similitud basada en promedios específicos de documentos, normalización de puntuaciones de similitud basada en promedios específicos de pares de fuentes, etc. Sistema-8: [13] dividió la representación del documento en dos partes: entidades nombradas y entidades no nombradas, y eligió una parte efectiva para cada clase de noticias. La tabla 3 y la tabla 4 muestran los costos normalizados ponderados por tema y los tiempos de comparación en los conjuntos de datos TDT2 y TDT3 respectivamente. Dado que no se contaba con un conjunto de datos de validación para ajustar el umbral θ nuevo en los experimentos en TDT2, solo informamos los costos normalizados mínimos para nuestros sistemas en la tabla 3. El sistema-5 supera a todos los demás sistemas, incluido el sistema-6, y realiza solo 2.78e+8 veces la comparación de tiempos en el procedimiento de detección, lo que representa solo el 13.4% del sistema-1. Tabla 3. Los resultados de NED en los sistemas TDT2 Min Norm(CDet) tiempos de comparación son: Sistema-1 0.5749 2.08e+9, Sistema-2 0.6673 3.77e+8, Sistema-3 0.5765 2.81e+8, Sistema-4 0.5431 2.99e+8, Sistema-5 0.5089 2.78e+8, Sistema-6 0.5300. Al evaluar en los costos normalizados en TDT3, utilizamos los umbrales óptimos obtenidos del conjunto de datos de TDT2 para todos los sistemas. El sistema-2 reduce los tiempos de comparación a 1.29e+9, que es solo el 18.3% del sistema-1, pero al mismo tiempo también obtiene un costo normalizado mínimo deteriorado que es 0.0499 más alto que el sistema-1. El Sistema-3 utiliza el nuevo procedimiento de detección basado en el índice de noticias en árbol. Requiere incluso menos tiempo de comparación que el sistema-2. Esto se debe a que las comparaciones de historias suelen arrojar mayores similitudes que las de grupos de historias, por lo que las historias tienden a combinarse en el sistema-3. Y el sistema-3 es básicamente equivalente al sistema-1 en los resultados de precisión. El System-4 ajusta los pesos de los términos basándose en la distancia de las distribuciones de términos entre todo el corpus y el conjunto de historias del clúster, lo que resulta en una mejora significativa de 0.0468 en comparación con el system-1. El mejor sistema (sistema-5) tiene un costo normalizado mínimo de 0.5012, lo cual es 0.0797 mejor que el sistema-1, y también mejor que cualquier otro resultado previamente reportado para este conjunto de datos [8, 13]. Además, el sistema-5 solo necesita 1.05e+8 veces de comparación, lo que equivale al 14.9% del sistema-1. Tabla 4. Los resultados de NED en los sistemas TDT3 Norm(CDet) Norma mínima(CDet) Tiempos de comparación Sistema-1 0.6159 0.5809 7.04e+8 Sistema-2① 0.6493 0.6308 1.29e+8 Sistema-3② 0.6197 0.5868 1.03e+8 Sistema-4② 0.5601 0.5341 1.03e+8 Sistema-5② 0.5413 0.5012 1.05e+8 Sistema-7 -- 0.5783 -Sistema-8 -- 0.5229 -① θ nuevo=0.13 ② θ inicial=0.13, λ =3, δ =0.15 La Figura 5 muestra las cinco curvas DET para nuestros sistemas en el conjunto de datos TDT3. El Sistema-5 logra el costo mínimo con una tasa de falsas alarmas de 0.0157 y una tasa de omisión de 0.4310. Podemos observar que System4 y System-5 obtienen una menor probabilidad de error de omisión en regiones de baja probabilidad de falsa alarma. La hipótesis es que se transfiere más valor de peso a los términos clave de los temas que a los términos no clave. La puntuación de similitud entre dos historias pertenecientes a diferentes temas es más baja que antes, porque los términos superpuestos suelen no ser términos clave de sus temas. 7.2 Selección de parámetros para la detección de árboles de indexación. La Figura 3 muestra los costos normalizados mínimos obtenidos por el sistema-3 en TDT3 utilizando diferentes parámetros. El parámetro de inicio θ se prueba en seis valores que van desde 0.03 hasta 0.18. Y el parámetro λ se prueba en cuatro valores: 1, 2, 3 y 4. Podemos ver que, cuando θ inicial se establece en 0.12, que es el más cercano a θ nuevo, los costos son más bajos que los demás. Esto es fácil de explicar, porque cuando se agrupan historias pertenecientes al mismo tema en un conjunto, es más razonable que el conjunto represente las historias en él. Cuando el parámetro λ se establece en 3 o 4, los costos son mejores que en otros casos, pero no hay mucha diferencia entre 3 y 4. 0 0.05 0.1 0.15 0.2 1 2 3 4 0.5 0.6 0.7 0.8 0.9 1 θ-initλ MinCost 0.6 0.65 0.7 0.75 0.8 0.85 0.9 Figura 3. Costo mínimo en TDT3 (δ =0.15) 0 0.05 0.1 0.15 0.2 1 2 3 4 0 0.5 1 1.5 2 2.5 x 10 8 θ-inicio λ Comparando tiempos 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 8 Figura 4. Comparando los tiempos en TDT3 (δ =0.15), la Figura 4 muestra los tiempos de comparación utilizados por el sistema-3 en TDT3 con los mismos parámetros que la Figura 3. Los tiempos de comparación dependen fuertemente de θ init. Debido a que cuanto mayor sea θ init, menos historias se combinan juntas, más veces de comparación se necesitan para la decisión de un nuevo evento. Por lo tanto, usamos θ init = 0.13, λ = 3, δ = 0.15 para los sistemas 3, 4 y 5. En esta configuración de parámetros, podemos obtener tanto costos normalizados mínimos bajos como menos tiempos de comparación. CONCLUSIÓN Hemos propuesto un procedimiento de detección basado en un árbol de indexación de noticias en nuestro modelo. Reduce los tiempos de comparación a aproximadamente una séptima parte del método tradicional sin afectar la precisión de NED. También hemos presentado dos extensiones al modelo básico TF-IDF. La primera extensión se realiza ajustando los pesos de los términos basados en las distribuciones de términos entre todo el corpus y un conjunto de historias de un clúster. Y la segunda extensión al modelo básico TF-IDF es un mejor uso de los tipos de términos (tipos de entidades nombradas y partes de la velocidad) de acuerdo con las categorías de noticias. Nuestros resultados experimentales en los conjuntos de datos TDT2 y TDT3 muestran que ambas extensiones contribuyen significativamente a la mejora en la precisión. No consideramos la información de tiempo de noticias como una pista para la tarea de NED, ya que la mayoría de los temas duran mucho tiempo y los conjuntos de datos de TDT solo abarcan un período relativamente corto (no más de 6 meses). Para el trabajo futuro, queremos recopilar un conjunto de noticias que abarque un período más largo de internet e integrar información temporal en la tarea de NED. Dado que el tema es un grupo de noticias relativamente grueso, también queremos refinar la granularidad del grupo a nivel de evento e identificar diferentes eventos y sus relaciones dentro de un tema. Agradecimientos: Este trabajo ha sido apoyado por la Fundación Nacional de Ciencias Naturales de China bajo la subvención No. 90604025. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son del autor(es) y no necesariamente reflejan las del patrocinador. REFERENCIAS [1] http://www.nist.gov/speech/tests/tdt/index.htm [2] En Detección y Seguimiento de Temas. Organización de la información basada en eventos. Kluwer Academic Publishers, 2002. .01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90 1 2 5 10 20 40 60 80 90 Probabilidad de Falsa Alarma (en %) Probabilidad de Omisión (en %) Curva Ponderada por Tema del SISTEMA1 Norma Mínima (Costo) del SISTEMA1 Curva Ponderada por Tema del SISTEMA2 Norma Mínima (Costo) del SISTEMA2 Curva Ponderada por Tema del SISTEMA3 Norma Mínima (Costo) del SISTEMA3 Curva Ponderada por Tema del SISTEMA4 Norma Mínima (Costo) del SISTEMA4 Curva Ponderada por Tema del SISTEMA5 Norma Mínima (Costo) del SISTEMA5 Rendimiento Aleatorio Figura 5. Curvas DET en TDT3 [3] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B.T. Archibald y X. Liu. Enfoques de aprendizaje para detectar y rastrear eventos de noticias. En el número especial de IEEE Intelligent Systems sobre Aplicaciones de Recuperación de Información Inteligente, volumen 14 (4), 1999, páginas 32-43. [4] Y. Yang, T. Pierce y J. Carbonell. Un estudio sobre la detección de eventos retrospectivos y en línea. En Actas de SIGIR-98, Melbourne, Australia, 1998, 28-36. [5] J. Allan, V. Lavrenko, D. Malin y R. Swan. Detecciones, límites y líneas de tiempo: Umass y tdt-3. En Actas del Taller de Detección y Seguimiento de Temas (TDT-3), Viena, VA, 2000, 167-174. [6] R. Papka y J. Allan. Detección de nuevos eventos en línea utilizando agrupamiento de un solo paso TÍTULO2:. Informe técnico UM-CS1998-021, 1998. [7] W. Lam, H. Meng, K. Wong y J. I'm sorry, but \"Yen\" is not a sentence. Can you please provide a sentence for me to translate into Spanish? Utilizando el Análisis Contextual para la Detección de Eventos de Noticias. Revista Internacional de Sistemas Inteligentes, 2001, 525-546. [8] B. Thorsten, C. Francine y F. Ayman. Un sistema para la detección de nuevos eventos. En Actas de la 26ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2003, 330-337. [9] S. Nicola y C. Joe. Combinando clasificadores semánticos y sintácticos de documentos para mejorar la detección de primeras noticias. En Actas de la 24ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell, and C. Jin. \n\nACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell y C. Jin. Detección de novedades condicionada al tema. En Actas de la 8ª Conferencia Internacional de ACM SIGKDD, ACM Press. 2002, 688-693. [11] M. Juha, A.M. Helena y S. Marko. Aplicando Clases Semánticas en la Detección y Seguimiento de Eventos. En Actas de la Conferencia Internacional sobre Procesamiento del Lenguaje Natural (ICON 2002), 2002, páginas 175-183. [12] M. Juha, A.M. Helena y S. Marko. Semántica simple en la detección y seguimiento de temas. Recuperación de información, 7(3-4): 2004, 347-368. [13] K. Giridhar y J. Allan. Clasificación de texto y entidades nombradas para la detección de nuevos eventos. En Actas de la 27ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2004, 297-304. [14] J. P. Callan, W. B. Croft, y S. M. Harding. El Sistema de Recuperación INQUERY. En Actas de DEXA-92, 3ra Conferencia Internacional sobre Aplicaciones de Bases de Datos y Sistemas Expertos, 1992, 78-83. [15] R. Krovetz. Viendo la morfología como un proceso de inferencia. En Actas de ACM SIGIR93, 1993, 61-81. [16] Y. Yang y J. Pedersen. Un estudio comparativo sobre la selección de características en la categorización de textos. En J. D. H. Fisher, editor, La Decimocuarta Conferencia Internacional sobre Aprendizaje Automático (ICML97), Morgan Kaufmann, 1997, 412-420. [17] T. M. Cover y J. A. I'm sorry, but \"Thomas\" is not a sentence. Can you provide more context or a complete sentence for me to translate into Spanish? Elementos de la teoría de la información. Wiley. 1991. [18] El consorcio de datos lingüísticos, http://www.ldc,upenn.edu/. [19] La definición de la tarea TDT 2001 y el plan de evaluación, http://www.nist.gov/speech/tests/tdt/tdt2001/evalplan.htm. [20] R. E. Schapire y Y. Cantante. Boostexter: Un sistema basado en Boosting para la categorización de texto. En Machine Learning 39(2/3):1, Kluwer Academic Publishers, 2000, 35-168. [21] K. Giridhar y J. Allan. 2005. Utilizando nombres y temas para la detección de nuevos eventos. En Actas de la Conferencia de Tecnología Humana y la Conferencia sobre Métodos Empíricos en Lenguaje Natural, Vancouver, 2005, 121-128",
    "original_sentences": [
        "New Event Detection Based on Indexing-tree and Named Entity Zhang Kuo Tsinghua University Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Tsinghua University Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Tsinghua University Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn ABSTRACT New Event Detection (NED) aims at detecting from one or multiple streams of news stories that which one is reported on a new event (i.e.",
        "not reported previously).",
        "With the overwhelming volume of news available today, there is an increasing need for a NED system which is able to detect new events more efficiently and accurately.",
        "In this paper we propose a new NED model to speed up the NED task by using news indexing-tree dynamically.",
        "Moreover, based on the observation that terms of different types have different effects for NED task, two term reweighting approaches are proposed to improve NED accuracy.",
        "In the first approach, we propose to adjust term weights dynamically based on previous story clusters and in the second approach, we propose to employ statistics on training data to learn the named entity reweighting model for each class of stories.",
        "Experimental results on two Linguistic Data Consortium (LDC) datasets TDT2 and TDT3 show that the proposed model can improve both efficiency and accuracy of NED task significantly, compared to the baseline system and other existing systems.",
        "Categories and Subject Descriptors H.3.3 [Information Systems]: Information Search and Retrieval; H.4.2 [Information Systems Applications]: Types of Systemsdecision support.",
        "General Terms Algorithms, Performance, Experimentation 1.",
        "INTRODUCTION Topic Detection and Tracking (TDT) program aims to develop techniques which can effectively organize, search and structure news text materials from a variety of newswire and broadcast media [1].",
        "New Event Detection (NED) is one of the five tasks in TDT.",
        "It is the task of online identification of the earliest report for each topic as soon as that report arrives in the sequence of documents.",
        "A Topic is defined as a seminal event or activity, along with directly related events and activities [2].",
        "An Event is defined as something (non-trivial) happening in a certain place at a certain time [3].",
        "For instance, when a bomb explodes in a building, the exploding is the seminal event that triggers the topic, and other stories on the same topic would be those discussing salvaging efforts, the search for perpetrators, arrests and trial and so on.",
        "Useful news information is usually buried in a mass of data generated everyday.",
        "Therefore, NED systems are very useful for people who need to detect novel information from real-time news stream.",
        "These real-life needs often occur in domains like financial markets, news analysis, and intelligence gathering.",
        "In most of state-of-the-art (currently) NED systems, each news story on hand is compared to all the previous received stories.",
        "If all the similarities between them do not exceed a threshold, then the story triggers a new event.",
        "They are usually in the form of cosine similarity or Hellinger similarity metric.",
        "The core problem of NED is to identify whether two stories are on the same topic.",
        "Obviously, these systems cannot take advantage of topic information.",
        "Further more, it is not acceptable in real applications because of the large amount of computation required in the NED process.",
        "Other systems organize previous stories into clusters (each cluster corresponds to a topic), and new story is compared to the previous clusters instead of stories.",
        "This manner can reduce comparing times significantly.",
        "Nevertheless, it has been proved that this manner is less accurate [4, 5].",
        "This is because sometimes stories within a topic drift far away from each other, which could lead low similarity between a story and its topic.",
        "On the other hand, some proposed NED systems tried to improve accuracy by making better use of named entities [10, 11, 12, 13].",
        "However, none of the systems have considered that terms of different types (e.g.",
        "Noun, Verb or Person name) have different effects for different classes of stories in determining whether two stories are on the same topic.",
        "For example, the names of election candidates (Person name) are very important for stories of election class; the locations (Location name) where accidents happened are important for stories of accidents class.",
        "So, in NED, there still exist following three problems to be investigated: (1) How to speed up the detection procedure while do not decrease the detection accuracy? (2) How to make good use of cluster (topic) information to improve accuracy? (3) How to obtain better news story representation by better understanding of named entities.",
        "Driven by these problems, we have proposed three approaches in this paper. (1)To make the detection procedure faster, we propose a new NED procedure based on news indexing-tree created dynamically.",
        "Story indexing-tree is created by assembling similar stories together to form news clusters in different hierarchies according to their values of similarity.",
        "Comparisons between current story and previous clusters could help find the most similar story in less comparing times.",
        "The new procedure can reduce the amount of comparing times without hurting accuracy. (2)We use the clusters of the first floor in the indexing-tree as news topics, in which term weights are adjusted dynamically according to term distribution in the clusters.",
        "In this approach, cluster (topic) information is used properly, so the problem of theme decentralization is avoided. (3)Based on observations on the statistics obtained from training data, we found that terms of different types (e.g.",
        "Noun and Verb) have different effects for different classes of stories in determining whether two stories are on the same topic.",
        "And we propose to use statistics to optimize the weights of the terms of different types in a story according to the news class that the story belongs to.",
        "On TDT3 dataset, the new NED model just uses 14.9% comparing times of the basic model, while its minimum normalized cost is 0.5012, which is 0.0797 better than the basic model, and also better than any other results previously reported for this dataset [8, 13].",
        "The rest of the paper is organized as follows.",
        "We start off this paper by summarizing the previous work in NED in section 2.",
        "Section 3 presents the basic model for NED that most current systems use.",
        "Section 4 describes our new detection procedure based on news indexing-tree.",
        "In section 5, two term reweighting methods are proposed to improve NED accuracy.",
        "Section 6 gives our experimental data and evaluation metrics.",
        "We finally wrap up with the experimental results in Section 7, and the conclusions and future work in Section 8. 2.",
        "RELATED WORK Papka et al. proposed Single-Pass clustering on NED [6].",
        "When a new story was encountered, it was processed immediately to extract term features and a query representation of the storys content is built up.",
        "Then it was compared with all the previous queries.",
        "If the document did not trigger any queries by exceeding a threshold, it was marked as a new event.",
        "Lam et al build up previous query representations of story clusters, each of which corresponds to a topic [7].",
        "In this manner comparisons happen between stories and clusters.",
        "Recent years, most work focus on proposing better methods on comparison of stories and document representation.",
        "Brants et al. [8] extended a basic incremental TF-IDF model to include sourcespecific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, term reweighting based on inverse event frequencies, and segmentation of documents.",
        "Good improvements on TDT bench-marks were shown.",
        "Stokes et al. [9] utilized a combination of evidence from two distinct representations of a documents content.",
        "One of the representations was the usual free text vector, the other made use of lexical chains (created using WordNet) to build another term vector.",
        "Then the two representations are combined in a linear fashion.",
        "A marginal increase in effectiveness was achieved when the combined representation was used.",
        "Some efforts have been done on how to utilize named entities to improve NED.",
        "Yang et al. gave location named entities four times weight than other terms and named entities [10].",
        "DOREMI research group combined semantic similarities of person names, location names and time together with textual similarity [11][12].",
        "UMass [13] research group split document representation into two parts: named entities and non-named entities.",
        "And it was found that some classes of news could achieve better performance using named entity representation, while some other classes of news could achieve better performance using non-named entity representation.",
        "Both [10] and [13] used text categorization technique to classify news stories in advance.",
        "In [13] news stories are classified automatically at first, and then test sensitivities of names and non-name terms for NED for each class.",
        "In [10] frequent terms for each class are removed from document representation.",
        "For example, word election does not help identify different elections.",
        "In their work, effectiveness of different kinds of names (or terms with different POS) for NED in different news classes are not investigated.",
        "We use statistical analysis to reveal the fact and use it to improve NED performance. 3.",
        "BASIC MODEL In this section, we present the basic New Event Detection model which is similar to what most current systems apply.",
        "Then, we propose our new model by extending the basic model.",
        "New Event Detection systems use news story stream as input, in which stories are strictly time-ordered.",
        "Only previously received stories are available when dealing with current story.",
        "The output is a decision for whether the current story is on a new event or not and the confidence of the decision.",
        "Usually, a NED model consists of three parts: story representation, similarity calculation and detection procedure. 3.1 Story Representation Preprocessing is needed before generating story representation.",
        "For preprocessing, we tokenize words, recognize abbreviations, normalize abbreviations, add part-of-speech tags, remove stopwords included in the stop list used in InQuery [14], replace words with their stems using K-stem algorithm[15], and then generate word vector for each news story.",
        "We use incremental TF-IDF model for term weight calculation [4].",
        "In a TF-IDF model, term frequency in a news document is weighted by the inverse document frequency, which is generated from training corpus.",
        "When a new term occurs in testing process, there are two solutions: simply ignore the new term or set df of the term as a small const (e.g. df = 1).",
        "The new term receives too low weight in the first solution (0) and too high weight in the second solution.",
        "In incremental TF-IDF model, document frequencies are updated dynamically in each time step t: 1( ) ( ) ( )t t D tdf w df w df w−= + (1) where Dt represents news story set received in time t, and dfDt(w) means the number of documents that term w occurs in, and dft(w) means the total number of documents that term w occurs in before time t. In this work, each time window includes 50 news stories.",
        "Thus, each story d received in t is represented as follows: 1 2{ ( , , ), ( , , ),..., ( , , )}nd weight d t w weight d t w weight d t w→ where n means the number of distinct terms in story d, and ( , , )weight d t w means the weight of term w in story d at time t: log( ( , ) 1) log(( 1) /( ( ) 0.5)) ( , , ) log( ( , ) 1) log(( 1) /( ( ) 0.5)) t t t t w d tf d w N df w weight d t w tf d w N df w ∈ + + + = + + +∑ (2) where Nt means the total number of news stories before time t, and tf(d,w) means how many times term w occurs in news story d. 3.2 Similarity Calculation We use Hellinger distance for the calculation of similarity between two stories, for two stories d and d at time t, their similarity is defined as follows: , ( , , ) ( , , ) * ( , , ) w d d sim d d t weight d t w weight d t w ∈ = ∑ (3) 3.3 Detection Procedure For each story d received in time step t, the value ( ) ( ) ( ) ( ( , , )) time d time d n d max sim d d t < = (4) is a score used to determine whether d is a story about a new topic and at the same time is an indication of the confidence in our decision [8]. time(d) means the publication time of story d. If the score exceeds the thresholdθ new, then there exists a sufficiently similar document, thus d is a old story, otherwise, there is no sufficiently similar previous document, thus d is an new story. 4.",
        "New NED Procedure Traditional NED systems can be classified into two main types on the aspect of detection procedure: (1) S-S type, in which the story on hand is compared to each story received previously, and use the highest similarity to determine whether current story is about a new event; (2) S-C type, in which the story on hand is compared to all previous clusters each of which representing a topic, and the highest similarity is used for final decision for current story.",
        "If the highest similarity exceeds thresholdθ new, then it is an old story, and put it into the most similar cluster; otherwise it is a new story and create a new cluster.",
        "Previous work show that the first manner is more accurate than the second one [4][5].",
        "Since sometimes stories within a topic drift far away from each other, a story may have very low similarity with its topic.",
        "So using similarities between stories for determining new story is better than using similarities between story and clusters.",
        "Nevertheless, the first manner needs much more comparing times which means the first manner is low efficient.",
        "We propose a new detection procedure which uses comparisons with previous clusters to help find the most similar story in less comparing times, and the final new event decision is made according to the most similar story.",
        "Therefore, we can get both the accuracy of S-S type methods and the efficiency of S-C type methods.",
        "The new procedure creates a news indexing-tree dynamically, in which similar stories are put together to form a hierarchy of clusters.",
        "We index similar stories together by their common ancestor (a cluster node).",
        "Dissimilar stories are indexed in different clusters.",
        "When a story is coming, we use comparisons between the current story and previous hierarchical clusters to help find the most similar story which is useful for new event decision.",
        "After the new event decision is made, the current story is inserted to the indexing-tree for the following detection.",
        "The news indexing-tree is defined formally as follows: S-Tree = {r, NC , NS , E} where r is the root of S-Tree, NC is the set of all cluster nodes, NS is the set of all story nodes, and E is the set of all edges in S-Tree.",
        "We define a set of constraints for a S-Tree: ⅰ . , is an non-terminal node in the treeC i i N i∀ ∈ → ⅱ . , is a terminal node in the treeS i i N i∀ ∈ → ⅲ . , out degree of is at least 2C i i N i∀ ∈ → ⅳ . , is represented as the centroid of its desendantsC i i iN∀ ∈ → For a news story di, the comparison procedure and inserting procedure based on indexing-tree are defined as follows.",
        "An example is shown by Figure 1 and Figure 2.",
        "Figure 1.",
        "Comparison procedure Figure 2.",
        "Inserting procedure Comparison procedure: Step 1: compare di to all the direct child nodes of r and select λ nodes with highest similarities, e.g., C1 2 and C1 3 in Figure 1.",
        "Step 2: for each selected node in the last step, e.g.",
        "C1 2, compare di to all its direct child nodes, and select λ nodes with highest similarities, e.g.",
        "C2 2 and d8.",
        "Repeat step 2 for all non-terminal nodes.",
        "Step 3: record the terminal node with the highest similarty to di, e.g. s5, and the similarity value (0.20).",
        "Inserting di to the S-tree with r as root: Find the node n which is direct child of r in the path from r to the terminal node with highest similarity s, e.g.",
        "C1 2.",
        "If s is smaller than θ init+(h-1)δ , then add di to the tree as a direct child of r. Otherwise, if n is a terminal node, then create a cluster node instead of n, and add both n and di as its direct children; if n is an non-terminal node, then repeat this procedure and insert di to the sub-tree with n as root recursively.",
        "Here h is the length between n and the root of S-tree.",
        "The more the stories in a cluster similar to each other, the better the cluster represents the stories in it.",
        "Hence we add no constraints on the maximum of trees height and degree of a node.",
        "Therefore, we cannot give the complexity of this indexing-tree based procedure.",
        "But we will give the number of comparing times needed by the new procedure in our experiments in section7. 5.",
        "Term Reweighting Methods In this section, two term reweighting methods are proposed to improve NED accuracy.",
        "In the first method, a new way is explored for better using of cluster (topic) information.",
        "The second one finds a better way to make use of named entities based on news classification. 5.1 Term Reweighting Based on Distribution Distance TF-IDF is the most prevalent model used in information retrieval systems.",
        "The basic idea is that the fewer documents a term appears in, the more important the term is in discrimination of documents (relevant or not relevant to a query containing the term).",
        "Nevertheless, in TDT domain, we need to discriminate documents with regard to topics rather than queries.",
        "Intuitively, using cluster (topic) vectors to compare with subsequent news stories should outperform using story vectors.",
        "Unfortunately, the experimental results do not support this intuition [4][5].",
        "Based on observation on data, we find the reason is that a news topic usually contains many directly or indirectly related events, while they all have their own sub-subjects which are usually different with each other.",
        "Take the topic described in section 1 as an example, events like the explosion and salvage have very low similarities with events about criminal trial, therefore stories about trial would have low similarity with the topic vector built on its previous events.",
        "This section focuses on how to effectively make use of topic information and at the same time avoid the problem of content decentralization.",
        "At first, we classify terms into 5 classes to help analysis the needs of the modified model: Term class A: terms that occur frequently in the whole corpus, e.g., year and people.",
        "Terms of this class should be given low weights because they do not help much for topic discrimination.",
        "Term class B: terms that occur frequently within a news category, e.g., election, storm.",
        "They are useful to distinguish two stories in different news categories.",
        "However, they cannot provide information to determine whether two stories are on the same or different topics.",
        "In another words, term election and term storm are not helpful in differentiate two election campaigns and two storm disasters.",
        "Therefore, terms of this class should be assigned lower weights.",
        "Term class C: terms that occur frequently in a topic, and infrequently in other topics, e.g., the name of a crash plane, the name of a specific hurricane.",
        "News stories that belong to different topics rarely have overlap terms in this class.",
        "The more frequently a term appears in a topic, the more important the term is for a story belonging to the topic, therefore the term should be set higher weight.",
        "Term class D: terms that appear in a topic exclusively, but not frequently.",
        "For example, the name of a fireman who did very well in a salvage action, which may appears in only two or three stories but never appeared in other topics.",
        "Terms of this type should receive more weights than in TF-IDF model.",
        "However, since they are not popular in the topic, it is not appropriate to give them too high weights.",
        "Term class E: terms with low document frequency, and appear in different topics.",
        "Terms of this class should receive lower weights.",
        "Now we analyze whether TF-IDF model can give proper weights to the five classes of terms.",
        "Obviously, terms of class A are lowly weighted in TF-IDF model, which is conformable with the requirement described above.",
        "In TF-IDF model, terms of class B are highly dependant with the number of stories in a news class.",
        "TF-IDF model cannot provide low weights if the story containing the term belongs to a relative small news class.",
        "For a term of class C, the more frequently it appears in a topic, the less weight TFIDF model gives to it.",
        "This strongly conflicts with the requirement of terms in class C. For terms of class D, TF-IDF model gives them high weights correctly.",
        "But for terms of class E, TF-IDF model gives high weights to them which are not conformable with the requirement of low weights.",
        "To sum up, terms of class B, C, E cannot be properly weighted in TF-IDF model.",
        "So, we propose a modified model to resolve this problem.",
        "When θ init andθ new are set closely, we assume that most of the stories in a first-level cluster (a direct child node of root node) are on the same topic.",
        "Therefore, we make use of a first-level cluster to capture term distribution (df for all the terms within the cluster) within the topic dynamically.",
        "KL divergence of term distribution in a first-level cluster and the whole story set is used to adjust term weights:  ( , , ) * (1 * ( || )) ( , , ) ( , , ) * (1 * ( || )) cw tw cw tw w d D weight d t w KL P P weight d t w weight d t w KL P P γ γ ∈ + = +∑ (5) where ( ) ( ) ( ) ( ) 1,cw cw c c c c df w df w p y p y N N = = − (6) ( ) ( ) ( ) ( ) 1,t t tw tw t t df w df w p y p y N N = = − (7) where dfc(w) is the number of documents containing term w within cluster C, and Nc is the number of documents in cluster C, and Nt is the total number of documents that arrive before time step t. γ is a const parameter, now is manually set 3.",
        "KL divergence is defined as follows [17]: ( ) ( || ) ( ) log ( )x p x KL P Q p x q x = ∑ (8) The basic idea is: for a story in a topic, the more a term occurs within the topic, and the less it occurs in other topics, it should be assigned higher weights.",
        "Obviously, modified model can meet all the requirements of the five term classes listed above. 5.2 Term Reweighting Based on Term Type and Story Class Previous work found that some classes of news stories could achieve good improvements by giving extra weight to named entities.",
        "But we find that terms of different types should be given different amount of extra weight for different classes of news stories.",
        "We use open-NLP1 to recognize named entity types and part-ofspeech tags for terms that appear in news stories.",
        "Named entity types include person name, organization name, location name, date, time, money and percentage, and five POSs are selected: none (NN), verb (VB), adjective (JJ), adverb (RB) and cardinal number (CD).",
        "Statistical analysis shows topic-level discriminative terms types for different classes of stories.",
        "For the sake of convenience, named entity type and part-of-speech tags are uniformly called term type in subsequent sections.",
        "Determining whether two stories are about the same topic is a basic component for NED task.",
        "So at first we use 2 χ statistic to compute correlations between terms and topics.",
        "For a term t and a topic T, a contingence table is derived: Table 1.",
        "A 2×2 Contingence Table Doc Number belong to topic T not belong to topic T include t A B not include t C D The 2 χ statistic for a specific term t with respect to topic T is defined to be [16]: 2 2 ( , ) ( ) * ( ) ( ) * ( ) * ( ) * ( ) w T A B C D AD CB A C B D A B C D χ = + + + − + + + + (9) News topics for the TDT task are further classified into 11 rules of interpretations (ROIs) 2 .",
        "The ROI can be seen as a higher level class of stories.",
        "The average correlation between a term type and a topic ROI is computed as: 2 avg 2 ( , )( ( , ) )k m m km kT R w P w TP R p w T R P χ χ ∈ ∈ ∑ ∑（ , ）= 1 1 k=1…K, m=1…M (10) where K is the number of term types (set 12 constantly in the paper).",
        "M is the number news classes (ROIs, set 11 in the paper).",
        "Pk represents the set of all terms of type k, and Rm represents the set of all topics of class m, p(t,T) means the probability that t occurs in topic T. Because of limitation of space, only parts of the term types (9 term types) and parts of news classes (8 classes) are listed in table 2 with the average correlation values between them.",
        "The statistics is derived from labeled data in TDT2 corpus. (Results in table 2 are already normalized for convenience in comparison.)",
        "The statistics in table 2 indicates the usefulness of different term types in topic discrimination with respect to different news classes.",
        "We can see that, location name is the most useful term type for three news classes: Natural Disasters, Violence or War, Finances.",
        "And for three other categories Elections, Legal/Criminal Cases, Science and Discovery, person name is the most discriminative term type.",
        "For Scandals/Hearings, date is the most important information for topic discrimination.",
        "In addition, Legal/Criminal Cases and Finance topics have higher correlation with money terms, while Science and Discovery have higher correlation with percentage terms.",
        "Non-name terms are more stable for different classes. 1 . http://opennlp.sourceforge.net/ 2 . http://projects.ldc.upenn.edu/TDT3/Guide/label.html From the analysis of table 2, it is reasonable to adjust term weight according to their term type and the news class the story belongs to.",
        "New term weights are reweighted as follows: ( ) ( ) ( ) ( ) ( , , ) * ( , , ) ( , , ) * class d D type w T class d D type w w d weight d t w weight d t w weight d t w α α ∈ = ∑ (11) where type(w) represents the type of term w, and class(d) represents the class of story d, c kα is reweighting parameter for news class c and term type k. In the work, we just simply use statistics in table 2 as the reweighting parameters.",
        "Even thought using the statistics directly may not the best choice, we do not discuss how to automatically obtain the best parameters.",
        "We will try to use machine learning techniques to obtain the best parameters in the future work.",
        "In the work, we use BoosTexter [20] to classify all stories into one of the 11 ROIs.",
        "BoosTexter is a boosting based machine learning program, which creates a series of simple rules for building a classifier for text or attribute-value data.",
        "We use term weight generated using TF-IDF model as feature for story classification.",
        "We trained the model on the 12000 judged English stories in TDT2, and classify the rest of the stories in TDT2 and all stories in TDT3.",
        "Classification results are used for term reweighting in formula (11).",
        "Since the class labels of topic-off stories are not given in TDT datasets, we cannot give the classification accuracy here.",
        "Thus we do not discuss the effects of classification accuracy to NED performance in the paper. 6.",
        "EXPERIMENTAL SETUP 6.1 Datasets We used two LDC [18] datasets TDT2 and TDT3 for our experiments.",
        "TDT2 contains news stories from January to June 1998.",
        "It contains around 54,000 stories from sources like ABC, Associated Press, CNN, New York Times, Public Radio International, Voice of America etc.",
        "Only English stories in the collection were considered.",
        "TDT3 contains approximately 31,000 English stories collected from October to December 1998.",
        "In addition to the sources used in TDT2, it also contains stories from NBC and MSNBC TV broadcasts.",
        "We used transcribed versions of the TV and radio broadcasts besides textual news.",
        "TDT2 dataset is labeled with about 100 topics, and approximately 12,000 English stories belong to at least one of these topics.",
        "TDT3 dataset is labeled with about 120 topics, and approximately 8000 English stories belong to at least one of these topics.",
        "All the topics are classified into 11 Rules of Interpretation: (1)Elections, (2)Scandals/Hearings, (3)Legal/Criminal Cases, (4)Natural Disasters, (5)Accidents, (6)Ongoing Violence or War, (7)Science and Discovery News, (8)Finance, (9)New Law, (10)Sports News, (11)MISC.",
        "News. 6.2 Evaluation Metric TDT uses a cost function CDet that combines the probabilities of missing a new story and a false alarm [19]: * * * *Det Miss Miss Target FA FA NontargetC C P P C P P= + (12) Table 2.",
        "Average correlation between term types and news classes where CMiss means the cost of missing a new story, PMiss means the probability of missing a new story, and PTarget means the probability of seeing a new story in the data; CFA means the cost of a false alarm, PFA means the probability of a false alarm, and PNontarget means the probability of seeing an old story.",
        "The cost CDet is normalized such that a perfect system scores 0 and a trivial system, which is the better one of mark all stories as new or old, scores 1: ( ( * , * ) ) Det Det Miss Target FA Nontarget C Norm C min C P C P = (13) New event detection system gives two outputs for each story.",
        "The first part is yes or no indicating whether the story triggers a new event or not.",
        "The second part is a score indicating confidence of the first decision.",
        "Confidence scores can be used to plot DET curve, i.e., curves that plot false alarm vs. miss probabilities.",
        "Minimum normalized cost can be determined if optimal threshold on the score were chosen. 7.",
        "EXPERIMENTAL RESULTS 7.1 Main Results To test the approaches proposed in the model, we implemented and tested five systems: System-1: this system is used as baseline.",
        "It is implemented based on the basic model described in section 3, i.e., using incremental TF-IDF model to generate term weights, and using Hellinger distance to compute document similarity.",
        "Similarity score normalization is also employed [8].",
        "S-S detection procedure is used.",
        "System-2: this system is the same as system-1 except that S-C detection procedure is used.",
        "System-3: this system is the same as system-1 except that it uses the new detection procedure which is based on indexing-tree.",
        "System-4: implemented based on the approach presented in section 5.1, i.e., terms are reweighted according to the distance between term distributions in a cluster and all stories.",
        "The new detection procedure is used.",
        "System-5: implemented based on the approach presented in section 5.2, i.e., terms of different types are reweighted according to news class using trained parameters.",
        "The new detection procedure is used.",
        "The following are some other NED systems: System-6: [21] for each pair of stories, it computes three similarity values for named entity, non-named entity and all terms respectively.",
        "And employ Support Vector Machine to predict new or old using the similarity values as features.",
        "System-7: [8] it extended a basic incremental TF-IDF model to include source-specific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, etc.",
        "System-8: [13] it split document representation into two parts: named entities and non-named entities, and choose one effective part for each news class.",
        "Table 3 and table 4 show topic-weighted normalized costs and comparing times on TDT2 and TDT3 datasets respectively.",
        "Since no heldout data set for fine-tuning the threshold θ new was available for experiments on TDT2, we only report minimum normalized costs for our systems in table 3.",
        "System-5 outperforms all other systems including system-6, and it performs only 2.78e+8 comparing times in detection procedure which is only 13.4% of system-1.",
        "Table 3.",
        "NED results on TDT2 Systems Min Norm(CDet) Cmp times System-1 0.5749 2.08e+9 System-2① 0.6673 3.77e+8 System-3② 0.5765 2.81e+8 System-4② 0.5431 2.99e+8 System-5② 0.5089 2.78e+8 System-6 0.5300 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 When evaluating on the normalized costs on TDT3, we use the optimal thresholds obtained from TDT2 data set for all systems.",
        "System-2 reduces comparing times to 1.29e+9 which is just 18.3% of system-1, but at the same time it also gets a deteriorated minimum normalized cost which is 0.0499 higher than system-1.",
        "System-3 uses the new detection procedure based on news indexing-tree.",
        "It requires even less comparing times than system-2.",
        "This is because story-story comparisons usually yield greater similarities than story-cluster ones, so stories tend to be combined Location Person Date Organization Money Percentage NN JJ CD Elections 0.37 1 0.04 0.58 0.08 0.03 0.32 0.13 0.1 Scandals/Hearings 0.66 0.62 0.28 1 0.11 0.02 0.27 0.13 0.05 Legal/Criminal Cases 0.48 1 0.02 0.62 0.15 0 0.22 0.24 0.09 Natural Disasters 1 0.27 0 0.04 0.04 0 0.25 0.04 0.02 Violence or War 1 0.36 0.02 0.14 0.02 0.04 0.21 0.11 0.02 Science and Discovery 0.11 1 0.01 0.22 0.08 0.12 0.19 0.08 0.03 Finances 1 0.45 0.04 0.98 0.13 0.02 0.29 0.06 0.05 Sports 0.16 0.27 0.01 1 0.02 0 0.11 0.03 0.01 together in system-3.",
        "And system-3 is basically equivalent to system-1 in accuracy results.",
        "System-4 adjusts term weights based on the distance of term distributions between the whole corpus and cluster story set, yielding a good improvement by 0.0468 compared to system-1.",
        "The best system (system-5) has a minimum normalized cost 0.5012, which is 0.0797 better than system-1, and also better than any other results previously reported for this dataset [8, 13].",
        "Further more, system-5 only needs 1.05e+8 comparing times which is 14.9% of system-1.",
        "Table 4.",
        "NED results on TDT3 Systems Norm(CDet) Min Norm(CDet) Cmp times System-1 0.6159 0.5809 7.04e+8 System-2① 0.6493 0.6308 1.29e+8 System-3② 0.6197 0.5868 1.03e+8 System-4② 0.5601 0.5341 1.03e+8 System-5② 0.5413 0.5012 1.05e+8 System-7 -- 0.5783 -System-8 -- 0.5229 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 Figure5 shows the five DET curves for our systems on data set TDT3.",
        "System-5 achieves the minimum cost at a false alarm rate of 0.0157 and a miss rate of 0.4310.",
        "We can observe that System4 and System-5 obtain lower miss probability at regions of low false alarm probabilities.",
        "The hypothesis is that, more weight value is transferred to key terms of topics from non-key terms.",
        "Similarity score between two stories belonging to different topics are lower than before, because their overlapping terms are usually not key terms of their topics. 7.2 Parameter selection for indexing-tree detection Figure 3 shows the minimum normalized costs obtained by system-3 on TDT3 using different parameters.",
        "Theθ init parameter is tested on six values spanning from 0.03 to 0.18.",
        "And the λ parameter is tested on four values 1, 2, 3 and 4.",
        "We can see that, whenθ init is set to 0.12, which is the closest one toθ new, the costs are lower than others.",
        "This is easy to explain, because when stories belonging to the same topic are put in a cluster, it is more reasonable for the cluster to represent the stories in it.",
        "When parameter λ is set to 3 or 4, the costs are better than other cases, but there is no much difference between 3 and 4. 0 0.05 0.1 0.15 0.2 1 2 3 4 0.5 0.6 0.7 0.8 0.9 1 θ-initλ MinCost 0.6 0.65 0.7 0.75 0.8 0.85 0.9 Figure 3.",
        "Min Cost on TDT3 (δ =0.15) 0 0.05 0.1 0.15 0.2 1 2 3 4 0 0.5 1 1.5 2 2.5 x 10 8 θ-init λ Comparingtimes 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 8 Figure 4.",
        "Comparing times on TDT3 (δ =0.15) Figure 4 gives the comparing times used by system-3 on TDT3 with the same parameters as figure 3.",
        "The comparing times are strongly dependent onθ init.",
        "Because the greaterθ init is, the less stories combined together, the more comparing times are needed for new event decision.",
        "So we useθ init =0.13,λ =3,δ =0.15 for system-3, 4, and 5.",
        "In this parameter setting, we can get both low minimum normalized costs and less comparing times. 8.",
        "CONCLUSION We have proposed a news indexing-tree based detection procedure in our model.",
        "It reduces comparing times to about one seventh of traditional method without hurting NED accuracy.",
        "We also have presented two extensions to the basic TF-IDF model.",
        "The first extension is made by adjust term weights based on term distributions between the whole corpus and a cluster story set.",
        "And the second extension to basic TF-IDF model is better use of term types (named entities types and part-of-speed) according to news categories.",
        "Our experimental results on TDT2 and TDT3 datasets show that both of the two extensions contribute significantly to improvement in accuracy.",
        "We did not consider news time information as a clue for NED task, since most of the topics last for a long time and TDT data sets only span for a relative short period (no more than 6 months).",
        "For the future work, we want to collect news set which span for a longer period from internet, and integrate time information in NED task.",
        "Since topic is a relative coarse-grained news cluster, we also want to refine cluster granularity to event-level, and identify different events and their relations within a topic.",
        "Acknowledgments This work is supported by the National Natural Science Foundation of China under Grant No. 90604025.",
        "Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsor. 9.",
        "REFERENCES [1] http://www.nist.gov/speech/tests/tdt/index.htm [2] In Topic Detection and Tracking.",
        "Event-based Information Organization.",
        "Kluwer Academic Publishers, 2002. .01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90 1 2 5 10 20 40 60 80 90 False Alarm Probability (in %) MissProbability(in%) SYSTEM1 Topic Weighted Curve SYSTEM1 Min Norm(Cost) SYSTEM2 Topic Weighted Curve SYSTEM2 Min Norm(Cost) SYSTEM3 Topic Weighted Curve SYSTEM3 Min Norm(Cost) SYSTEM4 Topic Weighted Curve SYSTEM4 Min Norm(Cost) SYSTEM5 Topic Weighted Curve SYSTEM5 Min Norm(Cost) Random Performance Figure 5.",
        "DET curves on TDT3 [3] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B.T.",
        "Archibald, and X. Liu.",
        "Learning Approaches for Detecting and Tracking News Events.",
        "In IEEE Intelligent Systems Special Issue on Applications of Intelligent Information Retrieval, volume 14 (4), 1999, 32-43. [4] Y. Yang, T. Pierce, and J. Carbonell.",
        "A Study on Retrospective and On-line Event Detection.",
        "In Proceedings of SIGIR-98, Melbourne, Australia, 1998, 28-36. [5] J. Allan, V. Lavrenko, D. Malin, and R. Swan.",
        "Detections, Bounds, and Timelines: Umass and tdt-3.",
        "In Proceedings of Topic Detection and Tracking Workshop (TDT-3), Vienna, VA, 2000, 167-174. [6] R. Papka and J. Allan.",
        "On-line New Event Detection Using Single Pass Clustering TITLE2:.",
        "Technical Report UM-CS1998-021, 1998. [7] W. Lam, H. Meng, K. Wong, and J.",
        "Yen.",
        "Using Contextual Analysis for News Event Detection.",
        "International Journal on Intelligent Systems, 2001, 525-546. [8] B. Thorsten, C. Francine, and F. Ayman.",
        "A System for New Event Detection.",
        "In Proceedings of the 26th Annual International ACM SIGIR Conference, New York, NY, USA.",
        "ACM Press. 2003, 330-337. [9] S. Nicola and C. Joe.",
        "Combining Semantic and Syntactic Document Classifiers to Improve First Story Detection.",
        "In Proceedings of the 24th Annual International ACM SIGIR Conference, New York, NY, USA.",
        "ACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
        "Topicconditioned Novelty Detection.",
        "In Proceedings of the 8th ACM SIGKDD International Conference, ACM Press. 2002, 688-693. [11] M. Juha, A.M. Helena, and S. Marko.",
        "Applying Semantic Classes in Event Detection and Tracking.",
        "In Proceedings of International Conference on Natural Language Processing (ICON 2002), 2002, pages 175-183. [12] M. Juha, A.M. Helena, and S. Marko.",
        "Simple Semantics in Topic Detection and Tracking.",
        "Information Retrieval, 7(3-4): 2004, 347-368. [13] K. Giridhar and J. Allan.",
        "Text Classification and Named Entities for New Event Detection.",
        "In Proceedings of the 27th Annual International ACM SIGIR Conference, New York, NY, USA.",
        "ACM Press. 2004, 297-304. [14] J. P. Callan, W. B. Croft, and S. M. Harding.",
        "The INQUERY Retrieval System.",
        "In Proceedings of DEXA-92, 3rd International Conference on Database and Expert Systems Applications, 1992, 78-83. [15] R. Krovetz.",
        "Viewing Morphology as An Inference Process.",
        "In Proceedings of ACM SIGIR93, 1993, 61-81. [16] Y. Yang and J. Pedersen.",
        "A Comparative Study on Feature Selection in Text Categorization.",
        "In J. D. H. Fisher, editor, The Fourteenth International Conference on Machine Learning (ICML97), Morgan Kaufmann, 1997, 412-420. [17] T. M. Cover, and J.A.",
        "Thomas.",
        "Elements of Information Theory.",
        "Wiley. 1991. [18] The linguistic data consortium, http://www.ldc,upenn.edu/. [19] The 2001 TDT task definition and evaluation plan, http://www.nist.gov/speech/tests/tdt/tdt2001/evalplan.htm. [20] R. E. Schapire and Y.",
        "Singer.",
        "Boostexter: A Boosting-based System for Text Categorization.",
        "In Machine Learning 39(2/3):1, Kluwer Academic Publishers, 2000, 35-168. [21] K. Giridhar and J. Allan. 2005.",
        "Using Names and Topics for New Event Detection.",
        "In Proceedings of Human Technology Conference and Conference on Empirical Methods in Natural Language, Vancouver, 2005, 121-128"
    ],
    "translated_text_sentences": [
        "Detección de nuevos eventos basada en árbol de indexación y entidad nombrada Zhang Kuo Universidad Tsinghua Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Universidad Tsinghua Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Universidad Tsinghua Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn RESUMEN La Detección de Nuevos Eventos (NED) tiene como objetivo detectar de una o varias corrientes de noticias cuál se informa sobre un nuevo evento (es decir,",
        "no reportado previamente).",
        "Con el abrumador volumen de noticias disponible hoy en día, hay una creciente necesidad de un sistema NED que sea capaz de detectar eventos nuevos de manera más eficiente y precisa.",
        "En este artículo proponemos un nuevo modelo de NED para acelerar la tarea de NED mediante el uso dinámico de un árbol de indexación de noticias.",
        "Además, basándose en la observación de que los términos de diferentes tipos tienen diferentes efectos para la tarea de NED, se proponen dos enfoques de reponderación de términos para mejorar la precisión de NED.",
        "En el primer enfoque, proponemos ajustar dinámicamente los pesos de los términos basados en los clusters de historias previas y en el segundo enfoque, proponemos emplear estadísticas en los datos de entrenamiento para aprender el modelo de reajuste de entidades nombradas para cada clase de historias.",
        "Los resultados experimentales en dos conjuntos de datos del Consorcio de Datos Lingüísticos (LDC), TDT2 y TDT3, muestran que el modelo propuesto puede mejorar significativamente tanto la eficiencia como la precisión de la tarea de NED, en comparación con el sistema base y otros sistemas existentes.",
        "Categorías y Descriptores de Asignaturas H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información; H.4.2 [Aplicaciones de Sistemas de Información]: Tipos de Sistemas de Apoyo a Decisiones.",
        "Términos generales Algoritmos, Rendimiento, Experimentación 1.",
        "El programa de Detección y Seguimiento de Temas (TDT) tiene como objetivo desarrollar técnicas que puedan organizar, buscar y estructurar de manera efectiva materiales de texto de noticias de una variedad de agencias de noticias y medios de difusión [1].",
        "La Detección de Eventos Nuevos (DEN) es una de las cinco tareas en TDT.",
        "Es la tarea de identificación en línea del informe más temprano para cada tema tan pronto como ese informe llegue en la secuencia de documentos.",
        "Un tema se define como un evento o actividad seminal, junto con eventos y actividades directamente relacionados [2].",
        "Un evento se define como algo (no trivial) que sucede en un lugar y en un momento determinados [3].",
        "Por ejemplo, cuando una bomba explota en un edificio, la explosión es el evento seminal que desencadena el tema, y otras historias sobre el mismo tema serían aquellas que discuten los esfuerzos de salvamento, la búsqueda de los perpetradores, arrestos y juicios, entre otros.",
        "La información útil de noticias suele estar enterrada en una masa de datos generados a diario.",
        "Por lo tanto, los sistemas NED son muy útiles para las personas que necesitan detectar información novedosa de un flujo de noticias en tiempo real.",
        "Estas necesidades de la vida real a menudo se presentan en ámbitos como los mercados financieros, el análisis de noticias y la recopilación de inteligencia.",
        "En la mayoría de los sistemas NED de última generación, cada noticia disponible se compara con todas las noticias previamente recibidas.",
        "Si todas las similitudes entre ellos no superan un umbral, entonces la historia desencadena un nuevo evento.",
        "Por lo general, se presentan en forma de similitud de coseno o métrica de similitud de Hellinger.",
        "El problema central de NED es identificar si dos historias tratan sobre el mismo tema.",
        "Obviamente, estos sistemas no pueden aprovechar la información del tema.",
        "Además, no es aceptable en aplicaciones reales debido a la gran cantidad de cálculos requeridos en el proceso de NED.",
        "Otros sistemas organizan historias anteriores en grupos (cada grupo corresponde a un tema), y la nueva historia se compara con los grupos anteriores en lugar de con historias.",
        "Esta forma puede reducir significativamente los tiempos de comparación.",
        "Sin embargo, se ha demostrado que este método es menos preciso [4, 5].",
        "Esto se debe a que a veces las historias dentro de un tema se alejan mucho entre sí, lo que podría resultar en una baja similitud entre una historia y su tema.",
        "Por otro lado, algunos sistemas NED propuestos intentaron mejorar la precisión al hacer un mejor uso de las entidades nombradas [10, 11, 12, 13].",
        "Sin embargo, ninguno de los sistemas ha considerado que los términos de diferentes tipos (por ejemplo,",
        "Los sustantivos, verbos o nombres de personas tienen diferentes efectos en diferentes clases de historias para determinar si dos historias tratan sobre el mismo tema.",
        "Por ejemplo, los nombres de los candidatos electorales (Nombre de la persona) son muy importantes para las historias de la clase de elecciones; las ubicaciones (Nombre de la ubicación) donde ocurrieron los accidentes son importantes para las historias de la clase de accidentes.",
        "Por lo tanto, en NED, todavía existen los siguientes tres problemas por investigar: (1) ¿Cómo acelerar el procedimiento de detección sin disminuir la precisión de la detección? (2) ¿Cómo aprovechar mejor la información de los grupos (temas) para mejorar la precisión? (3) ¿Cómo obtener una mejor representación de historias de noticias mediante una mejor comprensión de las entidades nombradas?",
        "Impulsados por estos problemas, hemos propuesto tres enfoques en este documento. (1) Para hacer que el procedimiento de detección sea más rápido, proponemos un nuevo procedimiento de NED basado en un árbol de indexación de noticias creado dinámicamente.",
        "El índice de historias en árbol se crea ensamblando historias similares para formar grupos de noticias en diferentes jerarquías según sus valores de similitud.",
        "Las comparaciones entre la historia actual y los grupos anteriores podrían ayudar a encontrar la historia más similar en menos tiempo de comparación.",
        "El nuevo procedimiento puede reducir la cantidad de veces de comparación sin afectar la precisión. Utilizamos los grupos del primer piso en el árbol de indexación como temas de noticias, en los cuales los pesos de los términos se ajustan dinámicamente según la distribución de términos en los grupos.",
        "En este enfoque, se utiliza adecuadamente la información de los grupos (temas), por lo que se evita el problema de la descentralización del tema. Basándonos en observaciones de las estadísticas obtenidas de los datos de entrenamiento, encontramos que los términos de diferentes tipos (por ejemplo,",
        "Los sustantivos y verbos tienen diferentes efectos para diferentes clases de historias al determinar si dos historias tratan sobre el mismo tema.",
        "Y proponemos utilizar estadísticas para optimizar los pesos de los términos de diferentes tipos en una historia de acuerdo a la clase de noticias a la que pertenece la historia.",
        "En el conjunto de datos TDT3, el nuevo modelo NED solo utiliza un 14.9% en comparación con los tiempos del modelo básico, mientras que su costo normalizado mínimo es de 0.5012, lo que es 0.0797 mejor que el modelo básico, y también mejor que cualquier otro resultado previamente reportado para este conjunto de datos [8, 13].",
        "El resto del documento está organizado de la siguiente manera.",
        "Comenzamos este artículo resumiendo el trabajo previo en NED en la sección 2.",
        "La sección 3 presenta el modelo básico para NED que la mayoría de los sistemas actuales utilizan.",
        "La sección 4 describe nuestro nuevo procedimiento de detección basado en el índice de árbol de noticias.",
        "En la sección 5, se proponen dos métodos de reponderación de términos para mejorar la precisión de NED.",
        "La sección 6 presenta nuestros datos experimentales y métricas de evaluación.",
        "Finalmente concluimos con los resultados experimentales en la Sección 7, y las conclusiones y trabajo futuro en la Sección 8.",
        "TRABAJO RELACIONADO Papka et al. propusieron el agrupamiento de un solo paso en NED [6].",
        "Cuando se encontraba una nueva historia, esta era procesada inmediatamente para extraer características de términos y se construía una representación de consulta del contenido de la historia.",
        "Luego se comparó con todas las consultas anteriores.",
        "Si el documento no generaba ninguna consulta al exceder un umbral, se marcaba como un nuevo evento.",
        "Lam et al construyen representaciones de consultas previas de grupos de historias, cada una de las cuales corresponde a un tema [7].",
        "De esta manera se realizan comparaciones entre historias y grupos.",
        "En los últimos años, la mayoría de los trabajos se han centrado en proponer mejores métodos para la comparación de historias y la representación de documentos.",
        "Brants et al. [8] extendieron un modelo básico incremental de TF-IDF para incluir modelos específicos de fuente, normalización de puntajes de similitud basada en promedios específicos de documentos, normalización de puntajes de similitud basada en promedios específicos de pares de fuentes, reponderación de términos basada en frecuencias de eventos inversas y segmentación de documentos.",
        "Se mostraron buenas mejoras en los puntos de referencia de TDT.",
        "Stokes et al. [9] utilizaron una combinación de evidencia de dos representaciones distintas del contenido de un documento.",
        "Una de las representaciones era el vector de texto libre habitual, la otra hacía uso de cadenas léxicas (creadas utilizando WordNet) para construir otro vector de términos.",
        "Luego, las dos representaciones se combinan de forma lineal.",
        "Se logró un aumento marginal en la efectividad cuando se utilizó la representación combinada.",
        "Algunos esfuerzos se han realizado sobre cómo utilizar entidades nombradas para mejorar la desambiguación de entidades nombradas (NED).",
        "Yang et al. asignaron un peso cuatro veces mayor a las entidades con nombres de ubicación que a otros términos y entidades con nombres [10].",
        "El grupo de investigación DOREMI combinó similitudes semánticas de nombres de personas, nombres de lugares y tiempo junto con similitud textual [11][12].",
        "El grupo de investigación de UMass [13] dividió la representación del documento en dos partes: entidades nombradas y entidades no nombradas.",
        "Y se descubrió que algunas clases de noticias podían lograr un mejor rendimiento utilizando la representación de entidades nombradas, mientras que otras clases de noticias podían lograr un mejor rendimiento utilizando la representación sin entidades nombradas.",
        "Tanto [10] como [13] utilizaron la técnica de categorización de texto para clasificar previamente las noticias.",
        "En [13] las noticias se clasifican automáticamente al principio, y luego se prueban las sensibilidades de los nombres y términos que no son nombres para NED para cada clase.",
        "En [10], se eliminan los términos frecuentes de cada clase de la representación del documento.",
        "Por ejemplo, la palabra \"elección\" no ayuda a identificar diferentes elecciones.",
        "En su trabajo, no se investiga la efectividad de diferentes tipos de nombres (o términos con diferentes categorías gramaticales) para NED en diferentes clases de noticias.",
        "Utilizamos análisis estadístico para revelar el hecho y utilizarlo para mejorar el rendimiento de NED. 3.",
        "MODELO BÁSICO En esta sección, presentamos el modelo básico de Detección de Nuevos Eventos que es similar a lo que la mayoría de los sistemas actuales aplican.",
        "Luego, proponemos nuestro nuevo modelo ampliando el modelo básico.",
        "Los sistemas de detección de eventos nuevos utilizan flujos de noticias como entrada, en los cuales las historias están estrictamente ordenadas por tiempo.",
        "Solo están disponibles las historias recibidas previamente al tratar con la historia actual.",
        "La salida es una decisión sobre si la historia actual es un evento nuevo o no y la confianza de la decisión.",
        "Por lo general, un modelo NED consta de tres partes: representación de la historia, cálculo de similitud y procedimiento de detección. 3.1 Representación de la historia Se necesita preprocesamiento antes de generar la representación de la historia.",
        "Para el preprocesamiento, tokenizamos palabras, reconocemos abreviaturas, normalizamos abreviaturas, agregamos etiquetas de partes del discurso, eliminamos palabras vacías incluidas en la lista de paradas utilizada en InQuery [14], reemplazamos palabras con sus raíces utilizando el algoritmo K-stem[15], y luego generamos un vector de palabras para cada noticia.",
        "Utilizamos el modelo TF-IDF incremental para el cálculo del peso de los términos [4].",
        "En un modelo TF-IDF, la frecuencia de término en un documento de noticias se pondera por la frecuencia inversa del documento, la cual se genera a partir del corpus de entrenamiento.",
        "Cuando surge un nuevo término en el proceso de prueba, hay dos soluciones: simplemente ignorar el nuevo término o establecer df del término como una constante pequeña (por ejemplo, df = 1).",
        "El nuevo término recibe un peso demasiado bajo en la primera solución (0) y un peso demasiado alto en la segunda solución.",
        "En el modelo TF-IDF incremental, las frecuencias de documentos se actualizan dinámicamente en cada paso de tiempo t: 1( ) ( ) ( )t t D tdf w df w df w−= + (1) donde Dt representa el conjunto de noticias recibidas en el tiempo t, y dfDt(w) significa el número de documentos en los que aparece el término w, y dft(w) significa el número total de documentos en los que aparece el término w antes del tiempo t. En este trabajo, cada ventana de tiempo incluye 50 noticias.",
        "Así, cada historia d recibida en t se representa de la siguiente manera: 1 2{ ( , , ), ( , , ),..., ( , , )}nd peso d t w peso d t w peso d t w→ donde n significa el número de términos distintos en la historia d, y ( , , )peso d t w significa el peso del término w en la historia d en el tiempo t: log( ( , ) 1) log(( 1) /( ( ) 0.5)) ( , , ) log( ( , ) 1) log(( 1) /( ( ) 0.5)) t t t t w d tf d w N df w peso d t w tf d w N df w ∈ + + + = + + +∑ (2) donde Nt significa el número total de historias de noticias antes del tiempo t, y tf(d,w) significa cuántas veces aparece el término w en la historia de noticias d. 3.2 Cálculo de Similitud Utilizamos la distancia de Hellinger para el cálculo de la similitud entre dos historias, para dos historias d y d en el tiempo t, su similitud se define de la siguiente manera: , ( , , ) ( , , ) * ( , , ) w d d sim d d t peso d t w peso d t w ∈ = ∑ (3) 3.3 Procedimiento de Detección Para cada historia d recibida en el paso de tiempo t, el valor ( ) ( ) ( ) ( ( , , )) tiempo d tiempo d n d max sim d d t < = (4) es un puntaje utilizado para determinar si d es una historia sobre un nuevo tema y al mismo tiempo es una indicación de la confianza en nuestra decisión [8]. tiempo(d) significa el tiempo de publicación de la historia d. Si el puntaje supera el umbral θ new, entonces existe un documento suficientemente similar, por lo tanto, d es una historia antigua, de lo contrario, no hay un documento previo suficientemente similar, por lo tanto, d es una historia nueva. 4.",
        "Nuevo procedimiento NED Los sistemas NED tradicionales se pueden clasificar en dos tipos principales en cuanto al procedimiento de detección: (1) tipo S-S, en el que la historia en cuestión se compara con cada historia recibida previamente, y se utiliza la mayor similitud para determinar si la historia actual trata sobre un evento nuevo; (2) tipo S-C, en el que la historia en cuestión se compara con todos los clusters anteriores, cada uno de los cuales representa un tema, y se utiliza la mayor similitud para la decisión final sobre la historia actual.",
        "Si la similitud más alta supera el umbral θ nuevo, entonces es una historia antigua y se coloca en el clúster más similar; de lo contrario, es una historia nueva y se crea un nuevo clúster.",
        "Trabajos anteriores muestran que la primera forma es más precisa que la segunda [4][5].",
        "Dado que a veces las historias dentro de un tema se alejan mucho entre sí, una historia puede tener una similitud muy baja con su tema.",
        "Por lo tanto, utilizar similitudes entre historias para determinar una nueva historia es mejor que utilizar similitudes entre historias y grupos.",
        "Sin embargo, el primer método requiere mucho más tiempo de comparación, lo que significa que es menos eficiente.",
        "Proponemos un nuevo procedimiento de detección que utiliza comparaciones con clusters anteriores para ayudar a encontrar la historia más similar en menos tiempo de comparación, y la decisión final sobre el nuevo evento se toma de acuerdo con la historia más similar.",
        "Por lo tanto, podemos obtener tanto la precisión de los métodos de tipo S-S como la eficiencia de los métodos de tipo S-C.",
        "El nuevo procedimiento crea un árbol de indexación de noticias dinámicamente, en el cual se agrupan historias similares para formar una jerarquía de clusters.",
        "Indexamos historias similares juntas por su ancestro común (un nodo de clúster).",
        "Historias diferentes están indexadas en diferentes grupos.",
        "Cuando una historia está llegando, utilizamos comparaciones entre la historia actual y los grupos jerárquicos anteriores para ayudar a encontrar la historia más similar que es útil para la toma de decisiones sobre nuevos eventos.",
        "Después de que se tome la decisión sobre el nuevo evento, la historia actual se inserta en el árbol de indexación para la detección siguiente.",
        "El índice de noticias en forma de árbol se define formalmente de la siguiente manera: S-Tree = {r, NC, NS, E}, donde r es la raíz de S-Tree, NC es el conjunto de todos los nodos de clúster, NS es el conjunto de todos los nodos de historia, y E es el conjunto de todos los bordes en S-Tree.",
        "Definimos un conjunto de restricciones para un S-Árbol: ⅰ. , es un nodo no terminal en el árbolC i i N i∀ ∈ → ⅱ. , es un nodo terminal en el árbolS i i N i∀ ∈ → ⅲ. , el grado de salida de es al menos 2C i i N i∀ ∈ → ⅳ. , se representa como el centroide de sus descendientesC i i iN∀ ∈ → Para una noticia di, el procedimiento de comparación y el procedimiento de inserción basados en el árbol de indexación se definen de la siguiente manera.",
        "Un ejemplo se muestra en la Figura 1 y la Figura 2.",
        "Figura 1.",
        "Procedimiento de comparación Figura 2.",
        "Procedimiento de comparación de inserción: Paso 1: comparar di con todos los nodos hijos directos de r y seleccionar λ nodos con las similitudes más altas, por ejemplo, C1 2 y C1 3 en la Figura 1.",
        "Paso 2: para cada nodo seleccionado en el paso anterior, por ejemplo.",
        "C1 2, compara di con todos sus nodos hijos directos y selecciona λ nodos con las similitudes más altas, por ejemplo.",
        "C2 2 y d8.",
        "Repetir el paso 2 para todos los nodos no terminales.",
        "Paso 3: registrar el nodo terminal con la mayor similitud a di, por ejemplo, s5, y el valor de similitud (0.20).",
        "Insertando di en el S-árbol con r como raíz: Encuentra el nodo n que es hijo directo de r en el camino desde r hasta el nodo terminal con la similitud más alta s, por ejemplo.",
        "C1 2.",
        "Si s es menor que θ init + (h-1)δ, entonces agregar di al árbol como un hijo directo de r. De lo contrario, si n es un nodo terminal, crear un nodo de clúster en lugar de n, y agregar tanto n como di como sus hijos directos; si n es un nodo no terminal, repetir este procedimiento e insertar di en el subárbol con n como raíz de forma recursiva.",
        "Aquí h es la longitud entre n y la raíz del árbol S.",
        "Cuanto más similares sean las historias en un grupo, mejor representará el grupo a las historias en él.",
        "Por lo tanto, no imponemos restricciones en la altura máxima de los árboles y el grado de un nodo.",
        "Por lo tanto, no podemos dar la complejidad de este procedimiento basado en árboles de indexación.",
        "Pero proporcionaremos el número de veces de comparación necesarias por el nuevo procedimiento en nuestros experimentos en la sección 7.5.",
        "En esta sección, se proponen dos métodos de reponderación de términos para mejorar la precisión de NED.",
        "En el primer método, se explora una nueva forma de utilizar de manera más efectiva la información de los grupos (temas).",
        "El segundo encuentra una mejor manera de utilizar entidades nombradas basadas en la clasificación de noticias. 5.1 Reajuste de términos basado en la distancia de distribución TF-IDF es el modelo más prevalente utilizado en sistemas de recuperación de información.",
        "La idea básica es que cuanto menos documentos aparezca un término, más importante es el término en la discriminación de documentos (relevantes o no relevantes para una consulta que contenga el término).",
        "Sin embargo, en el dominio de TDT, necesitamos discriminar documentos en función de los temas en lugar de las consultas.",
        "Intuitivamente, el uso de vectores de clúster (tema) para comparar con historias de noticias posteriores debería tener un mejor rendimiento que el uso de vectores de historias.",
        "Desafortunadamente, los resultados experimentales no respaldan esta intuición [4][5].",
        "Basándonos en la observación de los datos, encontramos que la razón es que un tema de noticias generalmente contiene muchos eventos directa o indirectamente relacionados, y todos ellos tienen sus propios subtemas que suelen ser diferentes entre sí.",
        "Tomemos el tema descrito en la sección 1 como ejemplo, eventos como la explosión y el rescate tienen similitudes muy bajas con eventos sobre juicios criminales, por lo tanto, las historias sobre juicios tendrían una baja similitud con el vector de tema construido en base a sus eventos previos.",
        "Esta sección se centra en cómo hacer un uso efectivo de la información del tema y al mismo tiempo evitar el problema de la descentralización del contenido.",
        "Al principio, clasificamos los términos en 5 clases para ayudar en el análisis de las necesidades del modelo modificado: Clase de términos A: términos que ocurren con frecuencia en todo el corpus, por ejemplo, año y personas.",
        "Los términos de esta clase deben recibir pesos bajos porque no ayudan mucho en la discriminación de temas.",
        "Clase B de términos: términos que ocurren con frecuencia dentro de una categoría de noticias, por ejemplo, elección, tormenta.",
        "Son útiles para distinguir dos historias en diferentes categorías de noticias.",
        "Sin embargo, no pueden proporcionar información para determinar si dos historias tratan sobre el mismo tema o temas diferentes.",
        "En otras palabras, los términos elección y tormenta no son útiles para diferenciar dos campañas electorales y dos desastres de tormenta.",
        "Por lo tanto, los términos de esta clase deberían asignarse pesos más bajos.",
        "Clase de término C: términos que ocurren con frecuencia en un tema y raramente en otros temas, por ejemplo, el nombre de un avión accidentado, el nombre de un huracán específico.",
        "Las noticias que pertenecen a diferentes temas rara vez tienen términos superpuestos en esta clase.",
        "Cuanto más frecuentemente aparezca un término en un tema, más importante es el término para una historia perteneciente al tema, por lo tanto, el término debería tener un peso más alto.",
        "Clase de término D: términos que aparecen en un tema de manera exclusiva, pero no con frecuencia.",
        "Por ejemplo, el nombre de un bombero que se desempeñó muy bien en una acción de salvamento, que puede aparecer en solo dos o tres historias pero nunca ha aparecido en otros temas.",
        "Los términos de este tipo deberían recibir más peso que en el modelo TF-IDF.",
        "Sin embargo, dado que no son populares en el tema, no es apropiado asignarles pesos demasiado altos.",
        "Clase de término E: términos con baja frecuencia en documentos y que aparecen en diferentes temas.",
        "Los términos de esta clase deberían recibir pesos más bajos.",
        "Ahora analizamos si el modelo TF-IDF puede asignar pesos adecuados a las cinco clases de términos.",
        "Obviamente, los términos de la clase A tienen un peso bajo en el modelo TF-IDF, lo cual es acorde con el requisito descrito anteriormente.",
        "En el modelo TF-IDF, los términos de la clase B dependen altamente del número de historias en una clase de noticias.",
        "El modelo TF-IDF no puede proporcionar pesos bajos si la historia que contiene el término pertenece a una clase de noticias relativamente pequeña.",
        "Para un término de clase C, cuanto más frecuentemente aparezca en un tema, menos peso le da el modelo TFIDF.",
        "Esto entra en conflicto directo con el requisito de términos en la clase C. Para los términos de la clase D, el modelo TF-IDF les asigna pesos altos de manera correcta.",
        "Pero para los términos de la clase E, el modelo TF-IDF asigna pesos altos a ellos que no son conformes con el requisito de pesos bajos.",
        "En resumen, los términos de clase B, C, E no pueden ser ponderados adecuadamente en el modelo TF-IDF.",
        "Por lo tanto, proponemos un modelo modificado para resolver este problema.",
        "Cuando θ init y θ new están configurados de cerca, asumimos que la mayoría de las historias en un clúster de primer nivel (un nodo hijo directo del nodo raíz) están en el mismo tema.",
        "Por lo tanto, utilizamos un clúster de primer nivel para capturar la distribución de términos (df para todos los términos dentro del clúster) dentro del tema de forma dinámica.",
        "La divergencia de Kullback-Leibler de la distribución de términos en un clúster de primer nivel y el conjunto completo de historias se utiliza para ajustar los pesos de los términos: ( , , ) * (1 * ( || )) ( , , ) ( , , ) * (1 * ( || )) cw tw cw tw w d D peso d t w KL P P peso d t w peso d t w KL P P γ γ ∈ + = +∑ (5) donde ( ) ( ) ( ) ( ) 1,cw cw c c c c df w df w p y p y N N = = − (6) ( ) ( ) ( ) ( ) 1,t t tw tw t t df w df w p y p y N N = = − (7) donde dfc(w) es el número de documentos que contienen el término w dentro del clúster C, y Nc es el número de documentos en el clúster C, y Nt es el número total de documentos que llegan antes del paso de tiempo t. γ es un parámetro constante, actualmente configurado manualmente en 3.",
        "La divergencia de Kullback-Leibler se define de la siguiente manera [17]: La idea básica es que, para una historia en un tema, cuanto más ocurre un término dentro del tema y menos ocurre en otros temas, se le deben asignar pesos más altos.",
        "Obviamente, el modelo modificado puede cumplir con todos los requisitos de las cinco clases de términos mencionadas anteriormente. 5.2 Reajuste de Términos Basado en el Tipo de Término y la Clase de Historia. Trabajos anteriores encontraron que algunas clases de historias de noticias podrían lograr mejoras significativas al dar un peso adicional a las entidades nombradas.",
        "Pero encontramos que los términos de diferentes tipos deben recibir una cantidad diferente de peso adicional para diferentes clases de noticias.",
        "Utilizamos open-NLP1 para reconocer tipos de entidades nombradas y etiquetas de partes del discurso para términos que aparecen en noticias.",
        "Los tipos de entidades nombradas incluyen nombre de persona, nombre de organización, nombre de ubicación, fecha, hora, dinero y porcentaje, y se seleccionan cinco partes del discurso: ninguno (NN), verbo (VB), adjetivo (JJ), adverbio (RB) y número cardinal (CD).",
        "El análisis estadístico muestra tipos de términos discriminativos a nivel de tema para diferentes clases de historias.",
        "Por conveniencia, los tipos de entidad nombrada y etiquetas de parte de la oración se llaman uniformemente tipo de término en las secciones siguientes.",
        "Determinar si dos historias tratan sobre el mismo tema es un componente básico para la tarea de NED.",
        "Por lo tanto, al principio usamos la estadística χ² para calcular las correlaciones entre términos y temas.",
        "Para un término t y un tema T, se deriva una tabla de contingencia: Tabla 1.",
        "Una tabla de contingencia de 2×2 con el número de documentos pertenecientes al tema T y no pertenecientes al tema T incluye t A B no incluye t C D. La estadística χ² para un término específico t con respecto al tema T se define como [16]: χ² = ( ) ( ) * ( ) ( ) * ( ) * ( ) * ( ) w T A B C D AD CB A C B D A B C D + + + − + + + + (9). Los temas de noticias para la tarea TDT se clasifican además en 11 reglas de interpretaciones (ROIs) 2.",
        "El ROI se puede ver como una clase de historias de nivel superior.",
        "El promedio de correlación entre un tipo de término y un ROI de tema se calcula como: 2 avg 2 ( , )( ( , ) )k m m km kT R w P w TP R p w T R P χ χ ∈ ∈ ∑ ∑（ , ）= 1 1 k=1…K, m=1…M (10) donde K es el número de tipos de términos (fijado constantemente en 12 en el artículo).",
        "M es el número de nuevas clases (ROIs, establecido en el conjunto 11 del artículo).",
        "Pk representa el conjunto de todos los términos de tipo k, y Rm representa el conjunto de todos los temas de la clase m, p(t,T) significa la probabilidad de que t ocurra en el tema T. Debido a limitaciones de espacio, solo se enumeran partes de los tipos de términos (9 tipos de términos) y partes de las clases de noticias (8 clases) en la tabla 2 con los valores promedio de correlación entre ellos.",
        "Las estadísticas se derivan de datos etiquetados en el corpus TDT2. (Los resultados en la tabla 2 ya están normalizados para mayor comodidad en la comparación).",
        "Las estadísticas en la tabla 2 indican la utilidad de diferentes tipos de términos en la discriminación de temas con respecto a diferentes clases de noticias.",
        "Podemos ver que el nombre de la ubicación es el tipo de término más útil para tres clases de noticias: Desastres Naturales, Violencia o Guerra, Finanzas.",
        "Y para otras tres categorías: Elecciones, Casos Legales/Criminales, Ciencia y Descubrimiento, el nombre de la persona es el tipo de término más discriminativo.",
        "Para Escándalos/Audiencias, la fecha es la información más importante para la discriminación de temas.",
        "Además, los casos legales/criminales y los temas financieros tienen una correlación más alta con términos relacionados con el dinero, mientras que la ciencia y el descubrimiento tienen una correlación más alta con términos de porcentaje.",
        "Los términos no nominales son más estables para diferentes clases. 1. http://opennlp.sourceforge.net/ 2. http://projects.ldc.upenn.edu/TDT3/Guide/label.html A partir del análisis de la tabla 2, es razonable ajustar el peso de los términos según su tipo de término y la clase de noticias a la que pertenece la historia.",
        "Los nuevos pesos de los términos se recalculan de la siguiente manera: ( ) ( ) ( ) ( ) ( , , ) * ( , , ) ( , , ) * clase d D tipo w T clase d D tipo w w d peso d t w peso d t w peso d t w α α ∈ = ∑ (11) donde type(w) representa el tipo del término w, y class(d) representa la clase de la historia d, c kα es el parámetro de recalibración para la clase de noticias c y el tipo de término k. En el trabajo, simplemente utilizamos las estadísticas de la tabla 2 como parámetros de recalibración.",
        "Aunque usar las estadísticas directamente puede que no sea la mejor opción, no discutimos cómo obtener automáticamente los mejores parámetros.",
        "Intentaremos utilizar técnicas de aprendizaje automático para obtener los mejores parámetros en el trabajo futuro.",
        "En el trabajo, utilizamos BoosTexter [20] para clasificar todas las historias en uno de los 11 ROIs.",
        "BoosTexter es un programa de aprendizaje automático basado en boosting, que crea una serie de reglas simples para construir un clasificador de datos de texto o atributos-valor.",
        "Utilizamos el peso del término generado utilizando el modelo TF-IDF como característica para la clasificación de historias.",
        "Entrenamos el modelo con las 12000 historias en inglés evaluadas en TDT2, y clasificamos el resto de las historias en TDT2 y todas las historias en TDT3.",
        "Los resultados de clasificación se utilizan para la reponderación de términos en la fórmula (11).",
        "Dado que las etiquetas de clase de las historias de tema apagado no se proporcionan en los conjuntos de datos de TDT, no podemos proporcionar la precisión de clasificación aquí.",
        "Por lo tanto, no discutimos los efectos de la precisión de la clasificación en el rendimiento de NED en el artículo. 6.",
        "CONFIGURACIÓN EXPERIMENTAL 6.1 Conjuntos de datos Utilizamos dos conjuntos de datos de LDC [18], TDT2 y TDT3, para nuestros experimentos.",
        "TDT2 contiene noticias desde enero hasta junio de 1998.",
        "Contiene alrededor de 54,000 historias de fuentes como ABC, Associated Press, CNN, New York Times, Public Radio International, Voice of America, etc.",
        "Solo se consideraron historias en inglés en la colección.",
        "TDT3 contiene aproximadamente 31,000 historias en inglés recopiladas de octubre a diciembre de 1998.",
        "Además de las fuentes utilizadas en TDT2, también contiene historias de las transmisiones de televisión de NBC y MSNBC.",
        "Utilizamos versiones transcritas de las emisiones de televisión y radio además de noticias textuales.",
        "El conjunto de datos TDT2 está etiquetado con alrededor de 100 temas, y aproximadamente 12,000 historias en inglés pertenecen al menos a uno de estos temas.",
        "El conjunto de datos TDT3 está etiquetado con alrededor de 120 temas, y aproximadamente 8000 historias en inglés pertenecen al menos a uno de estos temas.",
        "Todos los temas están clasificados en 11 Reglas de Interpretación: (1) Elecciones, (2) Escándalos/Audiencias, (3) Casos Legales/Criminales, (4) Desastres Naturales, (5) Accidentes, (6) Violencia o Guerra en Curso, (7) Noticias de Ciencia y Descubrimientos, (8) Finanzas, (9) Nueva Ley, (10) Noticias Deportivas, (11) Varios.",
        "Noticias. 6.2 Métrica de evaluación TDT utiliza una función de costo CDet que combina las probabilidades de perder una nueva historia y una falsa alarma [19]: * * * *Det Miss Miss Target FA FA NontargetC C P P C P P= + (12) Tabla 2.",
        "Correlación promedio entre tipos de términos y clases de noticias donde CMiss significa el costo de perder una nueva historia, PMiss significa la probabilidad de perder una nueva historia, y PTarget significa la probabilidad de ver una nueva historia en los datos; CFA significa el costo de una falsa alarma, PFA significa la probabilidad de una falsa alarma, y PNontarget significa la probabilidad de ver una historia antigua.",
        "El costo CDet se normaliza de tal manera que un sistema perfecto obtiene una puntuación de 0 y un sistema trivial, que es el mejor de marcar todas las historias como nuevas o antiguas, obtiene una puntuación de 1: ( ( * , * ) ) Det Det Miss Target FA Nontarget C Norm C min C P C P = (13) El sistema de detección de eventos nuevos proporciona dos salidas para cada historia.",
        "La primera parte es sí o no, indicando si la historia desencadena un nuevo evento o no.",
        "La segunda parte es una puntuación que indica la confianza de la primera decisión.",
        "Los puntajes de confianza se pueden utilizar para trazar la curva DET, es decir, curvas que representan la probabilidad de falsa alarma frente a la probabilidad de omisión.",
        "El costo normalizado mínimo se puede determinar si se elige el umbral óptimo en la puntuación.",
        "RESULTADOS EXPERIMENTALES 7.1 Resultados principales Para probar los enfoques propuestos en el modelo, implementamos y probamos cinco sistemas: Sistema-1: este sistema se utiliza como referencia.",
        "Se implementa basándose en el modelo básico descrito en la sección 3, es decir, utilizando el modelo TF-IDF incremental para generar pesos de términos y utilizando la distancia de Hellinger para calcular la similitud entre documentos.",
        "La normalización del puntaje de similitud también se emplea [8].",
        "Se utiliza el procedimiento de detección S-S.",
        "Sistema-2: este sistema es igual que el sistema-1 excepto que se utiliza el procedimiento de detección S-C.",
        "Sistema-3: este sistema es igual que el sistema-1 excepto que utiliza el nuevo procedimiento de detección que se basa en un árbol de indexación.",
        "Sistema-4: implementado basado en el enfoque presentado en la sección 5.1, es decir, los términos se vuelven a ponderar según la distancia entre las distribuciones de términos en un grupo y todas las historias.",
        "Se utiliza el nuevo procedimiento de detección.",
        "Sistema-5: implementado basado en el enfoque presentado en la sección 5.2, es decir, los términos de diferentes tipos son reponderados según la clase de noticias utilizando parámetros entrenados.",
        "El nuevo procedimiento de detección se utiliza.",
        "Los siguientes son algunos otros sistemas NED: Sistema-6: [21] para cada par de historias, calcula tres valores de similitud para entidad nombrada, entidad no nombrada y todos los términos respectivamente.",
        "Y emplear Máquina de Vectores de Soporte para predecir nuevo o antiguo utilizando los valores de similitud como características.",
        "Sistema-7: [8] extendió un modelo básico incremental de TF-IDF para incluir modelos específicos de origen, normalización de puntuaciones de similitud basada en promedios específicos de documentos, normalización de puntuaciones de similitud basada en promedios específicos de pares de fuentes, etc.",
        "Sistema-8: [13] dividió la representación del documento en dos partes: entidades nombradas y entidades no nombradas, y eligió una parte efectiva para cada clase de noticias.",
        "La tabla 3 y la tabla 4 muestran los costos normalizados ponderados por tema y los tiempos de comparación en los conjuntos de datos TDT2 y TDT3 respectivamente.",
        "Dado que no se contaba con un conjunto de datos de validación para ajustar el umbral θ nuevo en los experimentos en TDT2, solo informamos los costos normalizados mínimos para nuestros sistemas en la tabla 3.",
        "El sistema-5 supera a todos los demás sistemas, incluido el sistema-6, y realiza solo 2.78e+8 veces la comparación de tiempos en el procedimiento de detección, lo que representa solo el 13.4% del sistema-1.",
        "Tabla 3.",
        "Los resultados de NED en los sistemas TDT2 Min Norm(CDet) tiempos de comparación son: Sistema-1 0.5749 2.08e+9, Sistema-2 0.6673 3.77e+8, Sistema-3 0.5765 2.81e+8, Sistema-4 0.5431 2.99e+8, Sistema-5 0.5089 2.78e+8, Sistema-6 0.5300. Al evaluar en los costos normalizados en TDT3, utilizamos los umbrales óptimos obtenidos del conjunto de datos de TDT2 para todos los sistemas.",
        "El sistema-2 reduce los tiempos de comparación a 1.29e+9, que es solo el 18.3% del sistema-1, pero al mismo tiempo también obtiene un costo normalizado mínimo deteriorado que es 0.0499 más alto que el sistema-1.",
        "El Sistema-3 utiliza el nuevo procedimiento de detección basado en el índice de noticias en árbol.",
        "Requiere incluso menos tiempo de comparación que el sistema-2.",
        "Esto se debe a que las comparaciones de historias suelen arrojar mayores similitudes que las de grupos de historias, por lo que las historias tienden a combinarse en el sistema-3.",
        "Y el sistema-3 es básicamente equivalente al sistema-1 en los resultados de precisión.",
        "El System-4 ajusta los pesos de los términos basándose en la distancia de las distribuciones de términos entre todo el corpus y el conjunto de historias del clúster, lo que resulta en una mejora significativa de 0.0468 en comparación con el system-1.",
        "El mejor sistema (sistema-5) tiene un costo normalizado mínimo de 0.5012, lo cual es 0.0797 mejor que el sistema-1, y también mejor que cualquier otro resultado previamente reportado para este conjunto de datos [8, 13].",
        "Además, el sistema-5 solo necesita 1.05e+8 veces de comparación, lo que equivale al 14.9% del sistema-1.",
        "Tabla 4.",
        "Los resultados de NED en los sistemas TDT3 Norm(CDet) Norma mínima(CDet) Tiempos de comparación Sistema-1 0.6159 0.5809 7.04e+8 Sistema-2① 0.6493 0.6308 1.29e+8 Sistema-3② 0.6197 0.5868 1.03e+8 Sistema-4② 0.5601 0.5341 1.03e+8 Sistema-5② 0.5413 0.5012 1.05e+8 Sistema-7 -- 0.5783 -Sistema-8 -- 0.5229 -① θ nuevo=0.13 ② θ inicial=0.13, λ =3, δ =0.15 La Figura 5 muestra las cinco curvas DET para nuestros sistemas en el conjunto de datos TDT3.",
        "El Sistema-5 logra el costo mínimo con una tasa de falsas alarmas de 0.0157 y una tasa de omisión de 0.4310.",
        "Podemos observar que System4 y System-5 obtienen una menor probabilidad de error de omisión en regiones de baja probabilidad de falsa alarma.",
        "La hipótesis es que se transfiere más valor de peso a los términos clave de los temas que a los términos no clave.",
        "La puntuación de similitud entre dos historias pertenecientes a diferentes temas es más baja que antes, porque los términos superpuestos suelen no ser términos clave de sus temas. 7.2 Selección de parámetros para la detección de árboles de indexación. La Figura 3 muestra los costos normalizados mínimos obtenidos por el sistema-3 en TDT3 utilizando diferentes parámetros.",
        "El parámetro de inicio θ se prueba en seis valores que van desde 0.03 hasta 0.18.",
        "Y el parámetro λ se prueba en cuatro valores: 1, 2, 3 y 4.",
        "Podemos ver que, cuando θ inicial se establece en 0.12, que es el más cercano a θ nuevo, los costos son más bajos que los demás.",
        "Esto es fácil de explicar, porque cuando se agrupan historias pertenecientes al mismo tema en un conjunto, es más razonable que el conjunto represente las historias en él.",
        "Cuando el parámetro λ se establece en 3 o 4, los costos son mejores que en otros casos, pero no hay mucha diferencia entre 3 y 4. 0 0.05 0.1 0.15 0.2 1 2 3 4 0.5 0.6 0.7 0.8 0.9 1 θ-initλ MinCost 0.6 0.65 0.7 0.75 0.8 0.85 0.9 Figura 3.",
        "Costo mínimo en TDT3 (δ =0.15) 0 0.05 0.1 0.15 0.2 1 2 3 4 0 0.5 1 1.5 2 2.5 x 10 8 θ-inicio λ Comparando tiempos 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 8 Figura 4.",
        "Comparando los tiempos en TDT3 (δ =0.15), la Figura 4 muestra los tiempos de comparación utilizados por el sistema-3 en TDT3 con los mismos parámetros que la Figura 3.",
        "Los tiempos de comparación dependen fuertemente de θ init.",
        "Debido a que cuanto mayor sea θ init, menos historias se combinan juntas, más veces de comparación se necesitan para la decisión de un nuevo evento.",
        "Por lo tanto, usamos θ init = 0.13, λ = 3, δ = 0.15 para los sistemas 3, 4 y 5.",
        "En esta configuración de parámetros, podemos obtener tanto costos normalizados mínimos bajos como menos tiempos de comparación.",
        "CONCLUSIÓN Hemos propuesto un procedimiento de detección basado en un árbol de indexación de noticias en nuestro modelo.",
        "Reduce los tiempos de comparación a aproximadamente una séptima parte del método tradicional sin afectar la precisión de NED.",
        "También hemos presentado dos extensiones al modelo básico TF-IDF.",
        "La primera extensión se realiza ajustando los pesos de los términos basados en las distribuciones de términos entre todo el corpus y un conjunto de historias de un clúster.",
        "Y la segunda extensión al modelo básico TF-IDF es un mejor uso de los tipos de términos (tipos de entidades nombradas y partes de la velocidad) de acuerdo con las categorías de noticias.",
        "Nuestros resultados experimentales en los conjuntos de datos TDT2 y TDT3 muestran que ambas extensiones contribuyen significativamente a la mejora en la precisión.",
        "No consideramos la información de tiempo de noticias como una pista para la tarea de NED, ya que la mayoría de los temas duran mucho tiempo y los conjuntos de datos de TDT solo abarcan un período relativamente corto (no más de 6 meses).",
        "Para el trabajo futuro, queremos recopilar un conjunto de noticias que abarque un período más largo de internet e integrar información temporal en la tarea de NED.",
        "Dado que el tema es un grupo de noticias relativamente grueso, también queremos refinar la granularidad del grupo a nivel de evento e identificar diferentes eventos y sus relaciones dentro de un tema.",
        "Agradecimientos: Este trabajo ha sido apoyado por la Fundación Nacional de Ciencias Naturales de China bajo la subvención No. 90604025.",
        "Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son del autor(es) y no necesariamente reflejan las del patrocinador.",
        "REFERENCIAS [1] http://www.nist.gov/speech/tests/tdt/index.htm [2] En Detección y Seguimiento de Temas.",
        "Organización de la información basada en eventos.",
        "Kluwer Academic Publishers, 2002. .01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90 1 2 5 10 20 40 60 80 90 Probabilidad de Falsa Alarma (en %) Probabilidad de Omisión (en %) Curva Ponderada por Tema del SISTEMA1 Norma Mínima (Costo) del SISTEMA1 Curva Ponderada por Tema del SISTEMA2 Norma Mínima (Costo) del SISTEMA2 Curva Ponderada por Tema del SISTEMA3 Norma Mínima (Costo) del SISTEMA3 Curva Ponderada por Tema del SISTEMA4 Norma Mínima (Costo) del SISTEMA4 Curva Ponderada por Tema del SISTEMA5 Norma Mínima (Costo) del SISTEMA5 Rendimiento Aleatorio Figura 5.",
        "Curvas DET en TDT3 [3] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B.T.",
        "Archibald y X. Liu.",
        "Enfoques de aprendizaje para detectar y rastrear eventos de noticias.",
        "En el número especial de IEEE Intelligent Systems sobre Aplicaciones de Recuperación de Información Inteligente, volumen 14 (4), 1999, páginas 32-43. [4] Y. Yang, T. Pierce y J. Carbonell.",
        "Un estudio sobre la detección de eventos retrospectivos y en línea.",
        "En Actas de SIGIR-98, Melbourne, Australia, 1998, 28-36. [5] J. Allan, V. Lavrenko, D. Malin y R. Swan.",
        "Detecciones, límites y líneas de tiempo: Umass y tdt-3.",
        "En Actas del Taller de Detección y Seguimiento de Temas (TDT-3), Viena, VA, 2000, 167-174. [6] R. Papka y J. Allan.",
        "Detección de nuevos eventos en línea utilizando agrupamiento de un solo paso TÍTULO2:.",
        "Informe técnico UM-CS1998-021, 1998. [7] W. Lam, H. Meng, K. Wong y J.",
        "I'm sorry, but \"Yen\" is not a sentence. Can you please provide a sentence for me to translate into Spanish?",
        "Utilizando el Análisis Contextual para la Detección de Eventos de Noticias.",
        "Revista Internacional de Sistemas Inteligentes, 2001, 525-546. [8] B. Thorsten, C. Francine y F. Ayman.",
        "Un sistema para la detección de nuevos eventos.",
        "En Actas de la 26ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU.",
        "ACM Press. 2003, 330-337. [9] S. Nicola y C. Joe.",
        "Combinando clasificadores semánticos y sintácticos de documentos para mejorar la detección de primeras noticias.",
        "En Actas de la 24ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU.",
        "ACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell, and C. Jin. \n\nACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell y C. Jin.",
        "Detección de novedades condicionada al tema.",
        "En Actas de la 8ª Conferencia Internacional de ACM SIGKDD, ACM Press. 2002, 688-693. [11] M. Juha, A.M. Helena y S. Marko.",
        "Aplicando Clases Semánticas en la Detección y Seguimiento de Eventos.",
        "En Actas de la Conferencia Internacional sobre Procesamiento del Lenguaje Natural (ICON 2002), 2002, páginas 175-183. [12] M. Juha, A.M. Helena y S. Marko.",
        "Semántica simple en la detección y seguimiento de temas.",
        "Recuperación de información, 7(3-4): 2004, 347-368. [13] K. Giridhar y J. Allan.",
        "Clasificación de texto y entidades nombradas para la detección de nuevos eventos.",
        "En Actas de la 27ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU.",
        "ACM Press. 2004, 297-304. [14] J. P. Callan, W. B. Croft, y S. M. Harding.",
        "El Sistema de Recuperación INQUERY.",
        "En Actas de DEXA-92, 3ra Conferencia Internacional sobre Aplicaciones de Bases de Datos y Sistemas Expertos, 1992, 78-83. [15] R. Krovetz.",
        "Viendo la morfología como un proceso de inferencia.",
        "En Actas de ACM SIGIR93, 1993, 61-81. [16] Y. Yang y J. Pedersen.",
        "Un estudio comparativo sobre la selección de características en la categorización de textos.",
        "En J. D. H. Fisher, editor, La Decimocuarta Conferencia Internacional sobre Aprendizaje Automático (ICML97), Morgan Kaufmann, 1997, 412-420. [17] T. M. Cover y J. A.",
        "I'm sorry, but \"Thomas\" is not a sentence. Can you provide more context or a complete sentence for me to translate into Spanish?",
        "Elementos de la teoría de la información.",
        "Wiley. 1991. [18] El consorcio de datos lingüísticos, http://www.ldc,upenn.edu/. [19] La definición de la tarea TDT 2001 y el plan de evaluación, http://www.nist.gov/speech/tests/tdt/tdt2001/evalplan.htm. [20] R. E. Schapire y Y.",
        "Cantante.",
        "Boostexter: Un sistema basado en Boosting para la categorización de texto.",
        "En Machine Learning 39(2/3):1, Kluwer Academic Publishers, 2000, 35-168. [21] K. Giridhar y J. Allan. 2005.",
        "Utilizando nombres y temas para la detección de nuevos eventos.",
        "En Actas de la Conferencia de Tecnología Humana y la Conferencia sobre Métodos Empíricos en Lenguaje Natural, Vancouver, 2005, 121-128"
    ],
    "error_count": 8,
    "keys": {
        "new event detection": {
            "translated_key": "Detección de Eventos Nuevos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "<br>new event detection</br> Based on Indexing-tree and Named Entity Zhang Kuo Tsinghua University Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Tsinghua University Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Tsinghua University Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn ABSTRACT <br>new event detection</br> (NED) aims at detecting from one or multiple streams of news stories that which one is reported on a new event (i.e.",
                "not reported previously).",
                "With the overwhelming volume of news available today, there is an increasing need for a NED system which is able to detect new events more efficiently and accurately.",
                "In this paper we propose a new NED model to speed up the NED task by using news indexing-tree dynamically.",
                "Moreover, based on the observation that terms of different types have different effects for NED task, two term reweighting approaches are proposed to improve NED accuracy.",
                "In the first approach, we propose to adjust term weights dynamically based on previous story clusters and in the second approach, we propose to employ statistics on training data to learn the named entity reweighting model for each class of stories.",
                "Experimental results on two Linguistic Data Consortium (LDC) datasets TDT2 and TDT3 show that the proposed model can improve both efficiency and accuracy of NED task significantly, compared to the baseline system and other existing systems.",
                "Categories and Subject Descriptors H.3.3 [Information Systems]: Information Search and Retrieval; H.4.2 [Information Systems Applications]: Types of Systemsdecision support.",
                "General Terms Algorithms, Performance, Experimentation 1.",
                "INTRODUCTION Topic Detection and Tracking (TDT) program aims to develop techniques which can effectively organize, search and structure news text materials from a variety of newswire and broadcast media [1].",
                "<br>new event detection</br> (NED) is one of the five tasks in TDT.",
                "It is the task of online identification of the earliest report for each topic as soon as that report arrives in the sequence of documents.",
                "A Topic is defined as a seminal event or activity, along with directly related events and activities [2].",
                "An Event is defined as something (non-trivial) happening in a certain place at a certain time [3].",
                "For instance, when a bomb explodes in a building, the exploding is the seminal event that triggers the topic, and other stories on the same topic would be those discussing salvaging efforts, the search for perpetrators, arrests and trial and so on.",
                "Useful news information is usually buried in a mass of data generated everyday.",
                "Therefore, NED systems are very useful for people who need to detect novel information from real-time news stream.",
                "These real-life needs often occur in domains like financial markets, news analysis, and intelligence gathering.",
                "In most of state-of-the-art (currently) NED systems, each news story on hand is compared to all the previous received stories.",
                "If all the similarities between them do not exceed a threshold, then the story triggers a new event.",
                "They are usually in the form of cosine similarity or Hellinger similarity metric.",
                "The core problem of NED is to identify whether two stories are on the same topic.",
                "Obviously, these systems cannot take advantage of topic information.",
                "Further more, it is not acceptable in real applications because of the large amount of computation required in the NED process.",
                "Other systems organize previous stories into clusters (each cluster corresponds to a topic), and new story is compared to the previous clusters instead of stories.",
                "This manner can reduce comparing times significantly.",
                "Nevertheless, it has been proved that this manner is less accurate [4, 5].",
                "This is because sometimes stories within a topic drift far away from each other, which could lead low similarity between a story and its topic.",
                "On the other hand, some proposed NED systems tried to improve accuracy by making better use of named entities [10, 11, 12, 13].",
                "However, none of the systems have considered that terms of different types (e.g.",
                "Noun, Verb or Person name) have different effects for different classes of stories in determining whether two stories are on the same topic.",
                "For example, the names of election candidates (Person name) are very important for stories of election class; the locations (Location name) where accidents happened are important for stories of accidents class.",
                "So, in NED, there still exist following three problems to be investigated: (1) How to speed up the detection procedure while do not decrease the detection accuracy? (2) How to make good use of cluster (topic) information to improve accuracy? (3) How to obtain better news story representation by better understanding of named entities.",
                "Driven by these problems, we have proposed three approaches in this paper. (1)To make the detection procedure faster, we propose a new NED procedure based on news indexing-tree created dynamically.",
                "Story indexing-tree is created by assembling similar stories together to form news clusters in different hierarchies according to their values of similarity.",
                "Comparisons between current story and previous clusters could help find the most similar story in less comparing times.",
                "The new procedure can reduce the amount of comparing times without hurting accuracy. (2)We use the clusters of the first floor in the indexing-tree as news topics, in which term weights are adjusted dynamically according to term distribution in the clusters.",
                "In this approach, cluster (topic) information is used properly, so the problem of theme decentralization is avoided. (3)Based on observations on the statistics obtained from training data, we found that terms of different types (e.g.",
                "Noun and Verb) have different effects for different classes of stories in determining whether two stories are on the same topic.",
                "And we propose to use statistics to optimize the weights of the terms of different types in a story according to the news class that the story belongs to.",
                "On TDT3 dataset, the new NED model just uses 14.9% comparing times of the basic model, while its minimum normalized cost is 0.5012, which is 0.0797 better than the basic model, and also better than any other results previously reported for this dataset [8, 13].",
                "The rest of the paper is organized as follows.",
                "We start off this paper by summarizing the previous work in NED in section 2.",
                "Section 3 presents the basic model for NED that most current systems use.",
                "Section 4 describes our new detection procedure based on news indexing-tree.",
                "In section 5, two term reweighting methods are proposed to improve NED accuracy.",
                "Section 6 gives our experimental data and evaluation metrics.",
                "We finally wrap up with the experimental results in Section 7, and the conclusions and future work in Section 8. 2.",
                "RELATED WORK Papka et al. proposed Single-Pass clustering on NED [6].",
                "When a new story was encountered, it was processed immediately to extract term features and a query representation of the storys content is built up.",
                "Then it was compared with all the previous queries.",
                "If the document did not trigger any queries by exceeding a threshold, it was marked as a new event.",
                "Lam et al build up previous query representations of story clusters, each of which corresponds to a topic [7].",
                "In this manner comparisons happen between stories and clusters.",
                "Recent years, most work focus on proposing better methods on comparison of stories and document representation.",
                "Brants et al. [8] extended a basic incremental TF-IDF model to include sourcespecific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, term reweighting based on inverse event frequencies, and segmentation of documents.",
                "Good improvements on TDT bench-marks were shown.",
                "Stokes et al. [9] utilized a combination of evidence from two distinct representations of a documents content.",
                "One of the representations was the usual free text vector, the other made use of lexical chains (created using WordNet) to build another term vector.",
                "Then the two representations are combined in a linear fashion.",
                "A marginal increase in effectiveness was achieved when the combined representation was used.",
                "Some efforts have been done on how to utilize named entities to improve NED.",
                "Yang et al. gave location named entities four times weight than other terms and named entities [10].",
                "DOREMI research group combined semantic similarities of person names, location names and time together with textual similarity [11][12].",
                "UMass [13] research group split document representation into two parts: named entities and non-named entities.",
                "And it was found that some classes of news could achieve better performance using named entity representation, while some other classes of news could achieve better performance using non-named entity representation.",
                "Both [10] and [13] used text categorization technique to classify news stories in advance.",
                "In [13] news stories are classified automatically at first, and then test sensitivities of names and non-name terms for NED for each class.",
                "In [10] frequent terms for each class are removed from document representation.",
                "For example, word election does not help identify different elections.",
                "In their work, effectiveness of different kinds of names (or terms with different POS) for NED in different news classes are not investigated.",
                "We use statistical analysis to reveal the fact and use it to improve NED performance. 3.",
                "BASIC MODEL In this section, we present the basic <br>new event detection</br> model which is similar to what most current systems apply.",
                "Then, we propose our new model by extending the basic model.",
                "<br>new event detection</br> systems use news story stream as input, in which stories are strictly time-ordered.",
                "Only previously received stories are available when dealing with current story.",
                "The output is a decision for whether the current story is on a new event or not and the confidence of the decision.",
                "Usually, a NED model consists of three parts: story representation, similarity calculation and detection procedure. 3.1 Story Representation Preprocessing is needed before generating story representation.",
                "For preprocessing, we tokenize words, recognize abbreviations, normalize abbreviations, add part-of-speech tags, remove stopwords included in the stop list used in InQuery [14], replace words with their stems using K-stem algorithm[15], and then generate word vector for each news story.",
                "We use incremental TF-IDF model for term weight calculation [4].",
                "In a TF-IDF model, term frequency in a news document is weighted by the inverse document frequency, which is generated from training corpus.",
                "When a new term occurs in testing process, there are two solutions: simply ignore the new term or set df of the term as a small const (e.g. df = 1).",
                "The new term receives too low weight in the first solution (0) and too high weight in the second solution.",
                "In incremental TF-IDF model, document frequencies are updated dynamically in each time step t: 1( ) ( ) ( )t t D tdf w df w df w−= + (1) where Dt represents news story set received in time t, and dfDt(w) means the number of documents that term w occurs in, and dft(w) means the total number of documents that term w occurs in before time t. In this work, each time window includes 50 news stories.",
                "Thus, each story d received in t is represented as follows: 1 2{ ( , , ), ( , , ),..., ( , , )}nd weight d t w weight d t w weight d t w→ where n means the number of distinct terms in story d, and ( , , )weight d t w means the weight of term w in story d at time t: log( ( , ) 1) log(( 1) /( ( ) 0.5)) ( , , ) log( ( , ) 1) log(( 1) /( ( ) 0.5)) t t t t w d tf d w N df w weight d t w tf d w N df w ∈ + + + = + + +∑ (2) where Nt means the total number of news stories before time t, and tf(d,w) means how many times term w occurs in news story d. 3.2 Similarity Calculation We use Hellinger distance for the calculation of similarity between two stories, for two stories d and d at time t, their similarity is defined as follows: , ( , , ) ( , , ) * ( , , ) w d d sim d d t weight d t w weight d t w ∈ = ∑ (3) 3.3 Detection Procedure For each story d received in time step t, the value ( ) ( ) ( ) ( ( , , )) time d time d n d max sim d d t < = (4) is a score used to determine whether d is a story about a new topic and at the same time is an indication of the confidence in our decision [8]. time(d) means the publication time of story d. If the score exceeds the thresholdθ new, then there exists a sufficiently similar document, thus d is a old story, otherwise, there is no sufficiently similar previous document, thus d is an new story. 4.",
                "New NED Procedure Traditional NED systems can be classified into two main types on the aspect of detection procedure: (1) S-S type, in which the story on hand is compared to each story received previously, and use the highest similarity to determine whether current story is about a new event; (2) S-C type, in which the story on hand is compared to all previous clusters each of which representing a topic, and the highest similarity is used for final decision for current story.",
                "If the highest similarity exceeds thresholdθ new, then it is an old story, and put it into the most similar cluster; otherwise it is a new story and create a new cluster.",
                "Previous work show that the first manner is more accurate than the second one [4][5].",
                "Since sometimes stories within a topic drift far away from each other, a story may have very low similarity with its topic.",
                "So using similarities between stories for determining new story is better than using similarities between story and clusters.",
                "Nevertheless, the first manner needs much more comparing times which means the first manner is low efficient.",
                "We propose a new detection procedure which uses comparisons with previous clusters to help find the most similar story in less comparing times, and the final new event decision is made according to the most similar story.",
                "Therefore, we can get both the accuracy of S-S type methods and the efficiency of S-C type methods.",
                "The new procedure creates a news indexing-tree dynamically, in which similar stories are put together to form a hierarchy of clusters.",
                "We index similar stories together by their common ancestor (a cluster node).",
                "Dissimilar stories are indexed in different clusters.",
                "When a story is coming, we use comparisons between the current story and previous hierarchical clusters to help find the most similar story which is useful for new event decision.",
                "After the new event decision is made, the current story is inserted to the indexing-tree for the following detection.",
                "The news indexing-tree is defined formally as follows: S-Tree = {r, NC , NS , E} where r is the root of S-Tree, NC is the set of all cluster nodes, NS is the set of all story nodes, and E is the set of all edges in S-Tree.",
                "We define a set of constraints for a S-Tree: ⅰ . , is an non-terminal node in the treeC i i N i∀ ∈ → ⅱ . , is a terminal node in the treeS i i N i∀ ∈ → ⅲ . , out degree of is at least 2C i i N i∀ ∈ → ⅳ . , is represented as the centroid of its desendantsC i i iN∀ ∈ → For a news story di, the comparison procedure and inserting procedure based on indexing-tree are defined as follows.",
                "An example is shown by Figure 1 and Figure 2.",
                "Figure 1.",
                "Comparison procedure Figure 2.",
                "Inserting procedure Comparison procedure: Step 1: compare di to all the direct child nodes of r and select λ nodes with highest similarities, e.g., C1 2 and C1 3 in Figure 1.",
                "Step 2: for each selected node in the last step, e.g.",
                "C1 2, compare di to all its direct child nodes, and select λ nodes with highest similarities, e.g.",
                "C2 2 and d8.",
                "Repeat step 2 for all non-terminal nodes.",
                "Step 3: record the terminal node with the highest similarty to di, e.g. s5, and the similarity value (0.20).",
                "Inserting di to the S-tree with r as root: Find the node n which is direct child of r in the path from r to the terminal node with highest similarity s, e.g.",
                "C1 2.",
                "If s is smaller than θ init+(h-1)δ , then add di to the tree as a direct child of r. Otherwise, if n is a terminal node, then create a cluster node instead of n, and add both n and di as its direct children; if n is an non-terminal node, then repeat this procedure and insert di to the sub-tree with n as root recursively.",
                "Here h is the length between n and the root of S-tree.",
                "The more the stories in a cluster similar to each other, the better the cluster represents the stories in it.",
                "Hence we add no constraints on the maximum of trees height and degree of a node.",
                "Therefore, we cannot give the complexity of this indexing-tree based procedure.",
                "But we will give the number of comparing times needed by the new procedure in our experiments in section7. 5.",
                "Term Reweighting Methods In this section, two term reweighting methods are proposed to improve NED accuracy.",
                "In the first method, a new way is explored for better using of cluster (topic) information.",
                "The second one finds a better way to make use of named entities based on news classification. 5.1 Term Reweighting Based on Distribution Distance TF-IDF is the most prevalent model used in information retrieval systems.",
                "The basic idea is that the fewer documents a term appears in, the more important the term is in discrimination of documents (relevant or not relevant to a query containing the term).",
                "Nevertheless, in TDT domain, we need to discriminate documents with regard to topics rather than queries.",
                "Intuitively, using cluster (topic) vectors to compare with subsequent news stories should outperform using story vectors.",
                "Unfortunately, the experimental results do not support this intuition [4][5].",
                "Based on observation on data, we find the reason is that a news topic usually contains many directly or indirectly related events, while they all have their own sub-subjects which are usually different with each other.",
                "Take the topic described in section 1 as an example, events like the explosion and salvage have very low similarities with events about criminal trial, therefore stories about trial would have low similarity with the topic vector built on its previous events.",
                "This section focuses on how to effectively make use of topic information and at the same time avoid the problem of content decentralization.",
                "At first, we classify terms into 5 classes to help analysis the needs of the modified model: Term class A: terms that occur frequently in the whole corpus, e.g., year and people.",
                "Terms of this class should be given low weights because they do not help much for topic discrimination.",
                "Term class B: terms that occur frequently within a news category, e.g., election, storm.",
                "They are useful to distinguish two stories in different news categories.",
                "However, they cannot provide information to determine whether two stories are on the same or different topics.",
                "In another words, term election and term storm are not helpful in differentiate two election campaigns and two storm disasters.",
                "Therefore, terms of this class should be assigned lower weights.",
                "Term class C: terms that occur frequently in a topic, and infrequently in other topics, e.g., the name of a crash plane, the name of a specific hurricane.",
                "News stories that belong to different topics rarely have overlap terms in this class.",
                "The more frequently a term appears in a topic, the more important the term is for a story belonging to the topic, therefore the term should be set higher weight.",
                "Term class D: terms that appear in a topic exclusively, but not frequently.",
                "For example, the name of a fireman who did very well in a salvage action, which may appears in only two or three stories but never appeared in other topics.",
                "Terms of this type should receive more weights than in TF-IDF model.",
                "However, since they are not popular in the topic, it is not appropriate to give them too high weights.",
                "Term class E: terms with low document frequency, and appear in different topics.",
                "Terms of this class should receive lower weights.",
                "Now we analyze whether TF-IDF model can give proper weights to the five classes of terms.",
                "Obviously, terms of class A are lowly weighted in TF-IDF model, which is conformable with the requirement described above.",
                "In TF-IDF model, terms of class B are highly dependant with the number of stories in a news class.",
                "TF-IDF model cannot provide low weights if the story containing the term belongs to a relative small news class.",
                "For a term of class C, the more frequently it appears in a topic, the less weight TFIDF model gives to it.",
                "This strongly conflicts with the requirement of terms in class C. For terms of class D, TF-IDF model gives them high weights correctly.",
                "But for terms of class E, TF-IDF model gives high weights to them which are not conformable with the requirement of low weights.",
                "To sum up, terms of class B, C, E cannot be properly weighted in TF-IDF model.",
                "So, we propose a modified model to resolve this problem.",
                "When θ init andθ new are set closely, we assume that most of the stories in a first-level cluster (a direct child node of root node) are on the same topic.",
                "Therefore, we make use of a first-level cluster to capture term distribution (df for all the terms within the cluster) within the topic dynamically.",
                "KL divergence of term distribution in a first-level cluster and the whole story set is used to adjust term weights:  ( , , ) * (1 * ( || )) ( , , ) ( , , ) * (1 * ( || )) cw tw cw tw w d D weight d t w KL P P weight d t w weight d t w KL P P γ γ ∈ + = +∑ (5) where ( ) ( ) ( ) ( ) 1,cw cw c c c c df w df w p y p y N N = = − (6) ( ) ( ) ( ) ( ) 1,t t tw tw t t df w df w p y p y N N = = − (7) where dfc(w) is the number of documents containing term w within cluster C, and Nc is the number of documents in cluster C, and Nt is the total number of documents that arrive before time step t. γ is a const parameter, now is manually set 3.",
                "KL divergence is defined as follows [17]: ( ) ( || ) ( ) log ( )x p x KL P Q p x q x = ∑ (8) The basic idea is: for a story in a topic, the more a term occurs within the topic, and the less it occurs in other topics, it should be assigned higher weights.",
                "Obviously, modified model can meet all the requirements of the five term classes listed above. 5.2 Term Reweighting Based on Term Type and Story Class Previous work found that some classes of news stories could achieve good improvements by giving extra weight to named entities.",
                "But we find that terms of different types should be given different amount of extra weight for different classes of news stories.",
                "We use open-NLP1 to recognize named entity types and part-ofspeech tags for terms that appear in news stories.",
                "Named entity types include person name, organization name, location name, date, time, money and percentage, and five POSs are selected: none (NN), verb (VB), adjective (JJ), adverb (RB) and cardinal number (CD).",
                "Statistical analysis shows topic-level discriminative terms types for different classes of stories.",
                "For the sake of convenience, named entity type and part-of-speech tags are uniformly called term type in subsequent sections.",
                "Determining whether two stories are about the same topic is a basic component for NED task.",
                "So at first we use 2 χ statistic to compute correlations between terms and topics.",
                "For a term t and a topic T, a contingence table is derived: Table 1.",
                "A 2×2 Contingence Table Doc Number belong to topic T not belong to topic T include t A B not include t C D The 2 χ statistic for a specific term t with respect to topic T is defined to be [16]: 2 2 ( , ) ( ) * ( ) ( ) * ( ) * ( ) * ( ) w T A B C D AD CB A C B D A B C D χ = + + + − + + + + (9) News topics for the TDT task are further classified into 11 rules of interpretations (ROIs) 2 .",
                "The ROI can be seen as a higher level class of stories.",
                "The average correlation between a term type and a topic ROI is computed as: 2 avg 2 ( , )( ( , ) )k m m km kT R w P w TP R p w T R P χ χ ∈ ∈ ∑ ∑（ , ）= 1 1 k=1…K, m=1…M (10) where K is the number of term types (set 12 constantly in the paper).",
                "M is the number news classes (ROIs, set 11 in the paper).",
                "Pk represents the set of all terms of type k, and Rm represents the set of all topics of class m, p(t,T) means the probability that t occurs in topic T. Because of limitation of space, only parts of the term types (9 term types) and parts of news classes (8 classes) are listed in table 2 with the average correlation values between them.",
                "The statistics is derived from labeled data in TDT2 corpus. (Results in table 2 are already normalized for convenience in comparison.)",
                "The statistics in table 2 indicates the usefulness of different term types in topic discrimination with respect to different news classes.",
                "We can see that, location name is the most useful term type for three news classes: Natural Disasters, Violence or War, Finances.",
                "And for three other categories Elections, Legal/Criminal Cases, Science and Discovery, person name is the most discriminative term type.",
                "For Scandals/Hearings, date is the most important information for topic discrimination.",
                "In addition, Legal/Criminal Cases and Finance topics have higher correlation with money terms, while Science and Discovery have higher correlation with percentage terms.",
                "Non-name terms are more stable for different classes. 1 . http://opennlp.sourceforge.net/ 2 . http://projects.ldc.upenn.edu/TDT3/Guide/label.html From the analysis of table 2, it is reasonable to adjust term weight according to their term type and the news class the story belongs to.",
                "New term weights are reweighted as follows: ( ) ( ) ( ) ( ) ( , , ) * ( , , ) ( , , ) * class d D type w T class d D type w w d weight d t w weight d t w weight d t w α α ∈ = ∑ (11) where type(w) represents the type of term w, and class(d) represents the class of story d, c kα is reweighting parameter for news class c and term type k. In the work, we just simply use statistics in table 2 as the reweighting parameters.",
                "Even thought using the statistics directly may not the best choice, we do not discuss how to automatically obtain the best parameters.",
                "We will try to use machine learning techniques to obtain the best parameters in the future work.",
                "In the work, we use BoosTexter [20] to classify all stories into one of the 11 ROIs.",
                "BoosTexter is a boosting based machine learning program, which creates a series of simple rules for building a classifier for text or attribute-value data.",
                "We use term weight generated using TF-IDF model as feature for story classification.",
                "We trained the model on the 12000 judged English stories in TDT2, and classify the rest of the stories in TDT2 and all stories in TDT3.",
                "Classification results are used for term reweighting in formula (11).",
                "Since the class labels of topic-off stories are not given in TDT datasets, we cannot give the classification accuracy here.",
                "Thus we do not discuss the effects of classification accuracy to NED performance in the paper. 6.",
                "EXPERIMENTAL SETUP 6.1 Datasets We used two LDC [18] datasets TDT2 and TDT3 for our experiments.",
                "TDT2 contains news stories from January to June 1998.",
                "It contains around 54,000 stories from sources like ABC, Associated Press, CNN, New York Times, Public Radio International, Voice of America etc.",
                "Only English stories in the collection were considered.",
                "TDT3 contains approximately 31,000 English stories collected from October to December 1998.",
                "In addition to the sources used in TDT2, it also contains stories from NBC and MSNBC TV broadcasts.",
                "We used transcribed versions of the TV and radio broadcasts besides textual news.",
                "TDT2 dataset is labeled with about 100 topics, and approximately 12,000 English stories belong to at least one of these topics.",
                "TDT3 dataset is labeled with about 120 topics, and approximately 8000 English stories belong to at least one of these topics.",
                "All the topics are classified into 11 Rules of Interpretation: (1)Elections, (2)Scandals/Hearings, (3)Legal/Criminal Cases, (4)Natural Disasters, (5)Accidents, (6)Ongoing Violence or War, (7)Science and Discovery News, (8)Finance, (9)New Law, (10)Sports News, (11)MISC.",
                "News. 6.2 Evaluation Metric TDT uses a cost function CDet that combines the probabilities of missing a new story and a false alarm [19]: * * * *Det Miss Miss Target FA FA NontargetC C P P C P P= + (12) Table 2.",
                "Average correlation between term types and news classes where CMiss means the cost of missing a new story, PMiss means the probability of missing a new story, and PTarget means the probability of seeing a new story in the data; CFA means the cost of a false alarm, PFA means the probability of a false alarm, and PNontarget means the probability of seeing an old story.",
                "The cost CDet is normalized such that a perfect system scores 0 and a trivial system, which is the better one of mark all stories as new or old, scores 1: ( ( * , * ) ) Det Det Miss Target FA Nontarget C Norm C min C P C P = (13) <br>new event detection</br> system gives two outputs for each story.",
                "The first part is yes or no indicating whether the story triggers a new event or not.",
                "The second part is a score indicating confidence of the first decision.",
                "Confidence scores can be used to plot DET curve, i.e., curves that plot false alarm vs. miss probabilities.",
                "Minimum normalized cost can be determined if optimal threshold on the score were chosen. 7.",
                "EXPERIMENTAL RESULTS 7.1 Main Results To test the approaches proposed in the model, we implemented and tested five systems: System-1: this system is used as baseline.",
                "It is implemented based on the basic model described in section 3, i.e., using incremental TF-IDF model to generate term weights, and using Hellinger distance to compute document similarity.",
                "Similarity score normalization is also employed [8].",
                "S-S detection procedure is used.",
                "System-2: this system is the same as system-1 except that S-C detection procedure is used.",
                "System-3: this system is the same as system-1 except that it uses the new detection procedure which is based on indexing-tree.",
                "System-4: implemented based on the approach presented in section 5.1, i.e., terms are reweighted according to the distance between term distributions in a cluster and all stories.",
                "The new detection procedure is used.",
                "System-5: implemented based on the approach presented in section 5.2, i.e., terms of different types are reweighted according to news class using trained parameters.",
                "The new detection procedure is used.",
                "The following are some other NED systems: System-6: [21] for each pair of stories, it computes three similarity values for named entity, non-named entity and all terms respectively.",
                "And employ Support Vector Machine to predict new or old using the similarity values as features.",
                "System-7: [8] it extended a basic incremental TF-IDF model to include source-specific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, etc.",
                "System-8: [13] it split document representation into two parts: named entities and non-named entities, and choose one effective part for each news class.",
                "Table 3 and table 4 show topic-weighted normalized costs and comparing times on TDT2 and TDT3 datasets respectively.",
                "Since no heldout data set for fine-tuning the threshold θ new was available for experiments on TDT2, we only report minimum normalized costs for our systems in table 3.",
                "System-5 outperforms all other systems including system-6, and it performs only 2.78e+8 comparing times in detection procedure which is only 13.4% of system-1.",
                "Table 3.",
                "NED results on TDT2 Systems Min Norm(CDet) Cmp times System-1 0.5749 2.08e+9 System-2① 0.6673 3.77e+8 System-3② 0.5765 2.81e+8 System-4② 0.5431 2.99e+8 System-5② 0.5089 2.78e+8 System-6 0.5300 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 When evaluating on the normalized costs on TDT3, we use the optimal thresholds obtained from TDT2 data set for all systems.",
                "System-2 reduces comparing times to 1.29e+9 which is just 18.3% of system-1, but at the same time it also gets a deteriorated minimum normalized cost which is 0.0499 higher than system-1.",
                "System-3 uses the new detection procedure based on news indexing-tree.",
                "It requires even less comparing times than system-2.",
                "This is because story-story comparisons usually yield greater similarities than story-cluster ones, so stories tend to be combined Location Person Date Organization Money Percentage NN JJ CD Elections 0.37 1 0.04 0.58 0.08 0.03 0.32 0.13 0.1 Scandals/Hearings 0.66 0.62 0.28 1 0.11 0.02 0.27 0.13 0.05 Legal/Criminal Cases 0.48 1 0.02 0.62 0.15 0 0.22 0.24 0.09 Natural Disasters 1 0.27 0 0.04 0.04 0 0.25 0.04 0.02 Violence or War 1 0.36 0.02 0.14 0.02 0.04 0.21 0.11 0.02 Science and Discovery 0.11 1 0.01 0.22 0.08 0.12 0.19 0.08 0.03 Finances 1 0.45 0.04 0.98 0.13 0.02 0.29 0.06 0.05 Sports 0.16 0.27 0.01 1 0.02 0 0.11 0.03 0.01 together in system-3.",
                "And system-3 is basically equivalent to system-1 in accuracy results.",
                "System-4 adjusts term weights based on the distance of term distributions between the whole corpus and cluster story set, yielding a good improvement by 0.0468 compared to system-1.",
                "The best system (system-5) has a minimum normalized cost 0.5012, which is 0.0797 better than system-1, and also better than any other results previously reported for this dataset [8, 13].",
                "Further more, system-5 only needs 1.05e+8 comparing times which is 14.9% of system-1.",
                "Table 4.",
                "NED results on TDT3 Systems Norm(CDet) Min Norm(CDet) Cmp times System-1 0.6159 0.5809 7.04e+8 System-2① 0.6493 0.6308 1.29e+8 System-3② 0.6197 0.5868 1.03e+8 System-4② 0.5601 0.5341 1.03e+8 System-5② 0.5413 0.5012 1.05e+8 System-7 -- 0.5783 -System-8 -- 0.5229 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 Figure5 shows the five DET curves for our systems on data set TDT3.",
                "System-5 achieves the minimum cost at a false alarm rate of 0.0157 and a miss rate of 0.4310.",
                "We can observe that System4 and System-5 obtain lower miss probability at regions of low false alarm probabilities.",
                "The hypothesis is that, more weight value is transferred to key terms of topics from non-key terms.",
                "Similarity score between two stories belonging to different topics are lower than before, because their overlapping terms are usually not key terms of their topics. 7.2 Parameter selection for indexing-tree detection Figure 3 shows the minimum normalized costs obtained by system-3 on TDT3 using different parameters.",
                "Theθ init parameter is tested on six values spanning from 0.03 to 0.18.",
                "And the λ parameter is tested on four values 1, 2, 3 and 4.",
                "We can see that, whenθ init is set to 0.12, which is the closest one toθ new, the costs are lower than others.",
                "This is easy to explain, because when stories belonging to the same topic are put in a cluster, it is more reasonable for the cluster to represent the stories in it.",
                "When parameter λ is set to 3 or 4, the costs are better than other cases, but there is no much difference between 3 and 4. 0 0.05 0.1 0.15 0.2 1 2 3 4 0.5 0.6 0.7 0.8 0.9 1 θ-initλ MinCost 0.6 0.65 0.7 0.75 0.8 0.85 0.9 Figure 3.",
                "Min Cost on TDT3 (δ =0.15) 0 0.05 0.1 0.15 0.2 1 2 3 4 0 0.5 1 1.5 2 2.5 x 10 8 θ-init λ Comparingtimes 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 8 Figure 4.",
                "Comparing times on TDT3 (δ =0.15) Figure 4 gives the comparing times used by system-3 on TDT3 with the same parameters as figure 3.",
                "The comparing times are strongly dependent onθ init.",
                "Because the greaterθ init is, the less stories combined together, the more comparing times are needed for new event decision.",
                "So we useθ init =0.13,λ =3,δ =0.15 for system-3, 4, and 5.",
                "In this parameter setting, we can get both low minimum normalized costs and less comparing times. 8.",
                "CONCLUSION We have proposed a news indexing-tree based detection procedure in our model.",
                "It reduces comparing times to about one seventh of traditional method without hurting NED accuracy.",
                "We also have presented two extensions to the basic TF-IDF model.",
                "The first extension is made by adjust term weights based on term distributions between the whole corpus and a cluster story set.",
                "And the second extension to basic TF-IDF model is better use of term types (named entities types and part-of-speed) according to news categories.",
                "Our experimental results on TDT2 and TDT3 datasets show that both of the two extensions contribute significantly to improvement in accuracy.",
                "We did not consider news time information as a clue for NED task, since most of the topics last for a long time and TDT data sets only span for a relative short period (no more than 6 months).",
                "For the future work, we want to collect news set which span for a longer period from internet, and integrate time information in NED task.",
                "Since topic is a relative coarse-grained news cluster, we also want to refine cluster granularity to event-level, and identify different events and their relations within a topic.",
                "Acknowledgments This work is supported by the National Natural Science Foundation of China under Grant No. 90604025.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsor. 9.",
                "REFERENCES [1] http://www.nist.gov/speech/tests/tdt/index.htm [2] In Topic Detection and Tracking.",
                "Event-based Information Organization.",
                "Kluwer Academic Publishers, 2002. .01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90 1 2 5 10 20 40 60 80 90 False Alarm Probability (in %) MissProbability(in%) SYSTEM1 Topic Weighted Curve SYSTEM1 Min Norm(Cost) SYSTEM2 Topic Weighted Curve SYSTEM2 Min Norm(Cost) SYSTEM3 Topic Weighted Curve SYSTEM3 Min Norm(Cost) SYSTEM4 Topic Weighted Curve SYSTEM4 Min Norm(Cost) SYSTEM5 Topic Weighted Curve SYSTEM5 Min Norm(Cost) Random Performance Figure 5.",
                "DET curves on TDT3 [3] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B.T.",
                "Archibald, and X. Liu.",
                "Learning Approaches for Detecting and Tracking News Events.",
                "In IEEE Intelligent Systems Special Issue on Applications of Intelligent Information Retrieval, volume 14 (4), 1999, 32-43. [4] Y. Yang, T. Pierce, and J. Carbonell.",
                "A Study on Retrospective and On-line Event Detection.",
                "In Proceedings of SIGIR-98, Melbourne, Australia, 1998, 28-36. [5] J. Allan, V. Lavrenko, D. Malin, and R. Swan.",
                "Detections, Bounds, and Timelines: Umass and tdt-3.",
                "In Proceedings of Topic Detection and Tracking Workshop (TDT-3), Vienna, VA, 2000, 167-174. [6] R. Papka and J. Allan.",
                "On-line <br>new event detection</br> Using Single Pass Clustering TITLE2:.",
                "Technical Report UM-CS1998-021, 1998. [7] W. Lam, H. Meng, K. Wong, and J.",
                "Yen.",
                "Using Contextual Analysis for News Event Detection.",
                "International Journal on Intelligent Systems, 2001, 525-546. [8] B. Thorsten, C. Francine, and F. Ayman.",
                "A System for <br>new event detection</br>.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2003, 330-337. [9] S. Nicola and C. Joe.",
                "Combining Semantic and Syntactic Document Classifiers to Improve First Story Detection.",
                "In Proceedings of the 24th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topicconditioned Novelty Detection.",
                "In Proceedings of the 8th ACM SIGKDD International Conference, ACM Press. 2002, 688-693. [11] M. Juha, A.M. Helena, and S. Marko.",
                "Applying Semantic Classes in Event Detection and Tracking.",
                "In Proceedings of International Conference on Natural Language Processing (ICON 2002), 2002, pages 175-183. [12] M. Juha, A.M. Helena, and S. Marko.",
                "Simple Semantics in Topic Detection and Tracking.",
                "Information Retrieval, 7(3-4): 2004, 347-368. [13] K. Giridhar and J. Allan.",
                "Text Classification and Named Entities for <br>new event detection</br>.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2004, 297-304. [14] J. P. Callan, W. B. Croft, and S. M. Harding.",
                "The INQUERY Retrieval System.",
                "In Proceedings of DEXA-92, 3rd International Conference on Database and Expert Systems Applications, 1992, 78-83. [15] R. Krovetz.",
                "Viewing Morphology as An Inference Process.",
                "In Proceedings of ACM SIGIR93, 1993, 61-81. [16] Y. Yang and J. Pedersen.",
                "A Comparative Study on Feature Selection in Text Categorization.",
                "In J. D. H. Fisher, editor, The Fourteenth International Conference on Machine Learning (ICML97), Morgan Kaufmann, 1997, 412-420. [17] T. M. Cover, and J.A.",
                "Thomas.",
                "Elements of Information Theory.",
                "Wiley. 1991. [18] The linguistic data consortium, http://www.ldc,upenn.edu/. [19] The 2001 TDT task definition and evaluation plan, http://www.nist.gov/speech/tests/tdt/tdt2001/evalplan.htm. [20] R. E. Schapire and Y.",
                "Singer.",
                "Boostexter: A Boosting-based System for Text Categorization.",
                "In Machine Learning 39(2/3):1, Kluwer Academic Publishers, 2000, 35-168. [21] K. Giridhar and J. Allan. 2005.",
                "Using Names and Topics for <br>new event detection</br>.",
                "In Proceedings of Human Technology Conference and Conference on Empirical Methods in Natural Language, Vancouver, 2005, 121-128"
            ],
            "original_annotated_samples": [
                "<br>new event detection</br> Based on Indexing-tree and Named Entity Zhang Kuo Tsinghua University Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Tsinghua University Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Tsinghua University Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn ABSTRACT <br>new event detection</br> (NED) aims at detecting from one or multiple streams of news stories that which one is reported on a new event (i.e.",
                "<br>new event detection</br> (NED) is one of the five tasks in TDT.",
                "BASIC MODEL In this section, we present the basic <br>new event detection</br> model which is similar to what most current systems apply.",
                "<br>new event detection</br> systems use news story stream as input, in which stories are strictly time-ordered.",
                "The cost CDet is normalized such that a perfect system scores 0 and a trivial system, which is the better one of mark all stories as new or old, scores 1: ( ( * , * ) ) Det Det Miss Target FA Nontarget C Norm C min C P C P = (13) <br>new event detection</br> system gives two outputs for each story."
            ],
            "translated_annotated_samples": [
                "Detección de nuevos eventos basada en árbol de indexación y entidad nombrada Zhang Kuo Universidad Tsinghua Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Universidad Tsinghua Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Universidad Tsinghua Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn RESUMEN La Detección de Nuevos Eventos (NED) tiene como objetivo detectar de una o varias corrientes de noticias cuál se informa sobre un nuevo evento (es decir,",
                "La <br>Detección de Eventos Nuevos</br> (DEN) es una de las cinco tareas en TDT.",
                "MODELO BÁSICO En esta sección, presentamos el modelo básico de Detección de Nuevos Eventos que es similar a lo que la mayoría de los sistemas actuales aplican.",
                "Los sistemas de <br>detección de eventos nuevos</br> utilizan flujos de noticias como entrada, en los cuales las historias están estrictamente ordenadas por tiempo.",
                "El costo CDet se normaliza de tal manera que un sistema perfecto obtiene una puntuación de 0 y un sistema trivial, que es el mejor de marcar todas las historias como nuevas o antiguas, obtiene una puntuación de 1: ( ( * , * ) ) Det Det Miss Target FA Nontarget C Norm C min C P C P = (13) El sistema de <br>detección de eventos nuevos</br> proporciona dos salidas para cada historia."
            ],
            "translated_text": "Detección de nuevos eventos basada en árbol de indexación y entidad nombrada Zhang Kuo Universidad Tsinghua Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Universidad Tsinghua Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Universidad Tsinghua Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn RESUMEN La Detección de Nuevos Eventos (NED) tiene como objetivo detectar de una o varias corrientes de noticias cuál se informa sobre un nuevo evento (es decir, no reportado previamente). Con el abrumador volumen de noticias disponible hoy en día, hay una creciente necesidad de un sistema NED que sea capaz de detectar eventos nuevos de manera más eficiente y precisa. En este artículo proponemos un nuevo modelo de NED para acelerar la tarea de NED mediante el uso dinámico de un árbol de indexación de noticias. Además, basándose en la observación de que los términos de diferentes tipos tienen diferentes efectos para la tarea de NED, se proponen dos enfoques de reponderación de términos para mejorar la precisión de NED. En el primer enfoque, proponemos ajustar dinámicamente los pesos de los términos basados en los clusters de historias previas y en el segundo enfoque, proponemos emplear estadísticas en los datos de entrenamiento para aprender el modelo de reajuste de entidades nombradas para cada clase de historias. Los resultados experimentales en dos conjuntos de datos del Consorcio de Datos Lingüísticos (LDC), TDT2 y TDT3, muestran que el modelo propuesto puede mejorar significativamente tanto la eficiencia como la precisión de la tarea de NED, en comparación con el sistema base y otros sistemas existentes. Categorías y Descriptores de Asignaturas H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información; H.4.2 [Aplicaciones de Sistemas de Información]: Tipos de Sistemas de Apoyo a Decisiones. Términos generales Algoritmos, Rendimiento, Experimentación 1. El programa de Detección y Seguimiento de Temas (TDT) tiene como objetivo desarrollar técnicas que puedan organizar, buscar y estructurar de manera efectiva materiales de texto de noticias de una variedad de agencias de noticias y medios de difusión [1]. La <br>Detección de Eventos Nuevos</br> (DEN) es una de las cinco tareas en TDT. Es la tarea de identificación en línea del informe más temprano para cada tema tan pronto como ese informe llegue en la secuencia de documentos. Un tema se define como un evento o actividad seminal, junto con eventos y actividades directamente relacionados [2]. Un evento se define como algo (no trivial) que sucede en un lugar y en un momento determinados [3]. Por ejemplo, cuando una bomba explota en un edificio, la explosión es el evento seminal que desencadena el tema, y otras historias sobre el mismo tema serían aquellas que discuten los esfuerzos de salvamento, la búsqueda de los perpetradores, arrestos y juicios, entre otros. La información útil de noticias suele estar enterrada en una masa de datos generados a diario. Por lo tanto, los sistemas NED son muy útiles para las personas que necesitan detectar información novedosa de un flujo de noticias en tiempo real. Estas necesidades de la vida real a menudo se presentan en ámbitos como los mercados financieros, el análisis de noticias y la recopilación de inteligencia. En la mayoría de los sistemas NED de última generación, cada noticia disponible se compara con todas las noticias previamente recibidas. Si todas las similitudes entre ellos no superan un umbral, entonces la historia desencadena un nuevo evento. Por lo general, se presentan en forma de similitud de coseno o métrica de similitud de Hellinger. El problema central de NED es identificar si dos historias tratan sobre el mismo tema. Obviamente, estos sistemas no pueden aprovechar la información del tema. Además, no es aceptable en aplicaciones reales debido a la gran cantidad de cálculos requeridos en el proceso de NED. Otros sistemas organizan historias anteriores en grupos (cada grupo corresponde a un tema), y la nueva historia se compara con los grupos anteriores en lugar de con historias. Esta forma puede reducir significativamente los tiempos de comparación. Sin embargo, se ha demostrado que este método es menos preciso [4, 5]. Esto se debe a que a veces las historias dentro de un tema se alejan mucho entre sí, lo que podría resultar en una baja similitud entre una historia y su tema. Por otro lado, algunos sistemas NED propuestos intentaron mejorar la precisión al hacer un mejor uso de las entidades nombradas [10, 11, 12, 13]. Sin embargo, ninguno de los sistemas ha considerado que los términos de diferentes tipos (por ejemplo, Los sustantivos, verbos o nombres de personas tienen diferentes efectos en diferentes clases de historias para determinar si dos historias tratan sobre el mismo tema. Por ejemplo, los nombres de los candidatos electorales (Nombre de la persona) son muy importantes para las historias de la clase de elecciones; las ubicaciones (Nombre de la ubicación) donde ocurrieron los accidentes son importantes para las historias de la clase de accidentes. Por lo tanto, en NED, todavía existen los siguientes tres problemas por investigar: (1) ¿Cómo acelerar el procedimiento de detección sin disminuir la precisión de la detección? (2) ¿Cómo aprovechar mejor la información de los grupos (temas) para mejorar la precisión? (3) ¿Cómo obtener una mejor representación de historias de noticias mediante una mejor comprensión de las entidades nombradas? Impulsados por estos problemas, hemos propuesto tres enfoques en este documento. (1) Para hacer que el procedimiento de detección sea más rápido, proponemos un nuevo procedimiento de NED basado en un árbol de indexación de noticias creado dinámicamente. El índice de historias en árbol se crea ensamblando historias similares para formar grupos de noticias en diferentes jerarquías según sus valores de similitud. Las comparaciones entre la historia actual y los grupos anteriores podrían ayudar a encontrar la historia más similar en menos tiempo de comparación. El nuevo procedimiento puede reducir la cantidad de veces de comparación sin afectar la precisión. Utilizamos los grupos del primer piso en el árbol de indexación como temas de noticias, en los cuales los pesos de los términos se ajustan dinámicamente según la distribución de términos en los grupos. En este enfoque, se utiliza adecuadamente la información de los grupos (temas), por lo que se evita el problema de la descentralización del tema. Basándonos en observaciones de las estadísticas obtenidas de los datos de entrenamiento, encontramos que los términos de diferentes tipos (por ejemplo, Los sustantivos y verbos tienen diferentes efectos para diferentes clases de historias al determinar si dos historias tratan sobre el mismo tema. Y proponemos utilizar estadísticas para optimizar los pesos de los términos de diferentes tipos en una historia de acuerdo a la clase de noticias a la que pertenece la historia. En el conjunto de datos TDT3, el nuevo modelo NED solo utiliza un 14.9% en comparación con los tiempos del modelo básico, mientras que su costo normalizado mínimo es de 0.5012, lo que es 0.0797 mejor que el modelo básico, y también mejor que cualquier otro resultado previamente reportado para este conjunto de datos [8, 13]. El resto del documento está organizado de la siguiente manera. Comenzamos este artículo resumiendo el trabajo previo en NED en la sección 2. La sección 3 presenta el modelo básico para NED que la mayoría de los sistemas actuales utilizan. La sección 4 describe nuestro nuevo procedimiento de detección basado en el índice de árbol de noticias. En la sección 5, se proponen dos métodos de reponderación de términos para mejorar la precisión de NED. La sección 6 presenta nuestros datos experimentales y métricas de evaluación. Finalmente concluimos con los resultados experimentales en la Sección 7, y las conclusiones y trabajo futuro en la Sección 8. TRABAJO RELACIONADO Papka et al. propusieron el agrupamiento de un solo paso en NED [6]. Cuando se encontraba una nueva historia, esta era procesada inmediatamente para extraer características de términos y se construía una representación de consulta del contenido de la historia. Luego se comparó con todas las consultas anteriores. Si el documento no generaba ninguna consulta al exceder un umbral, se marcaba como un nuevo evento. Lam et al construyen representaciones de consultas previas de grupos de historias, cada una de las cuales corresponde a un tema [7]. De esta manera se realizan comparaciones entre historias y grupos. En los últimos años, la mayoría de los trabajos se han centrado en proponer mejores métodos para la comparación de historias y la representación de documentos. Brants et al. [8] extendieron un modelo básico incremental de TF-IDF para incluir modelos específicos de fuente, normalización de puntajes de similitud basada en promedios específicos de documentos, normalización de puntajes de similitud basada en promedios específicos de pares de fuentes, reponderación de términos basada en frecuencias de eventos inversas y segmentación de documentos. Se mostraron buenas mejoras en los puntos de referencia de TDT. Stokes et al. [9] utilizaron una combinación de evidencia de dos representaciones distintas del contenido de un documento. Una de las representaciones era el vector de texto libre habitual, la otra hacía uso de cadenas léxicas (creadas utilizando WordNet) para construir otro vector de términos. Luego, las dos representaciones se combinan de forma lineal. Se logró un aumento marginal en la efectividad cuando se utilizó la representación combinada. Algunos esfuerzos se han realizado sobre cómo utilizar entidades nombradas para mejorar la desambiguación de entidades nombradas (NED). Yang et al. asignaron un peso cuatro veces mayor a las entidades con nombres de ubicación que a otros términos y entidades con nombres [10]. El grupo de investigación DOREMI combinó similitudes semánticas de nombres de personas, nombres de lugares y tiempo junto con similitud textual [11][12]. El grupo de investigación de UMass [13] dividió la representación del documento en dos partes: entidades nombradas y entidades no nombradas. Y se descubrió que algunas clases de noticias podían lograr un mejor rendimiento utilizando la representación de entidades nombradas, mientras que otras clases de noticias podían lograr un mejor rendimiento utilizando la representación sin entidades nombradas. Tanto [10] como [13] utilizaron la técnica de categorización de texto para clasificar previamente las noticias. En [13] las noticias se clasifican automáticamente al principio, y luego se prueban las sensibilidades de los nombres y términos que no son nombres para NED para cada clase. En [10], se eliminan los términos frecuentes de cada clase de la representación del documento. Por ejemplo, la palabra \"elección\" no ayuda a identificar diferentes elecciones. En su trabajo, no se investiga la efectividad de diferentes tipos de nombres (o términos con diferentes categorías gramaticales) para NED en diferentes clases de noticias. Utilizamos análisis estadístico para revelar el hecho y utilizarlo para mejorar el rendimiento de NED. 3. MODELO BÁSICO En esta sección, presentamos el modelo básico de Detección de Nuevos Eventos que es similar a lo que la mayoría de los sistemas actuales aplican. Luego, proponemos nuestro nuevo modelo ampliando el modelo básico. Los sistemas de <br>detección de eventos nuevos</br> utilizan flujos de noticias como entrada, en los cuales las historias están estrictamente ordenadas por tiempo. Solo están disponibles las historias recibidas previamente al tratar con la historia actual. La salida es una decisión sobre si la historia actual es un evento nuevo o no y la confianza de la decisión. Por lo general, un modelo NED consta de tres partes: representación de la historia, cálculo de similitud y procedimiento de detección. 3.1 Representación de la historia Se necesita preprocesamiento antes de generar la representación de la historia. Para el preprocesamiento, tokenizamos palabras, reconocemos abreviaturas, normalizamos abreviaturas, agregamos etiquetas de partes del discurso, eliminamos palabras vacías incluidas en la lista de paradas utilizada en InQuery [14], reemplazamos palabras con sus raíces utilizando el algoritmo K-stem[15], y luego generamos un vector de palabras para cada noticia. Utilizamos el modelo TF-IDF incremental para el cálculo del peso de los términos [4]. En un modelo TF-IDF, la frecuencia de término en un documento de noticias se pondera por la frecuencia inversa del documento, la cual se genera a partir del corpus de entrenamiento. Cuando surge un nuevo término en el proceso de prueba, hay dos soluciones: simplemente ignorar el nuevo término o establecer df del término como una constante pequeña (por ejemplo, df = 1). El nuevo término recibe un peso demasiado bajo en la primera solución (0) y un peso demasiado alto en la segunda solución. En el modelo TF-IDF incremental, las frecuencias de documentos se actualizan dinámicamente en cada paso de tiempo t: 1( ) ( ) ( )t t D tdf w df w df w−= + (1) donde Dt representa el conjunto de noticias recibidas en el tiempo t, y dfDt(w) significa el número de documentos en los que aparece el término w, y dft(w) significa el número total de documentos en los que aparece el término w antes del tiempo t. En este trabajo, cada ventana de tiempo incluye 50 noticias. Así, cada historia d recibida en t se representa de la siguiente manera: 1 2{ ( , , ), ( , , ),..., ( , , )}nd peso d t w peso d t w peso d t w→ donde n significa el número de términos distintos en la historia d, y ( , , )peso d t w significa el peso del término w en la historia d en el tiempo t: log( ( , ) 1) log(( 1) /( ( ) 0.5)) ( , , ) log( ( , ) 1) log(( 1) /( ( ) 0.5)) t t t t w d tf d w N df w peso d t w tf d w N df w ∈ + + + = + + +∑ (2) donde Nt significa el número total de historias de noticias antes del tiempo t, y tf(d,w) significa cuántas veces aparece el término w en la historia de noticias d. 3.2 Cálculo de Similitud Utilizamos la distancia de Hellinger para el cálculo de la similitud entre dos historias, para dos historias d y d en el tiempo t, su similitud se define de la siguiente manera: , ( , , ) ( , , ) * ( , , ) w d d sim d d t peso d t w peso d t w ∈ = ∑ (3) 3.3 Procedimiento de Detección Para cada historia d recibida en el paso de tiempo t, el valor ( ) ( ) ( ) ( ( , , )) tiempo d tiempo d n d max sim d d t < = (4) es un puntaje utilizado para determinar si d es una historia sobre un nuevo tema y al mismo tiempo es una indicación de la confianza en nuestra decisión [8]. tiempo(d) significa el tiempo de publicación de la historia d. Si el puntaje supera el umbral θ new, entonces existe un documento suficientemente similar, por lo tanto, d es una historia antigua, de lo contrario, no hay un documento previo suficientemente similar, por lo tanto, d es una historia nueva. 4. Nuevo procedimiento NED Los sistemas NED tradicionales se pueden clasificar en dos tipos principales en cuanto al procedimiento de detección: (1) tipo S-S, en el que la historia en cuestión se compara con cada historia recibida previamente, y se utiliza la mayor similitud para determinar si la historia actual trata sobre un evento nuevo; (2) tipo S-C, en el que la historia en cuestión se compara con todos los clusters anteriores, cada uno de los cuales representa un tema, y se utiliza la mayor similitud para la decisión final sobre la historia actual. Si la similitud más alta supera el umbral θ nuevo, entonces es una historia antigua y se coloca en el clúster más similar; de lo contrario, es una historia nueva y se crea un nuevo clúster. Trabajos anteriores muestran que la primera forma es más precisa que la segunda [4][5]. Dado que a veces las historias dentro de un tema se alejan mucho entre sí, una historia puede tener una similitud muy baja con su tema. Por lo tanto, utilizar similitudes entre historias para determinar una nueva historia es mejor que utilizar similitudes entre historias y grupos. Sin embargo, el primer método requiere mucho más tiempo de comparación, lo que significa que es menos eficiente. Proponemos un nuevo procedimiento de detección que utiliza comparaciones con clusters anteriores para ayudar a encontrar la historia más similar en menos tiempo de comparación, y la decisión final sobre el nuevo evento se toma de acuerdo con la historia más similar. Por lo tanto, podemos obtener tanto la precisión de los métodos de tipo S-S como la eficiencia de los métodos de tipo S-C. El nuevo procedimiento crea un árbol de indexación de noticias dinámicamente, en el cual se agrupan historias similares para formar una jerarquía de clusters. Indexamos historias similares juntas por su ancestro común (un nodo de clúster). Historias diferentes están indexadas en diferentes grupos. Cuando una historia está llegando, utilizamos comparaciones entre la historia actual y los grupos jerárquicos anteriores para ayudar a encontrar la historia más similar que es útil para la toma de decisiones sobre nuevos eventos. Después de que se tome la decisión sobre el nuevo evento, la historia actual se inserta en el árbol de indexación para la detección siguiente. El índice de noticias en forma de árbol se define formalmente de la siguiente manera: S-Tree = {r, NC, NS, E}, donde r es la raíz de S-Tree, NC es el conjunto de todos los nodos de clúster, NS es el conjunto de todos los nodos de historia, y E es el conjunto de todos los bordes en S-Tree. Definimos un conjunto de restricciones para un S-Árbol: ⅰ. , es un nodo no terminal en el árbolC i i N i∀ ∈ → ⅱ. , es un nodo terminal en el árbolS i i N i∀ ∈ → ⅲ. , el grado de salida de es al menos 2C i i N i∀ ∈ → ⅳ. , se representa como el centroide de sus descendientesC i i iN∀ ∈ → Para una noticia di, el procedimiento de comparación y el procedimiento de inserción basados en el árbol de indexación se definen de la siguiente manera. Un ejemplo se muestra en la Figura 1 y la Figura 2. Figura 1. Procedimiento de comparación Figura 2. Procedimiento de comparación de inserción: Paso 1: comparar di con todos los nodos hijos directos de r y seleccionar λ nodos con las similitudes más altas, por ejemplo, C1 2 y C1 3 en la Figura 1. Paso 2: para cada nodo seleccionado en el paso anterior, por ejemplo. C1 2, compara di con todos sus nodos hijos directos y selecciona λ nodos con las similitudes más altas, por ejemplo. C2 2 y d8. Repetir el paso 2 para todos los nodos no terminales. Paso 3: registrar el nodo terminal con la mayor similitud a di, por ejemplo, s5, y el valor de similitud (0.20). Insertando di en el S-árbol con r como raíz: Encuentra el nodo n que es hijo directo de r en el camino desde r hasta el nodo terminal con la similitud más alta s, por ejemplo. C1 2. Si s es menor que θ init + (h-1)δ, entonces agregar di al árbol como un hijo directo de r. De lo contrario, si n es un nodo terminal, crear un nodo de clúster en lugar de n, y agregar tanto n como di como sus hijos directos; si n es un nodo no terminal, repetir este procedimiento e insertar di en el subárbol con n como raíz de forma recursiva. Aquí h es la longitud entre n y la raíz del árbol S. Cuanto más similares sean las historias en un grupo, mejor representará el grupo a las historias en él. Por lo tanto, no imponemos restricciones en la altura máxima de los árboles y el grado de un nodo. Por lo tanto, no podemos dar la complejidad de este procedimiento basado en árboles de indexación. Pero proporcionaremos el número de veces de comparación necesarias por el nuevo procedimiento en nuestros experimentos en la sección 7.5. En esta sección, se proponen dos métodos de reponderación de términos para mejorar la precisión de NED. En el primer método, se explora una nueva forma de utilizar de manera más efectiva la información de los grupos (temas). El segundo encuentra una mejor manera de utilizar entidades nombradas basadas en la clasificación de noticias. 5.1 Reajuste de términos basado en la distancia de distribución TF-IDF es el modelo más prevalente utilizado en sistemas de recuperación de información. La idea básica es que cuanto menos documentos aparezca un término, más importante es el término en la discriminación de documentos (relevantes o no relevantes para una consulta que contenga el término). Sin embargo, en el dominio de TDT, necesitamos discriminar documentos en función de los temas en lugar de las consultas. Intuitivamente, el uso de vectores de clúster (tema) para comparar con historias de noticias posteriores debería tener un mejor rendimiento que el uso de vectores de historias. Desafortunadamente, los resultados experimentales no respaldan esta intuición [4][5]. Basándonos en la observación de los datos, encontramos que la razón es que un tema de noticias generalmente contiene muchos eventos directa o indirectamente relacionados, y todos ellos tienen sus propios subtemas que suelen ser diferentes entre sí. Tomemos el tema descrito en la sección 1 como ejemplo, eventos como la explosión y el rescate tienen similitudes muy bajas con eventos sobre juicios criminales, por lo tanto, las historias sobre juicios tendrían una baja similitud con el vector de tema construido en base a sus eventos previos. Esta sección se centra en cómo hacer un uso efectivo de la información del tema y al mismo tiempo evitar el problema de la descentralización del contenido. Al principio, clasificamos los términos en 5 clases para ayudar en el análisis de las necesidades del modelo modificado: Clase de términos A: términos que ocurren con frecuencia en todo el corpus, por ejemplo, año y personas. Los términos de esta clase deben recibir pesos bajos porque no ayudan mucho en la discriminación de temas. Clase B de términos: términos que ocurren con frecuencia dentro de una categoría de noticias, por ejemplo, elección, tormenta. Son útiles para distinguir dos historias en diferentes categorías de noticias. Sin embargo, no pueden proporcionar información para determinar si dos historias tratan sobre el mismo tema o temas diferentes. En otras palabras, los términos elección y tormenta no son útiles para diferenciar dos campañas electorales y dos desastres de tormenta. Por lo tanto, los términos de esta clase deberían asignarse pesos más bajos. Clase de término C: términos que ocurren con frecuencia en un tema y raramente en otros temas, por ejemplo, el nombre de un avión accidentado, el nombre de un huracán específico. Las noticias que pertenecen a diferentes temas rara vez tienen términos superpuestos en esta clase. Cuanto más frecuentemente aparezca un término en un tema, más importante es el término para una historia perteneciente al tema, por lo tanto, el término debería tener un peso más alto. Clase de término D: términos que aparecen en un tema de manera exclusiva, pero no con frecuencia. Por ejemplo, el nombre de un bombero que se desempeñó muy bien en una acción de salvamento, que puede aparecer en solo dos o tres historias pero nunca ha aparecido en otros temas. Los términos de este tipo deberían recibir más peso que en el modelo TF-IDF. Sin embargo, dado que no son populares en el tema, no es apropiado asignarles pesos demasiado altos. Clase de término E: términos con baja frecuencia en documentos y que aparecen en diferentes temas. Los términos de esta clase deberían recibir pesos más bajos. Ahora analizamos si el modelo TF-IDF puede asignar pesos adecuados a las cinco clases de términos. Obviamente, los términos de la clase A tienen un peso bajo en el modelo TF-IDF, lo cual es acorde con el requisito descrito anteriormente. En el modelo TF-IDF, los términos de la clase B dependen altamente del número de historias en una clase de noticias. El modelo TF-IDF no puede proporcionar pesos bajos si la historia que contiene el término pertenece a una clase de noticias relativamente pequeña. Para un término de clase C, cuanto más frecuentemente aparezca en un tema, menos peso le da el modelo TFIDF. Esto entra en conflicto directo con el requisito de términos en la clase C. Para los términos de la clase D, el modelo TF-IDF les asigna pesos altos de manera correcta. Pero para los términos de la clase E, el modelo TF-IDF asigna pesos altos a ellos que no son conformes con el requisito de pesos bajos. En resumen, los términos de clase B, C, E no pueden ser ponderados adecuadamente en el modelo TF-IDF. Por lo tanto, proponemos un modelo modificado para resolver este problema. Cuando θ init y θ new están configurados de cerca, asumimos que la mayoría de las historias en un clúster de primer nivel (un nodo hijo directo del nodo raíz) están en el mismo tema. Por lo tanto, utilizamos un clúster de primer nivel para capturar la distribución de términos (df para todos los términos dentro del clúster) dentro del tema de forma dinámica. La divergencia de Kullback-Leibler de la distribución de términos en un clúster de primer nivel y el conjunto completo de historias se utiliza para ajustar los pesos de los términos: ( , , ) * (1 * ( || )) ( , , ) ( , , ) * (1 * ( || )) cw tw cw tw w d D peso d t w KL P P peso d t w peso d t w KL P P γ γ ∈ + = +∑ (5) donde ( ) ( ) ( ) ( ) 1,cw cw c c c c df w df w p y p y N N = = − (6) ( ) ( ) ( ) ( ) 1,t t tw tw t t df w df w p y p y N N = = − (7) donde dfc(w) es el número de documentos que contienen el término w dentro del clúster C, y Nc es el número de documentos en el clúster C, y Nt es el número total de documentos que llegan antes del paso de tiempo t. γ es un parámetro constante, actualmente configurado manualmente en 3. La divergencia de Kullback-Leibler se define de la siguiente manera [17]: La idea básica es que, para una historia en un tema, cuanto más ocurre un término dentro del tema y menos ocurre en otros temas, se le deben asignar pesos más altos. Obviamente, el modelo modificado puede cumplir con todos los requisitos de las cinco clases de términos mencionadas anteriormente. 5.2 Reajuste de Términos Basado en el Tipo de Término y la Clase de Historia. Trabajos anteriores encontraron que algunas clases de historias de noticias podrían lograr mejoras significativas al dar un peso adicional a las entidades nombradas. Pero encontramos que los términos de diferentes tipos deben recibir una cantidad diferente de peso adicional para diferentes clases de noticias. Utilizamos open-NLP1 para reconocer tipos de entidades nombradas y etiquetas de partes del discurso para términos que aparecen en noticias. Los tipos de entidades nombradas incluyen nombre de persona, nombre de organización, nombre de ubicación, fecha, hora, dinero y porcentaje, y se seleccionan cinco partes del discurso: ninguno (NN), verbo (VB), adjetivo (JJ), adverbio (RB) y número cardinal (CD). El análisis estadístico muestra tipos de términos discriminativos a nivel de tema para diferentes clases de historias. Por conveniencia, los tipos de entidad nombrada y etiquetas de parte de la oración se llaman uniformemente tipo de término en las secciones siguientes. Determinar si dos historias tratan sobre el mismo tema es un componente básico para la tarea de NED. Por lo tanto, al principio usamos la estadística χ² para calcular las correlaciones entre términos y temas. Para un término t y un tema T, se deriva una tabla de contingencia: Tabla 1. Una tabla de contingencia de 2×2 con el número de documentos pertenecientes al tema T y no pertenecientes al tema T incluye t A B no incluye t C D. La estadística χ² para un término específico t con respecto al tema T se define como [16]: χ² = ( ) ( ) * ( ) ( ) * ( ) * ( ) * ( ) w T A B C D AD CB A C B D A B C D + + + − + + + + (9). Los temas de noticias para la tarea TDT se clasifican además en 11 reglas de interpretaciones (ROIs) 2. El ROI se puede ver como una clase de historias de nivel superior. El promedio de correlación entre un tipo de término y un ROI de tema se calcula como: 2 avg 2 ( , )( ( , ) )k m m km kT R w P w TP R p w T R P χ χ ∈ ∈ ∑ ∑（ , ）= 1 1 k=1…K, m=1…M (10) donde K es el número de tipos de términos (fijado constantemente en 12 en el artículo). M es el número de nuevas clases (ROIs, establecido en el conjunto 11 del artículo). Pk representa el conjunto de todos los términos de tipo k, y Rm representa el conjunto de todos los temas de la clase m, p(t,T) significa la probabilidad de que t ocurra en el tema T. Debido a limitaciones de espacio, solo se enumeran partes de los tipos de términos (9 tipos de términos) y partes de las clases de noticias (8 clases) en la tabla 2 con los valores promedio de correlación entre ellos. Las estadísticas se derivan de datos etiquetados en el corpus TDT2. (Los resultados en la tabla 2 ya están normalizados para mayor comodidad en la comparación). Las estadísticas en la tabla 2 indican la utilidad de diferentes tipos de términos en la discriminación de temas con respecto a diferentes clases de noticias. Podemos ver que el nombre de la ubicación es el tipo de término más útil para tres clases de noticias: Desastres Naturales, Violencia o Guerra, Finanzas. Y para otras tres categorías: Elecciones, Casos Legales/Criminales, Ciencia y Descubrimiento, el nombre de la persona es el tipo de término más discriminativo. Para Escándalos/Audiencias, la fecha es la información más importante para la discriminación de temas. Además, los casos legales/criminales y los temas financieros tienen una correlación más alta con términos relacionados con el dinero, mientras que la ciencia y el descubrimiento tienen una correlación más alta con términos de porcentaje. Los términos no nominales son más estables para diferentes clases. 1. http://opennlp.sourceforge.net/ 2. http://projects.ldc.upenn.edu/TDT3/Guide/label.html A partir del análisis de la tabla 2, es razonable ajustar el peso de los términos según su tipo de término y la clase de noticias a la que pertenece la historia. Los nuevos pesos de los términos se recalculan de la siguiente manera: ( ) ( ) ( ) ( ) ( , , ) * ( , , ) ( , , ) * clase d D tipo w T clase d D tipo w w d peso d t w peso d t w peso d t w α α ∈ = ∑ (11) donde type(w) representa el tipo del término w, y class(d) representa la clase de la historia d, c kα es el parámetro de recalibración para la clase de noticias c y el tipo de término k. En el trabajo, simplemente utilizamos las estadísticas de la tabla 2 como parámetros de recalibración. Aunque usar las estadísticas directamente puede que no sea la mejor opción, no discutimos cómo obtener automáticamente los mejores parámetros. Intentaremos utilizar técnicas de aprendizaje automático para obtener los mejores parámetros en el trabajo futuro. En el trabajo, utilizamos BoosTexter [20] para clasificar todas las historias en uno de los 11 ROIs. BoosTexter es un programa de aprendizaje automático basado en boosting, que crea una serie de reglas simples para construir un clasificador de datos de texto o atributos-valor. Utilizamos el peso del término generado utilizando el modelo TF-IDF como característica para la clasificación de historias. Entrenamos el modelo con las 12000 historias en inglés evaluadas en TDT2, y clasificamos el resto de las historias en TDT2 y todas las historias en TDT3. Los resultados de clasificación se utilizan para la reponderación de términos en la fórmula (11). Dado que las etiquetas de clase de las historias de tema apagado no se proporcionan en los conjuntos de datos de TDT, no podemos proporcionar la precisión de clasificación aquí. Por lo tanto, no discutimos los efectos de la precisión de la clasificación en el rendimiento de NED en el artículo. 6. CONFIGURACIÓN EXPERIMENTAL 6.1 Conjuntos de datos Utilizamos dos conjuntos de datos de LDC [18], TDT2 y TDT3, para nuestros experimentos. TDT2 contiene noticias desde enero hasta junio de 1998. Contiene alrededor de 54,000 historias de fuentes como ABC, Associated Press, CNN, New York Times, Public Radio International, Voice of America, etc. Solo se consideraron historias en inglés en la colección. TDT3 contiene aproximadamente 31,000 historias en inglés recopiladas de octubre a diciembre de 1998. Además de las fuentes utilizadas en TDT2, también contiene historias de las transmisiones de televisión de NBC y MSNBC. Utilizamos versiones transcritas de las emisiones de televisión y radio además de noticias textuales. El conjunto de datos TDT2 está etiquetado con alrededor de 100 temas, y aproximadamente 12,000 historias en inglés pertenecen al menos a uno de estos temas. El conjunto de datos TDT3 está etiquetado con alrededor de 120 temas, y aproximadamente 8000 historias en inglés pertenecen al menos a uno de estos temas. Todos los temas están clasificados en 11 Reglas de Interpretación: (1) Elecciones, (2) Escándalos/Audiencias, (3) Casos Legales/Criminales, (4) Desastres Naturales, (5) Accidentes, (6) Violencia o Guerra en Curso, (7) Noticias de Ciencia y Descubrimientos, (8) Finanzas, (9) Nueva Ley, (10) Noticias Deportivas, (11) Varios. Noticias. 6.2 Métrica de evaluación TDT utiliza una función de costo CDet que combina las probabilidades de perder una nueva historia y una falsa alarma [19]: * * * *Det Miss Miss Target FA FA NontargetC C P P C P P= + (12) Tabla 2. Correlación promedio entre tipos de términos y clases de noticias donde CMiss significa el costo de perder una nueva historia, PMiss significa la probabilidad de perder una nueva historia, y PTarget significa la probabilidad de ver una nueva historia en los datos; CFA significa el costo de una falsa alarma, PFA significa la probabilidad de una falsa alarma, y PNontarget significa la probabilidad de ver una historia antigua. El costo CDet se normaliza de tal manera que un sistema perfecto obtiene una puntuación de 0 y un sistema trivial, que es el mejor de marcar todas las historias como nuevas o antiguas, obtiene una puntuación de 1: ( ( * , * ) ) Det Det Miss Target FA Nontarget C Norm C min C P C P = (13) El sistema de <br>detección de eventos nuevos</br> proporciona dos salidas para cada historia. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "streams of news stories": {
            "translated_key": "corrientes de noticias",
            "is_in_text": true,
            "original_annotated_sentences": [
                "New Event Detection Based on Indexing-tree and Named Entity Zhang Kuo Tsinghua University Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Tsinghua University Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Tsinghua University Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn ABSTRACT New Event Detection (NED) aims at detecting from one or multiple <br>streams of news stories</br> that which one is reported on a new event (i.e.",
                "not reported previously).",
                "With the overwhelming volume of news available today, there is an increasing need for a NED system which is able to detect new events more efficiently and accurately.",
                "In this paper we propose a new NED model to speed up the NED task by using news indexing-tree dynamically.",
                "Moreover, based on the observation that terms of different types have different effects for NED task, two term reweighting approaches are proposed to improve NED accuracy.",
                "In the first approach, we propose to adjust term weights dynamically based on previous story clusters and in the second approach, we propose to employ statistics on training data to learn the named entity reweighting model for each class of stories.",
                "Experimental results on two Linguistic Data Consortium (LDC) datasets TDT2 and TDT3 show that the proposed model can improve both efficiency and accuracy of NED task significantly, compared to the baseline system and other existing systems.",
                "Categories and Subject Descriptors H.3.3 [Information Systems]: Information Search and Retrieval; H.4.2 [Information Systems Applications]: Types of Systemsdecision support.",
                "General Terms Algorithms, Performance, Experimentation 1.",
                "INTRODUCTION Topic Detection and Tracking (TDT) program aims to develop techniques which can effectively organize, search and structure news text materials from a variety of newswire and broadcast media [1].",
                "New Event Detection (NED) is one of the five tasks in TDT.",
                "It is the task of online identification of the earliest report for each topic as soon as that report arrives in the sequence of documents.",
                "A Topic is defined as a seminal event or activity, along with directly related events and activities [2].",
                "An Event is defined as something (non-trivial) happening in a certain place at a certain time [3].",
                "For instance, when a bomb explodes in a building, the exploding is the seminal event that triggers the topic, and other stories on the same topic would be those discussing salvaging efforts, the search for perpetrators, arrests and trial and so on.",
                "Useful news information is usually buried in a mass of data generated everyday.",
                "Therefore, NED systems are very useful for people who need to detect novel information from real-time news stream.",
                "These real-life needs often occur in domains like financial markets, news analysis, and intelligence gathering.",
                "In most of state-of-the-art (currently) NED systems, each news story on hand is compared to all the previous received stories.",
                "If all the similarities between them do not exceed a threshold, then the story triggers a new event.",
                "They are usually in the form of cosine similarity or Hellinger similarity metric.",
                "The core problem of NED is to identify whether two stories are on the same topic.",
                "Obviously, these systems cannot take advantage of topic information.",
                "Further more, it is not acceptable in real applications because of the large amount of computation required in the NED process.",
                "Other systems organize previous stories into clusters (each cluster corresponds to a topic), and new story is compared to the previous clusters instead of stories.",
                "This manner can reduce comparing times significantly.",
                "Nevertheless, it has been proved that this manner is less accurate [4, 5].",
                "This is because sometimes stories within a topic drift far away from each other, which could lead low similarity between a story and its topic.",
                "On the other hand, some proposed NED systems tried to improve accuracy by making better use of named entities [10, 11, 12, 13].",
                "However, none of the systems have considered that terms of different types (e.g.",
                "Noun, Verb or Person name) have different effects for different classes of stories in determining whether two stories are on the same topic.",
                "For example, the names of election candidates (Person name) are very important for stories of election class; the locations (Location name) where accidents happened are important for stories of accidents class.",
                "So, in NED, there still exist following three problems to be investigated: (1) How to speed up the detection procedure while do not decrease the detection accuracy? (2) How to make good use of cluster (topic) information to improve accuracy? (3) How to obtain better news story representation by better understanding of named entities.",
                "Driven by these problems, we have proposed three approaches in this paper. (1)To make the detection procedure faster, we propose a new NED procedure based on news indexing-tree created dynamically.",
                "Story indexing-tree is created by assembling similar stories together to form news clusters in different hierarchies according to their values of similarity.",
                "Comparisons between current story and previous clusters could help find the most similar story in less comparing times.",
                "The new procedure can reduce the amount of comparing times without hurting accuracy. (2)We use the clusters of the first floor in the indexing-tree as news topics, in which term weights are adjusted dynamically according to term distribution in the clusters.",
                "In this approach, cluster (topic) information is used properly, so the problem of theme decentralization is avoided. (3)Based on observations on the statistics obtained from training data, we found that terms of different types (e.g.",
                "Noun and Verb) have different effects for different classes of stories in determining whether two stories are on the same topic.",
                "And we propose to use statistics to optimize the weights of the terms of different types in a story according to the news class that the story belongs to.",
                "On TDT3 dataset, the new NED model just uses 14.9% comparing times of the basic model, while its minimum normalized cost is 0.5012, which is 0.0797 better than the basic model, and also better than any other results previously reported for this dataset [8, 13].",
                "The rest of the paper is organized as follows.",
                "We start off this paper by summarizing the previous work in NED in section 2.",
                "Section 3 presents the basic model for NED that most current systems use.",
                "Section 4 describes our new detection procedure based on news indexing-tree.",
                "In section 5, two term reweighting methods are proposed to improve NED accuracy.",
                "Section 6 gives our experimental data and evaluation metrics.",
                "We finally wrap up with the experimental results in Section 7, and the conclusions and future work in Section 8. 2.",
                "RELATED WORK Papka et al. proposed Single-Pass clustering on NED [6].",
                "When a new story was encountered, it was processed immediately to extract term features and a query representation of the storys content is built up.",
                "Then it was compared with all the previous queries.",
                "If the document did not trigger any queries by exceeding a threshold, it was marked as a new event.",
                "Lam et al build up previous query representations of story clusters, each of which corresponds to a topic [7].",
                "In this manner comparisons happen between stories and clusters.",
                "Recent years, most work focus on proposing better methods on comparison of stories and document representation.",
                "Brants et al. [8] extended a basic incremental TF-IDF model to include sourcespecific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, term reweighting based on inverse event frequencies, and segmentation of documents.",
                "Good improvements on TDT bench-marks were shown.",
                "Stokes et al. [9] utilized a combination of evidence from two distinct representations of a documents content.",
                "One of the representations was the usual free text vector, the other made use of lexical chains (created using WordNet) to build another term vector.",
                "Then the two representations are combined in a linear fashion.",
                "A marginal increase in effectiveness was achieved when the combined representation was used.",
                "Some efforts have been done on how to utilize named entities to improve NED.",
                "Yang et al. gave location named entities four times weight than other terms and named entities [10].",
                "DOREMI research group combined semantic similarities of person names, location names and time together with textual similarity [11][12].",
                "UMass [13] research group split document representation into two parts: named entities and non-named entities.",
                "And it was found that some classes of news could achieve better performance using named entity representation, while some other classes of news could achieve better performance using non-named entity representation.",
                "Both [10] and [13] used text categorization technique to classify news stories in advance.",
                "In [13] news stories are classified automatically at first, and then test sensitivities of names and non-name terms for NED for each class.",
                "In [10] frequent terms for each class are removed from document representation.",
                "For example, word election does not help identify different elections.",
                "In their work, effectiveness of different kinds of names (or terms with different POS) for NED in different news classes are not investigated.",
                "We use statistical analysis to reveal the fact and use it to improve NED performance. 3.",
                "BASIC MODEL In this section, we present the basic New Event Detection model which is similar to what most current systems apply.",
                "Then, we propose our new model by extending the basic model.",
                "New Event Detection systems use news story stream as input, in which stories are strictly time-ordered.",
                "Only previously received stories are available when dealing with current story.",
                "The output is a decision for whether the current story is on a new event or not and the confidence of the decision.",
                "Usually, a NED model consists of three parts: story representation, similarity calculation and detection procedure. 3.1 Story Representation Preprocessing is needed before generating story representation.",
                "For preprocessing, we tokenize words, recognize abbreviations, normalize abbreviations, add part-of-speech tags, remove stopwords included in the stop list used in InQuery [14], replace words with their stems using K-stem algorithm[15], and then generate word vector for each news story.",
                "We use incremental TF-IDF model for term weight calculation [4].",
                "In a TF-IDF model, term frequency in a news document is weighted by the inverse document frequency, which is generated from training corpus.",
                "When a new term occurs in testing process, there are two solutions: simply ignore the new term or set df of the term as a small const (e.g. df = 1).",
                "The new term receives too low weight in the first solution (0) and too high weight in the second solution.",
                "In incremental TF-IDF model, document frequencies are updated dynamically in each time step t: 1( ) ( ) ( )t t D tdf w df w df w−= + (1) where Dt represents news story set received in time t, and dfDt(w) means the number of documents that term w occurs in, and dft(w) means the total number of documents that term w occurs in before time t. In this work, each time window includes 50 news stories.",
                "Thus, each story d received in t is represented as follows: 1 2{ ( , , ), ( , , ),..., ( , , )}nd weight d t w weight d t w weight d t w→ where n means the number of distinct terms in story d, and ( , , )weight d t w means the weight of term w in story d at time t: log( ( , ) 1) log(( 1) /( ( ) 0.5)) ( , , ) log( ( , ) 1) log(( 1) /( ( ) 0.5)) t t t t w d tf d w N df w weight d t w tf d w N df w ∈ + + + = + + +∑ (2) where Nt means the total number of news stories before time t, and tf(d,w) means how many times term w occurs in news story d. 3.2 Similarity Calculation We use Hellinger distance for the calculation of similarity between two stories, for two stories d and d at time t, their similarity is defined as follows: , ( , , ) ( , , ) * ( , , ) w d d sim d d t weight d t w weight d t w ∈ = ∑ (3) 3.3 Detection Procedure For each story d received in time step t, the value ( ) ( ) ( ) ( ( , , )) time d time d n d max sim d d t < = (4) is a score used to determine whether d is a story about a new topic and at the same time is an indication of the confidence in our decision [8]. time(d) means the publication time of story d. If the score exceeds the thresholdθ new, then there exists a sufficiently similar document, thus d is a old story, otherwise, there is no sufficiently similar previous document, thus d is an new story. 4.",
                "New NED Procedure Traditional NED systems can be classified into two main types on the aspect of detection procedure: (1) S-S type, in which the story on hand is compared to each story received previously, and use the highest similarity to determine whether current story is about a new event; (2) S-C type, in which the story on hand is compared to all previous clusters each of which representing a topic, and the highest similarity is used for final decision for current story.",
                "If the highest similarity exceeds thresholdθ new, then it is an old story, and put it into the most similar cluster; otherwise it is a new story and create a new cluster.",
                "Previous work show that the first manner is more accurate than the second one [4][5].",
                "Since sometimes stories within a topic drift far away from each other, a story may have very low similarity with its topic.",
                "So using similarities between stories for determining new story is better than using similarities between story and clusters.",
                "Nevertheless, the first manner needs much more comparing times which means the first manner is low efficient.",
                "We propose a new detection procedure which uses comparisons with previous clusters to help find the most similar story in less comparing times, and the final new event decision is made according to the most similar story.",
                "Therefore, we can get both the accuracy of S-S type methods and the efficiency of S-C type methods.",
                "The new procedure creates a news indexing-tree dynamically, in which similar stories are put together to form a hierarchy of clusters.",
                "We index similar stories together by their common ancestor (a cluster node).",
                "Dissimilar stories are indexed in different clusters.",
                "When a story is coming, we use comparisons between the current story and previous hierarchical clusters to help find the most similar story which is useful for new event decision.",
                "After the new event decision is made, the current story is inserted to the indexing-tree for the following detection.",
                "The news indexing-tree is defined formally as follows: S-Tree = {r, NC , NS , E} where r is the root of S-Tree, NC is the set of all cluster nodes, NS is the set of all story nodes, and E is the set of all edges in S-Tree.",
                "We define a set of constraints for a S-Tree: ⅰ . , is an non-terminal node in the treeC i i N i∀ ∈ → ⅱ . , is a terminal node in the treeS i i N i∀ ∈ → ⅲ . , out degree of is at least 2C i i N i∀ ∈ → ⅳ . , is represented as the centroid of its desendantsC i i iN∀ ∈ → For a news story di, the comparison procedure and inserting procedure based on indexing-tree are defined as follows.",
                "An example is shown by Figure 1 and Figure 2.",
                "Figure 1.",
                "Comparison procedure Figure 2.",
                "Inserting procedure Comparison procedure: Step 1: compare di to all the direct child nodes of r and select λ nodes with highest similarities, e.g., C1 2 and C1 3 in Figure 1.",
                "Step 2: for each selected node in the last step, e.g.",
                "C1 2, compare di to all its direct child nodes, and select λ nodes with highest similarities, e.g.",
                "C2 2 and d8.",
                "Repeat step 2 for all non-terminal nodes.",
                "Step 3: record the terminal node with the highest similarty to di, e.g. s5, and the similarity value (0.20).",
                "Inserting di to the S-tree with r as root: Find the node n which is direct child of r in the path from r to the terminal node with highest similarity s, e.g.",
                "C1 2.",
                "If s is smaller than θ init+(h-1)δ , then add di to the tree as a direct child of r. Otherwise, if n is a terminal node, then create a cluster node instead of n, and add both n and di as its direct children; if n is an non-terminal node, then repeat this procedure and insert di to the sub-tree with n as root recursively.",
                "Here h is the length between n and the root of S-tree.",
                "The more the stories in a cluster similar to each other, the better the cluster represents the stories in it.",
                "Hence we add no constraints on the maximum of trees height and degree of a node.",
                "Therefore, we cannot give the complexity of this indexing-tree based procedure.",
                "But we will give the number of comparing times needed by the new procedure in our experiments in section7. 5.",
                "Term Reweighting Methods In this section, two term reweighting methods are proposed to improve NED accuracy.",
                "In the first method, a new way is explored for better using of cluster (topic) information.",
                "The second one finds a better way to make use of named entities based on news classification. 5.1 Term Reweighting Based on Distribution Distance TF-IDF is the most prevalent model used in information retrieval systems.",
                "The basic idea is that the fewer documents a term appears in, the more important the term is in discrimination of documents (relevant or not relevant to a query containing the term).",
                "Nevertheless, in TDT domain, we need to discriminate documents with regard to topics rather than queries.",
                "Intuitively, using cluster (topic) vectors to compare with subsequent news stories should outperform using story vectors.",
                "Unfortunately, the experimental results do not support this intuition [4][5].",
                "Based on observation on data, we find the reason is that a news topic usually contains many directly or indirectly related events, while they all have their own sub-subjects which are usually different with each other.",
                "Take the topic described in section 1 as an example, events like the explosion and salvage have very low similarities with events about criminal trial, therefore stories about trial would have low similarity with the topic vector built on its previous events.",
                "This section focuses on how to effectively make use of topic information and at the same time avoid the problem of content decentralization.",
                "At first, we classify terms into 5 classes to help analysis the needs of the modified model: Term class A: terms that occur frequently in the whole corpus, e.g., year and people.",
                "Terms of this class should be given low weights because they do not help much for topic discrimination.",
                "Term class B: terms that occur frequently within a news category, e.g., election, storm.",
                "They are useful to distinguish two stories in different news categories.",
                "However, they cannot provide information to determine whether two stories are on the same or different topics.",
                "In another words, term election and term storm are not helpful in differentiate two election campaigns and two storm disasters.",
                "Therefore, terms of this class should be assigned lower weights.",
                "Term class C: terms that occur frequently in a topic, and infrequently in other topics, e.g., the name of a crash plane, the name of a specific hurricane.",
                "News stories that belong to different topics rarely have overlap terms in this class.",
                "The more frequently a term appears in a topic, the more important the term is for a story belonging to the topic, therefore the term should be set higher weight.",
                "Term class D: terms that appear in a topic exclusively, but not frequently.",
                "For example, the name of a fireman who did very well in a salvage action, which may appears in only two or three stories but never appeared in other topics.",
                "Terms of this type should receive more weights than in TF-IDF model.",
                "However, since they are not popular in the topic, it is not appropriate to give them too high weights.",
                "Term class E: terms with low document frequency, and appear in different topics.",
                "Terms of this class should receive lower weights.",
                "Now we analyze whether TF-IDF model can give proper weights to the five classes of terms.",
                "Obviously, terms of class A are lowly weighted in TF-IDF model, which is conformable with the requirement described above.",
                "In TF-IDF model, terms of class B are highly dependant with the number of stories in a news class.",
                "TF-IDF model cannot provide low weights if the story containing the term belongs to a relative small news class.",
                "For a term of class C, the more frequently it appears in a topic, the less weight TFIDF model gives to it.",
                "This strongly conflicts with the requirement of terms in class C. For terms of class D, TF-IDF model gives them high weights correctly.",
                "But for terms of class E, TF-IDF model gives high weights to them which are not conformable with the requirement of low weights.",
                "To sum up, terms of class B, C, E cannot be properly weighted in TF-IDF model.",
                "So, we propose a modified model to resolve this problem.",
                "When θ init andθ new are set closely, we assume that most of the stories in a first-level cluster (a direct child node of root node) are on the same topic.",
                "Therefore, we make use of a first-level cluster to capture term distribution (df for all the terms within the cluster) within the topic dynamically.",
                "KL divergence of term distribution in a first-level cluster and the whole story set is used to adjust term weights:  ( , , ) * (1 * ( || )) ( , , ) ( , , ) * (1 * ( || )) cw tw cw tw w d D weight d t w KL P P weight d t w weight d t w KL P P γ γ ∈ + = +∑ (5) where ( ) ( ) ( ) ( ) 1,cw cw c c c c df w df w p y p y N N = = − (6) ( ) ( ) ( ) ( ) 1,t t tw tw t t df w df w p y p y N N = = − (7) where dfc(w) is the number of documents containing term w within cluster C, and Nc is the number of documents in cluster C, and Nt is the total number of documents that arrive before time step t. γ is a const parameter, now is manually set 3.",
                "KL divergence is defined as follows [17]: ( ) ( || ) ( ) log ( )x p x KL P Q p x q x = ∑ (8) The basic idea is: for a story in a topic, the more a term occurs within the topic, and the less it occurs in other topics, it should be assigned higher weights.",
                "Obviously, modified model can meet all the requirements of the five term classes listed above. 5.2 Term Reweighting Based on Term Type and Story Class Previous work found that some classes of news stories could achieve good improvements by giving extra weight to named entities.",
                "But we find that terms of different types should be given different amount of extra weight for different classes of news stories.",
                "We use open-NLP1 to recognize named entity types and part-ofspeech tags for terms that appear in news stories.",
                "Named entity types include person name, organization name, location name, date, time, money and percentage, and five POSs are selected: none (NN), verb (VB), adjective (JJ), adverb (RB) and cardinal number (CD).",
                "Statistical analysis shows topic-level discriminative terms types for different classes of stories.",
                "For the sake of convenience, named entity type and part-of-speech tags are uniformly called term type in subsequent sections.",
                "Determining whether two stories are about the same topic is a basic component for NED task.",
                "So at first we use 2 χ statistic to compute correlations between terms and topics.",
                "For a term t and a topic T, a contingence table is derived: Table 1.",
                "A 2×2 Contingence Table Doc Number belong to topic T not belong to topic T include t A B not include t C D The 2 χ statistic for a specific term t with respect to topic T is defined to be [16]: 2 2 ( , ) ( ) * ( ) ( ) * ( ) * ( ) * ( ) w T A B C D AD CB A C B D A B C D χ = + + + − + + + + (9) News topics for the TDT task are further classified into 11 rules of interpretations (ROIs) 2 .",
                "The ROI can be seen as a higher level class of stories.",
                "The average correlation between a term type and a topic ROI is computed as: 2 avg 2 ( , )( ( , ) )k m m km kT R w P w TP R p w T R P χ χ ∈ ∈ ∑ ∑（ , ）= 1 1 k=1…K, m=1…M (10) where K is the number of term types (set 12 constantly in the paper).",
                "M is the number news classes (ROIs, set 11 in the paper).",
                "Pk represents the set of all terms of type k, and Rm represents the set of all topics of class m, p(t,T) means the probability that t occurs in topic T. Because of limitation of space, only parts of the term types (9 term types) and parts of news classes (8 classes) are listed in table 2 with the average correlation values between them.",
                "The statistics is derived from labeled data in TDT2 corpus. (Results in table 2 are already normalized for convenience in comparison.)",
                "The statistics in table 2 indicates the usefulness of different term types in topic discrimination with respect to different news classes.",
                "We can see that, location name is the most useful term type for three news classes: Natural Disasters, Violence or War, Finances.",
                "And for three other categories Elections, Legal/Criminal Cases, Science and Discovery, person name is the most discriminative term type.",
                "For Scandals/Hearings, date is the most important information for topic discrimination.",
                "In addition, Legal/Criminal Cases and Finance topics have higher correlation with money terms, while Science and Discovery have higher correlation with percentage terms.",
                "Non-name terms are more stable for different classes. 1 . http://opennlp.sourceforge.net/ 2 . http://projects.ldc.upenn.edu/TDT3/Guide/label.html From the analysis of table 2, it is reasonable to adjust term weight according to their term type and the news class the story belongs to.",
                "New term weights are reweighted as follows: ( ) ( ) ( ) ( ) ( , , ) * ( , , ) ( , , ) * class d D type w T class d D type w w d weight d t w weight d t w weight d t w α α ∈ = ∑ (11) where type(w) represents the type of term w, and class(d) represents the class of story d, c kα is reweighting parameter for news class c and term type k. In the work, we just simply use statistics in table 2 as the reweighting parameters.",
                "Even thought using the statistics directly may not the best choice, we do not discuss how to automatically obtain the best parameters.",
                "We will try to use machine learning techniques to obtain the best parameters in the future work.",
                "In the work, we use BoosTexter [20] to classify all stories into one of the 11 ROIs.",
                "BoosTexter is a boosting based machine learning program, which creates a series of simple rules for building a classifier for text or attribute-value data.",
                "We use term weight generated using TF-IDF model as feature for story classification.",
                "We trained the model on the 12000 judged English stories in TDT2, and classify the rest of the stories in TDT2 and all stories in TDT3.",
                "Classification results are used for term reweighting in formula (11).",
                "Since the class labels of topic-off stories are not given in TDT datasets, we cannot give the classification accuracy here.",
                "Thus we do not discuss the effects of classification accuracy to NED performance in the paper. 6.",
                "EXPERIMENTAL SETUP 6.1 Datasets We used two LDC [18] datasets TDT2 and TDT3 for our experiments.",
                "TDT2 contains news stories from January to June 1998.",
                "It contains around 54,000 stories from sources like ABC, Associated Press, CNN, New York Times, Public Radio International, Voice of America etc.",
                "Only English stories in the collection were considered.",
                "TDT3 contains approximately 31,000 English stories collected from October to December 1998.",
                "In addition to the sources used in TDT2, it also contains stories from NBC and MSNBC TV broadcasts.",
                "We used transcribed versions of the TV and radio broadcasts besides textual news.",
                "TDT2 dataset is labeled with about 100 topics, and approximately 12,000 English stories belong to at least one of these topics.",
                "TDT3 dataset is labeled with about 120 topics, and approximately 8000 English stories belong to at least one of these topics.",
                "All the topics are classified into 11 Rules of Interpretation: (1)Elections, (2)Scandals/Hearings, (3)Legal/Criminal Cases, (4)Natural Disasters, (5)Accidents, (6)Ongoing Violence or War, (7)Science and Discovery News, (8)Finance, (9)New Law, (10)Sports News, (11)MISC.",
                "News. 6.2 Evaluation Metric TDT uses a cost function CDet that combines the probabilities of missing a new story and a false alarm [19]: * * * *Det Miss Miss Target FA FA NontargetC C P P C P P= + (12) Table 2.",
                "Average correlation between term types and news classes where CMiss means the cost of missing a new story, PMiss means the probability of missing a new story, and PTarget means the probability of seeing a new story in the data; CFA means the cost of a false alarm, PFA means the probability of a false alarm, and PNontarget means the probability of seeing an old story.",
                "The cost CDet is normalized such that a perfect system scores 0 and a trivial system, which is the better one of mark all stories as new or old, scores 1: ( ( * , * ) ) Det Det Miss Target FA Nontarget C Norm C min C P C P = (13) New event detection system gives two outputs for each story.",
                "The first part is yes or no indicating whether the story triggers a new event or not.",
                "The second part is a score indicating confidence of the first decision.",
                "Confidence scores can be used to plot DET curve, i.e., curves that plot false alarm vs. miss probabilities.",
                "Minimum normalized cost can be determined if optimal threshold on the score were chosen. 7.",
                "EXPERIMENTAL RESULTS 7.1 Main Results To test the approaches proposed in the model, we implemented and tested five systems: System-1: this system is used as baseline.",
                "It is implemented based on the basic model described in section 3, i.e., using incremental TF-IDF model to generate term weights, and using Hellinger distance to compute document similarity.",
                "Similarity score normalization is also employed [8].",
                "S-S detection procedure is used.",
                "System-2: this system is the same as system-1 except that S-C detection procedure is used.",
                "System-3: this system is the same as system-1 except that it uses the new detection procedure which is based on indexing-tree.",
                "System-4: implemented based on the approach presented in section 5.1, i.e., terms are reweighted according to the distance between term distributions in a cluster and all stories.",
                "The new detection procedure is used.",
                "System-5: implemented based on the approach presented in section 5.2, i.e., terms of different types are reweighted according to news class using trained parameters.",
                "The new detection procedure is used.",
                "The following are some other NED systems: System-6: [21] for each pair of stories, it computes three similarity values for named entity, non-named entity and all terms respectively.",
                "And employ Support Vector Machine to predict new or old using the similarity values as features.",
                "System-7: [8] it extended a basic incremental TF-IDF model to include source-specific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, etc.",
                "System-8: [13] it split document representation into two parts: named entities and non-named entities, and choose one effective part for each news class.",
                "Table 3 and table 4 show topic-weighted normalized costs and comparing times on TDT2 and TDT3 datasets respectively.",
                "Since no heldout data set for fine-tuning the threshold θ new was available for experiments on TDT2, we only report minimum normalized costs for our systems in table 3.",
                "System-5 outperforms all other systems including system-6, and it performs only 2.78e+8 comparing times in detection procedure which is only 13.4% of system-1.",
                "Table 3.",
                "NED results on TDT2 Systems Min Norm(CDet) Cmp times System-1 0.5749 2.08e+9 System-2① 0.6673 3.77e+8 System-3② 0.5765 2.81e+8 System-4② 0.5431 2.99e+8 System-5② 0.5089 2.78e+8 System-6 0.5300 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 When evaluating on the normalized costs on TDT3, we use the optimal thresholds obtained from TDT2 data set for all systems.",
                "System-2 reduces comparing times to 1.29e+9 which is just 18.3% of system-1, but at the same time it also gets a deteriorated minimum normalized cost which is 0.0499 higher than system-1.",
                "System-3 uses the new detection procedure based on news indexing-tree.",
                "It requires even less comparing times than system-2.",
                "This is because story-story comparisons usually yield greater similarities than story-cluster ones, so stories tend to be combined Location Person Date Organization Money Percentage NN JJ CD Elections 0.37 1 0.04 0.58 0.08 0.03 0.32 0.13 0.1 Scandals/Hearings 0.66 0.62 0.28 1 0.11 0.02 0.27 0.13 0.05 Legal/Criminal Cases 0.48 1 0.02 0.62 0.15 0 0.22 0.24 0.09 Natural Disasters 1 0.27 0 0.04 0.04 0 0.25 0.04 0.02 Violence or War 1 0.36 0.02 0.14 0.02 0.04 0.21 0.11 0.02 Science and Discovery 0.11 1 0.01 0.22 0.08 0.12 0.19 0.08 0.03 Finances 1 0.45 0.04 0.98 0.13 0.02 0.29 0.06 0.05 Sports 0.16 0.27 0.01 1 0.02 0 0.11 0.03 0.01 together in system-3.",
                "And system-3 is basically equivalent to system-1 in accuracy results.",
                "System-4 adjusts term weights based on the distance of term distributions between the whole corpus and cluster story set, yielding a good improvement by 0.0468 compared to system-1.",
                "The best system (system-5) has a minimum normalized cost 0.5012, which is 0.0797 better than system-1, and also better than any other results previously reported for this dataset [8, 13].",
                "Further more, system-5 only needs 1.05e+8 comparing times which is 14.9% of system-1.",
                "Table 4.",
                "NED results on TDT3 Systems Norm(CDet) Min Norm(CDet) Cmp times System-1 0.6159 0.5809 7.04e+8 System-2① 0.6493 0.6308 1.29e+8 System-3② 0.6197 0.5868 1.03e+8 System-4② 0.5601 0.5341 1.03e+8 System-5② 0.5413 0.5012 1.05e+8 System-7 -- 0.5783 -System-8 -- 0.5229 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 Figure5 shows the five DET curves for our systems on data set TDT3.",
                "System-5 achieves the minimum cost at a false alarm rate of 0.0157 and a miss rate of 0.4310.",
                "We can observe that System4 and System-5 obtain lower miss probability at regions of low false alarm probabilities.",
                "The hypothesis is that, more weight value is transferred to key terms of topics from non-key terms.",
                "Similarity score between two stories belonging to different topics are lower than before, because their overlapping terms are usually not key terms of their topics. 7.2 Parameter selection for indexing-tree detection Figure 3 shows the minimum normalized costs obtained by system-3 on TDT3 using different parameters.",
                "Theθ init parameter is tested on six values spanning from 0.03 to 0.18.",
                "And the λ parameter is tested on four values 1, 2, 3 and 4.",
                "We can see that, whenθ init is set to 0.12, which is the closest one toθ new, the costs are lower than others.",
                "This is easy to explain, because when stories belonging to the same topic are put in a cluster, it is more reasonable for the cluster to represent the stories in it.",
                "When parameter λ is set to 3 or 4, the costs are better than other cases, but there is no much difference between 3 and 4. 0 0.05 0.1 0.15 0.2 1 2 3 4 0.5 0.6 0.7 0.8 0.9 1 θ-initλ MinCost 0.6 0.65 0.7 0.75 0.8 0.85 0.9 Figure 3.",
                "Min Cost on TDT3 (δ =0.15) 0 0.05 0.1 0.15 0.2 1 2 3 4 0 0.5 1 1.5 2 2.5 x 10 8 θ-init λ Comparingtimes 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 8 Figure 4.",
                "Comparing times on TDT3 (δ =0.15) Figure 4 gives the comparing times used by system-3 on TDT3 with the same parameters as figure 3.",
                "The comparing times are strongly dependent onθ init.",
                "Because the greaterθ init is, the less stories combined together, the more comparing times are needed for new event decision.",
                "So we useθ init =0.13,λ =3,δ =0.15 for system-3, 4, and 5.",
                "In this parameter setting, we can get both low minimum normalized costs and less comparing times. 8.",
                "CONCLUSION We have proposed a news indexing-tree based detection procedure in our model.",
                "It reduces comparing times to about one seventh of traditional method without hurting NED accuracy.",
                "We also have presented two extensions to the basic TF-IDF model.",
                "The first extension is made by adjust term weights based on term distributions between the whole corpus and a cluster story set.",
                "And the second extension to basic TF-IDF model is better use of term types (named entities types and part-of-speed) according to news categories.",
                "Our experimental results on TDT2 and TDT3 datasets show that both of the two extensions contribute significantly to improvement in accuracy.",
                "We did not consider news time information as a clue for NED task, since most of the topics last for a long time and TDT data sets only span for a relative short period (no more than 6 months).",
                "For the future work, we want to collect news set which span for a longer period from internet, and integrate time information in NED task.",
                "Since topic is a relative coarse-grained news cluster, we also want to refine cluster granularity to event-level, and identify different events and their relations within a topic.",
                "Acknowledgments This work is supported by the National Natural Science Foundation of China under Grant No. 90604025.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsor. 9.",
                "REFERENCES [1] http://www.nist.gov/speech/tests/tdt/index.htm [2] In Topic Detection and Tracking.",
                "Event-based Information Organization.",
                "Kluwer Academic Publishers, 2002. .01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90 1 2 5 10 20 40 60 80 90 False Alarm Probability (in %) MissProbability(in%) SYSTEM1 Topic Weighted Curve SYSTEM1 Min Norm(Cost) SYSTEM2 Topic Weighted Curve SYSTEM2 Min Norm(Cost) SYSTEM3 Topic Weighted Curve SYSTEM3 Min Norm(Cost) SYSTEM4 Topic Weighted Curve SYSTEM4 Min Norm(Cost) SYSTEM5 Topic Weighted Curve SYSTEM5 Min Norm(Cost) Random Performance Figure 5.",
                "DET curves on TDT3 [3] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B.T.",
                "Archibald, and X. Liu.",
                "Learning Approaches for Detecting and Tracking News Events.",
                "In IEEE Intelligent Systems Special Issue on Applications of Intelligent Information Retrieval, volume 14 (4), 1999, 32-43. [4] Y. Yang, T. Pierce, and J. Carbonell.",
                "A Study on Retrospective and On-line Event Detection.",
                "In Proceedings of SIGIR-98, Melbourne, Australia, 1998, 28-36. [5] J. Allan, V. Lavrenko, D. Malin, and R. Swan.",
                "Detections, Bounds, and Timelines: Umass and tdt-3.",
                "In Proceedings of Topic Detection and Tracking Workshop (TDT-3), Vienna, VA, 2000, 167-174. [6] R. Papka and J. Allan.",
                "On-line New Event Detection Using Single Pass Clustering TITLE2:.",
                "Technical Report UM-CS1998-021, 1998. [7] W. Lam, H. Meng, K. Wong, and J.",
                "Yen.",
                "Using Contextual Analysis for News Event Detection.",
                "International Journal on Intelligent Systems, 2001, 525-546. [8] B. Thorsten, C. Francine, and F. Ayman.",
                "A System for New Event Detection.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2003, 330-337. [9] S. Nicola and C. Joe.",
                "Combining Semantic and Syntactic Document Classifiers to Improve First Story Detection.",
                "In Proceedings of the 24th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topicconditioned Novelty Detection.",
                "In Proceedings of the 8th ACM SIGKDD International Conference, ACM Press. 2002, 688-693. [11] M. Juha, A.M. Helena, and S. Marko.",
                "Applying Semantic Classes in Event Detection and Tracking.",
                "In Proceedings of International Conference on Natural Language Processing (ICON 2002), 2002, pages 175-183. [12] M. Juha, A.M. Helena, and S. Marko.",
                "Simple Semantics in Topic Detection and Tracking.",
                "Information Retrieval, 7(3-4): 2004, 347-368. [13] K. Giridhar and J. Allan.",
                "Text Classification and Named Entities for New Event Detection.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2004, 297-304. [14] J. P. Callan, W. B. Croft, and S. M. Harding.",
                "The INQUERY Retrieval System.",
                "In Proceedings of DEXA-92, 3rd International Conference on Database and Expert Systems Applications, 1992, 78-83. [15] R. Krovetz.",
                "Viewing Morphology as An Inference Process.",
                "In Proceedings of ACM SIGIR93, 1993, 61-81. [16] Y. Yang and J. Pedersen.",
                "A Comparative Study on Feature Selection in Text Categorization.",
                "In J. D. H. Fisher, editor, The Fourteenth International Conference on Machine Learning (ICML97), Morgan Kaufmann, 1997, 412-420. [17] T. M. Cover, and J.A.",
                "Thomas.",
                "Elements of Information Theory.",
                "Wiley. 1991. [18] The linguistic data consortium, http://www.ldc,upenn.edu/. [19] The 2001 TDT task definition and evaluation plan, http://www.nist.gov/speech/tests/tdt/tdt2001/evalplan.htm. [20] R. E. Schapire and Y.",
                "Singer.",
                "Boostexter: A Boosting-based System for Text Categorization.",
                "In Machine Learning 39(2/3):1, Kluwer Academic Publishers, 2000, 35-168. [21] K. Giridhar and J. Allan. 2005.",
                "Using Names and Topics for New Event Detection.",
                "In Proceedings of Human Technology Conference and Conference on Empirical Methods in Natural Language, Vancouver, 2005, 121-128"
            ],
            "original_annotated_samples": [
                "New Event Detection Based on Indexing-tree and Named Entity Zhang Kuo Tsinghua University Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Tsinghua University Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Tsinghua University Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn ABSTRACT New Event Detection (NED) aims at detecting from one or multiple <br>streams of news stories</br> that which one is reported on a new event (i.e."
            ],
            "translated_annotated_samples": [
                "Detección de nuevos eventos basada en árbol de indexación y entidad nombrada Zhang Kuo Universidad Tsinghua Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Universidad Tsinghua Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Universidad Tsinghua Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn RESUMEN La Detección de Nuevos Eventos (NED) tiene como objetivo detectar de una o varias <br>corrientes de noticias</br> cuál se informa sobre un nuevo evento (es decir,"
            ],
            "translated_text": "Detección de nuevos eventos basada en árbol de indexación y entidad nombrada Zhang Kuo Universidad Tsinghua Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Universidad Tsinghua Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Universidad Tsinghua Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn RESUMEN La Detección de Nuevos Eventos (NED) tiene como objetivo detectar de una o varias <br>corrientes de noticias</br> cuál se informa sobre un nuevo evento (es decir, no reportado previamente). Con el abrumador volumen de noticias disponible hoy en día, hay una creciente necesidad de un sistema NED que sea capaz de detectar eventos nuevos de manera más eficiente y precisa. En este artículo proponemos un nuevo modelo de NED para acelerar la tarea de NED mediante el uso dinámico de un árbol de indexación de noticias. Además, basándose en la observación de que los términos de diferentes tipos tienen diferentes efectos para la tarea de NED, se proponen dos enfoques de reponderación de términos para mejorar la precisión de NED. En el primer enfoque, proponemos ajustar dinámicamente los pesos de los términos basados en los clusters de historias previas y en el segundo enfoque, proponemos emplear estadísticas en los datos de entrenamiento para aprender el modelo de reajuste de entidades nombradas para cada clase de historias. Los resultados experimentales en dos conjuntos de datos del Consorcio de Datos Lingüísticos (LDC), TDT2 y TDT3, muestran que el modelo propuesto puede mejorar significativamente tanto la eficiencia como la precisión de la tarea de NED, en comparación con el sistema base y otros sistemas existentes. Categorías y Descriptores de Asignaturas H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información; H.4.2 [Aplicaciones de Sistemas de Información]: Tipos de Sistemas de Apoyo a Decisiones. Términos generales Algoritmos, Rendimiento, Experimentación 1. El programa de Detección y Seguimiento de Temas (TDT) tiene como objetivo desarrollar técnicas que puedan organizar, buscar y estructurar de manera efectiva materiales de texto de noticias de una variedad de agencias de noticias y medios de difusión [1]. La Detección de Eventos Nuevos (DEN) es una de las cinco tareas en TDT. Es la tarea de identificación en línea del informe más temprano para cada tema tan pronto como ese informe llegue en la secuencia de documentos. Un tema se define como un evento o actividad seminal, junto con eventos y actividades directamente relacionados [2]. Un evento se define como algo (no trivial) que sucede en un lugar y en un momento determinados [3]. Por ejemplo, cuando una bomba explota en un edificio, la explosión es el evento seminal que desencadena el tema, y otras historias sobre el mismo tema serían aquellas que discuten los esfuerzos de salvamento, la búsqueda de los perpetradores, arrestos y juicios, entre otros. La información útil de noticias suele estar enterrada en una masa de datos generados a diario. Por lo tanto, los sistemas NED son muy útiles para las personas que necesitan detectar información novedosa de un flujo de noticias en tiempo real. Estas necesidades de la vida real a menudo se presentan en ámbitos como los mercados financieros, el análisis de noticias y la recopilación de inteligencia. En la mayoría de los sistemas NED de última generación, cada noticia disponible se compara con todas las noticias previamente recibidas. Si todas las similitudes entre ellos no superan un umbral, entonces la historia desencadena un nuevo evento. Por lo general, se presentan en forma de similitud de coseno o métrica de similitud de Hellinger. El problema central de NED es identificar si dos historias tratan sobre el mismo tema. Obviamente, estos sistemas no pueden aprovechar la información del tema. Además, no es aceptable en aplicaciones reales debido a la gran cantidad de cálculos requeridos en el proceso de NED. Otros sistemas organizan historias anteriores en grupos (cada grupo corresponde a un tema), y la nueva historia se compara con los grupos anteriores en lugar de con historias. Esta forma puede reducir significativamente los tiempos de comparación. Sin embargo, se ha demostrado que este método es menos preciso [4, 5]. Esto se debe a que a veces las historias dentro de un tema se alejan mucho entre sí, lo que podría resultar en una baja similitud entre una historia y su tema. Por otro lado, algunos sistemas NED propuestos intentaron mejorar la precisión al hacer un mejor uso de las entidades nombradas [10, 11, 12, 13]. Sin embargo, ninguno de los sistemas ha considerado que los términos de diferentes tipos (por ejemplo, Los sustantivos, verbos o nombres de personas tienen diferentes efectos en diferentes clases de historias para determinar si dos historias tratan sobre el mismo tema. Por ejemplo, los nombres de los candidatos electorales (Nombre de la persona) son muy importantes para las historias de la clase de elecciones; las ubicaciones (Nombre de la ubicación) donde ocurrieron los accidentes son importantes para las historias de la clase de accidentes. Por lo tanto, en NED, todavía existen los siguientes tres problemas por investigar: (1) ¿Cómo acelerar el procedimiento de detección sin disminuir la precisión de la detección? (2) ¿Cómo aprovechar mejor la información de los grupos (temas) para mejorar la precisión? (3) ¿Cómo obtener una mejor representación de historias de noticias mediante una mejor comprensión de las entidades nombradas? Impulsados por estos problemas, hemos propuesto tres enfoques en este documento. (1) Para hacer que el procedimiento de detección sea más rápido, proponemos un nuevo procedimiento de NED basado en un árbol de indexación de noticias creado dinámicamente. El índice de historias en árbol se crea ensamblando historias similares para formar grupos de noticias en diferentes jerarquías según sus valores de similitud. Las comparaciones entre la historia actual y los grupos anteriores podrían ayudar a encontrar la historia más similar en menos tiempo de comparación. El nuevo procedimiento puede reducir la cantidad de veces de comparación sin afectar la precisión. Utilizamos los grupos del primer piso en el árbol de indexación como temas de noticias, en los cuales los pesos de los términos se ajustan dinámicamente según la distribución de términos en los grupos. En este enfoque, se utiliza adecuadamente la información de los grupos (temas), por lo que se evita el problema de la descentralización del tema. Basándonos en observaciones de las estadísticas obtenidas de los datos de entrenamiento, encontramos que los términos de diferentes tipos (por ejemplo, Los sustantivos y verbos tienen diferentes efectos para diferentes clases de historias al determinar si dos historias tratan sobre el mismo tema. Y proponemos utilizar estadísticas para optimizar los pesos de los términos de diferentes tipos en una historia de acuerdo a la clase de noticias a la que pertenece la historia. En el conjunto de datos TDT3, el nuevo modelo NED solo utiliza un 14.9% en comparación con los tiempos del modelo básico, mientras que su costo normalizado mínimo es de 0.5012, lo que es 0.0797 mejor que el modelo básico, y también mejor que cualquier otro resultado previamente reportado para este conjunto de datos [8, 13]. El resto del documento está organizado de la siguiente manera. Comenzamos este artículo resumiendo el trabajo previo en NED en la sección 2. La sección 3 presenta el modelo básico para NED que la mayoría de los sistemas actuales utilizan. La sección 4 describe nuestro nuevo procedimiento de detección basado en el índice de árbol de noticias. En la sección 5, se proponen dos métodos de reponderación de términos para mejorar la precisión de NED. La sección 6 presenta nuestros datos experimentales y métricas de evaluación. Finalmente concluimos con los resultados experimentales en la Sección 7, y las conclusiones y trabajo futuro en la Sección 8. TRABAJO RELACIONADO Papka et al. propusieron el agrupamiento de un solo paso en NED [6]. Cuando se encontraba una nueva historia, esta era procesada inmediatamente para extraer características de términos y se construía una representación de consulta del contenido de la historia. Luego se comparó con todas las consultas anteriores. Si el documento no generaba ninguna consulta al exceder un umbral, se marcaba como un nuevo evento. Lam et al construyen representaciones de consultas previas de grupos de historias, cada una de las cuales corresponde a un tema [7]. De esta manera se realizan comparaciones entre historias y grupos. En los últimos años, la mayoría de los trabajos se han centrado en proponer mejores métodos para la comparación de historias y la representación de documentos. Brants et al. [8] extendieron un modelo básico incremental de TF-IDF para incluir modelos específicos de fuente, normalización de puntajes de similitud basada en promedios específicos de documentos, normalización de puntajes de similitud basada en promedios específicos de pares de fuentes, reponderación de términos basada en frecuencias de eventos inversas y segmentación de documentos. Se mostraron buenas mejoras en los puntos de referencia de TDT. Stokes et al. [9] utilizaron una combinación de evidencia de dos representaciones distintas del contenido de un documento. Una de las representaciones era el vector de texto libre habitual, la otra hacía uso de cadenas léxicas (creadas utilizando WordNet) para construir otro vector de términos. Luego, las dos representaciones se combinan de forma lineal. Se logró un aumento marginal en la efectividad cuando se utilizó la representación combinada. Algunos esfuerzos se han realizado sobre cómo utilizar entidades nombradas para mejorar la desambiguación de entidades nombradas (NED). Yang et al. asignaron un peso cuatro veces mayor a las entidades con nombres de ubicación que a otros términos y entidades con nombres [10]. El grupo de investigación DOREMI combinó similitudes semánticas de nombres de personas, nombres de lugares y tiempo junto con similitud textual [11][12]. El grupo de investigación de UMass [13] dividió la representación del documento en dos partes: entidades nombradas y entidades no nombradas. Y se descubrió que algunas clases de noticias podían lograr un mejor rendimiento utilizando la representación de entidades nombradas, mientras que otras clases de noticias podían lograr un mejor rendimiento utilizando la representación sin entidades nombradas. Tanto [10] como [13] utilizaron la técnica de categorización de texto para clasificar previamente las noticias. En [13] las noticias se clasifican automáticamente al principio, y luego se prueban las sensibilidades de los nombres y términos que no son nombres para NED para cada clase. En [10], se eliminan los términos frecuentes de cada clase de la representación del documento. Por ejemplo, la palabra \"elección\" no ayuda a identificar diferentes elecciones. En su trabajo, no se investiga la efectividad de diferentes tipos de nombres (o términos con diferentes categorías gramaticales) para NED en diferentes clases de noticias. Utilizamos análisis estadístico para revelar el hecho y utilizarlo para mejorar el rendimiento de NED. 3. MODELO BÁSICO En esta sección, presentamos el modelo básico de Detección de Nuevos Eventos que es similar a lo que la mayoría de los sistemas actuales aplican. Luego, proponemos nuestro nuevo modelo ampliando el modelo básico. Los sistemas de detección de eventos nuevos utilizan flujos de noticias como entrada, en los cuales las historias están estrictamente ordenadas por tiempo. Solo están disponibles las historias recibidas previamente al tratar con la historia actual. La salida es una decisión sobre si la historia actual es un evento nuevo o no y la confianza de la decisión. Por lo general, un modelo NED consta de tres partes: representación de la historia, cálculo de similitud y procedimiento de detección. 3.1 Representación de la historia Se necesita preprocesamiento antes de generar la representación de la historia. Para el preprocesamiento, tokenizamos palabras, reconocemos abreviaturas, normalizamos abreviaturas, agregamos etiquetas de partes del discurso, eliminamos palabras vacías incluidas en la lista de paradas utilizada en InQuery [14], reemplazamos palabras con sus raíces utilizando el algoritmo K-stem[15], y luego generamos un vector de palabras para cada noticia. Utilizamos el modelo TF-IDF incremental para el cálculo del peso de los términos [4]. En un modelo TF-IDF, la frecuencia de término en un documento de noticias se pondera por la frecuencia inversa del documento, la cual se genera a partir del corpus de entrenamiento. Cuando surge un nuevo término en el proceso de prueba, hay dos soluciones: simplemente ignorar el nuevo término o establecer df del término como una constante pequeña (por ejemplo, df = 1). El nuevo término recibe un peso demasiado bajo en la primera solución (0) y un peso demasiado alto en la segunda solución. En el modelo TF-IDF incremental, las frecuencias de documentos se actualizan dinámicamente en cada paso de tiempo t: 1( ) ( ) ( )t t D tdf w df w df w−= + (1) donde Dt representa el conjunto de noticias recibidas en el tiempo t, y dfDt(w) significa el número de documentos en los que aparece el término w, y dft(w) significa el número total de documentos en los que aparece el término w antes del tiempo t. En este trabajo, cada ventana de tiempo incluye 50 noticias. Así, cada historia d recibida en t se representa de la siguiente manera: 1 2{ ( , , ), ( , , ),..., ( , , )}nd peso d t w peso d t w peso d t w→ donde n significa el número de términos distintos en la historia d, y ( , , )peso d t w significa el peso del término w en la historia d en el tiempo t: log( ( , ) 1) log(( 1) /( ( ) 0.5)) ( , , ) log( ( , ) 1) log(( 1) /( ( ) 0.5)) t t t t w d tf d w N df w peso d t w tf d w N df w ∈ + + + = + + +∑ (2) donde Nt significa el número total de historias de noticias antes del tiempo t, y tf(d,w) significa cuántas veces aparece el término w en la historia de noticias d. 3.2 Cálculo de Similitud Utilizamos la distancia de Hellinger para el cálculo de la similitud entre dos historias, para dos historias d y d en el tiempo t, su similitud se define de la siguiente manera: , ( , , ) ( , , ) * ( , , ) w d d sim d d t peso d t w peso d t w ∈ = ∑ (3) 3.3 Procedimiento de Detección Para cada historia d recibida en el paso de tiempo t, el valor ( ) ( ) ( ) ( ( , , )) tiempo d tiempo d n d max sim d d t < = (4) es un puntaje utilizado para determinar si d es una historia sobre un nuevo tema y al mismo tiempo es una indicación de la confianza en nuestra decisión [8]. tiempo(d) significa el tiempo de publicación de la historia d. Si el puntaje supera el umbral θ new, entonces existe un documento suficientemente similar, por lo tanto, d es una historia antigua, de lo contrario, no hay un documento previo suficientemente similar, por lo tanto, d es una historia nueva. 4. Nuevo procedimiento NED Los sistemas NED tradicionales se pueden clasificar en dos tipos principales en cuanto al procedimiento de detección: (1) tipo S-S, en el que la historia en cuestión se compara con cada historia recibida previamente, y se utiliza la mayor similitud para determinar si la historia actual trata sobre un evento nuevo; (2) tipo S-C, en el que la historia en cuestión se compara con todos los clusters anteriores, cada uno de los cuales representa un tema, y se utiliza la mayor similitud para la decisión final sobre la historia actual. Si la similitud más alta supera el umbral θ nuevo, entonces es una historia antigua y se coloca en el clúster más similar; de lo contrario, es una historia nueva y se crea un nuevo clúster. Trabajos anteriores muestran que la primera forma es más precisa que la segunda [4][5]. Dado que a veces las historias dentro de un tema se alejan mucho entre sí, una historia puede tener una similitud muy baja con su tema. Por lo tanto, utilizar similitudes entre historias para determinar una nueva historia es mejor que utilizar similitudes entre historias y grupos. Sin embargo, el primer método requiere mucho más tiempo de comparación, lo que significa que es menos eficiente. Proponemos un nuevo procedimiento de detección que utiliza comparaciones con clusters anteriores para ayudar a encontrar la historia más similar en menos tiempo de comparación, y la decisión final sobre el nuevo evento se toma de acuerdo con la historia más similar. Por lo tanto, podemos obtener tanto la precisión de los métodos de tipo S-S como la eficiencia de los métodos de tipo S-C. El nuevo procedimiento crea un árbol de indexación de noticias dinámicamente, en el cual se agrupan historias similares para formar una jerarquía de clusters. Indexamos historias similares juntas por su ancestro común (un nodo de clúster). Historias diferentes están indexadas en diferentes grupos. Cuando una historia está llegando, utilizamos comparaciones entre la historia actual y los grupos jerárquicos anteriores para ayudar a encontrar la historia más similar que es útil para la toma de decisiones sobre nuevos eventos. Después de que se tome la decisión sobre el nuevo evento, la historia actual se inserta en el árbol de indexación para la detección siguiente. El índice de noticias en forma de árbol se define formalmente de la siguiente manera: S-Tree = {r, NC, NS, E}, donde r es la raíz de S-Tree, NC es el conjunto de todos los nodos de clúster, NS es el conjunto de todos los nodos de historia, y E es el conjunto de todos los bordes en S-Tree. Definimos un conjunto de restricciones para un S-Árbol: ⅰ. , es un nodo no terminal en el árbolC i i N i∀ ∈ → ⅱ. , es un nodo terminal en el árbolS i i N i∀ ∈ → ⅲ. , el grado de salida de es al menos 2C i i N i∀ ∈ → ⅳ. , se representa como el centroide de sus descendientesC i i iN∀ ∈ → Para una noticia di, el procedimiento de comparación y el procedimiento de inserción basados en el árbol de indexación se definen de la siguiente manera. Un ejemplo se muestra en la Figura 1 y la Figura 2. Figura 1. Procedimiento de comparación Figura 2. Procedimiento de comparación de inserción: Paso 1: comparar di con todos los nodos hijos directos de r y seleccionar λ nodos con las similitudes más altas, por ejemplo, C1 2 y C1 3 en la Figura 1. Paso 2: para cada nodo seleccionado en el paso anterior, por ejemplo. C1 2, compara di con todos sus nodos hijos directos y selecciona λ nodos con las similitudes más altas, por ejemplo. C2 2 y d8. Repetir el paso 2 para todos los nodos no terminales. Paso 3: registrar el nodo terminal con la mayor similitud a di, por ejemplo, s5, y el valor de similitud (0.20). Insertando di en el S-árbol con r como raíz: Encuentra el nodo n que es hijo directo de r en el camino desde r hasta el nodo terminal con la similitud más alta s, por ejemplo. C1 2. Si s es menor que θ init + (h-1)δ, entonces agregar di al árbol como un hijo directo de r. De lo contrario, si n es un nodo terminal, crear un nodo de clúster en lugar de n, y agregar tanto n como di como sus hijos directos; si n es un nodo no terminal, repetir este procedimiento e insertar di en el subárbol con n como raíz de forma recursiva. Aquí h es la longitud entre n y la raíz del árbol S. Cuanto más similares sean las historias en un grupo, mejor representará el grupo a las historias en él. Por lo tanto, no imponemos restricciones en la altura máxima de los árboles y el grado de un nodo. Por lo tanto, no podemos dar la complejidad de este procedimiento basado en árboles de indexación. Pero proporcionaremos el número de veces de comparación necesarias por el nuevo procedimiento en nuestros experimentos en la sección 7.5. En esta sección, se proponen dos métodos de reponderación de términos para mejorar la precisión de NED. En el primer método, se explora una nueva forma de utilizar de manera más efectiva la información de los grupos (temas). El segundo encuentra una mejor manera de utilizar entidades nombradas basadas en la clasificación de noticias. 5.1 Reajuste de términos basado en la distancia de distribución TF-IDF es el modelo más prevalente utilizado en sistemas de recuperación de información. La idea básica es que cuanto menos documentos aparezca un término, más importante es el término en la discriminación de documentos (relevantes o no relevantes para una consulta que contenga el término). Sin embargo, en el dominio de TDT, necesitamos discriminar documentos en función de los temas en lugar de las consultas. Intuitivamente, el uso de vectores de clúster (tema) para comparar con historias de noticias posteriores debería tener un mejor rendimiento que el uso de vectores de historias. Desafortunadamente, los resultados experimentales no respaldan esta intuición [4][5]. Basándonos en la observación de los datos, encontramos que la razón es que un tema de noticias generalmente contiene muchos eventos directa o indirectamente relacionados, y todos ellos tienen sus propios subtemas que suelen ser diferentes entre sí. Tomemos el tema descrito en la sección 1 como ejemplo, eventos como la explosión y el rescate tienen similitudes muy bajas con eventos sobre juicios criminales, por lo tanto, las historias sobre juicios tendrían una baja similitud con el vector de tema construido en base a sus eventos previos. Esta sección se centra en cómo hacer un uso efectivo de la información del tema y al mismo tiempo evitar el problema de la descentralización del contenido. Al principio, clasificamos los términos en 5 clases para ayudar en el análisis de las necesidades del modelo modificado: Clase de términos A: términos que ocurren con frecuencia en todo el corpus, por ejemplo, año y personas. Los términos de esta clase deben recibir pesos bajos porque no ayudan mucho en la discriminación de temas. Clase B de términos: términos que ocurren con frecuencia dentro de una categoría de noticias, por ejemplo, elección, tormenta. Son útiles para distinguir dos historias en diferentes categorías de noticias. Sin embargo, no pueden proporcionar información para determinar si dos historias tratan sobre el mismo tema o temas diferentes. En otras palabras, los términos elección y tormenta no son útiles para diferenciar dos campañas electorales y dos desastres de tormenta. Por lo tanto, los términos de esta clase deberían asignarse pesos más bajos. Clase de término C: términos que ocurren con frecuencia en un tema y raramente en otros temas, por ejemplo, el nombre de un avión accidentado, el nombre de un huracán específico. Las noticias que pertenecen a diferentes temas rara vez tienen términos superpuestos en esta clase. Cuanto más frecuentemente aparezca un término en un tema, más importante es el término para una historia perteneciente al tema, por lo tanto, el término debería tener un peso más alto. Clase de término D: términos que aparecen en un tema de manera exclusiva, pero no con frecuencia. Por ejemplo, el nombre de un bombero que se desempeñó muy bien en una acción de salvamento, que puede aparecer en solo dos o tres historias pero nunca ha aparecido en otros temas. Los términos de este tipo deberían recibir más peso que en el modelo TF-IDF. Sin embargo, dado que no son populares en el tema, no es apropiado asignarles pesos demasiado altos. Clase de término E: términos con baja frecuencia en documentos y que aparecen en diferentes temas. Los términos de esta clase deberían recibir pesos más bajos. Ahora analizamos si el modelo TF-IDF puede asignar pesos adecuados a las cinco clases de términos. Obviamente, los términos de la clase A tienen un peso bajo en el modelo TF-IDF, lo cual es acorde con el requisito descrito anteriormente. En el modelo TF-IDF, los términos de la clase B dependen altamente del número de historias en una clase de noticias. El modelo TF-IDF no puede proporcionar pesos bajos si la historia que contiene el término pertenece a una clase de noticias relativamente pequeña. Para un término de clase C, cuanto más frecuentemente aparezca en un tema, menos peso le da el modelo TFIDF. Esto entra en conflicto directo con el requisito de términos en la clase C. Para los términos de la clase D, el modelo TF-IDF les asigna pesos altos de manera correcta. Pero para los términos de la clase E, el modelo TF-IDF asigna pesos altos a ellos que no son conformes con el requisito de pesos bajos. En resumen, los términos de clase B, C, E no pueden ser ponderados adecuadamente en el modelo TF-IDF. Por lo tanto, proponemos un modelo modificado para resolver este problema. Cuando θ init y θ new están configurados de cerca, asumimos que la mayoría de las historias en un clúster de primer nivel (un nodo hijo directo del nodo raíz) están en el mismo tema. Por lo tanto, utilizamos un clúster de primer nivel para capturar la distribución de términos (df para todos los términos dentro del clúster) dentro del tema de forma dinámica. La divergencia de Kullback-Leibler de la distribución de términos en un clúster de primer nivel y el conjunto completo de historias se utiliza para ajustar los pesos de los términos: ( , , ) * (1 * ( || )) ( , , ) ( , , ) * (1 * ( || )) cw tw cw tw w d D peso d t w KL P P peso d t w peso d t w KL P P γ γ ∈ + = +∑ (5) donde ( ) ( ) ( ) ( ) 1,cw cw c c c c df w df w p y p y N N = = − (6) ( ) ( ) ( ) ( ) 1,t t tw tw t t df w df w p y p y N N = = − (7) donde dfc(w) es el número de documentos que contienen el término w dentro del clúster C, y Nc es el número de documentos en el clúster C, y Nt es el número total de documentos que llegan antes del paso de tiempo t. γ es un parámetro constante, actualmente configurado manualmente en 3. La divergencia de Kullback-Leibler se define de la siguiente manera [17]: La idea básica es que, para una historia en un tema, cuanto más ocurre un término dentro del tema y menos ocurre en otros temas, se le deben asignar pesos más altos. Obviamente, el modelo modificado puede cumplir con todos los requisitos de las cinco clases de términos mencionadas anteriormente. 5.2 Reajuste de Términos Basado en el Tipo de Término y la Clase de Historia. Trabajos anteriores encontraron que algunas clases de historias de noticias podrían lograr mejoras significativas al dar un peso adicional a las entidades nombradas. Pero encontramos que los términos de diferentes tipos deben recibir una cantidad diferente de peso adicional para diferentes clases de noticias. Utilizamos open-NLP1 para reconocer tipos de entidades nombradas y etiquetas de partes del discurso para términos que aparecen en noticias. Los tipos de entidades nombradas incluyen nombre de persona, nombre de organización, nombre de ubicación, fecha, hora, dinero y porcentaje, y se seleccionan cinco partes del discurso: ninguno (NN), verbo (VB), adjetivo (JJ), adverbio (RB) y número cardinal (CD). El análisis estadístico muestra tipos de términos discriminativos a nivel de tema para diferentes clases de historias. Por conveniencia, los tipos de entidad nombrada y etiquetas de parte de la oración se llaman uniformemente tipo de término en las secciones siguientes. Determinar si dos historias tratan sobre el mismo tema es un componente básico para la tarea de NED. Por lo tanto, al principio usamos la estadística χ² para calcular las correlaciones entre términos y temas. Para un término t y un tema T, se deriva una tabla de contingencia: Tabla 1. Una tabla de contingencia de 2×2 con el número de documentos pertenecientes al tema T y no pertenecientes al tema T incluye t A B no incluye t C D. La estadística χ² para un término específico t con respecto al tema T se define como [16]: χ² = ( ) ( ) * ( ) ( ) * ( ) * ( ) * ( ) w T A B C D AD CB A C B D A B C D + + + − + + + + (9). Los temas de noticias para la tarea TDT se clasifican además en 11 reglas de interpretaciones (ROIs) 2. El ROI se puede ver como una clase de historias de nivel superior. El promedio de correlación entre un tipo de término y un ROI de tema se calcula como: 2 avg 2 ( , )( ( , ) )k m m km kT R w P w TP R p w T R P χ χ ∈ ∈ ∑ ∑（ , ）= 1 1 k=1…K, m=1…M (10) donde K es el número de tipos de términos (fijado constantemente en 12 en el artículo). M es el número de nuevas clases (ROIs, establecido en el conjunto 11 del artículo). Pk representa el conjunto de todos los términos de tipo k, y Rm representa el conjunto de todos los temas de la clase m, p(t,T) significa la probabilidad de que t ocurra en el tema T. Debido a limitaciones de espacio, solo se enumeran partes de los tipos de términos (9 tipos de términos) y partes de las clases de noticias (8 clases) en la tabla 2 con los valores promedio de correlación entre ellos. Las estadísticas se derivan de datos etiquetados en el corpus TDT2. (Los resultados en la tabla 2 ya están normalizados para mayor comodidad en la comparación). Las estadísticas en la tabla 2 indican la utilidad de diferentes tipos de términos en la discriminación de temas con respecto a diferentes clases de noticias. Podemos ver que el nombre de la ubicación es el tipo de término más útil para tres clases de noticias: Desastres Naturales, Violencia o Guerra, Finanzas. Y para otras tres categorías: Elecciones, Casos Legales/Criminales, Ciencia y Descubrimiento, el nombre de la persona es el tipo de término más discriminativo. Para Escándalos/Audiencias, la fecha es la información más importante para la discriminación de temas. Además, los casos legales/criminales y los temas financieros tienen una correlación más alta con términos relacionados con el dinero, mientras que la ciencia y el descubrimiento tienen una correlación más alta con términos de porcentaje. Los términos no nominales son más estables para diferentes clases. 1. http://opennlp.sourceforge.net/ 2. http://projects.ldc.upenn.edu/TDT3/Guide/label.html A partir del análisis de la tabla 2, es razonable ajustar el peso de los términos según su tipo de término y la clase de noticias a la que pertenece la historia. Los nuevos pesos de los términos se recalculan de la siguiente manera: ( ) ( ) ( ) ( ) ( , , ) * ( , , ) ( , , ) * clase d D tipo w T clase d D tipo w w d peso d t w peso d t w peso d t w α α ∈ = ∑ (11) donde type(w) representa el tipo del término w, y class(d) representa la clase de la historia d, c kα es el parámetro de recalibración para la clase de noticias c y el tipo de término k. En el trabajo, simplemente utilizamos las estadísticas de la tabla 2 como parámetros de recalibración. Aunque usar las estadísticas directamente puede que no sea la mejor opción, no discutimos cómo obtener automáticamente los mejores parámetros. Intentaremos utilizar técnicas de aprendizaje automático para obtener los mejores parámetros en el trabajo futuro. En el trabajo, utilizamos BoosTexter [20] para clasificar todas las historias en uno de los 11 ROIs. BoosTexter es un programa de aprendizaje automático basado en boosting, que crea una serie de reglas simples para construir un clasificador de datos de texto o atributos-valor. Utilizamos el peso del término generado utilizando el modelo TF-IDF como característica para la clasificación de historias. Entrenamos el modelo con las 12000 historias en inglés evaluadas en TDT2, y clasificamos el resto de las historias en TDT2 y todas las historias en TDT3. Los resultados de clasificación se utilizan para la reponderación de términos en la fórmula (11). Dado que las etiquetas de clase de las historias de tema apagado no se proporcionan en los conjuntos de datos de TDT, no podemos proporcionar la precisión de clasificación aquí. Por lo tanto, no discutimos los efectos de la precisión de la clasificación en el rendimiento de NED en el artículo. 6. CONFIGURACIÓN EXPERIMENTAL 6.1 Conjuntos de datos Utilizamos dos conjuntos de datos de LDC [18], TDT2 y TDT3, para nuestros experimentos. TDT2 contiene noticias desde enero hasta junio de 1998. Contiene alrededor de 54,000 historias de fuentes como ABC, Associated Press, CNN, New York Times, Public Radio International, Voice of America, etc. Solo se consideraron historias en inglés en la colección. TDT3 contiene aproximadamente 31,000 historias en inglés recopiladas de octubre a diciembre de 1998. Además de las fuentes utilizadas en TDT2, también contiene historias de las transmisiones de televisión de NBC y MSNBC. Utilizamos versiones transcritas de las emisiones de televisión y radio además de noticias textuales. El conjunto de datos TDT2 está etiquetado con alrededor de 100 temas, y aproximadamente 12,000 historias en inglés pertenecen al menos a uno de estos temas. El conjunto de datos TDT3 está etiquetado con alrededor de 120 temas, y aproximadamente 8000 historias en inglés pertenecen al menos a uno de estos temas. Todos los temas están clasificados en 11 Reglas de Interpretación: (1) Elecciones, (2) Escándalos/Audiencias, (3) Casos Legales/Criminales, (4) Desastres Naturales, (5) Accidentes, (6) Violencia o Guerra en Curso, (7) Noticias de Ciencia y Descubrimientos, (8) Finanzas, (9) Nueva Ley, (10) Noticias Deportivas, (11) Varios. Noticias. 6.2 Métrica de evaluación TDT utiliza una función de costo CDet que combina las probabilidades de perder una nueva historia y una falsa alarma [19]: * * * *Det Miss Miss Target FA FA NontargetC C P P C P P= + (12) Tabla 2. Correlación promedio entre tipos de términos y clases de noticias donde CMiss significa el costo de perder una nueva historia, PMiss significa la probabilidad de perder una nueva historia, y PTarget significa la probabilidad de ver una nueva historia en los datos; CFA significa el costo de una falsa alarma, PFA significa la probabilidad de una falsa alarma, y PNontarget significa la probabilidad de ver una historia antigua. El costo CDet se normaliza de tal manera que un sistema perfecto obtiene una puntuación de 0 y un sistema trivial, que es el mejor de marcar todas las historias como nuevas o antiguas, obtiene una puntuación de 1: ( ( * , * ) ) Det Det Miss Target FA Nontarget C Norm C min C P C P = (13) El sistema de detección de eventos nuevos proporciona dos salidas para cada historia. La primera parte es sí o no, indicando si la historia desencadena un nuevo evento o no. La segunda parte es una puntuación que indica la confianza de la primera decisión. Los puntajes de confianza se pueden utilizar para trazar la curva DET, es decir, curvas que representan la probabilidad de falsa alarma frente a la probabilidad de omisión. El costo normalizado mínimo se puede determinar si se elige el umbral óptimo en la puntuación. RESULTADOS EXPERIMENTALES 7.1 Resultados principales Para probar los enfoques propuestos en el modelo, implementamos y probamos cinco sistemas: Sistema-1: este sistema se utiliza como referencia. Se implementa basándose en el modelo básico descrito en la sección 3, es decir, utilizando el modelo TF-IDF incremental para generar pesos de términos y utilizando la distancia de Hellinger para calcular la similitud entre documentos. La normalización del puntaje de similitud también se emplea [8]. Se utiliza el procedimiento de detección S-S. Sistema-2: este sistema es igual que el sistema-1 excepto que se utiliza el procedimiento de detección S-C. Sistema-3: este sistema es igual que el sistema-1 excepto que utiliza el nuevo procedimiento de detección que se basa en un árbol de indexación. Sistema-4: implementado basado en el enfoque presentado en la sección 5.1, es decir, los términos se vuelven a ponderar según la distancia entre las distribuciones de términos en un grupo y todas las historias. Se utiliza el nuevo procedimiento de detección. Sistema-5: implementado basado en el enfoque presentado en la sección 5.2, es decir, los términos de diferentes tipos son reponderados según la clase de noticias utilizando parámetros entrenados. El nuevo procedimiento de detección se utiliza. Los siguientes son algunos otros sistemas NED: Sistema-6: [21] para cada par de historias, calcula tres valores de similitud para entidad nombrada, entidad no nombrada y todos los términos respectivamente. Y emplear Máquina de Vectores de Soporte para predecir nuevo o antiguo utilizando los valores de similitud como características. Sistema-7: [8] extendió un modelo básico incremental de TF-IDF para incluir modelos específicos de origen, normalización de puntuaciones de similitud basada en promedios específicos de documentos, normalización de puntuaciones de similitud basada en promedios específicos de pares de fuentes, etc. Sistema-8: [13] dividió la representación del documento en dos partes: entidades nombradas y entidades no nombradas, y eligió una parte efectiva para cada clase de noticias. La tabla 3 y la tabla 4 muestran los costos normalizados ponderados por tema y los tiempos de comparación en los conjuntos de datos TDT2 y TDT3 respectivamente. Dado que no se contaba con un conjunto de datos de validación para ajustar el umbral θ nuevo en los experimentos en TDT2, solo informamos los costos normalizados mínimos para nuestros sistemas en la tabla 3. El sistema-5 supera a todos los demás sistemas, incluido el sistema-6, y realiza solo 2.78e+8 veces la comparación de tiempos en el procedimiento de detección, lo que representa solo el 13.4% del sistema-1. Tabla 3. Los resultados de NED en los sistemas TDT2 Min Norm(CDet) tiempos de comparación son: Sistema-1 0.5749 2.08e+9, Sistema-2 0.6673 3.77e+8, Sistema-3 0.5765 2.81e+8, Sistema-4 0.5431 2.99e+8, Sistema-5 0.5089 2.78e+8, Sistema-6 0.5300. Al evaluar en los costos normalizados en TDT3, utilizamos los umbrales óptimos obtenidos del conjunto de datos de TDT2 para todos los sistemas. El sistema-2 reduce los tiempos de comparación a 1.29e+9, que es solo el 18.3% del sistema-1, pero al mismo tiempo también obtiene un costo normalizado mínimo deteriorado que es 0.0499 más alto que el sistema-1. El Sistema-3 utiliza el nuevo procedimiento de detección basado en el índice de noticias en árbol. Requiere incluso menos tiempo de comparación que el sistema-2. Esto se debe a que las comparaciones de historias suelen arrojar mayores similitudes que las de grupos de historias, por lo que las historias tienden a combinarse en el sistema-3. Y el sistema-3 es básicamente equivalente al sistema-1 en los resultados de precisión. El System-4 ajusta los pesos de los términos basándose en la distancia de las distribuciones de términos entre todo el corpus y el conjunto de historias del clúster, lo que resulta en una mejora significativa de 0.0468 en comparación con el system-1. El mejor sistema (sistema-5) tiene un costo normalizado mínimo de 0.5012, lo cual es 0.0797 mejor que el sistema-1, y también mejor que cualquier otro resultado previamente reportado para este conjunto de datos [8, 13]. Además, el sistema-5 solo necesita 1.05e+8 veces de comparación, lo que equivale al 14.9% del sistema-1. Tabla 4. Los resultados de NED en los sistemas TDT3 Norm(CDet) Norma mínima(CDet) Tiempos de comparación Sistema-1 0.6159 0.5809 7.04e+8 Sistema-2① 0.6493 0.6308 1.29e+8 Sistema-3② 0.6197 0.5868 1.03e+8 Sistema-4② 0.5601 0.5341 1.03e+8 Sistema-5② 0.5413 0.5012 1.05e+8 Sistema-7 -- 0.5783 -Sistema-8 -- 0.5229 -① θ nuevo=0.13 ② θ inicial=0.13, λ =3, δ =0.15 La Figura 5 muestra las cinco curvas DET para nuestros sistemas en el conjunto de datos TDT3. El Sistema-5 logra el costo mínimo con una tasa de falsas alarmas de 0.0157 y una tasa de omisión de 0.4310. Podemos observar que System4 y System-5 obtienen una menor probabilidad de error de omisión en regiones de baja probabilidad de falsa alarma. La hipótesis es que se transfiere más valor de peso a los términos clave de los temas que a los términos no clave. La puntuación de similitud entre dos historias pertenecientes a diferentes temas es más baja que antes, porque los términos superpuestos suelen no ser términos clave de sus temas. 7.2 Selección de parámetros para la detección de árboles de indexación. La Figura 3 muestra los costos normalizados mínimos obtenidos por el sistema-3 en TDT3 utilizando diferentes parámetros. El parámetro de inicio θ se prueba en seis valores que van desde 0.03 hasta 0.18. Y el parámetro λ se prueba en cuatro valores: 1, 2, 3 y 4. Podemos ver que, cuando θ inicial se establece en 0.12, que es el más cercano a θ nuevo, los costos son más bajos que los demás. Esto es fácil de explicar, porque cuando se agrupan historias pertenecientes al mismo tema en un conjunto, es más razonable que el conjunto represente las historias en él. Cuando el parámetro λ se establece en 3 o 4, los costos son mejores que en otros casos, pero no hay mucha diferencia entre 3 y 4. 0 0.05 0.1 0.15 0.2 1 2 3 4 0.5 0.6 0.7 0.8 0.9 1 θ-initλ MinCost 0.6 0.65 0.7 0.75 0.8 0.85 0.9 Figura 3. Costo mínimo en TDT3 (δ =0.15) 0 0.05 0.1 0.15 0.2 1 2 3 4 0 0.5 1 1.5 2 2.5 x 10 8 θ-inicio λ Comparando tiempos 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 8 Figura 4. Comparando los tiempos en TDT3 (δ =0.15), la Figura 4 muestra los tiempos de comparación utilizados por el sistema-3 en TDT3 con los mismos parámetros que la Figura 3. Los tiempos de comparación dependen fuertemente de θ init. Debido a que cuanto mayor sea θ init, menos historias se combinan juntas, más veces de comparación se necesitan para la decisión de un nuevo evento. Por lo tanto, usamos θ init = 0.13, λ = 3, δ = 0.15 para los sistemas 3, 4 y 5. En esta configuración de parámetros, podemos obtener tanto costos normalizados mínimos bajos como menos tiempos de comparación. CONCLUSIÓN Hemos propuesto un procedimiento de detección basado en un árbol de indexación de noticias en nuestro modelo. Reduce los tiempos de comparación a aproximadamente una séptima parte del método tradicional sin afectar la precisión de NED. También hemos presentado dos extensiones al modelo básico TF-IDF. La primera extensión se realiza ajustando los pesos de los términos basados en las distribuciones de términos entre todo el corpus y un conjunto de historias de un clúster. Y la segunda extensión al modelo básico TF-IDF es un mejor uso de los tipos de términos (tipos de entidades nombradas y partes de la velocidad) de acuerdo con las categorías de noticias. Nuestros resultados experimentales en los conjuntos de datos TDT2 y TDT3 muestran que ambas extensiones contribuyen significativamente a la mejora en la precisión. No consideramos la información de tiempo de noticias como una pista para la tarea de NED, ya que la mayoría de los temas duran mucho tiempo y los conjuntos de datos de TDT solo abarcan un período relativamente corto (no más de 6 meses). Para el trabajo futuro, queremos recopilar un conjunto de noticias que abarque un período más largo de internet e integrar información temporal en la tarea de NED. Dado que el tema es un grupo de noticias relativamente grueso, también queremos refinar la granularidad del grupo a nivel de evento e identificar diferentes eventos y sus relaciones dentro de un tema. Agradecimientos: Este trabajo ha sido apoyado por la Fundación Nacional de Ciencias Naturales de China bajo la subvención No. 90604025. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son del autor(es) y no necesariamente reflejan las del patrocinador. REFERENCIAS [1] http://www.nist.gov/speech/tests/tdt/index.htm [2] En Detección y Seguimiento de Temas. Organización de la información basada en eventos. Kluwer Academic Publishers, 2002. .01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90 1 2 5 10 20 40 60 80 90 Probabilidad de Falsa Alarma (en %) Probabilidad de Omisión (en %) Curva Ponderada por Tema del SISTEMA1 Norma Mínima (Costo) del SISTEMA1 Curva Ponderada por Tema del SISTEMA2 Norma Mínima (Costo) del SISTEMA2 Curva Ponderada por Tema del SISTEMA3 Norma Mínima (Costo) del SISTEMA3 Curva Ponderada por Tema del SISTEMA4 Norma Mínima (Costo) del SISTEMA4 Curva Ponderada por Tema del SISTEMA5 Norma Mínima (Costo) del SISTEMA5 Rendimiento Aleatorio Figura 5. Curvas DET en TDT3 [3] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B.T. Archibald y X. Liu. Enfoques de aprendizaje para detectar y rastrear eventos de noticias. En el número especial de IEEE Intelligent Systems sobre Aplicaciones de Recuperación de Información Inteligente, volumen 14 (4), 1999, páginas 32-43. [4] Y. Yang, T. Pierce y J. Carbonell. Un estudio sobre la detección de eventos retrospectivos y en línea. En Actas de SIGIR-98, Melbourne, Australia, 1998, 28-36. [5] J. Allan, V. Lavrenko, D. Malin y R. Swan. Detecciones, límites y líneas de tiempo: Umass y tdt-3. En Actas del Taller de Detección y Seguimiento de Temas (TDT-3), Viena, VA, 2000, 167-174. [6] R. Papka y J. Allan. Detección de nuevos eventos en línea utilizando agrupamiento de un solo paso TÍTULO2:. Informe técnico UM-CS1998-021, 1998. [7] W. Lam, H. Meng, K. Wong y J. I'm sorry, but \"Yen\" is not a sentence. Can you please provide a sentence for me to translate into Spanish? Utilizando el Análisis Contextual para la Detección de Eventos de Noticias. Revista Internacional de Sistemas Inteligentes, 2001, 525-546. [8] B. Thorsten, C. Francine y F. Ayman. Un sistema para la detección de nuevos eventos. En Actas de la 26ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2003, 330-337. [9] S. Nicola y C. Joe. Combinando clasificadores semánticos y sintácticos de documentos para mejorar la detección de primeras noticias. En Actas de la 24ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell, and C. Jin. \n\nACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell y C. Jin. Detección de novedades condicionada al tema. En Actas de la 8ª Conferencia Internacional de ACM SIGKDD, ACM Press. 2002, 688-693. [11] M. Juha, A.M. Helena y S. Marko. Aplicando Clases Semánticas en la Detección y Seguimiento de Eventos. En Actas de la Conferencia Internacional sobre Procesamiento del Lenguaje Natural (ICON 2002), 2002, páginas 175-183. [12] M. Juha, A.M. Helena y S. Marko. Semántica simple en la detección y seguimiento de temas. Recuperación de información, 7(3-4): 2004, 347-368. [13] K. Giridhar y J. Allan. Clasificación de texto y entidades nombradas para la detección de nuevos eventos. En Actas de la 27ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2004, 297-304. [14] J. P. Callan, W. B. Croft, y S. M. Harding. El Sistema de Recuperación INQUERY. En Actas de DEXA-92, 3ra Conferencia Internacional sobre Aplicaciones de Bases de Datos y Sistemas Expertos, 1992, 78-83. [15] R. Krovetz. Viendo la morfología como un proceso de inferencia. En Actas de ACM SIGIR93, 1993, 61-81. [16] Y. Yang y J. Pedersen. Un estudio comparativo sobre la selección de características en la categorización de textos. En J. D. H. Fisher, editor, La Decimocuarta Conferencia Internacional sobre Aprendizaje Automático (ICML97), Morgan Kaufmann, 1997, 412-420. [17] T. M. Cover y J. A. I'm sorry, but \"Thomas\" is not a sentence. Can you provide more context or a complete sentence for me to translate into Spanish? Elementos de la teoría de la información. Wiley. 1991. [18] El consorcio de datos lingüísticos, http://www.ldc,upenn.edu/. [19] La definición de la tarea TDT 2001 y el plan de evaluación, http://www.nist.gov/speech/tests/tdt/tdt2001/evalplan.htm. [20] R. E. Schapire y Y. Cantante. Boostexter: Un sistema basado en Boosting para la categorización de texto. En Machine Learning 39(2/3):1, Kluwer Academic Publishers, 2000, 35-168. [21] K. Giridhar y J. Allan. 2005. Utilizando nombres y temas para la detección de nuevos eventos. En Actas de la Conferencia de Tecnología Humana y la Conferencia sobre Métodos Empíricos en Lenguaje Natural, Vancouver, 2005, 121-128 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "news story stream": {
            "translated_key": "flujos de noticias",
            "is_in_text": true,
            "original_annotated_sentences": [
                "New Event Detection Based on Indexing-tree and Named Entity Zhang Kuo Tsinghua University Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Tsinghua University Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Tsinghua University Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn ABSTRACT New Event Detection (NED) aims at detecting from one or multiple streams of news stories that which one is reported on a new event (i.e.",
                "not reported previously).",
                "With the overwhelming volume of news available today, there is an increasing need for a NED system which is able to detect new events more efficiently and accurately.",
                "In this paper we propose a new NED model to speed up the NED task by using news indexing-tree dynamically.",
                "Moreover, based on the observation that terms of different types have different effects for NED task, two term reweighting approaches are proposed to improve NED accuracy.",
                "In the first approach, we propose to adjust term weights dynamically based on previous story clusters and in the second approach, we propose to employ statistics on training data to learn the named entity reweighting model for each class of stories.",
                "Experimental results on two Linguistic Data Consortium (LDC) datasets TDT2 and TDT3 show that the proposed model can improve both efficiency and accuracy of NED task significantly, compared to the baseline system and other existing systems.",
                "Categories and Subject Descriptors H.3.3 [Information Systems]: Information Search and Retrieval; H.4.2 [Information Systems Applications]: Types of Systemsdecision support.",
                "General Terms Algorithms, Performance, Experimentation 1.",
                "INTRODUCTION Topic Detection and Tracking (TDT) program aims to develop techniques which can effectively organize, search and structure news text materials from a variety of newswire and broadcast media [1].",
                "New Event Detection (NED) is one of the five tasks in TDT.",
                "It is the task of online identification of the earliest report for each topic as soon as that report arrives in the sequence of documents.",
                "A Topic is defined as a seminal event or activity, along with directly related events and activities [2].",
                "An Event is defined as something (non-trivial) happening in a certain place at a certain time [3].",
                "For instance, when a bomb explodes in a building, the exploding is the seminal event that triggers the topic, and other stories on the same topic would be those discussing salvaging efforts, the search for perpetrators, arrests and trial and so on.",
                "Useful news information is usually buried in a mass of data generated everyday.",
                "Therefore, NED systems are very useful for people who need to detect novel information from real-time news stream.",
                "These real-life needs often occur in domains like financial markets, news analysis, and intelligence gathering.",
                "In most of state-of-the-art (currently) NED systems, each news story on hand is compared to all the previous received stories.",
                "If all the similarities between them do not exceed a threshold, then the story triggers a new event.",
                "They are usually in the form of cosine similarity or Hellinger similarity metric.",
                "The core problem of NED is to identify whether two stories are on the same topic.",
                "Obviously, these systems cannot take advantage of topic information.",
                "Further more, it is not acceptable in real applications because of the large amount of computation required in the NED process.",
                "Other systems organize previous stories into clusters (each cluster corresponds to a topic), and new story is compared to the previous clusters instead of stories.",
                "This manner can reduce comparing times significantly.",
                "Nevertheless, it has been proved that this manner is less accurate [4, 5].",
                "This is because sometimes stories within a topic drift far away from each other, which could lead low similarity between a story and its topic.",
                "On the other hand, some proposed NED systems tried to improve accuracy by making better use of named entities [10, 11, 12, 13].",
                "However, none of the systems have considered that terms of different types (e.g.",
                "Noun, Verb or Person name) have different effects for different classes of stories in determining whether two stories are on the same topic.",
                "For example, the names of election candidates (Person name) are very important for stories of election class; the locations (Location name) where accidents happened are important for stories of accidents class.",
                "So, in NED, there still exist following three problems to be investigated: (1) How to speed up the detection procedure while do not decrease the detection accuracy? (2) How to make good use of cluster (topic) information to improve accuracy? (3) How to obtain better news story representation by better understanding of named entities.",
                "Driven by these problems, we have proposed three approaches in this paper. (1)To make the detection procedure faster, we propose a new NED procedure based on news indexing-tree created dynamically.",
                "Story indexing-tree is created by assembling similar stories together to form news clusters in different hierarchies according to their values of similarity.",
                "Comparisons between current story and previous clusters could help find the most similar story in less comparing times.",
                "The new procedure can reduce the amount of comparing times without hurting accuracy. (2)We use the clusters of the first floor in the indexing-tree as news topics, in which term weights are adjusted dynamically according to term distribution in the clusters.",
                "In this approach, cluster (topic) information is used properly, so the problem of theme decentralization is avoided. (3)Based on observations on the statistics obtained from training data, we found that terms of different types (e.g.",
                "Noun and Verb) have different effects for different classes of stories in determining whether two stories are on the same topic.",
                "And we propose to use statistics to optimize the weights of the terms of different types in a story according to the news class that the story belongs to.",
                "On TDT3 dataset, the new NED model just uses 14.9% comparing times of the basic model, while its minimum normalized cost is 0.5012, which is 0.0797 better than the basic model, and also better than any other results previously reported for this dataset [8, 13].",
                "The rest of the paper is organized as follows.",
                "We start off this paper by summarizing the previous work in NED in section 2.",
                "Section 3 presents the basic model for NED that most current systems use.",
                "Section 4 describes our new detection procedure based on news indexing-tree.",
                "In section 5, two term reweighting methods are proposed to improve NED accuracy.",
                "Section 6 gives our experimental data and evaluation metrics.",
                "We finally wrap up with the experimental results in Section 7, and the conclusions and future work in Section 8. 2.",
                "RELATED WORK Papka et al. proposed Single-Pass clustering on NED [6].",
                "When a new story was encountered, it was processed immediately to extract term features and a query representation of the storys content is built up.",
                "Then it was compared with all the previous queries.",
                "If the document did not trigger any queries by exceeding a threshold, it was marked as a new event.",
                "Lam et al build up previous query representations of story clusters, each of which corresponds to a topic [7].",
                "In this manner comparisons happen between stories and clusters.",
                "Recent years, most work focus on proposing better methods on comparison of stories and document representation.",
                "Brants et al. [8] extended a basic incremental TF-IDF model to include sourcespecific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, term reweighting based on inverse event frequencies, and segmentation of documents.",
                "Good improvements on TDT bench-marks were shown.",
                "Stokes et al. [9] utilized a combination of evidence from two distinct representations of a documents content.",
                "One of the representations was the usual free text vector, the other made use of lexical chains (created using WordNet) to build another term vector.",
                "Then the two representations are combined in a linear fashion.",
                "A marginal increase in effectiveness was achieved when the combined representation was used.",
                "Some efforts have been done on how to utilize named entities to improve NED.",
                "Yang et al. gave location named entities four times weight than other terms and named entities [10].",
                "DOREMI research group combined semantic similarities of person names, location names and time together with textual similarity [11][12].",
                "UMass [13] research group split document representation into two parts: named entities and non-named entities.",
                "And it was found that some classes of news could achieve better performance using named entity representation, while some other classes of news could achieve better performance using non-named entity representation.",
                "Both [10] and [13] used text categorization technique to classify news stories in advance.",
                "In [13] news stories are classified automatically at first, and then test sensitivities of names and non-name terms for NED for each class.",
                "In [10] frequent terms for each class are removed from document representation.",
                "For example, word election does not help identify different elections.",
                "In their work, effectiveness of different kinds of names (or terms with different POS) for NED in different news classes are not investigated.",
                "We use statistical analysis to reveal the fact and use it to improve NED performance. 3.",
                "BASIC MODEL In this section, we present the basic New Event Detection model which is similar to what most current systems apply.",
                "Then, we propose our new model by extending the basic model.",
                "New Event Detection systems use <br>news story stream</br> as input, in which stories are strictly time-ordered.",
                "Only previously received stories are available when dealing with current story.",
                "The output is a decision for whether the current story is on a new event or not and the confidence of the decision.",
                "Usually, a NED model consists of three parts: story representation, similarity calculation and detection procedure. 3.1 Story Representation Preprocessing is needed before generating story representation.",
                "For preprocessing, we tokenize words, recognize abbreviations, normalize abbreviations, add part-of-speech tags, remove stopwords included in the stop list used in InQuery [14], replace words with their stems using K-stem algorithm[15], and then generate word vector for each news story.",
                "We use incremental TF-IDF model for term weight calculation [4].",
                "In a TF-IDF model, term frequency in a news document is weighted by the inverse document frequency, which is generated from training corpus.",
                "When a new term occurs in testing process, there are two solutions: simply ignore the new term or set df of the term as a small const (e.g. df = 1).",
                "The new term receives too low weight in the first solution (0) and too high weight in the second solution.",
                "In incremental TF-IDF model, document frequencies are updated dynamically in each time step t: 1( ) ( ) ( )t t D tdf w df w df w−= + (1) where Dt represents news story set received in time t, and dfDt(w) means the number of documents that term w occurs in, and dft(w) means the total number of documents that term w occurs in before time t. In this work, each time window includes 50 news stories.",
                "Thus, each story d received in t is represented as follows: 1 2{ ( , , ), ( , , ),..., ( , , )}nd weight d t w weight d t w weight d t w→ where n means the number of distinct terms in story d, and ( , , )weight d t w means the weight of term w in story d at time t: log( ( , ) 1) log(( 1) /( ( ) 0.5)) ( , , ) log( ( , ) 1) log(( 1) /( ( ) 0.5)) t t t t w d tf d w N df w weight d t w tf d w N df w ∈ + + + = + + +∑ (2) where Nt means the total number of news stories before time t, and tf(d,w) means how many times term w occurs in news story d. 3.2 Similarity Calculation We use Hellinger distance for the calculation of similarity between two stories, for two stories d and d at time t, their similarity is defined as follows: , ( , , ) ( , , ) * ( , , ) w d d sim d d t weight d t w weight d t w ∈ = ∑ (3) 3.3 Detection Procedure For each story d received in time step t, the value ( ) ( ) ( ) ( ( , , )) time d time d n d max sim d d t < = (4) is a score used to determine whether d is a story about a new topic and at the same time is an indication of the confidence in our decision [8]. time(d) means the publication time of story d. If the score exceeds the thresholdθ new, then there exists a sufficiently similar document, thus d is a old story, otherwise, there is no sufficiently similar previous document, thus d is an new story. 4.",
                "New NED Procedure Traditional NED systems can be classified into two main types on the aspect of detection procedure: (1) S-S type, in which the story on hand is compared to each story received previously, and use the highest similarity to determine whether current story is about a new event; (2) S-C type, in which the story on hand is compared to all previous clusters each of which representing a topic, and the highest similarity is used for final decision for current story.",
                "If the highest similarity exceeds thresholdθ new, then it is an old story, and put it into the most similar cluster; otherwise it is a new story and create a new cluster.",
                "Previous work show that the first manner is more accurate than the second one [4][5].",
                "Since sometimes stories within a topic drift far away from each other, a story may have very low similarity with its topic.",
                "So using similarities between stories for determining new story is better than using similarities between story and clusters.",
                "Nevertheless, the first manner needs much more comparing times which means the first manner is low efficient.",
                "We propose a new detection procedure which uses comparisons with previous clusters to help find the most similar story in less comparing times, and the final new event decision is made according to the most similar story.",
                "Therefore, we can get both the accuracy of S-S type methods and the efficiency of S-C type methods.",
                "The new procedure creates a news indexing-tree dynamically, in which similar stories are put together to form a hierarchy of clusters.",
                "We index similar stories together by their common ancestor (a cluster node).",
                "Dissimilar stories are indexed in different clusters.",
                "When a story is coming, we use comparisons between the current story and previous hierarchical clusters to help find the most similar story which is useful for new event decision.",
                "After the new event decision is made, the current story is inserted to the indexing-tree for the following detection.",
                "The news indexing-tree is defined formally as follows: S-Tree = {r, NC , NS , E} where r is the root of S-Tree, NC is the set of all cluster nodes, NS is the set of all story nodes, and E is the set of all edges in S-Tree.",
                "We define a set of constraints for a S-Tree: ⅰ . , is an non-terminal node in the treeC i i N i∀ ∈ → ⅱ . , is a terminal node in the treeS i i N i∀ ∈ → ⅲ . , out degree of is at least 2C i i N i∀ ∈ → ⅳ . , is represented as the centroid of its desendantsC i i iN∀ ∈ → For a news story di, the comparison procedure and inserting procedure based on indexing-tree are defined as follows.",
                "An example is shown by Figure 1 and Figure 2.",
                "Figure 1.",
                "Comparison procedure Figure 2.",
                "Inserting procedure Comparison procedure: Step 1: compare di to all the direct child nodes of r and select λ nodes with highest similarities, e.g., C1 2 and C1 3 in Figure 1.",
                "Step 2: for each selected node in the last step, e.g.",
                "C1 2, compare di to all its direct child nodes, and select λ nodes with highest similarities, e.g.",
                "C2 2 and d8.",
                "Repeat step 2 for all non-terminal nodes.",
                "Step 3: record the terminal node with the highest similarty to di, e.g. s5, and the similarity value (0.20).",
                "Inserting di to the S-tree with r as root: Find the node n which is direct child of r in the path from r to the terminal node with highest similarity s, e.g.",
                "C1 2.",
                "If s is smaller than θ init+(h-1)δ , then add di to the tree as a direct child of r. Otherwise, if n is a terminal node, then create a cluster node instead of n, and add both n and di as its direct children; if n is an non-terminal node, then repeat this procedure and insert di to the sub-tree with n as root recursively.",
                "Here h is the length between n and the root of S-tree.",
                "The more the stories in a cluster similar to each other, the better the cluster represents the stories in it.",
                "Hence we add no constraints on the maximum of trees height and degree of a node.",
                "Therefore, we cannot give the complexity of this indexing-tree based procedure.",
                "But we will give the number of comparing times needed by the new procedure in our experiments in section7. 5.",
                "Term Reweighting Methods In this section, two term reweighting methods are proposed to improve NED accuracy.",
                "In the first method, a new way is explored for better using of cluster (topic) information.",
                "The second one finds a better way to make use of named entities based on news classification. 5.1 Term Reweighting Based on Distribution Distance TF-IDF is the most prevalent model used in information retrieval systems.",
                "The basic idea is that the fewer documents a term appears in, the more important the term is in discrimination of documents (relevant or not relevant to a query containing the term).",
                "Nevertheless, in TDT domain, we need to discriminate documents with regard to topics rather than queries.",
                "Intuitively, using cluster (topic) vectors to compare with subsequent news stories should outperform using story vectors.",
                "Unfortunately, the experimental results do not support this intuition [4][5].",
                "Based on observation on data, we find the reason is that a news topic usually contains many directly or indirectly related events, while they all have their own sub-subjects which are usually different with each other.",
                "Take the topic described in section 1 as an example, events like the explosion and salvage have very low similarities with events about criminal trial, therefore stories about trial would have low similarity with the topic vector built on its previous events.",
                "This section focuses on how to effectively make use of topic information and at the same time avoid the problem of content decentralization.",
                "At first, we classify terms into 5 classes to help analysis the needs of the modified model: Term class A: terms that occur frequently in the whole corpus, e.g., year and people.",
                "Terms of this class should be given low weights because they do not help much for topic discrimination.",
                "Term class B: terms that occur frequently within a news category, e.g., election, storm.",
                "They are useful to distinguish two stories in different news categories.",
                "However, they cannot provide information to determine whether two stories are on the same or different topics.",
                "In another words, term election and term storm are not helpful in differentiate two election campaigns and two storm disasters.",
                "Therefore, terms of this class should be assigned lower weights.",
                "Term class C: terms that occur frequently in a topic, and infrequently in other topics, e.g., the name of a crash plane, the name of a specific hurricane.",
                "News stories that belong to different topics rarely have overlap terms in this class.",
                "The more frequently a term appears in a topic, the more important the term is for a story belonging to the topic, therefore the term should be set higher weight.",
                "Term class D: terms that appear in a topic exclusively, but not frequently.",
                "For example, the name of a fireman who did very well in a salvage action, which may appears in only two or three stories but never appeared in other topics.",
                "Terms of this type should receive more weights than in TF-IDF model.",
                "However, since they are not popular in the topic, it is not appropriate to give them too high weights.",
                "Term class E: terms with low document frequency, and appear in different topics.",
                "Terms of this class should receive lower weights.",
                "Now we analyze whether TF-IDF model can give proper weights to the five classes of terms.",
                "Obviously, terms of class A are lowly weighted in TF-IDF model, which is conformable with the requirement described above.",
                "In TF-IDF model, terms of class B are highly dependant with the number of stories in a news class.",
                "TF-IDF model cannot provide low weights if the story containing the term belongs to a relative small news class.",
                "For a term of class C, the more frequently it appears in a topic, the less weight TFIDF model gives to it.",
                "This strongly conflicts with the requirement of terms in class C. For terms of class D, TF-IDF model gives them high weights correctly.",
                "But for terms of class E, TF-IDF model gives high weights to them which are not conformable with the requirement of low weights.",
                "To sum up, terms of class B, C, E cannot be properly weighted in TF-IDF model.",
                "So, we propose a modified model to resolve this problem.",
                "When θ init andθ new are set closely, we assume that most of the stories in a first-level cluster (a direct child node of root node) are on the same topic.",
                "Therefore, we make use of a first-level cluster to capture term distribution (df for all the terms within the cluster) within the topic dynamically.",
                "KL divergence of term distribution in a first-level cluster and the whole story set is used to adjust term weights:  ( , , ) * (1 * ( || )) ( , , ) ( , , ) * (1 * ( || )) cw tw cw tw w d D weight d t w KL P P weight d t w weight d t w KL P P γ γ ∈ + = +∑ (5) where ( ) ( ) ( ) ( ) 1,cw cw c c c c df w df w p y p y N N = = − (6) ( ) ( ) ( ) ( ) 1,t t tw tw t t df w df w p y p y N N = = − (7) where dfc(w) is the number of documents containing term w within cluster C, and Nc is the number of documents in cluster C, and Nt is the total number of documents that arrive before time step t. γ is a const parameter, now is manually set 3.",
                "KL divergence is defined as follows [17]: ( ) ( || ) ( ) log ( )x p x KL P Q p x q x = ∑ (8) The basic idea is: for a story in a topic, the more a term occurs within the topic, and the less it occurs in other topics, it should be assigned higher weights.",
                "Obviously, modified model can meet all the requirements of the five term classes listed above. 5.2 Term Reweighting Based on Term Type and Story Class Previous work found that some classes of news stories could achieve good improvements by giving extra weight to named entities.",
                "But we find that terms of different types should be given different amount of extra weight for different classes of news stories.",
                "We use open-NLP1 to recognize named entity types and part-ofspeech tags for terms that appear in news stories.",
                "Named entity types include person name, organization name, location name, date, time, money and percentage, and five POSs are selected: none (NN), verb (VB), adjective (JJ), adverb (RB) and cardinal number (CD).",
                "Statistical analysis shows topic-level discriminative terms types for different classes of stories.",
                "For the sake of convenience, named entity type and part-of-speech tags are uniformly called term type in subsequent sections.",
                "Determining whether two stories are about the same topic is a basic component for NED task.",
                "So at first we use 2 χ statistic to compute correlations between terms and topics.",
                "For a term t and a topic T, a contingence table is derived: Table 1.",
                "A 2×2 Contingence Table Doc Number belong to topic T not belong to topic T include t A B not include t C D The 2 χ statistic for a specific term t with respect to topic T is defined to be [16]: 2 2 ( , ) ( ) * ( ) ( ) * ( ) * ( ) * ( ) w T A B C D AD CB A C B D A B C D χ = + + + − + + + + (9) News topics for the TDT task are further classified into 11 rules of interpretations (ROIs) 2 .",
                "The ROI can be seen as a higher level class of stories.",
                "The average correlation between a term type and a topic ROI is computed as: 2 avg 2 ( , )( ( , ) )k m m km kT R w P w TP R p w T R P χ χ ∈ ∈ ∑ ∑（ , ）= 1 1 k=1…K, m=1…M (10) where K is the number of term types (set 12 constantly in the paper).",
                "M is the number news classes (ROIs, set 11 in the paper).",
                "Pk represents the set of all terms of type k, and Rm represents the set of all topics of class m, p(t,T) means the probability that t occurs in topic T. Because of limitation of space, only parts of the term types (9 term types) and parts of news classes (8 classes) are listed in table 2 with the average correlation values between them.",
                "The statistics is derived from labeled data in TDT2 corpus. (Results in table 2 are already normalized for convenience in comparison.)",
                "The statistics in table 2 indicates the usefulness of different term types in topic discrimination with respect to different news classes.",
                "We can see that, location name is the most useful term type for three news classes: Natural Disasters, Violence or War, Finances.",
                "And for three other categories Elections, Legal/Criminal Cases, Science and Discovery, person name is the most discriminative term type.",
                "For Scandals/Hearings, date is the most important information for topic discrimination.",
                "In addition, Legal/Criminal Cases and Finance topics have higher correlation with money terms, while Science and Discovery have higher correlation with percentage terms.",
                "Non-name terms are more stable for different classes. 1 . http://opennlp.sourceforge.net/ 2 . http://projects.ldc.upenn.edu/TDT3/Guide/label.html From the analysis of table 2, it is reasonable to adjust term weight according to their term type and the news class the story belongs to.",
                "New term weights are reweighted as follows: ( ) ( ) ( ) ( ) ( , , ) * ( , , ) ( , , ) * class d D type w T class d D type w w d weight d t w weight d t w weight d t w α α ∈ = ∑ (11) where type(w) represents the type of term w, and class(d) represents the class of story d, c kα is reweighting parameter for news class c and term type k. In the work, we just simply use statistics in table 2 as the reweighting parameters.",
                "Even thought using the statistics directly may not the best choice, we do not discuss how to automatically obtain the best parameters.",
                "We will try to use machine learning techniques to obtain the best parameters in the future work.",
                "In the work, we use BoosTexter [20] to classify all stories into one of the 11 ROIs.",
                "BoosTexter is a boosting based machine learning program, which creates a series of simple rules for building a classifier for text or attribute-value data.",
                "We use term weight generated using TF-IDF model as feature for story classification.",
                "We trained the model on the 12000 judged English stories in TDT2, and classify the rest of the stories in TDT2 and all stories in TDT3.",
                "Classification results are used for term reweighting in formula (11).",
                "Since the class labels of topic-off stories are not given in TDT datasets, we cannot give the classification accuracy here.",
                "Thus we do not discuss the effects of classification accuracy to NED performance in the paper. 6.",
                "EXPERIMENTAL SETUP 6.1 Datasets We used two LDC [18] datasets TDT2 and TDT3 for our experiments.",
                "TDT2 contains news stories from January to June 1998.",
                "It contains around 54,000 stories from sources like ABC, Associated Press, CNN, New York Times, Public Radio International, Voice of America etc.",
                "Only English stories in the collection were considered.",
                "TDT3 contains approximately 31,000 English stories collected from October to December 1998.",
                "In addition to the sources used in TDT2, it also contains stories from NBC and MSNBC TV broadcasts.",
                "We used transcribed versions of the TV and radio broadcasts besides textual news.",
                "TDT2 dataset is labeled with about 100 topics, and approximately 12,000 English stories belong to at least one of these topics.",
                "TDT3 dataset is labeled with about 120 topics, and approximately 8000 English stories belong to at least one of these topics.",
                "All the topics are classified into 11 Rules of Interpretation: (1)Elections, (2)Scandals/Hearings, (3)Legal/Criminal Cases, (4)Natural Disasters, (5)Accidents, (6)Ongoing Violence or War, (7)Science and Discovery News, (8)Finance, (9)New Law, (10)Sports News, (11)MISC.",
                "News. 6.2 Evaluation Metric TDT uses a cost function CDet that combines the probabilities of missing a new story and a false alarm [19]: * * * *Det Miss Miss Target FA FA NontargetC C P P C P P= + (12) Table 2.",
                "Average correlation between term types and news classes where CMiss means the cost of missing a new story, PMiss means the probability of missing a new story, and PTarget means the probability of seeing a new story in the data; CFA means the cost of a false alarm, PFA means the probability of a false alarm, and PNontarget means the probability of seeing an old story.",
                "The cost CDet is normalized such that a perfect system scores 0 and a trivial system, which is the better one of mark all stories as new or old, scores 1: ( ( * , * ) ) Det Det Miss Target FA Nontarget C Norm C min C P C P = (13) New event detection system gives two outputs for each story.",
                "The first part is yes or no indicating whether the story triggers a new event or not.",
                "The second part is a score indicating confidence of the first decision.",
                "Confidence scores can be used to plot DET curve, i.e., curves that plot false alarm vs. miss probabilities.",
                "Minimum normalized cost can be determined if optimal threshold on the score were chosen. 7.",
                "EXPERIMENTAL RESULTS 7.1 Main Results To test the approaches proposed in the model, we implemented and tested five systems: System-1: this system is used as baseline.",
                "It is implemented based on the basic model described in section 3, i.e., using incremental TF-IDF model to generate term weights, and using Hellinger distance to compute document similarity.",
                "Similarity score normalization is also employed [8].",
                "S-S detection procedure is used.",
                "System-2: this system is the same as system-1 except that S-C detection procedure is used.",
                "System-3: this system is the same as system-1 except that it uses the new detection procedure which is based on indexing-tree.",
                "System-4: implemented based on the approach presented in section 5.1, i.e., terms are reweighted according to the distance between term distributions in a cluster and all stories.",
                "The new detection procedure is used.",
                "System-5: implemented based on the approach presented in section 5.2, i.e., terms of different types are reweighted according to news class using trained parameters.",
                "The new detection procedure is used.",
                "The following are some other NED systems: System-6: [21] for each pair of stories, it computes three similarity values for named entity, non-named entity and all terms respectively.",
                "And employ Support Vector Machine to predict new or old using the similarity values as features.",
                "System-7: [8] it extended a basic incremental TF-IDF model to include source-specific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, etc.",
                "System-8: [13] it split document representation into two parts: named entities and non-named entities, and choose one effective part for each news class.",
                "Table 3 and table 4 show topic-weighted normalized costs and comparing times on TDT2 and TDT3 datasets respectively.",
                "Since no heldout data set for fine-tuning the threshold θ new was available for experiments on TDT2, we only report minimum normalized costs for our systems in table 3.",
                "System-5 outperforms all other systems including system-6, and it performs only 2.78e+8 comparing times in detection procedure which is only 13.4% of system-1.",
                "Table 3.",
                "NED results on TDT2 Systems Min Norm(CDet) Cmp times System-1 0.5749 2.08e+9 System-2① 0.6673 3.77e+8 System-3② 0.5765 2.81e+8 System-4② 0.5431 2.99e+8 System-5② 0.5089 2.78e+8 System-6 0.5300 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 When evaluating on the normalized costs on TDT3, we use the optimal thresholds obtained from TDT2 data set for all systems.",
                "System-2 reduces comparing times to 1.29e+9 which is just 18.3% of system-1, but at the same time it also gets a deteriorated minimum normalized cost which is 0.0499 higher than system-1.",
                "System-3 uses the new detection procedure based on news indexing-tree.",
                "It requires even less comparing times than system-2.",
                "This is because story-story comparisons usually yield greater similarities than story-cluster ones, so stories tend to be combined Location Person Date Organization Money Percentage NN JJ CD Elections 0.37 1 0.04 0.58 0.08 0.03 0.32 0.13 0.1 Scandals/Hearings 0.66 0.62 0.28 1 0.11 0.02 0.27 0.13 0.05 Legal/Criminal Cases 0.48 1 0.02 0.62 0.15 0 0.22 0.24 0.09 Natural Disasters 1 0.27 0 0.04 0.04 0 0.25 0.04 0.02 Violence or War 1 0.36 0.02 0.14 0.02 0.04 0.21 0.11 0.02 Science and Discovery 0.11 1 0.01 0.22 0.08 0.12 0.19 0.08 0.03 Finances 1 0.45 0.04 0.98 0.13 0.02 0.29 0.06 0.05 Sports 0.16 0.27 0.01 1 0.02 0 0.11 0.03 0.01 together in system-3.",
                "And system-3 is basically equivalent to system-1 in accuracy results.",
                "System-4 adjusts term weights based on the distance of term distributions between the whole corpus and cluster story set, yielding a good improvement by 0.0468 compared to system-1.",
                "The best system (system-5) has a minimum normalized cost 0.5012, which is 0.0797 better than system-1, and also better than any other results previously reported for this dataset [8, 13].",
                "Further more, system-5 only needs 1.05e+8 comparing times which is 14.9% of system-1.",
                "Table 4.",
                "NED results on TDT3 Systems Norm(CDet) Min Norm(CDet) Cmp times System-1 0.6159 0.5809 7.04e+8 System-2① 0.6493 0.6308 1.29e+8 System-3② 0.6197 0.5868 1.03e+8 System-4② 0.5601 0.5341 1.03e+8 System-5② 0.5413 0.5012 1.05e+8 System-7 -- 0.5783 -System-8 -- 0.5229 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 Figure5 shows the five DET curves for our systems on data set TDT3.",
                "System-5 achieves the minimum cost at a false alarm rate of 0.0157 and a miss rate of 0.4310.",
                "We can observe that System4 and System-5 obtain lower miss probability at regions of low false alarm probabilities.",
                "The hypothesis is that, more weight value is transferred to key terms of topics from non-key terms.",
                "Similarity score between two stories belonging to different topics are lower than before, because their overlapping terms are usually not key terms of their topics. 7.2 Parameter selection for indexing-tree detection Figure 3 shows the minimum normalized costs obtained by system-3 on TDT3 using different parameters.",
                "Theθ init parameter is tested on six values spanning from 0.03 to 0.18.",
                "And the λ parameter is tested on four values 1, 2, 3 and 4.",
                "We can see that, whenθ init is set to 0.12, which is the closest one toθ new, the costs are lower than others.",
                "This is easy to explain, because when stories belonging to the same topic are put in a cluster, it is more reasonable for the cluster to represent the stories in it.",
                "When parameter λ is set to 3 or 4, the costs are better than other cases, but there is no much difference between 3 and 4. 0 0.05 0.1 0.15 0.2 1 2 3 4 0.5 0.6 0.7 0.8 0.9 1 θ-initλ MinCost 0.6 0.65 0.7 0.75 0.8 0.85 0.9 Figure 3.",
                "Min Cost on TDT3 (δ =0.15) 0 0.05 0.1 0.15 0.2 1 2 3 4 0 0.5 1 1.5 2 2.5 x 10 8 θ-init λ Comparingtimes 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 8 Figure 4.",
                "Comparing times on TDT3 (δ =0.15) Figure 4 gives the comparing times used by system-3 on TDT3 with the same parameters as figure 3.",
                "The comparing times are strongly dependent onθ init.",
                "Because the greaterθ init is, the less stories combined together, the more comparing times are needed for new event decision.",
                "So we useθ init =0.13,λ =3,δ =0.15 for system-3, 4, and 5.",
                "In this parameter setting, we can get both low minimum normalized costs and less comparing times. 8.",
                "CONCLUSION We have proposed a news indexing-tree based detection procedure in our model.",
                "It reduces comparing times to about one seventh of traditional method without hurting NED accuracy.",
                "We also have presented two extensions to the basic TF-IDF model.",
                "The first extension is made by adjust term weights based on term distributions between the whole corpus and a cluster story set.",
                "And the second extension to basic TF-IDF model is better use of term types (named entities types and part-of-speed) according to news categories.",
                "Our experimental results on TDT2 and TDT3 datasets show that both of the two extensions contribute significantly to improvement in accuracy.",
                "We did not consider news time information as a clue for NED task, since most of the topics last for a long time and TDT data sets only span for a relative short period (no more than 6 months).",
                "For the future work, we want to collect news set which span for a longer period from internet, and integrate time information in NED task.",
                "Since topic is a relative coarse-grained news cluster, we also want to refine cluster granularity to event-level, and identify different events and their relations within a topic.",
                "Acknowledgments This work is supported by the National Natural Science Foundation of China under Grant No. 90604025.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsor. 9.",
                "REFERENCES [1] http://www.nist.gov/speech/tests/tdt/index.htm [2] In Topic Detection and Tracking.",
                "Event-based Information Organization.",
                "Kluwer Academic Publishers, 2002. .01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90 1 2 5 10 20 40 60 80 90 False Alarm Probability (in %) MissProbability(in%) SYSTEM1 Topic Weighted Curve SYSTEM1 Min Norm(Cost) SYSTEM2 Topic Weighted Curve SYSTEM2 Min Norm(Cost) SYSTEM3 Topic Weighted Curve SYSTEM3 Min Norm(Cost) SYSTEM4 Topic Weighted Curve SYSTEM4 Min Norm(Cost) SYSTEM5 Topic Weighted Curve SYSTEM5 Min Norm(Cost) Random Performance Figure 5.",
                "DET curves on TDT3 [3] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B.T.",
                "Archibald, and X. Liu.",
                "Learning Approaches for Detecting and Tracking News Events.",
                "In IEEE Intelligent Systems Special Issue on Applications of Intelligent Information Retrieval, volume 14 (4), 1999, 32-43. [4] Y. Yang, T. Pierce, and J. Carbonell.",
                "A Study on Retrospective and On-line Event Detection.",
                "In Proceedings of SIGIR-98, Melbourne, Australia, 1998, 28-36. [5] J. Allan, V. Lavrenko, D. Malin, and R. Swan.",
                "Detections, Bounds, and Timelines: Umass and tdt-3.",
                "In Proceedings of Topic Detection and Tracking Workshop (TDT-3), Vienna, VA, 2000, 167-174. [6] R. Papka and J. Allan.",
                "On-line New Event Detection Using Single Pass Clustering TITLE2:.",
                "Technical Report UM-CS1998-021, 1998. [7] W. Lam, H. Meng, K. Wong, and J.",
                "Yen.",
                "Using Contextual Analysis for News Event Detection.",
                "International Journal on Intelligent Systems, 2001, 525-546. [8] B. Thorsten, C. Francine, and F. Ayman.",
                "A System for New Event Detection.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2003, 330-337. [9] S. Nicola and C. Joe.",
                "Combining Semantic and Syntactic Document Classifiers to Improve First Story Detection.",
                "In Proceedings of the 24th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topicconditioned Novelty Detection.",
                "In Proceedings of the 8th ACM SIGKDD International Conference, ACM Press. 2002, 688-693. [11] M. Juha, A.M. Helena, and S. Marko.",
                "Applying Semantic Classes in Event Detection and Tracking.",
                "In Proceedings of International Conference on Natural Language Processing (ICON 2002), 2002, pages 175-183. [12] M. Juha, A.M. Helena, and S. Marko.",
                "Simple Semantics in Topic Detection and Tracking.",
                "Information Retrieval, 7(3-4): 2004, 347-368. [13] K. Giridhar and J. Allan.",
                "Text Classification and Named Entities for New Event Detection.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2004, 297-304. [14] J. P. Callan, W. B. Croft, and S. M. Harding.",
                "The INQUERY Retrieval System.",
                "In Proceedings of DEXA-92, 3rd International Conference on Database and Expert Systems Applications, 1992, 78-83. [15] R. Krovetz.",
                "Viewing Morphology as An Inference Process.",
                "In Proceedings of ACM SIGIR93, 1993, 61-81. [16] Y. Yang and J. Pedersen.",
                "A Comparative Study on Feature Selection in Text Categorization.",
                "In J. D. H. Fisher, editor, The Fourteenth International Conference on Machine Learning (ICML97), Morgan Kaufmann, 1997, 412-420. [17] T. M. Cover, and J.A.",
                "Thomas.",
                "Elements of Information Theory.",
                "Wiley. 1991. [18] The linguistic data consortium, http://www.ldc,upenn.edu/. [19] The 2001 TDT task definition and evaluation plan, http://www.nist.gov/speech/tests/tdt/tdt2001/evalplan.htm. [20] R. E. Schapire and Y.",
                "Singer.",
                "Boostexter: A Boosting-based System for Text Categorization.",
                "In Machine Learning 39(2/3):1, Kluwer Academic Publishers, 2000, 35-168. [21] K. Giridhar and J. Allan. 2005.",
                "Using Names and Topics for New Event Detection.",
                "In Proceedings of Human Technology Conference and Conference on Empirical Methods in Natural Language, Vancouver, 2005, 121-128"
            ],
            "original_annotated_samples": [
                "New Event Detection systems use <br>news story stream</br> as input, in which stories are strictly time-ordered."
            ],
            "translated_annotated_samples": [
                "Los sistemas de detección de eventos nuevos utilizan <br>flujos de noticias</br> como entrada, en los cuales las historias están estrictamente ordenadas por tiempo."
            ],
            "translated_text": "Detección de nuevos eventos basada en árbol de indexación y entidad nombrada Zhang Kuo Universidad Tsinghua Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Universidad Tsinghua Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Universidad Tsinghua Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn RESUMEN La Detección de Nuevos Eventos (NED) tiene como objetivo detectar de una o varias corrientes de noticias cuál se informa sobre un nuevo evento (es decir, no reportado previamente). Con el abrumador volumen de noticias disponible hoy en día, hay una creciente necesidad de un sistema NED que sea capaz de detectar eventos nuevos de manera más eficiente y precisa. En este artículo proponemos un nuevo modelo de NED para acelerar la tarea de NED mediante el uso dinámico de un árbol de indexación de noticias. Además, basándose en la observación de que los términos de diferentes tipos tienen diferentes efectos para la tarea de NED, se proponen dos enfoques de reponderación de términos para mejorar la precisión de NED. En el primer enfoque, proponemos ajustar dinámicamente los pesos de los términos basados en los clusters de historias previas y en el segundo enfoque, proponemos emplear estadísticas en los datos de entrenamiento para aprender el modelo de reajuste de entidades nombradas para cada clase de historias. Los resultados experimentales en dos conjuntos de datos del Consorcio de Datos Lingüísticos (LDC), TDT2 y TDT3, muestran que el modelo propuesto puede mejorar significativamente tanto la eficiencia como la precisión de la tarea de NED, en comparación con el sistema base y otros sistemas existentes. Categorías y Descriptores de Asignaturas H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información; H.4.2 [Aplicaciones de Sistemas de Información]: Tipos de Sistemas de Apoyo a Decisiones. Términos generales Algoritmos, Rendimiento, Experimentación 1. El programa de Detección y Seguimiento de Temas (TDT) tiene como objetivo desarrollar técnicas que puedan organizar, buscar y estructurar de manera efectiva materiales de texto de noticias de una variedad de agencias de noticias y medios de difusión [1]. La Detección de Eventos Nuevos (DEN) es una de las cinco tareas en TDT. Es la tarea de identificación en línea del informe más temprano para cada tema tan pronto como ese informe llegue en la secuencia de documentos. Un tema se define como un evento o actividad seminal, junto con eventos y actividades directamente relacionados [2]. Un evento se define como algo (no trivial) que sucede en un lugar y en un momento determinados [3]. Por ejemplo, cuando una bomba explota en un edificio, la explosión es el evento seminal que desencadena el tema, y otras historias sobre el mismo tema serían aquellas que discuten los esfuerzos de salvamento, la búsqueda de los perpetradores, arrestos y juicios, entre otros. La información útil de noticias suele estar enterrada en una masa de datos generados a diario. Por lo tanto, los sistemas NED son muy útiles para las personas que necesitan detectar información novedosa de un flujo de noticias en tiempo real. Estas necesidades de la vida real a menudo se presentan en ámbitos como los mercados financieros, el análisis de noticias y la recopilación de inteligencia. En la mayoría de los sistemas NED de última generación, cada noticia disponible se compara con todas las noticias previamente recibidas. Si todas las similitudes entre ellos no superan un umbral, entonces la historia desencadena un nuevo evento. Por lo general, se presentan en forma de similitud de coseno o métrica de similitud de Hellinger. El problema central de NED es identificar si dos historias tratan sobre el mismo tema. Obviamente, estos sistemas no pueden aprovechar la información del tema. Además, no es aceptable en aplicaciones reales debido a la gran cantidad de cálculos requeridos en el proceso de NED. Otros sistemas organizan historias anteriores en grupos (cada grupo corresponde a un tema), y la nueva historia se compara con los grupos anteriores en lugar de con historias. Esta forma puede reducir significativamente los tiempos de comparación. Sin embargo, se ha demostrado que este método es menos preciso [4, 5]. Esto se debe a que a veces las historias dentro de un tema se alejan mucho entre sí, lo que podría resultar en una baja similitud entre una historia y su tema. Por otro lado, algunos sistemas NED propuestos intentaron mejorar la precisión al hacer un mejor uso de las entidades nombradas [10, 11, 12, 13]. Sin embargo, ninguno de los sistemas ha considerado que los términos de diferentes tipos (por ejemplo, Los sustantivos, verbos o nombres de personas tienen diferentes efectos en diferentes clases de historias para determinar si dos historias tratan sobre el mismo tema. Por ejemplo, los nombres de los candidatos electorales (Nombre de la persona) son muy importantes para las historias de la clase de elecciones; las ubicaciones (Nombre de la ubicación) donde ocurrieron los accidentes son importantes para las historias de la clase de accidentes. Por lo tanto, en NED, todavía existen los siguientes tres problemas por investigar: (1) ¿Cómo acelerar el procedimiento de detección sin disminuir la precisión de la detección? (2) ¿Cómo aprovechar mejor la información de los grupos (temas) para mejorar la precisión? (3) ¿Cómo obtener una mejor representación de historias de noticias mediante una mejor comprensión de las entidades nombradas? Impulsados por estos problemas, hemos propuesto tres enfoques en este documento. (1) Para hacer que el procedimiento de detección sea más rápido, proponemos un nuevo procedimiento de NED basado en un árbol de indexación de noticias creado dinámicamente. El índice de historias en árbol se crea ensamblando historias similares para formar grupos de noticias en diferentes jerarquías según sus valores de similitud. Las comparaciones entre la historia actual y los grupos anteriores podrían ayudar a encontrar la historia más similar en menos tiempo de comparación. El nuevo procedimiento puede reducir la cantidad de veces de comparación sin afectar la precisión. Utilizamos los grupos del primer piso en el árbol de indexación como temas de noticias, en los cuales los pesos de los términos se ajustan dinámicamente según la distribución de términos en los grupos. En este enfoque, se utiliza adecuadamente la información de los grupos (temas), por lo que se evita el problema de la descentralización del tema. Basándonos en observaciones de las estadísticas obtenidas de los datos de entrenamiento, encontramos que los términos de diferentes tipos (por ejemplo, Los sustantivos y verbos tienen diferentes efectos para diferentes clases de historias al determinar si dos historias tratan sobre el mismo tema. Y proponemos utilizar estadísticas para optimizar los pesos de los términos de diferentes tipos en una historia de acuerdo a la clase de noticias a la que pertenece la historia. En el conjunto de datos TDT3, el nuevo modelo NED solo utiliza un 14.9% en comparación con los tiempos del modelo básico, mientras que su costo normalizado mínimo es de 0.5012, lo que es 0.0797 mejor que el modelo básico, y también mejor que cualquier otro resultado previamente reportado para este conjunto de datos [8, 13]. El resto del documento está organizado de la siguiente manera. Comenzamos este artículo resumiendo el trabajo previo en NED en la sección 2. La sección 3 presenta el modelo básico para NED que la mayoría de los sistemas actuales utilizan. La sección 4 describe nuestro nuevo procedimiento de detección basado en el índice de árbol de noticias. En la sección 5, se proponen dos métodos de reponderación de términos para mejorar la precisión de NED. La sección 6 presenta nuestros datos experimentales y métricas de evaluación. Finalmente concluimos con los resultados experimentales en la Sección 7, y las conclusiones y trabajo futuro en la Sección 8. TRABAJO RELACIONADO Papka et al. propusieron el agrupamiento de un solo paso en NED [6]. Cuando se encontraba una nueva historia, esta era procesada inmediatamente para extraer características de términos y se construía una representación de consulta del contenido de la historia. Luego se comparó con todas las consultas anteriores. Si el documento no generaba ninguna consulta al exceder un umbral, se marcaba como un nuevo evento. Lam et al construyen representaciones de consultas previas de grupos de historias, cada una de las cuales corresponde a un tema [7]. De esta manera se realizan comparaciones entre historias y grupos. En los últimos años, la mayoría de los trabajos se han centrado en proponer mejores métodos para la comparación de historias y la representación de documentos. Brants et al. [8] extendieron un modelo básico incremental de TF-IDF para incluir modelos específicos de fuente, normalización de puntajes de similitud basada en promedios específicos de documentos, normalización de puntajes de similitud basada en promedios específicos de pares de fuentes, reponderación de términos basada en frecuencias de eventos inversas y segmentación de documentos. Se mostraron buenas mejoras en los puntos de referencia de TDT. Stokes et al. [9] utilizaron una combinación de evidencia de dos representaciones distintas del contenido de un documento. Una de las representaciones era el vector de texto libre habitual, la otra hacía uso de cadenas léxicas (creadas utilizando WordNet) para construir otro vector de términos. Luego, las dos representaciones se combinan de forma lineal. Se logró un aumento marginal en la efectividad cuando se utilizó la representación combinada. Algunos esfuerzos se han realizado sobre cómo utilizar entidades nombradas para mejorar la desambiguación de entidades nombradas (NED). Yang et al. asignaron un peso cuatro veces mayor a las entidades con nombres de ubicación que a otros términos y entidades con nombres [10]. El grupo de investigación DOREMI combinó similitudes semánticas de nombres de personas, nombres de lugares y tiempo junto con similitud textual [11][12]. El grupo de investigación de UMass [13] dividió la representación del documento en dos partes: entidades nombradas y entidades no nombradas. Y se descubrió que algunas clases de noticias podían lograr un mejor rendimiento utilizando la representación de entidades nombradas, mientras que otras clases de noticias podían lograr un mejor rendimiento utilizando la representación sin entidades nombradas. Tanto [10] como [13] utilizaron la técnica de categorización de texto para clasificar previamente las noticias. En [13] las noticias se clasifican automáticamente al principio, y luego se prueban las sensibilidades de los nombres y términos que no son nombres para NED para cada clase. En [10], se eliminan los términos frecuentes de cada clase de la representación del documento. Por ejemplo, la palabra \"elección\" no ayuda a identificar diferentes elecciones. En su trabajo, no se investiga la efectividad de diferentes tipos de nombres (o términos con diferentes categorías gramaticales) para NED en diferentes clases de noticias. Utilizamos análisis estadístico para revelar el hecho y utilizarlo para mejorar el rendimiento de NED. 3. MODELO BÁSICO En esta sección, presentamos el modelo básico de Detección de Nuevos Eventos que es similar a lo que la mayoría de los sistemas actuales aplican. Luego, proponemos nuestro nuevo modelo ampliando el modelo básico. Los sistemas de detección de eventos nuevos utilizan <br>flujos de noticias</br> como entrada, en los cuales las historias están estrictamente ordenadas por tiempo. Solo están disponibles las historias recibidas previamente al tratar con la historia actual. La salida es una decisión sobre si la historia actual es un evento nuevo o no y la confianza de la decisión. Por lo general, un modelo NED consta de tres partes: representación de la historia, cálculo de similitud y procedimiento de detección. 3.1 Representación de la historia Se necesita preprocesamiento antes de generar la representación de la historia. Para el preprocesamiento, tokenizamos palabras, reconocemos abreviaturas, normalizamos abreviaturas, agregamos etiquetas de partes del discurso, eliminamos palabras vacías incluidas en la lista de paradas utilizada en InQuery [14], reemplazamos palabras con sus raíces utilizando el algoritmo K-stem[15], y luego generamos un vector de palabras para cada noticia. Utilizamos el modelo TF-IDF incremental para el cálculo del peso de los términos [4]. En un modelo TF-IDF, la frecuencia de término en un documento de noticias se pondera por la frecuencia inversa del documento, la cual se genera a partir del corpus de entrenamiento. Cuando surge un nuevo término en el proceso de prueba, hay dos soluciones: simplemente ignorar el nuevo término o establecer df del término como una constante pequeña (por ejemplo, df = 1). El nuevo término recibe un peso demasiado bajo en la primera solución (0) y un peso demasiado alto en la segunda solución. En el modelo TF-IDF incremental, las frecuencias de documentos se actualizan dinámicamente en cada paso de tiempo t: 1( ) ( ) ( )t t D tdf w df w df w−= + (1) donde Dt representa el conjunto de noticias recibidas en el tiempo t, y dfDt(w) significa el número de documentos en los que aparece el término w, y dft(w) significa el número total de documentos en los que aparece el término w antes del tiempo t. En este trabajo, cada ventana de tiempo incluye 50 noticias. Así, cada historia d recibida en t se representa de la siguiente manera: 1 2{ ( , , ), ( , , ),..., ( , , )}nd peso d t w peso d t w peso d t w→ donde n significa el número de términos distintos en la historia d, y ( , , )peso d t w significa el peso del término w en la historia d en el tiempo t: log( ( , ) 1) log(( 1) /( ( ) 0.5)) ( , , ) log( ( , ) 1) log(( 1) /( ( ) 0.5)) t t t t w d tf d w N df w peso d t w tf d w N df w ∈ + + + = + + +∑ (2) donde Nt significa el número total de historias de noticias antes del tiempo t, y tf(d,w) significa cuántas veces aparece el término w en la historia de noticias d. 3.2 Cálculo de Similitud Utilizamos la distancia de Hellinger para el cálculo de la similitud entre dos historias, para dos historias d y d en el tiempo t, su similitud se define de la siguiente manera: , ( , , ) ( , , ) * ( , , ) w d d sim d d t peso d t w peso d t w ∈ = ∑ (3) 3.3 Procedimiento de Detección Para cada historia d recibida en el paso de tiempo t, el valor ( ) ( ) ( ) ( ( , , )) tiempo d tiempo d n d max sim d d t < = (4) es un puntaje utilizado para determinar si d es una historia sobre un nuevo tema y al mismo tiempo es una indicación de la confianza en nuestra decisión [8]. tiempo(d) significa el tiempo de publicación de la historia d. Si el puntaje supera el umbral θ new, entonces existe un documento suficientemente similar, por lo tanto, d es una historia antigua, de lo contrario, no hay un documento previo suficientemente similar, por lo tanto, d es una historia nueva. 4. Nuevo procedimiento NED Los sistemas NED tradicionales se pueden clasificar en dos tipos principales en cuanto al procedimiento de detección: (1) tipo S-S, en el que la historia en cuestión se compara con cada historia recibida previamente, y se utiliza la mayor similitud para determinar si la historia actual trata sobre un evento nuevo; (2) tipo S-C, en el que la historia en cuestión se compara con todos los clusters anteriores, cada uno de los cuales representa un tema, y se utiliza la mayor similitud para la decisión final sobre la historia actual. Si la similitud más alta supera el umbral θ nuevo, entonces es una historia antigua y se coloca en el clúster más similar; de lo contrario, es una historia nueva y se crea un nuevo clúster. Trabajos anteriores muestran que la primera forma es más precisa que la segunda [4][5]. Dado que a veces las historias dentro de un tema se alejan mucho entre sí, una historia puede tener una similitud muy baja con su tema. Por lo tanto, utilizar similitudes entre historias para determinar una nueva historia es mejor que utilizar similitudes entre historias y grupos. Sin embargo, el primer método requiere mucho más tiempo de comparación, lo que significa que es menos eficiente. Proponemos un nuevo procedimiento de detección que utiliza comparaciones con clusters anteriores para ayudar a encontrar la historia más similar en menos tiempo de comparación, y la decisión final sobre el nuevo evento se toma de acuerdo con la historia más similar. Por lo tanto, podemos obtener tanto la precisión de los métodos de tipo S-S como la eficiencia de los métodos de tipo S-C. El nuevo procedimiento crea un árbol de indexación de noticias dinámicamente, en el cual se agrupan historias similares para formar una jerarquía de clusters. Indexamos historias similares juntas por su ancestro común (un nodo de clúster). Historias diferentes están indexadas en diferentes grupos. Cuando una historia está llegando, utilizamos comparaciones entre la historia actual y los grupos jerárquicos anteriores para ayudar a encontrar la historia más similar que es útil para la toma de decisiones sobre nuevos eventos. Después de que se tome la decisión sobre el nuevo evento, la historia actual se inserta en el árbol de indexación para la detección siguiente. El índice de noticias en forma de árbol se define formalmente de la siguiente manera: S-Tree = {r, NC, NS, E}, donde r es la raíz de S-Tree, NC es el conjunto de todos los nodos de clúster, NS es el conjunto de todos los nodos de historia, y E es el conjunto de todos los bordes en S-Tree. Definimos un conjunto de restricciones para un S-Árbol: ⅰ. , es un nodo no terminal en el árbolC i i N i∀ ∈ → ⅱ. , es un nodo terminal en el árbolS i i N i∀ ∈ → ⅲ. , el grado de salida de es al menos 2C i i N i∀ ∈ → ⅳ. , se representa como el centroide de sus descendientesC i i iN∀ ∈ → Para una noticia di, el procedimiento de comparación y el procedimiento de inserción basados en el árbol de indexación se definen de la siguiente manera. Un ejemplo se muestra en la Figura 1 y la Figura 2. Figura 1. Procedimiento de comparación Figura 2. Procedimiento de comparación de inserción: Paso 1: comparar di con todos los nodos hijos directos de r y seleccionar λ nodos con las similitudes más altas, por ejemplo, C1 2 y C1 3 en la Figura 1. Paso 2: para cada nodo seleccionado en el paso anterior, por ejemplo. C1 2, compara di con todos sus nodos hijos directos y selecciona λ nodos con las similitudes más altas, por ejemplo. C2 2 y d8. Repetir el paso 2 para todos los nodos no terminales. Paso 3: registrar el nodo terminal con la mayor similitud a di, por ejemplo, s5, y el valor de similitud (0.20). Insertando di en el S-árbol con r como raíz: Encuentra el nodo n que es hijo directo de r en el camino desde r hasta el nodo terminal con la similitud más alta s, por ejemplo. C1 2. Si s es menor que θ init + (h-1)δ, entonces agregar di al árbol como un hijo directo de r. De lo contrario, si n es un nodo terminal, crear un nodo de clúster en lugar de n, y agregar tanto n como di como sus hijos directos; si n es un nodo no terminal, repetir este procedimiento e insertar di en el subárbol con n como raíz de forma recursiva. Aquí h es la longitud entre n y la raíz del árbol S. Cuanto más similares sean las historias en un grupo, mejor representará el grupo a las historias en él. Por lo tanto, no imponemos restricciones en la altura máxima de los árboles y el grado de un nodo. Por lo tanto, no podemos dar la complejidad de este procedimiento basado en árboles de indexación. Pero proporcionaremos el número de veces de comparación necesarias por el nuevo procedimiento en nuestros experimentos en la sección 7.5. En esta sección, se proponen dos métodos de reponderación de términos para mejorar la precisión de NED. En el primer método, se explora una nueva forma de utilizar de manera más efectiva la información de los grupos (temas). El segundo encuentra una mejor manera de utilizar entidades nombradas basadas en la clasificación de noticias. 5.1 Reajuste de términos basado en la distancia de distribución TF-IDF es el modelo más prevalente utilizado en sistemas de recuperación de información. La idea básica es que cuanto menos documentos aparezca un término, más importante es el término en la discriminación de documentos (relevantes o no relevantes para una consulta que contenga el término). Sin embargo, en el dominio de TDT, necesitamos discriminar documentos en función de los temas en lugar de las consultas. Intuitivamente, el uso de vectores de clúster (tema) para comparar con historias de noticias posteriores debería tener un mejor rendimiento que el uso de vectores de historias. Desafortunadamente, los resultados experimentales no respaldan esta intuición [4][5]. Basándonos en la observación de los datos, encontramos que la razón es que un tema de noticias generalmente contiene muchos eventos directa o indirectamente relacionados, y todos ellos tienen sus propios subtemas que suelen ser diferentes entre sí. Tomemos el tema descrito en la sección 1 como ejemplo, eventos como la explosión y el rescate tienen similitudes muy bajas con eventos sobre juicios criminales, por lo tanto, las historias sobre juicios tendrían una baja similitud con el vector de tema construido en base a sus eventos previos. Esta sección se centra en cómo hacer un uso efectivo de la información del tema y al mismo tiempo evitar el problema de la descentralización del contenido. Al principio, clasificamos los términos en 5 clases para ayudar en el análisis de las necesidades del modelo modificado: Clase de términos A: términos que ocurren con frecuencia en todo el corpus, por ejemplo, año y personas. Los términos de esta clase deben recibir pesos bajos porque no ayudan mucho en la discriminación de temas. Clase B de términos: términos que ocurren con frecuencia dentro de una categoría de noticias, por ejemplo, elección, tormenta. Son útiles para distinguir dos historias en diferentes categorías de noticias. Sin embargo, no pueden proporcionar información para determinar si dos historias tratan sobre el mismo tema o temas diferentes. En otras palabras, los términos elección y tormenta no son útiles para diferenciar dos campañas electorales y dos desastres de tormenta. Por lo tanto, los términos de esta clase deberían asignarse pesos más bajos. Clase de término C: términos que ocurren con frecuencia en un tema y raramente en otros temas, por ejemplo, el nombre de un avión accidentado, el nombre de un huracán específico. Las noticias que pertenecen a diferentes temas rara vez tienen términos superpuestos en esta clase. Cuanto más frecuentemente aparezca un término en un tema, más importante es el término para una historia perteneciente al tema, por lo tanto, el término debería tener un peso más alto. Clase de término D: términos que aparecen en un tema de manera exclusiva, pero no con frecuencia. Por ejemplo, el nombre de un bombero que se desempeñó muy bien en una acción de salvamento, que puede aparecer en solo dos o tres historias pero nunca ha aparecido en otros temas. Los términos de este tipo deberían recibir más peso que en el modelo TF-IDF. Sin embargo, dado que no son populares en el tema, no es apropiado asignarles pesos demasiado altos. Clase de término E: términos con baja frecuencia en documentos y que aparecen en diferentes temas. Los términos de esta clase deberían recibir pesos más bajos. Ahora analizamos si el modelo TF-IDF puede asignar pesos adecuados a las cinco clases de términos. Obviamente, los términos de la clase A tienen un peso bajo en el modelo TF-IDF, lo cual es acorde con el requisito descrito anteriormente. En el modelo TF-IDF, los términos de la clase B dependen altamente del número de historias en una clase de noticias. El modelo TF-IDF no puede proporcionar pesos bajos si la historia que contiene el término pertenece a una clase de noticias relativamente pequeña. Para un término de clase C, cuanto más frecuentemente aparezca en un tema, menos peso le da el modelo TFIDF. Esto entra en conflicto directo con el requisito de términos en la clase C. Para los términos de la clase D, el modelo TF-IDF les asigna pesos altos de manera correcta. Pero para los términos de la clase E, el modelo TF-IDF asigna pesos altos a ellos que no son conformes con el requisito de pesos bajos. En resumen, los términos de clase B, C, E no pueden ser ponderados adecuadamente en el modelo TF-IDF. Por lo tanto, proponemos un modelo modificado para resolver este problema. Cuando θ init y θ new están configurados de cerca, asumimos que la mayoría de las historias en un clúster de primer nivel (un nodo hijo directo del nodo raíz) están en el mismo tema. Por lo tanto, utilizamos un clúster de primer nivel para capturar la distribución de términos (df para todos los términos dentro del clúster) dentro del tema de forma dinámica. La divergencia de Kullback-Leibler de la distribución de términos en un clúster de primer nivel y el conjunto completo de historias se utiliza para ajustar los pesos de los términos: ( , , ) * (1 * ( || )) ( , , ) ( , , ) * (1 * ( || )) cw tw cw tw w d D peso d t w KL P P peso d t w peso d t w KL P P γ γ ∈ + = +∑ (5) donde ( ) ( ) ( ) ( ) 1,cw cw c c c c df w df w p y p y N N = = − (6) ( ) ( ) ( ) ( ) 1,t t tw tw t t df w df w p y p y N N = = − (7) donde dfc(w) es el número de documentos que contienen el término w dentro del clúster C, y Nc es el número de documentos en el clúster C, y Nt es el número total de documentos que llegan antes del paso de tiempo t. γ es un parámetro constante, actualmente configurado manualmente en 3. La divergencia de Kullback-Leibler se define de la siguiente manera [17]: La idea básica es que, para una historia en un tema, cuanto más ocurre un término dentro del tema y menos ocurre en otros temas, se le deben asignar pesos más altos. Obviamente, el modelo modificado puede cumplir con todos los requisitos de las cinco clases de términos mencionadas anteriormente. 5.2 Reajuste de Términos Basado en el Tipo de Término y la Clase de Historia. Trabajos anteriores encontraron que algunas clases de historias de noticias podrían lograr mejoras significativas al dar un peso adicional a las entidades nombradas. Pero encontramos que los términos de diferentes tipos deben recibir una cantidad diferente de peso adicional para diferentes clases de noticias. Utilizamos open-NLP1 para reconocer tipos de entidades nombradas y etiquetas de partes del discurso para términos que aparecen en noticias. Los tipos de entidades nombradas incluyen nombre de persona, nombre de organización, nombre de ubicación, fecha, hora, dinero y porcentaje, y se seleccionan cinco partes del discurso: ninguno (NN), verbo (VB), adjetivo (JJ), adverbio (RB) y número cardinal (CD). El análisis estadístico muestra tipos de términos discriminativos a nivel de tema para diferentes clases de historias. Por conveniencia, los tipos de entidad nombrada y etiquetas de parte de la oración se llaman uniformemente tipo de término en las secciones siguientes. Determinar si dos historias tratan sobre el mismo tema es un componente básico para la tarea de NED. Por lo tanto, al principio usamos la estadística χ² para calcular las correlaciones entre términos y temas. Para un término t y un tema T, se deriva una tabla de contingencia: Tabla 1. Una tabla de contingencia de 2×2 con el número de documentos pertenecientes al tema T y no pertenecientes al tema T incluye t A B no incluye t C D. La estadística χ² para un término específico t con respecto al tema T se define como [16]: χ² = ( ) ( ) * ( ) ( ) * ( ) * ( ) * ( ) w T A B C D AD CB A C B D A B C D + + + − + + + + (9). Los temas de noticias para la tarea TDT se clasifican además en 11 reglas de interpretaciones (ROIs) 2. El ROI se puede ver como una clase de historias de nivel superior. El promedio de correlación entre un tipo de término y un ROI de tema se calcula como: 2 avg 2 ( , )( ( , ) )k m m km kT R w P w TP R p w T R P χ χ ∈ ∈ ∑ ∑（ , ）= 1 1 k=1…K, m=1…M (10) donde K es el número de tipos de términos (fijado constantemente en 12 en el artículo). M es el número de nuevas clases (ROIs, establecido en el conjunto 11 del artículo). Pk representa el conjunto de todos los términos de tipo k, y Rm representa el conjunto de todos los temas de la clase m, p(t,T) significa la probabilidad de que t ocurra en el tema T. Debido a limitaciones de espacio, solo se enumeran partes de los tipos de términos (9 tipos de términos) y partes de las clases de noticias (8 clases) en la tabla 2 con los valores promedio de correlación entre ellos. Las estadísticas se derivan de datos etiquetados en el corpus TDT2. (Los resultados en la tabla 2 ya están normalizados para mayor comodidad en la comparación). Las estadísticas en la tabla 2 indican la utilidad de diferentes tipos de términos en la discriminación de temas con respecto a diferentes clases de noticias. Podemos ver que el nombre de la ubicación es el tipo de término más útil para tres clases de noticias: Desastres Naturales, Violencia o Guerra, Finanzas. Y para otras tres categorías: Elecciones, Casos Legales/Criminales, Ciencia y Descubrimiento, el nombre de la persona es el tipo de término más discriminativo. Para Escándalos/Audiencias, la fecha es la información más importante para la discriminación de temas. Además, los casos legales/criminales y los temas financieros tienen una correlación más alta con términos relacionados con el dinero, mientras que la ciencia y el descubrimiento tienen una correlación más alta con términos de porcentaje. Los términos no nominales son más estables para diferentes clases. 1. http://opennlp.sourceforge.net/ 2. http://projects.ldc.upenn.edu/TDT3/Guide/label.html A partir del análisis de la tabla 2, es razonable ajustar el peso de los términos según su tipo de término y la clase de noticias a la que pertenece la historia. Los nuevos pesos de los términos se recalculan de la siguiente manera: ( ) ( ) ( ) ( ) ( , , ) * ( , , ) ( , , ) * clase d D tipo w T clase d D tipo w w d peso d t w peso d t w peso d t w α α ∈ = ∑ (11) donde type(w) representa el tipo del término w, y class(d) representa la clase de la historia d, c kα es el parámetro de recalibración para la clase de noticias c y el tipo de término k. En el trabajo, simplemente utilizamos las estadísticas de la tabla 2 como parámetros de recalibración. Aunque usar las estadísticas directamente puede que no sea la mejor opción, no discutimos cómo obtener automáticamente los mejores parámetros. Intentaremos utilizar técnicas de aprendizaje automático para obtener los mejores parámetros en el trabajo futuro. En el trabajo, utilizamos BoosTexter [20] para clasificar todas las historias en uno de los 11 ROIs. BoosTexter es un programa de aprendizaje automático basado en boosting, que crea una serie de reglas simples para construir un clasificador de datos de texto o atributos-valor. Utilizamos el peso del término generado utilizando el modelo TF-IDF como característica para la clasificación de historias. Entrenamos el modelo con las 12000 historias en inglés evaluadas en TDT2, y clasificamos el resto de las historias en TDT2 y todas las historias en TDT3. Los resultados de clasificación se utilizan para la reponderación de términos en la fórmula (11). Dado que las etiquetas de clase de las historias de tema apagado no se proporcionan en los conjuntos de datos de TDT, no podemos proporcionar la precisión de clasificación aquí. Por lo tanto, no discutimos los efectos de la precisión de la clasificación en el rendimiento de NED en el artículo. 6. CONFIGURACIÓN EXPERIMENTAL 6.1 Conjuntos de datos Utilizamos dos conjuntos de datos de LDC [18], TDT2 y TDT3, para nuestros experimentos. TDT2 contiene noticias desde enero hasta junio de 1998. Contiene alrededor de 54,000 historias de fuentes como ABC, Associated Press, CNN, New York Times, Public Radio International, Voice of America, etc. Solo se consideraron historias en inglés en la colección. TDT3 contiene aproximadamente 31,000 historias en inglés recopiladas de octubre a diciembre de 1998. Además de las fuentes utilizadas en TDT2, también contiene historias de las transmisiones de televisión de NBC y MSNBC. Utilizamos versiones transcritas de las emisiones de televisión y radio además de noticias textuales. El conjunto de datos TDT2 está etiquetado con alrededor de 100 temas, y aproximadamente 12,000 historias en inglés pertenecen al menos a uno de estos temas. El conjunto de datos TDT3 está etiquetado con alrededor de 120 temas, y aproximadamente 8000 historias en inglés pertenecen al menos a uno de estos temas. Todos los temas están clasificados en 11 Reglas de Interpretación: (1) Elecciones, (2) Escándalos/Audiencias, (3) Casos Legales/Criminales, (4) Desastres Naturales, (5) Accidentes, (6) Violencia o Guerra en Curso, (7) Noticias de Ciencia y Descubrimientos, (8) Finanzas, (9) Nueva Ley, (10) Noticias Deportivas, (11) Varios. Noticias. 6.2 Métrica de evaluación TDT utiliza una función de costo CDet que combina las probabilidades de perder una nueva historia y una falsa alarma [19]: * * * *Det Miss Miss Target FA FA NontargetC C P P C P P= + (12) Tabla 2. Correlación promedio entre tipos de términos y clases de noticias donde CMiss significa el costo de perder una nueva historia, PMiss significa la probabilidad de perder una nueva historia, y PTarget significa la probabilidad de ver una nueva historia en los datos; CFA significa el costo de una falsa alarma, PFA significa la probabilidad de una falsa alarma, y PNontarget significa la probabilidad de ver una historia antigua. El costo CDet se normaliza de tal manera que un sistema perfecto obtiene una puntuación de 0 y un sistema trivial, que es el mejor de marcar todas las historias como nuevas o antiguas, obtiene una puntuación de 1: ( ( * , * ) ) Det Det Miss Target FA Nontarget C Norm C min C P C P = (13) El sistema de detección de eventos nuevos proporciona dos salidas para cada historia. La primera parte es sí o no, indicando si la historia desencadena un nuevo evento o no. La segunda parte es una puntuación que indica la confianza de la primera decisión. Los puntajes de confianza se pueden utilizar para trazar la curva DET, es decir, curvas que representan la probabilidad de falsa alarma frente a la probabilidad de omisión. El costo normalizado mínimo se puede determinar si se elige el umbral óptimo en la puntuación. RESULTADOS EXPERIMENTALES 7.1 Resultados principales Para probar los enfoques propuestos en el modelo, implementamos y probamos cinco sistemas: Sistema-1: este sistema se utiliza como referencia. Se implementa basándose en el modelo básico descrito en la sección 3, es decir, utilizando el modelo TF-IDF incremental para generar pesos de términos y utilizando la distancia de Hellinger para calcular la similitud entre documentos. La normalización del puntaje de similitud también se emplea [8]. Se utiliza el procedimiento de detección S-S. Sistema-2: este sistema es igual que el sistema-1 excepto que se utiliza el procedimiento de detección S-C. Sistema-3: este sistema es igual que el sistema-1 excepto que utiliza el nuevo procedimiento de detección que se basa en un árbol de indexación. Sistema-4: implementado basado en el enfoque presentado en la sección 5.1, es decir, los términos se vuelven a ponderar según la distancia entre las distribuciones de términos en un grupo y todas las historias. Se utiliza el nuevo procedimiento de detección. Sistema-5: implementado basado en el enfoque presentado en la sección 5.2, es decir, los términos de diferentes tipos son reponderados según la clase de noticias utilizando parámetros entrenados. El nuevo procedimiento de detección se utiliza. Los siguientes son algunos otros sistemas NED: Sistema-6: [21] para cada par de historias, calcula tres valores de similitud para entidad nombrada, entidad no nombrada y todos los términos respectivamente. Y emplear Máquina de Vectores de Soporte para predecir nuevo o antiguo utilizando los valores de similitud como características. Sistema-7: [8] extendió un modelo básico incremental de TF-IDF para incluir modelos específicos de origen, normalización de puntuaciones de similitud basada en promedios específicos de documentos, normalización de puntuaciones de similitud basada en promedios específicos de pares de fuentes, etc. Sistema-8: [13] dividió la representación del documento en dos partes: entidades nombradas y entidades no nombradas, y eligió una parte efectiva para cada clase de noticias. La tabla 3 y la tabla 4 muestran los costos normalizados ponderados por tema y los tiempos de comparación en los conjuntos de datos TDT2 y TDT3 respectivamente. Dado que no se contaba con un conjunto de datos de validación para ajustar el umbral θ nuevo en los experimentos en TDT2, solo informamos los costos normalizados mínimos para nuestros sistemas en la tabla 3. El sistema-5 supera a todos los demás sistemas, incluido el sistema-6, y realiza solo 2.78e+8 veces la comparación de tiempos en el procedimiento de detección, lo que representa solo el 13.4% del sistema-1. Tabla 3. Los resultados de NED en los sistemas TDT2 Min Norm(CDet) tiempos de comparación son: Sistema-1 0.5749 2.08e+9, Sistema-2 0.6673 3.77e+8, Sistema-3 0.5765 2.81e+8, Sistema-4 0.5431 2.99e+8, Sistema-5 0.5089 2.78e+8, Sistema-6 0.5300. Al evaluar en los costos normalizados en TDT3, utilizamos los umbrales óptimos obtenidos del conjunto de datos de TDT2 para todos los sistemas. El sistema-2 reduce los tiempos de comparación a 1.29e+9, que es solo el 18.3% del sistema-1, pero al mismo tiempo también obtiene un costo normalizado mínimo deteriorado que es 0.0499 más alto que el sistema-1. El Sistema-3 utiliza el nuevo procedimiento de detección basado en el índice de noticias en árbol. Requiere incluso menos tiempo de comparación que el sistema-2. Esto se debe a que las comparaciones de historias suelen arrojar mayores similitudes que las de grupos de historias, por lo que las historias tienden a combinarse en el sistema-3. Y el sistema-3 es básicamente equivalente al sistema-1 en los resultados de precisión. El System-4 ajusta los pesos de los términos basándose en la distancia de las distribuciones de términos entre todo el corpus y el conjunto de historias del clúster, lo que resulta en una mejora significativa de 0.0468 en comparación con el system-1. El mejor sistema (sistema-5) tiene un costo normalizado mínimo de 0.5012, lo cual es 0.0797 mejor que el sistema-1, y también mejor que cualquier otro resultado previamente reportado para este conjunto de datos [8, 13]. Además, el sistema-5 solo necesita 1.05e+8 veces de comparación, lo que equivale al 14.9% del sistema-1. Tabla 4. Los resultados de NED en los sistemas TDT3 Norm(CDet) Norma mínima(CDet) Tiempos de comparación Sistema-1 0.6159 0.5809 7.04e+8 Sistema-2① 0.6493 0.6308 1.29e+8 Sistema-3② 0.6197 0.5868 1.03e+8 Sistema-4② 0.5601 0.5341 1.03e+8 Sistema-5② 0.5413 0.5012 1.05e+8 Sistema-7 -- 0.5783 -Sistema-8 -- 0.5229 -① θ nuevo=0.13 ② θ inicial=0.13, λ =3, δ =0.15 La Figura 5 muestra las cinco curvas DET para nuestros sistemas en el conjunto de datos TDT3. El Sistema-5 logra el costo mínimo con una tasa de falsas alarmas de 0.0157 y una tasa de omisión de 0.4310. Podemos observar que System4 y System-5 obtienen una menor probabilidad de error de omisión en regiones de baja probabilidad de falsa alarma. La hipótesis es que se transfiere más valor de peso a los términos clave de los temas que a los términos no clave. La puntuación de similitud entre dos historias pertenecientes a diferentes temas es más baja que antes, porque los términos superpuestos suelen no ser términos clave de sus temas. 7.2 Selección de parámetros para la detección de árboles de indexación. La Figura 3 muestra los costos normalizados mínimos obtenidos por el sistema-3 en TDT3 utilizando diferentes parámetros. El parámetro de inicio θ se prueba en seis valores que van desde 0.03 hasta 0.18. Y el parámetro λ se prueba en cuatro valores: 1, 2, 3 y 4. Podemos ver que, cuando θ inicial se establece en 0.12, que es el más cercano a θ nuevo, los costos son más bajos que los demás. Esto es fácil de explicar, porque cuando se agrupan historias pertenecientes al mismo tema en un conjunto, es más razonable que el conjunto represente las historias en él. Cuando el parámetro λ se establece en 3 o 4, los costos son mejores que en otros casos, pero no hay mucha diferencia entre 3 y 4. 0 0.05 0.1 0.15 0.2 1 2 3 4 0.5 0.6 0.7 0.8 0.9 1 θ-initλ MinCost 0.6 0.65 0.7 0.75 0.8 0.85 0.9 Figura 3. Costo mínimo en TDT3 (δ =0.15) 0 0.05 0.1 0.15 0.2 1 2 3 4 0 0.5 1 1.5 2 2.5 x 10 8 θ-inicio λ Comparando tiempos 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 8 Figura 4. Comparando los tiempos en TDT3 (δ =0.15), la Figura 4 muestra los tiempos de comparación utilizados por el sistema-3 en TDT3 con los mismos parámetros que la Figura 3. Los tiempos de comparación dependen fuertemente de θ init. Debido a que cuanto mayor sea θ init, menos historias se combinan juntas, más veces de comparación se necesitan para la decisión de un nuevo evento. Por lo tanto, usamos θ init = 0.13, λ = 3, δ = 0.15 para los sistemas 3, 4 y 5. En esta configuración de parámetros, podemos obtener tanto costos normalizados mínimos bajos como menos tiempos de comparación. CONCLUSIÓN Hemos propuesto un procedimiento de detección basado en un árbol de indexación de noticias en nuestro modelo. Reduce los tiempos de comparación a aproximadamente una séptima parte del método tradicional sin afectar la precisión de NED. También hemos presentado dos extensiones al modelo básico TF-IDF. La primera extensión se realiza ajustando los pesos de los términos basados en las distribuciones de términos entre todo el corpus y un conjunto de historias de un clúster. Y la segunda extensión al modelo básico TF-IDF es un mejor uso de los tipos de términos (tipos de entidades nombradas y partes de la velocidad) de acuerdo con las categorías de noticias. Nuestros resultados experimentales en los conjuntos de datos TDT2 y TDT3 muestran que ambas extensiones contribuyen significativamente a la mejora en la precisión. No consideramos la información de tiempo de noticias como una pista para la tarea de NED, ya que la mayoría de los temas duran mucho tiempo y los conjuntos de datos de TDT solo abarcan un período relativamente corto (no más de 6 meses). Para el trabajo futuro, queremos recopilar un conjunto de noticias que abarque un período más largo de internet e integrar información temporal en la tarea de NED. Dado que el tema es un grupo de noticias relativamente grueso, también queremos refinar la granularidad del grupo a nivel de evento e identificar diferentes eventos y sus relaciones dentro de un tema. Agradecimientos: Este trabajo ha sido apoyado por la Fundación Nacional de Ciencias Naturales de China bajo la subvención No. 90604025. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son del autor(es) y no necesariamente reflejan las del patrocinador. REFERENCIAS [1] http://www.nist.gov/speech/tests/tdt/index.htm [2] En Detección y Seguimiento de Temas. Organización de la información basada en eventos. Kluwer Academic Publishers, 2002. .01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90 1 2 5 10 20 40 60 80 90 Probabilidad de Falsa Alarma (en %) Probabilidad de Omisión (en %) Curva Ponderada por Tema del SISTEMA1 Norma Mínima (Costo) del SISTEMA1 Curva Ponderada por Tema del SISTEMA2 Norma Mínima (Costo) del SISTEMA2 Curva Ponderada por Tema del SISTEMA3 Norma Mínima (Costo) del SISTEMA3 Curva Ponderada por Tema del SISTEMA4 Norma Mínima (Costo) del SISTEMA4 Curva Ponderada por Tema del SISTEMA5 Norma Mínima (Costo) del SISTEMA5 Rendimiento Aleatorio Figura 5. Curvas DET en TDT3 [3] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B.T. Archibald y X. Liu. Enfoques de aprendizaje para detectar y rastrear eventos de noticias. En el número especial de IEEE Intelligent Systems sobre Aplicaciones de Recuperación de Información Inteligente, volumen 14 (4), 1999, páginas 32-43. [4] Y. Yang, T. Pierce y J. Carbonell. Un estudio sobre la detección de eventos retrospectivos y en línea. En Actas de SIGIR-98, Melbourne, Australia, 1998, 28-36. [5] J. Allan, V. Lavrenko, D. Malin y R. Swan. Detecciones, límites y líneas de tiempo: Umass y tdt-3. En Actas del Taller de Detección y Seguimiento de Temas (TDT-3), Viena, VA, 2000, 167-174. [6] R. Papka y J. Allan. Detección de nuevos eventos en línea utilizando agrupamiento de un solo paso TÍTULO2:. Informe técnico UM-CS1998-021, 1998. [7] W. Lam, H. Meng, K. Wong y J. I'm sorry, but \"Yen\" is not a sentence. Can you please provide a sentence for me to translate into Spanish? Utilizando el Análisis Contextual para la Detección de Eventos de Noticias. Revista Internacional de Sistemas Inteligentes, 2001, 525-546. [8] B. Thorsten, C. Francine y F. Ayman. Un sistema para la detección de nuevos eventos. En Actas de la 26ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2003, 330-337. [9] S. Nicola y C. Joe. Combinando clasificadores semánticos y sintácticos de documentos para mejorar la detección de primeras noticias. En Actas de la 24ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell, and C. Jin. \n\nACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell y C. Jin. Detección de novedades condicionada al tema. En Actas de la 8ª Conferencia Internacional de ACM SIGKDD, ACM Press. 2002, 688-693. [11] M. Juha, A.M. Helena y S. Marko. Aplicando Clases Semánticas en la Detección y Seguimiento de Eventos. En Actas de la Conferencia Internacional sobre Procesamiento del Lenguaje Natural (ICON 2002), 2002, páginas 175-183. [12] M. Juha, A.M. Helena y S. Marko. Semántica simple en la detección y seguimiento de temas. Recuperación de información, 7(3-4): 2004, 347-368. [13] K. Giridhar y J. Allan. Clasificación de texto y entidades nombradas para la detección de nuevos eventos. En Actas de la 27ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2004, 297-304. [14] J. P. Callan, W. B. Croft, y S. M. Harding. El Sistema de Recuperación INQUERY. En Actas de DEXA-92, 3ra Conferencia Internacional sobre Aplicaciones de Bases de Datos y Sistemas Expertos, 1992, 78-83. [15] R. Krovetz. Viendo la morfología como un proceso de inferencia. En Actas de ACM SIGIR93, 1993, 61-81. [16] Y. Yang y J. Pedersen. Un estudio comparativo sobre la selección de características en la categorización de textos. En J. D. H. Fisher, editor, La Decimocuarta Conferencia Internacional sobre Aprendizaje Automático (ICML97), Morgan Kaufmann, 1997, 412-420. [17] T. M. Cover y J. A. I'm sorry, but \"Thomas\" is not a sentence. Can you provide more context or a complete sentence for me to translate into Spanish? Elementos de la teoría de la información. Wiley. 1991. [18] El consorcio de datos lingüísticos, http://www.ldc,upenn.edu/. [19] La definición de la tarea TDT 2001 y el plan de evaluación, http://www.nist.gov/speech/tests/tdt/tdt2001/evalplan.htm. [20] R. E. Schapire y Y. Cantante. Boostexter: Un sistema basado en Boosting para la categorización de texto. En Machine Learning 39(2/3):1, Kluwer Academic Publishers, 2000, 35-168. [21] K. Giridhar y J. Allan. 2005. Utilizando nombres y temas para la detección de nuevos eventos. En Actas de la Conferencia de Tecnología Humana y la Conferencia sobre Métodos Empíricos en Lenguaje Natural, Vancouver, 2005, 121-128 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "volume of news": {
            "translated_key": "volumen de noticias",
            "is_in_text": true,
            "original_annotated_sentences": [
                "New Event Detection Based on Indexing-tree and Named Entity Zhang Kuo Tsinghua University Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Tsinghua University Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Tsinghua University Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn ABSTRACT New Event Detection (NED) aims at detecting from one or multiple streams of news stories that which one is reported on a new event (i.e.",
                "not reported previously).",
                "With the overwhelming <br>volume of news</br> available today, there is an increasing need for a NED system which is able to detect new events more efficiently and accurately.",
                "In this paper we propose a new NED model to speed up the NED task by using news indexing-tree dynamically.",
                "Moreover, based on the observation that terms of different types have different effects for NED task, two term reweighting approaches are proposed to improve NED accuracy.",
                "In the first approach, we propose to adjust term weights dynamically based on previous story clusters and in the second approach, we propose to employ statistics on training data to learn the named entity reweighting model for each class of stories.",
                "Experimental results on two Linguistic Data Consortium (LDC) datasets TDT2 and TDT3 show that the proposed model can improve both efficiency and accuracy of NED task significantly, compared to the baseline system and other existing systems.",
                "Categories and Subject Descriptors H.3.3 [Information Systems]: Information Search and Retrieval; H.4.2 [Information Systems Applications]: Types of Systemsdecision support.",
                "General Terms Algorithms, Performance, Experimentation 1.",
                "INTRODUCTION Topic Detection and Tracking (TDT) program aims to develop techniques which can effectively organize, search and structure news text materials from a variety of newswire and broadcast media [1].",
                "New Event Detection (NED) is one of the five tasks in TDT.",
                "It is the task of online identification of the earliest report for each topic as soon as that report arrives in the sequence of documents.",
                "A Topic is defined as a seminal event or activity, along with directly related events and activities [2].",
                "An Event is defined as something (non-trivial) happening in a certain place at a certain time [3].",
                "For instance, when a bomb explodes in a building, the exploding is the seminal event that triggers the topic, and other stories on the same topic would be those discussing salvaging efforts, the search for perpetrators, arrests and trial and so on.",
                "Useful news information is usually buried in a mass of data generated everyday.",
                "Therefore, NED systems are very useful for people who need to detect novel information from real-time news stream.",
                "These real-life needs often occur in domains like financial markets, news analysis, and intelligence gathering.",
                "In most of state-of-the-art (currently) NED systems, each news story on hand is compared to all the previous received stories.",
                "If all the similarities between them do not exceed a threshold, then the story triggers a new event.",
                "They are usually in the form of cosine similarity or Hellinger similarity metric.",
                "The core problem of NED is to identify whether two stories are on the same topic.",
                "Obviously, these systems cannot take advantage of topic information.",
                "Further more, it is not acceptable in real applications because of the large amount of computation required in the NED process.",
                "Other systems organize previous stories into clusters (each cluster corresponds to a topic), and new story is compared to the previous clusters instead of stories.",
                "This manner can reduce comparing times significantly.",
                "Nevertheless, it has been proved that this manner is less accurate [4, 5].",
                "This is because sometimes stories within a topic drift far away from each other, which could lead low similarity between a story and its topic.",
                "On the other hand, some proposed NED systems tried to improve accuracy by making better use of named entities [10, 11, 12, 13].",
                "However, none of the systems have considered that terms of different types (e.g.",
                "Noun, Verb or Person name) have different effects for different classes of stories in determining whether two stories are on the same topic.",
                "For example, the names of election candidates (Person name) are very important for stories of election class; the locations (Location name) where accidents happened are important for stories of accidents class.",
                "So, in NED, there still exist following three problems to be investigated: (1) How to speed up the detection procedure while do not decrease the detection accuracy? (2) How to make good use of cluster (topic) information to improve accuracy? (3) How to obtain better news story representation by better understanding of named entities.",
                "Driven by these problems, we have proposed three approaches in this paper. (1)To make the detection procedure faster, we propose a new NED procedure based on news indexing-tree created dynamically.",
                "Story indexing-tree is created by assembling similar stories together to form news clusters in different hierarchies according to their values of similarity.",
                "Comparisons between current story and previous clusters could help find the most similar story in less comparing times.",
                "The new procedure can reduce the amount of comparing times without hurting accuracy. (2)We use the clusters of the first floor in the indexing-tree as news topics, in which term weights are adjusted dynamically according to term distribution in the clusters.",
                "In this approach, cluster (topic) information is used properly, so the problem of theme decentralization is avoided. (3)Based on observations on the statistics obtained from training data, we found that terms of different types (e.g.",
                "Noun and Verb) have different effects for different classes of stories in determining whether two stories are on the same topic.",
                "And we propose to use statistics to optimize the weights of the terms of different types in a story according to the news class that the story belongs to.",
                "On TDT3 dataset, the new NED model just uses 14.9% comparing times of the basic model, while its minimum normalized cost is 0.5012, which is 0.0797 better than the basic model, and also better than any other results previously reported for this dataset [8, 13].",
                "The rest of the paper is organized as follows.",
                "We start off this paper by summarizing the previous work in NED in section 2.",
                "Section 3 presents the basic model for NED that most current systems use.",
                "Section 4 describes our new detection procedure based on news indexing-tree.",
                "In section 5, two term reweighting methods are proposed to improve NED accuracy.",
                "Section 6 gives our experimental data and evaluation metrics.",
                "We finally wrap up with the experimental results in Section 7, and the conclusions and future work in Section 8. 2.",
                "RELATED WORK Papka et al. proposed Single-Pass clustering on NED [6].",
                "When a new story was encountered, it was processed immediately to extract term features and a query representation of the storys content is built up.",
                "Then it was compared with all the previous queries.",
                "If the document did not trigger any queries by exceeding a threshold, it was marked as a new event.",
                "Lam et al build up previous query representations of story clusters, each of which corresponds to a topic [7].",
                "In this manner comparisons happen between stories and clusters.",
                "Recent years, most work focus on proposing better methods on comparison of stories and document representation.",
                "Brants et al. [8] extended a basic incremental TF-IDF model to include sourcespecific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, term reweighting based on inverse event frequencies, and segmentation of documents.",
                "Good improvements on TDT bench-marks were shown.",
                "Stokes et al. [9] utilized a combination of evidence from two distinct representations of a documents content.",
                "One of the representations was the usual free text vector, the other made use of lexical chains (created using WordNet) to build another term vector.",
                "Then the two representations are combined in a linear fashion.",
                "A marginal increase in effectiveness was achieved when the combined representation was used.",
                "Some efforts have been done on how to utilize named entities to improve NED.",
                "Yang et al. gave location named entities four times weight than other terms and named entities [10].",
                "DOREMI research group combined semantic similarities of person names, location names and time together with textual similarity [11][12].",
                "UMass [13] research group split document representation into two parts: named entities and non-named entities.",
                "And it was found that some classes of news could achieve better performance using named entity representation, while some other classes of news could achieve better performance using non-named entity representation.",
                "Both [10] and [13] used text categorization technique to classify news stories in advance.",
                "In [13] news stories are classified automatically at first, and then test sensitivities of names and non-name terms for NED for each class.",
                "In [10] frequent terms for each class are removed from document representation.",
                "For example, word election does not help identify different elections.",
                "In their work, effectiveness of different kinds of names (or terms with different POS) for NED in different news classes are not investigated.",
                "We use statistical analysis to reveal the fact and use it to improve NED performance. 3.",
                "BASIC MODEL In this section, we present the basic New Event Detection model which is similar to what most current systems apply.",
                "Then, we propose our new model by extending the basic model.",
                "New Event Detection systems use news story stream as input, in which stories are strictly time-ordered.",
                "Only previously received stories are available when dealing with current story.",
                "The output is a decision for whether the current story is on a new event or not and the confidence of the decision.",
                "Usually, a NED model consists of three parts: story representation, similarity calculation and detection procedure. 3.1 Story Representation Preprocessing is needed before generating story representation.",
                "For preprocessing, we tokenize words, recognize abbreviations, normalize abbreviations, add part-of-speech tags, remove stopwords included in the stop list used in InQuery [14], replace words with their stems using K-stem algorithm[15], and then generate word vector for each news story.",
                "We use incremental TF-IDF model for term weight calculation [4].",
                "In a TF-IDF model, term frequency in a news document is weighted by the inverse document frequency, which is generated from training corpus.",
                "When a new term occurs in testing process, there are two solutions: simply ignore the new term or set df of the term as a small const (e.g. df = 1).",
                "The new term receives too low weight in the first solution (0) and too high weight in the second solution.",
                "In incremental TF-IDF model, document frequencies are updated dynamically in each time step t: 1( ) ( ) ( )t t D tdf w df w df w−= + (1) where Dt represents news story set received in time t, and dfDt(w) means the number of documents that term w occurs in, and dft(w) means the total number of documents that term w occurs in before time t. In this work, each time window includes 50 news stories.",
                "Thus, each story d received in t is represented as follows: 1 2{ ( , , ), ( , , ),..., ( , , )}nd weight d t w weight d t w weight d t w→ where n means the number of distinct terms in story d, and ( , , )weight d t w means the weight of term w in story d at time t: log( ( , ) 1) log(( 1) /( ( ) 0.5)) ( , , ) log( ( , ) 1) log(( 1) /( ( ) 0.5)) t t t t w d tf d w N df w weight d t w tf d w N df w ∈ + + + = + + +∑ (2) where Nt means the total number of news stories before time t, and tf(d,w) means how many times term w occurs in news story d. 3.2 Similarity Calculation We use Hellinger distance for the calculation of similarity between two stories, for two stories d and d at time t, their similarity is defined as follows: , ( , , ) ( , , ) * ( , , ) w d d sim d d t weight d t w weight d t w ∈ = ∑ (3) 3.3 Detection Procedure For each story d received in time step t, the value ( ) ( ) ( ) ( ( , , )) time d time d n d max sim d d t < = (4) is a score used to determine whether d is a story about a new topic and at the same time is an indication of the confidence in our decision [8]. time(d) means the publication time of story d. If the score exceeds the thresholdθ new, then there exists a sufficiently similar document, thus d is a old story, otherwise, there is no sufficiently similar previous document, thus d is an new story. 4.",
                "New NED Procedure Traditional NED systems can be classified into two main types on the aspect of detection procedure: (1) S-S type, in which the story on hand is compared to each story received previously, and use the highest similarity to determine whether current story is about a new event; (2) S-C type, in which the story on hand is compared to all previous clusters each of which representing a topic, and the highest similarity is used for final decision for current story.",
                "If the highest similarity exceeds thresholdθ new, then it is an old story, and put it into the most similar cluster; otherwise it is a new story and create a new cluster.",
                "Previous work show that the first manner is more accurate than the second one [4][5].",
                "Since sometimes stories within a topic drift far away from each other, a story may have very low similarity with its topic.",
                "So using similarities between stories for determining new story is better than using similarities between story and clusters.",
                "Nevertheless, the first manner needs much more comparing times which means the first manner is low efficient.",
                "We propose a new detection procedure which uses comparisons with previous clusters to help find the most similar story in less comparing times, and the final new event decision is made according to the most similar story.",
                "Therefore, we can get both the accuracy of S-S type methods and the efficiency of S-C type methods.",
                "The new procedure creates a news indexing-tree dynamically, in which similar stories are put together to form a hierarchy of clusters.",
                "We index similar stories together by their common ancestor (a cluster node).",
                "Dissimilar stories are indexed in different clusters.",
                "When a story is coming, we use comparisons between the current story and previous hierarchical clusters to help find the most similar story which is useful for new event decision.",
                "After the new event decision is made, the current story is inserted to the indexing-tree for the following detection.",
                "The news indexing-tree is defined formally as follows: S-Tree = {r, NC , NS , E} where r is the root of S-Tree, NC is the set of all cluster nodes, NS is the set of all story nodes, and E is the set of all edges in S-Tree.",
                "We define a set of constraints for a S-Tree: ⅰ . , is an non-terminal node in the treeC i i N i∀ ∈ → ⅱ . , is a terminal node in the treeS i i N i∀ ∈ → ⅲ . , out degree of is at least 2C i i N i∀ ∈ → ⅳ . , is represented as the centroid of its desendantsC i i iN∀ ∈ → For a news story di, the comparison procedure and inserting procedure based on indexing-tree are defined as follows.",
                "An example is shown by Figure 1 and Figure 2.",
                "Figure 1.",
                "Comparison procedure Figure 2.",
                "Inserting procedure Comparison procedure: Step 1: compare di to all the direct child nodes of r and select λ nodes with highest similarities, e.g., C1 2 and C1 3 in Figure 1.",
                "Step 2: for each selected node in the last step, e.g.",
                "C1 2, compare di to all its direct child nodes, and select λ nodes with highest similarities, e.g.",
                "C2 2 and d8.",
                "Repeat step 2 for all non-terminal nodes.",
                "Step 3: record the terminal node with the highest similarty to di, e.g. s5, and the similarity value (0.20).",
                "Inserting di to the S-tree with r as root: Find the node n which is direct child of r in the path from r to the terminal node with highest similarity s, e.g.",
                "C1 2.",
                "If s is smaller than θ init+(h-1)δ , then add di to the tree as a direct child of r. Otherwise, if n is a terminal node, then create a cluster node instead of n, and add both n and di as its direct children; if n is an non-terminal node, then repeat this procedure and insert di to the sub-tree with n as root recursively.",
                "Here h is the length between n and the root of S-tree.",
                "The more the stories in a cluster similar to each other, the better the cluster represents the stories in it.",
                "Hence we add no constraints on the maximum of trees height and degree of a node.",
                "Therefore, we cannot give the complexity of this indexing-tree based procedure.",
                "But we will give the number of comparing times needed by the new procedure in our experiments in section7. 5.",
                "Term Reweighting Methods In this section, two term reweighting methods are proposed to improve NED accuracy.",
                "In the first method, a new way is explored for better using of cluster (topic) information.",
                "The second one finds a better way to make use of named entities based on news classification. 5.1 Term Reweighting Based on Distribution Distance TF-IDF is the most prevalent model used in information retrieval systems.",
                "The basic idea is that the fewer documents a term appears in, the more important the term is in discrimination of documents (relevant or not relevant to a query containing the term).",
                "Nevertheless, in TDT domain, we need to discriminate documents with regard to topics rather than queries.",
                "Intuitively, using cluster (topic) vectors to compare with subsequent news stories should outperform using story vectors.",
                "Unfortunately, the experimental results do not support this intuition [4][5].",
                "Based on observation on data, we find the reason is that a news topic usually contains many directly or indirectly related events, while they all have their own sub-subjects which are usually different with each other.",
                "Take the topic described in section 1 as an example, events like the explosion and salvage have very low similarities with events about criminal trial, therefore stories about trial would have low similarity with the topic vector built on its previous events.",
                "This section focuses on how to effectively make use of topic information and at the same time avoid the problem of content decentralization.",
                "At first, we classify terms into 5 classes to help analysis the needs of the modified model: Term class A: terms that occur frequently in the whole corpus, e.g., year and people.",
                "Terms of this class should be given low weights because they do not help much for topic discrimination.",
                "Term class B: terms that occur frequently within a news category, e.g., election, storm.",
                "They are useful to distinguish two stories in different news categories.",
                "However, they cannot provide information to determine whether two stories are on the same or different topics.",
                "In another words, term election and term storm are not helpful in differentiate two election campaigns and two storm disasters.",
                "Therefore, terms of this class should be assigned lower weights.",
                "Term class C: terms that occur frequently in a topic, and infrequently in other topics, e.g., the name of a crash plane, the name of a specific hurricane.",
                "News stories that belong to different topics rarely have overlap terms in this class.",
                "The more frequently a term appears in a topic, the more important the term is for a story belonging to the topic, therefore the term should be set higher weight.",
                "Term class D: terms that appear in a topic exclusively, but not frequently.",
                "For example, the name of a fireman who did very well in a salvage action, which may appears in only two or three stories but never appeared in other topics.",
                "Terms of this type should receive more weights than in TF-IDF model.",
                "However, since they are not popular in the topic, it is not appropriate to give them too high weights.",
                "Term class E: terms with low document frequency, and appear in different topics.",
                "Terms of this class should receive lower weights.",
                "Now we analyze whether TF-IDF model can give proper weights to the five classes of terms.",
                "Obviously, terms of class A are lowly weighted in TF-IDF model, which is conformable with the requirement described above.",
                "In TF-IDF model, terms of class B are highly dependant with the number of stories in a news class.",
                "TF-IDF model cannot provide low weights if the story containing the term belongs to a relative small news class.",
                "For a term of class C, the more frequently it appears in a topic, the less weight TFIDF model gives to it.",
                "This strongly conflicts with the requirement of terms in class C. For terms of class D, TF-IDF model gives them high weights correctly.",
                "But for terms of class E, TF-IDF model gives high weights to them which are not conformable with the requirement of low weights.",
                "To sum up, terms of class B, C, E cannot be properly weighted in TF-IDF model.",
                "So, we propose a modified model to resolve this problem.",
                "When θ init andθ new are set closely, we assume that most of the stories in a first-level cluster (a direct child node of root node) are on the same topic.",
                "Therefore, we make use of a first-level cluster to capture term distribution (df for all the terms within the cluster) within the topic dynamically.",
                "KL divergence of term distribution in a first-level cluster and the whole story set is used to adjust term weights:  ( , , ) * (1 * ( || )) ( , , ) ( , , ) * (1 * ( || )) cw tw cw tw w d D weight d t w KL P P weight d t w weight d t w KL P P γ γ ∈ + = +∑ (5) where ( ) ( ) ( ) ( ) 1,cw cw c c c c df w df w p y p y N N = = − (6) ( ) ( ) ( ) ( ) 1,t t tw tw t t df w df w p y p y N N = = − (7) where dfc(w) is the number of documents containing term w within cluster C, and Nc is the number of documents in cluster C, and Nt is the total number of documents that arrive before time step t. γ is a const parameter, now is manually set 3.",
                "KL divergence is defined as follows [17]: ( ) ( || ) ( ) log ( )x p x KL P Q p x q x = ∑ (8) The basic idea is: for a story in a topic, the more a term occurs within the topic, and the less it occurs in other topics, it should be assigned higher weights.",
                "Obviously, modified model can meet all the requirements of the five term classes listed above. 5.2 Term Reweighting Based on Term Type and Story Class Previous work found that some classes of news stories could achieve good improvements by giving extra weight to named entities.",
                "But we find that terms of different types should be given different amount of extra weight for different classes of news stories.",
                "We use open-NLP1 to recognize named entity types and part-ofspeech tags for terms that appear in news stories.",
                "Named entity types include person name, organization name, location name, date, time, money and percentage, and five POSs are selected: none (NN), verb (VB), adjective (JJ), adverb (RB) and cardinal number (CD).",
                "Statistical analysis shows topic-level discriminative terms types for different classes of stories.",
                "For the sake of convenience, named entity type and part-of-speech tags are uniformly called term type in subsequent sections.",
                "Determining whether two stories are about the same topic is a basic component for NED task.",
                "So at first we use 2 χ statistic to compute correlations between terms and topics.",
                "For a term t and a topic T, a contingence table is derived: Table 1.",
                "A 2×2 Contingence Table Doc Number belong to topic T not belong to topic T include t A B not include t C D The 2 χ statistic for a specific term t with respect to topic T is defined to be [16]: 2 2 ( , ) ( ) * ( ) ( ) * ( ) * ( ) * ( ) w T A B C D AD CB A C B D A B C D χ = + + + − + + + + (9) News topics for the TDT task are further classified into 11 rules of interpretations (ROIs) 2 .",
                "The ROI can be seen as a higher level class of stories.",
                "The average correlation between a term type and a topic ROI is computed as: 2 avg 2 ( , )( ( , ) )k m m km kT R w P w TP R p w T R P χ χ ∈ ∈ ∑ ∑（ , ）= 1 1 k=1…K, m=1…M (10) where K is the number of term types (set 12 constantly in the paper).",
                "M is the number news classes (ROIs, set 11 in the paper).",
                "Pk represents the set of all terms of type k, and Rm represents the set of all topics of class m, p(t,T) means the probability that t occurs in topic T. Because of limitation of space, only parts of the term types (9 term types) and parts of news classes (8 classes) are listed in table 2 with the average correlation values between them.",
                "The statistics is derived from labeled data in TDT2 corpus. (Results in table 2 are already normalized for convenience in comparison.)",
                "The statistics in table 2 indicates the usefulness of different term types in topic discrimination with respect to different news classes.",
                "We can see that, location name is the most useful term type for three news classes: Natural Disasters, Violence or War, Finances.",
                "And for three other categories Elections, Legal/Criminal Cases, Science and Discovery, person name is the most discriminative term type.",
                "For Scandals/Hearings, date is the most important information for topic discrimination.",
                "In addition, Legal/Criminal Cases and Finance topics have higher correlation with money terms, while Science and Discovery have higher correlation with percentage terms.",
                "Non-name terms are more stable for different classes. 1 . http://opennlp.sourceforge.net/ 2 . http://projects.ldc.upenn.edu/TDT3/Guide/label.html From the analysis of table 2, it is reasonable to adjust term weight according to their term type and the news class the story belongs to.",
                "New term weights are reweighted as follows: ( ) ( ) ( ) ( ) ( , , ) * ( , , ) ( , , ) * class d D type w T class d D type w w d weight d t w weight d t w weight d t w α α ∈ = ∑ (11) where type(w) represents the type of term w, and class(d) represents the class of story d, c kα is reweighting parameter for news class c and term type k. In the work, we just simply use statistics in table 2 as the reweighting parameters.",
                "Even thought using the statistics directly may not the best choice, we do not discuss how to automatically obtain the best parameters.",
                "We will try to use machine learning techniques to obtain the best parameters in the future work.",
                "In the work, we use BoosTexter [20] to classify all stories into one of the 11 ROIs.",
                "BoosTexter is a boosting based machine learning program, which creates a series of simple rules for building a classifier for text or attribute-value data.",
                "We use term weight generated using TF-IDF model as feature for story classification.",
                "We trained the model on the 12000 judged English stories in TDT2, and classify the rest of the stories in TDT2 and all stories in TDT3.",
                "Classification results are used for term reweighting in formula (11).",
                "Since the class labels of topic-off stories are not given in TDT datasets, we cannot give the classification accuracy here.",
                "Thus we do not discuss the effects of classification accuracy to NED performance in the paper. 6.",
                "EXPERIMENTAL SETUP 6.1 Datasets We used two LDC [18] datasets TDT2 and TDT3 for our experiments.",
                "TDT2 contains news stories from January to June 1998.",
                "It contains around 54,000 stories from sources like ABC, Associated Press, CNN, New York Times, Public Radio International, Voice of America etc.",
                "Only English stories in the collection were considered.",
                "TDT3 contains approximately 31,000 English stories collected from October to December 1998.",
                "In addition to the sources used in TDT2, it also contains stories from NBC and MSNBC TV broadcasts.",
                "We used transcribed versions of the TV and radio broadcasts besides textual news.",
                "TDT2 dataset is labeled with about 100 topics, and approximately 12,000 English stories belong to at least one of these topics.",
                "TDT3 dataset is labeled with about 120 topics, and approximately 8000 English stories belong to at least one of these topics.",
                "All the topics are classified into 11 Rules of Interpretation: (1)Elections, (2)Scandals/Hearings, (3)Legal/Criminal Cases, (4)Natural Disasters, (5)Accidents, (6)Ongoing Violence or War, (7)Science and Discovery News, (8)Finance, (9)New Law, (10)Sports News, (11)MISC.",
                "News. 6.2 Evaluation Metric TDT uses a cost function CDet that combines the probabilities of missing a new story and a false alarm [19]: * * * *Det Miss Miss Target FA FA NontargetC C P P C P P= + (12) Table 2.",
                "Average correlation between term types and news classes where CMiss means the cost of missing a new story, PMiss means the probability of missing a new story, and PTarget means the probability of seeing a new story in the data; CFA means the cost of a false alarm, PFA means the probability of a false alarm, and PNontarget means the probability of seeing an old story.",
                "The cost CDet is normalized such that a perfect system scores 0 and a trivial system, which is the better one of mark all stories as new or old, scores 1: ( ( * , * ) ) Det Det Miss Target FA Nontarget C Norm C min C P C P = (13) New event detection system gives two outputs for each story.",
                "The first part is yes or no indicating whether the story triggers a new event or not.",
                "The second part is a score indicating confidence of the first decision.",
                "Confidence scores can be used to plot DET curve, i.e., curves that plot false alarm vs. miss probabilities.",
                "Minimum normalized cost can be determined if optimal threshold on the score were chosen. 7.",
                "EXPERIMENTAL RESULTS 7.1 Main Results To test the approaches proposed in the model, we implemented and tested five systems: System-1: this system is used as baseline.",
                "It is implemented based on the basic model described in section 3, i.e., using incremental TF-IDF model to generate term weights, and using Hellinger distance to compute document similarity.",
                "Similarity score normalization is also employed [8].",
                "S-S detection procedure is used.",
                "System-2: this system is the same as system-1 except that S-C detection procedure is used.",
                "System-3: this system is the same as system-1 except that it uses the new detection procedure which is based on indexing-tree.",
                "System-4: implemented based on the approach presented in section 5.1, i.e., terms are reweighted according to the distance between term distributions in a cluster and all stories.",
                "The new detection procedure is used.",
                "System-5: implemented based on the approach presented in section 5.2, i.e., terms of different types are reweighted according to news class using trained parameters.",
                "The new detection procedure is used.",
                "The following are some other NED systems: System-6: [21] for each pair of stories, it computes three similarity values for named entity, non-named entity and all terms respectively.",
                "And employ Support Vector Machine to predict new or old using the similarity values as features.",
                "System-7: [8] it extended a basic incremental TF-IDF model to include source-specific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, etc.",
                "System-8: [13] it split document representation into two parts: named entities and non-named entities, and choose one effective part for each news class.",
                "Table 3 and table 4 show topic-weighted normalized costs and comparing times on TDT2 and TDT3 datasets respectively.",
                "Since no heldout data set for fine-tuning the threshold θ new was available for experiments on TDT2, we only report minimum normalized costs for our systems in table 3.",
                "System-5 outperforms all other systems including system-6, and it performs only 2.78e+8 comparing times in detection procedure which is only 13.4% of system-1.",
                "Table 3.",
                "NED results on TDT2 Systems Min Norm(CDet) Cmp times System-1 0.5749 2.08e+9 System-2① 0.6673 3.77e+8 System-3② 0.5765 2.81e+8 System-4② 0.5431 2.99e+8 System-5② 0.5089 2.78e+8 System-6 0.5300 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 When evaluating on the normalized costs on TDT3, we use the optimal thresholds obtained from TDT2 data set for all systems.",
                "System-2 reduces comparing times to 1.29e+9 which is just 18.3% of system-1, but at the same time it also gets a deteriorated minimum normalized cost which is 0.0499 higher than system-1.",
                "System-3 uses the new detection procedure based on news indexing-tree.",
                "It requires even less comparing times than system-2.",
                "This is because story-story comparisons usually yield greater similarities than story-cluster ones, so stories tend to be combined Location Person Date Organization Money Percentage NN JJ CD Elections 0.37 1 0.04 0.58 0.08 0.03 0.32 0.13 0.1 Scandals/Hearings 0.66 0.62 0.28 1 0.11 0.02 0.27 0.13 0.05 Legal/Criminal Cases 0.48 1 0.02 0.62 0.15 0 0.22 0.24 0.09 Natural Disasters 1 0.27 0 0.04 0.04 0 0.25 0.04 0.02 Violence or War 1 0.36 0.02 0.14 0.02 0.04 0.21 0.11 0.02 Science and Discovery 0.11 1 0.01 0.22 0.08 0.12 0.19 0.08 0.03 Finances 1 0.45 0.04 0.98 0.13 0.02 0.29 0.06 0.05 Sports 0.16 0.27 0.01 1 0.02 0 0.11 0.03 0.01 together in system-3.",
                "And system-3 is basically equivalent to system-1 in accuracy results.",
                "System-4 adjusts term weights based on the distance of term distributions between the whole corpus and cluster story set, yielding a good improvement by 0.0468 compared to system-1.",
                "The best system (system-5) has a minimum normalized cost 0.5012, which is 0.0797 better than system-1, and also better than any other results previously reported for this dataset [8, 13].",
                "Further more, system-5 only needs 1.05e+8 comparing times which is 14.9% of system-1.",
                "Table 4.",
                "NED results on TDT3 Systems Norm(CDet) Min Norm(CDet) Cmp times System-1 0.6159 0.5809 7.04e+8 System-2① 0.6493 0.6308 1.29e+8 System-3② 0.6197 0.5868 1.03e+8 System-4② 0.5601 0.5341 1.03e+8 System-5② 0.5413 0.5012 1.05e+8 System-7 -- 0.5783 -System-8 -- 0.5229 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 Figure5 shows the five DET curves for our systems on data set TDT3.",
                "System-5 achieves the minimum cost at a false alarm rate of 0.0157 and a miss rate of 0.4310.",
                "We can observe that System4 and System-5 obtain lower miss probability at regions of low false alarm probabilities.",
                "The hypothesis is that, more weight value is transferred to key terms of topics from non-key terms.",
                "Similarity score between two stories belonging to different topics are lower than before, because their overlapping terms are usually not key terms of their topics. 7.2 Parameter selection for indexing-tree detection Figure 3 shows the minimum normalized costs obtained by system-3 on TDT3 using different parameters.",
                "Theθ init parameter is tested on six values spanning from 0.03 to 0.18.",
                "And the λ parameter is tested on four values 1, 2, 3 and 4.",
                "We can see that, whenθ init is set to 0.12, which is the closest one toθ new, the costs are lower than others.",
                "This is easy to explain, because when stories belonging to the same topic are put in a cluster, it is more reasonable for the cluster to represent the stories in it.",
                "When parameter λ is set to 3 or 4, the costs are better than other cases, but there is no much difference between 3 and 4. 0 0.05 0.1 0.15 0.2 1 2 3 4 0.5 0.6 0.7 0.8 0.9 1 θ-initλ MinCost 0.6 0.65 0.7 0.75 0.8 0.85 0.9 Figure 3.",
                "Min Cost on TDT3 (δ =0.15) 0 0.05 0.1 0.15 0.2 1 2 3 4 0 0.5 1 1.5 2 2.5 x 10 8 θ-init λ Comparingtimes 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 8 Figure 4.",
                "Comparing times on TDT3 (δ =0.15) Figure 4 gives the comparing times used by system-3 on TDT3 with the same parameters as figure 3.",
                "The comparing times are strongly dependent onθ init.",
                "Because the greaterθ init is, the less stories combined together, the more comparing times are needed for new event decision.",
                "So we useθ init =0.13,λ =3,δ =0.15 for system-3, 4, and 5.",
                "In this parameter setting, we can get both low minimum normalized costs and less comparing times. 8.",
                "CONCLUSION We have proposed a news indexing-tree based detection procedure in our model.",
                "It reduces comparing times to about one seventh of traditional method without hurting NED accuracy.",
                "We also have presented two extensions to the basic TF-IDF model.",
                "The first extension is made by adjust term weights based on term distributions between the whole corpus and a cluster story set.",
                "And the second extension to basic TF-IDF model is better use of term types (named entities types and part-of-speed) according to news categories.",
                "Our experimental results on TDT2 and TDT3 datasets show that both of the two extensions contribute significantly to improvement in accuracy.",
                "We did not consider news time information as a clue for NED task, since most of the topics last for a long time and TDT data sets only span for a relative short period (no more than 6 months).",
                "For the future work, we want to collect news set which span for a longer period from internet, and integrate time information in NED task.",
                "Since topic is a relative coarse-grained news cluster, we also want to refine cluster granularity to event-level, and identify different events and their relations within a topic.",
                "Acknowledgments This work is supported by the National Natural Science Foundation of China under Grant No. 90604025.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsor. 9.",
                "REFERENCES [1] http://www.nist.gov/speech/tests/tdt/index.htm [2] In Topic Detection and Tracking.",
                "Event-based Information Organization.",
                "Kluwer Academic Publishers, 2002. .01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90 1 2 5 10 20 40 60 80 90 False Alarm Probability (in %) MissProbability(in%) SYSTEM1 Topic Weighted Curve SYSTEM1 Min Norm(Cost) SYSTEM2 Topic Weighted Curve SYSTEM2 Min Norm(Cost) SYSTEM3 Topic Weighted Curve SYSTEM3 Min Norm(Cost) SYSTEM4 Topic Weighted Curve SYSTEM4 Min Norm(Cost) SYSTEM5 Topic Weighted Curve SYSTEM5 Min Norm(Cost) Random Performance Figure 5.",
                "DET curves on TDT3 [3] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B.T.",
                "Archibald, and X. Liu.",
                "Learning Approaches for Detecting and Tracking News Events.",
                "In IEEE Intelligent Systems Special Issue on Applications of Intelligent Information Retrieval, volume 14 (4), 1999, 32-43. [4] Y. Yang, T. Pierce, and J. Carbonell.",
                "A Study on Retrospective and On-line Event Detection.",
                "In Proceedings of SIGIR-98, Melbourne, Australia, 1998, 28-36. [5] J. Allan, V. Lavrenko, D. Malin, and R. Swan.",
                "Detections, Bounds, and Timelines: Umass and tdt-3.",
                "In Proceedings of Topic Detection and Tracking Workshop (TDT-3), Vienna, VA, 2000, 167-174. [6] R. Papka and J. Allan.",
                "On-line New Event Detection Using Single Pass Clustering TITLE2:.",
                "Technical Report UM-CS1998-021, 1998. [7] W. Lam, H. Meng, K. Wong, and J.",
                "Yen.",
                "Using Contextual Analysis for News Event Detection.",
                "International Journal on Intelligent Systems, 2001, 525-546. [8] B. Thorsten, C. Francine, and F. Ayman.",
                "A System for New Event Detection.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2003, 330-337. [9] S. Nicola and C. Joe.",
                "Combining Semantic and Syntactic Document Classifiers to Improve First Story Detection.",
                "In Proceedings of the 24th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topicconditioned Novelty Detection.",
                "In Proceedings of the 8th ACM SIGKDD International Conference, ACM Press. 2002, 688-693. [11] M. Juha, A.M. Helena, and S. Marko.",
                "Applying Semantic Classes in Event Detection and Tracking.",
                "In Proceedings of International Conference on Natural Language Processing (ICON 2002), 2002, pages 175-183. [12] M. Juha, A.M. Helena, and S. Marko.",
                "Simple Semantics in Topic Detection and Tracking.",
                "Information Retrieval, 7(3-4): 2004, 347-368. [13] K. Giridhar and J. Allan.",
                "Text Classification and Named Entities for New Event Detection.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2004, 297-304. [14] J. P. Callan, W. B. Croft, and S. M. Harding.",
                "The INQUERY Retrieval System.",
                "In Proceedings of DEXA-92, 3rd International Conference on Database and Expert Systems Applications, 1992, 78-83. [15] R. Krovetz.",
                "Viewing Morphology as An Inference Process.",
                "In Proceedings of ACM SIGIR93, 1993, 61-81. [16] Y. Yang and J. Pedersen.",
                "A Comparative Study on Feature Selection in Text Categorization.",
                "In J. D. H. Fisher, editor, The Fourteenth International Conference on Machine Learning (ICML97), Morgan Kaufmann, 1997, 412-420. [17] T. M. Cover, and J.A.",
                "Thomas.",
                "Elements of Information Theory.",
                "Wiley. 1991. [18] The linguistic data consortium, http://www.ldc,upenn.edu/. [19] The 2001 TDT task definition and evaluation plan, http://www.nist.gov/speech/tests/tdt/tdt2001/evalplan.htm. [20] R. E. Schapire and Y.",
                "Singer.",
                "Boostexter: A Boosting-based System for Text Categorization.",
                "In Machine Learning 39(2/3):1, Kluwer Academic Publishers, 2000, 35-168. [21] K. Giridhar and J. Allan. 2005.",
                "Using Names and Topics for New Event Detection.",
                "In Proceedings of Human Technology Conference and Conference on Empirical Methods in Natural Language, Vancouver, 2005, 121-128"
            ],
            "original_annotated_samples": [
                "With the overwhelming <br>volume of news</br> available today, there is an increasing need for a NED system which is able to detect new events more efficiently and accurately."
            ],
            "translated_annotated_samples": [
                "Con el abrumador <br>volumen de noticias</br> disponible hoy en día, hay una creciente necesidad de un sistema NED que sea capaz de detectar eventos nuevos de manera más eficiente y precisa."
            ],
            "translated_text": "Detección de nuevos eventos basada en árbol de indexación y entidad nombrada Zhang Kuo Universidad Tsinghua Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Universidad Tsinghua Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Universidad Tsinghua Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn RESUMEN La Detección de Nuevos Eventos (NED) tiene como objetivo detectar de una o varias corrientes de noticias cuál se informa sobre un nuevo evento (es decir, no reportado previamente). Con el abrumador <br>volumen de noticias</br> disponible hoy en día, hay una creciente necesidad de un sistema NED que sea capaz de detectar eventos nuevos de manera más eficiente y precisa. En este artículo proponemos un nuevo modelo de NED para acelerar la tarea de NED mediante el uso dinámico de un árbol de indexación de noticias. Además, basándose en la observación de que los términos de diferentes tipos tienen diferentes efectos para la tarea de NED, se proponen dos enfoques de reponderación de términos para mejorar la precisión de NED. En el primer enfoque, proponemos ajustar dinámicamente los pesos de los términos basados en los clusters de historias previas y en el segundo enfoque, proponemos emplear estadísticas en los datos de entrenamiento para aprender el modelo de reajuste de entidades nombradas para cada clase de historias. Los resultados experimentales en dos conjuntos de datos del Consorcio de Datos Lingüísticos (LDC), TDT2 y TDT3, muestran que el modelo propuesto puede mejorar significativamente tanto la eficiencia como la precisión de la tarea de NED, en comparación con el sistema base y otros sistemas existentes. Categorías y Descriptores de Asignaturas H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información; H.4.2 [Aplicaciones de Sistemas de Información]: Tipos de Sistemas de Apoyo a Decisiones. Términos generales Algoritmos, Rendimiento, Experimentación 1. El programa de Detección y Seguimiento de Temas (TDT) tiene como objetivo desarrollar técnicas que puedan organizar, buscar y estructurar de manera efectiva materiales de texto de noticias de una variedad de agencias de noticias y medios de difusión [1]. La Detección de Eventos Nuevos (DEN) es una de las cinco tareas en TDT. Es la tarea de identificación en línea del informe más temprano para cada tema tan pronto como ese informe llegue en la secuencia de documentos. Un tema se define como un evento o actividad seminal, junto con eventos y actividades directamente relacionados [2]. Un evento se define como algo (no trivial) que sucede en un lugar y en un momento determinados [3]. Por ejemplo, cuando una bomba explota en un edificio, la explosión es el evento seminal que desencadena el tema, y otras historias sobre el mismo tema serían aquellas que discuten los esfuerzos de salvamento, la búsqueda de los perpetradores, arrestos y juicios, entre otros. La información útil de noticias suele estar enterrada en una masa de datos generados a diario. Por lo tanto, los sistemas NED son muy útiles para las personas que necesitan detectar información novedosa de un flujo de noticias en tiempo real. Estas necesidades de la vida real a menudo se presentan en ámbitos como los mercados financieros, el análisis de noticias y la recopilación de inteligencia. En la mayoría de los sistemas NED de última generación, cada noticia disponible se compara con todas las noticias previamente recibidas. Si todas las similitudes entre ellos no superan un umbral, entonces la historia desencadena un nuevo evento. Por lo general, se presentan en forma de similitud de coseno o métrica de similitud de Hellinger. El problema central de NED es identificar si dos historias tratan sobre el mismo tema. Obviamente, estos sistemas no pueden aprovechar la información del tema. Además, no es aceptable en aplicaciones reales debido a la gran cantidad de cálculos requeridos en el proceso de NED. Otros sistemas organizan historias anteriores en grupos (cada grupo corresponde a un tema), y la nueva historia se compara con los grupos anteriores en lugar de con historias. Esta forma puede reducir significativamente los tiempos de comparación. Sin embargo, se ha demostrado que este método es menos preciso [4, 5]. Esto se debe a que a veces las historias dentro de un tema se alejan mucho entre sí, lo que podría resultar en una baja similitud entre una historia y su tema. Por otro lado, algunos sistemas NED propuestos intentaron mejorar la precisión al hacer un mejor uso de las entidades nombradas [10, 11, 12, 13]. Sin embargo, ninguno de los sistemas ha considerado que los términos de diferentes tipos (por ejemplo, Los sustantivos, verbos o nombres de personas tienen diferentes efectos en diferentes clases de historias para determinar si dos historias tratan sobre el mismo tema. Por ejemplo, los nombres de los candidatos electorales (Nombre de la persona) son muy importantes para las historias de la clase de elecciones; las ubicaciones (Nombre de la ubicación) donde ocurrieron los accidentes son importantes para las historias de la clase de accidentes. Por lo tanto, en NED, todavía existen los siguientes tres problemas por investigar: (1) ¿Cómo acelerar el procedimiento de detección sin disminuir la precisión de la detección? (2) ¿Cómo aprovechar mejor la información de los grupos (temas) para mejorar la precisión? (3) ¿Cómo obtener una mejor representación de historias de noticias mediante una mejor comprensión de las entidades nombradas? Impulsados por estos problemas, hemos propuesto tres enfoques en este documento. (1) Para hacer que el procedimiento de detección sea más rápido, proponemos un nuevo procedimiento de NED basado en un árbol de indexación de noticias creado dinámicamente. El índice de historias en árbol se crea ensamblando historias similares para formar grupos de noticias en diferentes jerarquías según sus valores de similitud. Las comparaciones entre la historia actual y los grupos anteriores podrían ayudar a encontrar la historia más similar en menos tiempo de comparación. El nuevo procedimiento puede reducir la cantidad de veces de comparación sin afectar la precisión. Utilizamos los grupos del primer piso en el árbol de indexación como temas de noticias, en los cuales los pesos de los términos se ajustan dinámicamente según la distribución de términos en los grupos. En este enfoque, se utiliza adecuadamente la información de los grupos (temas), por lo que se evita el problema de la descentralización del tema. Basándonos en observaciones de las estadísticas obtenidas de los datos de entrenamiento, encontramos que los términos de diferentes tipos (por ejemplo, Los sustantivos y verbos tienen diferentes efectos para diferentes clases de historias al determinar si dos historias tratan sobre el mismo tema. Y proponemos utilizar estadísticas para optimizar los pesos de los términos de diferentes tipos en una historia de acuerdo a la clase de noticias a la que pertenece la historia. En el conjunto de datos TDT3, el nuevo modelo NED solo utiliza un 14.9% en comparación con los tiempos del modelo básico, mientras que su costo normalizado mínimo es de 0.5012, lo que es 0.0797 mejor que el modelo básico, y también mejor que cualquier otro resultado previamente reportado para este conjunto de datos [8, 13]. El resto del documento está organizado de la siguiente manera. Comenzamos este artículo resumiendo el trabajo previo en NED en la sección 2. La sección 3 presenta el modelo básico para NED que la mayoría de los sistemas actuales utilizan. La sección 4 describe nuestro nuevo procedimiento de detección basado en el índice de árbol de noticias. En la sección 5, se proponen dos métodos de reponderación de términos para mejorar la precisión de NED. La sección 6 presenta nuestros datos experimentales y métricas de evaluación. Finalmente concluimos con los resultados experimentales en la Sección 7, y las conclusiones y trabajo futuro en la Sección 8. TRABAJO RELACIONADO Papka et al. propusieron el agrupamiento de un solo paso en NED [6]. Cuando se encontraba una nueva historia, esta era procesada inmediatamente para extraer características de términos y se construía una representación de consulta del contenido de la historia. Luego se comparó con todas las consultas anteriores. Si el documento no generaba ninguna consulta al exceder un umbral, se marcaba como un nuevo evento. Lam et al construyen representaciones de consultas previas de grupos de historias, cada una de las cuales corresponde a un tema [7]. De esta manera se realizan comparaciones entre historias y grupos. En los últimos años, la mayoría de los trabajos se han centrado en proponer mejores métodos para la comparación de historias y la representación de documentos. Brants et al. [8] extendieron un modelo básico incremental de TF-IDF para incluir modelos específicos de fuente, normalización de puntajes de similitud basada en promedios específicos de documentos, normalización de puntajes de similitud basada en promedios específicos de pares de fuentes, reponderación de términos basada en frecuencias de eventos inversas y segmentación de documentos. Se mostraron buenas mejoras en los puntos de referencia de TDT. Stokes et al. [9] utilizaron una combinación de evidencia de dos representaciones distintas del contenido de un documento. Una de las representaciones era el vector de texto libre habitual, la otra hacía uso de cadenas léxicas (creadas utilizando WordNet) para construir otro vector de términos. Luego, las dos representaciones se combinan de forma lineal. Se logró un aumento marginal en la efectividad cuando se utilizó la representación combinada. Algunos esfuerzos se han realizado sobre cómo utilizar entidades nombradas para mejorar la desambiguación de entidades nombradas (NED). Yang et al. asignaron un peso cuatro veces mayor a las entidades con nombres de ubicación que a otros términos y entidades con nombres [10]. El grupo de investigación DOREMI combinó similitudes semánticas de nombres de personas, nombres de lugares y tiempo junto con similitud textual [11][12]. El grupo de investigación de UMass [13] dividió la representación del documento en dos partes: entidades nombradas y entidades no nombradas. Y se descubrió que algunas clases de noticias podían lograr un mejor rendimiento utilizando la representación de entidades nombradas, mientras que otras clases de noticias podían lograr un mejor rendimiento utilizando la representación sin entidades nombradas. Tanto [10] como [13] utilizaron la técnica de categorización de texto para clasificar previamente las noticias. En [13] las noticias se clasifican automáticamente al principio, y luego se prueban las sensibilidades de los nombres y términos que no son nombres para NED para cada clase. En [10], se eliminan los términos frecuentes de cada clase de la representación del documento. Por ejemplo, la palabra \"elección\" no ayuda a identificar diferentes elecciones. En su trabajo, no se investiga la efectividad de diferentes tipos de nombres (o términos con diferentes categorías gramaticales) para NED en diferentes clases de noticias. Utilizamos análisis estadístico para revelar el hecho y utilizarlo para mejorar el rendimiento de NED. 3. MODELO BÁSICO En esta sección, presentamos el modelo básico de Detección de Nuevos Eventos que es similar a lo que la mayoría de los sistemas actuales aplican. Luego, proponemos nuestro nuevo modelo ampliando el modelo básico. Los sistemas de detección de eventos nuevos utilizan flujos de noticias como entrada, en los cuales las historias están estrictamente ordenadas por tiempo. Solo están disponibles las historias recibidas previamente al tratar con la historia actual. La salida es una decisión sobre si la historia actual es un evento nuevo o no y la confianza de la decisión. Por lo general, un modelo NED consta de tres partes: representación de la historia, cálculo de similitud y procedimiento de detección. 3.1 Representación de la historia Se necesita preprocesamiento antes de generar la representación de la historia. Para el preprocesamiento, tokenizamos palabras, reconocemos abreviaturas, normalizamos abreviaturas, agregamos etiquetas de partes del discurso, eliminamos palabras vacías incluidas en la lista de paradas utilizada en InQuery [14], reemplazamos palabras con sus raíces utilizando el algoritmo K-stem[15], y luego generamos un vector de palabras para cada noticia. Utilizamos el modelo TF-IDF incremental para el cálculo del peso de los términos [4]. En un modelo TF-IDF, la frecuencia de término en un documento de noticias se pondera por la frecuencia inversa del documento, la cual se genera a partir del corpus de entrenamiento. Cuando surge un nuevo término en el proceso de prueba, hay dos soluciones: simplemente ignorar el nuevo término o establecer df del término como una constante pequeña (por ejemplo, df = 1). El nuevo término recibe un peso demasiado bajo en la primera solución (0) y un peso demasiado alto en la segunda solución. En el modelo TF-IDF incremental, las frecuencias de documentos se actualizan dinámicamente en cada paso de tiempo t: 1( ) ( ) ( )t t D tdf w df w df w−= + (1) donde Dt representa el conjunto de noticias recibidas en el tiempo t, y dfDt(w) significa el número de documentos en los que aparece el término w, y dft(w) significa el número total de documentos en los que aparece el término w antes del tiempo t. En este trabajo, cada ventana de tiempo incluye 50 noticias. Así, cada historia d recibida en t se representa de la siguiente manera: 1 2{ ( , , ), ( , , ),..., ( , , )}nd peso d t w peso d t w peso d t w→ donde n significa el número de términos distintos en la historia d, y ( , , )peso d t w significa el peso del término w en la historia d en el tiempo t: log( ( , ) 1) log(( 1) /( ( ) 0.5)) ( , , ) log( ( , ) 1) log(( 1) /( ( ) 0.5)) t t t t w d tf d w N df w peso d t w tf d w N df w ∈ + + + = + + +∑ (2) donde Nt significa el número total de historias de noticias antes del tiempo t, y tf(d,w) significa cuántas veces aparece el término w en la historia de noticias d. 3.2 Cálculo de Similitud Utilizamos la distancia de Hellinger para el cálculo de la similitud entre dos historias, para dos historias d y d en el tiempo t, su similitud se define de la siguiente manera: , ( , , ) ( , , ) * ( , , ) w d d sim d d t peso d t w peso d t w ∈ = ∑ (3) 3.3 Procedimiento de Detección Para cada historia d recibida en el paso de tiempo t, el valor ( ) ( ) ( ) ( ( , , )) tiempo d tiempo d n d max sim d d t < = (4) es un puntaje utilizado para determinar si d es una historia sobre un nuevo tema y al mismo tiempo es una indicación de la confianza en nuestra decisión [8]. tiempo(d) significa el tiempo de publicación de la historia d. Si el puntaje supera el umbral θ new, entonces existe un documento suficientemente similar, por lo tanto, d es una historia antigua, de lo contrario, no hay un documento previo suficientemente similar, por lo tanto, d es una historia nueva. 4. Nuevo procedimiento NED Los sistemas NED tradicionales se pueden clasificar en dos tipos principales en cuanto al procedimiento de detección: (1) tipo S-S, en el que la historia en cuestión se compara con cada historia recibida previamente, y se utiliza la mayor similitud para determinar si la historia actual trata sobre un evento nuevo; (2) tipo S-C, en el que la historia en cuestión se compara con todos los clusters anteriores, cada uno de los cuales representa un tema, y se utiliza la mayor similitud para la decisión final sobre la historia actual. Si la similitud más alta supera el umbral θ nuevo, entonces es una historia antigua y se coloca en el clúster más similar; de lo contrario, es una historia nueva y se crea un nuevo clúster. Trabajos anteriores muestran que la primera forma es más precisa que la segunda [4][5]. Dado que a veces las historias dentro de un tema se alejan mucho entre sí, una historia puede tener una similitud muy baja con su tema. Por lo tanto, utilizar similitudes entre historias para determinar una nueva historia es mejor que utilizar similitudes entre historias y grupos. Sin embargo, el primer método requiere mucho más tiempo de comparación, lo que significa que es menos eficiente. Proponemos un nuevo procedimiento de detección que utiliza comparaciones con clusters anteriores para ayudar a encontrar la historia más similar en menos tiempo de comparación, y la decisión final sobre el nuevo evento se toma de acuerdo con la historia más similar. Por lo tanto, podemos obtener tanto la precisión de los métodos de tipo S-S como la eficiencia de los métodos de tipo S-C. El nuevo procedimiento crea un árbol de indexación de noticias dinámicamente, en el cual se agrupan historias similares para formar una jerarquía de clusters. Indexamos historias similares juntas por su ancestro común (un nodo de clúster). Historias diferentes están indexadas en diferentes grupos. Cuando una historia está llegando, utilizamos comparaciones entre la historia actual y los grupos jerárquicos anteriores para ayudar a encontrar la historia más similar que es útil para la toma de decisiones sobre nuevos eventos. Después de que se tome la decisión sobre el nuevo evento, la historia actual se inserta en el árbol de indexación para la detección siguiente. El índice de noticias en forma de árbol se define formalmente de la siguiente manera: S-Tree = {r, NC, NS, E}, donde r es la raíz de S-Tree, NC es el conjunto de todos los nodos de clúster, NS es el conjunto de todos los nodos de historia, y E es el conjunto de todos los bordes en S-Tree. Definimos un conjunto de restricciones para un S-Árbol: ⅰ. , es un nodo no terminal en el árbolC i i N i∀ ∈ → ⅱ. , es un nodo terminal en el árbolS i i N i∀ ∈ → ⅲ. , el grado de salida de es al menos 2C i i N i∀ ∈ → ⅳ. , se representa como el centroide de sus descendientesC i i iN∀ ∈ → Para una noticia di, el procedimiento de comparación y el procedimiento de inserción basados en el árbol de indexación se definen de la siguiente manera. Un ejemplo se muestra en la Figura 1 y la Figura 2. Figura 1. Procedimiento de comparación Figura 2. Procedimiento de comparación de inserción: Paso 1: comparar di con todos los nodos hijos directos de r y seleccionar λ nodos con las similitudes más altas, por ejemplo, C1 2 y C1 3 en la Figura 1. Paso 2: para cada nodo seleccionado en el paso anterior, por ejemplo. C1 2, compara di con todos sus nodos hijos directos y selecciona λ nodos con las similitudes más altas, por ejemplo. C2 2 y d8. Repetir el paso 2 para todos los nodos no terminales. Paso 3: registrar el nodo terminal con la mayor similitud a di, por ejemplo, s5, y el valor de similitud (0.20). Insertando di en el S-árbol con r como raíz: Encuentra el nodo n que es hijo directo de r en el camino desde r hasta el nodo terminal con la similitud más alta s, por ejemplo. C1 2. Si s es menor que θ init + (h-1)δ, entonces agregar di al árbol como un hijo directo de r. De lo contrario, si n es un nodo terminal, crear un nodo de clúster en lugar de n, y agregar tanto n como di como sus hijos directos; si n es un nodo no terminal, repetir este procedimiento e insertar di en el subárbol con n como raíz de forma recursiva. Aquí h es la longitud entre n y la raíz del árbol S. Cuanto más similares sean las historias en un grupo, mejor representará el grupo a las historias en él. Por lo tanto, no imponemos restricciones en la altura máxima de los árboles y el grado de un nodo. Por lo tanto, no podemos dar la complejidad de este procedimiento basado en árboles de indexación. Pero proporcionaremos el número de veces de comparación necesarias por el nuevo procedimiento en nuestros experimentos en la sección 7.5. En esta sección, se proponen dos métodos de reponderación de términos para mejorar la precisión de NED. En el primer método, se explora una nueva forma de utilizar de manera más efectiva la información de los grupos (temas). El segundo encuentra una mejor manera de utilizar entidades nombradas basadas en la clasificación de noticias. 5.1 Reajuste de términos basado en la distancia de distribución TF-IDF es el modelo más prevalente utilizado en sistemas de recuperación de información. La idea básica es que cuanto menos documentos aparezca un término, más importante es el término en la discriminación de documentos (relevantes o no relevantes para una consulta que contenga el término). Sin embargo, en el dominio de TDT, necesitamos discriminar documentos en función de los temas en lugar de las consultas. Intuitivamente, el uso de vectores de clúster (tema) para comparar con historias de noticias posteriores debería tener un mejor rendimiento que el uso de vectores de historias. Desafortunadamente, los resultados experimentales no respaldan esta intuición [4][5]. Basándonos en la observación de los datos, encontramos que la razón es que un tema de noticias generalmente contiene muchos eventos directa o indirectamente relacionados, y todos ellos tienen sus propios subtemas que suelen ser diferentes entre sí. Tomemos el tema descrito en la sección 1 como ejemplo, eventos como la explosión y el rescate tienen similitudes muy bajas con eventos sobre juicios criminales, por lo tanto, las historias sobre juicios tendrían una baja similitud con el vector de tema construido en base a sus eventos previos. Esta sección se centra en cómo hacer un uso efectivo de la información del tema y al mismo tiempo evitar el problema de la descentralización del contenido. Al principio, clasificamos los términos en 5 clases para ayudar en el análisis de las necesidades del modelo modificado: Clase de términos A: términos que ocurren con frecuencia en todo el corpus, por ejemplo, año y personas. Los términos de esta clase deben recibir pesos bajos porque no ayudan mucho en la discriminación de temas. Clase B de términos: términos que ocurren con frecuencia dentro de una categoría de noticias, por ejemplo, elección, tormenta. Son útiles para distinguir dos historias en diferentes categorías de noticias. Sin embargo, no pueden proporcionar información para determinar si dos historias tratan sobre el mismo tema o temas diferentes. En otras palabras, los términos elección y tormenta no son útiles para diferenciar dos campañas electorales y dos desastres de tormenta. Por lo tanto, los términos de esta clase deberían asignarse pesos más bajos. Clase de término C: términos que ocurren con frecuencia en un tema y raramente en otros temas, por ejemplo, el nombre de un avión accidentado, el nombre de un huracán específico. Las noticias que pertenecen a diferentes temas rara vez tienen términos superpuestos en esta clase. Cuanto más frecuentemente aparezca un término en un tema, más importante es el término para una historia perteneciente al tema, por lo tanto, el término debería tener un peso más alto. Clase de término D: términos que aparecen en un tema de manera exclusiva, pero no con frecuencia. Por ejemplo, el nombre de un bombero que se desempeñó muy bien en una acción de salvamento, que puede aparecer en solo dos o tres historias pero nunca ha aparecido en otros temas. Los términos de este tipo deberían recibir más peso que en el modelo TF-IDF. Sin embargo, dado que no son populares en el tema, no es apropiado asignarles pesos demasiado altos. Clase de término E: términos con baja frecuencia en documentos y que aparecen en diferentes temas. Los términos de esta clase deberían recibir pesos más bajos. Ahora analizamos si el modelo TF-IDF puede asignar pesos adecuados a las cinco clases de términos. Obviamente, los términos de la clase A tienen un peso bajo en el modelo TF-IDF, lo cual es acorde con el requisito descrito anteriormente. En el modelo TF-IDF, los términos de la clase B dependen altamente del número de historias en una clase de noticias. El modelo TF-IDF no puede proporcionar pesos bajos si la historia que contiene el término pertenece a una clase de noticias relativamente pequeña. Para un término de clase C, cuanto más frecuentemente aparezca en un tema, menos peso le da el modelo TFIDF. Esto entra en conflicto directo con el requisito de términos en la clase C. Para los términos de la clase D, el modelo TF-IDF les asigna pesos altos de manera correcta. Pero para los términos de la clase E, el modelo TF-IDF asigna pesos altos a ellos que no son conformes con el requisito de pesos bajos. En resumen, los términos de clase B, C, E no pueden ser ponderados adecuadamente en el modelo TF-IDF. Por lo tanto, proponemos un modelo modificado para resolver este problema. Cuando θ init y θ new están configurados de cerca, asumimos que la mayoría de las historias en un clúster de primer nivel (un nodo hijo directo del nodo raíz) están en el mismo tema. Por lo tanto, utilizamos un clúster de primer nivel para capturar la distribución de términos (df para todos los términos dentro del clúster) dentro del tema de forma dinámica. La divergencia de Kullback-Leibler de la distribución de términos en un clúster de primer nivel y el conjunto completo de historias se utiliza para ajustar los pesos de los términos: ( , , ) * (1 * ( || )) ( , , ) ( , , ) * (1 * ( || )) cw tw cw tw w d D peso d t w KL P P peso d t w peso d t w KL P P γ γ ∈ + = +∑ (5) donde ( ) ( ) ( ) ( ) 1,cw cw c c c c df w df w p y p y N N = = − (6) ( ) ( ) ( ) ( ) 1,t t tw tw t t df w df w p y p y N N = = − (7) donde dfc(w) es el número de documentos que contienen el término w dentro del clúster C, y Nc es el número de documentos en el clúster C, y Nt es el número total de documentos que llegan antes del paso de tiempo t. γ es un parámetro constante, actualmente configurado manualmente en 3. La divergencia de Kullback-Leibler se define de la siguiente manera [17]: La idea básica es que, para una historia en un tema, cuanto más ocurre un término dentro del tema y menos ocurre en otros temas, se le deben asignar pesos más altos. Obviamente, el modelo modificado puede cumplir con todos los requisitos de las cinco clases de términos mencionadas anteriormente. 5.2 Reajuste de Términos Basado en el Tipo de Término y la Clase de Historia. Trabajos anteriores encontraron que algunas clases de historias de noticias podrían lograr mejoras significativas al dar un peso adicional a las entidades nombradas. Pero encontramos que los términos de diferentes tipos deben recibir una cantidad diferente de peso adicional para diferentes clases de noticias. Utilizamos open-NLP1 para reconocer tipos de entidades nombradas y etiquetas de partes del discurso para términos que aparecen en noticias. Los tipos de entidades nombradas incluyen nombre de persona, nombre de organización, nombre de ubicación, fecha, hora, dinero y porcentaje, y se seleccionan cinco partes del discurso: ninguno (NN), verbo (VB), adjetivo (JJ), adverbio (RB) y número cardinal (CD). El análisis estadístico muestra tipos de términos discriminativos a nivel de tema para diferentes clases de historias. Por conveniencia, los tipos de entidad nombrada y etiquetas de parte de la oración se llaman uniformemente tipo de término en las secciones siguientes. Determinar si dos historias tratan sobre el mismo tema es un componente básico para la tarea de NED. Por lo tanto, al principio usamos la estadística χ² para calcular las correlaciones entre términos y temas. Para un término t y un tema T, se deriva una tabla de contingencia: Tabla 1. Una tabla de contingencia de 2×2 con el número de documentos pertenecientes al tema T y no pertenecientes al tema T incluye t A B no incluye t C D. La estadística χ² para un término específico t con respecto al tema T se define como [16]: χ² = ( ) ( ) * ( ) ( ) * ( ) * ( ) * ( ) w T A B C D AD CB A C B D A B C D + + + − + + + + (9). Los temas de noticias para la tarea TDT se clasifican además en 11 reglas de interpretaciones (ROIs) 2. El ROI se puede ver como una clase de historias de nivel superior. El promedio de correlación entre un tipo de término y un ROI de tema se calcula como: 2 avg 2 ( , )( ( , ) )k m m km kT R w P w TP R p w T R P χ χ ∈ ∈ ∑ ∑（ , ）= 1 1 k=1…K, m=1…M (10) donde K es el número de tipos de términos (fijado constantemente en 12 en el artículo). M es el número de nuevas clases (ROIs, establecido en el conjunto 11 del artículo). Pk representa el conjunto de todos los términos de tipo k, y Rm representa el conjunto de todos los temas de la clase m, p(t,T) significa la probabilidad de que t ocurra en el tema T. Debido a limitaciones de espacio, solo se enumeran partes de los tipos de términos (9 tipos de términos) y partes de las clases de noticias (8 clases) en la tabla 2 con los valores promedio de correlación entre ellos. Las estadísticas se derivan de datos etiquetados en el corpus TDT2. (Los resultados en la tabla 2 ya están normalizados para mayor comodidad en la comparación). Las estadísticas en la tabla 2 indican la utilidad de diferentes tipos de términos en la discriminación de temas con respecto a diferentes clases de noticias. Podemos ver que el nombre de la ubicación es el tipo de término más útil para tres clases de noticias: Desastres Naturales, Violencia o Guerra, Finanzas. Y para otras tres categorías: Elecciones, Casos Legales/Criminales, Ciencia y Descubrimiento, el nombre de la persona es el tipo de término más discriminativo. Para Escándalos/Audiencias, la fecha es la información más importante para la discriminación de temas. Además, los casos legales/criminales y los temas financieros tienen una correlación más alta con términos relacionados con el dinero, mientras que la ciencia y el descubrimiento tienen una correlación más alta con términos de porcentaje. Los términos no nominales son más estables para diferentes clases. 1. http://opennlp.sourceforge.net/ 2. http://projects.ldc.upenn.edu/TDT3/Guide/label.html A partir del análisis de la tabla 2, es razonable ajustar el peso de los términos según su tipo de término y la clase de noticias a la que pertenece la historia. Los nuevos pesos de los términos se recalculan de la siguiente manera: ( ) ( ) ( ) ( ) ( , , ) * ( , , ) ( , , ) * clase d D tipo w T clase d D tipo w w d peso d t w peso d t w peso d t w α α ∈ = ∑ (11) donde type(w) representa el tipo del término w, y class(d) representa la clase de la historia d, c kα es el parámetro de recalibración para la clase de noticias c y el tipo de término k. En el trabajo, simplemente utilizamos las estadísticas de la tabla 2 como parámetros de recalibración. Aunque usar las estadísticas directamente puede que no sea la mejor opción, no discutimos cómo obtener automáticamente los mejores parámetros. Intentaremos utilizar técnicas de aprendizaje automático para obtener los mejores parámetros en el trabajo futuro. En el trabajo, utilizamos BoosTexter [20] para clasificar todas las historias en uno de los 11 ROIs. BoosTexter es un programa de aprendizaje automático basado en boosting, que crea una serie de reglas simples para construir un clasificador de datos de texto o atributos-valor. Utilizamos el peso del término generado utilizando el modelo TF-IDF como característica para la clasificación de historias. Entrenamos el modelo con las 12000 historias en inglés evaluadas en TDT2, y clasificamos el resto de las historias en TDT2 y todas las historias en TDT3. Los resultados de clasificación se utilizan para la reponderación de términos en la fórmula (11). Dado que las etiquetas de clase de las historias de tema apagado no se proporcionan en los conjuntos de datos de TDT, no podemos proporcionar la precisión de clasificación aquí. Por lo tanto, no discutimos los efectos de la precisión de la clasificación en el rendimiento de NED en el artículo. 6. CONFIGURACIÓN EXPERIMENTAL 6.1 Conjuntos de datos Utilizamos dos conjuntos de datos de LDC [18], TDT2 y TDT3, para nuestros experimentos. TDT2 contiene noticias desde enero hasta junio de 1998. Contiene alrededor de 54,000 historias de fuentes como ABC, Associated Press, CNN, New York Times, Public Radio International, Voice of America, etc. Solo se consideraron historias en inglés en la colección. TDT3 contiene aproximadamente 31,000 historias en inglés recopiladas de octubre a diciembre de 1998. Además de las fuentes utilizadas en TDT2, también contiene historias de las transmisiones de televisión de NBC y MSNBC. Utilizamos versiones transcritas de las emisiones de televisión y radio además de noticias textuales. El conjunto de datos TDT2 está etiquetado con alrededor de 100 temas, y aproximadamente 12,000 historias en inglés pertenecen al menos a uno de estos temas. El conjunto de datos TDT3 está etiquetado con alrededor de 120 temas, y aproximadamente 8000 historias en inglés pertenecen al menos a uno de estos temas. Todos los temas están clasificados en 11 Reglas de Interpretación: (1) Elecciones, (2) Escándalos/Audiencias, (3) Casos Legales/Criminales, (4) Desastres Naturales, (5) Accidentes, (6) Violencia o Guerra en Curso, (7) Noticias de Ciencia y Descubrimientos, (8) Finanzas, (9) Nueva Ley, (10) Noticias Deportivas, (11) Varios. Noticias. 6.2 Métrica de evaluación TDT utiliza una función de costo CDet que combina las probabilidades de perder una nueva historia y una falsa alarma [19]: * * * *Det Miss Miss Target FA FA NontargetC C P P C P P= + (12) Tabla 2. Correlación promedio entre tipos de términos y clases de noticias donde CMiss significa el costo de perder una nueva historia, PMiss significa la probabilidad de perder una nueva historia, y PTarget significa la probabilidad de ver una nueva historia en los datos; CFA significa el costo de una falsa alarma, PFA significa la probabilidad de una falsa alarma, y PNontarget significa la probabilidad de ver una historia antigua. El costo CDet se normaliza de tal manera que un sistema perfecto obtiene una puntuación de 0 y un sistema trivial, que es el mejor de marcar todas las historias como nuevas o antiguas, obtiene una puntuación de 1: ( ( * , * ) ) Det Det Miss Target FA Nontarget C Norm C min C P C P = (13) El sistema de detección de eventos nuevos proporciona dos salidas para cada historia. La primera parte es sí o no, indicando si la historia desencadena un nuevo evento o no. La segunda parte es una puntuación que indica la confianza de la primera decisión. Los puntajes de confianza se pueden utilizar para trazar la curva DET, es decir, curvas que representan la probabilidad de falsa alarma frente a la probabilidad de omisión. El costo normalizado mínimo se puede determinar si se elige el umbral óptimo en la puntuación. RESULTADOS EXPERIMENTALES 7.1 Resultados principales Para probar los enfoques propuestos en el modelo, implementamos y probamos cinco sistemas: Sistema-1: este sistema se utiliza como referencia. Se implementa basándose en el modelo básico descrito en la sección 3, es decir, utilizando el modelo TF-IDF incremental para generar pesos de términos y utilizando la distancia de Hellinger para calcular la similitud entre documentos. La normalización del puntaje de similitud también se emplea [8]. Se utiliza el procedimiento de detección S-S. Sistema-2: este sistema es igual que el sistema-1 excepto que se utiliza el procedimiento de detección S-C. Sistema-3: este sistema es igual que el sistema-1 excepto que utiliza el nuevo procedimiento de detección que se basa en un árbol de indexación. Sistema-4: implementado basado en el enfoque presentado en la sección 5.1, es decir, los términos se vuelven a ponderar según la distancia entre las distribuciones de términos en un grupo y todas las historias. Se utiliza el nuevo procedimiento de detección. Sistema-5: implementado basado en el enfoque presentado en la sección 5.2, es decir, los términos de diferentes tipos son reponderados según la clase de noticias utilizando parámetros entrenados. El nuevo procedimiento de detección se utiliza. Los siguientes son algunos otros sistemas NED: Sistema-6: [21] para cada par de historias, calcula tres valores de similitud para entidad nombrada, entidad no nombrada y todos los términos respectivamente. Y emplear Máquina de Vectores de Soporte para predecir nuevo o antiguo utilizando los valores de similitud como características. Sistema-7: [8] extendió un modelo básico incremental de TF-IDF para incluir modelos específicos de origen, normalización de puntuaciones de similitud basada en promedios específicos de documentos, normalización de puntuaciones de similitud basada en promedios específicos de pares de fuentes, etc. Sistema-8: [13] dividió la representación del documento en dos partes: entidades nombradas y entidades no nombradas, y eligió una parte efectiva para cada clase de noticias. La tabla 3 y la tabla 4 muestran los costos normalizados ponderados por tema y los tiempos de comparación en los conjuntos de datos TDT2 y TDT3 respectivamente. Dado que no se contaba con un conjunto de datos de validación para ajustar el umbral θ nuevo en los experimentos en TDT2, solo informamos los costos normalizados mínimos para nuestros sistemas en la tabla 3. El sistema-5 supera a todos los demás sistemas, incluido el sistema-6, y realiza solo 2.78e+8 veces la comparación de tiempos en el procedimiento de detección, lo que representa solo el 13.4% del sistema-1. Tabla 3. Los resultados de NED en los sistemas TDT2 Min Norm(CDet) tiempos de comparación son: Sistema-1 0.5749 2.08e+9, Sistema-2 0.6673 3.77e+8, Sistema-3 0.5765 2.81e+8, Sistema-4 0.5431 2.99e+8, Sistema-5 0.5089 2.78e+8, Sistema-6 0.5300. Al evaluar en los costos normalizados en TDT3, utilizamos los umbrales óptimos obtenidos del conjunto de datos de TDT2 para todos los sistemas. El sistema-2 reduce los tiempos de comparación a 1.29e+9, que es solo el 18.3% del sistema-1, pero al mismo tiempo también obtiene un costo normalizado mínimo deteriorado que es 0.0499 más alto que el sistema-1. El Sistema-3 utiliza el nuevo procedimiento de detección basado en el índice de noticias en árbol. Requiere incluso menos tiempo de comparación que el sistema-2. Esto se debe a que las comparaciones de historias suelen arrojar mayores similitudes que las de grupos de historias, por lo que las historias tienden a combinarse en el sistema-3. Y el sistema-3 es básicamente equivalente al sistema-1 en los resultados de precisión. El System-4 ajusta los pesos de los términos basándose en la distancia de las distribuciones de términos entre todo el corpus y el conjunto de historias del clúster, lo que resulta en una mejora significativa de 0.0468 en comparación con el system-1. El mejor sistema (sistema-5) tiene un costo normalizado mínimo de 0.5012, lo cual es 0.0797 mejor que el sistema-1, y también mejor que cualquier otro resultado previamente reportado para este conjunto de datos [8, 13]. Además, el sistema-5 solo necesita 1.05e+8 veces de comparación, lo que equivale al 14.9% del sistema-1. Tabla 4. Los resultados de NED en los sistemas TDT3 Norm(CDet) Norma mínima(CDet) Tiempos de comparación Sistema-1 0.6159 0.5809 7.04e+8 Sistema-2① 0.6493 0.6308 1.29e+8 Sistema-3② 0.6197 0.5868 1.03e+8 Sistema-4② 0.5601 0.5341 1.03e+8 Sistema-5② 0.5413 0.5012 1.05e+8 Sistema-7 -- 0.5783 -Sistema-8 -- 0.5229 -① θ nuevo=0.13 ② θ inicial=0.13, λ =3, δ =0.15 La Figura 5 muestra las cinco curvas DET para nuestros sistemas en el conjunto de datos TDT3. El Sistema-5 logra el costo mínimo con una tasa de falsas alarmas de 0.0157 y una tasa de omisión de 0.4310. Podemos observar que System4 y System-5 obtienen una menor probabilidad de error de omisión en regiones de baja probabilidad de falsa alarma. La hipótesis es que se transfiere más valor de peso a los términos clave de los temas que a los términos no clave. La puntuación de similitud entre dos historias pertenecientes a diferentes temas es más baja que antes, porque los términos superpuestos suelen no ser términos clave de sus temas. 7.2 Selección de parámetros para la detección de árboles de indexación. La Figura 3 muestra los costos normalizados mínimos obtenidos por el sistema-3 en TDT3 utilizando diferentes parámetros. El parámetro de inicio θ se prueba en seis valores que van desde 0.03 hasta 0.18. Y el parámetro λ se prueba en cuatro valores: 1, 2, 3 y 4. Podemos ver que, cuando θ inicial se establece en 0.12, que es el más cercano a θ nuevo, los costos son más bajos que los demás. Esto es fácil de explicar, porque cuando se agrupan historias pertenecientes al mismo tema en un conjunto, es más razonable que el conjunto represente las historias en él. Cuando el parámetro λ se establece en 3 o 4, los costos son mejores que en otros casos, pero no hay mucha diferencia entre 3 y 4. 0 0.05 0.1 0.15 0.2 1 2 3 4 0.5 0.6 0.7 0.8 0.9 1 θ-initλ MinCost 0.6 0.65 0.7 0.75 0.8 0.85 0.9 Figura 3. Costo mínimo en TDT3 (δ =0.15) 0 0.05 0.1 0.15 0.2 1 2 3 4 0 0.5 1 1.5 2 2.5 x 10 8 θ-inicio λ Comparando tiempos 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 8 Figura 4. Comparando los tiempos en TDT3 (δ =0.15), la Figura 4 muestra los tiempos de comparación utilizados por el sistema-3 en TDT3 con los mismos parámetros que la Figura 3. Los tiempos de comparación dependen fuertemente de θ init. Debido a que cuanto mayor sea θ init, menos historias se combinan juntas, más veces de comparación se necesitan para la decisión de un nuevo evento. Por lo tanto, usamos θ init = 0.13, λ = 3, δ = 0.15 para los sistemas 3, 4 y 5. En esta configuración de parámetros, podemos obtener tanto costos normalizados mínimos bajos como menos tiempos de comparación. CONCLUSIÓN Hemos propuesto un procedimiento de detección basado en un árbol de indexación de noticias en nuestro modelo. Reduce los tiempos de comparación a aproximadamente una séptima parte del método tradicional sin afectar la precisión de NED. También hemos presentado dos extensiones al modelo básico TF-IDF. La primera extensión se realiza ajustando los pesos de los términos basados en las distribuciones de términos entre todo el corpus y un conjunto de historias de un clúster. Y la segunda extensión al modelo básico TF-IDF es un mejor uso de los tipos de términos (tipos de entidades nombradas y partes de la velocidad) de acuerdo con las categorías de noticias. Nuestros resultados experimentales en los conjuntos de datos TDT2 y TDT3 muestran que ambas extensiones contribuyen significativamente a la mejora en la precisión. No consideramos la información de tiempo de noticias como una pista para la tarea de NED, ya que la mayoría de los temas duran mucho tiempo y los conjuntos de datos de TDT solo abarcan un período relativamente corto (no más de 6 meses). Para el trabajo futuro, queremos recopilar un conjunto de noticias que abarque un período más largo de internet e integrar información temporal en la tarea de NED. Dado que el tema es un grupo de noticias relativamente grueso, también queremos refinar la granularidad del grupo a nivel de evento e identificar diferentes eventos y sus relaciones dentro de un tema. Agradecimientos: Este trabajo ha sido apoyado por la Fundación Nacional de Ciencias Naturales de China bajo la subvención No. 90604025. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son del autor(es) y no necesariamente reflejan las del patrocinador. REFERENCIAS [1] http://www.nist.gov/speech/tests/tdt/index.htm [2] En Detección y Seguimiento de Temas. Organización de la información basada en eventos. Kluwer Academic Publishers, 2002. .01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90 1 2 5 10 20 40 60 80 90 Probabilidad de Falsa Alarma (en %) Probabilidad de Omisión (en %) Curva Ponderada por Tema del SISTEMA1 Norma Mínima (Costo) del SISTEMA1 Curva Ponderada por Tema del SISTEMA2 Norma Mínima (Costo) del SISTEMA2 Curva Ponderada por Tema del SISTEMA3 Norma Mínima (Costo) del SISTEMA3 Curva Ponderada por Tema del SISTEMA4 Norma Mínima (Costo) del SISTEMA4 Curva Ponderada por Tema del SISTEMA5 Norma Mínima (Costo) del SISTEMA5 Rendimiento Aleatorio Figura 5. Curvas DET en TDT3 [3] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B.T. Archibald y X. Liu. Enfoques de aprendizaje para detectar y rastrear eventos de noticias. En el número especial de IEEE Intelligent Systems sobre Aplicaciones de Recuperación de Información Inteligente, volumen 14 (4), 1999, páginas 32-43. [4] Y. Yang, T. Pierce y J. Carbonell. Un estudio sobre la detección de eventos retrospectivos y en línea. En Actas de SIGIR-98, Melbourne, Australia, 1998, 28-36. [5] J. Allan, V. Lavrenko, D. Malin y R. Swan. Detecciones, límites y líneas de tiempo: Umass y tdt-3. En Actas del Taller de Detección y Seguimiento de Temas (TDT-3), Viena, VA, 2000, 167-174. [6] R. Papka y J. Allan. Detección de nuevos eventos en línea utilizando agrupamiento de un solo paso TÍTULO2:. Informe técnico UM-CS1998-021, 1998. [7] W. Lam, H. Meng, K. Wong y J. I'm sorry, but \"Yen\" is not a sentence. Can you please provide a sentence for me to translate into Spanish? Utilizando el Análisis Contextual para la Detección de Eventos de Noticias. Revista Internacional de Sistemas Inteligentes, 2001, 525-546. [8] B. Thorsten, C. Francine y F. Ayman. Un sistema para la detección de nuevos eventos. En Actas de la 26ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2003, 330-337. [9] S. Nicola y C. Joe. Combinando clasificadores semánticos y sintácticos de documentos para mejorar la detección de primeras noticias. En Actas de la 24ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell, and C. Jin. \n\nACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell y C. Jin. Detección de novedades condicionada al tema. En Actas de la 8ª Conferencia Internacional de ACM SIGKDD, ACM Press. 2002, 688-693. [11] M. Juha, A.M. Helena y S. Marko. Aplicando Clases Semánticas en la Detección y Seguimiento de Eventos. En Actas de la Conferencia Internacional sobre Procesamiento del Lenguaje Natural (ICON 2002), 2002, páginas 175-183. [12] M. Juha, A.M. Helena y S. Marko. Semántica simple en la detección y seguimiento de temas. Recuperación de información, 7(3-4): 2004, 347-368. [13] K. Giridhar y J. Allan. Clasificación de texto y entidades nombradas para la detección de nuevos eventos. En Actas de la 27ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2004, 297-304. [14] J. P. Callan, W. B. Croft, y S. M. Harding. El Sistema de Recuperación INQUERY. En Actas de DEXA-92, 3ra Conferencia Internacional sobre Aplicaciones de Bases de Datos y Sistemas Expertos, 1992, 78-83. [15] R. Krovetz. Viendo la morfología como un proceso de inferencia. En Actas de ACM SIGIR93, 1993, 61-81. [16] Y. Yang y J. Pedersen. Un estudio comparativo sobre la selección de características en la categorización de textos. En J. D. H. Fisher, editor, La Decimocuarta Conferencia Internacional sobre Aprendizaje Automático (ICML97), Morgan Kaufmann, 1997, 412-420. [17] T. M. Cover y J. A. I'm sorry, but \"Thomas\" is not a sentence. Can you provide more context or a complete sentence for me to translate into Spanish? Elementos de la teoría de la información. Wiley. 1991. [18] El consorcio de datos lingüísticos, http://www.ldc,upenn.edu/. [19] La definición de la tarea TDT 2001 y el plan de evaluación, http://www.nist.gov/speech/tests/tdt/tdt2001/evalplan.htm. [20] R. E. Schapire y Y. Cantante. Boostexter: Un sistema basado en Boosting para la categorización de texto. En Machine Learning 39(2/3):1, Kluwer Academic Publishers, 2000, 35-168. [21] K. Giridhar y J. Allan. 2005. Utilizando nombres y temas para la detección de nuevos eventos. En Actas de la Conferencia de Tecnología Humana y la Conferencia sobre Métodos Empíricos en Lenguaje Natural, Vancouver, 2005, 121-128 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "news volume": {
            "translated_key": "volumen de noticias",
            "is_in_text": false,
            "original_annotated_sentences": [
                "New Event Detection Based on Indexing-tree and Named Entity Zhang Kuo Tsinghua University Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Tsinghua University Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Tsinghua University Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn ABSTRACT New Event Detection (NED) aims at detecting from one or multiple streams of news stories that which one is reported on a new event (i.e.",
                "not reported previously).",
                "With the overwhelming volume of news available today, there is an increasing need for a NED system which is able to detect new events more efficiently and accurately.",
                "In this paper we propose a new NED model to speed up the NED task by using news indexing-tree dynamically.",
                "Moreover, based on the observation that terms of different types have different effects for NED task, two term reweighting approaches are proposed to improve NED accuracy.",
                "In the first approach, we propose to adjust term weights dynamically based on previous story clusters and in the second approach, we propose to employ statistics on training data to learn the named entity reweighting model for each class of stories.",
                "Experimental results on two Linguistic Data Consortium (LDC) datasets TDT2 and TDT3 show that the proposed model can improve both efficiency and accuracy of NED task significantly, compared to the baseline system and other existing systems.",
                "Categories and Subject Descriptors H.3.3 [Information Systems]: Information Search and Retrieval; H.4.2 [Information Systems Applications]: Types of Systemsdecision support.",
                "General Terms Algorithms, Performance, Experimentation 1.",
                "INTRODUCTION Topic Detection and Tracking (TDT) program aims to develop techniques which can effectively organize, search and structure news text materials from a variety of newswire and broadcast media [1].",
                "New Event Detection (NED) is one of the five tasks in TDT.",
                "It is the task of online identification of the earliest report for each topic as soon as that report arrives in the sequence of documents.",
                "A Topic is defined as a seminal event or activity, along with directly related events and activities [2].",
                "An Event is defined as something (non-trivial) happening in a certain place at a certain time [3].",
                "For instance, when a bomb explodes in a building, the exploding is the seminal event that triggers the topic, and other stories on the same topic would be those discussing salvaging efforts, the search for perpetrators, arrests and trial and so on.",
                "Useful news information is usually buried in a mass of data generated everyday.",
                "Therefore, NED systems are very useful for people who need to detect novel information from real-time news stream.",
                "These real-life needs often occur in domains like financial markets, news analysis, and intelligence gathering.",
                "In most of state-of-the-art (currently) NED systems, each news story on hand is compared to all the previous received stories.",
                "If all the similarities between them do not exceed a threshold, then the story triggers a new event.",
                "They are usually in the form of cosine similarity or Hellinger similarity metric.",
                "The core problem of NED is to identify whether two stories are on the same topic.",
                "Obviously, these systems cannot take advantage of topic information.",
                "Further more, it is not acceptable in real applications because of the large amount of computation required in the NED process.",
                "Other systems organize previous stories into clusters (each cluster corresponds to a topic), and new story is compared to the previous clusters instead of stories.",
                "This manner can reduce comparing times significantly.",
                "Nevertheless, it has been proved that this manner is less accurate [4, 5].",
                "This is because sometimes stories within a topic drift far away from each other, which could lead low similarity between a story and its topic.",
                "On the other hand, some proposed NED systems tried to improve accuracy by making better use of named entities [10, 11, 12, 13].",
                "However, none of the systems have considered that terms of different types (e.g.",
                "Noun, Verb or Person name) have different effects for different classes of stories in determining whether two stories are on the same topic.",
                "For example, the names of election candidates (Person name) are very important for stories of election class; the locations (Location name) where accidents happened are important for stories of accidents class.",
                "So, in NED, there still exist following three problems to be investigated: (1) How to speed up the detection procedure while do not decrease the detection accuracy? (2) How to make good use of cluster (topic) information to improve accuracy? (3) How to obtain better news story representation by better understanding of named entities.",
                "Driven by these problems, we have proposed three approaches in this paper. (1)To make the detection procedure faster, we propose a new NED procedure based on news indexing-tree created dynamically.",
                "Story indexing-tree is created by assembling similar stories together to form news clusters in different hierarchies according to their values of similarity.",
                "Comparisons between current story and previous clusters could help find the most similar story in less comparing times.",
                "The new procedure can reduce the amount of comparing times without hurting accuracy. (2)We use the clusters of the first floor in the indexing-tree as news topics, in which term weights are adjusted dynamically according to term distribution in the clusters.",
                "In this approach, cluster (topic) information is used properly, so the problem of theme decentralization is avoided. (3)Based on observations on the statistics obtained from training data, we found that terms of different types (e.g.",
                "Noun and Verb) have different effects for different classes of stories in determining whether two stories are on the same topic.",
                "And we propose to use statistics to optimize the weights of the terms of different types in a story according to the news class that the story belongs to.",
                "On TDT3 dataset, the new NED model just uses 14.9% comparing times of the basic model, while its minimum normalized cost is 0.5012, which is 0.0797 better than the basic model, and also better than any other results previously reported for this dataset [8, 13].",
                "The rest of the paper is organized as follows.",
                "We start off this paper by summarizing the previous work in NED in section 2.",
                "Section 3 presents the basic model for NED that most current systems use.",
                "Section 4 describes our new detection procedure based on news indexing-tree.",
                "In section 5, two term reweighting methods are proposed to improve NED accuracy.",
                "Section 6 gives our experimental data and evaluation metrics.",
                "We finally wrap up with the experimental results in Section 7, and the conclusions and future work in Section 8. 2.",
                "RELATED WORK Papka et al. proposed Single-Pass clustering on NED [6].",
                "When a new story was encountered, it was processed immediately to extract term features and a query representation of the storys content is built up.",
                "Then it was compared with all the previous queries.",
                "If the document did not trigger any queries by exceeding a threshold, it was marked as a new event.",
                "Lam et al build up previous query representations of story clusters, each of which corresponds to a topic [7].",
                "In this manner comparisons happen between stories and clusters.",
                "Recent years, most work focus on proposing better methods on comparison of stories and document representation.",
                "Brants et al. [8] extended a basic incremental TF-IDF model to include sourcespecific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, term reweighting based on inverse event frequencies, and segmentation of documents.",
                "Good improvements on TDT bench-marks were shown.",
                "Stokes et al. [9] utilized a combination of evidence from two distinct representations of a documents content.",
                "One of the representations was the usual free text vector, the other made use of lexical chains (created using WordNet) to build another term vector.",
                "Then the two representations are combined in a linear fashion.",
                "A marginal increase in effectiveness was achieved when the combined representation was used.",
                "Some efforts have been done on how to utilize named entities to improve NED.",
                "Yang et al. gave location named entities four times weight than other terms and named entities [10].",
                "DOREMI research group combined semantic similarities of person names, location names and time together with textual similarity [11][12].",
                "UMass [13] research group split document representation into two parts: named entities and non-named entities.",
                "And it was found that some classes of news could achieve better performance using named entity representation, while some other classes of news could achieve better performance using non-named entity representation.",
                "Both [10] and [13] used text categorization technique to classify news stories in advance.",
                "In [13] news stories are classified automatically at first, and then test sensitivities of names and non-name terms for NED for each class.",
                "In [10] frequent terms for each class are removed from document representation.",
                "For example, word election does not help identify different elections.",
                "In their work, effectiveness of different kinds of names (or terms with different POS) for NED in different news classes are not investigated.",
                "We use statistical analysis to reveal the fact and use it to improve NED performance. 3.",
                "BASIC MODEL In this section, we present the basic New Event Detection model which is similar to what most current systems apply.",
                "Then, we propose our new model by extending the basic model.",
                "New Event Detection systems use news story stream as input, in which stories are strictly time-ordered.",
                "Only previously received stories are available when dealing with current story.",
                "The output is a decision for whether the current story is on a new event or not and the confidence of the decision.",
                "Usually, a NED model consists of three parts: story representation, similarity calculation and detection procedure. 3.1 Story Representation Preprocessing is needed before generating story representation.",
                "For preprocessing, we tokenize words, recognize abbreviations, normalize abbreviations, add part-of-speech tags, remove stopwords included in the stop list used in InQuery [14], replace words with their stems using K-stem algorithm[15], and then generate word vector for each news story.",
                "We use incremental TF-IDF model for term weight calculation [4].",
                "In a TF-IDF model, term frequency in a news document is weighted by the inverse document frequency, which is generated from training corpus.",
                "When a new term occurs in testing process, there are two solutions: simply ignore the new term or set df of the term as a small const (e.g. df = 1).",
                "The new term receives too low weight in the first solution (0) and too high weight in the second solution.",
                "In incremental TF-IDF model, document frequencies are updated dynamically in each time step t: 1( ) ( ) ( )t t D tdf w df w df w−= + (1) where Dt represents news story set received in time t, and dfDt(w) means the number of documents that term w occurs in, and dft(w) means the total number of documents that term w occurs in before time t. In this work, each time window includes 50 news stories.",
                "Thus, each story d received in t is represented as follows: 1 2{ ( , , ), ( , , ),..., ( , , )}nd weight d t w weight d t w weight d t w→ where n means the number of distinct terms in story d, and ( , , )weight d t w means the weight of term w in story d at time t: log( ( , ) 1) log(( 1) /( ( ) 0.5)) ( , , ) log( ( , ) 1) log(( 1) /( ( ) 0.5)) t t t t w d tf d w N df w weight d t w tf d w N df w ∈ + + + = + + +∑ (2) where Nt means the total number of news stories before time t, and tf(d,w) means how many times term w occurs in news story d. 3.2 Similarity Calculation We use Hellinger distance for the calculation of similarity between two stories, for two stories d and d at time t, their similarity is defined as follows: , ( , , ) ( , , ) * ( , , ) w d d sim d d t weight d t w weight d t w ∈ = ∑ (3) 3.3 Detection Procedure For each story d received in time step t, the value ( ) ( ) ( ) ( ( , , )) time d time d n d max sim d d t < = (4) is a score used to determine whether d is a story about a new topic and at the same time is an indication of the confidence in our decision [8]. time(d) means the publication time of story d. If the score exceeds the thresholdθ new, then there exists a sufficiently similar document, thus d is a old story, otherwise, there is no sufficiently similar previous document, thus d is an new story. 4.",
                "New NED Procedure Traditional NED systems can be classified into two main types on the aspect of detection procedure: (1) S-S type, in which the story on hand is compared to each story received previously, and use the highest similarity to determine whether current story is about a new event; (2) S-C type, in which the story on hand is compared to all previous clusters each of which representing a topic, and the highest similarity is used for final decision for current story.",
                "If the highest similarity exceeds thresholdθ new, then it is an old story, and put it into the most similar cluster; otherwise it is a new story and create a new cluster.",
                "Previous work show that the first manner is more accurate than the second one [4][5].",
                "Since sometimes stories within a topic drift far away from each other, a story may have very low similarity with its topic.",
                "So using similarities between stories for determining new story is better than using similarities between story and clusters.",
                "Nevertheless, the first manner needs much more comparing times which means the first manner is low efficient.",
                "We propose a new detection procedure which uses comparisons with previous clusters to help find the most similar story in less comparing times, and the final new event decision is made according to the most similar story.",
                "Therefore, we can get both the accuracy of S-S type methods and the efficiency of S-C type methods.",
                "The new procedure creates a news indexing-tree dynamically, in which similar stories are put together to form a hierarchy of clusters.",
                "We index similar stories together by their common ancestor (a cluster node).",
                "Dissimilar stories are indexed in different clusters.",
                "When a story is coming, we use comparisons between the current story and previous hierarchical clusters to help find the most similar story which is useful for new event decision.",
                "After the new event decision is made, the current story is inserted to the indexing-tree for the following detection.",
                "The news indexing-tree is defined formally as follows: S-Tree = {r, NC , NS , E} where r is the root of S-Tree, NC is the set of all cluster nodes, NS is the set of all story nodes, and E is the set of all edges in S-Tree.",
                "We define a set of constraints for a S-Tree: ⅰ . , is an non-terminal node in the treeC i i N i∀ ∈ → ⅱ . , is a terminal node in the treeS i i N i∀ ∈ → ⅲ . , out degree of is at least 2C i i N i∀ ∈ → ⅳ . , is represented as the centroid of its desendantsC i i iN∀ ∈ → For a news story di, the comparison procedure and inserting procedure based on indexing-tree are defined as follows.",
                "An example is shown by Figure 1 and Figure 2.",
                "Figure 1.",
                "Comparison procedure Figure 2.",
                "Inserting procedure Comparison procedure: Step 1: compare di to all the direct child nodes of r and select λ nodes with highest similarities, e.g., C1 2 and C1 3 in Figure 1.",
                "Step 2: for each selected node in the last step, e.g.",
                "C1 2, compare di to all its direct child nodes, and select λ nodes with highest similarities, e.g.",
                "C2 2 and d8.",
                "Repeat step 2 for all non-terminal nodes.",
                "Step 3: record the terminal node with the highest similarty to di, e.g. s5, and the similarity value (0.20).",
                "Inserting di to the S-tree with r as root: Find the node n which is direct child of r in the path from r to the terminal node with highest similarity s, e.g.",
                "C1 2.",
                "If s is smaller than θ init+(h-1)δ , then add di to the tree as a direct child of r. Otherwise, if n is a terminal node, then create a cluster node instead of n, and add both n and di as its direct children; if n is an non-terminal node, then repeat this procedure and insert di to the sub-tree with n as root recursively.",
                "Here h is the length between n and the root of S-tree.",
                "The more the stories in a cluster similar to each other, the better the cluster represents the stories in it.",
                "Hence we add no constraints on the maximum of trees height and degree of a node.",
                "Therefore, we cannot give the complexity of this indexing-tree based procedure.",
                "But we will give the number of comparing times needed by the new procedure in our experiments in section7. 5.",
                "Term Reweighting Methods In this section, two term reweighting methods are proposed to improve NED accuracy.",
                "In the first method, a new way is explored for better using of cluster (topic) information.",
                "The second one finds a better way to make use of named entities based on news classification. 5.1 Term Reweighting Based on Distribution Distance TF-IDF is the most prevalent model used in information retrieval systems.",
                "The basic idea is that the fewer documents a term appears in, the more important the term is in discrimination of documents (relevant or not relevant to a query containing the term).",
                "Nevertheless, in TDT domain, we need to discriminate documents with regard to topics rather than queries.",
                "Intuitively, using cluster (topic) vectors to compare with subsequent news stories should outperform using story vectors.",
                "Unfortunately, the experimental results do not support this intuition [4][5].",
                "Based on observation on data, we find the reason is that a news topic usually contains many directly or indirectly related events, while they all have their own sub-subjects which are usually different with each other.",
                "Take the topic described in section 1 as an example, events like the explosion and salvage have very low similarities with events about criminal trial, therefore stories about trial would have low similarity with the topic vector built on its previous events.",
                "This section focuses on how to effectively make use of topic information and at the same time avoid the problem of content decentralization.",
                "At first, we classify terms into 5 classes to help analysis the needs of the modified model: Term class A: terms that occur frequently in the whole corpus, e.g., year and people.",
                "Terms of this class should be given low weights because they do not help much for topic discrimination.",
                "Term class B: terms that occur frequently within a news category, e.g., election, storm.",
                "They are useful to distinguish two stories in different news categories.",
                "However, they cannot provide information to determine whether two stories are on the same or different topics.",
                "In another words, term election and term storm are not helpful in differentiate two election campaigns and two storm disasters.",
                "Therefore, terms of this class should be assigned lower weights.",
                "Term class C: terms that occur frequently in a topic, and infrequently in other topics, e.g., the name of a crash plane, the name of a specific hurricane.",
                "News stories that belong to different topics rarely have overlap terms in this class.",
                "The more frequently a term appears in a topic, the more important the term is for a story belonging to the topic, therefore the term should be set higher weight.",
                "Term class D: terms that appear in a topic exclusively, but not frequently.",
                "For example, the name of a fireman who did very well in a salvage action, which may appears in only two or three stories but never appeared in other topics.",
                "Terms of this type should receive more weights than in TF-IDF model.",
                "However, since they are not popular in the topic, it is not appropriate to give them too high weights.",
                "Term class E: terms with low document frequency, and appear in different topics.",
                "Terms of this class should receive lower weights.",
                "Now we analyze whether TF-IDF model can give proper weights to the five classes of terms.",
                "Obviously, terms of class A are lowly weighted in TF-IDF model, which is conformable with the requirement described above.",
                "In TF-IDF model, terms of class B are highly dependant with the number of stories in a news class.",
                "TF-IDF model cannot provide low weights if the story containing the term belongs to a relative small news class.",
                "For a term of class C, the more frequently it appears in a topic, the less weight TFIDF model gives to it.",
                "This strongly conflicts with the requirement of terms in class C. For terms of class D, TF-IDF model gives them high weights correctly.",
                "But for terms of class E, TF-IDF model gives high weights to them which are not conformable with the requirement of low weights.",
                "To sum up, terms of class B, C, E cannot be properly weighted in TF-IDF model.",
                "So, we propose a modified model to resolve this problem.",
                "When θ init andθ new are set closely, we assume that most of the stories in a first-level cluster (a direct child node of root node) are on the same topic.",
                "Therefore, we make use of a first-level cluster to capture term distribution (df for all the terms within the cluster) within the topic dynamically.",
                "KL divergence of term distribution in a first-level cluster and the whole story set is used to adjust term weights:  ( , , ) * (1 * ( || )) ( , , ) ( , , ) * (1 * ( || )) cw tw cw tw w d D weight d t w KL P P weight d t w weight d t w KL P P γ γ ∈ + = +∑ (5) where ( ) ( ) ( ) ( ) 1,cw cw c c c c df w df w p y p y N N = = − (6) ( ) ( ) ( ) ( ) 1,t t tw tw t t df w df w p y p y N N = = − (7) where dfc(w) is the number of documents containing term w within cluster C, and Nc is the number of documents in cluster C, and Nt is the total number of documents that arrive before time step t. γ is a const parameter, now is manually set 3.",
                "KL divergence is defined as follows [17]: ( ) ( || ) ( ) log ( )x p x KL P Q p x q x = ∑ (8) The basic idea is: for a story in a topic, the more a term occurs within the topic, and the less it occurs in other topics, it should be assigned higher weights.",
                "Obviously, modified model can meet all the requirements of the five term classes listed above. 5.2 Term Reweighting Based on Term Type and Story Class Previous work found that some classes of news stories could achieve good improvements by giving extra weight to named entities.",
                "But we find that terms of different types should be given different amount of extra weight for different classes of news stories.",
                "We use open-NLP1 to recognize named entity types and part-ofspeech tags for terms that appear in news stories.",
                "Named entity types include person name, organization name, location name, date, time, money and percentage, and five POSs are selected: none (NN), verb (VB), adjective (JJ), adverb (RB) and cardinal number (CD).",
                "Statistical analysis shows topic-level discriminative terms types for different classes of stories.",
                "For the sake of convenience, named entity type and part-of-speech tags are uniformly called term type in subsequent sections.",
                "Determining whether two stories are about the same topic is a basic component for NED task.",
                "So at first we use 2 χ statistic to compute correlations between terms and topics.",
                "For a term t and a topic T, a contingence table is derived: Table 1.",
                "A 2×2 Contingence Table Doc Number belong to topic T not belong to topic T include t A B not include t C D The 2 χ statistic for a specific term t with respect to topic T is defined to be [16]: 2 2 ( , ) ( ) * ( ) ( ) * ( ) * ( ) * ( ) w T A B C D AD CB A C B D A B C D χ = + + + − + + + + (9) News topics for the TDT task are further classified into 11 rules of interpretations (ROIs) 2 .",
                "The ROI can be seen as a higher level class of stories.",
                "The average correlation between a term type and a topic ROI is computed as: 2 avg 2 ( , )( ( , ) )k m m km kT R w P w TP R p w T R P χ χ ∈ ∈ ∑ ∑（ , ）= 1 1 k=1…K, m=1…M (10) where K is the number of term types (set 12 constantly in the paper).",
                "M is the number news classes (ROIs, set 11 in the paper).",
                "Pk represents the set of all terms of type k, and Rm represents the set of all topics of class m, p(t,T) means the probability that t occurs in topic T. Because of limitation of space, only parts of the term types (9 term types) and parts of news classes (8 classes) are listed in table 2 with the average correlation values between them.",
                "The statistics is derived from labeled data in TDT2 corpus. (Results in table 2 are already normalized for convenience in comparison.)",
                "The statistics in table 2 indicates the usefulness of different term types in topic discrimination with respect to different news classes.",
                "We can see that, location name is the most useful term type for three news classes: Natural Disasters, Violence or War, Finances.",
                "And for three other categories Elections, Legal/Criminal Cases, Science and Discovery, person name is the most discriminative term type.",
                "For Scandals/Hearings, date is the most important information for topic discrimination.",
                "In addition, Legal/Criminal Cases and Finance topics have higher correlation with money terms, while Science and Discovery have higher correlation with percentage terms.",
                "Non-name terms are more stable for different classes. 1 . http://opennlp.sourceforge.net/ 2 . http://projects.ldc.upenn.edu/TDT3/Guide/label.html From the analysis of table 2, it is reasonable to adjust term weight according to their term type and the news class the story belongs to.",
                "New term weights are reweighted as follows: ( ) ( ) ( ) ( ) ( , , ) * ( , , ) ( , , ) * class d D type w T class d D type w w d weight d t w weight d t w weight d t w α α ∈ = ∑ (11) where type(w) represents the type of term w, and class(d) represents the class of story d, c kα is reweighting parameter for news class c and term type k. In the work, we just simply use statistics in table 2 as the reweighting parameters.",
                "Even thought using the statistics directly may not the best choice, we do not discuss how to automatically obtain the best parameters.",
                "We will try to use machine learning techniques to obtain the best parameters in the future work.",
                "In the work, we use BoosTexter [20] to classify all stories into one of the 11 ROIs.",
                "BoosTexter is a boosting based machine learning program, which creates a series of simple rules for building a classifier for text or attribute-value data.",
                "We use term weight generated using TF-IDF model as feature for story classification.",
                "We trained the model on the 12000 judged English stories in TDT2, and classify the rest of the stories in TDT2 and all stories in TDT3.",
                "Classification results are used for term reweighting in formula (11).",
                "Since the class labels of topic-off stories are not given in TDT datasets, we cannot give the classification accuracy here.",
                "Thus we do not discuss the effects of classification accuracy to NED performance in the paper. 6.",
                "EXPERIMENTAL SETUP 6.1 Datasets We used two LDC [18] datasets TDT2 and TDT3 for our experiments.",
                "TDT2 contains news stories from January to June 1998.",
                "It contains around 54,000 stories from sources like ABC, Associated Press, CNN, New York Times, Public Radio International, Voice of America etc.",
                "Only English stories in the collection were considered.",
                "TDT3 contains approximately 31,000 English stories collected from October to December 1998.",
                "In addition to the sources used in TDT2, it also contains stories from NBC and MSNBC TV broadcasts.",
                "We used transcribed versions of the TV and radio broadcasts besides textual news.",
                "TDT2 dataset is labeled with about 100 topics, and approximately 12,000 English stories belong to at least one of these topics.",
                "TDT3 dataset is labeled with about 120 topics, and approximately 8000 English stories belong to at least one of these topics.",
                "All the topics are classified into 11 Rules of Interpretation: (1)Elections, (2)Scandals/Hearings, (3)Legal/Criminal Cases, (4)Natural Disasters, (5)Accidents, (6)Ongoing Violence or War, (7)Science and Discovery News, (8)Finance, (9)New Law, (10)Sports News, (11)MISC.",
                "News. 6.2 Evaluation Metric TDT uses a cost function CDet that combines the probabilities of missing a new story and a false alarm [19]: * * * *Det Miss Miss Target FA FA NontargetC C P P C P P= + (12) Table 2.",
                "Average correlation between term types and news classes where CMiss means the cost of missing a new story, PMiss means the probability of missing a new story, and PTarget means the probability of seeing a new story in the data; CFA means the cost of a false alarm, PFA means the probability of a false alarm, and PNontarget means the probability of seeing an old story.",
                "The cost CDet is normalized such that a perfect system scores 0 and a trivial system, which is the better one of mark all stories as new or old, scores 1: ( ( * , * ) ) Det Det Miss Target FA Nontarget C Norm C min C P C P = (13) New event detection system gives two outputs for each story.",
                "The first part is yes or no indicating whether the story triggers a new event or not.",
                "The second part is a score indicating confidence of the first decision.",
                "Confidence scores can be used to plot DET curve, i.e., curves that plot false alarm vs. miss probabilities.",
                "Minimum normalized cost can be determined if optimal threshold on the score were chosen. 7.",
                "EXPERIMENTAL RESULTS 7.1 Main Results To test the approaches proposed in the model, we implemented and tested five systems: System-1: this system is used as baseline.",
                "It is implemented based on the basic model described in section 3, i.e., using incremental TF-IDF model to generate term weights, and using Hellinger distance to compute document similarity.",
                "Similarity score normalization is also employed [8].",
                "S-S detection procedure is used.",
                "System-2: this system is the same as system-1 except that S-C detection procedure is used.",
                "System-3: this system is the same as system-1 except that it uses the new detection procedure which is based on indexing-tree.",
                "System-4: implemented based on the approach presented in section 5.1, i.e., terms are reweighted according to the distance between term distributions in a cluster and all stories.",
                "The new detection procedure is used.",
                "System-5: implemented based on the approach presented in section 5.2, i.e., terms of different types are reweighted according to news class using trained parameters.",
                "The new detection procedure is used.",
                "The following are some other NED systems: System-6: [21] for each pair of stories, it computes three similarity values for named entity, non-named entity and all terms respectively.",
                "And employ Support Vector Machine to predict new or old using the similarity values as features.",
                "System-7: [8] it extended a basic incremental TF-IDF model to include source-specific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, etc.",
                "System-8: [13] it split document representation into two parts: named entities and non-named entities, and choose one effective part for each news class.",
                "Table 3 and table 4 show topic-weighted normalized costs and comparing times on TDT2 and TDT3 datasets respectively.",
                "Since no heldout data set for fine-tuning the threshold θ new was available for experiments on TDT2, we only report minimum normalized costs for our systems in table 3.",
                "System-5 outperforms all other systems including system-6, and it performs only 2.78e+8 comparing times in detection procedure which is only 13.4% of system-1.",
                "Table 3.",
                "NED results on TDT2 Systems Min Norm(CDet) Cmp times System-1 0.5749 2.08e+9 System-2① 0.6673 3.77e+8 System-3② 0.5765 2.81e+8 System-4② 0.5431 2.99e+8 System-5② 0.5089 2.78e+8 System-6 0.5300 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 When evaluating on the normalized costs on TDT3, we use the optimal thresholds obtained from TDT2 data set for all systems.",
                "System-2 reduces comparing times to 1.29e+9 which is just 18.3% of system-1, but at the same time it also gets a deteriorated minimum normalized cost which is 0.0499 higher than system-1.",
                "System-3 uses the new detection procedure based on news indexing-tree.",
                "It requires even less comparing times than system-2.",
                "This is because story-story comparisons usually yield greater similarities than story-cluster ones, so stories tend to be combined Location Person Date Organization Money Percentage NN JJ CD Elections 0.37 1 0.04 0.58 0.08 0.03 0.32 0.13 0.1 Scandals/Hearings 0.66 0.62 0.28 1 0.11 0.02 0.27 0.13 0.05 Legal/Criminal Cases 0.48 1 0.02 0.62 0.15 0 0.22 0.24 0.09 Natural Disasters 1 0.27 0 0.04 0.04 0 0.25 0.04 0.02 Violence or War 1 0.36 0.02 0.14 0.02 0.04 0.21 0.11 0.02 Science and Discovery 0.11 1 0.01 0.22 0.08 0.12 0.19 0.08 0.03 Finances 1 0.45 0.04 0.98 0.13 0.02 0.29 0.06 0.05 Sports 0.16 0.27 0.01 1 0.02 0 0.11 0.03 0.01 together in system-3.",
                "And system-3 is basically equivalent to system-1 in accuracy results.",
                "System-4 adjusts term weights based on the distance of term distributions between the whole corpus and cluster story set, yielding a good improvement by 0.0468 compared to system-1.",
                "The best system (system-5) has a minimum normalized cost 0.5012, which is 0.0797 better than system-1, and also better than any other results previously reported for this dataset [8, 13].",
                "Further more, system-5 only needs 1.05e+8 comparing times which is 14.9% of system-1.",
                "Table 4.",
                "NED results on TDT3 Systems Norm(CDet) Min Norm(CDet) Cmp times System-1 0.6159 0.5809 7.04e+8 System-2① 0.6493 0.6308 1.29e+8 System-3② 0.6197 0.5868 1.03e+8 System-4② 0.5601 0.5341 1.03e+8 System-5② 0.5413 0.5012 1.05e+8 System-7 -- 0.5783 -System-8 -- 0.5229 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 Figure5 shows the five DET curves for our systems on data set TDT3.",
                "System-5 achieves the minimum cost at a false alarm rate of 0.0157 and a miss rate of 0.4310.",
                "We can observe that System4 and System-5 obtain lower miss probability at regions of low false alarm probabilities.",
                "The hypothesis is that, more weight value is transferred to key terms of topics from non-key terms.",
                "Similarity score between two stories belonging to different topics are lower than before, because their overlapping terms are usually not key terms of their topics. 7.2 Parameter selection for indexing-tree detection Figure 3 shows the minimum normalized costs obtained by system-3 on TDT3 using different parameters.",
                "Theθ init parameter is tested on six values spanning from 0.03 to 0.18.",
                "And the λ parameter is tested on four values 1, 2, 3 and 4.",
                "We can see that, whenθ init is set to 0.12, which is the closest one toθ new, the costs are lower than others.",
                "This is easy to explain, because when stories belonging to the same topic are put in a cluster, it is more reasonable for the cluster to represent the stories in it.",
                "When parameter λ is set to 3 or 4, the costs are better than other cases, but there is no much difference between 3 and 4. 0 0.05 0.1 0.15 0.2 1 2 3 4 0.5 0.6 0.7 0.8 0.9 1 θ-initλ MinCost 0.6 0.65 0.7 0.75 0.8 0.85 0.9 Figure 3.",
                "Min Cost on TDT3 (δ =0.15) 0 0.05 0.1 0.15 0.2 1 2 3 4 0 0.5 1 1.5 2 2.5 x 10 8 θ-init λ Comparingtimes 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 8 Figure 4.",
                "Comparing times on TDT3 (δ =0.15) Figure 4 gives the comparing times used by system-3 on TDT3 with the same parameters as figure 3.",
                "The comparing times are strongly dependent onθ init.",
                "Because the greaterθ init is, the less stories combined together, the more comparing times are needed for new event decision.",
                "So we useθ init =0.13,λ =3,δ =0.15 for system-3, 4, and 5.",
                "In this parameter setting, we can get both low minimum normalized costs and less comparing times. 8.",
                "CONCLUSION We have proposed a news indexing-tree based detection procedure in our model.",
                "It reduces comparing times to about one seventh of traditional method without hurting NED accuracy.",
                "We also have presented two extensions to the basic TF-IDF model.",
                "The first extension is made by adjust term weights based on term distributions between the whole corpus and a cluster story set.",
                "And the second extension to basic TF-IDF model is better use of term types (named entities types and part-of-speed) according to news categories.",
                "Our experimental results on TDT2 and TDT3 datasets show that both of the two extensions contribute significantly to improvement in accuracy.",
                "We did not consider news time information as a clue for NED task, since most of the topics last for a long time and TDT data sets only span for a relative short period (no more than 6 months).",
                "For the future work, we want to collect news set which span for a longer period from internet, and integrate time information in NED task.",
                "Since topic is a relative coarse-grained news cluster, we also want to refine cluster granularity to event-level, and identify different events and their relations within a topic.",
                "Acknowledgments This work is supported by the National Natural Science Foundation of China under Grant No. 90604025.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsor. 9.",
                "REFERENCES [1] http://www.nist.gov/speech/tests/tdt/index.htm [2] In Topic Detection and Tracking.",
                "Event-based Information Organization.",
                "Kluwer Academic Publishers, 2002. .01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90 1 2 5 10 20 40 60 80 90 False Alarm Probability (in %) MissProbability(in%) SYSTEM1 Topic Weighted Curve SYSTEM1 Min Norm(Cost) SYSTEM2 Topic Weighted Curve SYSTEM2 Min Norm(Cost) SYSTEM3 Topic Weighted Curve SYSTEM3 Min Norm(Cost) SYSTEM4 Topic Weighted Curve SYSTEM4 Min Norm(Cost) SYSTEM5 Topic Weighted Curve SYSTEM5 Min Norm(Cost) Random Performance Figure 5.",
                "DET curves on TDT3 [3] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B.T.",
                "Archibald, and X. Liu.",
                "Learning Approaches for Detecting and Tracking News Events.",
                "In IEEE Intelligent Systems Special Issue on Applications of Intelligent Information Retrieval, volume 14 (4), 1999, 32-43. [4] Y. Yang, T. Pierce, and J. Carbonell.",
                "A Study on Retrospective and On-line Event Detection.",
                "In Proceedings of SIGIR-98, Melbourne, Australia, 1998, 28-36. [5] J. Allan, V. Lavrenko, D. Malin, and R. Swan.",
                "Detections, Bounds, and Timelines: Umass and tdt-3.",
                "In Proceedings of Topic Detection and Tracking Workshop (TDT-3), Vienna, VA, 2000, 167-174. [6] R. Papka and J. Allan.",
                "On-line New Event Detection Using Single Pass Clustering TITLE2:.",
                "Technical Report UM-CS1998-021, 1998. [7] W. Lam, H. Meng, K. Wong, and J.",
                "Yen.",
                "Using Contextual Analysis for News Event Detection.",
                "International Journal on Intelligent Systems, 2001, 525-546. [8] B. Thorsten, C. Francine, and F. Ayman.",
                "A System for New Event Detection.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2003, 330-337. [9] S. Nicola and C. Joe.",
                "Combining Semantic and Syntactic Document Classifiers to Improve First Story Detection.",
                "In Proceedings of the 24th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topicconditioned Novelty Detection.",
                "In Proceedings of the 8th ACM SIGKDD International Conference, ACM Press. 2002, 688-693. [11] M. Juha, A.M. Helena, and S. Marko.",
                "Applying Semantic Classes in Event Detection and Tracking.",
                "In Proceedings of International Conference on Natural Language Processing (ICON 2002), 2002, pages 175-183. [12] M. Juha, A.M. Helena, and S. Marko.",
                "Simple Semantics in Topic Detection and Tracking.",
                "Information Retrieval, 7(3-4): 2004, 347-368. [13] K. Giridhar and J. Allan.",
                "Text Classification and Named Entities for New Event Detection.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2004, 297-304. [14] J. P. Callan, W. B. Croft, and S. M. Harding.",
                "The INQUERY Retrieval System.",
                "In Proceedings of DEXA-92, 3rd International Conference on Database and Expert Systems Applications, 1992, 78-83. [15] R. Krovetz.",
                "Viewing Morphology as An Inference Process.",
                "In Proceedings of ACM SIGIR93, 1993, 61-81. [16] Y. Yang and J. Pedersen.",
                "A Comparative Study on Feature Selection in Text Categorization.",
                "In J. D. H. Fisher, editor, The Fourteenth International Conference on Machine Learning (ICML97), Morgan Kaufmann, 1997, 412-420. [17] T. M. Cover, and J.A.",
                "Thomas.",
                "Elements of Information Theory.",
                "Wiley. 1991. [18] The linguistic data consortium, http://www.ldc,upenn.edu/. [19] The 2001 TDT task definition and evaluation plan, http://www.nist.gov/speech/tests/tdt/tdt2001/evalplan.htm. [20] R. E. Schapire and Y.",
                "Singer.",
                "Boostexter: A Boosting-based System for Text Categorization.",
                "In Machine Learning 39(2/3):1, Kluwer Academic Publishers, 2000, 35-168. [21] K. Giridhar and J. Allan. 2005.",
                "Using Names and Topics for New Event Detection.",
                "In Proceedings of Human Technology Conference and Conference on Empirical Methods in Natural Language, Vancouver, 2005, 121-128"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "speed up the ned task": {
            "translated_key": "acelerar la tarea de NED",
            "is_in_text": true,
            "original_annotated_sentences": [
                "New Event Detection Based on Indexing-tree and Named Entity Zhang Kuo Tsinghua University Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Tsinghua University Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Tsinghua University Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn ABSTRACT New Event Detection (NED) aims at detecting from one or multiple streams of news stories that which one is reported on a new event (i.e.",
                "not reported previously).",
                "With the overwhelming volume of news available today, there is an increasing need for a NED system which is able to detect new events more efficiently and accurately.",
                "In this paper we propose a new NED model to <br>speed up the ned task</br> by using news indexing-tree dynamically.",
                "Moreover, based on the observation that terms of different types have different effects for NED task, two term reweighting approaches are proposed to improve NED accuracy.",
                "In the first approach, we propose to adjust term weights dynamically based on previous story clusters and in the second approach, we propose to employ statistics on training data to learn the named entity reweighting model for each class of stories.",
                "Experimental results on two Linguistic Data Consortium (LDC) datasets TDT2 and TDT3 show that the proposed model can improve both efficiency and accuracy of NED task significantly, compared to the baseline system and other existing systems.",
                "Categories and Subject Descriptors H.3.3 [Information Systems]: Information Search and Retrieval; H.4.2 [Information Systems Applications]: Types of Systemsdecision support.",
                "General Terms Algorithms, Performance, Experimentation 1.",
                "INTRODUCTION Topic Detection and Tracking (TDT) program aims to develop techniques which can effectively organize, search and structure news text materials from a variety of newswire and broadcast media [1].",
                "New Event Detection (NED) is one of the five tasks in TDT.",
                "It is the task of online identification of the earliest report for each topic as soon as that report arrives in the sequence of documents.",
                "A Topic is defined as a seminal event or activity, along with directly related events and activities [2].",
                "An Event is defined as something (non-trivial) happening in a certain place at a certain time [3].",
                "For instance, when a bomb explodes in a building, the exploding is the seminal event that triggers the topic, and other stories on the same topic would be those discussing salvaging efforts, the search for perpetrators, arrests and trial and so on.",
                "Useful news information is usually buried in a mass of data generated everyday.",
                "Therefore, NED systems are very useful for people who need to detect novel information from real-time news stream.",
                "These real-life needs often occur in domains like financial markets, news analysis, and intelligence gathering.",
                "In most of state-of-the-art (currently) NED systems, each news story on hand is compared to all the previous received stories.",
                "If all the similarities between them do not exceed a threshold, then the story triggers a new event.",
                "They are usually in the form of cosine similarity or Hellinger similarity metric.",
                "The core problem of NED is to identify whether two stories are on the same topic.",
                "Obviously, these systems cannot take advantage of topic information.",
                "Further more, it is not acceptable in real applications because of the large amount of computation required in the NED process.",
                "Other systems organize previous stories into clusters (each cluster corresponds to a topic), and new story is compared to the previous clusters instead of stories.",
                "This manner can reduce comparing times significantly.",
                "Nevertheless, it has been proved that this manner is less accurate [4, 5].",
                "This is because sometimes stories within a topic drift far away from each other, which could lead low similarity between a story and its topic.",
                "On the other hand, some proposed NED systems tried to improve accuracy by making better use of named entities [10, 11, 12, 13].",
                "However, none of the systems have considered that terms of different types (e.g.",
                "Noun, Verb or Person name) have different effects for different classes of stories in determining whether two stories are on the same topic.",
                "For example, the names of election candidates (Person name) are very important for stories of election class; the locations (Location name) where accidents happened are important for stories of accidents class.",
                "So, in NED, there still exist following three problems to be investigated: (1) How to speed up the detection procedure while do not decrease the detection accuracy? (2) How to make good use of cluster (topic) information to improve accuracy? (3) How to obtain better news story representation by better understanding of named entities.",
                "Driven by these problems, we have proposed three approaches in this paper. (1)To make the detection procedure faster, we propose a new NED procedure based on news indexing-tree created dynamically.",
                "Story indexing-tree is created by assembling similar stories together to form news clusters in different hierarchies according to their values of similarity.",
                "Comparisons between current story and previous clusters could help find the most similar story in less comparing times.",
                "The new procedure can reduce the amount of comparing times without hurting accuracy. (2)We use the clusters of the first floor in the indexing-tree as news topics, in which term weights are adjusted dynamically according to term distribution in the clusters.",
                "In this approach, cluster (topic) information is used properly, so the problem of theme decentralization is avoided. (3)Based on observations on the statistics obtained from training data, we found that terms of different types (e.g.",
                "Noun and Verb) have different effects for different classes of stories in determining whether two stories are on the same topic.",
                "And we propose to use statistics to optimize the weights of the terms of different types in a story according to the news class that the story belongs to.",
                "On TDT3 dataset, the new NED model just uses 14.9% comparing times of the basic model, while its minimum normalized cost is 0.5012, which is 0.0797 better than the basic model, and also better than any other results previously reported for this dataset [8, 13].",
                "The rest of the paper is organized as follows.",
                "We start off this paper by summarizing the previous work in NED in section 2.",
                "Section 3 presents the basic model for NED that most current systems use.",
                "Section 4 describes our new detection procedure based on news indexing-tree.",
                "In section 5, two term reweighting methods are proposed to improve NED accuracy.",
                "Section 6 gives our experimental data and evaluation metrics.",
                "We finally wrap up with the experimental results in Section 7, and the conclusions and future work in Section 8. 2.",
                "RELATED WORK Papka et al. proposed Single-Pass clustering on NED [6].",
                "When a new story was encountered, it was processed immediately to extract term features and a query representation of the storys content is built up.",
                "Then it was compared with all the previous queries.",
                "If the document did not trigger any queries by exceeding a threshold, it was marked as a new event.",
                "Lam et al build up previous query representations of story clusters, each of which corresponds to a topic [7].",
                "In this manner comparisons happen between stories and clusters.",
                "Recent years, most work focus on proposing better methods on comparison of stories and document representation.",
                "Brants et al. [8] extended a basic incremental TF-IDF model to include sourcespecific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, term reweighting based on inverse event frequencies, and segmentation of documents.",
                "Good improvements on TDT bench-marks were shown.",
                "Stokes et al. [9] utilized a combination of evidence from two distinct representations of a documents content.",
                "One of the representations was the usual free text vector, the other made use of lexical chains (created using WordNet) to build another term vector.",
                "Then the two representations are combined in a linear fashion.",
                "A marginal increase in effectiveness was achieved when the combined representation was used.",
                "Some efforts have been done on how to utilize named entities to improve NED.",
                "Yang et al. gave location named entities four times weight than other terms and named entities [10].",
                "DOREMI research group combined semantic similarities of person names, location names and time together with textual similarity [11][12].",
                "UMass [13] research group split document representation into two parts: named entities and non-named entities.",
                "And it was found that some classes of news could achieve better performance using named entity representation, while some other classes of news could achieve better performance using non-named entity representation.",
                "Both [10] and [13] used text categorization technique to classify news stories in advance.",
                "In [13] news stories are classified automatically at first, and then test sensitivities of names and non-name terms for NED for each class.",
                "In [10] frequent terms for each class are removed from document representation.",
                "For example, word election does not help identify different elections.",
                "In their work, effectiveness of different kinds of names (or terms with different POS) for NED in different news classes are not investigated.",
                "We use statistical analysis to reveal the fact and use it to improve NED performance. 3.",
                "BASIC MODEL In this section, we present the basic New Event Detection model which is similar to what most current systems apply.",
                "Then, we propose our new model by extending the basic model.",
                "New Event Detection systems use news story stream as input, in which stories are strictly time-ordered.",
                "Only previously received stories are available when dealing with current story.",
                "The output is a decision for whether the current story is on a new event or not and the confidence of the decision.",
                "Usually, a NED model consists of three parts: story representation, similarity calculation and detection procedure. 3.1 Story Representation Preprocessing is needed before generating story representation.",
                "For preprocessing, we tokenize words, recognize abbreviations, normalize abbreviations, add part-of-speech tags, remove stopwords included in the stop list used in InQuery [14], replace words with their stems using K-stem algorithm[15], and then generate word vector for each news story.",
                "We use incremental TF-IDF model for term weight calculation [4].",
                "In a TF-IDF model, term frequency in a news document is weighted by the inverse document frequency, which is generated from training corpus.",
                "When a new term occurs in testing process, there are two solutions: simply ignore the new term or set df of the term as a small const (e.g. df = 1).",
                "The new term receives too low weight in the first solution (0) and too high weight in the second solution.",
                "In incremental TF-IDF model, document frequencies are updated dynamically in each time step t: 1( ) ( ) ( )t t D tdf w df w df w−= + (1) where Dt represents news story set received in time t, and dfDt(w) means the number of documents that term w occurs in, and dft(w) means the total number of documents that term w occurs in before time t. In this work, each time window includes 50 news stories.",
                "Thus, each story d received in t is represented as follows: 1 2{ ( , , ), ( , , ),..., ( , , )}nd weight d t w weight d t w weight d t w→ where n means the number of distinct terms in story d, and ( , , )weight d t w means the weight of term w in story d at time t: log( ( , ) 1) log(( 1) /( ( ) 0.5)) ( , , ) log( ( , ) 1) log(( 1) /( ( ) 0.5)) t t t t w d tf d w N df w weight d t w tf d w N df w ∈ + + + = + + +∑ (2) where Nt means the total number of news stories before time t, and tf(d,w) means how many times term w occurs in news story d. 3.2 Similarity Calculation We use Hellinger distance for the calculation of similarity between two stories, for two stories d and d at time t, their similarity is defined as follows: , ( , , ) ( , , ) * ( , , ) w d d sim d d t weight d t w weight d t w ∈ = ∑ (3) 3.3 Detection Procedure For each story d received in time step t, the value ( ) ( ) ( ) ( ( , , )) time d time d n d max sim d d t < = (4) is a score used to determine whether d is a story about a new topic and at the same time is an indication of the confidence in our decision [8]. time(d) means the publication time of story d. If the score exceeds the thresholdθ new, then there exists a sufficiently similar document, thus d is a old story, otherwise, there is no sufficiently similar previous document, thus d is an new story. 4.",
                "New NED Procedure Traditional NED systems can be classified into two main types on the aspect of detection procedure: (1) S-S type, in which the story on hand is compared to each story received previously, and use the highest similarity to determine whether current story is about a new event; (2) S-C type, in which the story on hand is compared to all previous clusters each of which representing a topic, and the highest similarity is used for final decision for current story.",
                "If the highest similarity exceeds thresholdθ new, then it is an old story, and put it into the most similar cluster; otherwise it is a new story and create a new cluster.",
                "Previous work show that the first manner is more accurate than the second one [4][5].",
                "Since sometimes stories within a topic drift far away from each other, a story may have very low similarity with its topic.",
                "So using similarities between stories for determining new story is better than using similarities between story and clusters.",
                "Nevertheless, the first manner needs much more comparing times which means the first manner is low efficient.",
                "We propose a new detection procedure which uses comparisons with previous clusters to help find the most similar story in less comparing times, and the final new event decision is made according to the most similar story.",
                "Therefore, we can get both the accuracy of S-S type methods and the efficiency of S-C type methods.",
                "The new procedure creates a news indexing-tree dynamically, in which similar stories are put together to form a hierarchy of clusters.",
                "We index similar stories together by their common ancestor (a cluster node).",
                "Dissimilar stories are indexed in different clusters.",
                "When a story is coming, we use comparisons between the current story and previous hierarchical clusters to help find the most similar story which is useful for new event decision.",
                "After the new event decision is made, the current story is inserted to the indexing-tree for the following detection.",
                "The news indexing-tree is defined formally as follows: S-Tree = {r, NC , NS , E} where r is the root of S-Tree, NC is the set of all cluster nodes, NS is the set of all story nodes, and E is the set of all edges in S-Tree.",
                "We define a set of constraints for a S-Tree: ⅰ . , is an non-terminal node in the treeC i i N i∀ ∈ → ⅱ . , is a terminal node in the treeS i i N i∀ ∈ → ⅲ . , out degree of is at least 2C i i N i∀ ∈ → ⅳ . , is represented as the centroid of its desendantsC i i iN∀ ∈ → For a news story di, the comparison procedure and inserting procedure based on indexing-tree are defined as follows.",
                "An example is shown by Figure 1 and Figure 2.",
                "Figure 1.",
                "Comparison procedure Figure 2.",
                "Inserting procedure Comparison procedure: Step 1: compare di to all the direct child nodes of r and select λ nodes with highest similarities, e.g., C1 2 and C1 3 in Figure 1.",
                "Step 2: for each selected node in the last step, e.g.",
                "C1 2, compare di to all its direct child nodes, and select λ nodes with highest similarities, e.g.",
                "C2 2 and d8.",
                "Repeat step 2 for all non-terminal nodes.",
                "Step 3: record the terminal node with the highest similarty to di, e.g. s5, and the similarity value (0.20).",
                "Inserting di to the S-tree with r as root: Find the node n which is direct child of r in the path from r to the terminal node with highest similarity s, e.g.",
                "C1 2.",
                "If s is smaller than θ init+(h-1)δ , then add di to the tree as a direct child of r. Otherwise, if n is a terminal node, then create a cluster node instead of n, and add both n and di as its direct children; if n is an non-terminal node, then repeat this procedure and insert di to the sub-tree with n as root recursively.",
                "Here h is the length between n and the root of S-tree.",
                "The more the stories in a cluster similar to each other, the better the cluster represents the stories in it.",
                "Hence we add no constraints on the maximum of trees height and degree of a node.",
                "Therefore, we cannot give the complexity of this indexing-tree based procedure.",
                "But we will give the number of comparing times needed by the new procedure in our experiments in section7. 5.",
                "Term Reweighting Methods In this section, two term reweighting methods are proposed to improve NED accuracy.",
                "In the first method, a new way is explored for better using of cluster (topic) information.",
                "The second one finds a better way to make use of named entities based on news classification. 5.1 Term Reweighting Based on Distribution Distance TF-IDF is the most prevalent model used in information retrieval systems.",
                "The basic idea is that the fewer documents a term appears in, the more important the term is in discrimination of documents (relevant or not relevant to a query containing the term).",
                "Nevertheless, in TDT domain, we need to discriminate documents with regard to topics rather than queries.",
                "Intuitively, using cluster (topic) vectors to compare with subsequent news stories should outperform using story vectors.",
                "Unfortunately, the experimental results do not support this intuition [4][5].",
                "Based on observation on data, we find the reason is that a news topic usually contains many directly or indirectly related events, while they all have their own sub-subjects which are usually different with each other.",
                "Take the topic described in section 1 as an example, events like the explosion and salvage have very low similarities with events about criminal trial, therefore stories about trial would have low similarity with the topic vector built on its previous events.",
                "This section focuses on how to effectively make use of topic information and at the same time avoid the problem of content decentralization.",
                "At first, we classify terms into 5 classes to help analysis the needs of the modified model: Term class A: terms that occur frequently in the whole corpus, e.g., year and people.",
                "Terms of this class should be given low weights because they do not help much for topic discrimination.",
                "Term class B: terms that occur frequently within a news category, e.g., election, storm.",
                "They are useful to distinguish two stories in different news categories.",
                "However, they cannot provide information to determine whether two stories are on the same or different topics.",
                "In another words, term election and term storm are not helpful in differentiate two election campaigns and two storm disasters.",
                "Therefore, terms of this class should be assigned lower weights.",
                "Term class C: terms that occur frequently in a topic, and infrequently in other topics, e.g., the name of a crash plane, the name of a specific hurricane.",
                "News stories that belong to different topics rarely have overlap terms in this class.",
                "The more frequently a term appears in a topic, the more important the term is for a story belonging to the topic, therefore the term should be set higher weight.",
                "Term class D: terms that appear in a topic exclusively, but not frequently.",
                "For example, the name of a fireman who did very well in a salvage action, which may appears in only two or three stories but never appeared in other topics.",
                "Terms of this type should receive more weights than in TF-IDF model.",
                "However, since they are not popular in the topic, it is not appropriate to give them too high weights.",
                "Term class E: terms with low document frequency, and appear in different topics.",
                "Terms of this class should receive lower weights.",
                "Now we analyze whether TF-IDF model can give proper weights to the five classes of terms.",
                "Obviously, terms of class A are lowly weighted in TF-IDF model, which is conformable with the requirement described above.",
                "In TF-IDF model, terms of class B are highly dependant with the number of stories in a news class.",
                "TF-IDF model cannot provide low weights if the story containing the term belongs to a relative small news class.",
                "For a term of class C, the more frequently it appears in a topic, the less weight TFIDF model gives to it.",
                "This strongly conflicts with the requirement of terms in class C. For terms of class D, TF-IDF model gives them high weights correctly.",
                "But for terms of class E, TF-IDF model gives high weights to them which are not conformable with the requirement of low weights.",
                "To sum up, terms of class B, C, E cannot be properly weighted in TF-IDF model.",
                "So, we propose a modified model to resolve this problem.",
                "When θ init andθ new are set closely, we assume that most of the stories in a first-level cluster (a direct child node of root node) are on the same topic.",
                "Therefore, we make use of a first-level cluster to capture term distribution (df for all the terms within the cluster) within the topic dynamically.",
                "KL divergence of term distribution in a first-level cluster and the whole story set is used to adjust term weights:  ( , , ) * (1 * ( || )) ( , , ) ( , , ) * (1 * ( || )) cw tw cw tw w d D weight d t w KL P P weight d t w weight d t w KL P P γ γ ∈ + = +∑ (5) where ( ) ( ) ( ) ( ) 1,cw cw c c c c df w df w p y p y N N = = − (6) ( ) ( ) ( ) ( ) 1,t t tw tw t t df w df w p y p y N N = = − (7) where dfc(w) is the number of documents containing term w within cluster C, and Nc is the number of documents in cluster C, and Nt is the total number of documents that arrive before time step t. γ is a const parameter, now is manually set 3.",
                "KL divergence is defined as follows [17]: ( ) ( || ) ( ) log ( )x p x KL P Q p x q x = ∑ (8) The basic idea is: for a story in a topic, the more a term occurs within the topic, and the less it occurs in other topics, it should be assigned higher weights.",
                "Obviously, modified model can meet all the requirements of the five term classes listed above. 5.2 Term Reweighting Based on Term Type and Story Class Previous work found that some classes of news stories could achieve good improvements by giving extra weight to named entities.",
                "But we find that terms of different types should be given different amount of extra weight for different classes of news stories.",
                "We use open-NLP1 to recognize named entity types and part-ofspeech tags for terms that appear in news stories.",
                "Named entity types include person name, organization name, location name, date, time, money and percentage, and five POSs are selected: none (NN), verb (VB), adjective (JJ), adverb (RB) and cardinal number (CD).",
                "Statistical analysis shows topic-level discriminative terms types for different classes of stories.",
                "For the sake of convenience, named entity type and part-of-speech tags are uniformly called term type in subsequent sections.",
                "Determining whether two stories are about the same topic is a basic component for NED task.",
                "So at first we use 2 χ statistic to compute correlations between terms and topics.",
                "For a term t and a topic T, a contingence table is derived: Table 1.",
                "A 2×2 Contingence Table Doc Number belong to topic T not belong to topic T include t A B not include t C D The 2 χ statistic for a specific term t with respect to topic T is defined to be [16]: 2 2 ( , ) ( ) * ( ) ( ) * ( ) * ( ) * ( ) w T A B C D AD CB A C B D A B C D χ = + + + − + + + + (9) News topics for the TDT task are further classified into 11 rules of interpretations (ROIs) 2 .",
                "The ROI can be seen as a higher level class of stories.",
                "The average correlation between a term type and a topic ROI is computed as: 2 avg 2 ( , )( ( , ) )k m m km kT R w P w TP R p w T R P χ χ ∈ ∈ ∑ ∑（ , ）= 1 1 k=1…K, m=1…M (10) where K is the number of term types (set 12 constantly in the paper).",
                "M is the number news classes (ROIs, set 11 in the paper).",
                "Pk represents the set of all terms of type k, and Rm represents the set of all topics of class m, p(t,T) means the probability that t occurs in topic T. Because of limitation of space, only parts of the term types (9 term types) and parts of news classes (8 classes) are listed in table 2 with the average correlation values between them.",
                "The statistics is derived from labeled data in TDT2 corpus. (Results in table 2 are already normalized for convenience in comparison.)",
                "The statistics in table 2 indicates the usefulness of different term types in topic discrimination with respect to different news classes.",
                "We can see that, location name is the most useful term type for three news classes: Natural Disasters, Violence or War, Finances.",
                "And for three other categories Elections, Legal/Criminal Cases, Science and Discovery, person name is the most discriminative term type.",
                "For Scandals/Hearings, date is the most important information for topic discrimination.",
                "In addition, Legal/Criminal Cases and Finance topics have higher correlation with money terms, while Science and Discovery have higher correlation with percentage terms.",
                "Non-name terms are more stable for different classes. 1 . http://opennlp.sourceforge.net/ 2 . http://projects.ldc.upenn.edu/TDT3/Guide/label.html From the analysis of table 2, it is reasonable to adjust term weight according to their term type and the news class the story belongs to.",
                "New term weights are reweighted as follows: ( ) ( ) ( ) ( ) ( , , ) * ( , , ) ( , , ) * class d D type w T class d D type w w d weight d t w weight d t w weight d t w α α ∈ = ∑ (11) where type(w) represents the type of term w, and class(d) represents the class of story d, c kα is reweighting parameter for news class c and term type k. In the work, we just simply use statistics in table 2 as the reweighting parameters.",
                "Even thought using the statistics directly may not the best choice, we do not discuss how to automatically obtain the best parameters.",
                "We will try to use machine learning techniques to obtain the best parameters in the future work.",
                "In the work, we use BoosTexter [20] to classify all stories into one of the 11 ROIs.",
                "BoosTexter is a boosting based machine learning program, which creates a series of simple rules for building a classifier for text or attribute-value data.",
                "We use term weight generated using TF-IDF model as feature for story classification.",
                "We trained the model on the 12000 judged English stories in TDT2, and classify the rest of the stories in TDT2 and all stories in TDT3.",
                "Classification results are used for term reweighting in formula (11).",
                "Since the class labels of topic-off stories are not given in TDT datasets, we cannot give the classification accuracy here.",
                "Thus we do not discuss the effects of classification accuracy to NED performance in the paper. 6.",
                "EXPERIMENTAL SETUP 6.1 Datasets We used two LDC [18] datasets TDT2 and TDT3 for our experiments.",
                "TDT2 contains news stories from January to June 1998.",
                "It contains around 54,000 stories from sources like ABC, Associated Press, CNN, New York Times, Public Radio International, Voice of America etc.",
                "Only English stories in the collection were considered.",
                "TDT3 contains approximately 31,000 English stories collected from October to December 1998.",
                "In addition to the sources used in TDT2, it also contains stories from NBC and MSNBC TV broadcasts.",
                "We used transcribed versions of the TV and radio broadcasts besides textual news.",
                "TDT2 dataset is labeled with about 100 topics, and approximately 12,000 English stories belong to at least one of these topics.",
                "TDT3 dataset is labeled with about 120 topics, and approximately 8000 English stories belong to at least one of these topics.",
                "All the topics are classified into 11 Rules of Interpretation: (1)Elections, (2)Scandals/Hearings, (3)Legal/Criminal Cases, (4)Natural Disasters, (5)Accidents, (6)Ongoing Violence or War, (7)Science and Discovery News, (8)Finance, (9)New Law, (10)Sports News, (11)MISC.",
                "News. 6.2 Evaluation Metric TDT uses a cost function CDet that combines the probabilities of missing a new story and a false alarm [19]: * * * *Det Miss Miss Target FA FA NontargetC C P P C P P= + (12) Table 2.",
                "Average correlation between term types and news classes where CMiss means the cost of missing a new story, PMiss means the probability of missing a new story, and PTarget means the probability of seeing a new story in the data; CFA means the cost of a false alarm, PFA means the probability of a false alarm, and PNontarget means the probability of seeing an old story.",
                "The cost CDet is normalized such that a perfect system scores 0 and a trivial system, which is the better one of mark all stories as new or old, scores 1: ( ( * , * ) ) Det Det Miss Target FA Nontarget C Norm C min C P C P = (13) New event detection system gives two outputs for each story.",
                "The first part is yes or no indicating whether the story triggers a new event or not.",
                "The second part is a score indicating confidence of the first decision.",
                "Confidence scores can be used to plot DET curve, i.e., curves that plot false alarm vs. miss probabilities.",
                "Minimum normalized cost can be determined if optimal threshold on the score were chosen. 7.",
                "EXPERIMENTAL RESULTS 7.1 Main Results To test the approaches proposed in the model, we implemented and tested five systems: System-1: this system is used as baseline.",
                "It is implemented based on the basic model described in section 3, i.e., using incremental TF-IDF model to generate term weights, and using Hellinger distance to compute document similarity.",
                "Similarity score normalization is also employed [8].",
                "S-S detection procedure is used.",
                "System-2: this system is the same as system-1 except that S-C detection procedure is used.",
                "System-3: this system is the same as system-1 except that it uses the new detection procedure which is based on indexing-tree.",
                "System-4: implemented based on the approach presented in section 5.1, i.e., terms are reweighted according to the distance between term distributions in a cluster and all stories.",
                "The new detection procedure is used.",
                "System-5: implemented based on the approach presented in section 5.2, i.e., terms of different types are reweighted according to news class using trained parameters.",
                "The new detection procedure is used.",
                "The following are some other NED systems: System-6: [21] for each pair of stories, it computes three similarity values for named entity, non-named entity and all terms respectively.",
                "And employ Support Vector Machine to predict new or old using the similarity values as features.",
                "System-7: [8] it extended a basic incremental TF-IDF model to include source-specific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, etc.",
                "System-8: [13] it split document representation into two parts: named entities and non-named entities, and choose one effective part for each news class.",
                "Table 3 and table 4 show topic-weighted normalized costs and comparing times on TDT2 and TDT3 datasets respectively.",
                "Since no heldout data set for fine-tuning the threshold θ new was available for experiments on TDT2, we only report minimum normalized costs for our systems in table 3.",
                "System-5 outperforms all other systems including system-6, and it performs only 2.78e+8 comparing times in detection procedure which is only 13.4% of system-1.",
                "Table 3.",
                "NED results on TDT2 Systems Min Norm(CDet) Cmp times System-1 0.5749 2.08e+9 System-2① 0.6673 3.77e+8 System-3② 0.5765 2.81e+8 System-4② 0.5431 2.99e+8 System-5② 0.5089 2.78e+8 System-6 0.5300 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 When evaluating on the normalized costs on TDT3, we use the optimal thresholds obtained from TDT2 data set for all systems.",
                "System-2 reduces comparing times to 1.29e+9 which is just 18.3% of system-1, but at the same time it also gets a deteriorated minimum normalized cost which is 0.0499 higher than system-1.",
                "System-3 uses the new detection procedure based on news indexing-tree.",
                "It requires even less comparing times than system-2.",
                "This is because story-story comparisons usually yield greater similarities than story-cluster ones, so stories tend to be combined Location Person Date Organization Money Percentage NN JJ CD Elections 0.37 1 0.04 0.58 0.08 0.03 0.32 0.13 0.1 Scandals/Hearings 0.66 0.62 0.28 1 0.11 0.02 0.27 0.13 0.05 Legal/Criminal Cases 0.48 1 0.02 0.62 0.15 0 0.22 0.24 0.09 Natural Disasters 1 0.27 0 0.04 0.04 0 0.25 0.04 0.02 Violence or War 1 0.36 0.02 0.14 0.02 0.04 0.21 0.11 0.02 Science and Discovery 0.11 1 0.01 0.22 0.08 0.12 0.19 0.08 0.03 Finances 1 0.45 0.04 0.98 0.13 0.02 0.29 0.06 0.05 Sports 0.16 0.27 0.01 1 0.02 0 0.11 0.03 0.01 together in system-3.",
                "And system-3 is basically equivalent to system-1 in accuracy results.",
                "System-4 adjusts term weights based on the distance of term distributions between the whole corpus and cluster story set, yielding a good improvement by 0.0468 compared to system-1.",
                "The best system (system-5) has a minimum normalized cost 0.5012, which is 0.0797 better than system-1, and also better than any other results previously reported for this dataset [8, 13].",
                "Further more, system-5 only needs 1.05e+8 comparing times which is 14.9% of system-1.",
                "Table 4.",
                "NED results on TDT3 Systems Norm(CDet) Min Norm(CDet) Cmp times System-1 0.6159 0.5809 7.04e+8 System-2① 0.6493 0.6308 1.29e+8 System-3② 0.6197 0.5868 1.03e+8 System-4② 0.5601 0.5341 1.03e+8 System-5② 0.5413 0.5012 1.05e+8 System-7 -- 0.5783 -System-8 -- 0.5229 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 Figure5 shows the five DET curves for our systems on data set TDT3.",
                "System-5 achieves the minimum cost at a false alarm rate of 0.0157 and a miss rate of 0.4310.",
                "We can observe that System4 and System-5 obtain lower miss probability at regions of low false alarm probabilities.",
                "The hypothesis is that, more weight value is transferred to key terms of topics from non-key terms.",
                "Similarity score between two stories belonging to different topics are lower than before, because their overlapping terms are usually not key terms of their topics. 7.2 Parameter selection for indexing-tree detection Figure 3 shows the minimum normalized costs obtained by system-3 on TDT3 using different parameters.",
                "Theθ init parameter is tested on six values spanning from 0.03 to 0.18.",
                "And the λ parameter is tested on four values 1, 2, 3 and 4.",
                "We can see that, whenθ init is set to 0.12, which is the closest one toθ new, the costs are lower than others.",
                "This is easy to explain, because when stories belonging to the same topic are put in a cluster, it is more reasonable for the cluster to represent the stories in it.",
                "When parameter λ is set to 3 or 4, the costs are better than other cases, but there is no much difference between 3 and 4. 0 0.05 0.1 0.15 0.2 1 2 3 4 0.5 0.6 0.7 0.8 0.9 1 θ-initλ MinCost 0.6 0.65 0.7 0.75 0.8 0.85 0.9 Figure 3.",
                "Min Cost on TDT3 (δ =0.15) 0 0.05 0.1 0.15 0.2 1 2 3 4 0 0.5 1 1.5 2 2.5 x 10 8 θ-init λ Comparingtimes 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 8 Figure 4.",
                "Comparing times on TDT3 (δ =0.15) Figure 4 gives the comparing times used by system-3 on TDT3 with the same parameters as figure 3.",
                "The comparing times are strongly dependent onθ init.",
                "Because the greaterθ init is, the less stories combined together, the more comparing times are needed for new event decision.",
                "So we useθ init =0.13,λ =3,δ =0.15 for system-3, 4, and 5.",
                "In this parameter setting, we can get both low minimum normalized costs and less comparing times. 8.",
                "CONCLUSION We have proposed a news indexing-tree based detection procedure in our model.",
                "It reduces comparing times to about one seventh of traditional method without hurting NED accuracy.",
                "We also have presented two extensions to the basic TF-IDF model.",
                "The first extension is made by adjust term weights based on term distributions between the whole corpus and a cluster story set.",
                "And the second extension to basic TF-IDF model is better use of term types (named entities types and part-of-speed) according to news categories.",
                "Our experimental results on TDT2 and TDT3 datasets show that both of the two extensions contribute significantly to improvement in accuracy.",
                "We did not consider news time information as a clue for NED task, since most of the topics last for a long time and TDT data sets only span for a relative short period (no more than 6 months).",
                "For the future work, we want to collect news set which span for a longer period from internet, and integrate time information in NED task.",
                "Since topic is a relative coarse-grained news cluster, we also want to refine cluster granularity to event-level, and identify different events and their relations within a topic.",
                "Acknowledgments This work is supported by the National Natural Science Foundation of China under Grant No. 90604025.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsor. 9.",
                "REFERENCES [1] http://www.nist.gov/speech/tests/tdt/index.htm [2] In Topic Detection and Tracking.",
                "Event-based Information Organization.",
                "Kluwer Academic Publishers, 2002. .01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90 1 2 5 10 20 40 60 80 90 False Alarm Probability (in %) MissProbability(in%) SYSTEM1 Topic Weighted Curve SYSTEM1 Min Norm(Cost) SYSTEM2 Topic Weighted Curve SYSTEM2 Min Norm(Cost) SYSTEM3 Topic Weighted Curve SYSTEM3 Min Norm(Cost) SYSTEM4 Topic Weighted Curve SYSTEM4 Min Norm(Cost) SYSTEM5 Topic Weighted Curve SYSTEM5 Min Norm(Cost) Random Performance Figure 5.",
                "DET curves on TDT3 [3] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B.T.",
                "Archibald, and X. Liu.",
                "Learning Approaches for Detecting and Tracking News Events.",
                "In IEEE Intelligent Systems Special Issue on Applications of Intelligent Information Retrieval, volume 14 (4), 1999, 32-43. [4] Y. Yang, T. Pierce, and J. Carbonell.",
                "A Study on Retrospective and On-line Event Detection.",
                "In Proceedings of SIGIR-98, Melbourne, Australia, 1998, 28-36. [5] J. Allan, V. Lavrenko, D. Malin, and R. Swan.",
                "Detections, Bounds, and Timelines: Umass and tdt-3.",
                "In Proceedings of Topic Detection and Tracking Workshop (TDT-3), Vienna, VA, 2000, 167-174. [6] R. Papka and J. Allan.",
                "On-line New Event Detection Using Single Pass Clustering TITLE2:.",
                "Technical Report UM-CS1998-021, 1998. [7] W. Lam, H. Meng, K. Wong, and J.",
                "Yen.",
                "Using Contextual Analysis for News Event Detection.",
                "International Journal on Intelligent Systems, 2001, 525-546. [8] B. Thorsten, C. Francine, and F. Ayman.",
                "A System for New Event Detection.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2003, 330-337. [9] S. Nicola and C. Joe.",
                "Combining Semantic and Syntactic Document Classifiers to Improve First Story Detection.",
                "In Proceedings of the 24th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topicconditioned Novelty Detection.",
                "In Proceedings of the 8th ACM SIGKDD International Conference, ACM Press. 2002, 688-693. [11] M. Juha, A.M. Helena, and S. Marko.",
                "Applying Semantic Classes in Event Detection and Tracking.",
                "In Proceedings of International Conference on Natural Language Processing (ICON 2002), 2002, pages 175-183. [12] M. Juha, A.M. Helena, and S. Marko.",
                "Simple Semantics in Topic Detection and Tracking.",
                "Information Retrieval, 7(3-4): 2004, 347-368. [13] K. Giridhar and J. Allan.",
                "Text Classification and Named Entities for New Event Detection.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2004, 297-304. [14] J. P. Callan, W. B. Croft, and S. M. Harding.",
                "The INQUERY Retrieval System.",
                "In Proceedings of DEXA-92, 3rd International Conference on Database and Expert Systems Applications, 1992, 78-83. [15] R. Krovetz.",
                "Viewing Morphology as An Inference Process.",
                "In Proceedings of ACM SIGIR93, 1993, 61-81. [16] Y. Yang and J. Pedersen.",
                "A Comparative Study on Feature Selection in Text Categorization.",
                "In J. D. H. Fisher, editor, The Fourteenth International Conference on Machine Learning (ICML97), Morgan Kaufmann, 1997, 412-420. [17] T. M. Cover, and J.A.",
                "Thomas.",
                "Elements of Information Theory.",
                "Wiley. 1991. [18] The linguistic data consortium, http://www.ldc,upenn.edu/. [19] The 2001 TDT task definition and evaluation plan, http://www.nist.gov/speech/tests/tdt/tdt2001/evalplan.htm. [20] R. E. Schapire and Y.",
                "Singer.",
                "Boostexter: A Boosting-based System for Text Categorization.",
                "In Machine Learning 39(2/3):1, Kluwer Academic Publishers, 2000, 35-168. [21] K. Giridhar and J. Allan. 2005.",
                "Using Names and Topics for New Event Detection.",
                "In Proceedings of Human Technology Conference and Conference on Empirical Methods in Natural Language, Vancouver, 2005, 121-128"
            ],
            "original_annotated_samples": [
                "In this paper we propose a new NED model to <br>speed up the ned task</br> by using news indexing-tree dynamically."
            ],
            "translated_annotated_samples": [
                "En este artículo proponemos un nuevo modelo de NED para <br>acelerar la tarea de NED</br> mediante el uso dinámico de un árbol de indexación de noticias."
            ],
            "translated_text": "Detección de nuevos eventos basada en árbol de indexación y entidad nombrada Zhang Kuo Universidad Tsinghua Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Universidad Tsinghua Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Universidad Tsinghua Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn RESUMEN La Detección de Nuevos Eventos (NED) tiene como objetivo detectar de una o varias corrientes de noticias cuál se informa sobre un nuevo evento (es decir, no reportado previamente). Con el abrumador volumen de noticias disponible hoy en día, hay una creciente necesidad de un sistema NED que sea capaz de detectar eventos nuevos de manera más eficiente y precisa. En este artículo proponemos un nuevo modelo de NED para <br>acelerar la tarea de NED</br> mediante el uso dinámico de un árbol de indexación de noticias. Además, basándose en la observación de que los términos de diferentes tipos tienen diferentes efectos para la tarea de NED, se proponen dos enfoques de reponderación de términos para mejorar la precisión de NED. En el primer enfoque, proponemos ajustar dinámicamente los pesos de los términos basados en los clusters de historias previas y en el segundo enfoque, proponemos emplear estadísticas en los datos de entrenamiento para aprender el modelo de reajuste de entidades nombradas para cada clase de historias. Los resultados experimentales en dos conjuntos de datos del Consorcio de Datos Lingüísticos (LDC), TDT2 y TDT3, muestran que el modelo propuesto puede mejorar significativamente tanto la eficiencia como la precisión de la tarea de NED, en comparación con el sistema base y otros sistemas existentes. Categorías y Descriptores de Asignaturas H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información; H.4.2 [Aplicaciones de Sistemas de Información]: Tipos de Sistemas de Apoyo a Decisiones. Términos generales Algoritmos, Rendimiento, Experimentación 1. El programa de Detección y Seguimiento de Temas (TDT) tiene como objetivo desarrollar técnicas que puedan organizar, buscar y estructurar de manera efectiva materiales de texto de noticias de una variedad de agencias de noticias y medios de difusión [1]. La Detección de Eventos Nuevos (DEN) es una de las cinco tareas en TDT. Es la tarea de identificación en línea del informe más temprano para cada tema tan pronto como ese informe llegue en la secuencia de documentos. Un tema se define como un evento o actividad seminal, junto con eventos y actividades directamente relacionados [2]. Un evento se define como algo (no trivial) que sucede en un lugar y en un momento determinados [3]. Por ejemplo, cuando una bomba explota en un edificio, la explosión es el evento seminal que desencadena el tema, y otras historias sobre el mismo tema serían aquellas que discuten los esfuerzos de salvamento, la búsqueda de los perpetradores, arrestos y juicios, entre otros. La información útil de noticias suele estar enterrada en una masa de datos generados a diario. Por lo tanto, los sistemas NED son muy útiles para las personas que necesitan detectar información novedosa de un flujo de noticias en tiempo real. Estas necesidades de la vida real a menudo se presentan en ámbitos como los mercados financieros, el análisis de noticias y la recopilación de inteligencia. En la mayoría de los sistemas NED de última generación, cada noticia disponible se compara con todas las noticias previamente recibidas. Si todas las similitudes entre ellos no superan un umbral, entonces la historia desencadena un nuevo evento. Por lo general, se presentan en forma de similitud de coseno o métrica de similitud de Hellinger. El problema central de NED es identificar si dos historias tratan sobre el mismo tema. Obviamente, estos sistemas no pueden aprovechar la información del tema. Además, no es aceptable en aplicaciones reales debido a la gran cantidad de cálculos requeridos en el proceso de NED. Otros sistemas organizan historias anteriores en grupos (cada grupo corresponde a un tema), y la nueva historia se compara con los grupos anteriores en lugar de con historias. Esta forma puede reducir significativamente los tiempos de comparación. Sin embargo, se ha demostrado que este método es menos preciso [4, 5]. Esto se debe a que a veces las historias dentro de un tema se alejan mucho entre sí, lo que podría resultar en una baja similitud entre una historia y su tema. Por otro lado, algunos sistemas NED propuestos intentaron mejorar la precisión al hacer un mejor uso de las entidades nombradas [10, 11, 12, 13]. Sin embargo, ninguno de los sistemas ha considerado que los términos de diferentes tipos (por ejemplo, Los sustantivos, verbos o nombres de personas tienen diferentes efectos en diferentes clases de historias para determinar si dos historias tratan sobre el mismo tema. Por ejemplo, los nombres de los candidatos electorales (Nombre de la persona) son muy importantes para las historias de la clase de elecciones; las ubicaciones (Nombre de la ubicación) donde ocurrieron los accidentes son importantes para las historias de la clase de accidentes. Por lo tanto, en NED, todavía existen los siguientes tres problemas por investigar: (1) ¿Cómo acelerar el procedimiento de detección sin disminuir la precisión de la detección? (2) ¿Cómo aprovechar mejor la información de los grupos (temas) para mejorar la precisión? (3) ¿Cómo obtener una mejor representación de historias de noticias mediante una mejor comprensión de las entidades nombradas? Impulsados por estos problemas, hemos propuesto tres enfoques en este documento. (1) Para hacer que el procedimiento de detección sea más rápido, proponemos un nuevo procedimiento de NED basado en un árbol de indexación de noticias creado dinámicamente. El índice de historias en árbol se crea ensamblando historias similares para formar grupos de noticias en diferentes jerarquías según sus valores de similitud. Las comparaciones entre la historia actual y los grupos anteriores podrían ayudar a encontrar la historia más similar en menos tiempo de comparación. El nuevo procedimiento puede reducir la cantidad de veces de comparación sin afectar la precisión. Utilizamos los grupos del primer piso en el árbol de indexación como temas de noticias, en los cuales los pesos de los términos se ajustan dinámicamente según la distribución de términos en los grupos. En este enfoque, se utiliza adecuadamente la información de los grupos (temas), por lo que se evita el problema de la descentralización del tema. Basándonos en observaciones de las estadísticas obtenidas de los datos de entrenamiento, encontramos que los términos de diferentes tipos (por ejemplo, Los sustantivos y verbos tienen diferentes efectos para diferentes clases de historias al determinar si dos historias tratan sobre el mismo tema. Y proponemos utilizar estadísticas para optimizar los pesos de los términos de diferentes tipos en una historia de acuerdo a la clase de noticias a la que pertenece la historia. En el conjunto de datos TDT3, el nuevo modelo NED solo utiliza un 14.9% en comparación con los tiempos del modelo básico, mientras que su costo normalizado mínimo es de 0.5012, lo que es 0.0797 mejor que el modelo básico, y también mejor que cualquier otro resultado previamente reportado para este conjunto de datos [8, 13]. El resto del documento está organizado de la siguiente manera. Comenzamos este artículo resumiendo el trabajo previo en NED en la sección 2. La sección 3 presenta el modelo básico para NED que la mayoría de los sistemas actuales utilizan. La sección 4 describe nuestro nuevo procedimiento de detección basado en el índice de árbol de noticias. En la sección 5, se proponen dos métodos de reponderación de términos para mejorar la precisión de NED. La sección 6 presenta nuestros datos experimentales y métricas de evaluación. Finalmente concluimos con los resultados experimentales en la Sección 7, y las conclusiones y trabajo futuro en la Sección 8. TRABAJO RELACIONADO Papka et al. propusieron el agrupamiento de un solo paso en NED [6]. Cuando se encontraba una nueva historia, esta era procesada inmediatamente para extraer características de términos y se construía una representación de consulta del contenido de la historia. Luego se comparó con todas las consultas anteriores. Si el documento no generaba ninguna consulta al exceder un umbral, se marcaba como un nuevo evento. Lam et al construyen representaciones de consultas previas de grupos de historias, cada una de las cuales corresponde a un tema [7]. De esta manera se realizan comparaciones entre historias y grupos. En los últimos años, la mayoría de los trabajos se han centrado en proponer mejores métodos para la comparación de historias y la representación de documentos. Brants et al. [8] extendieron un modelo básico incremental de TF-IDF para incluir modelos específicos de fuente, normalización de puntajes de similitud basada en promedios específicos de documentos, normalización de puntajes de similitud basada en promedios específicos de pares de fuentes, reponderación de términos basada en frecuencias de eventos inversas y segmentación de documentos. Se mostraron buenas mejoras en los puntos de referencia de TDT. Stokes et al. [9] utilizaron una combinación de evidencia de dos representaciones distintas del contenido de un documento. Una de las representaciones era el vector de texto libre habitual, la otra hacía uso de cadenas léxicas (creadas utilizando WordNet) para construir otro vector de términos. Luego, las dos representaciones se combinan de forma lineal. Se logró un aumento marginal en la efectividad cuando se utilizó la representación combinada. Algunos esfuerzos se han realizado sobre cómo utilizar entidades nombradas para mejorar la desambiguación de entidades nombradas (NED). Yang et al. asignaron un peso cuatro veces mayor a las entidades con nombres de ubicación que a otros términos y entidades con nombres [10]. El grupo de investigación DOREMI combinó similitudes semánticas de nombres de personas, nombres de lugares y tiempo junto con similitud textual [11][12]. El grupo de investigación de UMass [13] dividió la representación del documento en dos partes: entidades nombradas y entidades no nombradas. Y se descubrió que algunas clases de noticias podían lograr un mejor rendimiento utilizando la representación de entidades nombradas, mientras que otras clases de noticias podían lograr un mejor rendimiento utilizando la representación sin entidades nombradas. Tanto [10] como [13] utilizaron la técnica de categorización de texto para clasificar previamente las noticias. En [13] las noticias se clasifican automáticamente al principio, y luego se prueban las sensibilidades de los nombres y términos que no son nombres para NED para cada clase. En [10], se eliminan los términos frecuentes de cada clase de la representación del documento. Por ejemplo, la palabra \"elección\" no ayuda a identificar diferentes elecciones. En su trabajo, no se investiga la efectividad de diferentes tipos de nombres (o términos con diferentes categorías gramaticales) para NED en diferentes clases de noticias. Utilizamos análisis estadístico para revelar el hecho y utilizarlo para mejorar el rendimiento de NED. 3. MODELO BÁSICO En esta sección, presentamos el modelo básico de Detección de Nuevos Eventos que es similar a lo que la mayoría de los sistemas actuales aplican. Luego, proponemos nuestro nuevo modelo ampliando el modelo básico. Los sistemas de detección de eventos nuevos utilizan flujos de noticias como entrada, en los cuales las historias están estrictamente ordenadas por tiempo. Solo están disponibles las historias recibidas previamente al tratar con la historia actual. La salida es una decisión sobre si la historia actual es un evento nuevo o no y la confianza de la decisión. Por lo general, un modelo NED consta de tres partes: representación de la historia, cálculo de similitud y procedimiento de detección. 3.1 Representación de la historia Se necesita preprocesamiento antes de generar la representación de la historia. Para el preprocesamiento, tokenizamos palabras, reconocemos abreviaturas, normalizamos abreviaturas, agregamos etiquetas de partes del discurso, eliminamos palabras vacías incluidas en la lista de paradas utilizada en InQuery [14], reemplazamos palabras con sus raíces utilizando el algoritmo K-stem[15], y luego generamos un vector de palabras para cada noticia. Utilizamos el modelo TF-IDF incremental para el cálculo del peso de los términos [4]. En un modelo TF-IDF, la frecuencia de término en un documento de noticias se pondera por la frecuencia inversa del documento, la cual se genera a partir del corpus de entrenamiento. Cuando surge un nuevo término en el proceso de prueba, hay dos soluciones: simplemente ignorar el nuevo término o establecer df del término como una constante pequeña (por ejemplo, df = 1). El nuevo término recibe un peso demasiado bajo en la primera solución (0) y un peso demasiado alto en la segunda solución. En el modelo TF-IDF incremental, las frecuencias de documentos se actualizan dinámicamente en cada paso de tiempo t: 1( ) ( ) ( )t t D tdf w df w df w−= + (1) donde Dt representa el conjunto de noticias recibidas en el tiempo t, y dfDt(w) significa el número de documentos en los que aparece el término w, y dft(w) significa el número total de documentos en los que aparece el término w antes del tiempo t. En este trabajo, cada ventana de tiempo incluye 50 noticias. Así, cada historia d recibida en t se representa de la siguiente manera: 1 2{ ( , , ), ( , , ),..., ( , , )}nd peso d t w peso d t w peso d t w→ donde n significa el número de términos distintos en la historia d, y ( , , )peso d t w significa el peso del término w en la historia d en el tiempo t: log( ( , ) 1) log(( 1) /( ( ) 0.5)) ( , , ) log( ( , ) 1) log(( 1) /( ( ) 0.5)) t t t t w d tf d w N df w peso d t w tf d w N df w ∈ + + + = + + +∑ (2) donde Nt significa el número total de historias de noticias antes del tiempo t, y tf(d,w) significa cuántas veces aparece el término w en la historia de noticias d. 3.2 Cálculo de Similitud Utilizamos la distancia de Hellinger para el cálculo de la similitud entre dos historias, para dos historias d y d en el tiempo t, su similitud se define de la siguiente manera: , ( , , ) ( , , ) * ( , , ) w d d sim d d t peso d t w peso d t w ∈ = ∑ (3) 3.3 Procedimiento de Detección Para cada historia d recibida en el paso de tiempo t, el valor ( ) ( ) ( ) ( ( , , )) tiempo d tiempo d n d max sim d d t < = (4) es un puntaje utilizado para determinar si d es una historia sobre un nuevo tema y al mismo tiempo es una indicación de la confianza en nuestra decisión [8]. tiempo(d) significa el tiempo de publicación de la historia d. Si el puntaje supera el umbral θ new, entonces existe un documento suficientemente similar, por lo tanto, d es una historia antigua, de lo contrario, no hay un documento previo suficientemente similar, por lo tanto, d es una historia nueva. 4. Nuevo procedimiento NED Los sistemas NED tradicionales se pueden clasificar en dos tipos principales en cuanto al procedimiento de detección: (1) tipo S-S, en el que la historia en cuestión se compara con cada historia recibida previamente, y se utiliza la mayor similitud para determinar si la historia actual trata sobre un evento nuevo; (2) tipo S-C, en el que la historia en cuestión se compara con todos los clusters anteriores, cada uno de los cuales representa un tema, y se utiliza la mayor similitud para la decisión final sobre la historia actual. Si la similitud más alta supera el umbral θ nuevo, entonces es una historia antigua y se coloca en el clúster más similar; de lo contrario, es una historia nueva y se crea un nuevo clúster. Trabajos anteriores muestran que la primera forma es más precisa que la segunda [4][5]. Dado que a veces las historias dentro de un tema se alejan mucho entre sí, una historia puede tener una similitud muy baja con su tema. Por lo tanto, utilizar similitudes entre historias para determinar una nueva historia es mejor que utilizar similitudes entre historias y grupos. Sin embargo, el primer método requiere mucho más tiempo de comparación, lo que significa que es menos eficiente. Proponemos un nuevo procedimiento de detección que utiliza comparaciones con clusters anteriores para ayudar a encontrar la historia más similar en menos tiempo de comparación, y la decisión final sobre el nuevo evento se toma de acuerdo con la historia más similar. Por lo tanto, podemos obtener tanto la precisión de los métodos de tipo S-S como la eficiencia de los métodos de tipo S-C. El nuevo procedimiento crea un árbol de indexación de noticias dinámicamente, en el cual se agrupan historias similares para formar una jerarquía de clusters. Indexamos historias similares juntas por su ancestro común (un nodo de clúster). Historias diferentes están indexadas en diferentes grupos. Cuando una historia está llegando, utilizamos comparaciones entre la historia actual y los grupos jerárquicos anteriores para ayudar a encontrar la historia más similar que es útil para la toma de decisiones sobre nuevos eventos. Después de que se tome la decisión sobre el nuevo evento, la historia actual se inserta en el árbol de indexación para la detección siguiente. El índice de noticias en forma de árbol se define formalmente de la siguiente manera: S-Tree = {r, NC, NS, E}, donde r es la raíz de S-Tree, NC es el conjunto de todos los nodos de clúster, NS es el conjunto de todos los nodos de historia, y E es el conjunto de todos los bordes en S-Tree. Definimos un conjunto de restricciones para un S-Árbol: ⅰ. , es un nodo no terminal en el árbolC i i N i∀ ∈ → ⅱ. , es un nodo terminal en el árbolS i i N i∀ ∈ → ⅲ. , el grado de salida de es al menos 2C i i N i∀ ∈ → ⅳ. , se representa como el centroide de sus descendientesC i i iN∀ ∈ → Para una noticia di, el procedimiento de comparación y el procedimiento de inserción basados en el árbol de indexación se definen de la siguiente manera. Un ejemplo se muestra en la Figura 1 y la Figura 2. Figura 1. Procedimiento de comparación Figura 2. Procedimiento de comparación de inserción: Paso 1: comparar di con todos los nodos hijos directos de r y seleccionar λ nodos con las similitudes más altas, por ejemplo, C1 2 y C1 3 en la Figura 1. Paso 2: para cada nodo seleccionado en el paso anterior, por ejemplo. C1 2, compara di con todos sus nodos hijos directos y selecciona λ nodos con las similitudes más altas, por ejemplo. C2 2 y d8. Repetir el paso 2 para todos los nodos no terminales. Paso 3: registrar el nodo terminal con la mayor similitud a di, por ejemplo, s5, y el valor de similitud (0.20). Insertando di en el S-árbol con r como raíz: Encuentra el nodo n que es hijo directo de r en el camino desde r hasta el nodo terminal con la similitud más alta s, por ejemplo. C1 2. Si s es menor que θ init + (h-1)δ, entonces agregar di al árbol como un hijo directo de r. De lo contrario, si n es un nodo terminal, crear un nodo de clúster en lugar de n, y agregar tanto n como di como sus hijos directos; si n es un nodo no terminal, repetir este procedimiento e insertar di en el subárbol con n como raíz de forma recursiva. Aquí h es la longitud entre n y la raíz del árbol S. Cuanto más similares sean las historias en un grupo, mejor representará el grupo a las historias en él. Por lo tanto, no imponemos restricciones en la altura máxima de los árboles y el grado de un nodo. Por lo tanto, no podemos dar la complejidad de este procedimiento basado en árboles de indexación. Pero proporcionaremos el número de veces de comparación necesarias por el nuevo procedimiento en nuestros experimentos en la sección 7.5. En esta sección, se proponen dos métodos de reponderación de términos para mejorar la precisión de NED. En el primer método, se explora una nueva forma de utilizar de manera más efectiva la información de los grupos (temas). El segundo encuentra una mejor manera de utilizar entidades nombradas basadas en la clasificación de noticias. 5.1 Reajuste de términos basado en la distancia de distribución TF-IDF es el modelo más prevalente utilizado en sistemas de recuperación de información. La idea básica es que cuanto menos documentos aparezca un término, más importante es el término en la discriminación de documentos (relevantes o no relevantes para una consulta que contenga el término). Sin embargo, en el dominio de TDT, necesitamos discriminar documentos en función de los temas en lugar de las consultas. Intuitivamente, el uso de vectores de clúster (tema) para comparar con historias de noticias posteriores debería tener un mejor rendimiento que el uso de vectores de historias. Desafortunadamente, los resultados experimentales no respaldan esta intuición [4][5]. Basándonos en la observación de los datos, encontramos que la razón es que un tema de noticias generalmente contiene muchos eventos directa o indirectamente relacionados, y todos ellos tienen sus propios subtemas que suelen ser diferentes entre sí. Tomemos el tema descrito en la sección 1 como ejemplo, eventos como la explosión y el rescate tienen similitudes muy bajas con eventos sobre juicios criminales, por lo tanto, las historias sobre juicios tendrían una baja similitud con el vector de tema construido en base a sus eventos previos. Esta sección se centra en cómo hacer un uso efectivo de la información del tema y al mismo tiempo evitar el problema de la descentralización del contenido. Al principio, clasificamos los términos en 5 clases para ayudar en el análisis de las necesidades del modelo modificado: Clase de términos A: términos que ocurren con frecuencia en todo el corpus, por ejemplo, año y personas. Los términos de esta clase deben recibir pesos bajos porque no ayudan mucho en la discriminación de temas. Clase B de términos: términos que ocurren con frecuencia dentro de una categoría de noticias, por ejemplo, elección, tormenta. Son útiles para distinguir dos historias en diferentes categorías de noticias. Sin embargo, no pueden proporcionar información para determinar si dos historias tratan sobre el mismo tema o temas diferentes. En otras palabras, los términos elección y tormenta no son útiles para diferenciar dos campañas electorales y dos desastres de tormenta. Por lo tanto, los términos de esta clase deberían asignarse pesos más bajos. Clase de término C: términos que ocurren con frecuencia en un tema y raramente en otros temas, por ejemplo, el nombre de un avión accidentado, el nombre de un huracán específico. Las noticias que pertenecen a diferentes temas rara vez tienen términos superpuestos en esta clase. Cuanto más frecuentemente aparezca un término en un tema, más importante es el término para una historia perteneciente al tema, por lo tanto, el término debería tener un peso más alto. Clase de término D: términos que aparecen en un tema de manera exclusiva, pero no con frecuencia. Por ejemplo, el nombre de un bombero que se desempeñó muy bien en una acción de salvamento, que puede aparecer en solo dos o tres historias pero nunca ha aparecido en otros temas. Los términos de este tipo deberían recibir más peso que en el modelo TF-IDF. Sin embargo, dado que no son populares en el tema, no es apropiado asignarles pesos demasiado altos. Clase de término E: términos con baja frecuencia en documentos y que aparecen en diferentes temas. Los términos de esta clase deberían recibir pesos más bajos. Ahora analizamos si el modelo TF-IDF puede asignar pesos adecuados a las cinco clases de términos. Obviamente, los términos de la clase A tienen un peso bajo en el modelo TF-IDF, lo cual es acorde con el requisito descrito anteriormente. En el modelo TF-IDF, los términos de la clase B dependen altamente del número de historias en una clase de noticias. El modelo TF-IDF no puede proporcionar pesos bajos si la historia que contiene el término pertenece a una clase de noticias relativamente pequeña. Para un término de clase C, cuanto más frecuentemente aparezca en un tema, menos peso le da el modelo TFIDF. Esto entra en conflicto directo con el requisito de términos en la clase C. Para los términos de la clase D, el modelo TF-IDF les asigna pesos altos de manera correcta. Pero para los términos de la clase E, el modelo TF-IDF asigna pesos altos a ellos que no son conformes con el requisito de pesos bajos. En resumen, los términos de clase B, C, E no pueden ser ponderados adecuadamente en el modelo TF-IDF. Por lo tanto, proponemos un modelo modificado para resolver este problema. Cuando θ init y θ new están configurados de cerca, asumimos que la mayoría de las historias en un clúster de primer nivel (un nodo hijo directo del nodo raíz) están en el mismo tema. Por lo tanto, utilizamos un clúster de primer nivel para capturar la distribución de términos (df para todos los términos dentro del clúster) dentro del tema de forma dinámica. La divergencia de Kullback-Leibler de la distribución de términos en un clúster de primer nivel y el conjunto completo de historias se utiliza para ajustar los pesos de los términos: ( , , ) * (1 * ( || )) ( , , ) ( , , ) * (1 * ( || )) cw tw cw tw w d D peso d t w KL P P peso d t w peso d t w KL P P γ γ ∈ + = +∑ (5) donde ( ) ( ) ( ) ( ) 1,cw cw c c c c df w df w p y p y N N = = − (6) ( ) ( ) ( ) ( ) 1,t t tw tw t t df w df w p y p y N N = = − (7) donde dfc(w) es el número de documentos que contienen el término w dentro del clúster C, y Nc es el número de documentos en el clúster C, y Nt es el número total de documentos que llegan antes del paso de tiempo t. γ es un parámetro constante, actualmente configurado manualmente en 3. La divergencia de Kullback-Leibler se define de la siguiente manera [17]: La idea básica es que, para una historia en un tema, cuanto más ocurre un término dentro del tema y menos ocurre en otros temas, se le deben asignar pesos más altos. Obviamente, el modelo modificado puede cumplir con todos los requisitos de las cinco clases de términos mencionadas anteriormente. 5.2 Reajuste de Términos Basado en el Tipo de Término y la Clase de Historia. Trabajos anteriores encontraron que algunas clases de historias de noticias podrían lograr mejoras significativas al dar un peso adicional a las entidades nombradas. Pero encontramos que los términos de diferentes tipos deben recibir una cantidad diferente de peso adicional para diferentes clases de noticias. Utilizamos open-NLP1 para reconocer tipos de entidades nombradas y etiquetas de partes del discurso para términos que aparecen en noticias. Los tipos de entidades nombradas incluyen nombre de persona, nombre de organización, nombre de ubicación, fecha, hora, dinero y porcentaje, y se seleccionan cinco partes del discurso: ninguno (NN), verbo (VB), adjetivo (JJ), adverbio (RB) y número cardinal (CD). El análisis estadístico muestra tipos de términos discriminativos a nivel de tema para diferentes clases de historias. Por conveniencia, los tipos de entidad nombrada y etiquetas de parte de la oración se llaman uniformemente tipo de término en las secciones siguientes. Determinar si dos historias tratan sobre el mismo tema es un componente básico para la tarea de NED. Por lo tanto, al principio usamos la estadística χ² para calcular las correlaciones entre términos y temas. Para un término t y un tema T, se deriva una tabla de contingencia: Tabla 1. Una tabla de contingencia de 2×2 con el número de documentos pertenecientes al tema T y no pertenecientes al tema T incluye t A B no incluye t C D. La estadística χ² para un término específico t con respecto al tema T se define como [16]: χ² = ( ) ( ) * ( ) ( ) * ( ) * ( ) * ( ) w T A B C D AD CB A C B D A B C D + + + − + + + + (9). Los temas de noticias para la tarea TDT se clasifican además en 11 reglas de interpretaciones (ROIs) 2. El ROI se puede ver como una clase de historias de nivel superior. El promedio de correlación entre un tipo de término y un ROI de tema se calcula como: 2 avg 2 ( , )( ( , ) )k m m km kT R w P w TP R p w T R P χ χ ∈ ∈ ∑ ∑（ , ）= 1 1 k=1…K, m=1…M (10) donde K es el número de tipos de términos (fijado constantemente en 12 en el artículo). M es el número de nuevas clases (ROIs, establecido en el conjunto 11 del artículo). Pk representa el conjunto de todos los términos de tipo k, y Rm representa el conjunto de todos los temas de la clase m, p(t,T) significa la probabilidad de que t ocurra en el tema T. Debido a limitaciones de espacio, solo se enumeran partes de los tipos de términos (9 tipos de términos) y partes de las clases de noticias (8 clases) en la tabla 2 con los valores promedio de correlación entre ellos. Las estadísticas se derivan de datos etiquetados en el corpus TDT2. (Los resultados en la tabla 2 ya están normalizados para mayor comodidad en la comparación). Las estadísticas en la tabla 2 indican la utilidad de diferentes tipos de términos en la discriminación de temas con respecto a diferentes clases de noticias. Podemos ver que el nombre de la ubicación es el tipo de término más útil para tres clases de noticias: Desastres Naturales, Violencia o Guerra, Finanzas. Y para otras tres categorías: Elecciones, Casos Legales/Criminales, Ciencia y Descubrimiento, el nombre de la persona es el tipo de término más discriminativo. Para Escándalos/Audiencias, la fecha es la información más importante para la discriminación de temas. Además, los casos legales/criminales y los temas financieros tienen una correlación más alta con términos relacionados con el dinero, mientras que la ciencia y el descubrimiento tienen una correlación más alta con términos de porcentaje. Los términos no nominales son más estables para diferentes clases. 1. http://opennlp.sourceforge.net/ 2. http://projects.ldc.upenn.edu/TDT3/Guide/label.html A partir del análisis de la tabla 2, es razonable ajustar el peso de los términos según su tipo de término y la clase de noticias a la que pertenece la historia. Los nuevos pesos de los términos se recalculan de la siguiente manera: ( ) ( ) ( ) ( ) ( , , ) * ( , , ) ( , , ) * clase d D tipo w T clase d D tipo w w d peso d t w peso d t w peso d t w α α ∈ = ∑ (11) donde type(w) representa el tipo del término w, y class(d) representa la clase de la historia d, c kα es el parámetro de recalibración para la clase de noticias c y el tipo de término k. En el trabajo, simplemente utilizamos las estadísticas de la tabla 2 como parámetros de recalibración. Aunque usar las estadísticas directamente puede que no sea la mejor opción, no discutimos cómo obtener automáticamente los mejores parámetros. Intentaremos utilizar técnicas de aprendizaje automático para obtener los mejores parámetros en el trabajo futuro. En el trabajo, utilizamos BoosTexter [20] para clasificar todas las historias en uno de los 11 ROIs. BoosTexter es un programa de aprendizaje automático basado en boosting, que crea una serie de reglas simples para construir un clasificador de datos de texto o atributos-valor. Utilizamos el peso del término generado utilizando el modelo TF-IDF como característica para la clasificación de historias. Entrenamos el modelo con las 12000 historias en inglés evaluadas en TDT2, y clasificamos el resto de las historias en TDT2 y todas las historias en TDT3. Los resultados de clasificación se utilizan para la reponderación de términos en la fórmula (11). Dado que las etiquetas de clase de las historias de tema apagado no se proporcionan en los conjuntos de datos de TDT, no podemos proporcionar la precisión de clasificación aquí. Por lo tanto, no discutimos los efectos de la precisión de la clasificación en el rendimiento de NED en el artículo. 6. CONFIGURACIÓN EXPERIMENTAL 6.1 Conjuntos de datos Utilizamos dos conjuntos de datos de LDC [18], TDT2 y TDT3, para nuestros experimentos. TDT2 contiene noticias desde enero hasta junio de 1998. Contiene alrededor de 54,000 historias de fuentes como ABC, Associated Press, CNN, New York Times, Public Radio International, Voice of America, etc. Solo se consideraron historias en inglés en la colección. TDT3 contiene aproximadamente 31,000 historias en inglés recopiladas de octubre a diciembre de 1998. Además de las fuentes utilizadas en TDT2, también contiene historias de las transmisiones de televisión de NBC y MSNBC. Utilizamos versiones transcritas de las emisiones de televisión y radio además de noticias textuales. El conjunto de datos TDT2 está etiquetado con alrededor de 100 temas, y aproximadamente 12,000 historias en inglés pertenecen al menos a uno de estos temas. El conjunto de datos TDT3 está etiquetado con alrededor de 120 temas, y aproximadamente 8000 historias en inglés pertenecen al menos a uno de estos temas. Todos los temas están clasificados en 11 Reglas de Interpretación: (1) Elecciones, (2) Escándalos/Audiencias, (3) Casos Legales/Criminales, (4) Desastres Naturales, (5) Accidentes, (6) Violencia o Guerra en Curso, (7) Noticias de Ciencia y Descubrimientos, (8) Finanzas, (9) Nueva Ley, (10) Noticias Deportivas, (11) Varios. Noticias. 6.2 Métrica de evaluación TDT utiliza una función de costo CDet que combina las probabilidades de perder una nueva historia y una falsa alarma [19]: * * * *Det Miss Miss Target FA FA NontargetC C P P C P P= + (12) Tabla 2. Correlación promedio entre tipos de términos y clases de noticias donde CMiss significa el costo de perder una nueva historia, PMiss significa la probabilidad de perder una nueva historia, y PTarget significa la probabilidad de ver una nueva historia en los datos; CFA significa el costo de una falsa alarma, PFA significa la probabilidad de una falsa alarma, y PNontarget significa la probabilidad de ver una historia antigua. El costo CDet se normaliza de tal manera que un sistema perfecto obtiene una puntuación de 0 y un sistema trivial, que es el mejor de marcar todas las historias como nuevas o antiguas, obtiene una puntuación de 1: ( ( * , * ) ) Det Det Miss Target FA Nontarget C Norm C min C P C P = (13) El sistema de detección de eventos nuevos proporciona dos salidas para cada historia. La primera parte es sí o no, indicando si la historia desencadena un nuevo evento o no. La segunda parte es una puntuación que indica la confianza de la primera decisión. Los puntajes de confianza se pueden utilizar para trazar la curva DET, es decir, curvas que representan la probabilidad de falsa alarma frente a la probabilidad de omisión. El costo normalizado mínimo se puede determinar si se elige el umbral óptimo en la puntuación. RESULTADOS EXPERIMENTALES 7.1 Resultados principales Para probar los enfoques propuestos en el modelo, implementamos y probamos cinco sistemas: Sistema-1: este sistema se utiliza como referencia. Se implementa basándose en el modelo básico descrito en la sección 3, es decir, utilizando el modelo TF-IDF incremental para generar pesos de términos y utilizando la distancia de Hellinger para calcular la similitud entre documentos. La normalización del puntaje de similitud también se emplea [8]. Se utiliza el procedimiento de detección S-S. Sistema-2: este sistema es igual que el sistema-1 excepto que se utiliza el procedimiento de detección S-C. Sistema-3: este sistema es igual que el sistema-1 excepto que utiliza el nuevo procedimiento de detección que se basa en un árbol de indexación. Sistema-4: implementado basado en el enfoque presentado en la sección 5.1, es decir, los términos se vuelven a ponderar según la distancia entre las distribuciones de términos en un grupo y todas las historias. Se utiliza el nuevo procedimiento de detección. Sistema-5: implementado basado en el enfoque presentado en la sección 5.2, es decir, los términos de diferentes tipos son reponderados según la clase de noticias utilizando parámetros entrenados. El nuevo procedimiento de detección se utiliza. Los siguientes son algunos otros sistemas NED: Sistema-6: [21] para cada par de historias, calcula tres valores de similitud para entidad nombrada, entidad no nombrada y todos los términos respectivamente. Y emplear Máquina de Vectores de Soporte para predecir nuevo o antiguo utilizando los valores de similitud como características. Sistema-7: [8] extendió un modelo básico incremental de TF-IDF para incluir modelos específicos de origen, normalización de puntuaciones de similitud basada en promedios específicos de documentos, normalización de puntuaciones de similitud basada en promedios específicos de pares de fuentes, etc. Sistema-8: [13] dividió la representación del documento en dos partes: entidades nombradas y entidades no nombradas, y eligió una parte efectiva para cada clase de noticias. La tabla 3 y la tabla 4 muestran los costos normalizados ponderados por tema y los tiempos de comparación en los conjuntos de datos TDT2 y TDT3 respectivamente. Dado que no se contaba con un conjunto de datos de validación para ajustar el umbral θ nuevo en los experimentos en TDT2, solo informamos los costos normalizados mínimos para nuestros sistemas en la tabla 3. El sistema-5 supera a todos los demás sistemas, incluido el sistema-6, y realiza solo 2.78e+8 veces la comparación de tiempos en el procedimiento de detección, lo que representa solo el 13.4% del sistema-1. Tabla 3. Los resultados de NED en los sistemas TDT2 Min Norm(CDet) tiempos de comparación son: Sistema-1 0.5749 2.08e+9, Sistema-2 0.6673 3.77e+8, Sistema-3 0.5765 2.81e+8, Sistema-4 0.5431 2.99e+8, Sistema-5 0.5089 2.78e+8, Sistema-6 0.5300. Al evaluar en los costos normalizados en TDT3, utilizamos los umbrales óptimos obtenidos del conjunto de datos de TDT2 para todos los sistemas. El sistema-2 reduce los tiempos de comparación a 1.29e+9, que es solo el 18.3% del sistema-1, pero al mismo tiempo también obtiene un costo normalizado mínimo deteriorado que es 0.0499 más alto que el sistema-1. El Sistema-3 utiliza el nuevo procedimiento de detección basado en el índice de noticias en árbol. Requiere incluso menos tiempo de comparación que el sistema-2. Esto se debe a que las comparaciones de historias suelen arrojar mayores similitudes que las de grupos de historias, por lo que las historias tienden a combinarse en el sistema-3. Y el sistema-3 es básicamente equivalente al sistema-1 en los resultados de precisión. El System-4 ajusta los pesos de los términos basándose en la distancia de las distribuciones de términos entre todo el corpus y el conjunto de historias del clúster, lo que resulta en una mejora significativa de 0.0468 en comparación con el system-1. El mejor sistema (sistema-5) tiene un costo normalizado mínimo de 0.5012, lo cual es 0.0797 mejor que el sistema-1, y también mejor que cualquier otro resultado previamente reportado para este conjunto de datos [8, 13]. Además, el sistema-5 solo necesita 1.05e+8 veces de comparación, lo que equivale al 14.9% del sistema-1. Tabla 4. Los resultados de NED en los sistemas TDT3 Norm(CDet) Norma mínima(CDet) Tiempos de comparación Sistema-1 0.6159 0.5809 7.04e+8 Sistema-2① 0.6493 0.6308 1.29e+8 Sistema-3② 0.6197 0.5868 1.03e+8 Sistema-4② 0.5601 0.5341 1.03e+8 Sistema-5② 0.5413 0.5012 1.05e+8 Sistema-7 -- 0.5783 -Sistema-8 -- 0.5229 -① θ nuevo=0.13 ② θ inicial=0.13, λ =3, δ =0.15 La Figura 5 muestra las cinco curvas DET para nuestros sistemas en el conjunto de datos TDT3. El Sistema-5 logra el costo mínimo con una tasa de falsas alarmas de 0.0157 y una tasa de omisión de 0.4310. Podemos observar que System4 y System-5 obtienen una menor probabilidad de error de omisión en regiones de baja probabilidad de falsa alarma. La hipótesis es que se transfiere más valor de peso a los términos clave de los temas que a los términos no clave. La puntuación de similitud entre dos historias pertenecientes a diferentes temas es más baja que antes, porque los términos superpuestos suelen no ser términos clave de sus temas. 7.2 Selección de parámetros para la detección de árboles de indexación. La Figura 3 muestra los costos normalizados mínimos obtenidos por el sistema-3 en TDT3 utilizando diferentes parámetros. El parámetro de inicio θ se prueba en seis valores que van desde 0.03 hasta 0.18. Y el parámetro λ se prueba en cuatro valores: 1, 2, 3 y 4. Podemos ver que, cuando θ inicial se establece en 0.12, que es el más cercano a θ nuevo, los costos son más bajos que los demás. Esto es fácil de explicar, porque cuando se agrupan historias pertenecientes al mismo tema en un conjunto, es más razonable que el conjunto represente las historias en él. Cuando el parámetro λ se establece en 3 o 4, los costos son mejores que en otros casos, pero no hay mucha diferencia entre 3 y 4. 0 0.05 0.1 0.15 0.2 1 2 3 4 0.5 0.6 0.7 0.8 0.9 1 θ-initλ MinCost 0.6 0.65 0.7 0.75 0.8 0.85 0.9 Figura 3. Costo mínimo en TDT3 (δ =0.15) 0 0.05 0.1 0.15 0.2 1 2 3 4 0 0.5 1 1.5 2 2.5 x 10 8 θ-inicio λ Comparando tiempos 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 8 Figura 4. Comparando los tiempos en TDT3 (δ =0.15), la Figura 4 muestra los tiempos de comparación utilizados por el sistema-3 en TDT3 con los mismos parámetros que la Figura 3. Los tiempos de comparación dependen fuertemente de θ init. Debido a que cuanto mayor sea θ init, menos historias se combinan juntas, más veces de comparación se necesitan para la decisión de un nuevo evento. Por lo tanto, usamos θ init = 0.13, λ = 3, δ = 0.15 para los sistemas 3, 4 y 5. En esta configuración de parámetros, podemos obtener tanto costos normalizados mínimos bajos como menos tiempos de comparación. CONCLUSIÓN Hemos propuesto un procedimiento de detección basado en un árbol de indexación de noticias en nuestro modelo. Reduce los tiempos de comparación a aproximadamente una séptima parte del método tradicional sin afectar la precisión de NED. También hemos presentado dos extensiones al modelo básico TF-IDF. La primera extensión se realiza ajustando los pesos de los términos basados en las distribuciones de términos entre todo el corpus y un conjunto de historias de un clúster. Y la segunda extensión al modelo básico TF-IDF es un mejor uso de los tipos de términos (tipos de entidades nombradas y partes de la velocidad) de acuerdo con las categorías de noticias. Nuestros resultados experimentales en los conjuntos de datos TDT2 y TDT3 muestran que ambas extensiones contribuyen significativamente a la mejora en la precisión. No consideramos la información de tiempo de noticias como una pista para la tarea de NED, ya que la mayoría de los temas duran mucho tiempo y los conjuntos de datos de TDT solo abarcan un período relativamente corto (no más de 6 meses). Para el trabajo futuro, queremos recopilar un conjunto de noticias que abarque un período más largo de internet e integrar información temporal en la tarea de NED. Dado que el tema es un grupo de noticias relativamente grueso, también queremos refinar la granularidad del grupo a nivel de evento e identificar diferentes eventos y sus relaciones dentro de un tema. Agradecimientos: Este trabajo ha sido apoyado por la Fundación Nacional de Ciencias Naturales de China bajo la subvención No. 90604025. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son del autor(es) y no necesariamente reflejan las del patrocinador. REFERENCIAS [1] http://www.nist.gov/speech/tests/tdt/index.htm [2] En Detección y Seguimiento de Temas. Organización de la información basada en eventos. Kluwer Academic Publishers, 2002. .01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90 1 2 5 10 20 40 60 80 90 Probabilidad de Falsa Alarma (en %) Probabilidad de Omisión (en %) Curva Ponderada por Tema del SISTEMA1 Norma Mínima (Costo) del SISTEMA1 Curva Ponderada por Tema del SISTEMA2 Norma Mínima (Costo) del SISTEMA2 Curva Ponderada por Tema del SISTEMA3 Norma Mínima (Costo) del SISTEMA3 Curva Ponderada por Tema del SISTEMA4 Norma Mínima (Costo) del SISTEMA4 Curva Ponderada por Tema del SISTEMA5 Norma Mínima (Costo) del SISTEMA5 Rendimiento Aleatorio Figura 5. Curvas DET en TDT3 [3] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B.T. Archibald y X. Liu. Enfoques de aprendizaje para detectar y rastrear eventos de noticias. En el número especial de IEEE Intelligent Systems sobre Aplicaciones de Recuperación de Información Inteligente, volumen 14 (4), 1999, páginas 32-43. [4] Y. Yang, T. Pierce y J. Carbonell. Un estudio sobre la detección de eventos retrospectivos y en línea. En Actas de SIGIR-98, Melbourne, Australia, 1998, 28-36. [5] J. Allan, V. Lavrenko, D. Malin y R. Swan. Detecciones, límites y líneas de tiempo: Umass y tdt-3. En Actas del Taller de Detección y Seguimiento de Temas (TDT-3), Viena, VA, 2000, 167-174. [6] R. Papka y J. Allan. Detección de nuevos eventos en línea utilizando agrupamiento de un solo paso TÍTULO2:. Informe técnico UM-CS1998-021, 1998. [7] W. Lam, H. Meng, K. Wong y J. I'm sorry, but \"Yen\" is not a sentence. Can you please provide a sentence for me to translate into Spanish? Utilizando el Análisis Contextual para la Detección de Eventos de Noticias. Revista Internacional de Sistemas Inteligentes, 2001, 525-546. [8] B. Thorsten, C. Francine y F. Ayman. Un sistema para la detección de nuevos eventos. En Actas de la 26ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2003, 330-337. [9] S. Nicola y C. Joe. Combinando clasificadores semánticos y sintácticos de documentos para mejorar la detección de primeras noticias. En Actas de la 24ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell, and C. Jin. \n\nACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell y C. Jin. Detección de novedades condicionada al tema. En Actas de la 8ª Conferencia Internacional de ACM SIGKDD, ACM Press. 2002, 688-693. [11] M. Juha, A.M. Helena y S. Marko. Aplicando Clases Semánticas en la Detección y Seguimiento de Eventos. En Actas de la Conferencia Internacional sobre Procesamiento del Lenguaje Natural (ICON 2002), 2002, páginas 175-183. [12] M. Juha, A.M. Helena y S. Marko. Semántica simple en la detección y seguimiento de temas. Recuperación de información, 7(3-4): 2004, 347-368. [13] K. Giridhar y J. Allan. Clasificación de texto y entidades nombradas para la detección de nuevos eventos. En Actas de la 27ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2004, 297-304. [14] J. P. Callan, W. B. Croft, y S. M. Harding. El Sistema de Recuperación INQUERY. En Actas de DEXA-92, 3ra Conferencia Internacional sobre Aplicaciones de Bases de Datos y Sistemas Expertos, 1992, 78-83. [15] R. Krovetz. Viendo la morfología como un proceso de inferencia. En Actas de ACM SIGIR93, 1993, 61-81. [16] Y. Yang y J. Pedersen. Un estudio comparativo sobre la selección de características en la categorización de textos. En J. D. H. Fisher, editor, La Decimocuarta Conferencia Internacional sobre Aprendizaje Automático (ICML97), Morgan Kaufmann, 1997, 412-420. [17] T. M. Cover y J. A. I'm sorry, but \"Thomas\" is not a sentence. Can you provide more context or a complete sentence for me to translate into Spanish? Elementos de la teoría de la información. Wiley. 1991. [18] El consorcio de datos lingüísticos, http://www.ldc,upenn.edu/. [19] La definición de la tarea TDT 2001 y el plan de evaluación, http://www.nist.gov/speech/tests/tdt/tdt2001/evalplan.htm. [20] R. E. Schapire y Y. Cantante. Boostexter: Un sistema basado en Boosting para la categorización de texto. En Machine Learning 39(2/3):1, Kluwer Academic Publishers, 2000, 35-168. [21] K. Giridhar y J. Allan. 2005. Utilizando nombres y temas para la detección de nuevos eventos. En Actas de la Conferencia de Tecnología Humana y la Conferencia sobre Métodos Empíricos en Lenguaje Natural, Vancouver, 2005, 121-128 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "news indexing-tree": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "New Event Detection Based on Indexing-tree and Named Entity Zhang Kuo Tsinghua University Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Tsinghua University Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Tsinghua University Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn ABSTRACT New Event Detection (NED) aims at detecting from one or multiple streams of news stories that which one is reported on a new event (i.e.",
                "not reported previously).",
                "With the overwhelming volume of news available today, there is an increasing need for a NED system which is able to detect new events more efficiently and accurately.",
                "In this paper we propose a new NED model to speed up the NED task by using <br>news indexing-tree</br> dynamically.",
                "Moreover, based on the observation that terms of different types have different effects for NED task, two term reweighting approaches are proposed to improve NED accuracy.",
                "In the first approach, we propose to adjust term weights dynamically based on previous story clusters and in the second approach, we propose to employ statistics on training data to learn the named entity reweighting model for each class of stories.",
                "Experimental results on two Linguistic Data Consortium (LDC) datasets TDT2 and TDT3 show that the proposed model can improve both efficiency and accuracy of NED task significantly, compared to the baseline system and other existing systems.",
                "Categories and Subject Descriptors H.3.3 [Information Systems]: Information Search and Retrieval; H.4.2 [Information Systems Applications]: Types of Systemsdecision support.",
                "General Terms Algorithms, Performance, Experimentation 1.",
                "INTRODUCTION Topic Detection and Tracking (TDT) program aims to develop techniques which can effectively organize, search and structure news text materials from a variety of newswire and broadcast media [1].",
                "New Event Detection (NED) is one of the five tasks in TDT.",
                "It is the task of online identification of the earliest report for each topic as soon as that report arrives in the sequence of documents.",
                "A Topic is defined as a seminal event or activity, along with directly related events and activities [2].",
                "An Event is defined as something (non-trivial) happening in a certain place at a certain time [3].",
                "For instance, when a bomb explodes in a building, the exploding is the seminal event that triggers the topic, and other stories on the same topic would be those discussing salvaging efforts, the search for perpetrators, arrests and trial and so on.",
                "Useful news information is usually buried in a mass of data generated everyday.",
                "Therefore, NED systems are very useful for people who need to detect novel information from real-time news stream.",
                "These real-life needs often occur in domains like financial markets, news analysis, and intelligence gathering.",
                "In most of state-of-the-art (currently) NED systems, each news story on hand is compared to all the previous received stories.",
                "If all the similarities between them do not exceed a threshold, then the story triggers a new event.",
                "They are usually in the form of cosine similarity or Hellinger similarity metric.",
                "The core problem of NED is to identify whether two stories are on the same topic.",
                "Obviously, these systems cannot take advantage of topic information.",
                "Further more, it is not acceptable in real applications because of the large amount of computation required in the NED process.",
                "Other systems organize previous stories into clusters (each cluster corresponds to a topic), and new story is compared to the previous clusters instead of stories.",
                "This manner can reduce comparing times significantly.",
                "Nevertheless, it has been proved that this manner is less accurate [4, 5].",
                "This is because sometimes stories within a topic drift far away from each other, which could lead low similarity between a story and its topic.",
                "On the other hand, some proposed NED systems tried to improve accuracy by making better use of named entities [10, 11, 12, 13].",
                "However, none of the systems have considered that terms of different types (e.g.",
                "Noun, Verb or Person name) have different effects for different classes of stories in determining whether two stories are on the same topic.",
                "For example, the names of election candidates (Person name) are very important for stories of election class; the locations (Location name) where accidents happened are important for stories of accidents class.",
                "So, in NED, there still exist following three problems to be investigated: (1) How to speed up the detection procedure while do not decrease the detection accuracy? (2) How to make good use of cluster (topic) information to improve accuracy? (3) How to obtain better news story representation by better understanding of named entities.",
                "Driven by these problems, we have proposed three approaches in this paper. (1)To make the detection procedure faster, we propose a new NED procedure based on <br>news indexing-tree</br> created dynamically.",
                "Story indexing-tree is created by assembling similar stories together to form news clusters in different hierarchies according to their values of similarity.",
                "Comparisons between current story and previous clusters could help find the most similar story in less comparing times.",
                "The new procedure can reduce the amount of comparing times without hurting accuracy. (2)We use the clusters of the first floor in the indexing-tree as news topics, in which term weights are adjusted dynamically according to term distribution in the clusters.",
                "In this approach, cluster (topic) information is used properly, so the problem of theme decentralization is avoided. (3)Based on observations on the statistics obtained from training data, we found that terms of different types (e.g.",
                "Noun and Verb) have different effects for different classes of stories in determining whether two stories are on the same topic.",
                "And we propose to use statistics to optimize the weights of the terms of different types in a story according to the news class that the story belongs to.",
                "On TDT3 dataset, the new NED model just uses 14.9% comparing times of the basic model, while its minimum normalized cost is 0.5012, which is 0.0797 better than the basic model, and also better than any other results previously reported for this dataset [8, 13].",
                "The rest of the paper is organized as follows.",
                "We start off this paper by summarizing the previous work in NED in section 2.",
                "Section 3 presents the basic model for NED that most current systems use.",
                "Section 4 describes our new detection procedure based on <br>news indexing-tree</br>.",
                "In section 5, two term reweighting methods are proposed to improve NED accuracy.",
                "Section 6 gives our experimental data and evaluation metrics.",
                "We finally wrap up with the experimental results in Section 7, and the conclusions and future work in Section 8. 2.",
                "RELATED WORK Papka et al. proposed Single-Pass clustering on NED [6].",
                "When a new story was encountered, it was processed immediately to extract term features and a query representation of the storys content is built up.",
                "Then it was compared with all the previous queries.",
                "If the document did not trigger any queries by exceeding a threshold, it was marked as a new event.",
                "Lam et al build up previous query representations of story clusters, each of which corresponds to a topic [7].",
                "In this manner comparisons happen between stories and clusters.",
                "Recent years, most work focus on proposing better methods on comparison of stories and document representation.",
                "Brants et al. [8] extended a basic incremental TF-IDF model to include sourcespecific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, term reweighting based on inverse event frequencies, and segmentation of documents.",
                "Good improvements on TDT bench-marks were shown.",
                "Stokes et al. [9] utilized a combination of evidence from two distinct representations of a documents content.",
                "One of the representations was the usual free text vector, the other made use of lexical chains (created using WordNet) to build another term vector.",
                "Then the two representations are combined in a linear fashion.",
                "A marginal increase in effectiveness was achieved when the combined representation was used.",
                "Some efforts have been done on how to utilize named entities to improve NED.",
                "Yang et al. gave location named entities four times weight than other terms and named entities [10].",
                "DOREMI research group combined semantic similarities of person names, location names and time together with textual similarity [11][12].",
                "UMass [13] research group split document representation into two parts: named entities and non-named entities.",
                "And it was found that some classes of news could achieve better performance using named entity representation, while some other classes of news could achieve better performance using non-named entity representation.",
                "Both [10] and [13] used text categorization technique to classify news stories in advance.",
                "In [13] news stories are classified automatically at first, and then test sensitivities of names and non-name terms for NED for each class.",
                "In [10] frequent terms for each class are removed from document representation.",
                "For example, word election does not help identify different elections.",
                "In their work, effectiveness of different kinds of names (or terms with different POS) for NED in different news classes are not investigated.",
                "We use statistical analysis to reveal the fact and use it to improve NED performance. 3.",
                "BASIC MODEL In this section, we present the basic New Event Detection model which is similar to what most current systems apply.",
                "Then, we propose our new model by extending the basic model.",
                "New Event Detection systems use news story stream as input, in which stories are strictly time-ordered.",
                "Only previously received stories are available when dealing with current story.",
                "The output is a decision for whether the current story is on a new event or not and the confidence of the decision.",
                "Usually, a NED model consists of three parts: story representation, similarity calculation and detection procedure. 3.1 Story Representation Preprocessing is needed before generating story representation.",
                "For preprocessing, we tokenize words, recognize abbreviations, normalize abbreviations, add part-of-speech tags, remove stopwords included in the stop list used in InQuery [14], replace words with their stems using K-stem algorithm[15], and then generate word vector for each news story.",
                "We use incremental TF-IDF model for term weight calculation [4].",
                "In a TF-IDF model, term frequency in a news document is weighted by the inverse document frequency, which is generated from training corpus.",
                "When a new term occurs in testing process, there are two solutions: simply ignore the new term or set df of the term as a small const (e.g. df = 1).",
                "The new term receives too low weight in the first solution (0) and too high weight in the second solution.",
                "In incremental TF-IDF model, document frequencies are updated dynamically in each time step t: 1( ) ( ) ( )t t D tdf w df w df w−= + (1) where Dt represents news story set received in time t, and dfDt(w) means the number of documents that term w occurs in, and dft(w) means the total number of documents that term w occurs in before time t. In this work, each time window includes 50 news stories.",
                "Thus, each story d received in t is represented as follows: 1 2{ ( , , ), ( , , ),..., ( , , )}nd weight d t w weight d t w weight d t w→ where n means the number of distinct terms in story d, and ( , , )weight d t w means the weight of term w in story d at time t: log( ( , ) 1) log(( 1) /( ( ) 0.5)) ( , , ) log( ( , ) 1) log(( 1) /( ( ) 0.5)) t t t t w d tf d w N df w weight d t w tf d w N df w ∈ + + + = + + +∑ (2) where Nt means the total number of news stories before time t, and tf(d,w) means how many times term w occurs in news story d. 3.2 Similarity Calculation We use Hellinger distance for the calculation of similarity between two stories, for two stories d and d at time t, their similarity is defined as follows: , ( , , ) ( , , ) * ( , , ) w d d sim d d t weight d t w weight d t w ∈ = ∑ (3) 3.3 Detection Procedure For each story d received in time step t, the value ( ) ( ) ( ) ( ( , , )) time d time d n d max sim d d t < = (4) is a score used to determine whether d is a story about a new topic and at the same time is an indication of the confidence in our decision [8]. time(d) means the publication time of story d. If the score exceeds the thresholdθ new, then there exists a sufficiently similar document, thus d is a old story, otherwise, there is no sufficiently similar previous document, thus d is an new story. 4.",
                "New NED Procedure Traditional NED systems can be classified into two main types on the aspect of detection procedure: (1) S-S type, in which the story on hand is compared to each story received previously, and use the highest similarity to determine whether current story is about a new event; (2) S-C type, in which the story on hand is compared to all previous clusters each of which representing a topic, and the highest similarity is used for final decision for current story.",
                "If the highest similarity exceeds thresholdθ new, then it is an old story, and put it into the most similar cluster; otherwise it is a new story and create a new cluster.",
                "Previous work show that the first manner is more accurate than the second one [4][5].",
                "Since sometimes stories within a topic drift far away from each other, a story may have very low similarity with its topic.",
                "So using similarities between stories for determining new story is better than using similarities between story and clusters.",
                "Nevertheless, the first manner needs much more comparing times which means the first manner is low efficient.",
                "We propose a new detection procedure which uses comparisons with previous clusters to help find the most similar story in less comparing times, and the final new event decision is made according to the most similar story.",
                "Therefore, we can get both the accuracy of S-S type methods and the efficiency of S-C type methods.",
                "The new procedure creates a <br>news indexing-tree</br> dynamically, in which similar stories are put together to form a hierarchy of clusters.",
                "We index similar stories together by their common ancestor (a cluster node).",
                "Dissimilar stories are indexed in different clusters.",
                "When a story is coming, we use comparisons between the current story and previous hierarchical clusters to help find the most similar story which is useful for new event decision.",
                "After the new event decision is made, the current story is inserted to the indexing-tree for the following detection.",
                "The <br>news indexing-tree</br> is defined formally as follows: S-Tree = {r, NC , NS , E} where r is the root of S-Tree, NC is the set of all cluster nodes, NS is the set of all story nodes, and E is the set of all edges in S-Tree.",
                "We define a set of constraints for a S-Tree: ⅰ . , is an non-terminal node in the treeC i i N i∀ ∈ → ⅱ . , is a terminal node in the treeS i i N i∀ ∈ → ⅲ . , out degree of is at least 2C i i N i∀ ∈ → ⅳ . , is represented as the centroid of its desendantsC i i iN∀ ∈ → For a news story di, the comparison procedure and inserting procedure based on indexing-tree are defined as follows.",
                "An example is shown by Figure 1 and Figure 2.",
                "Figure 1.",
                "Comparison procedure Figure 2.",
                "Inserting procedure Comparison procedure: Step 1: compare di to all the direct child nodes of r and select λ nodes with highest similarities, e.g., C1 2 and C1 3 in Figure 1.",
                "Step 2: for each selected node in the last step, e.g.",
                "C1 2, compare di to all its direct child nodes, and select λ nodes with highest similarities, e.g.",
                "C2 2 and d8.",
                "Repeat step 2 for all non-terminal nodes.",
                "Step 3: record the terminal node with the highest similarty to di, e.g. s5, and the similarity value (0.20).",
                "Inserting di to the S-tree with r as root: Find the node n which is direct child of r in the path from r to the terminal node with highest similarity s, e.g.",
                "C1 2.",
                "If s is smaller than θ init+(h-1)δ , then add di to the tree as a direct child of r. Otherwise, if n is a terminal node, then create a cluster node instead of n, and add both n and di as its direct children; if n is an non-terminal node, then repeat this procedure and insert di to the sub-tree with n as root recursively.",
                "Here h is the length between n and the root of S-tree.",
                "The more the stories in a cluster similar to each other, the better the cluster represents the stories in it.",
                "Hence we add no constraints on the maximum of trees height and degree of a node.",
                "Therefore, we cannot give the complexity of this indexing-tree based procedure.",
                "But we will give the number of comparing times needed by the new procedure in our experiments in section7. 5.",
                "Term Reweighting Methods In this section, two term reweighting methods are proposed to improve NED accuracy.",
                "In the first method, a new way is explored for better using of cluster (topic) information.",
                "The second one finds a better way to make use of named entities based on news classification. 5.1 Term Reweighting Based on Distribution Distance TF-IDF is the most prevalent model used in information retrieval systems.",
                "The basic idea is that the fewer documents a term appears in, the more important the term is in discrimination of documents (relevant or not relevant to a query containing the term).",
                "Nevertheless, in TDT domain, we need to discriminate documents with regard to topics rather than queries.",
                "Intuitively, using cluster (topic) vectors to compare with subsequent news stories should outperform using story vectors.",
                "Unfortunately, the experimental results do not support this intuition [4][5].",
                "Based on observation on data, we find the reason is that a news topic usually contains many directly or indirectly related events, while they all have their own sub-subjects which are usually different with each other.",
                "Take the topic described in section 1 as an example, events like the explosion and salvage have very low similarities with events about criminal trial, therefore stories about trial would have low similarity with the topic vector built on its previous events.",
                "This section focuses on how to effectively make use of topic information and at the same time avoid the problem of content decentralization.",
                "At first, we classify terms into 5 classes to help analysis the needs of the modified model: Term class A: terms that occur frequently in the whole corpus, e.g., year and people.",
                "Terms of this class should be given low weights because they do not help much for topic discrimination.",
                "Term class B: terms that occur frequently within a news category, e.g., election, storm.",
                "They are useful to distinguish two stories in different news categories.",
                "However, they cannot provide information to determine whether two stories are on the same or different topics.",
                "In another words, term election and term storm are not helpful in differentiate two election campaigns and two storm disasters.",
                "Therefore, terms of this class should be assigned lower weights.",
                "Term class C: terms that occur frequently in a topic, and infrequently in other topics, e.g., the name of a crash plane, the name of a specific hurricane.",
                "News stories that belong to different topics rarely have overlap terms in this class.",
                "The more frequently a term appears in a topic, the more important the term is for a story belonging to the topic, therefore the term should be set higher weight.",
                "Term class D: terms that appear in a topic exclusively, but not frequently.",
                "For example, the name of a fireman who did very well in a salvage action, which may appears in only two or three stories but never appeared in other topics.",
                "Terms of this type should receive more weights than in TF-IDF model.",
                "However, since they are not popular in the topic, it is not appropriate to give them too high weights.",
                "Term class E: terms with low document frequency, and appear in different topics.",
                "Terms of this class should receive lower weights.",
                "Now we analyze whether TF-IDF model can give proper weights to the five classes of terms.",
                "Obviously, terms of class A are lowly weighted in TF-IDF model, which is conformable with the requirement described above.",
                "In TF-IDF model, terms of class B are highly dependant with the number of stories in a news class.",
                "TF-IDF model cannot provide low weights if the story containing the term belongs to a relative small news class.",
                "For a term of class C, the more frequently it appears in a topic, the less weight TFIDF model gives to it.",
                "This strongly conflicts with the requirement of terms in class C. For terms of class D, TF-IDF model gives them high weights correctly.",
                "But for terms of class E, TF-IDF model gives high weights to them which are not conformable with the requirement of low weights.",
                "To sum up, terms of class B, C, E cannot be properly weighted in TF-IDF model.",
                "So, we propose a modified model to resolve this problem.",
                "When θ init andθ new are set closely, we assume that most of the stories in a first-level cluster (a direct child node of root node) are on the same topic.",
                "Therefore, we make use of a first-level cluster to capture term distribution (df for all the terms within the cluster) within the topic dynamically.",
                "KL divergence of term distribution in a first-level cluster and the whole story set is used to adjust term weights:  ( , , ) * (1 * ( || )) ( , , ) ( , , ) * (1 * ( || )) cw tw cw tw w d D weight d t w KL P P weight d t w weight d t w KL P P γ γ ∈ + = +∑ (5) where ( ) ( ) ( ) ( ) 1,cw cw c c c c df w df w p y p y N N = = − (6) ( ) ( ) ( ) ( ) 1,t t tw tw t t df w df w p y p y N N = = − (7) where dfc(w) is the number of documents containing term w within cluster C, and Nc is the number of documents in cluster C, and Nt is the total number of documents that arrive before time step t. γ is a const parameter, now is manually set 3.",
                "KL divergence is defined as follows [17]: ( ) ( || ) ( ) log ( )x p x KL P Q p x q x = ∑ (8) The basic idea is: for a story in a topic, the more a term occurs within the topic, and the less it occurs in other topics, it should be assigned higher weights.",
                "Obviously, modified model can meet all the requirements of the five term classes listed above. 5.2 Term Reweighting Based on Term Type and Story Class Previous work found that some classes of news stories could achieve good improvements by giving extra weight to named entities.",
                "But we find that terms of different types should be given different amount of extra weight for different classes of news stories.",
                "We use open-NLP1 to recognize named entity types and part-ofspeech tags for terms that appear in news stories.",
                "Named entity types include person name, organization name, location name, date, time, money and percentage, and five POSs are selected: none (NN), verb (VB), adjective (JJ), adverb (RB) and cardinal number (CD).",
                "Statistical analysis shows topic-level discriminative terms types for different classes of stories.",
                "For the sake of convenience, named entity type and part-of-speech tags are uniformly called term type in subsequent sections.",
                "Determining whether two stories are about the same topic is a basic component for NED task.",
                "So at first we use 2 χ statistic to compute correlations between terms and topics.",
                "For a term t and a topic T, a contingence table is derived: Table 1.",
                "A 2×2 Contingence Table Doc Number belong to topic T not belong to topic T include t A B not include t C D The 2 χ statistic for a specific term t with respect to topic T is defined to be [16]: 2 2 ( , ) ( ) * ( ) ( ) * ( ) * ( ) * ( ) w T A B C D AD CB A C B D A B C D χ = + + + − + + + + (9) News topics for the TDT task are further classified into 11 rules of interpretations (ROIs) 2 .",
                "The ROI can be seen as a higher level class of stories.",
                "The average correlation between a term type and a topic ROI is computed as: 2 avg 2 ( , )( ( , ) )k m m km kT R w P w TP R p w T R P χ χ ∈ ∈ ∑ ∑（ , ）= 1 1 k=1…K, m=1…M (10) where K is the number of term types (set 12 constantly in the paper).",
                "M is the number news classes (ROIs, set 11 in the paper).",
                "Pk represents the set of all terms of type k, and Rm represents the set of all topics of class m, p(t,T) means the probability that t occurs in topic T. Because of limitation of space, only parts of the term types (9 term types) and parts of news classes (8 classes) are listed in table 2 with the average correlation values between them.",
                "The statistics is derived from labeled data in TDT2 corpus. (Results in table 2 are already normalized for convenience in comparison.)",
                "The statistics in table 2 indicates the usefulness of different term types in topic discrimination with respect to different news classes.",
                "We can see that, location name is the most useful term type for three news classes: Natural Disasters, Violence or War, Finances.",
                "And for three other categories Elections, Legal/Criminal Cases, Science and Discovery, person name is the most discriminative term type.",
                "For Scandals/Hearings, date is the most important information for topic discrimination.",
                "In addition, Legal/Criminal Cases and Finance topics have higher correlation with money terms, while Science and Discovery have higher correlation with percentage terms.",
                "Non-name terms are more stable for different classes. 1 . http://opennlp.sourceforge.net/ 2 . http://projects.ldc.upenn.edu/TDT3/Guide/label.html From the analysis of table 2, it is reasonable to adjust term weight according to their term type and the news class the story belongs to.",
                "New term weights are reweighted as follows: ( ) ( ) ( ) ( ) ( , , ) * ( , , ) ( , , ) * class d D type w T class d D type w w d weight d t w weight d t w weight d t w α α ∈ = ∑ (11) where type(w) represents the type of term w, and class(d) represents the class of story d, c kα is reweighting parameter for news class c and term type k. In the work, we just simply use statistics in table 2 as the reweighting parameters.",
                "Even thought using the statistics directly may not the best choice, we do not discuss how to automatically obtain the best parameters.",
                "We will try to use machine learning techniques to obtain the best parameters in the future work.",
                "In the work, we use BoosTexter [20] to classify all stories into one of the 11 ROIs.",
                "BoosTexter is a boosting based machine learning program, which creates a series of simple rules for building a classifier for text or attribute-value data.",
                "We use term weight generated using TF-IDF model as feature for story classification.",
                "We trained the model on the 12000 judged English stories in TDT2, and classify the rest of the stories in TDT2 and all stories in TDT3.",
                "Classification results are used for term reweighting in formula (11).",
                "Since the class labels of topic-off stories are not given in TDT datasets, we cannot give the classification accuracy here.",
                "Thus we do not discuss the effects of classification accuracy to NED performance in the paper. 6.",
                "EXPERIMENTAL SETUP 6.1 Datasets We used two LDC [18] datasets TDT2 and TDT3 for our experiments.",
                "TDT2 contains news stories from January to June 1998.",
                "It contains around 54,000 stories from sources like ABC, Associated Press, CNN, New York Times, Public Radio International, Voice of America etc.",
                "Only English stories in the collection were considered.",
                "TDT3 contains approximately 31,000 English stories collected from October to December 1998.",
                "In addition to the sources used in TDT2, it also contains stories from NBC and MSNBC TV broadcasts.",
                "We used transcribed versions of the TV and radio broadcasts besides textual news.",
                "TDT2 dataset is labeled with about 100 topics, and approximately 12,000 English stories belong to at least one of these topics.",
                "TDT3 dataset is labeled with about 120 topics, and approximately 8000 English stories belong to at least one of these topics.",
                "All the topics are classified into 11 Rules of Interpretation: (1)Elections, (2)Scandals/Hearings, (3)Legal/Criminal Cases, (4)Natural Disasters, (5)Accidents, (6)Ongoing Violence or War, (7)Science and Discovery News, (8)Finance, (9)New Law, (10)Sports News, (11)MISC.",
                "News. 6.2 Evaluation Metric TDT uses a cost function CDet that combines the probabilities of missing a new story and a false alarm [19]: * * * *Det Miss Miss Target FA FA NontargetC C P P C P P= + (12) Table 2.",
                "Average correlation between term types and news classes where CMiss means the cost of missing a new story, PMiss means the probability of missing a new story, and PTarget means the probability of seeing a new story in the data; CFA means the cost of a false alarm, PFA means the probability of a false alarm, and PNontarget means the probability of seeing an old story.",
                "The cost CDet is normalized such that a perfect system scores 0 and a trivial system, which is the better one of mark all stories as new or old, scores 1: ( ( * , * ) ) Det Det Miss Target FA Nontarget C Norm C min C P C P = (13) New event detection system gives two outputs for each story.",
                "The first part is yes or no indicating whether the story triggers a new event or not.",
                "The second part is a score indicating confidence of the first decision.",
                "Confidence scores can be used to plot DET curve, i.e., curves that plot false alarm vs. miss probabilities.",
                "Minimum normalized cost can be determined if optimal threshold on the score were chosen. 7.",
                "EXPERIMENTAL RESULTS 7.1 Main Results To test the approaches proposed in the model, we implemented and tested five systems: System-1: this system is used as baseline.",
                "It is implemented based on the basic model described in section 3, i.e., using incremental TF-IDF model to generate term weights, and using Hellinger distance to compute document similarity.",
                "Similarity score normalization is also employed [8].",
                "S-S detection procedure is used.",
                "System-2: this system is the same as system-1 except that S-C detection procedure is used.",
                "System-3: this system is the same as system-1 except that it uses the new detection procedure which is based on indexing-tree.",
                "System-4: implemented based on the approach presented in section 5.1, i.e., terms are reweighted according to the distance between term distributions in a cluster and all stories.",
                "The new detection procedure is used.",
                "System-5: implemented based on the approach presented in section 5.2, i.e., terms of different types are reweighted according to news class using trained parameters.",
                "The new detection procedure is used.",
                "The following are some other NED systems: System-6: [21] for each pair of stories, it computes three similarity values for named entity, non-named entity and all terms respectively.",
                "And employ Support Vector Machine to predict new or old using the similarity values as features.",
                "System-7: [8] it extended a basic incremental TF-IDF model to include source-specific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, etc.",
                "System-8: [13] it split document representation into two parts: named entities and non-named entities, and choose one effective part for each news class.",
                "Table 3 and table 4 show topic-weighted normalized costs and comparing times on TDT2 and TDT3 datasets respectively.",
                "Since no heldout data set for fine-tuning the threshold θ new was available for experiments on TDT2, we only report minimum normalized costs for our systems in table 3.",
                "System-5 outperforms all other systems including system-6, and it performs only 2.78e+8 comparing times in detection procedure which is only 13.4% of system-1.",
                "Table 3.",
                "NED results on TDT2 Systems Min Norm(CDet) Cmp times System-1 0.5749 2.08e+9 System-2① 0.6673 3.77e+8 System-3② 0.5765 2.81e+8 System-4② 0.5431 2.99e+8 System-5② 0.5089 2.78e+8 System-6 0.5300 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 When evaluating on the normalized costs on TDT3, we use the optimal thresholds obtained from TDT2 data set for all systems.",
                "System-2 reduces comparing times to 1.29e+9 which is just 18.3% of system-1, but at the same time it also gets a deteriorated minimum normalized cost which is 0.0499 higher than system-1.",
                "System-3 uses the new detection procedure based on <br>news indexing-tree</br>.",
                "It requires even less comparing times than system-2.",
                "This is because story-story comparisons usually yield greater similarities than story-cluster ones, so stories tend to be combined Location Person Date Organization Money Percentage NN JJ CD Elections 0.37 1 0.04 0.58 0.08 0.03 0.32 0.13 0.1 Scandals/Hearings 0.66 0.62 0.28 1 0.11 0.02 0.27 0.13 0.05 Legal/Criminal Cases 0.48 1 0.02 0.62 0.15 0 0.22 0.24 0.09 Natural Disasters 1 0.27 0 0.04 0.04 0 0.25 0.04 0.02 Violence or War 1 0.36 0.02 0.14 0.02 0.04 0.21 0.11 0.02 Science and Discovery 0.11 1 0.01 0.22 0.08 0.12 0.19 0.08 0.03 Finances 1 0.45 0.04 0.98 0.13 0.02 0.29 0.06 0.05 Sports 0.16 0.27 0.01 1 0.02 0 0.11 0.03 0.01 together in system-3.",
                "And system-3 is basically equivalent to system-1 in accuracy results.",
                "System-4 adjusts term weights based on the distance of term distributions between the whole corpus and cluster story set, yielding a good improvement by 0.0468 compared to system-1.",
                "The best system (system-5) has a minimum normalized cost 0.5012, which is 0.0797 better than system-1, and also better than any other results previously reported for this dataset [8, 13].",
                "Further more, system-5 only needs 1.05e+8 comparing times which is 14.9% of system-1.",
                "Table 4.",
                "NED results on TDT3 Systems Norm(CDet) Min Norm(CDet) Cmp times System-1 0.6159 0.5809 7.04e+8 System-2① 0.6493 0.6308 1.29e+8 System-3② 0.6197 0.5868 1.03e+8 System-4② 0.5601 0.5341 1.03e+8 System-5② 0.5413 0.5012 1.05e+8 System-7 -- 0.5783 -System-8 -- 0.5229 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 Figure5 shows the five DET curves for our systems on data set TDT3.",
                "System-5 achieves the minimum cost at a false alarm rate of 0.0157 and a miss rate of 0.4310.",
                "We can observe that System4 and System-5 obtain lower miss probability at regions of low false alarm probabilities.",
                "The hypothesis is that, more weight value is transferred to key terms of topics from non-key terms.",
                "Similarity score between two stories belonging to different topics are lower than before, because their overlapping terms are usually not key terms of their topics. 7.2 Parameter selection for indexing-tree detection Figure 3 shows the minimum normalized costs obtained by system-3 on TDT3 using different parameters.",
                "Theθ init parameter is tested on six values spanning from 0.03 to 0.18.",
                "And the λ parameter is tested on four values 1, 2, 3 and 4.",
                "We can see that, whenθ init is set to 0.12, which is the closest one toθ new, the costs are lower than others.",
                "This is easy to explain, because when stories belonging to the same topic are put in a cluster, it is more reasonable for the cluster to represent the stories in it.",
                "When parameter λ is set to 3 or 4, the costs are better than other cases, but there is no much difference between 3 and 4. 0 0.05 0.1 0.15 0.2 1 2 3 4 0.5 0.6 0.7 0.8 0.9 1 θ-initλ MinCost 0.6 0.65 0.7 0.75 0.8 0.85 0.9 Figure 3.",
                "Min Cost on TDT3 (δ =0.15) 0 0.05 0.1 0.15 0.2 1 2 3 4 0 0.5 1 1.5 2 2.5 x 10 8 θ-init λ Comparingtimes 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 8 Figure 4.",
                "Comparing times on TDT3 (δ =0.15) Figure 4 gives the comparing times used by system-3 on TDT3 with the same parameters as figure 3.",
                "The comparing times are strongly dependent onθ init.",
                "Because the greaterθ init is, the less stories combined together, the more comparing times are needed for new event decision.",
                "So we useθ init =0.13,λ =3,δ =0.15 for system-3, 4, and 5.",
                "In this parameter setting, we can get both low minimum normalized costs and less comparing times. 8.",
                "CONCLUSION We have proposed a <br>news indexing-tree</br> based detection procedure in our model.",
                "It reduces comparing times to about one seventh of traditional method without hurting NED accuracy.",
                "We also have presented two extensions to the basic TF-IDF model.",
                "The first extension is made by adjust term weights based on term distributions between the whole corpus and a cluster story set.",
                "And the second extension to basic TF-IDF model is better use of term types (named entities types and part-of-speed) according to news categories.",
                "Our experimental results on TDT2 and TDT3 datasets show that both of the two extensions contribute significantly to improvement in accuracy.",
                "We did not consider news time information as a clue for NED task, since most of the topics last for a long time and TDT data sets only span for a relative short period (no more than 6 months).",
                "For the future work, we want to collect news set which span for a longer period from internet, and integrate time information in NED task.",
                "Since topic is a relative coarse-grained news cluster, we also want to refine cluster granularity to event-level, and identify different events and their relations within a topic.",
                "Acknowledgments This work is supported by the National Natural Science Foundation of China under Grant No. 90604025.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsor. 9.",
                "REFERENCES [1] http://www.nist.gov/speech/tests/tdt/index.htm [2] In Topic Detection and Tracking.",
                "Event-based Information Organization.",
                "Kluwer Academic Publishers, 2002. .01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90 1 2 5 10 20 40 60 80 90 False Alarm Probability (in %) MissProbability(in%) SYSTEM1 Topic Weighted Curve SYSTEM1 Min Norm(Cost) SYSTEM2 Topic Weighted Curve SYSTEM2 Min Norm(Cost) SYSTEM3 Topic Weighted Curve SYSTEM3 Min Norm(Cost) SYSTEM4 Topic Weighted Curve SYSTEM4 Min Norm(Cost) SYSTEM5 Topic Weighted Curve SYSTEM5 Min Norm(Cost) Random Performance Figure 5.",
                "DET curves on TDT3 [3] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B.T.",
                "Archibald, and X. Liu.",
                "Learning Approaches for Detecting and Tracking News Events.",
                "In IEEE Intelligent Systems Special Issue on Applications of Intelligent Information Retrieval, volume 14 (4), 1999, 32-43. [4] Y. Yang, T. Pierce, and J. Carbonell.",
                "A Study on Retrospective and On-line Event Detection.",
                "In Proceedings of SIGIR-98, Melbourne, Australia, 1998, 28-36. [5] J. Allan, V. Lavrenko, D. Malin, and R. Swan.",
                "Detections, Bounds, and Timelines: Umass and tdt-3.",
                "In Proceedings of Topic Detection and Tracking Workshop (TDT-3), Vienna, VA, 2000, 167-174. [6] R. Papka and J. Allan.",
                "On-line New Event Detection Using Single Pass Clustering TITLE2:.",
                "Technical Report UM-CS1998-021, 1998. [7] W. Lam, H. Meng, K. Wong, and J.",
                "Yen.",
                "Using Contextual Analysis for News Event Detection.",
                "International Journal on Intelligent Systems, 2001, 525-546. [8] B. Thorsten, C. Francine, and F. Ayman.",
                "A System for New Event Detection.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2003, 330-337. [9] S. Nicola and C. Joe.",
                "Combining Semantic and Syntactic Document Classifiers to Improve First Story Detection.",
                "In Proceedings of the 24th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topicconditioned Novelty Detection.",
                "In Proceedings of the 8th ACM SIGKDD International Conference, ACM Press. 2002, 688-693. [11] M. Juha, A.M. Helena, and S. Marko.",
                "Applying Semantic Classes in Event Detection and Tracking.",
                "In Proceedings of International Conference on Natural Language Processing (ICON 2002), 2002, pages 175-183. [12] M. Juha, A.M. Helena, and S. Marko.",
                "Simple Semantics in Topic Detection and Tracking.",
                "Information Retrieval, 7(3-4): 2004, 347-368. [13] K. Giridhar and J. Allan.",
                "Text Classification and Named Entities for New Event Detection.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2004, 297-304. [14] J. P. Callan, W. B. Croft, and S. M. Harding.",
                "The INQUERY Retrieval System.",
                "In Proceedings of DEXA-92, 3rd International Conference on Database and Expert Systems Applications, 1992, 78-83. [15] R. Krovetz.",
                "Viewing Morphology as An Inference Process.",
                "In Proceedings of ACM SIGIR93, 1993, 61-81. [16] Y. Yang and J. Pedersen.",
                "A Comparative Study on Feature Selection in Text Categorization.",
                "In J. D. H. Fisher, editor, The Fourteenth International Conference on Machine Learning (ICML97), Morgan Kaufmann, 1997, 412-420. [17] T. M. Cover, and J.A.",
                "Thomas.",
                "Elements of Information Theory.",
                "Wiley. 1991. [18] The linguistic data consortium, http://www.ldc,upenn.edu/. [19] The 2001 TDT task definition and evaluation plan, http://www.nist.gov/speech/tests/tdt/tdt2001/evalplan.htm. [20] R. E. Schapire and Y.",
                "Singer.",
                "Boostexter: A Boosting-based System for Text Categorization.",
                "In Machine Learning 39(2/3):1, Kluwer Academic Publishers, 2000, 35-168. [21] K. Giridhar and J. Allan. 2005.",
                "Using Names and Topics for New Event Detection.",
                "In Proceedings of Human Technology Conference and Conference on Empirical Methods in Natural Language, Vancouver, 2005, 121-128"
            ],
            "original_annotated_samples": [
                "In this paper we propose a new NED model to speed up the NED task by using <br>news indexing-tree</br> dynamically.",
                "Driven by these problems, we have proposed three approaches in this paper. (1)To make the detection procedure faster, we propose a new NED procedure based on <br>news indexing-tree</br> created dynamically.",
                "Section 4 describes our new detection procedure based on <br>news indexing-tree</br>.",
                "The new procedure creates a <br>news indexing-tree</br> dynamically, in which similar stories are put together to form a hierarchy of clusters.",
                "The <br>news indexing-tree</br> is defined formally as follows: S-Tree = {r, NC , NS , E} where r is the root of S-Tree, NC is the set of all cluster nodes, NS is the set of all story nodes, and E is the set of all edges in S-Tree."
            ],
            "translated_annotated_samples": [
                "En este artículo proponemos un nuevo modelo de NED para acelerar la tarea de NED mediante el uso dinámico de un <br>árbol de indexación de noticias</br>.",
                "Impulsados por estos problemas, hemos propuesto tres enfoques en este documento. (1) Para hacer que el procedimiento de detección sea más rápido, proponemos un nuevo procedimiento de NED basado en un <br>árbol de indexación de noticias</br> creado dinámicamente.",
                "La sección 4 describe nuestro nuevo procedimiento de detección basado en el <br>índice de árbol de noticias</br>.",
                "El nuevo procedimiento crea un <br>árbol de indexación de noticias</br> dinámicamente, en el cual se agrupan historias similares para formar una jerarquía de clusters.",
                "El <br>índice de noticias en forma de árbol</br> se define formalmente de la siguiente manera: S-Tree = {r, NC, NS, E}, donde r es la raíz de S-Tree, NC es el conjunto de todos los nodos de clúster, NS es el conjunto de todos los nodos de historia, y E es el conjunto de todos los bordes en S-Tree."
            ],
            "translated_text": "Detección de nuevos eventos basada en árbol de indexación y entidad nombrada Zhang Kuo Universidad Tsinghua Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Universidad Tsinghua Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Universidad Tsinghua Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn RESUMEN La Detección de Nuevos Eventos (NED) tiene como objetivo detectar de una o varias corrientes de noticias cuál se informa sobre un nuevo evento (es decir, no reportado previamente). Con el abrumador volumen de noticias disponible hoy en día, hay una creciente necesidad de un sistema NED que sea capaz de detectar eventos nuevos de manera más eficiente y precisa. En este artículo proponemos un nuevo modelo de NED para acelerar la tarea de NED mediante el uso dinámico de un <br>árbol de indexación de noticias</br>. Además, basándose en la observación de que los términos de diferentes tipos tienen diferentes efectos para la tarea de NED, se proponen dos enfoques de reponderación de términos para mejorar la precisión de NED. En el primer enfoque, proponemos ajustar dinámicamente los pesos de los términos basados en los clusters de historias previas y en el segundo enfoque, proponemos emplear estadísticas en los datos de entrenamiento para aprender el modelo de reajuste de entidades nombradas para cada clase de historias. Los resultados experimentales en dos conjuntos de datos del Consorcio de Datos Lingüísticos (LDC), TDT2 y TDT3, muestran que el modelo propuesto puede mejorar significativamente tanto la eficiencia como la precisión de la tarea de NED, en comparación con el sistema base y otros sistemas existentes. Categorías y Descriptores de Asignaturas H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información; H.4.2 [Aplicaciones de Sistemas de Información]: Tipos de Sistemas de Apoyo a Decisiones. Términos generales Algoritmos, Rendimiento, Experimentación 1. El programa de Detección y Seguimiento de Temas (TDT) tiene como objetivo desarrollar técnicas que puedan organizar, buscar y estructurar de manera efectiva materiales de texto de noticias de una variedad de agencias de noticias y medios de difusión [1]. La Detección de Eventos Nuevos (DEN) es una de las cinco tareas en TDT. Es la tarea de identificación en línea del informe más temprano para cada tema tan pronto como ese informe llegue en la secuencia de documentos. Un tema se define como un evento o actividad seminal, junto con eventos y actividades directamente relacionados [2]. Un evento se define como algo (no trivial) que sucede en un lugar y en un momento determinados [3]. Por ejemplo, cuando una bomba explota en un edificio, la explosión es el evento seminal que desencadena el tema, y otras historias sobre el mismo tema serían aquellas que discuten los esfuerzos de salvamento, la búsqueda de los perpetradores, arrestos y juicios, entre otros. La información útil de noticias suele estar enterrada en una masa de datos generados a diario. Por lo tanto, los sistemas NED son muy útiles para las personas que necesitan detectar información novedosa de un flujo de noticias en tiempo real. Estas necesidades de la vida real a menudo se presentan en ámbitos como los mercados financieros, el análisis de noticias y la recopilación de inteligencia. En la mayoría de los sistemas NED de última generación, cada noticia disponible se compara con todas las noticias previamente recibidas. Si todas las similitudes entre ellos no superan un umbral, entonces la historia desencadena un nuevo evento. Por lo general, se presentan en forma de similitud de coseno o métrica de similitud de Hellinger. El problema central de NED es identificar si dos historias tratan sobre el mismo tema. Obviamente, estos sistemas no pueden aprovechar la información del tema. Además, no es aceptable en aplicaciones reales debido a la gran cantidad de cálculos requeridos en el proceso de NED. Otros sistemas organizan historias anteriores en grupos (cada grupo corresponde a un tema), y la nueva historia se compara con los grupos anteriores en lugar de con historias. Esta forma puede reducir significativamente los tiempos de comparación. Sin embargo, se ha demostrado que este método es menos preciso [4, 5]. Esto se debe a que a veces las historias dentro de un tema se alejan mucho entre sí, lo que podría resultar en una baja similitud entre una historia y su tema. Por otro lado, algunos sistemas NED propuestos intentaron mejorar la precisión al hacer un mejor uso de las entidades nombradas [10, 11, 12, 13]. Sin embargo, ninguno de los sistemas ha considerado que los términos de diferentes tipos (por ejemplo, Los sustantivos, verbos o nombres de personas tienen diferentes efectos en diferentes clases de historias para determinar si dos historias tratan sobre el mismo tema. Por ejemplo, los nombres de los candidatos electorales (Nombre de la persona) son muy importantes para las historias de la clase de elecciones; las ubicaciones (Nombre de la ubicación) donde ocurrieron los accidentes son importantes para las historias de la clase de accidentes. Por lo tanto, en NED, todavía existen los siguientes tres problemas por investigar: (1) ¿Cómo acelerar el procedimiento de detección sin disminuir la precisión de la detección? (2) ¿Cómo aprovechar mejor la información de los grupos (temas) para mejorar la precisión? (3) ¿Cómo obtener una mejor representación de historias de noticias mediante una mejor comprensión de las entidades nombradas? Impulsados por estos problemas, hemos propuesto tres enfoques en este documento. (1) Para hacer que el procedimiento de detección sea más rápido, proponemos un nuevo procedimiento de NED basado en un <br>árbol de indexación de noticias</br> creado dinámicamente. El índice de historias en árbol se crea ensamblando historias similares para formar grupos de noticias en diferentes jerarquías según sus valores de similitud. Las comparaciones entre la historia actual y los grupos anteriores podrían ayudar a encontrar la historia más similar en menos tiempo de comparación. El nuevo procedimiento puede reducir la cantidad de veces de comparación sin afectar la precisión. Utilizamos los grupos del primer piso en el árbol de indexación como temas de noticias, en los cuales los pesos de los términos se ajustan dinámicamente según la distribución de términos en los grupos. En este enfoque, se utiliza adecuadamente la información de los grupos (temas), por lo que se evita el problema de la descentralización del tema. Basándonos en observaciones de las estadísticas obtenidas de los datos de entrenamiento, encontramos que los términos de diferentes tipos (por ejemplo, Los sustantivos y verbos tienen diferentes efectos para diferentes clases de historias al determinar si dos historias tratan sobre el mismo tema. Y proponemos utilizar estadísticas para optimizar los pesos de los términos de diferentes tipos en una historia de acuerdo a la clase de noticias a la que pertenece la historia. En el conjunto de datos TDT3, el nuevo modelo NED solo utiliza un 14.9% en comparación con los tiempos del modelo básico, mientras que su costo normalizado mínimo es de 0.5012, lo que es 0.0797 mejor que el modelo básico, y también mejor que cualquier otro resultado previamente reportado para este conjunto de datos [8, 13]. El resto del documento está organizado de la siguiente manera. Comenzamos este artículo resumiendo el trabajo previo en NED en la sección 2. La sección 3 presenta el modelo básico para NED que la mayoría de los sistemas actuales utilizan. La sección 4 describe nuestro nuevo procedimiento de detección basado en el <br>índice de árbol de noticias</br>. En la sección 5, se proponen dos métodos de reponderación de términos para mejorar la precisión de NED. La sección 6 presenta nuestros datos experimentales y métricas de evaluación. Finalmente concluimos con los resultados experimentales en la Sección 7, y las conclusiones y trabajo futuro en la Sección 8. TRABAJO RELACIONADO Papka et al. propusieron el agrupamiento de un solo paso en NED [6]. Cuando se encontraba una nueva historia, esta era procesada inmediatamente para extraer características de términos y se construía una representación de consulta del contenido de la historia. Luego se comparó con todas las consultas anteriores. Si el documento no generaba ninguna consulta al exceder un umbral, se marcaba como un nuevo evento. Lam et al construyen representaciones de consultas previas de grupos de historias, cada una de las cuales corresponde a un tema [7]. De esta manera se realizan comparaciones entre historias y grupos. En los últimos años, la mayoría de los trabajos se han centrado en proponer mejores métodos para la comparación de historias y la representación de documentos. Brants et al. [8] extendieron un modelo básico incremental de TF-IDF para incluir modelos específicos de fuente, normalización de puntajes de similitud basada en promedios específicos de documentos, normalización de puntajes de similitud basada en promedios específicos de pares de fuentes, reponderación de términos basada en frecuencias de eventos inversas y segmentación de documentos. Se mostraron buenas mejoras en los puntos de referencia de TDT. Stokes et al. [9] utilizaron una combinación de evidencia de dos representaciones distintas del contenido de un documento. Una de las representaciones era el vector de texto libre habitual, la otra hacía uso de cadenas léxicas (creadas utilizando WordNet) para construir otro vector de términos. Luego, las dos representaciones se combinan de forma lineal. Se logró un aumento marginal en la efectividad cuando se utilizó la representación combinada. Algunos esfuerzos se han realizado sobre cómo utilizar entidades nombradas para mejorar la desambiguación de entidades nombradas (NED). Yang et al. asignaron un peso cuatro veces mayor a las entidades con nombres de ubicación que a otros términos y entidades con nombres [10]. El grupo de investigación DOREMI combinó similitudes semánticas de nombres de personas, nombres de lugares y tiempo junto con similitud textual [11][12]. El grupo de investigación de UMass [13] dividió la representación del documento en dos partes: entidades nombradas y entidades no nombradas. Y se descubrió que algunas clases de noticias podían lograr un mejor rendimiento utilizando la representación de entidades nombradas, mientras que otras clases de noticias podían lograr un mejor rendimiento utilizando la representación sin entidades nombradas. Tanto [10] como [13] utilizaron la técnica de categorización de texto para clasificar previamente las noticias. En [13] las noticias se clasifican automáticamente al principio, y luego se prueban las sensibilidades de los nombres y términos que no son nombres para NED para cada clase. En [10], se eliminan los términos frecuentes de cada clase de la representación del documento. Por ejemplo, la palabra \"elección\" no ayuda a identificar diferentes elecciones. En su trabajo, no se investiga la efectividad de diferentes tipos de nombres (o términos con diferentes categorías gramaticales) para NED en diferentes clases de noticias. Utilizamos análisis estadístico para revelar el hecho y utilizarlo para mejorar el rendimiento de NED. 3. MODELO BÁSICO En esta sección, presentamos el modelo básico de Detección de Nuevos Eventos que es similar a lo que la mayoría de los sistemas actuales aplican. Luego, proponemos nuestro nuevo modelo ampliando el modelo básico. Los sistemas de detección de eventos nuevos utilizan flujos de noticias como entrada, en los cuales las historias están estrictamente ordenadas por tiempo. Solo están disponibles las historias recibidas previamente al tratar con la historia actual. La salida es una decisión sobre si la historia actual es un evento nuevo o no y la confianza de la decisión. Por lo general, un modelo NED consta de tres partes: representación de la historia, cálculo de similitud y procedimiento de detección. 3.1 Representación de la historia Se necesita preprocesamiento antes de generar la representación de la historia. Para el preprocesamiento, tokenizamos palabras, reconocemos abreviaturas, normalizamos abreviaturas, agregamos etiquetas de partes del discurso, eliminamos palabras vacías incluidas en la lista de paradas utilizada en InQuery [14], reemplazamos palabras con sus raíces utilizando el algoritmo K-stem[15], y luego generamos un vector de palabras para cada noticia. Utilizamos el modelo TF-IDF incremental para el cálculo del peso de los términos [4]. En un modelo TF-IDF, la frecuencia de término en un documento de noticias se pondera por la frecuencia inversa del documento, la cual se genera a partir del corpus de entrenamiento. Cuando surge un nuevo término en el proceso de prueba, hay dos soluciones: simplemente ignorar el nuevo término o establecer df del término como una constante pequeña (por ejemplo, df = 1). El nuevo término recibe un peso demasiado bajo en la primera solución (0) y un peso demasiado alto en la segunda solución. En el modelo TF-IDF incremental, las frecuencias de documentos se actualizan dinámicamente en cada paso de tiempo t: 1( ) ( ) ( )t t D tdf w df w df w−= + (1) donde Dt representa el conjunto de noticias recibidas en el tiempo t, y dfDt(w) significa el número de documentos en los que aparece el término w, y dft(w) significa el número total de documentos en los que aparece el término w antes del tiempo t. En este trabajo, cada ventana de tiempo incluye 50 noticias. Así, cada historia d recibida en t se representa de la siguiente manera: 1 2{ ( , , ), ( , , ),..., ( , , )}nd peso d t w peso d t w peso d t w→ donde n significa el número de términos distintos en la historia d, y ( , , )peso d t w significa el peso del término w en la historia d en el tiempo t: log( ( , ) 1) log(( 1) /( ( ) 0.5)) ( , , ) log( ( , ) 1) log(( 1) /( ( ) 0.5)) t t t t w d tf d w N df w peso d t w tf d w N df w ∈ + + + = + + +∑ (2) donde Nt significa el número total de historias de noticias antes del tiempo t, y tf(d,w) significa cuántas veces aparece el término w en la historia de noticias d. 3.2 Cálculo de Similitud Utilizamos la distancia de Hellinger para el cálculo de la similitud entre dos historias, para dos historias d y d en el tiempo t, su similitud se define de la siguiente manera: , ( , , ) ( , , ) * ( , , ) w d d sim d d t peso d t w peso d t w ∈ = ∑ (3) 3.3 Procedimiento de Detección Para cada historia d recibida en el paso de tiempo t, el valor ( ) ( ) ( ) ( ( , , )) tiempo d tiempo d n d max sim d d t < = (4) es un puntaje utilizado para determinar si d es una historia sobre un nuevo tema y al mismo tiempo es una indicación de la confianza en nuestra decisión [8]. tiempo(d) significa el tiempo de publicación de la historia d. Si el puntaje supera el umbral θ new, entonces existe un documento suficientemente similar, por lo tanto, d es una historia antigua, de lo contrario, no hay un documento previo suficientemente similar, por lo tanto, d es una historia nueva. 4. Nuevo procedimiento NED Los sistemas NED tradicionales se pueden clasificar en dos tipos principales en cuanto al procedimiento de detección: (1) tipo S-S, en el que la historia en cuestión se compara con cada historia recibida previamente, y se utiliza la mayor similitud para determinar si la historia actual trata sobre un evento nuevo; (2) tipo S-C, en el que la historia en cuestión se compara con todos los clusters anteriores, cada uno de los cuales representa un tema, y se utiliza la mayor similitud para la decisión final sobre la historia actual. Si la similitud más alta supera el umbral θ nuevo, entonces es una historia antigua y se coloca en el clúster más similar; de lo contrario, es una historia nueva y se crea un nuevo clúster. Trabajos anteriores muestran que la primera forma es más precisa que la segunda [4][5]. Dado que a veces las historias dentro de un tema se alejan mucho entre sí, una historia puede tener una similitud muy baja con su tema. Por lo tanto, utilizar similitudes entre historias para determinar una nueva historia es mejor que utilizar similitudes entre historias y grupos. Sin embargo, el primer método requiere mucho más tiempo de comparación, lo que significa que es menos eficiente. Proponemos un nuevo procedimiento de detección que utiliza comparaciones con clusters anteriores para ayudar a encontrar la historia más similar en menos tiempo de comparación, y la decisión final sobre el nuevo evento se toma de acuerdo con la historia más similar. Por lo tanto, podemos obtener tanto la precisión de los métodos de tipo S-S como la eficiencia de los métodos de tipo S-C. El nuevo procedimiento crea un <br>árbol de indexación de noticias</br> dinámicamente, en el cual se agrupan historias similares para formar una jerarquía de clusters. Indexamos historias similares juntas por su ancestro común (un nodo de clúster). Historias diferentes están indexadas en diferentes grupos. Cuando una historia está llegando, utilizamos comparaciones entre la historia actual y los grupos jerárquicos anteriores para ayudar a encontrar la historia más similar que es útil para la toma de decisiones sobre nuevos eventos. Después de que se tome la decisión sobre el nuevo evento, la historia actual se inserta en el árbol de indexación para la detección siguiente. El <br>índice de noticias en forma de árbol</br> se define formalmente de la siguiente manera: S-Tree = {r, NC, NS, E}, donde r es la raíz de S-Tree, NC es el conjunto de todos los nodos de clúster, NS es el conjunto de todos los nodos de historia, y E es el conjunto de todos los bordes en S-Tree. ",
            "candidates": [],
            "error": [
                [
                    "árbol de indexación de noticias",
                    "árbol de indexación de noticias",
                    "índice de árbol de noticias",
                    "árbol de indexación de noticias",
                    "índice de noticias en forma de árbol"
                ]
            ]
        },
        "term reweighting approach": {
            "translated_key": "enfoques de reponderación de términos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "New Event Detection Based on Indexing-tree and Named Entity Zhang Kuo Tsinghua University Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Tsinghua University Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Tsinghua University Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn ABSTRACT New Event Detection (NED) aims at detecting from one or multiple streams of news stories that which one is reported on a new event (i.e.",
                "not reported previously).",
                "With the overwhelming volume of news available today, there is an increasing need for a NED system which is able to detect new events more efficiently and accurately.",
                "In this paper we propose a new NED model to speed up the NED task by using news indexing-tree dynamically.",
                "Moreover, based on the observation that terms of different types have different effects for NED task, two <br>term reweighting approach</br>es are proposed to improve NED accuracy.",
                "In the first approach, we propose to adjust term weights dynamically based on previous story clusters and in the second approach, we propose to employ statistics on training data to learn the named entity reweighting model for each class of stories.",
                "Experimental results on two Linguistic Data Consortium (LDC) datasets TDT2 and TDT3 show that the proposed model can improve both efficiency and accuracy of NED task significantly, compared to the baseline system and other existing systems.",
                "Categories and Subject Descriptors H.3.3 [Information Systems]: Information Search and Retrieval; H.4.2 [Information Systems Applications]: Types of Systemsdecision support.",
                "General Terms Algorithms, Performance, Experimentation 1.",
                "INTRODUCTION Topic Detection and Tracking (TDT) program aims to develop techniques which can effectively organize, search and structure news text materials from a variety of newswire and broadcast media [1].",
                "New Event Detection (NED) is one of the five tasks in TDT.",
                "It is the task of online identification of the earliest report for each topic as soon as that report arrives in the sequence of documents.",
                "A Topic is defined as a seminal event or activity, along with directly related events and activities [2].",
                "An Event is defined as something (non-trivial) happening in a certain place at a certain time [3].",
                "For instance, when a bomb explodes in a building, the exploding is the seminal event that triggers the topic, and other stories on the same topic would be those discussing salvaging efforts, the search for perpetrators, arrests and trial and so on.",
                "Useful news information is usually buried in a mass of data generated everyday.",
                "Therefore, NED systems are very useful for people who need to detect novel information from real-time news stream.",
                "These real-life needs often occur in domains like financial markets, news analysis, and intelligence gathering.",
                "In most of state-of-the-art (currently) NED systems, each news story on hand is compared to all the previous received stories.",
                "If all the similarities between them do not exceed a threshold, then the story triggers a new event.",
                "They are usually in the form of cosine similarity or Hellinger similarity metric.",
                "The core problem of NED is to identify whether two stories are on the same topic.",
                "Obviously, these systems cannot take advantage of topic information.",
                "Further more, it is not acceptable in real applications because of the large amount of computation required in the NED process.",
                "Other systems organize previous stories into clusters (each cluster corresponds to a topic), and new story is compared to the previous clusters instead of stories.",
                "This manner can reduce comparing times significantly.",
                "Nevertheless, it has been proved that this manner is less accurate [4, 5].",
                "This is because sometimes stories within a topic drift far away from each other, which could lead low similarity between a story and its topic.",
                "On the other hand, some proposed NED systems tried to improve accuracy by making better use of named entities [10, 11, 12, 13].",
                "However, none of the systems have considered that terms of different types (e.g.",
                "Noun, Verb or Person name) have different effects for different classes of stories in determining whether two stories are on the same topic.",
                "For example, the names of election candidates (Person name) are very important for stories of election class; the locations (Location name) where accidents happened are important for stories of accidents class.",
                "So, in NED, there still exist following three problems to be investigated: (1) How to speed up the detection procedure while do not decrease the detection accuracy? (2) How to make good use of cluster (topic) information to improve accuracy? (3) How to obtain better news story representation by better understanding of named entities.",
                "Driven by these problems, we have proposed three approaches in this paper. (1)To make the detection procedure faster, we propose a new NED procedure based on news indexing-tree created dynamically.",
                "Story indexing-tree is created by assembling similar stories together to form news clusters in different hierarchies according to their values of similarity.",
                "Comparisons between current story and previous clusters could help find the most similar story in less comparing times.",
                "The new procedure can reduce the amount of comparing times without hurting accuracy. (2)We use the clusters of the first floor in the indexing-tree as news topics, in which term weights are adjusted dynamically according to term distribution in the clusters.",
                "In this approach, cluster (topic) information is used properly, so the problem of theme decentralization is avoided. (3)Based on observations on the statistics obtained from training data, we found that terms of different types (e.g.",
                "Noun and Verb) have different effects for different classes of stories in determining whether two stories are on the same topic.",
                "And we propose to use statistics to optimize the weights of the terms of different types in a story according to the news class that the story belongs to.",
                "On TDT3 dataset, the new NED model just uses 14.9% comparing times of the basic model, while its minimum normalized cost is 0.5012, which is 0.0797 better than the basic model, and also better than any other results previously reported for this dataset [8, 13].",
                "The rest of the paper is organized as follows.",
                "We start off this paper by summarizing the previous work in NED in section 2.",
                "Section 3 presents the basic model for NED that most current systems use.",
                "Section 4 describes our new detection procedure based on news indexing-tree.",
                "In section 5, two term reweighting methods are proposed to improve NED accuracy.",
                "Section 6 gives our experimental data and evaluation metrics.",
                "We finally wrap up with the experimental results in Section 7, and the conclusions and future work in Section 8. 2.",
                "RELATED WORK Papka et al. proposed Single-Pass clustering on NED [6].",
                "When a new story was encountered, it was processed immediately to extract term features and a query representation of the storys content is built up.",
                "Then it was compared with all the previous queries.",
                "If the document did not trigger any queries by exceeding a threshold, it was marked as a new event.",
                "Lam et al build up previous query representations of story clusters, each of which corresponds to a topic [7].",
                "In this manner comparisons happen between stories and clusters.",
                "Recent years, most work focus on proposing better methods on comparison of stories and document representation.",
                "Brants et al. [8] extended a basic incremental TF-IDF model to include sourcespecific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, term reweighting based on inverse event frequencies, and segmentation of documents.",
                "Good improvements on TDT bench-marks were shown.",
                "Stokes et al. [9] utilized a combination of evidence from two distinct representations of a documents content.",
                "One of the representations was the usual free text vector, the other made use of lexical chains (created using WordNet) to build another term vector.",
                "Then the two representations are combined in a linear fashion.",
                "A marginal increase in effectiveness was achieved when the combined representation was used.",
                "Some efforts have been done on how to utilize named entities to improve NED.",
                "Yang et al. gave location named entities four times weight than other terms and named entities [10].",
                "DOREMI research group combined semantic similarities of person names, location names and time together with textual similarity [11][12].",
                "UMass [13] research group split document representation into two parts: named entities and non-named entities.",
                "And it was found that some classes of news could achieve better performance using named entity representation, while some other classes of news could achieve better performance using non-named entity representation.",
                "Both [10] and [13] used text categorization technique to classify news stories in advance.",
                "In [13] news stories are classified automatically at first, and then test sensitivities of names and non-name terms for NED for each class.",
                "In [10] frequent terms for each class are removed from document representation.",
                "For example, word election does not help identify different elections.",
                "In their work, effectiveness of different kinds of names (or terms with different POS) for NED in different news classes are not investigated.",
                "We use statistical analysis to reveal the fact and use it to improve NED performance. 3.",
                "BASIC MODEL In this section, we present the basic New Event Detection model which is similar to what most current systems apply.",
                "Then, we propose our new model by extending the basic model.",
                "New Event Detection systems use news story stream as input, in which stories are strictly time-ordered.",
                "Only previously received stories are available when dealing with current story.",
                "The output is a decision for whether the current story is on a new event or not and the confidence of the decision.",
                "Usually, a NED model consists of three parts: story representation, similarity calculation and detection procedure. 3.1 Story Representation Preprocessing is needed before generating story representation.",
                "For preprocessing, we tokenize words, recognize abbreviations, normalize abbreviations, add part-of-speech tags, remove stopwords included in the stop list used in InQuery [14], replace words with their stems using K-stem algorithm[15], and then generate word vector for each news story.",
                "We use incremental TF-IDF model for term weight calculation [4].",
                "In a TF-IDF model, term frequency in a news document is weighted by the inverse document frequency, which is generated from training corpus.",
                "When a new term occurs in testing process, there are two solutions: simply ignore the new term or set df of the term as a small const (e.g. df = 1).",
                "The new term receives too low weight in the first solution (0) and too high weight in the second solution.",
                "In incremental TF-IDF model, document frequencies are updated dynamically in each time step t: 1( ) ( ) ( )t t D tdf w df w df w−= + (1) where Dt represents news story set received in time t, and dfDt(w) means the number of documents that term w occurs in, and dft(w) means the total number of documents that term w occurs in before time t. In this work, each time window includes 50 news stories.",
                "Thus, each story d received in t is represented as follows: 1 2{ ( , , ), ( , , ),..., ( , , )}nd weight d t w weight d t w weight d t w→ where n means the number of distinct terms in story d, and ( , , )weight d t w means the weight of term w in story d at time t: log( ( , ) 1) log(( 1) /( ( ) 0.5)) ( , , ) log( ( , ) 1) log(( 1) /( ( ) 0.5)) t t t t w d tf d w N df w weight d t w tf d w N df w ∈ + + + = + + +∑ (2) where Nt means the total number of news stories before time t, and tf(d,w) means how many times term w occurs in news story d. 3.2 Similarity Calculation We use Hellinger distance for the calculation of similarity between two stories, for two stories d and d at time t, their similarity is defined as follows: , ( , , ) ( , , ) * ( , , ) w d d sim d d t weight d t w weight d t w ∈ = ∑ (3) 3.3 Detection Procedure For each story d received in time step t, the value ( ) ( ) ( ) ( ( , , )) time d time d n d max sim d d t < = (4) is a score used to determine whether d is a story about a new topic and at the same time is an indication of the confidence in our decision [8]. time(d) means the publication time of story d. If the score exceeds the thresholdθ new, then there exists a sufficiently similar document, thus d is a old story, otherwise, there is no sufficiently similar previous document, thus d is an new story. 4.",
                "New NED Procedure Traditional NED systems can be classified into two main types on the aspect of detection procedure: (1) S-S type, in which the story on hand is compared to each story received previously, and use the highest similarity to determine whether current story is about a new event; (2) S-C type, in which the story on hand is compared to all previous clusters each of which representing a topic, and the highest similarity is used for final decision for current story.",
                "If the highest similarity exceeds thresholdθ new, then it is an old story, and put it into the most similar cluster; otherwise it is a new story and create a new cluster.",
                "Previous work show that the first manner is more accurate than the second one [4][5].",
                "Since sometimes stories within a topic drift far away from each other, a story may have very low similarity with its topic.",
                "So using similarities between stories for determining new story is better than using similarities between story and clusters.",
                "Nevertheless, the first manner needs much more comparing times which means the first manner is low efficient.",
                "We propose a new detection procedure which uses comparisons with previous clusters to help find the most similar story in less comparing times, and the final new event decision is made according to the most similar story.",
                "Therefore, we can get both the accuracy of S-S type methods and the efficiency of S-C type methods.",
                "The new procedure creates a news indexing-tree dynamically, in which similar stories are put together to form a hierarchy of clusters.",
                "We index similar stories together by their common ancestor (a cluster node).",
                "Dissimilar stories are indexed in different clusters.",
                "When a story is coming, we use comparisons between the current story and previous hierarchical clusters to help find the most similar story which is useful for new event decision.",
                "After the new event decision is made, the current story is inserted to the indexing-tree for the following detection.",
                "The news indexing-tree is defined formally as follows: S-Tree = {r, NC , NS , E} where r is the root of S-Tree, NC is the set of all cluster nodes, NS is the set of all story nodes, and E is the set of all edges in S-Tree.",
                "We define a set of constraints for a S-Tree: ⅰ . , is an non-terminal node in the treeC i i N i∀ ∈ → ⅱ . , is a terminal node in the treeS i i N i∀ ∈ → ⅲ . , out degree of is at least 2C i i N i∀ ∈ → ⅳ . , is represented as the centroid of its desendantsC i i iN∀ ∈ → For a news story di, the comparison procedure and inserting procedure based on indexing-tree are defined as follows.",
                "An example is shown by Figure 1 and Figure 2.",
                "Figure 1.",
                "Comparison procedure Figure 2.",
                "Inserting procedure Comparison procedure: Step 1: compare di to all the direct child nodes of r and select λ nodes with highest similarities, e.g., C1 2 and C1 3 in Figure 1.",
                "Step 2: for each selected node in the last step, e.g.",
                "C1 2, compare di to all its direct child nodes, and select λ nodes with highest similarities, e.g.",
                "C2 2 and d8.",
                "Repeat step 2 for all non-terminal nodes.",
                "Step 3: record the terminal node with the highest similarty to di, e.g. s5, and the similarity value (0.20).",
                "Inserting di to the S-tree with r as root: Find the node n which is direct child of r in the path from r to the terminal node with highest similarity s, e.g.",
                "C1 2.",
                "If s is smaller than θ init+(h-1)δ , then add di to the tree as a direct child of r. Otherwise, if n is a terminal node, then create a cluster node instead of n, and add both n and di as its direct children; if n is an non-terminal node, then repeat this procedure and insert di to the sub-tree with n as root recursively.",
                "Here h is the length between n and the root of S-tree.",
                "The more the stories in a cluster similar to each other, the better the cluster represents the stories in it.",
                "Hence we add no constraints on the maximum of trees height and degree of a node.",
                "Therefore, we cannot give the complexity of this indexing-tree based procedure.",
                "But we will give the number of comparing times needed by the new procedure in our experiments in section7. 5.",
                "Term Reweighting Methods In this section, two term reweighting methods are proposed to improve NED accuracy.",
                "In the first method, a new way is explored for better using of cluster (topic) information.",
                "The second one finds a better way to make use of named entities based on news classification. 5.1 Term Reweighting Based on Distribution Distance TF-IDF is the most prevalent model used in information retrieval systems.",
                "The basic idea is that the fewer documents a term appears in, the more important the term is in discrimination of documents (relevant or not relevant to a query containing the term).",
                "Nevertheless, in TDT domain, we need to discriminate documents with regard to topics rather than queries.",
                "Intuitively, using cluster (topic) vectors to compare with subsequent news stories should outperform using story vectors.",
                "Unfortunately, the experimental results do not support this intuition [4][5].",
                "Based on observation on data, we find the reason is that a news topic usually contains many directly or indirectly related events, while they all have their own sub-subjects which are usually different with each other.",
                "Take the topic described in section 1 as an example, events like the explosion and salvage have very low similarities with events about criminal trial, therefore stories about trial would have low similarity with the topic vector built on its previous events.",
                "This section focuses on how to effectively make use of topic information and at the same time avoid the problem of content decentralization.",
                "At first, we classify terms into 5 classes to help analysis the needs of the modified model: Term class A: terms that occur frequently in the whole corpus, e.g., year and people.",
                "Terms of this class should be given low weights because they do not help much for topic discrimination.",
                "Term class B: terms that occur frequently within a news category, e.g., election, storm.",
                "They are useful to distinguish two stories in different news categories.",
                "However, they cannot provide information to determine whether two stories are on the same or different topics.",
                "In another words, term election and term storm are not helpful in differentiate two election campaigns and two storm disasters.",
                "Therefore, terms of this class should be assigned lower weights.",
                "Term class C: terms that occur frequently in a topic, and infrequently in other topics, e.g., the name of a crash plane, the name of a specific hurricane.",
                "News stories that belong to different topics rarely have overlap terms in this class.",
                "The more frequently a term appears in a topic, the more important the term is for a story belonging to the topic, therefore the term should be set higher weight.",
                "Term class D: terms that appear in a topic exclusively, but not frequently.",
                "For example, the name of a fireman who did very well in a salvage action, which may appears in only two or three stories but never appeared in other topics.",
                "Terms of this type should receive more weights than in TF-IDF model.",
                "However, since they are not popular in the topic, it is not appropriate to give them too high weights.",
                "Term class E: terms with low document frequency, and appear in different topics.",
                "Terms of this class should receive lower weights.",
                "Now we analyze whether TF-IDF model can give proper weights to the five classes of terms.",
                "Obviously, terms of class A are lowly weighted in TF-IDF model, which is conformable with the requirement described above.",
                "In TF-IDF model, terms of class B are highly dependant with the number of stories in a news class.",
                "TF-IDF model cannot provide low weights if the story containing the term belongs to a relative small news class.",
                "For a term of class C, the more frequently it appears in a topic, the less weight TFIDF model gives to it.",
                "This strongly conflicts with the requirement of terms in class C. For terms of class D, TF-IDF model gives them high weights correctly.",
                "But for terms of class E, TF-IDF model gives high weights to them which are not conformable with the requirement of low weights.",
                "To sum up, terms of class B, C, E cannot be properly weighted in TF-IDF model.",
                "So, we propose a modified model to resolve this problem.",
                "When θ init andθ new are set closely, we assume that most of the stories in a first-level cluster (a direct child node of root node) are on the same topic.",
                "Therefore, we make use of a first-level cluster to capture term distribution (df for all the terms within the cluster) within the topic dynamically.",
                "KL divergence of term distribution in a first-level cluster and the whole story set is used to adjust term weights:  ( , , ) * (1 * ( || )) ( , , ) ( , , ) * (1 * ( || )) cw tw cw tw w d D weight d t w KL P P weight d t w weight d t w KL P P γ γ ∈ + = +∑ (5) where ( ) ( ) ( ) ( ) 1,cw cw c c c c df w df w p y p y N N = = − (6) ( ) ( ) ( ) ( ) 1,t t tw tw t t df w df w p y p y N N = = − (7) where dfc(w) is the number of documents containing term w within cluster C, and Nc is the number of documents in cluster C, and Nt is the total number of documents that arrive before time step t. γ is a const parameter, now is manually set 3.",
                "KL divergence is defined as follows [17]: ( ) ( || ) ( ) log ( )x p x KL P Q p x q x = ∑ (8) The basic idea is: for a story in a topic, the more a term occurs within the topic, and the less it occurs in other topics, it should be assigned higher weights.",
                "Obviously, modified model can meet all the requirements of the five term classes listed above. 5.2 Term Reweighting Based on Term Type and Story Class Previous work found that some classes of news stories could achieve good improvements by giving extra weight to named entities.",
                "But we find that terms of different types should be given different amount of extra weight for different classes of news stories.",
                "We use open-NLP1 to recognize named entity types and part-ofspeech tags for terms that appear in news stories.",
                "Named entity types include person name, organization name, location name, date, time, money and percentage, and five POSs are selected: none (NN), verb (VB), adjective (JJ), adverb (RB) and cardinal number (CD).",
                "Statistical analysis shows topic-level discriminative terms types for different classes of stories.",
                "For the sake of convenience, named entity type and part-of-speech tags are uniformly called term type in subsequent sections.",
                "Determining whether two stories are about the same topic is a basic component for NED task.",
                "So at first we use 2 χ statistic to compute correlations between terms and topics.",
                "For a term t and a topic T, a contingence table is derived: Table 1.",
                "A 2×2 Contingence Table Doc Number belong to topic T not belong to topic T include t A B not include t C D The 2 χ statistic for a specific term t with respect to topic T is defined to be [16]: 2 2 ( , ) ( ) * ( ) ( ) * ( ) * ( ) * ( ) w T A B C D AD CB A C B D A B C D χ = + + + − + + + + (9) News topics for the TDT task are further classified into 11 rules of interpretations (ROIs) 2 .",
                "The ROI can be seen as a higher level class of stories.",
                "The average correlation between a term type and a topic ROI is computed as: 2 avg 2 ( , )( ( , ) )k m m km kT R w P w TP R p w T R P χ χ ∈ ∈ ∑ ∑（ , ）= 1 1 k=1…K, m=1…M (10) where K is the number of term types (set 12 constantly in the paper).",
                "M is the number news classes (ROIs, set 11 in the paper).",
                "Pk represents the set of all terms of type k, and Rm represents the set of all topics of class m, p(t,T) means the probability that t occurs in topic T. Because of limitation of space, only parts of the term types (9 term types) and parts of news classes (8 classes) are listed in table 2 with the average correlation values between them.",
                "The statistics is derived from labeled data in TDT2 corpus. (Results in table 2 are already normalized for convenience in comparison.)",
                "The statistics in table 2 indicates the usefulness of different term types in topic discrimination with respect to different news classes.",
                "We can see that, location name is the most useful term type for three news classes: Natural Disasters, Violence or War, Finances.",
                "And for three other categories Elections, Legal/Criminal Cases, Science and Discovery, person name is the most discriminative term type.",
                "For Scandals/Hearings, date is the most important information for topic discrimination.",
                "In addition, Legal/Criminal Cases and Finance topics have higher correlation with money terms, while Science and Discovery have higher correlation with percentage terms.",
                "Non-name terms are more stable for different classes. 1 . http://opennlp.sourceforge.net/ 2 . http://projects.ldc.upenn.edu/TDT3/Guide/label.html From the analysis of table 2, it is reasonable to adjust term weight according to their term type and the news class the story belongs to.",
                "New term weights are reweighted as follows: ( ) ( ) ( ) ( ) ( , , ) * ( , , ) ( , , ) * class d D type w T class d D type w w d weight d t w weight d t w weight d t w α α ∈ = ∑ (11) where type(w) represents the type of term w, and class(d) represents the class of story d, c kα is reweighting parameter for news class c and term type k. In the work, we just simply use statistics in table 2 as the reweighting parameters.",
                "Even thought using the statistics directly may not the best choice, we do not discuss how to automatically obtain the best parameters.",
                "We will try to use machine learning techniques to obtain the best parameters in the future work.",
                "In the work, we use BoosTexter [20] to classify all stories into one of the 11 ROIs.",
                "BoosTexter is a boosting based machine learning program, which creates a series of simple rules for building a classifier for text or attribute-value data.",
                "We use term weight generated using TF-IDF model as feature for story classification.",
                "We trained the model on the 12000 judged English stories in TDT2, and classify the rest of the stories in TDT2 and all stories in TDT3.",
                "Classification results are used for term reweighting in formula (11).",
                "Since the class labels of topic-off stories are not given in TDT datasets, we cannot give the classification accuracy here.",
                "Thus we do not discuss the effects of classification accuracy to NED performance in the paper. 6.",
                "EXPERIMENTAL SETUP 6.1 Datasets We used two LDC [18] datasets TDT2 and TDT3 for our experiments.",
                "TDT2 contains news stories from January to June 1998.",
                "It contains around 54,000 stories from sources like ABC, Associated Press, CNN, New York Times, Public Radio International, Voice of America etc.",
                "Only English stories in the collection were considered.",
                "TDT3 contains approximately 31,000 English stories collected from October to December 1998.",
                "In addition to the sources used in TDT2, it also contains stories from NBC and MSNBC TV broadcasts.",
                "We used transcribed versions of the TV and radio broadcasts besides textual news.",
                "TDT2 dataset is labeled with about 100 topics, and approximately 12,000 English stories belong to at least one of these topics.",
                "TDT3 dataset is labeled with about 120 topics, and approximately 8000 English stories belong to at least one of these topics.",
                "All the topics are classified into 11 Rules of Interpretation: (1)Elections, (2)Scandals/Hearings, (3)Legal/Criminal Cases, (4)Natural Disasters, (5)Accidents, (6)Ongoing Violence or War, (7)Science and Discovery News, (8)Finance, (9)New Law, (10)Sports News, (11)MISC.",
                "News. 6.2 Evaluation Metric TDT uses a cost function CDet that combines the probabilities of missing a new story and a false alarm [19]: * * * *Det Miss Miss Target FA FA NontargetC C P P C P P= + (12) Table 2.",
                "Average correlation between term types and news classes where CMiss means the cost of missing a new story, PMiss means the probability of missing a new story, and PTarget means the probability of seeing a new story in the data; CFA means the cost of a false alarm, PFA means the probability of a false alarm, and PNontarget means the probability of seeing an old story.",
                "The cost CDet is normalized such that a perfect system scores 0 and a trivial system, which is the better one of mark all stories as new or old, scores 1: ( ( * , * ) ) Det Det Miss Target FA Nontarget C Norm C min C P C P = (13) New event detection system gives two outputs for each story.",
                "The first part is yes or no indicating whether the story triggers a new event or not.",
                "The second part is a score indicating confidence of the first decision.",
                "Confidence scores can be used to plot DET curve, i.e., curves that plot false alarm vs. miss probabilities.",
                "Minimum normalized cost can be determined if optimal threshold on the score were chosen. 7.",
                "EXPERIMENTAL RESULTS 7.1 Main Results To test the approaches proposed in the model, we implemented and tested five systems: System-1: this system is used as baseline.",
                "It is implemented based on the basic model described in section 3, i.e., using incremental TF-IDF model to generate term weights, and using Hellinger distance to compute document similarity.",
                "Similarity score normalization is also employed [8].",
                "S-S detection procedure is used.",
                "System-2: this system is the same as system-1 except that S-C detection procedure is used.",
                "System-3: this system is the same as system-1 except that it uses the new detection procedure which is based on indexing-tree.",
                "System-4: implemented based on the approach presented in section 5.1, i.e., terms are reweighted according to the distance between term distributions in a cluster and all stories.",
                "The new detection procedure is used.",
                "System-5: implemented based on the approach presented in section 5.2, i.e., terms of different types are reweighted according to news class using trained parameters.",
                "The new detection procedure is used.",
                "The following are some other NED systems: System-6: [21] for each pair of stories, it computes three similarity values for named entity, non-named entity and all terms respectively.",
                "And employ Support Vector Machine to predict new or old using the similarity values as features.",
                "System-7: [8] it extended a basic incremental TF-IDF model to include source-specific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, etc.",
                "System-8: [13] it split document representation into two parts: named entities and non-named entities, and choose one effective part for each news class.",
                "Table 3 and table 4 show topic-weighted normalized costs and comparing times on TDT2 and TDT3 datasets respectively.",
                "Since no heldout data set for fine-tuning the threshold θ new was available for experiments on TDT2, we only report minimum normalized costs for our systems in table 3.",
                "System-5 outperforms all other systems including system-6, and it performs only 2.78e+8 comparing times in detection procedure which is only 13.4% of system-1.",
                "Table 3.",
                "NED results on TDT2 Systems Min Norm(CDet) Cmp times System-1 0.5749 2.08e+9 System-2① 0.6673 3.77e+8 System-3② 0.5765 2.81e+8 System-4② 0.5431 2.99e+8 System-5② 0.5089 2.78e+8 System-6 0.5300 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 When evaluating on the normalized costs on TDT3, we use the optimal thresholds obtained from TDT2 data set for all systems.",
                "System-2 reduces comparing times to 1.29e+9 which is just 18.3% of system-1, but at the same time it also gets a deteriorated minimum normalized cost which is 0.0499 higher than system-1.",
                "System-3 uses the new detection procedure based on news indexing-tree.",
                "It requires even less comparing times than system-2.",
                "This is because story-story comparisons usually yield greater similarities than story-cluster ones, so stories tend to be combined Location Person Date Organization Money Percentage NN JJ CD Elections 0.37 1 0.04 0.58 0.08 0.03 0.32 0.13 0.1 Scandals/Hearings 0.66 0.62 0.28 1 0.11 0.02 0.27 0.13 0.05 Legal/Criminal Cases 0.48 1 0.02 0.62 0.15 0 0.22 0.24 0.09 Natural Disasters 1 0.27 0 0.04 0.04 0 0.25 0.04 0.02 Violence or War 1 0.36 0.02 0.14 0.02 0.04 0.21 0.11 0.02 Science and Discovery 0.11 1 0.01 0.22 0.08 0.12 0.19 0.08 0.03 Finances 1 0.45 0.04 0.98 0.13 0.02 0.29 0.06 0.05 Sports 0.16 0.27 0.01 1 0.02 0 0.11 0.03 0.01 together in system-3.",
                "And system-3 is basically equivalent to system-1 in accuracy results.",
                "System-4 adjusts term weights based on the distance of term distributions between the whole corpus and cluster story set, yielding a good improvement by 0.0468 compared to system-1.",
                "The best system (system-5) has a minimum normalized cost 0.5012, which is 0.0797 better than system-1, and also better than any other results previously reported for this dataset [8, 13].",
                "Further more, system-5 only needs 1.05e+8 comparing times which is 14.9% of system-1.",
                "Table 4.",
                "NED results on TDT3 Systems Norm(CDet) Min Norm(CDet) Cmp times System-1 0.6159 0.5809 7.04e+8 System-2① 0.6493 0.6308 1.29e+8 System-3② 0.6197 0.5868 1.03e+8 System-4② 0.5601 0.5341 1.03e+8 System-5② 0.5413 0.5012 1.05e+8 System-7 -- 0.5783 -System-8 -- 0.5229 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 Figure5 shows the five DET curves for our systems on data set TDT3.",
                "System-5 achieves the minimum cost at a false alarm rate of 0.0157 and a miss rate of 0.4310.",
                "We can observe that System4 and System-5 obtain lower miss probability at regions of low false alarm probabilities.",
                "The hypothesis is that, more weight value is transferred to key terms of topics from non-key terms.",
                "Similarity score between two stories belonging to different topics are lower than before, because their overlapping terms are usually not key terms of their topics. 7.2 Parameter selection for indexing-tree detection Figure 3 shows the minimum normalized costs obtained by system-3 on TDT3 using different parameters.",
                "Theθ init parameter is tested on six values spanning from 0.03 to 0.18.",
                "And the λ parameter is tested on four values 1, 2, 3 and 4.",
                "We can see that, whenθ init is set to 0.12, which is the closest one toθ new, the costs are lower than others.",
                "This is easy to explain, because when stories belonging to the same topic are put in a cluster, it is more reasonable for the cluster to represent the stories in it.",
                "When parameter λ is set to 3 or 4, the costs are better than other cases, but there is no much difference between 3 and 4. 0 0.05 0.1 0.15 0.2 1 2 3 4 0.5 0.6 0.7 0.8 0.9 1 θ-initλ MinCost 0.6 0.65 0.7 0.75 0.8 0.85 0.9 Figure 3.",
                "Min Cost on TDT3 (δ =0.15) 0 0.05 0.1 0.15 0.2 1 2 3 4 0 0.5 1 1.5 2 2.5 x 10 8 θ-init λ Comparingtimes 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 8 Figure 4.",
                "Comparing times on TDT3 (δ =0.15) Figure 4 gives the comparing times used by system-3 on TDT3 with the same parameters as figure 3.",
                "The comparing times are strongly dependent onθ init.",
                "Because the greaterθ init is, the less stories combined together, the more comparing times are needed for new event decision.",
                "So we useθ init =0.13,λ =3,δ =0.15 for system-3, 4, and 5.",
                "In this parameter setting, we can get both low minimum normalized costs and less comparing times. 8.",
                "CONCLUSION We have proposed a news indexing-tree based detection procedure in our model.",
                "It reduces comparing times to about one seventh of traditional method without hurting NED accuracy.",
                "We also have presented two extensions to the basic TF-IDF model.",
                "The first extension is made by adjust term weights based on term distributions between the whole corpus and a cluster story set.",
                "And the second extension to basic TF-IDF model is better use of term types (named entities types and part-of-speed) according to news categories.",
                "Our experimental results on TDT2 and TDT3 datasets show that both of the two extensions contribute significantly to improvement in accuracy.",
                "We did not consider news time information as a clue for NED task, since most of the topics last for a long time and TDT data sets only span for a relative short period (no more than 6 months).",
                "For the future work, we want to collect news set which span for a longer period from internet, and integrate time information in NED task.",
                "Since topic is a relative coarse-grained news cluster, we also want to refine cluster granularity to event-level, and identify different events and their relations within a topic.",
                "Acknowledgments This work is supported by the National Natural Science Foundation of China under Grant No. 90604025.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsor. 9.",
                "REFERENCES [1] http://www.nist.gov/speech/tests/tdt/index.htm [2] In Topic Detection and Tracking.",
                "Event-based Information Organization.",
                "Kluwer Academic Publishers, 2002. .01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90 1 2 5 10 20 40 60 80 90 False Alarm Probability (in %) MissProbability(in%) SYSTEM1 Topic Weighted Curve SYSTEM1 Min Norm(Cost) SYSTEM2 Topic Weighted Curve SYSTEM2 Min Norm(Cost) SYSTEM3 Topic Weighted Curve SYSTEM3 Min Norm(Cost) SYSTEM4 Topic Weighted Curve SYSTEM4 Min Norm(Cost) SYSTEM5 Topic Weighted Curve SYSTEM5 Min Norm(Cost) Random Performance Figure 5.",
                "DET curves on TDT3 [3] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B.T.",
                "Archibald, and X. Liu.",
                "Learning Approaches for Detecting and Tracking News Events.",
                "In IEEE Intelligent Systems Special Issue on Applications of Intelligent Information Retrieval, volume 14 (4), 1999, 32-43. [4] Y. Yang, T. Pierce, and J. Carbonell.",
                "A Study on Retrospective and On-line Event Detection.",
                "In Proceedings of SIGIR-98, Melbourne, Australia, 1998, 28-36. [5] J. Allan, V. Lavrenko, D. Malin, and R. Swan.",
                "Detections, Bounds, and Timelines: Umass and tdt-3.",
                "In Proceedings of Topic Detection and Tracking Workshop (TDT-3), Vienna, VA, 2000, 167-174. [6] R. Papka and J. Allan.",
                "On-line New Event Detection Using Single Pass Clustering TITLE2:.",
                "Technical Report UM-CS1998-021, 1998. [7] W. Lam, H. Meng, K. Wong, and J.",
                "Yen.",
                "Using Contextual Analysis for News Event Detection.",
                "International Journal on Intelligent Systems, 2001, 525-546. [8] B. Thorsten, C. Francine, and F. Ayman.",
                "A System for New Event Detection.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2003, 330-337. [9] S. Nicola and C. Joe.",
                "Combining Semantic and Syntactic Document Classifiers to Improve First Story Detection.",
                "In Proceedings of the 24th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topicconditioned Novelty Detection.",
                "In Proceedings of the 8th ACM SIGKDD International Conference, ACM Press. 2002, 688-693. [11] M. Juha, A.M. Helena, and S. Marko.",
                "Applying Semantic Classes in Event Detection and Tracking.",
                "In Proceedings of International Conference on Natural Language Processing (ICON 2002), 2002, pages 175-183. [12] M. Juha, A.M. Helena, and S. Marko.",
                "Simple Semantics in Topic Detection and Tracking.",
                "Information Retrieval, 7(3-4): 2004, 347-368. [13] K. Giridhar and J. Allan.",
                "Text Classification and Named Entities for New Event Detection.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2004, 297-304. [14] J. P. Callan, W. B. Croft, and S. M. Harding.",
                "The INQUERY Retrieval System.",
                "In Proceedings of DEXA-92, 3rd International Conference on Database and Expert Systems Applications, 1992, 78-83. [15] R. Krovetz.",
                "Viewing Morphology as An Inference Process.",
                "In Proceedings of ACM SIGIR93, 1993, 61-81. [16] Y. Yang and J. Pedersen.",
                "A Comparative Study on Feature Selection in Text Categorization.",
                "In J. D. H. Fisher, editor, The Fourteenth International Conference on Machine Learning (ICML97), Morgan Kaufmann, 1997, 412-420. [17] T. M. Cover, and J.A.",
                "Thomas.",
                "Elements of Information Theory.",
                "Wiley. 1991. [18] The linguistic data consortium, http://www.ldc,upenn.edu/. [19] The 2001 TDT task definition and evaluation plan, http://www.nist.gov/speech/tests/tdt/tdt2001/evalplan.htm. [20] R. E. Schapire and Y.",
                "Singer.",
                "Boostexter: A Boosting-based System for Text Categorization.",
                "In Machine Learning 39(2/3):1, Kluwer Academic Publishers, 2000, 35-168. [21] K. Giridhar and J. Allan. 2005.",
                "Using Names and Topics for New Event Detection.",
                "In Proceedings of Human Technology Conference and Conference on Empirical Methods in Natural Language, Vancouver, 2005, 121-128"
            ],
            "original_annotated_samples": [
                "Moreover, based on the observation that terms of different types have different effects for NED task, two <br>term reweighting approach</br>es are proposed to improve NED accuracy."
            ],
            "translated_annotated_samples": [
                "Además, basándose en la observación de que los términos de diferentes tipos tienen diferentes efectos para la tarea de NED, se proponen dos <br>enfoques de reponderación de términos</br> para mejorar la precisión de NED."
            ],
            "translated_text": "Detección de nuevos eventos basada en árbol de indexación y entidad nombrada Zhang Kuo Universidad Tsinghua Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Universidad Tsinghua Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Universidad Tsinghua Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn RESUMEN La Detección de Nuevos Eventos (NED) tiene como objetivo detectar de una o varias corrientes de noticias cuál se informa sobre un nuevo evento (es decir, no reportado previamente). Con el abrumador volumen de noticias disponible hoy en día, hay una creciente necesidad de un sistema NED que sea capaz de detectar eventos nuevos de manera más eficiente y precisa. En este artículo proponemos un nuevo modelo de NED para acelerar la tarea de NED mediante el uso dinámico de un árbol de indexación de noticias. Además, basándose en la observación de que los términos de diferentes tipos tienen diferentes efectos para la tarea de NED, se proponen dos <br>enfoques de reponderación de términos</br> para mejorar la precisión de NED. En el primer enfoque, proponemos ajustar dinámicamente los pesos de los términos basados en los clusters de historias previas y en el segundo enfoque, proponemos emplear estadísticas en los datos de entrenamiento para aprender el modelo de reajuste de entidades nombradas para cada clase de historias. Los resultados experimentales en dos conjuntos de datos del Consorcio de Datos Lingüísticos (LDC), TDT2 y TDT3, muestran que el modelo propuesto puede mejorar significativamente tanto la eficiencia como la precisión de la tarea de NED, en comparación con el sistema base y otros sistemas existentes. Categorías y Descriptores de Asignaturas H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información; H.4.2 [Aplicaciones de Sistemas de Información]: Tipos de Sistemas de Apoyo a Decisiones. Términos generales Algoritmos, Rendimiento, Experimentación 1. El programa de Detección y Seguimiento de Temas (TDT) tiene como objetivo desarrollar técnicas que puedan organizar, buscar y estructurar de manera efectiva materiales de texto de noticias de una variedad de agencias de noticias y medios de difusión [1]. La Detección de Eventos Nuevos (DEN) es una de las cinco tareas en TDT. Es la tarea de identificación en línea del informe más temprano para cada tema tan pronto como ese informe llegue en la secuencia de documentos. Un tema se define como un evento o actividad seminal, junto con eventos y actividades directamente relacionados [2]. Un evento se define como algo (no trivial) que sucede en un lugar y en un momento determinados [3]. Por ejemplo, cuando una bomba explota en un edificio, la explosión es el evento seminal que desencadena el tema, y otras historias sobre el mismo tema serían aquellas que discuten los esfuerzos de salvamento, la búsqueda de los perpetradores, arrestos y juicios, entre otros. La información útil de noticias suele estar enterrada en una masa de datos generados a diario. Por lo tanto, los sistemas NED son muy útiles para las personas que necesitan detectar información novedosa de un flujo de noticias en tiempo real. Estas necesidades de la vida real a menudo se presentan en ámbitos como los mercados financieros, el análisis de noticias y la recopilación de inteligencia. En la mayoría de los sistemas NED de última generación, cada noticia disponible se compara con todas las noticias previamente recibidas. Si todas las similitudes entre ellos no superan un umbral, entonces la historia desencadena un nuevo evento. Por lo general, se presentan en forma de similitud de coseno o métrica de similitud de Hellinger. El problema central de NED es identificar si dos historias tratan sobre el mismo tema. Obviamente, estos sistemas no pueden aprovechar la información del tema. Además, no es aceptable en aplicaciones reales debido a la gran cantidad de cálculos requeridos en el proceso de NED. Otros sistemas organizan historias anteriores en grupos (cada grupo corresponde a un tema), y la nueva historia se compara con los grupos anteriores en lugar de con historias. Esta forma puede reducir significativamente los tiempos de comparación. Sin embargo, se ha demostrado que este método es menos preciso [4, 5]. Esto se debe a que a veces las historias dentro de un tema se alejan mucho entre sí, lo que podría resultar en una baja similitud entre una historia y su tema. Por otro lado, algunos sistemas NED propuestos intentaron mejorar la precisión al hacer un mejor uso de las entidades nombradas [10, 11, 12, 13]. Sin embargo, ninguno de los sistemas ha considerado que los términos de diferentes tipos (por ejemplo, Los sustantivos, verbos o nombres de personas tienen diferentes efectos en diferentes clases de historias para determinar si dos historias tratan sobre el mismo tema. Por ejemplo, los nombres de los candidatos electorales (Nombre de la persona) son muy importantes para las historias de la clase de elecciones; las ubicaciones (Nombre de la ubicación) donde ocurrieron los accidentes son importantes para las historias de la clase de accidentes. Por lo tanto, en NED, todavía existen los siguientes tres problemas por investigar: (1) ¿Cómo acelerar el procedimiento de detección sin disminuir la precisión de la detección? (2) ¿Cómo aprovechar mejor la información de los grupos (temas) para mejorar la precisión? (3) ¿Cómo obtener una mejor representación de historias de noticias mediante una mejor comprensión de las entidades nombradas? Impulsados por estos problemas, hemos propuesto tres enfoques en este documento. (1) Para hacer que el procedimiento de detección sea más rápido, proponemos un nuevo procedimiento de NED basado en un árbol de indexación de noticias creado dinámicamente. El índice de historias en árbol se crea ensamblando historias similares para formar grupos de noticias en diferentes jerarquías según sus valores de similitud. Las comparaciones entre la historia actual y los grupos anteriores podrían ayudar a encontrar la historia más similar en menos tiempo de comparación. El nuevo procedimiento puede reducir la cantidad de veces de comparación sin afectar la precisión. Utilizamos los grupos del primer piso en el árbol de indexación como temas de noticias, en los cuales los pesos de los términos se ajustan dinámicamente según la distribución de términos en los grupos. En este enfoque, se utiliza adecuadamente la información de los grupos (temas), por lo que se evita el problema de la descentralización del tema. Basándonos en observaciones de las estadísticas obtenidas de los datos de entrenamiento, encontramos que los términos de diferentes tipos (por ejemplo, Los sustantivos y verbos tienen diferentes efectos para diferentes clases de historias al determinar si dos historias tratan sobre el mismo tema. Y proponemos utilizar estadísticas para optimizar los pesos de los términos de diferentes tipos en una historia de acuerdo a la clase de noticias a la que pertenece la historia. En el conjunto de datos TDT3, el nuevo modelo NED solo utiliza un 14.9% en comparación con los tiempos del modelo básico, mientras que su costo normalizado mínimo es de 0.5012, lo que es 0.0797 mejor que el modelo básico, y también mejor que cualquier otro resultado previamente reportado para este conjunto de datos [8, 13]. El resto del documento está organizado de la siguiente manera. Comenzamos este artículo resumiendo el trabajo previo en NED en la sección 2. La sección 3 presenta el modelo básico para NED que la mayoría de los sistemas actuales utilizan. La sección 4 describe nuestro nuevo procedimiento de detección basado en el índice de árbol de noticias. En la sección 5, se proponen dos métodos de reponderación de términos para mejorar la precisión de NED. La sección 6 presenta nuestros datos experimentales y métricas de evaluación. Finalmente concluimos con los resultados experimentales en la Sección 7, y las conclusiones y trabajo futuro en la Sección 8. TRABAJO RELACIONADO Papka et al. propusieron el agrupamiento de un solo paso en NED [6]. Cuando se encontraba una nueva historia, esta era procesada inmediatamente para extraer características de términos y se construía una representación de consulta del contenido de la historia. Luego se comparó con todas las consultas anteriores. Si el documento no generaba ninguna consulta al exceder un umbral, se marcaba como un nuevo evento. Lam et al construyen representaciones de consultas previas de grupos de historias, cada una de las cuales corresponde a un tema [7]. De esta manera se realizan comparaciones entre historias y grupos. En los últimos años, la mayoría de los trabajos se han centrado en proponer mejores métodos para la comparación de historias y la representación de documentos. Brants et al. [8] extendieron un modelo básico incremental de TF-IDF para incluir modelos específicos de fuente, normalización de puntajes de similitud basada en promedios específicos de documentos, normalización de puntajes de similitud basada en promedios específicos de pares de fuentes, reponderación de términos basada en frecuencias de eventos inversas y segmentación de documentos. Se mostraron buenas mejoras en los puntos de referencia de TDT. Stokes et al. [9] utilizaron una combinación de evidencia de dos representaciones distintas del contenido de un documento. Una de las representaciones era el vector de texto libre habitual, la otra hacía uso de cadenas léxicas (creadas utilizando WordNet) para construir otro vector de términos. Luego, las dos representaciones se combinan de forma lineal. Se logró un aumento marginal en la efectividad cuando se utilizó la representación combinada. Algunos esfuerzos se han realizado sobre cómo utilizar entidades nombradas para mejorar la desambiguación de entidades nombradas (NED). Yang et al. asignaron un peso cuatro veces mayor a las entidades con nombres de ubicación que a otros términos y entidades con nombres [10]. El grupo de investigación DOREMI combinó similitudes semánticas de nombres de personas, nombres de lugares y tiempo junto con similitud textual [11][12]. El grupo de investigación de UMass [13] dividió la representación del documento en dos partes: entidades nombradas y entidades no nombradas. Y se descubrió que algunas clases de noticias podían lograr un mejor rendimiento utilizando la representación de entidades nombradas, mientras que otras clases de noticias podían lograr un mejor rendimiento utilizando la representación sin entidades nombradas. Tanto [10] como [13] utilizaron la técnica de categorización de texto para clasificar previamente las noticias. En [13] las noticias se clasifican automáticamente al principio, y luego se prueban las sensibilidades de los nombres y términos que no son nombres para NED para cada clase. En [10], se eliminan los términos frecuentes de cada clase de la representación del documento. Por ejemplo, la palabra \"elección\" no ayuda a identificar diferentes elecciones. En su trabajo, no se investiga la efectividad de diferentes tipos de nombres (o términos con diferentes categorías gramaticales) para NED en diferentes clases de noticias. Utilizamos análisis estadístico para revelar el hecho y utilizarlo para mejorar el rendimiento de NED. 3. MODELO BÁSICO En esta sección, presentamos el modelo básico de Detección de Nuevos Eventos que es similar a lo que la mayoría de los sistemas actuales aplican. Luego, proponemos nuestro nuevo modelo ampliando el modelo básico. Los sistemas de detección de eventos nuevos utilizan flujos de noticias como entrada, en los cuales las historias están estrictamente ordenadas por tiempo. Solo están disponibles las historias recibidas previamente al tratar con la historia actual. La salida es una decisión sobre si la historia actual es un evento nuevo o no y la confianza de la decisión. Por lo general, un modelo NED consta de tres partes: representación de la historia, cálculo de similitud y procedimiento de detección. 3.1 Representación de la historia Se necesita preprocesamiento antes de generar la representación de la historia. Para el preprocesamiento, tokenizamos palabras, reconocemos abreviaturas, normalizamos abreviaturas, agregamos etiquetas de partes del discurso, eliminamos palabras vacías incluidas en la lista de paradas utilizada en InQuery [14], reemplazamos palabras con sus raíces utilizando el algoritmo K-stem[15], y luego generamos un vector de palabras para cada noticia. Utilizamos el modelo TF-IDF incremental para el cálculo del peso de los términos [4]. En un modelo TF-IDF, la frecuencia de término en un documento de noticias se pondera por la frecuencia inversa del documento, la cual se genera a partir del corpus de entrenamiento. Cuando surge un nuevo término en el proceso de prueba, hay dos soluciones: simplemente ignorar el nuevo término o establecer df del término como una constante pequeña (por ejemplo, df = 1). El nuevo término recibe un peso demasiado bajo en la primera solución (0) y un peso demasiado alto en la segunda solución. En el modelo TF-IDF incremental, las frecuencias de documentos se actualizan dinámicamente en cada paso de tiempo t: 1( ) ( ) ( )t t D tdf w df w df w−= + (1) donde Dt representa el conjunto de noticias recibidas en el tiempo t, y dfDt(w) significa el número de documentos en los que aparece el término w, y dft(w) significa el número total de documentos en los que aparece el término w antes del tiempo t. En este trabajo, cada ventana de tiempo incluye 50 noticias. Así, cada historia d recibida en t se representa de la siguiente manera: 1 2{ ( , , ), ( , , ),..., ( , , )}nd peso d t w peso d t w peso d t w→ donde n significa el número de términos distintos en la historia d, y ( , , )peso d t w significa el peso del término w en la historia d en el tiempo t: log( ( , ) 1) log(( 1) /( ( ) 0.5)) ( , , ) log( ( , ) 1) log(( 1) /( ( ) 0.5)) t t t t w d tf d w N df w peso d t w tf d w N df w ∈ + + + = + + +∑ (2) donde Nt significa el número total de historias de noticias antes del tiempo t, y tf(d,w) significa cuántas veces aparece el término w en la historia de noticias d. 3.2 Cálculo de Similitud Utilizamos la distancia de Hellinger para el cálculo de la similitud entre dos historias, para dos historias d y d en el tiempo t, su similitud se define de la siguiente manera: , ( , , ) ( , , ) * ( , , ) w d d sim d d t peso d t w peso d t w ∈ = ∑ (3) 3.3 Procedimiento de Detección Para cada historia d recibida en el paso de tiempo t, el valor ( ) ( ) ( ) ( ( , , )) tiempo d tiempo d n d max sim d d t < = (4) es un puntaje utilizado para determinar si d es una historia sobre un nuevo tema y al mismo tiempo es una indicación de la confianza en nuestra decisión [8]. tiempo(d) significa el tiempo de publicación de la historia d. Si el puntaje supera el umbral θ new, entonces existe un documento suficientemente similar, por lo tanto, d es una historia antigua, de lo contrario, no hay un documento previo suficientemente similar, por lo tanto, d es una historia nueva. 4. Nuevo procedimiento NED Los sistemas NED tradicionales se pueden clasificar en dos tipos principales en cuanto al procedimiento de detección: (1) tipo S-S, en el que la historia en cuestión se compara con cada historia recibida previamente, y se utiliza la mayor similitud para determinar si la historia actual trata sobre un evento nuevo; (2) tipo S-C, en el que la historia en cuestión se compara con todos los clusters anteriores, cada uno de los cuales representa un tema, y se utiliza la mayor similitud para la decisión final sobre la historia actual. Si la similitud más alta supera el umbral θ nuevo, entonces es una historia antigua y se coloca en el clúster más similar; de lo contrario, es una historia nueva y se crea un nuevo clúster. Trabajos anteriores muestran que la primera forma es más precisa que la segunda [4][5]. Dado que a veces las historias dentro de un tema se alejan mucho entre sí, una historia puede tener una similitud muy baja con su tema. Por lo tanto, utilizar similitudes entre historias para determinar una nueva historia es mejor que utilizar similitudes entre historias y grupos. Sin embargo, el primer método requiere mucho más tiempo de comparación, lo que significa que es menos eficiente. Proponemos un nuevo procedimiento de detección que utiliza comparaciones con clusters anteriores para ayudar a encontrar la historia más similar en menos tiempo de comparación, y la decisión final sobre el nuevo evento se toma de acuerdo con la historia más similar. Por lo tanto, podemos obtener tanto la precisión de los métodos de tipo S-S como la eficiencia de los métodos de tipo S-C. El nuevo procedimiento crea un árbol de indexación de noticias dinámicamente, en el cual se agrupan historias similares para formar una jerarquía de clusters. Indexamos historias similares juntas por su ancestro común (un nodo de clúster). Historias diferentes están indexadas en diferentes grupos. Cuando una historia está llegando, utilizamos comparaciones entre la historia actual y los grupos jerárquicos anteriores para ayudar a encontrar la historia más similar que es útil para la toma de decisiones sobre nuevos eventos. Después de que se tome la decisión sobre el nuevo evento, la historia actual se inserta en el árbol de indexación para la detección siguiente. El índice de noticias en forma de árbol se define formalmente de la siguiente manera: S-Tree = {r, NC, NS, E}, donde r es la raíz de S-Tree, NC es el conjunto de todos los nodos de clúster, NS es el conjunto de todos los nodos de historia, y E es el conjunto de todos los bordes en S-Tree. Definimos un conjunto de restricciones para un S-Árbol: ⅰ. , es un nodo no terminal en el árbolC i i N i∀ ∈ → ⅱ. , es un nodo terminal en el árbolS i i N i∀ ∈ → ⅲ. , el grado de salida de es al menos 2C i i N i∀ ∈ → ⅳ. , se representa como el centroide de sus descendientesC i i iN∀ ∈ → Para una noticia di, el procedimiento de comparación y el procedimiento de inserción basados en el árbol de indexación se definen de la siguiente manera. Un ejemplo se muestra en la Figura 1 y la Figura 2. Figura 1. Procedimiento de comparación Figura 2. Procedimiento de comparación de inserción: Paso 1: comparar di con todos los nodos hijos directos de r y seleccionar λ nodos con las similitudes más altas, por ejemplo, C1 2 y C1 3 en la Figura 1. Paso 2: para cada nodo seleccionado en el paso anterior, por ejemplo. C1 2, compara di con todos sus nodos hijos directos y selecciona λ nodos con las similitudes más altas, por ejemplo. C2 2 y d8. Repetir el paso 2 para todos los nodos no terminales. Paso 3: registrar el nodo terminal con la mayor similitud a di, por ejemplo, s5, y el valor de similitud (0.20). Insertando di en el S-árbol con r como raíz: Encuentra el nodo n que es hijo directo de r en el camino desde r hasta el nodo terminal con la similitud más alta s, por ejemplo. C1 2. Si s es menor que θ init + (h-1)δ, entonces agregar di al árbol como un hijo directo de r. De lo contrario, si n es un nodo terminal, crear un nodo de clúster en lugar de n, y agregar tanto n como di como sus hijos directos; si n es un nodo no terminal, repetir este procedimiento e insertar di en el subárbol con n como raíz de forma recursiva. Aquí h es la longitud entre n y la raíz del árbol S. Cuanto más similares sean las historias en un grupo, mejor representará el grupo a las historias en él. Por lo tanto, no imponemos restricciones en la altura máxima de los árboles y el grado de un nodo. Por lo tanto, no podemos dar la complejidad de este procedimiento basado en árboles de indexación. Pero proporcionaremos el número de veces de comparación necesarias por el nuevo procedimiento en nuestros experimentos en la sección 7.5. En esta sección, se proponen dos métodos de reponderación de términos para mejorar la precisión de NED. En el primer método, se explora una nueva forma de utilizar de manera más efectiva la información de los grupos (temas). El segundo encuentra una mejor manera de utilizar entidades nombradas basadas en la clasificación de noticias. 5.1 Reajuste de términos basado en la distancia de distribución TF-IDF es el modelo más prevalente utilizado en sistemas de recuperación de información. La idea básica es que cuanto menos documentos aparezca un término, más importante es el término en la discriminación de documentos (relevantes o no relevantes para una consulta que contenga el término). Sin embargo, en el dominio de TDT, necesitamos discriminar documentos en función de los temas en lugar de las consultas. Intuitivamente, el uso de vectores de clúster (tema) para comparar con historias de noticias posteriores debería tener un mejor rendimiento que el uso de vectores de historias. Desafortunadamente, los resultados experimentales no respaldan esta intuición [4][5]. Basándonos en la observación de los datos, encontramos que la razón es que un tema de noticias generalmente contiene muchos eventos directa o indirectamente relacionados, y todos ellos tienen sus propios subtemas que suelen ser diferentes entre sí. Tomemos el tema descrito en la sección 1 como ejemplo, eventos como la explosión y el rescate tienen similitudes muy bajas con eventos sobre juicios criminales, por lo tanto, las historias sobre juicios tendrían una baja similitud con el vector de tema construido en base a sus eventos previos. Esta sección se centra en cómo hacer un uso efectivo de la información del tema y al mismo tiempo evitar el problema de la descentralización del contenido. Al principio, clasificamos los términos en 5 clases para ayudar en el análisis de las necesidades del modelo modificado: Clase de términos A: términos que ocurren con frecuencia en todo el corpus, por ejemplo, año y personas. Los términos de esta clase deben recibir pesos bajos porque no ayudan mucho en la discriminación de temas. Clase B de términos: términos que ocurren con frecuencia dentro de una categoría de noticias, por ejemplo, elección, tormenta. Son útiles para distinguir dos historias en diferentes categorías de noticias. Sin embargo, no pueden proporcionar información para determinar si dos historias tratan sobre el mismo tema o temas diferentes. En otras palabras, los términos elección y tormenta no son útiles para diferenciar dos campañas electorales y dos desastres de tormenta. Por lo tanto, los términos de esta clase deberían asignarse pesos más bajos. Clase de término C: términos que ocurren con frecuencia en un tema y raramente en otros temas, por ejemplo, el nombre de un avión accidentado, el nombre de un huracán específico. Las noticias que pertenecen a diferentes temas rara vez tienen términos superpuestos en esta clase. Cuanto más frecuentemente aparezca un término en un tema, más importante es el término para una historia perteneciente al tema, por lo tanto, el término debería tener un peso más alto. Clase de término D: términos que aparecen en un tema de manera exclusiva, pero no con frecuencia. Por ejemplo, el nombre de un bombero que se desempeñó muy bien en una acción de salvamento, que puede aparecer en solo dos o tres historias pero nunca ha aparecido en otros temas. Los términos de este tipo deberían recibir más peso que en el modelo TF-IDF. Sin embargo, dado que no son populares en el tema, no es apropiado asignarles pesos demasiado altos. Clase de término E: términos con baja frecuencia en documentos y que aparecen en diferentes temas. Los términos de esta clase deberían recibir pesos más bajos. Ahora analizamos si el modelo TF-IDF puede asignar pesos adecuados a las cinco clases de términos. Obviamente, los términos de la clase A tienen un peso bajo en el modelo TF-IDF, lo cual es acorde con el requisito descrito anteriormente. En el modelo TF-IDF, los términos de la clase B dependen altamente del número de historias en una clase de noticias. El modelo TF-IDF no puede proporcionar pesos bajos si la historia que contiene el término pertenece a una clase de noticias relativamente pequeña. Para un término de clase C, cuanto más frecuentemente aparezca en un tema, menos peso le da el modelo TFIDF. Esto entra en conflicto directo con el requisito de términos en la clase C. Para los términos de la clase D, el modelo TF-IDF les asigna pesos altos de manera correcta. Pero para los términos de la clase E, el modelo TF-IDF asigna pesos altos a ellos que no son conformes con el requisito de pesos bajos. En resumen, los términos de clase B, C, E no pueden ser ponderados adecuadamente en el modelo TF-IDF. Por lo tanto, proponemos un modelo modificado para resolver este problema. Cuando θ init y θ new están configurados de cerca, asumimos que la mayoría de las historias en un clúster de primer nivel (un nodo hijo directo del nodo raíz) están en el mismo tema. Por lo tanto, utilizamos un clúster de primer nivel para capturar la distribución de términos (df para todos los términos dentro del clúster) dentro del tema de forma dinámica. La divergencia de Kullback-Leibler de la distribución de términos en un clúster de primer nivel y el conjunto completo de historias se utiliza para ajustar los pesos de los términos: ( , , ) * (1 * ( || )) ( , , ) ( , , ) * (1 * ( || )) cw tw cw tw w d D peso d t w KL P P peso d t w peso d t w KL P P γ γ ∈ + = +∑ (5) donde ( ) ( ) ( ) ( ) 1,cw cw c c c c df w df w p y p y N N = = − (6) ( ) ( ) ( ) ( ) 1,t t tw tw t t df w df w p y p y N N = = − (7) donde dfc(w) es el número de documentos que contienen el término w dentro del clúster C, y Nc es el número de documentos en el clúster C, y Nt es el número total de documentos que llegan antes del paso de tiempo t. γ es un parámetro constante, actualmente configurado manualmente en 3. La divergencia de Kullback-Leibler se define de la siguiente manera [17]: La idea básica es que, para una historia en un tema, cuanto más ocurre un término dentro del tema y menos ocurre en otros temas, se le deben asignar pesos más altos. Obviamente, el modelo modificado puede cumplir con todos los requisitos de las cinco clases de términos mencionadas anteriormente. 5.2 Reajuste de Términos Basado en el Tipo de Término y la Clase de Historia. Trabajos anteriores encontraron que algunas clases de historias de noticias podrían lograr mejoras significativas al dar un peso adicional a las entidades nombradas. Pero encontramos que los términos de diferentes tipos deben recibir una cantidad diferente de peso adicional para diferentes clases de noticias. Utilizamos open-NLP1 para reconocer tipos de entidades nombradas y etiquetas de partes del discurso para términos que aparecen en noticias. Los tipos de entidades nombradas incluyen nombre de persona, nombre de organización, nombre de ubicación, fecha, hora, dinero y porcentaje, y se seleccionan cinco partes del discurso: ninguno (NN), verbo (VB), adjetivo (JJ), adverbio (RB) y número cardinal (CD). El análisis estadístico muestra tipos de términos discriminativos a nivel de tema para diferentes clases de historias. Por conveniencia, los tipos de entidad nombrada y etiquetas de parte de la oración se llaman uniformemente tipo de término en las secciones siguientes. Determinar si dos historias tratan sobre el mismo tema es un componente básico para la tarea de NED. Por lo tanto, al principio usamos la estadística χ² para calcular las correlaciones entre términos y temas. Para un término t y un tema T, se deriva una tabla de contingencia: Tabla 1. Una tabla de contingencia de 2×2 con el número de documentos pertenecientes al tema T y no pertenecientes al tema T incluye t A B no incluye t C D. La estadística χ² para un término específico t con respecto al tema T se define como [16]: χ² = ( ) ( ) * ( ) ( ) * ( ) * ( ) * ( ) w T A B C D AD CB A C B D A B C D + + + − + + + + (9). Los temas de noticias para la tarea TDT se clasifican además en 11 reglas de interpretaciones (ROIs) 2. El ROI se puede ver como una clase de historias de nivel superior. El promedio de correlación entre un tipo de término y un ROI de tema se calcula como: 2 avg 2 ( , )( ( , ) )k m m km kT R w P w TP R p w T R P χ χ ∈ ∈ ∑ ∑（ , ）= 1 1 k=1…K, m=1…M (10) donde K es el número de tipos de términos (fijado constantemente en 12 en el artículo). M es el número de nuevas clases (ROIs, establecido en el conjunto 11 del artículo). Pk representa el conjunto de todos los términos de tipo k, y Rm representa el conjunto de todos los temas de la clase m, p(t,T) significa la probabilidad de que t ocurra en el tema T. Debido a limitaciones de espacio, solo se enumeran partes de los tipos de términos (9 tipos de términos) y partes de las clases de noticias (8 clases) en la tabla 2 con los valores promedio de correlación entre ellos. Las estadísticas se derivan de datos etiquetados en el corpus TDT2. (Los resultados en la tabla 2 ya están normalizados para mayor comodidad en la comparación). Las estadísticas en la tabla 2 indican la utilidad de diferentes tipos de términos en la discriminación de temas con respecto a diferentes clases de noticias. Podemos ver que el nombre de la ubicación es el tipo de término más útil para tres clases de noticias: Desastres Naturales, Violencia o Guerra, Finanzas. Y para otras tres categorías: Elecciones, Casos Legales/Criminales, Ciencia y Descubrimiento, el nombre de la persona es el tipo de término más discriminativo. Para Escándalos/Audiencias, la fecha es la información más importante para la discriminación de temas. Además, los casos legales/criminales y los temas financieros tienen una correlación más alta con términos relacionados con el dinero, mientras que la ciencia y el descubrimiento tienen una correlación más alta con términos de porcentaje. Los términos no nominales son más estables para diferentes clases. 1. http://opennlp.sourceforge.net/ 2. http://projects.ldc.upenn.edu/TDT3/Guide/label.html A partir del análisis de la tabla 2, es razonable ajustar el peso de los términos según su tipo de término y la clase de noticias a la que pertenece la historia. Los nuevos pesos de los términos se recalculan de la siguiente manera: ( ) ( ) ( ) ( ) ( , , ) * ( , , ) ( , , ) * clase d D tipo w T clase d D tipo w w d peso d t w peso d t w peso d t w α α ∈ = ∑ (11) donde type(w) representa el tipo del término w, y class(d) representa la clase de la historia d, c kα es el parámetro de recalibración para la clase de noticias c y el tipo de término k. En el trabajo, simplemente utilizamos las estadísticas de la tabla 2 como parámetros de recalibración. Aunque usar las estadísticas directamente puede que no sea la mejor opción, no discutimos cómo obtener automáticamente los mejores parámetros. Intentaremos utilizar técnicas de aprendizaje automático para obtener los mejores parámetros en el trabajo futuro. En el trabajo, utilizamos BoosTexter [20] para clasificar todas las historias en uno de los 11 ROIs. BoosTexter es un programa de aprendizaje automático basado en boosting, que crea una serie de reglas simples para construir un clasificador de datos de texto o atributos-valor. Utilizamos el peso del término generado utilizando el modelo TF-IDF como característica para la clasificación de historias. Entrenamos el modelo con las 12000 historias en inglés evaluadas en TDT2, y clasificamos el resto de las historias en TDT2 y todas las historias en TDT3. Los resultados de clasificación se utilizan para la reponderación de términos en la fórmula (11). Dado que las etiquetas de clase de las historias de tema apagado no se proporcionan en los conjuntos de datos de TDT, no podemos proporcionar la precisión de clasificación aquí. Por lo tanto, no discutimos los efectos de la precisión de la clasificación en el rendimiento de NED en el artículo. 6. CONFIGURACIÓN EXPERIMENTAL 6.1 Conjuntos de datos Utilizamos dos conjuntos de datos de LDC [18], TDT2 y TDT3, para nuestros experimentos. TDT2 contiene noticias desde enero hasta junio de 1998. Contiene alrededor de 54,000 historias de fuentes como ABC, Associated Press, CNN, New York Times, Public Radio International, Voice of America, etc. Solo se consideraron historias en inglés en la colección. TDT3 contiene aproximadamente 31,000 historias en inglés recopiladas de octubre a diciembre de 1998. Además de las fuentes utilizadas en TDT2, también contiene historias de las transmisiones de televisión de NBC y MSNBC. Utilizamos versiones transcritas de las emisiones de televisión y radio además de noticias textuales. El conjunto de datos TDT2 está etiquetado con alrededor de 100 temas, y aproximadamente 12,000 historias en inglés pertenecen al menos a uno de estos temas. El conjunto de datos TDT3 está etiquetado con alrededor de 120 temas, y aproximadamente 8000 historias en inglés pertenecen al menos a uno de estos temas. Todos los temas están clasificados en 11 Reglas de Interpretación: (1) Elecciones, (2) Escándalos/Audiencias, (3) Casos Legales/Criminales, (4) Desastres Naturales, (5) Accidentes, (6) Violencia o Guerra en Curso, (7) Noticias de Ciencia y Descubrimientos, (8) Finanzas, (9) Nueva Ley, (10) Noticias Deportivas, (11) Varios. Noticias. 6.2 Métrica de evaluación TDT utiliza una función de costo CDet que combina las probabilidades de perder una nueva historia y una falsa alarma [19]: * * * *Det Miss Miss Target FA FA NontargetC C P P C P P= + (12) Tabla 2. Correlación promedio entre tipos de términos y clases de noticias donde CMiss significa el costo de perder una nueva historia, PMiss significa la probabilidad de perder una nueva historia, y PTarget significa la probabilidad de ver una nueva historia en los datos; CFA significa el costo de una falsa alarma, PFA significa la probabilidad de una falsa alarma, y PNontarget significa la probabilidad de ver una historia antigua. El costo CDet se normaliza de tal manera que un sistema perfecto obtiene una puntuación de 0 y un sistema trivial, que es el mejor de marcar todas las historias como nuevas o antiguas, obtiene una puntuación de 1: ( ( * , * ) ) Det Det Miss Target FA Nontarget C Norm C min C P C P = (13) El sistema de detección de eventos nuevos proporciona dos salidas para cada historia. La primera parte es sí o no, indicando si la historia desencadena un nuevo evento o no. La segunda parte es una puntuación que indica la confianza de la primera decisión. Los puntajes de confianza se pueden utilizar para trazar la curva DET, es decir, curvas que representan la probabilidad de falsa alarma frente a la probabilidad de omisión. El costo normalizado mínimo se puede determinar si se elige el umbral óptimo en la puntuación. RESULTADOS EXPERIMENTALES 7.1 Resultados principales Para probar los enfoques propuestos en el modelo, implementamos y probamos cinco sistemas: Sistema-1: este sistema se utiliza como referencia. Se implementa basándose en el modelo básico descrito en la sección 3, es decir, utilizando el modelo TF-IDF incremental para generar pesos de términos y utilizando la distancia de Hellinger para calcular la similitud entre documentos. La normalización del puntaje de similitud también se emplea [8]. Se utiliza el procedimiento de detección S-S. Sistema-2: este sistema es igual que el sistema-1 excepto que se utiliza el procedimiento de detección S-C. Sistema-3: este sistema es igual que el sistema-1 excepto que utiliza el nuevo procedimiento de detección que se basa en un árbol de indexación. Sistema-4: implementado basado en el enfoque presentado en la sección 5.1, es decir, los términos se vuelven a ponderar según la distancia entre las distribuciones de términos en un grupo y todas las historias. Se utiliza el nuevo procedimiento de detección. Sistema-5: implementado basado en el enfoque presentado en la sección 5.2, es decir, los términos de diferentes tipos son reponderados según la clase de noticias utilizando parámetros entrenados. El nuevo procedimiento de detección se utiliza. Los siguientes son algunos otros sistemas NED: Sistema-6: [21] para cada par de historias, calcula tres valores de similitud para entidad nombrada, entidad no nombrada y todos los términos respectivamente. Y emplear Máquina de Vectores de Soporte para predecir nuevo o antiguo utilizando los valores de similitud como características. Sistema-7: [8] extendió un modelo básico incremental de TF-IDF para incluir modelos específicos de origen, normalización de puntuaciones de similitud basada en promedios específicos de documentos, normalización de puntuaciones de similitud basada en promedios específicos de pares de fuentes, etc. Sistema-8: [13] dividió la representación del documento en dos partes: entidades nombradas y entidades no nombradas, y eligió una parte efectiva para cada clase de noticias. La tabla 3 y la tabla 4 muestran los costos normalizados ponderados por tema y los tiempos de comparación en los conjuntos de datos TDT2 y TDT3 respectivamente. Dado que no se contaba con un conjunto de datos de validación para ajustar el umbral θ nuevo en los experimentos en TDT2, solo informamos los costos normalizados mínimos para nuestros sistemas en la tabla 3. El sistema-5 supera a todos los demás sistemas, incluido el sistema-6, y realiza solo 2.78e+8 veces la comparación de tiempos en el procedimiento de detección, lo que representa solo el 13.4% del sistema-1. Tabla 3. Los resultados de NED en los sistemas TDT2 Min Norm(CDet) tiempos de comparación son: Sistema-1 0.5749 2.08e+9, Sistema-2 0.6673 3.77e+8, Sistema-3 0.5765 2.81e+8, Sistema-4 0.5431 2.99e+8, Sistema-5 0.5089 2.78e+8, Sistema-6 0.5300. Al evaluar en los costos normalizados en TDT3, utilizamos los umbrales óptimos obtenidos del conjunto de datos de TDT2 para todos los sistemas. El sistema-2 reduce los tiempos de comparación a 1.29e+9, que es solo el 18.3% del sistema-1, pero al mismo tiempo también obtiene un costo normalizado mínimo deteriorado que es 0.0499 más alto que el sistema-1. El Sistema-3 utiliza el nuevo procedimiento de detección basado en el índice de noticias en árbol. Requiere incluso menos tiempo de comparación que el sistema-2. Esto se debe a que las comparaciones de historias suelen arrojar mayores similitudes que las de grupos de historias, por lo que las historias tienden a combinarse en el sistema-3. Y el sistema-3 es básicamente equivalente al sistema-1 en los resultados de precisión. El System-4 ajusta los pesos de los términos basándose en la distancia de las distribuciones de términos entre todo el corpus y el conjunto de historias del clúster, lo que resulta en una mejora significativa de 0.0468 en comparación con el system-1. El mejor sistema (sistema-5) tiene un costo normalizado mínimo de 0.5012, lo cual es 0.0797 mejor que el sistema-1, y también mejor que cualquier otro resultado previamente reportado para este conjunto de datos [8, 13]. Además, el sistema-5 solo necesita 1.05e+8 veces de comparación, lo que equivale al 14.9% del sistema-1. Tabla 4. Los resultados de NED en los sistemas TDT3 Norm(CDet) Norma mínima(CDet) Tiempos de comparación Sistema-1 0.6159 0.5809 7.04e+8 Sistema-2① 0.6493 0.6308 1.29e+8 Sistema-3② 0.6197 0.5868 1.03e+8 Sistema-4② 0.5601 0.5341 1.03e+8 Sistema-5② 0.5413 0.5012 1.05e+8 Sistema-7 -- 0.5783 -Sistema-8 -- 0.5229 -① θ nuevo=0.13 ② θ inicial=0.13, λ =3, δ =0.15 La Figura 5 muestra las cinco curvas DET para nuestros sistemas en el conjunto de datos TDT3. El Sistema-5 logra el costo mínimo con una tasa de falsas alarmas de 0.0157 y una tasa de omisión de 0.4310. Podemos observar que System4 y System-5 obtienen una menor probabilidad de error de omisión en regiones de baja probabilidad de falsa alarma. La hipótesis es que se transfiere más valor de peso a los términos clave de los temas que a los términos no clave. La puntuación de similitud entre dos historias pertenecientes a diferentes temas es más baja que antes, porque los términos superpuestos suelen no ser términos clave de sus temas. 7.2 Selección de parámetros para la detección de árboles de indexación. La Figura 3 muestra los costos normalizados mínimos obtenidos por el sistema-3 en TDT3 utilizando diferentes parámetros. El parámetro de inicio θ se prueba en seis valores que van desde 0.03 hasta 0.18. Y el parámetro λ se prueba en cuatro valores: 1, 2, 3 y 4. Podemos ver que, cuando θ inicial se establece en 0.12, que es el más cercano a θ nuevo, los costos son más bajos que los demás. Esto es fácil de explicar, porque cuando se agrupan historias pertenecientes al mismo tema en un conjunto, es más razonable que el conjunto represente las historias en él. Cuando el parámetro λ se establece en 3 o 4, los costos son mejores que en otros casos, pero no hay mucha diferencia entre 3 y 4. 0 0.05 0.1 0.15 0.2 1 2 3 4 0.5 0.6 0.7 0.8 0.9 1 θ-initλ MinCost 0.6 0.65 0.7 0.75 0.8 0.85 0.9 Figura 3. Costo mínimo en TDT3 (δ =0.15) 0 0.05 0.1 0.15 0.2 1 2 3 4 0 0.5 1 1.5 2 2.5 x 10 8 θ-inicio λ Comparando tiempos 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 8 Figura 4. Comparando los tiempos en TDT3 (δ =0.15), la Figura 4 muestra los tiempos de comparación utilizados por el sistema-3 en TDT3 con los mismos parámetros que la Figura 3. Los tiempos de comparación dependen fuertemente de θ init. Debido a que cuanto mayor sea θ init, menos historias se combinan juntas, más veces de comparación se necesitan para la decisión de un nuevo evento. Por lo tanto, usamos θ init = 0.13, λ = 3, δ = 0.15 para los sistemas 3, 4 y 5. En esta configuración de parámetros, podemos obtener tanto costos normalizados mínimos bajos como menos tiempos de comparación. CONCLUSIÓN Hemos propuesto un procedimiento de detección basado en un árbol de indexación de noticias en nuestro modelo. Reduce los tiempos de comparación a aproximadamente una séptima parte del método tradicional sin afectar la precisión de NED. También hemos presentado dos extensiones al modelo básico TF-IDF. La primera extensión se realiza ajustando los pesos de los términos basados en las distribuciones de términos entre todo el corpus y un conjunto de historias de un clúster. Y la segunda extensión al modelo básico TF-IDF es un mejor uso de los tipos de términos (tipos de entidades nombradas y partes de la velocidad) de acuerdo con las categorías de noticias. Nuestros resultados experimentales en los conjuntos de datos TDT2 y TDT3 muestran que ambas extensiones contribuyen significativamente a la mejora en la precisión. No consideramos la información de tiempo de noticias como una pista para la tarea de NED, ya que la mayoría de los temas duran mucho tiempo y los conjuntos de datos de TDT solo abarcan un período relativamente corto (no más de 6 meses). Para el trabajo futuro, queremos recopilar un conjunto de noticias que abarque un período más largo de internet e integrar información temporal en la tarea de NED. Dado que el tema es un grupo de noticias relativamente grueso, también queremos refinar la granularidad del grupo a nivel de evento e identificar diferentes eventos y sus relaciones dentro de un tema. Agradecimientos: Este trabajo ha sido apoyado por la Fundación Nacional de Ciencias Naturales de China bajo la subvención No. 90604025. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son del autor(es) y no necesariamente reflejan las del patrocinador. REFERENCIAS [1] http://www.nist.gov/speech/tests/tdt/index.htm [2] En Detección y Seguimiento de Temas. Organización de la información basada en eventos. Kluwer Academic Publishers, 2002. .01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90 1 2 5 10 20 40 60 80 90 Probabilidad de Falsa Alarma (en %) Probabilidad de Omisión (en %) Curva Ponderada por Tema del SISTEMA1 Norma Mínima (Costo) del SISTEMA1 Curva Ponderada por Tema del SISTEMA2 Norma Mínima (Costo) del SISTEMA2 Curva Ponderada por Tema del SISTEMA3 Norma Mínima (Costo) del SISTEMA3 Curva Ponderada por Tema del SISTEMA4 Norma Mínima (Costo) del SISTEMA4 Curva Ponderada por Tema del SISTEMA5 Norma Mínima (Costo) del SISTEMA5 Rendimiento Aleatorio Figura 5. Curvas DET en TDT3 [3] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B.T. Archibald y X. Liu. Enfoques de aprendizaje para detectar y rastrear eventos de noticias. En el número especial de IEEE Intelligent Systems sobre Aplicaciones de Recuperación de Información Inteligente, volumen 14 (4), 1999, páginas 32-43. [4] Y. Yang, T. Pierce y J. Carbonell. Un estudio sobre la detección de eventos retrospectivos y en línea. En Actas de SIGIR-98, Melbourne, Australia, 1998, 28-36. [5] J. Allan, V. Lavrenko, D. Malin y R. Swan. Detecciones, límites y líneas de tiempo: Umass y tdt-3. En Actas del Taller de Detección y Seguimiento de Temas (TDT-3), Viena, VA, 2000, 167-174. [6] R. Papka y J. Allan. Detección de nuevos eventos en línea utilizando agrupamiento de un solo paso TÍTULO2:. Informe técnico UM-CS1998-021, 1998. [7] W. Lam, H. Meng, K. Wong y J. I'm sorry, but \"Yen\" is not a sentence. Can you please provide a sentence for me to translate into Spanish? Utilizando el Análisis Contextual para la Detección de Eventos de Noticias. Revista Internacional de Sistemas Inteligentes, 2001, 525-546. [8] B. Thorsten, C. Francine y F. Ayman. Un sistema para la detección de nuevos eventos. En Actas de la 26ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2003, 330-337. [9] S. Nicola y C. Joe. Combinando clasificadores semánticos y sintácticos de documentos para mejorar la detección de primeras noticias. En Actas de la 24ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell, and C. Jin. \n\nACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell y C. Jin. Detección de novedades condicionada al tema. En Actas de la 8ª Conferencia Internacional de ACM SIGKDD, ACM Press. 2002, 688-693. [11] M. Juha, A.M. Helena y S. Marko. Aplicando Clases Semánticas en la Detección y Seguimiento de Eventos. En Actas de la Conferencia Internacional sobre Procesamiento del Lenguaje Natural (ICON 2002), 2002, páginas 175-183. [12] M. Juha, A.M. Helena y S. Marko. Semántica simple en la detección y seguimiento de temas. Recuperación de información, 7(3-4): 2004, 347-368. [13] K. Giridhar y J. Allan. Clasificación de texto y entidades nombradas para la detección de nuevos eventos. En Actas de la 27ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2004, 297-304. [14] J. P. Callan, W. B. Croft, y S. M. Harding. El Sistema de Recuperación INQUERY. En Actas de DEXA-92, 3ra Conferencia Internacional sobre Aplicaciones de Bases de Datos y Sistemas Expertos, 1992, 78-83. [15] R. Krovetz. Viendo la morfología como un proceso de inferencia. En Actas de ACM SIGIR93, 1993, 61-81. [16] Y. Yang y J. Pedersen. Un estudio comparativo sobre la selección de características en la categorización de textos. En J. D. H. Fisher, editor, La Decimocuarta Conferencia Internacional sobre Aprendizaje Automático (ICML97), Morgan Kaufmann, 1997, 412-420. [17] T. M. Cover y J. A. I'm sorry, but \"Thomas\" is not a sentence. Can you provide more context or a complete sentence for me to translate into Spanish? Elementos de la teoría de la información. Wiley. 1991. [18] El consorcio de datos lingüísticos, http://www.ldc,upenn.edu/. [19] La definición de la tarea TDT 2001 y el plan de evaluación, http://www.nist.gov/speech/tests/tdt/tdt2001/evalplan.htm. [20] R. E. Schapire y Y. Cantante. Boostexter: Un sistema basado en Boosting para la categorización de texto. En Machine Learning 39(2/3):1, Kluwer Academic Publishers, 2000, 35-168. [21] K. Giridhar y J. Allan. 2005. Utilizando nombres y temas para la detección de nuevos eventos. En Actas de la Conferencia de Tecnología Humana y la Conferencia sobre Métodos Empíricos en Lenguaje Natural, Vancouver, 2005, 121-128 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "ned accuracy": {
            "translated_key": "precisión de NED",
            "is_in_text": true,
            "original_annotated_sentences": [
                "New Event Detection Based on Indexing-tree and Named Entity Zhang Kuo Tsinghua University Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Tsinghua University Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Tsinghua University Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn ABSTRACT New Event Detection (NED) aims at detecting from one or multiple streams of news stories that which one is reported on a new event (i.e.",
                "not reported previously).",
                "With the overwhelming volume of news available today, there is an increasing need for a NED system which is able to detect new events more efficiently and accurately.",
                "In this paper we propose a new NED model to speed up the NED task by using news indexing-tree dynamically.",
                "Moreover, based on the observation that terms of different types have different effects for NED task, two term reweighting approaches are proposed to improve <br>ned accuracy</br>.",
                "In the first approach, we propose to adjust term weights dynamically based on previous story clusters and in the second approach, we propose to employ statistics on training data to learn the named entity reweighting model for each class of stories.",
                "Experimental results on two Linguistic Data Consortium (LDC) datasets TDT2 and TDT3 show that the proposed model can improve both efficiency and accuracy of NED task significantly, compared to the baseline system and other existing systems.",
                "Categories and Subject Descriptors H.3.3 [Information Systems]: Information Search and Retrieval; H.4.2 [Information Systems Applications]: Types of Systemsdecision support.",
                "General Terms Algorithms, Performance, Experimentation 1.",
                "INTRODUCTION Topic Detection and Tracking (TDT) program aims to develop techniques which can effectively organize, search and structure news text materials from a variety of newswire and broadcast media [1].",
                "New Event Detection (NED) is one of the five tasks in TDT.",
                "It is the task of online identification of the earliest report for each topic as soon as that report arrives in the sequence of documents.",
                "A Topic is defined as a seminal event or activity, along with directly related events and activities [2].",
                "An Event is defined as something (non-trivial) happening in a certain place at a certain time [3].",
                "For instance, when a bomb explodes in a building, the exploding is the seminal event that triggers the topic, and other stories on the same topic would be those discussing salvaging efforts, the search for perpetrators, arrests and trial and so on.",
                "Useful news information is usually buried in a mass of data generated everyday.",
                "Therefore, NED systems are very useful for people who need to detect novel information from real-time news stream.",
                "These real-life needs often occur in domains like financial markets, news analysis, and intelligence gathering.",
                "In most of state-of-the-art (currently) NED systems, each news story on hand is compared to all the previous received stories.",
                "If all the similarities between them do not exceed a threshold, then the story triggers a new event.",
                "They are usually in the form of cosine similarity or Hellinger similarity metric.",
                "The core problem of NED is to identify whether two stories are on the same topic.",
                "Obviously, these systems cannot take advantage of topic information.",
                "Further more, it is not acceptable in real applications because of the large amount of computation required in the NED process.",
                "Other systems organize previous stories into clusters (each cluster corresponds to a topic), and new story is compared to the previous clusters instead of stories.",
                "This manner can reduce comparing times significantly.",
                "Nevertheless, it has been proved that this manner is less accurate [4, 5].",
                "This is because sometimes stories within a topic drift far away from each other, which could lead low similarity between a story and its topic.",
                "On the other hand, some proposed NED systems tried to improve accuracy by making better use of named entities [10, 11, 12, 13].",
                "However, none of the systems have considered that terms of different types (e.g.",
                "Noun, Verb or Person name) have different effects for different classes of stories in determining whether two stories are on the same topic.",
                "For example, the names of election candidates (Person name) are very important for stories of election class; the locations (Location name) where accidents happened are important for stories of accidents class.",
                "So, in NED, there still exist following three problems to be investigated: (1) How to speed up the detection procedure while do not decrease the detection accuracy? (2) How to make good use of cluster (topic) information to improve accuracy? (3) How to obtain better news story representation by better understanding of named entities.",
                "Driven by these problems, we have proposed three approaches in this paper. (1)To make the detection procedure faster, we propose a new NED procedure based on news indexing-tree created dynamically.",
                "Story indexing-tree is created by assembling similar stories together to form news clusters in different hierarchies according to their values of similarity.",
                "Comparisons between current story and previous clusters could help find the most similar story in less comparing times.",
                "The new procedure can reduce the amount of comparing times without hurting accuracy. (2)We use the clusters of the first floor in the indexing-tree as news topics, in which term weights are adjusted dynamically according to term distribution in the clusters.",
                "In this approach, cluster (topic) information is used properly, so the problem of theme decentralization is avoided. (3)Based on observations on the statistics obtained from training data, we found that terms of different types (e.g.",
                "Noun and Verb) have different effects for different classes of stories in determining whether two stories are on the same topic.",
                "And we propose to use statistics to optimize the weights of the terms of different types in a story according to the news class that the story belongs to.",
                "On TDT3 dataset, the new NED model just uses 14.9% comparing times of the basic model, while its minimum normalized cost is 0.5012, which is 0.0797 better than the basic model, and also better than any other results previously reported for this dataset [8, 13].",
                "The rest of the paper is organized as follows.",
                "We start off this paper by summarizing the previous work in NED in section 2.",
                "Section 3 presents the basic model for NED that most current systems use.",
                "Section 4 describes our new detection procedure based on news indexing-tree.",
                "In section 5, two term reweighting methods are proposed to improve <br>ned accuracy</br>.",
                "Section 6 gives our experimental data and evaluation metrics.",
                "We finally wrap up with the experimental results in Section 7, and the conclusions and future work in Section 8. 2.",
                "RELATED WORK Papka et al. proposed Single-Pass clustering on NED [6].",
                "When a new story was encountered, it was processed immediately to extract term features and a query representation of the storys content is built up.",
                "Then it was compared with all the previous queries.",
                "If the document did not trigger any queries by exceeding a threshold, it was marked as a new event.",
                "Lam et al build up previous query representations of story clusters, each of which corresponds to a topic [7].",
                "In this manner comparisons happen between stories and clusters.",
                "Recent years, most work focus on proposing better methods on comparison of stories and document representation.",
                "Brants et al. [8] extended a basic incremental TF-IDF model to include sourcespecific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, term reweighting based on inverse event frequencies, and segmentation of documents.",
                "Good improvements on TDT bench-marks were shown.",
                "Stokes et al. [9] utilized a combination of evidence from two distinct representations of a documents content.",
                "One of the representations was the usual free text vector, the other made use of lexical chains (created using WordNet) to build another term vector.",
                "Then the two representations are combined in a linear fashion.",
                "A marginal increase in effectiveness was achieved when the combined representation was used.",
                "Some efforts have been done on how to utilize named entities to improve NED.",
                "Yang et al. gave location named entities four times weight than other terms and named entities [10].",
                "DOREMI research group combined semantic similarities of person names, location names and time together with textual similarity [11][12].",
                "UMass [13] research group split document representation into two parts: named entities and non-named entities.",
                "And it was found that some classes of news could achieve better performance using named entity representation, while some other classes of news could achieve better performance using non-named entity representation.",
                "Both [10] and [13] used text categorization technique to classify news stories in advance.",
                "In [13] news stories are classified automatically at first, and then test sensitivities of names and non-name terms for NED for each class.",
                "In [10] frequent terms for each class are removed from document representation.",
                "For example, word election does not help identify different elections.",
                "In their work, effectiveness of different kinds of names (or terms with different POS) for NED in different news classes are not investigated.",
                "We use statistical analysis to reveal the fact and use it to improve NED performance. 3.",
                "BASIC MODEL In this section, we present the basic New Event Detection model which is similar to what most current systems apply.",
                "Then, we propose our new model by extending the basic model.",
                "New Event Detection systems use news story stream as input, in which stories are strictly time-ordered.",
                "Only previously received stories are available when dealing with current story.",
                "The output is a decision for whether the current story is on a new event or not and the confidence of the decision.",
                "Usually, a NED model consists of three parts: story representation, similarity calculation and detection procedure. 3.1 Story Representation Preprocessing is needed before generating story representation.",
                "For preprocessing, we tokenize words, recognize abbreviations, normalize abbreviations, add part-of-speech tags, remove stopwords included in the stop list used in InQuery [14], replace words with their stems using K-stem algorithm[15], and then generate word vector for each news story.",
                "We use incremental TF-IDF model for term weight calculation [4].",
                "In a TF-IDF model, term frequency in a news document is weighted by the inverse document frequency, which is generated from training corpus.",
                "When a new term occurs in testing process, there are two solutions: simply ignore the new term or set df of the term as a small const (e.g. df = 1).",
                "The new term receives too low weight in the first solution (0) and too high weight in the second solution.",
                "In incremental TF-IDF model, document frequencies are updated dynamically in each time step t: 1( ) ( ) ( )t t D tdf w df w df w−= + (1) where Dt represents news story set received in time t, and dfDt(w) means the number of documents that term w occurs in, and dft(w) means the total number of documents that term w occurs in before time t. In this work, each time window includes 50 news stories.",
                "Thus, each story d received in t is represented as follows: 1 2{ ( , , ), ( , , ),..., ( , , )}nd weight d t w weight d t w weight d t w→ where n means the number of distinct terms in story d, and ( , , )weight d t w means the weight of term w in story d at time t: log( ( , ) 1) log(( 1) /( ( ) 0.5)) ( , , ) log( ( , ) 1) log(( 1) /( ( ) 0.5)) t t t t w d tf d w N df w weight d t w tf d w N df w ∈ + + + = + + +∑ (2) where Nt means the total number of news stories before time t, and tf(d,w) means how many times term w occurs in news story d. 3.2 Similarity Calculation We use Hellinger distance for the calculation of similarity between two stories, for two stories d and d at time t, their similarity is defined as follows: , ( , , ) ( , , ) * ( , , ) w d d sim d d t weight d t w weight d t w ∈ = ∑ (3) 3.3 Detection Procedure For each story d received in time step t, the value ( ) ( ) ( ) ( ( , , )) time d time d n d max sim d d t < = (4) is a score used to determine whether d is a story about a new topic and at the same time is an indication of the confidence in our decision [8]. time(d) means the publication time of story d. If the score exceeds the thresholdθ new, then there exists a sufficiently similar document, thus d is a old story, otherwise, there is no sufficiently similar previous document, thus d is an new story. 4.",
                "New NED Procedure Traditional NED systems can be classified into two main types on the aspect of detection procedure: (1) S-S type, in which the story on hand is compared to each story received previously, and use the highest similarity to determine whether current story is about a new event; (2) S-C type, in which the story on hand is compared to all previous clusters each of which representing a topic, and the highest similarity is used for final decision for current story.",
                "If the highest similarity exceeds thresholdθ new, then it is an old story, and put it into the most similar cluster; otherwise it is a new story and create a new cluster.",
                "Previous work show that the first manner is more accurate than the second one [4][5].",
                "Since sometimes stories within a topic drift far away from each other, a story may have very low similarity with its topic.",
                "So using similarities between stories for determining new story is better than using similarities between story and clusters.",
                "Nevertheless, the first manner needs much more comparing times which means the first manner is low efficient.",
                "We propose a new detection procedure which uses comparisons with previous clusters to help find the most similar story in less comparing times, and the final new event decision is made according to the most similar story.",
                "Therefore, we can get both the accuracy of S-S type methods and the efficiency of S-C type methods.",
                "The new procedure creates a news indexing-tree dynamically, in which similar stories are put together to form a hierarchy of clusters.",
                "We index similar stories together by their common ancestor (a cluster node).",
                "Dissimilar stories are indexed in different clusters.",
                "When a story is coming, we use comparisons between the current story and previous hierarchical clusters to help find the most similar story which is useful for new event decision.",
                "After the new event decision is made, the current story is inserted to the indexing-tree for the following detection.",
                "The news indexing-tree is defined formally as follows: S-Tree = {r, NC , NS , E} where r is the root of S-Tree, NC is the set of all cluster nodes, NS is the set of all story nodes, and E is the set of all edges in S-Tree.",
                "We define a set of constraints for a S-Tree: ⅰ . , is an non-terminal node in the treeC i i N i∀ ∈ → ⅱ . , is a terminal node in the treeS i i N i∀ ∈ → ⅲ . , out degree of is at least 2C i i N i∀ ∈ → ⅳ . , is represented as the centroid of its desendantsC i i iN∀ ∈ → For a news story di, the comparison procedure and inserting procedure based on indexing-tree are defined as follows.",
                "An example is shown by Figure 1 and Figure 2.",
                "Figure 1.",
                "Comparison procedure Figure 2.",
                "Inserting procedure Comparison procedure: Step 1: compare di to all the direct child nodes of r and select λ nodes with highest similarities, e.g., C1 2 and C1 3 in Figure 1.",
                "Step 2: for each selected node in the last step, e.g.",
                "C1 2, compare di to all its direct child nodes, and select λ nodes with highest similarities, e.g.",
                "C2 2 and d8.",
                "Repeat step 2 for all non-terminal nodes.",
                "Step 3: record the terminal node with the highest similarty to di, e.g. s5, and the similarity value (0.20).",
                "Inserting di to the S-tree with r as root: Find the node n which is direct child of r in the path from r to the terminal node with highest similarity s, e.g.",
                "C1 2.",
                "If s is smaller than θ init+(h-1)δ , then add di to the tree as a direct child of r. Otherwise, if n is a terminal node, then create a cluster node instead of n, and add both n and di as its direct children; if n is an non-terminal node, then repeat this procedure and insert di to the sub-tree with n as root recursively.",
                "Here h is the length between n and the root of S-tree.",
                "The more the stories in a cluster similar to each other, the better the cluster represents the stories in it.",
                "Hence we add no constraints on the maximum of trees height and degree of a node.",
                "Therefore, we cannot give the complexity of this indexing-tree based procedure.",
                "But we will give the number of comparing times needed by the new procedure in our experiments in section7. 5.",
                "Term Reweighting Methods In this section, two term reweighting methods are proposed to improve <br>ned accuracy</br>.",
                "In the first method, a new way is explored for better using of cluster (topic) information.",
                "The second one finds a better way to make use of named entities based on news classification. 5.1 Term Reweighting Based on Distribution Distance TF-IDF is the most prevalent model used in information retrieval systems.",
                "The basic idea is that the fewer documents a term appears in, the more important the term is in discrimination of documents (relevant or not relevant to a query containing the term).",
                "Nevertheless, in TDT domain, we need to discriminate documents with regard to topics rather than queries.",
                "Intuitively, using cluster (topic) vectors to compare with subsequent news stories should outperform using story vectors.",
                "Unfortunately, the experimental results do not support this intuition [4][5].",
                "Based on observation on data, we find the reason is that a news topic usually contains many directly or indirectly related events, while they all have their own sub-subjects which are usually different with each other.",
                "Take the topic described in section 1 as an example, events like the explosion and salvage have very low similarities with events about criminal trial, therefore stories about trial would have low similarity with the topic vector built on its previous events.",
                "This section focuses on how to effectively make use of topic information and at the same time avoid the problem of content decentralization.",
                "At first, we classify terms into 5 classes to help analysis the needs of the modified model: Term class A: terms that occur frequently in the whole corpus, e.g., year and people.",
                "Terms of this class should be given low weights because they do not help much for topic discrimination.",
                "Term class B: terms that occur frequently within a news category, e.g., election, storm.",
                "They are useful to distinguish two stories in different news categories.",
                "However, they cannot provide information to determine whether two stories are on the same or different topics.",
                "In another words, term election and term storm are not helpful in differentiate two election campaigns and two storm disasters.",
                "Therefore, terms of this class should be assigned lower weights.",
                "Term class C: terms that occur frequently in a topic, and infrequently in other topics, e.g., the name of a crash plane, the name of a specific hurricane.",
                "News stories that belong to different topics rarely have overlap terms in this class.",
                "The more frequently a term appears in a topic, the more important the term is for a story belonging to the topic, therefore the term should be set higher weight.",
                "Term class D: terms that appear in a topic exclusively, but not frequently.",
                "For example, the name of a fireman who did very well in a salvage action, which may appears in only two or three stories but never appeared in other topics.",
                "Terms of this type should receive more weights than in TF-IDF model.",
                "However, since they are not popular in the topic, it is not appropriate to give them too high weights.",
                "Term class E: terms with low document frequency, and appear in different topics.",
                "Terms of this class should receive lower weights.",
                "Now we analyze whether TF-IDF model can give proper weights to the five classes of terms.",
                "Obviously, terms of class A are lowly weighted in TF-IDF model, which is conformable with the requirement described above.",
                "In TF-IDF model, terms of class B are highly dependant with the number of stories in a news class.",
                "TF-IDF model cannot provide low weights if the story containing the term belongs to a relative small news class.",
                "For a term of class C, the more frequently it appears in a topic, the less weight TFIDF model gives to it.",
                "This strongly conflicts with the requirement of terms in class C. For terms of class D, TF-IDF model gives them high weights correctly.",
                "But for terms of class E, TF-IDF model gives high weights to them which are not conformable with the requirement of low weights.",
                "To sum up, terms of class B, C, E cannot be properly weighted in TF-IDF model.",
                "So, we propose a modified model to resolve this problem.",
                "When θ init andθ new are set closely, we assume that most of the stories in a first-level cluster (a direct child node of root node) are on the same topic.",
                "Therefore, we make use of a first-level cluster to capture term distribution (df for all the terms within the cluster) within the topic dynamically.",
                "KL divergence of term distribution in a first-level cluster and the whole story set is used to adjust term weights:  ( , , ) * (1 * ( || )) ( , , ) ( , , ) * (1 * ( || )) cw tw cw tw w d D weight d t w KL P P weight d t w weight d t w KL P P γ γ ∈ + = +∑ (5) where ( ) ( ) ( ) ( ) 1,cw cw c c c c df w df w p y p y N N = = − (6) ( ) ( ) ( ) ( ) 1,t t tw tw t t df w df w p y p y N N = = − (7) where dfc(w) is the number of documents containing term w within cluster C, and Nc is the number of documents in cluster C, and Nt is the total number of documents that arrive before time step t. γ is a const parameter, now is manually set 3.",
                "KL divergence is defined as follows [17]: ( ) ( || ) ( ) log ( )x p x KL P Q p x q x = ∑ (8) The basic idea is: for a story in a topic, the more a term occurs within the topic, and the less it occurs in other topics, it should be assigned higher weights.",
                "Obviously, modified model can meet all the requirements of the five term classes listed above. 5.2 Term Reweighting Based on Term Type and Story Class Previous work found that some classes of news stories could achieve good improvements by giving extra weight to named entities.",
                "But we find that terms of different types should be given different amount of extra weight for different classes of news stories.",
                "We use open-NLP1 to recognize named entity types and part-ofspeech tags for terms that appear in news stories.",
                "Named entity types include person name, organization name, location name, date, time, money and percentage, and five POSs are selected: none (NN), verb (VB), adjective (JJ), adverb (RB) and cardinal number (CD).",
                "Statistical analysis shows topic-level discriminative terms types for different classes of stories.",
                "For the sake of convenience, named entity type and part-of-speech tags are uniformly called term type in subsequent sections.",
                "Determining whether two stories are about the same topic is a basic component for NED task.",
                "So at first we use 2 χ statistic to compute correlations between terms and topics.",
                "For a term t and a topic T, a contingence table is derived: Table 1.",
                "A 2×2 Contingence Table Doc Number belong to topic T not belong to topic T include t A B not include t C D The 2 χ statistic for a specific term t with respect to topic T is defined to be [16]: 2 2 ( , ) ( ) * ( ) ( ) * ( ) * ( ) * ( ) w T A B C D AD CB A C B D A B C D χ = + + + − + + + + (9) News topics for the TDT task are further classified into 11 rules of interpretations (ROIs) 2 .",
                "The ROI can be seen as a higher level class of stories.",
                "The average correlation between a term type and a topic ROI is computed as: 2 avg 2 ( , )( ( , ) )k m m km kT R w P w TP R p w T R P χ χ ∈ ∈ ∑ ∑（ , ）= 1 1 k=1…K, m=1…M (10) where K is the number of term types (set 12 constantly in the paper).",
                "M is the number news classes (ROIs, set 11 in the paper).",
                "Pk represents the set of all terms of type k, and Rm represents the set of all topics of class m, p(t,T) means the probability that t occurs in topic T. Because of limitation of space, only parts of the term types (9 term types) and parts of news classes (8 classes) are listed in table 2 with the average correlation values between them.",
                "The statistics is derived from labeled data in TDT2 corpus. (Results in table 2 are already normalized for convenience in comparison.)",
                "The statistics in table 2 indicates the usefulness of different term types in topic discrimination with respect to different news classes.",
                "We can see that, location name is the most useful term type for three news classes: Natural Disasters, Violence or War, Finances.",
                "And for three other categories Elections, Legal/Criminal Cases, Science and Discovery, person name is the most discriminative term type.",
                "For Scandals/Hearings, date is the most important information for topic discrimination.",
                "In addition, Legal/Criminal Cases and Finance topics have higher correlation with money terms, while Science and Discovery have higher correlation with percentage terms.",
                "Non-name terms are more stable for different classes. 1 . http://opennlp.sourceforge.net/ 2 . http://projects.ldc.upenn.edu/TDT3/Guide/label.html From the analysis of table 2, it is reasonable to adjust term weight according to their term type and the news class the story belongs to.",
                "New term weights are reweighted as follows: ( ) ( ) ( ) ( ) ( , , ) * ( , , ) ( , , ) * class d D type w T class d D type w w d weight d t w weight d t w weight d t w α α ∈ = ∑ (11) where type(w) represents the type of term w, and class(d) represents the class of story d, c kα is reweighting parameter for news class c and term type k. In the work, we just simply use statistics in table 2 as the reweighting parameters.",
                "Even thought using the statistics directly may not the best choice, we do not discuss how to automatically obtain the best parameters.",
                "We will try to use machine learning techniques to obtain the best parameters in the future work.",
                "In the work, we use BoosTexter [20] to classify all stories into one of the 11 ROIs.",
                "BoosTexter is a boosting based machine learning program, which creates a series of simple rules for building a classifier for text or attribute-value data.",
                "We use term weight generated using TF-IDF model as feature for story classification.",
                "We trained the model on the 12000 judged English stories in TDT2, and classify the rest of the stories in TDT2 and all stories in TDT3.",
                "Classification results are used for term reweighting in formula (11).",
                "Since the class labels of topic-off stories are not given in TDT datasets, we cannot give the classification accuracy here.",
                "Thus we do not discuss the effects of classification accuracy to NED performance in the paper. 6.",
                "EXPERIMENTAL SETUP 6.1 Datasets We used two LDC [18] datasets TDT2 and TDT3 for our experiments.",
                "TDT2 contains news stories from January to June 1998.",
                "It contains around 54,000 stories from sources like ABC, Associated Press, CNN, New York Times, Public Radio International, Voice of America etc.",
                "Only English stories in the collection were considered.",
                "TDT3 contains approximately 31,000 English stories collected from October to December 1998.",
                "In addition to the sources used in TDT2, it also contains stories from NBC and MSNBC TV broadcasts.",
                "We used transcribed versions of the TV and radio broadcasts besides textual news.",
                "TDT2 dataset is labeled with about 100 topics, and approximately 12,000 English stories belong to at least one of these topics.",
                "TDT3 dataset is labeled with about 120 topics, and approximately 8000 English stories belong to at least one of these topics.",
                "All the topics are classified into 11 Rules of Interpretation: (1)Elections, (2)Scandals/Hearings, (3)Legal/Criminal Cases, (4)Natural Disasters, (5)Accidents, (6)Ongoing Violence or War, (7)Science and Discovery News, (8)Finance, (9)New Law, (10)Sports News, (11)MISC.",
                "News. 6.2 Evaluation Metric TDT uses a cost function CDet that combines the probabilities of missing a new story and a false alarm [19]: * * * *Det Miss Miss Target FA FA NontargetC C P P C P P= + (12) Table 2.",
                "Average correlation between term types and news classes where CMiss means the cost of missing a new story, PMiss means the probability of missing a new story, and PTarget means the probability of seeing a new story in the data; CFA means the cost of a false alarm, PFA means the probability of a false alarm, and PNontarget means the probability of seeing an old story.",
                "The cost CDet is normalized such that a perfect system scores 0 and a trivial system, which is the better one of mark all stories as new or old, scores 1: ( ( * , * ) ) Det Det Miss Target FA Nontarget C Norm C min C P C P = (13) New event detection system gives two outputs for each story.",
                "The first part is yes or no indicating whether the story triggers a new event or not.",
                "The second part is a score indicating confidence of the first decision.",
                "Confidence scores can be used to plot DET curve, i.e., curves that plot false alarm vs. miss probabilities.",
                "Minimum normalized cost can be determined if optimal threshold on the score were chosen. 7.",
                "EXPERIMENTAL RESULTS 7.1 Main Results To test the approaches proposed in the model, we implemented and tested five systems: System-1: this system is used as baseline.",
                "It is implemented based on the basic model described in section 3, i.e., using incremental TF-IDF model to generate term weights, and using Hellinger distance to compute document similarity.",
                "Similarity score normalization is also employed [8].",
                "S-S detection procedure is used.",
                "System-2: this system is the same as system-1 except that S-C detection procedure is used.",
                "System-3: this system is the same as system-1 except that it uses the new detection procedure which is based on indexing-tree.",
                "System-4: implemented based on the approach presented in section 5.1, i.e., terms are reweighted according to the distance between term distributions in a cluster and all stories.",
                "The new detection procedure is used.",
                "System-5: implemented based on the approach presented in section 5.2, i.e., terms of different types are reweighted according to news class using trained parameters.",
                "The new detection procedure is used.",
                "The following are some other NED systems: System-6: [21] for each pair of stories, it computes three similarity values for named entity, non-named entity and all terms respectively.",
                "And employ Support Vector Machine to predict new or old using the similarity values as features.",
                "System-7: [8] it extended a basic incremental TF-IDF model to include source-specific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, etc.",
                "System-8: [13] it split document representation into two parts: named entities and non-named entities, and choose one effective part for each news class.",
                "Table 3 and table 4 show topic-weighted normalized costs and comparing times on TDT2 and TDT3 datasets respectively.",
                "Since no heldout data set for fine-tuning the threshold θ new was available for experiments on TDT2, we only report minimum normalized costs for our systems in table 3.",
                "System-5 outperforms all other systems including system-6, and it performs only 2.78e+8 comparing times in detection procedure which is only 13.4% of system-1.",
                "Table 3.",
                "NED results on TDT2 Systems Min Norm(CDet) Cmp times System-1 0.5749 2.08e+9 System-2① 0.6673 3.77e+8 System-3② 0.5765 2.81e+8 System-4② 0.5431 2.99e+8 System-5② 0.5089 2.78e+8 System-6 0.5300 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 When evaluating on the normalized costs on TDT3, we use the optimal thresholds obtained from TDT2 data set for all systems.",
                "System-2 reduces comparing times to 1.29e+9 which is just 18.3% of system-1, but at the same time it also gets a deteriorated minimum normalized cost which is 0.0499 higher than system-1.",
                "System-3 uses the new detection procedure based on news indexing-tree.",
                "It requires even less comparing times than system-2.",
                "This is because story-story comparisons usually yield greater similarities than story-cluster ones, so stories tend to be combined Location Person Date Organization Money Percentage NN JJ CD Elections 0.37 1 0.04 0.58 0.08 0.03 0.32 0.13 0.1 Scandals/Hearings 0.66 0.62 0.28 1 0.11 0.02 0.27 0.13 0.05 Legal/Criminal Cases 0.48 1 0.02 0.62 0.15 0 0.22 0.24 0.09 Natural Disasters 1 0.27 0 0.04 0.04 0 0.25 0.04 0.02 Violence or War 1 0.36 0.02 0.14 0.02 0.04 0.21 0.11 0.02 Science and Discovery 0.11 1 0.01 0.22 0.08 0.12 0.19 0.08 0.03 Finances 1 0.45 0.04 0.98 0.13 0.02 0.29 0.06 0.05 Sports 0.16 0.27 0.01 1 0.02 0 0.11 0.03 0.01 together in system-3.",
                "And system-3 is basically equivalent to system-1 in accuracy results.",
                "System-4 adjusts term weights based on the distance of term distributions between the whole corpus and cluster story set, yielding a good improvement by 0.0468 compared to system-1.",
                "The best system (system-5) has a minimum normalized cost 0.5012, which is 0.0797 better than system-1, and also better than any other results previously reported for this dataset [8, 13].",
                "Further more, system-5 only needs 1.05e+8 comparing times which is 14.9% of system-1.",
                "Table 4.",
                "NED results on TDT3 Systems Norm(CDet) Min Norm(CDet) Cmp times System-1 0.6159 0.5809 7.04e+8 System-2① 0.6493 0.6308 1.29e+8 System-3② 0.6197 0.5868 1.03e+8 System-4② 0.5601 0.5341 1.03e+8 System-5② 0.5413 0.5012 1.05e+8 System-7 -- 0.5783 -System-8 -- 0.5229 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 Figure5 shows the five DET curves for our systems on data set TDT3.",
                "System-5 achieves the minimum cost at a false alarm rate of 0.0157 and a miss rate of 0.4310.",
                "We can observe that System4 and System-5 obtain lower miss probability at regions of low false alarm probabilities.",
                "The hypothesis is that, more weight value is transferred to key terms of topics from non-key terms.",
                "Similarity score between two stories belonging to different topics are lower than before, because their overlapping terms are usually not key terms of their topics. 7.2 Parameter selection for indexing-tree detection Figure 3 shows the minimum normalized costs obtained by system-3 on TDT3 using different parameters.",
                "Theθ init parameter is tested on six values spanning from 0.03 to 0.18.",
                "And the λ parameter is tested on four values 1, 2, 3 and 4.",
                "We can see that, whenθ init is set to 0.12, which is the closest one toθ new, the costs are lower than others.",
                "This is easy to explain, because when stories belonging to the same topic are put in a cluster, it is more reasonable for the cluster to represent the stories in it.",
                "When parameter λ is set to 3 or 4, the costs are better than other cases, but there is no much difference between 3 and 4. 0 0.05 0.1 0.15 0.2 1 2 3 4 0.5 0.6 0.7 0.8 0.9 1 θ-initλ MinCost 0.6 0.65 0.7 0.75 0.8 0.85 0.9 Figure 3.",
                "Min Cost on TDT3 (δ =0.15) 0 0.05 0.1 0.15 0.2 1 2 3 4 0 0.5 1 1.5 2 2.5 x 10 8 θ-init λ Comparingtimes 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 8 Figure 4.",
                "Comparing times on TDT3 (δ =0.15) Figure 4 gives the comparing times used by system-3 on TDT3 with the same parameters as figure 3.",
                "The comparing times are strongly dependent onθ init.",
                "Because the greaterθ init is, the less stories combined together, the more comparing times are needed for new event decision.",
                "So we useθ init =0.13,λ =3,δ =0.15 for system-3, 4, and 5.",
                "In this parameter setting, we can get both low minimum normalized costs and less comparing times. 8.",
                "CONCLUSION We have proposed a news indexing-tree based detection procedure in our model.",
                "It reduces comparing times to about one seventh of traditional method without hurting <br>ned accuracy</br>.",
                "We also have presented two extensions to the basic TF-IDF model.",
                "The first extension is made by adjust term weights based on term distributions between the whole corpus and a cluster story set.",
                "And the second extension to basic TF-IDF model is better use of term types (named entities types and part-of-speed) according to news categories.",
                "Our experimental results on TDT2 and TDT3 datasets show that both of the two extensions contribute significantly to improvement in accuracy.",
                "We did not consider news time information as a clue for NED task, since most of the topics last for a long time and TDT data sets only span for a relative short period (no more than 6 months).",
                "For the future work, we want to collect news set which span for a longer period from internet, and integrate time information in NED task.",
                "Since topic is a relative coarse-grained news cluster, we also want to refine cluster granularity to event-level, and identify different events and their relations within a topic.",
                "Acknowledgments This work is supported by the National Natural Science Foundation of China under Grant No. 90604025.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsor. 9.",
                "REFERENCES [1] http://www.nist.gov/speech/tests/tdt/index.htm [2] In Topic Detection and Tracking.",
                "Event-based Information Organization.",
                "Kluwer Academic Publishers, 2002. .01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90 1 2 5 10 20 40 60 80 90 False Alarm Probability (in %) MissProbability(in%) SYSTEM1 Topic Weighted Curve SYSTEM1 Min Norm(Cost) SYSTEM2 Topic Weighted Curve SYSTEM2 Min Norm(Cost) SYSTEM3 Topic Weighted Curve SYSTEM3 Min Norm(Cost) SYSTEM4 Topic Weighted Curve SYSTEM4 Min Norm(Cost) SYSTEM5 Topic Weighted Curve SYSTEM5 Min Norm(Cost) Random Performance Figure 5.",
                "DET curves on TDT3 [3] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B.T.",
                "Archibald, and X. Liu.",
                "Learning Approaches for Detecting and Tracking News Events.",
                "In IEEE Intelligent Systems Special Issue on Applications of Intelligent Information Retrieval, volume 14 (4), 1999, 32-43. [4] Y. Yang, T. Pierce, and J. Carbonell.",
                "A Study on Retrospective and On-line Event Detection.",
                "In Proceedings of SIGIR-98, Melbourne, Australia, 1998, 28-36. [5] J. Allan, V. Lavrenko, D. Malin, and R. Swan.",
                "Detections, Bounds, and Timelines: Umass and tdt-3.",
                "In Proceedings of Topic Detection and Tracking Workshop (TDT-3), Vienna, VA, 2000, 167-174. [6] R. Papka and J. Allan.",
                "On-line New Event Detection Using Single Pass Clustering TITLE2:.",
                "Technical Report UM-CS1998-021, 1998. [7] W. Lam, H. Meng, K. Wong, and J.",
                "Yen.",
                "Using Contextual Analysis for News Event Detection.",
                "International Journal on Intelligent Systems, 2001, 525-546. [8] B. Thorsten, C. Francine, and F. Ayman.",
                "A System for New Event Detection.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2003, 330-337. [9] S. Nicola and C. Joe.",
                "Combining Semantic and Syntactic Document Classifiers to Improve First Story Detection.",
                "In Proceedings of the 24th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topicconditioned Novelty Detection.",
                "In Proceedings of the 8th ACM SIGKDD International Conference, ACM Press. 2002, 688-693. [11] M. Juha, A.M. Helena, and S. Marko.",
                "Applying Semantic Classes in Event Detection and Tracking.",
                "In Proceedings of International Conference on Natural Language Processing (ICON 2002), 2002, pages 175-183. [12] M. Juha, A.M. Helena, and S. Marko.",
                "Simple Semantics in Topic Detection and Tracking.",
                "Information Retrieval, 7(3-4): 2004, 347-368. [13] K. Giridhar and J. Allan.",
                "Text Classification and Named Entities for New Event Detection.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2004, 297-304. [14] J. P. Callan, W. B. Croft, and S. M. Harding.",
                "The INQUERY Retrieval System.",
                "In Proceedings of DEXA-92, 3rd International Conference on Database and Expert Systems Applications, 1992, 78-83. [15] R. Krovetz.",
                "Viewing Morphology as An Inference Process.",
                "In Proceedings of ACM SIGIR93, 1993, 61-81. [16] Y. Yang and J. Pedersen.",
                "A Comparative Study on Feature Selection in Text Categorization.",
                "In J. D. H. Fisher, editor, The Fourteenth International Conference on Machine Learning (ICML97), Morgan Kaufmann, 1997, 412-420. [17] T. M. Cover, and J.A.",
                "Thomas.",
                "Elements of Information Theory.",
                "Wiley. 1991. [18] The linguistic data consortium, http://www.ldc,upenn.edu/. [19] The 2001 TDT task definition and evaluation plan, http://www.nist.gov/speech/tests/tdt/tdt2001/evalplan.htm. [20] R. E. Schapire and Y.",
                "Singer.",
                "Boostexter: A Boosting-based System for Text Categorization.",
                "In Machine Learning 39(2/3):1, Kluwer Academic Publishers, 2000, 35-168. [21] K. Giridhar and J. Allan. 2005.",
                "Using Names and Topics for New Event Detection.",
                "In Proceedings of Human Technology Conference and Conference on Empirical Methods in Natural Language, Vancouver, 2005, 121-128"
            ],
            "original_annotated_samples": [
                "Moreover, based on the observation that terms of different types have different effects for NED task, two term reweighting approaches are proposed to improve <br>ned accuracy</br>.",
                "In section 5, two term reweighting methods are proposed to improve <br>ned accuracy</br>.",
                "Term Reweighting Methods In this section, two term reweighting methods are proposed to improve <br>ned accuracy</br>.",
                "It reduces comparing times to about one seventh of traditional method without hurting <br>ned accuracy</br>."
            ],
            "translated_annotated_samples": [
                "Además, basándose en la observación de que los términos de diferentes tipos tienen diferentes efectos para la tarea de NED, se proponen dos enfoques de reponderación de términos para mejorar la <br>precisión de NED</br>.",
                "En la sección 5, se proponen dos métodos de reponderación de términos para mejorar la <br>precisión de NED</br>.",
                "En esta sección, se proponen dos métodos de reponderación de términos para mejorar la <br>precisión de NED</br>.",
                "Reduce los tiempos de comparación a aproximadamente una séptima parte del método tradicional sin afectar la <br>precisión de NED</br>."
            ],
            "translated_text": "Detección de nuevos eventos basada en árbol de indexación y entidad nombrada Zhang Kuo Universidad Tsinghua Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Universidad Tsinghua Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Universidad Tsinghua Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn RESUMEN La Detección de Nuevos Eventos (NED) tiene como objetivo detectar de una o varias corrientes de noticias cuál se informa sobre un nuevo evento (es decir, no reportado previamente). Con el abrumador volumen de noticias disponible hoy en día, hay una creciente necesidad de un sistema NED que sea capaz de detectar eventos nuevos de manera más eficiente y precisa. En este artículo proponemos un nuevo modelo de NED para acelerar la tarea de NED mediante el uso dinámico de un árbol de indexación de noticias. Además, basándose en la observación de que los términos de diferentes tipos tienen diferentes efectos para la tarea de NED, se proponen dos enfoques de reponderación de términos para mejorar la <br>precisión de NED</br>. En el primer enfoque, proponemos ajustar dinámicamente los pesos de los términos basados en los clusters de historias previas y en el segundo enfoque, proponemos emplear estadísticas en los datos de entrenamiento para aprender el modelo de reajuste de entidades nombradas para cada clase de historias. Los resultados experimentales en dos conjuntos de datos del Consorcio de Datos Lingüísticos (LDC), TDT2 y TDT3, muestran que el modelo propuesto puede mejorar significativamente tanto la eficiencia como la precisión de la tarea de NED, en comparación con el sistema base y otros sistemas existentes. Categorías y Descriptores de Asignaturas H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información; H.4.2 [Aplicaciones de Sistemas de Información]: Tipos de Sistemas de Apoyo a Decisiones. Términos generales Algoritmos, Rendimiento, Experimentación 1. El programa de Detección y Seguimiento de Temas (TDT) tiene como objetivo desarrollar técnicas que puedan organizar, buscar y estructurar de manera efectiva materiales de texto de noticias de una variedad de agencias de noticias y medios de difusión [1]. La Detección de Eventos Nuevos (DEN) es una de las cinco tareas en TDT. Es la tarea de identificación en línea del informe más temprano para cada tema tan pronto como ese informe llegue en la secuencia de documentos. Un tema se define como un evento o actividad seminal, junto con eventos y actividades directamente relacionados [2]. Un evento se define como algo (no trivial) que sucede en un lugar y en un momento determinados [3]. Por ejemplo, cuando una bomba explota en un edificio, la explosión es el evento seminal que desencadena el tema, y otras historias sobre el mismo tema serían aquellas que discuten los esfuerzos de salvamento, la búsqueda de los perpetradores, arrestos y juicios, entre otros. La información útil de noticias suele estar enterrada en una masa de datos generados a diario. Por lo tanto, los sistemas NED son muy útiles para las personas que necesitan detectar información novedosa de un flujo de noticias en tiempo real. Estas necesidades de la vida real a menudo se presentan en ámbitos como los mercados financieros, el análisis de noticias y la recopilación de inteligencia. En la mayoría de los sistemas NED de última generación, cada noticia disponible se compara con todas las noticias previamente recibidas. Si todas las similitudes entre ellos no superan un umbral, entonces la historia desencadena un nuevo evento. Por lo general, se presentan en forma de similitud de coseno o métrica de similitud de Hellinger. El problema central de NED es identificar si dos historias tratan sobre el mismo tema. Obviamente, estos sistemas no pueden aprovechar la información del tema. Además, no es aceptable en aplicaciones reales debido a la gran cantidad de cálculos requeridos en el proceso de NED. Otros sistemas organizan historias anteriores en grupos (cada grupo corresponde a un tema), y la nueva historia se compara con los grupos anteriores en lugar de con historias. Esta forma puede reducir significativamente los tiempos de comparación. Sin embargo, se ha demostrado que este método es menos preciso [4, 5]. Esto se debe a que a veces las historias dentro de un tema se alejan mucho entre sí, lo que podría resultar en una baja similitud entre una historia y su tema. Por otro lado, algunos sistemas NED propuestos intentaron mejorar la precisión al hacer un mejor uso de las entidades nombradas [10, 11, 12, 13]. Sin embargo, ninguno de los sistemas ha considerado que los términos de diferentes tipos (por ejemplo, Los sustantivos, verbos o nombres de personas tienen diferentes efectos en diferentes clases de historias para determinar si dos historias tratan sobre el mismo tema. Por ejemplo, los nombres de los candidatos electorales (Nombre de la persona) son muy importantes para las historias de la clase de elecciones; las ubicaciones (Nombre de la ubicación) donde ocurrieron los accidentes son importantes para las historias de la clase de accidentes. Por lo tanto, en NED, todavía existen los siguientes tres problemas por investigar: (1) ¿Cómo acelerar el procedimiento de detección sin disminuir la precisión de la detección? (2) ¿Cómo aprovechar mejor la información de los grupos (temas) para mejorar la precisión? (3) ¿Cómo obtener una mejor representación de historias de noticias mediante una mejor comprensión de las entidades nombradas? Impulsados por estos problemas, hemos propuesto tres enfoques en este documento. (1) Para hacer que el procedimiento de detección sea más rápido, proponemos un nuevo procedimiento de NED basado en un árbol de indexación de noticias creado dinámicamente. El índice de historias en árbol se crea ensamblando historias similares para formar grupos de noticias en diferentes jerarquías según sus valores de similitud. Las comparaciones entre la historia actual y los grupos anteriores podrían ayudar a encontrar la historia más similar en menos tiempo de comparación. El nuevo procedimiento puede reducir la cantidad de veces de comparación sin afectar la precisión. Utilizamos los grupos del primer piso en el árbol de indexación como temas de noticias, en los cuales los pesos de los términos se ajustan dinámicamente según la distribución de términos en los grupos. En este enfoque, se utiliza adecuadamente la información de los grupos (temas), por lo que se evita el problema de la descentralización del tema. Basándonos en observaciones de las estadísticas obtenidas de los datos de entrenamiento, encontramos que los términos de diferentes tipos (por ejemplo, Los sustantivos y verbos tienen diferentes efectos para diferentes clases de historias al determinar si dos historias tratan sobre el mismo tema. Y proponemos utilizar estadísticas para optimizar los pesos de los términos de diferentes tipos en una historia de acuerdo a la clase de noticias a la que pertenece la historia. En el conjunto de datos TDT3, el nuevo modelo NED solo utiliza un 14.9% en comparación con los tiempos del modelo básico, mientras que su costo normalizado mínimo es de 0.5012, lo que es 0.0797 mejor que el modelo básico, y también mejor que cualquier otro resultado previamente reportado para este conjunto de datos [8, 13]. El resto del documento está organizado de la siguiente manera. Comenzamos este artículo resumiendo el trabajo previo en NED en la sección 2. La sección 3 presenta el modelo básico para NED que la mayoría de los sistemas actuales utilizan. La sección 4 describe nuestro nuevo procedimiento de detección basado en el índice de árbol de noticias. En la sección 5, se proponen dos métodos de reponderación de términos para mejorar la <br>precisión de NED</br>. La sección 6 presenta nuestros datos experimentales y métricas de evaluación. Finalmente concluimos con los resultados experimentales en la Sección 7, y las conclusiones y trabajo futuro en la Sección 8. TRABAJO RELACIONADO Papka et al. propusieron el agrupamiento de un solo paso en NED [6]. Cuando se encontraba una nueva historia, esta era procesada inmediatamente para extraer características de términos y se construía una representación de consulta del contenido de la historia. Luego se comparó con todas las consultas anteriores. Si el documento no generaba ninguna consulta al exceder un umbral, se marcaba como un nuevo evento. Lam et al construyen representaciones de consultas previas de grupos de historias, cada una de las cuales corresponde a un tema [7]. De esta manera se realizan comparaciones entre historias y grupos. En los últimos años, la mayoría de los trabajos se han centrado en proponer mejores métodos para la comparación de historias y la representación de documentos. Brants et al. [8] extendieron un modelo básico incremental de TF-IDF para incluir modelos específicos de fuente, normalización de puntajes de similitud basada en promedios específicos de documentos, normalización de puntajes de similitud basada en promedios específicos de pares de fuentes, reponderación de términos basada en frecuencias de eventos inversas y segmentación de documentos. Se mostraron buenas mejoras en los puntos de referencia de TDT. Stokes et al. [9] utilizaron una combinación de evidencia de dos representaciones distintas del contenido de un documento. Una de las representaciones era el vector de texto libre habitual, la otra hacía uso de cadenas léxicas (creadas utilizando WordNet) para construir otro vector de términos. Luego, las dos representaciones se combinan de forma lineal. Se logró un aumento marginal en la efectividad cuando se utilizó la representación combinada. Algunos esfuerzos se han realizado sobre cómo utilizar entidades nombradas para mejorar la desambiguación de entidades nombradas (NED). Yang et al. asignaron un peso cuatro veces mayor a las entidades con nombres de ubicación que a otros términos y entidades con nombres [10]. El grupo de investigación DOREMI combinó similitudes semánticas de nombres de personas, nombres de lugares y tiempo junto con similitud textual [11][12]. El grupo de investigación de UMass [13] dividió la representación del documento en dos partes: entidades nombradas y entidades no nombradas. Y se descubrió que algunas clases de noticias podían lograr un mejor rendimiento utilizando la representación de entidades nombradas, mientras que otras clases de noticias podían lograr un mejor rendimiento utilizando la representación sin entidades nombradas. Tanto [10] como [13] utilizaron la técnica de categorización de texto para clasificar previamente las noticias. En [13] las noticias se clasifican automáticamente al principio, y luego se prueban las sensibilidades de los nombres y términos que no son nombres para NED para cada clase. En [10], se eliminan los términos frecuentes de cada clase de la representación del documento. Por ejemplo, la palabra \"elección\" no ayuda a identificar diferentes elecciones. En su trabajo, no se investiga la efectividad de diferentes tipos de nombres (o términos con diferentes categorías gramaticales) para NED en diferentes clases de noticias. Utilizamos análisis estadístico para revelar el hecho y utilizarlo para mejorar el rendimiento de NED. 3. MODELO BÁSICO En esta sección, presentamos el modelo básico de Detección de Nuevos Eventos que es similar a lo que la mayoría de los sistemas actuales aplican. Luego, proponemos nuestro nuevo modelo ampliando el modelo básico. Los sistemas de detección de eventos nuevos utilizan flujos de noticias como entrada, en los cuales las historias están estrictamente ordenadas por tiempo. Solo están disponibles las historias recibidas previamente al tratar con la historia actual. La salida es una decisión sobre si la historia actual es un evento nuevo o no y la confianza de la decisión. Por lo general, un modelo NED consta de tres partes: representación de la historia, cálculo de similitud y procedimiento de detección. 3.1 Representación de la historia Se necesita preprocesamiento antes de generar la representación de la historia. Para el preprocesamiento, tokenizamos palabras, reconocemos abreviaturas, normalizamos abreviaturas, agregamos etiquetas de partes del discurso, eliminamos palabras vacías incluidas en la lista de paradas utilizada en InQuery [14], reemplazamos palabras con sus raíces utilizando el algoritmo K-stem[15], y luego generamos un vector de palabras para cada noticia. Utilizamos el modelo TF-IDF incremental para el cálculo del peso de los términos [4]. En un modelo TF-IDF, la frecuencia de término en un documento de noticias se pondera por la frecuencia inversa del documento, la cual se genera a partir del corpus de entrenamiento. Cuando surge un nuevo término en el proceso de prueba, hay dos soluciones: simplemente ignorar el nuevo término o establecer df del término como una constante pequeña (por ejemplo, df = 1). El nuevo término recibe un peso demasiado bajo en la primera solución (0) y un peso demasiado alto en la segunda solución. En el modelo TF-IDF incremental, las frecuencias de documentos se actualizan dinámicamente en cada paso de tiempo t: 1( ) ( ) ( )t t D tdf w df w df w−= + (1) donde Dt representa el conjunto de noticias recibidas en el tiempo t, y dfDt(w) significa el número de documentos en los que aparece el término w, y dft(w) significa el número total de documentos en los que aparece el término w antes del tiempo t. En este trabajo, cada ventana de tiempo incluye 50 noticias. Así, cada historia d recibida en t se representa de la siguiente manera: 1 2{ ( , , ), ( , , ),..., ( , , )}nd peso d t w peso d t w peso d t w→ donde n significa el número de términos distintos en la historia d, y ( , , )peso d t w significa el peso del término w en la historia d en el tiempo t: log( ( , ) 1) log(( 1) /( ( ) 0.5)) ( , , ) log( ( , ) 1) log(( 1) /( ( ) 0.5)) t t t t w d tf d w N df w peso d t w tf d w N df w ∈ + + + = + + +∑ (2) donde Nt significa el número total de historias de noticias antes del tiempo t, y tf(d,w) significa cuántas veces aparece el término w en la historia de noticias d. 3.2 Cálculo de Similitud Utilizamos la distancia de Hellinger para el cálculo de la similitud entre dos historias, para dos historias d y d en el tiempo t, su similitud se define de la siguiente manera: , ( , , ) ( , , ) * ( , , ) w d d sim d d t peso d t w peso d t w ∈ = ∑ (3) 3.3 Procedimiento de Detección Para cada historia d recibida en el paso de tiempo t, el valor ( ) ( ) ( ) ( ( , , )) tiempo d tiempo d n d max sim d d t < = (4) es un puntaje utilizado para determinar si d es una historia sobre un nuevo tema y al mismo tiempo es una indicación de la confianza en nuestra decisión [8]. tiempo(d) significa el tiempo de publicación de la historia d. Si el puntaje supera el umbral θ new, entonces existe un documento suficientemente similar, por lo tanto, d es una historia antigua, de lo contrario, no hay un documento previo suficientemente similar, por lo tanto, d es una historia nueva. 4. Nuevo procedimiento NED Los sistemas NED tradicionales se pueden clasificar en dos tipos principales en cuanto al procedimiento de detección: (1) tipo S-S, en el que la historia en cuestión se compara con cada historia recibida previamente, y se utiliza la mayor similitud para determinar si la historia actual trata sobre un evento nuevo; (2) tipo S-C, en el que la historia en cuestión se compara con todos los clusters anteriores, cada uno de los cuales representa un tema, y se utiliza la mayor similitud para la decisión final sobre la historia actual. Si la similitud más alta supera el umbral θ nuevo, entonces es una historia antigua y se coloca en el clúster más similar; de lo contrario, es una historia nueva y se crea un nuevo clúster. Trabajos anteriores muestran que la primera forma es más precisa que la segunda [4][5]. Dado que a veces las historias dentro de un tema se alejan mucho entre sí, una historia puede tener una similitud muy baja con su tema. Por lo tanto, utilizar similitudes entre historias para determinar una nueva historia es mejor que utilizar similitudes entre historias y grupos. Sin embargo, el primer método requiere mucho más tiempo de comparación, lo que significa que es menos eficiente. Proponemos un nuevo procedimiento de detección que utiliza comparaciones con clusters anteriores para ayudar a encontrar la historia más similar en menos tiempo de comparación, y la decisión final sobre el nuevo evento se toma de acuerdo con la historia más similar. Por lo tanto, podemos obtener tanto la precisión de los métodos de tipo S-S como la eficiencia de los métodos de tipo S-C. El nuevo procedimiento crea un árbol de indexación de noticias dinámicamente, en el cual se agrupan historias similares para formar una jerarquía de clusters. Indexamos historias similares juntas por su ancestro común (un nodo de clúster). Historias diferentes están indexadas en diferentes grupos. Cuando una historia está llegando, utilizamos comparaciones entre la historia actual y los grupos jerárquicos anteriores para ayudar a encontrar la historia más similar que es útil para la toma de decisiones sobre nuevos eventos. Después de que se tome la decisión sobre el nuevo evento, la historia actual se inserta en el árbol de indexación para la detección siguiente. El índice de noticias en forma de árbol se define formalmente de la siguiente manera: S-Tree = {r, NC, NS, E}, donde r es la raíz de S-Tree, NC es el conjunto de todos los nodos de clúster, NS es el conjunto de todos los nodos de historia, y E es el conjunto de todos los bordes en S-Tree. Definimos un conjunto de restricciones para un S-Árbol: ⅰ. , es un nodo no terminal en el árbolC i i N i∀ ∈ → ⅱ. , es un nodo terminal en el árbolS i i N i∀ ∈ → ⅲ. , el grado de salida de es al menos 2C i i N i∀ ∈ → ⅳ. , se representa como el centroide de sus descendientesC i i iN∀ ∈ → Para una noticia di, el procedimiento de comparación y el procedimiento de inserción basados en el árbol de indexación se definen de la siguiente manera. Un ejemplo se muestra en la Figura 1 y la Figura 2. Figura 1. Procedimiento de comparación Figura 2. Procedimiento de comparación de inserción: Paso 1: comparar di con todos los nodos hijos directos de r y seleccionar λ nodos con las similitudes más altas, por ejemplo, C1 2 y C1 3 en la Figura 1. Paso 2: para cada nodo seleccionado en el paso anterior, por ejemplo. C1 2, compara di con todos sus nodos hijos directos y selecciona λ nodos con las similitudes más altas, por ejemplo. C2 2 y d8. Repetir el paso 2 para todos los nodos no terminales. Paso 3: registrar el nodo terminal con la mayor similitud a di, por ejemplo, s5, y el valor de similitud (0.20). Insertando di en el S-árbol con r como raíz: Encuentra el nodo n que es hijo directo de r en el camino desde r hasta el nodo terminal con la similitud más alta s, por ejemplo. C1 2. Si s es menor que θ init + (h-1)δ, entonces agregar di al árbol como un hijo directo de r. De lo contrario, si n es un nodo terminal, crear un nodo de clúster en lugar de n, y agregar tanto n como di como sus hijos directos; si n es un nodo no terminal, repetir este procedimiento e insertar di en el subárbol con n como raíz de forma recursiva. Aquí h es la longitud entre n y la raíz del árbol S. Cuanto más similares sean las historias en un grupo, mejor representará el grupo a las historias en él. Por lo tanto, no imponemos restricciones en la altura máxima de los árboles y el grado de un nodo. Por lo tanto, no podemos dar la complejidad de este procedimiento basado en árboles de indexación. Pero proporcionaremos el número de veces de comparación necesarias por el nuevo procedimiento en nuestros experimentos en la sección 7.5. En esta sección, se proponen dos métodos de reponderación de términos para mejorar la <br>precisión de NED</br>. En el primer método, se explora una nueva forma de utilizar de manera más efectiva la información de los grupos (temas). El segundo encuentra una mejor manera de utilizar entidades nombradas basadas en la clasificación de noticias. 5.1 Reajuste de términos basado en la distancia de distribución TF-IDF es el modelo más prevalente utilizado en sistemas de recuperación de información. La idea básica es que cuanto menos documentos aparezca un término, más importante es el término en la discriminación de documentos (relevantes o no relevantes para una consulta que contenga el término). Sin embargo, en el dominio de TDT, necesitamos discriminar documentos en función de los temas en lugar de las consultas. Intuitivamente, el uso de vectores de clúster (tema) para comparar con historias de noticias posteriores debería tener un mejor rendimiento que el uso de vectores de historias. Desafortunadamente, los resultados experimentales no respaldan esta intuición [4][5]. Basándonos en la observación de los datos, encontramos que la razón es que un tema de noticias generalmente contiene muchos eventos directa o indirectamente relacionados, y todos ellos tienen sus propios subtemas que suelen ser diferentes entre sí. Tomemos el tema descrito en la sección 1 como ejemplo, eventos como la explosión y el rescate tienen similitudes muy bajas con eventos sobre juicios criminales, por lo tanto, las historias sobre juicios tendrían una baja similitud con el vector de tema construido en base a sus eventos previos. Esta sección se centra en cómo hacer un uso efectivo de la información del tema y al mismo tiempo evitar el problema de la descentralización del contenido. Al principio, clasificamos los términos en 5 clases para ayudar en el análisis de las necesidades del modelo modificado: Clase de términos A: términos que ocurren con frecuencia en todo el corpus, por ejemplo, año y personas. Los términos de esta clase deben recibir pesos bajos porque no ayudan mucho en la discriminación de temas. Clase B de términos: términos que ocurren con frecuencia dentro de una categoría de noticias, por ejemplo, elección, tormenta. Son útiles para distinguir dos historias en diferentes categorías de noticias. Sin embargo, no pueden proporcionar información para determinar si dos historias tratan sobre el mismo tema o temas diferentes. En otras palabras, los términos elección y tormenta no son útiles para diferenciar dos campañas electorales y dos desastres de tormenta. Por lo tanto, los términos de esta clase deberían asignarse pesos más bajos. Clase de término C: términos que ocurren con frecuencia en un tema y raramente en otros temas, por ejemplo, el nombre de un avión accidentado, el nombre de un huracán específico. Las noticias que pertenecen a diferentes temas rara vez tienen términos superpuestos en esta clase. Cuanto más frecuentemente aparezca un término en un tema, más importante es el término para una historia perteneciente al tema, por lo tanto, el término debería tener un peso más alto. Clase de término D: términos que aparecen en un tema de manera exclusiva, pero no con frecuencia. Por ejemplo, el nombre de un bombero que se desempeñó muy bien en una acción de salvamento, que puede aparecer en solo dos o tres historias pero nunca ha aparecido en otros temas. Los términos de este tipo deberían recibir más peso que en el modelo TF-IDF. Sin embargo, dado que no son populares en el tema, no es apropiado asignarles pesos demasiado altos. Clase de término E: términos con baja frecuencia en documentos y que aparecen en diferentes temas. Los términos de esta clase deberían recibir pesos más bajos. Ahora analizamos si el modelo TF-IDF puede asignar pesos adecuados a las cinco clases de términos. Obviamente, los términos de la clase A tienen un peso bajo en el modelo TF-IDF, lo cual es acorde con el requisito descrito anteriormente. En el modelo TF-IDF, los términos de la clase B dependen altamente del número de historias en una clase de noticias. El modelo TF-IDF no puede proporcionar pesos bajos si la historia que contiene el término pertenece a una clase de noticias relativamente pequeña. Para un término de clase C, cuanto más frecuentemente aparezca en un tema, menos peso le da el modelo TFIDF. Esto entra en conflicto directo con el requisito de términos en la clase C. Para los términos de la clase D, el modelo TF-IDF les asigna pesos altos de manera correcta. Pero para los términos de la clase E, el modelo TF-IDF asigna pesos altos a ellos que no son conformes con el requisito de pesos bajos. En resumen, los términos de clase B, C, E no pueden ser ponderados adecuadamente en el modelo TF-IDF. Por lo tanto, proponemos un modelo modificado para resolver este problema. Cuando θ init y θ new están configurados de cerca, asumimos que la mayoría de las historias en un clúster de primer nivel (un nodo hijo directo del nodo raíz) están en el mismo tema. Por lo tanto, utilizamos un clúster de primer nivel para capturar la distribución de términos (df para todos los términos dentro del clúster) dentro del tema de forma dinámica. La divergencia de Kullback-Leibler de la distribución de términos en un clúster de primer nivel y el conjunto completo de historias se utiliza para ajustar los pesos de los términos: ( , , ) * (1 * ( || )) ( , , ) ( , , ) * (1 * ( || )) cw tw cw tw w d D peso d t w KL P P peso d t w peso d t w KL P P γ γ ∈ + = +∑ (5) donde ( ) ( ) ( ) ( ) 1,cw cw c c c c df w df w p y p y N N = = − (6) ( ) ( ) ( ) ( ) 1,t t tw tw t t df w df w p y p y N N = = − (7) donde dfc(w) es el número de documentos que contienen el término w dentro del clúster C, y Nc es el número de documentos en el clúster C, y Nt es el número total de documentos que llegan antes del paso de tiempo t. γ es un parámetro constante, actualmente configurado manualmente en 3. La divergencia de Kullback-Leibler se define de la siguiente manera [17]: La idea básica es que, para una historia en un tema, cuanto más ocurre un término dentro del tema y menos ocurre en otros temas, se le deben asignar pesos más altos. Obviamente, el modelo modificado puede cumplir con todos los requisitos de las cinco clases de términos mencionadas anteriormente. 5.2 Reajuste de Términos Basado en el Tipo de Término y la Clase de Historia. Trabajos anteriores encontraron que algunas clases de historias de noticias podrían lograr mejoras significativas al dar un peso adicional a las entidades nombradas. Pero encontramos que los términos de diferentes tipos deben recibir una cantidad diferente de peso adicional para diferentes clases de noticias. Utilizamos open-NLP1 para reconocer tipos de entidades nombradas y etiquetas de partes del discurso para términos que aparecen en noticias. Los tipos de entidades nombradas incluyen nombre de persona, nombre de organización, nombre de ubicación, fecha, hora, dinero y porcentaje, y se seleccionan cinco partes del discurso: ninguno (NN), verbo (VB), adjetivo (JJ), adverbio (RB) y número cardinal (CD). El análisis estadístico muestra tipos de términos discriminativos a nivel de tema para diferentes clases de historias. Por conveniencia, los tipos de entidad nombrada y etiquetas de parte de la oración se llaman uniformemente tipo de término en las secciones siguientes. Determinar si dos historias tratan sobre el mismo tema es un componente básico para la tarea de NED. Por lo tanto, al principio usamos la estadística χ² para calcular las correlaciones entre términos y temas. Para un término t y un tema T, se deriva una tabla de contingencia: Tabla 1. Una tabla de contingencia de 2×2 con el número de documentos pertenecientes al tema T y no pertenecientes al tema T incluye t A B no incluye t C D. La estadística χ² para un término específico t con respecto al tema T se define como [16]: χ² = ( ) ( ) * ( ) ( ) * ( ) * ( ) * ( ) w T A B C D AD CB A C B D A B C D + + + − + + + + (9). Los temas de noticias para la tarea TDT se clasifican además en 11 reglas de interpretaciones (ROIs) 2. El ROI se puede ver como una clase de historias de nivel superior. El promedio de correlación entre un tipo de término y un ROI de tema se calcula como: 2 avg 2 ( , )( ( , ) )k m m km kT R w P w TP R p w T R P χ χ ∈ ∈ ∑ ∑（ , ）= 1 1 k=1…K, m=1…M (10) donde K es el número de tipos de términos (fijado constantemente en 12 en el artículo). M es el número de nuevas clases (ROIs, establecido en el conjunto 11 del artículo). Pk representa el conjunto de todos los términos de tipo k, y Rm representa el conjunto de todos los temas de la clase m, p(t,T) significa la probabilidad de que t ocurra en el tema T. Debido a limitaciones de espacio, solo se enumeran partes de los tipos de términos (9 tipos de términos) y partes de las clases de noticias (8 clases) en la tabla 2 con los valores promedio de correlación entre ellos. Las estadísticas se derivan de datos etiquetados en el corpus TDT2. (Los resultados en la tabla 2 ya están normalizados para mayor comodidad en la comparación). Las estadísticas en la tabla 2 indican la utilidad de diferentes tipos de términos en la discriminación de temas con respecto a diferentes clases de noticias. Podemos ver que el nombre de la ubicación es el tipo de término más útil para tres clases de noticias: Desastres Naturales, Violencia o Guerra, Finanzas. Y para otras tres categorías: Elecciones, Casos Legales/Criminales, Ciencia y Descubrimiento, el nombre de la persona es el tipo de término más discriminativo. Para Escándalos/Audiencias, la fecha es la información más importante para la discriminación de temas. Además, los casos legales/criminales y los temas financieros tienen una correlación más alta con términos relacionados con el dinero, mientras que la ciencia y el descubrimiento tienen una correlación más alta con términos de porcentaje. Los términos no nominales son más estables para diferentes clases. 1. http://opennlp.sourceforge.net/ 2. http://projects.ldc.upenn.edu/TDT3/Guide/label.html A partir del análisis de la tabla 2, es razonable ajustar el peso de los términos según su tipo de término y la clase de noticias a la que pertenece la historia. Los nuevos pesos de los términos se recalculan de la siguiente manera: ( ) ( ) ( ) ( ) ( , , ) * ( , , ) ( , , ) * clase d D tipo w T clase d D tipo w w d peso d t w peso d t w peso d t w α α ∈ = ∑ (11) donde type(w) representa el tipo del término w, y class(d) representa la clase de la historia d, c kα es el parámetro de recalibración para la clase de noticias c y el tipo de término k. En el trabajo, simplemente utilizamos las estadísticas de la tabla 2 como parámetros de recalibración. Aunque usar las estadísticas directamente puede que no sea la mejor opción, no discutimos cómo obtener automáticamente los mejores parámetros. Intentaremos utilizar técnicas de aprendizaje automático para obtener los mejores parámetros en el trabajo futuro. En el trabajo, utilizamos BoosTexter [20] para clasificar todas las historias en uno de los 11 ROIs. BoosTexter es un programa de aprendizaje automático basado en boosting, que crea una serie de reglas simples para construir un clasificador de datos de texto o atributos-valor. Utilizamos el peso del término generado utilizando el modelo TF-IDF como característica para la clasificación de historias. Entrenamos el modelo con las 12000 historias en inglés evaluadas en TDT2, y clasificamos el resto de las historias en TDT2 y todas las historias en TDT3. Los resultados de clasificación se utilizan para la reponderación de términos en la fórmula (11). Dado que las etiquetas de clase de las historias de tema apagado no se proporcionan en los conjuntos de datos de TDT, no podemos proporcionar la precisión de clasificación aquí. Por lo tanto, no discutimos los efectos de la precisión de la clasificación en el rendimiento de NED en el artículo. 6. CONFIGURACIÓN EXPERIMENTAL 6.1 Conjuntos de datos Utilizamos dos conjuntos de datos de LDC [18], TDT2 y TDT3, para nuestros experimentos. TDT2 contiene noticias desde enero hasta junio de 1998. Contiene alrededor de 54,000 historias de fuentes como ABC, Associated Press, CNN, New York Times, Public Radio International, Voice of America, etc. Solo se consideraron historias en inglés en la colección. TDT3 contiene aproximadamente 31,000 historias en inglés recopiladas de octubre a diciembre de 1998. Además de las fuentes utilizadas en TDT2, también contiene historias de las transmisiones de televisión de NBC y MSNBC. Utilizamos versiones transcritas de las emisiones de televisión y radio además de noticias textuales. El conjunto de datos TDT2 está etiquetado con alrededor de 100 temas, y aproximadamente 12,000 historias en inglés pertenecen al menos a uno de estos temas. El conjunto de datos TDT3 está etiquetado con alrededor de 120 temas, y aproximadamente 8000 historias en inglés pertenecen al menos a uno de estos temas. Todos los temas están clasificados en 11 Reglas de Interpretación: (1) Elecciones, (2) Escándalos/Audiencias, (3) Casos Legales/Criminales, (4) Desastres Naturales, (5) Accidentes, (6) Violencia o Guerra en Curso, (7) Noticias de Ciencia y Descubrimientos, (8) Finanzas, (9) Nueva Ley, (10) Noticias Deportivas, (11) Varios. Noticias. 6.2 Métrica de evaluación TDT utiliza una función de costo CDet que combina las probabilidades de perder una nueva historia y una falsa alarma [19]: * * * *Det Miss Miss Target FA FA NontargetC C P P C P P= + (12) Tabla 2. Correlación promedio entre tipos de términos y clases de noticias donde CMiss significa el costo de perder una nueva historia, PMiss significa la probabilidad de perder una nueva historia, y PTarget significa la probabilidad de ver una nueva historia en los datos; CFA significa el costo de una falsa alarma, PFA significa la probabilidad de una falsa alarma, y PNontarget significa la probabilidad de ver una historia antigua. El costo CDet se normaliza de tal manera que un sistema perfecto obtiene una puntuación de 0 y un sistema trivial, que es el mejor de marcar todas las historias como nuevas o antiguas, obtiene una puntuación de 1: ( ( * , * ) ) Det Det Miss Target FA Nontarget C Norm C min C P C P = (13) El sistema de detección de eventos nuevos proporciona dos salidas para cada historia. La primera parte es sí o no, indicando si la historia desencadena un nuevo evento o no. La segunda parte es una puntuación que indica la confianza de la primera decisión. Los puntajes de confianza se pueden utilizar para trazar la curva DET, es decir, curvas que representan la probabilidad de falsa alarma frente a la probabilidad de omisión. El costo normalizado mínimo se puede determinar si se elige el umbral óptimo en la puntuación. RESULTADOS EXPERIMENTALES 7.1 Resultados principales Para probar los enfoques propuestos en el modelo, implementamos y probamos cinco sistemas: Sistema-1: este sistema se utiliza como referencia. Se implementa basándose en el modelo básico descrito en la sección 3, es decir, utilizando el modelo TF-IDF incremental para generar pesos de términos y utilizando la distancia de Hellinger para calcular la similitud entre documentos. La normalización del puntaje de similitud también se emplea [8]. Se utiliza el procedimiento de detección S-S. Sistema-2: este sistema es igual que el sistema-1 excepto que se utiliza el procedimiento de detección S-C. Sistema-3: este sistema es igual que el sistema-1 excepto que utiliza el nuevo procedimiento de detección que se basa en un árbol de indexación. Sistema-4: implementado basado en el enfoque presentado en la sección 5.1, es decir, los términos se vuelven a ponderar según la distancia entre las distribuciones de términos en un grupo y todas las historias. Se utiliza el nuevo procedimiento de detección. Sistema-5: implementado basado en el enfoque presentado en la sección 5.2, es decir, los términos de diferentes tipos son reponderados según la clase de noticias utilizando parámetros entrenados. El nuevo procedimiento de detección se utiliza. Los siguientes son algunos otros sistemas NED: Sistema-6: [21] para cada par de historias, calcula tres valores de similitud para entidad nombrada, entidad no nombrada y todos los términos respectivamente. Y emplear Máquina de Vectores de Soporte para predecir nuevo o antiguo utilizando los valores de similitud como características. Sistema-7: [8] extendió un modelo básico incremental de TF-IDF para incluir modelos específicos de origen, normalización de puntuaciones de similitud basada en promedios específicos de documentos, normalización de puntuaciones de similitud basada en promedios específicos de pares de fuentes, etc. Sistema-8: [13] dividió la representación del documento en dos partes: entidades nombradas y entidades no nombradas, y eligió una parte efectiva para cada clase de noticias. La tabla 3 y la tabla 4 muestran los costos normalizados ponderados por tema y los tiempos de comparación en los conjuntos de datos TDT2 y TDT3 respectivamente. Dado que no se contaba con un conjunto de datos de validación para ajustar el umbral θ nuevo en los experimentos en TDT2, solo informamos los costos normalizados mínimos para nuestros sistemas en la tabla 3. El sistema-5 supera a todos los demás sistemas, incluido el sistema-6, y realiza solo 2.78e+8 veces la comparación de tiempos en el procedimiento de detección, lo que representa solo el 13.4% del sistema-1. Tabla 3. Los resultados de NED en los sistemas TDT2 Min Norm(CDet) tiempos de comparación son: Sistema-1 0.5749 2.08e+9, Sistema-2 0.6673 3.77e+8, Sistema-3 0.5765 2.81e+8, Sistema-4 0.5431 2.99e+8, Sistema-5 0.5089 2.78e+8, Sistema-6 0.5300. Al evaluar en los costos normalizados en TDT3, utilizamos los umbrales óptimos obtenidos del conjunto de datos de TDT2 para todos los sistemas. El sistema-2 reduce los tiempos de comparación a 1.29e+9, que es solo el 18.3% del sistema-1, pero al mismo tiempo también obtiene un costo normalizado mínimo deteriorado que es 0.0499 más alto que el sistema-1. El Sistema-3 utiliza el nuevo procedimiento de detección basado en el índice de noticias en árbol. Requiere incluso menos tiempo de comparación que el sistema-2. Esto se debe a que las comparaciones de historias suelen arrojar mayores similitudes que las de grupos de historias, por lo que las historias tienden a combinarse en el sistema-3. Y el sistema-3 es básicamente equivalente al sistema-1 en los resultados de precisión. El System-4 ajusta los pesos de los términos basándose en la distancia de las distribuciones de términos entre todo el corpus y el conjunto de historias del clúster, lo que resulta en una mejora significativa de 0.0468 en comparación con el system-1. El mejor sistema (sistema-5) tiene un costo normalizado mínimo de 0.5012, lo cual es 0.0797 mejor que el sistema-1, y también mejor que cualquier otro resultado previamente reportado para este conjunto de datos [8, 13]. Además, el sistema-5 solo necesita 1.05e+8 veces de comparación, lo que equivale al 14.9% del sistema-1. Tabla 4. Los resultados de NED en los sistemas TDT3 Norm(CDet) Norma mínima(CDet) Tiempos de comparación Sistema-1 0.6159 0.5809 7.04e+8 Sistema-2① 0.6493 0.6308 1.29e+8 Sistema-3② 0.6197 0.5868 1.03e+8 Sistema-4② 0.5601 0.5341 1.03e+8 Sistema-5② 0.5413 0.5012 1.05e+8 Sistema-7 -- 0.5783 -Sistema-8 -- 0.5229 -① θ nuevo=0.13 ② θ inicial=0.13, λ =3, δ =0.15 La Figura 5 muestra las cinco curvas DET para nuestros sistemas en el conjunto de datos TDT3. El Sistema-5 logra el costo mínimo con una tasa de falsas alarmas de 0.0157 y una tasa de omisión de 0.4310. Podemos observar que System4 y System-5 obtienen una menor probabilidad de error de omisión en regiones de baja probabilidad de falsa alarma. La hipótesis es que se transfiere más valor de peso a los términos clave de los temas que a los términos no clave. La puntuación de similitud entre dos historias pertenecientes a diferentes temas es más baja que antes, porque los términos superpuestos suelen no ser términos clave de sus temas. 7.2 Selección de parámetros para la detección de árboles de indexación. La Figura 3 muestra los costos normalizados mínimos obtenidos por el sistema-3 en TDT3 utilizando diferentes parámetros. El parámetro de inicio θ se prueba en seis valores que van desde 0.03 hasta 0.18. Y el parámetro λ se prueba en cuatro valores: 1, 2, 3 y 4. Podemos ver que, cuando θ inicial se establece en 0.12, que es el más cercano a θ nuevo, los costos son más bajos que los demás. Esto es fácil de explicar, porque cuando se agrupan historias pertenecientes al mismo tema en un conjunto, es más razonable que el conjunto represente las historias en él. Cuando el parámetro λ se establece en 3 o 4, los costos son mejores que en otros casos, pero no hay mucha diferencia entre 3 y 4. 0 0.05 0.1 0.15 0.2 1 2 3 4 0.5 0.6 0.7 0.8 0.9 1 θ-initλ MinCost 0.6 0.65 0.7 0.75 0.8 0.85 0.9 Figura 3. Costo mínimo en TDT3 (δ =0.15) 0 0.05 0.1 0.15 0.2 1 2 3 4 0 0.5 1 1.5 2 2.5 x 10 8 θ-inicio λ Comparando tiempos 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 8 Figura 4. Comparando los tiempos en TDT3 (δ =0.15), la Figura 4 muestra los tiempos de comparación utilizados por el sistema-3 en TDT3 con los mismos parámetros que la Figura 3. Los tiempos de comparación dependen fuertemente de θ init. Debido a que cuanto mayor sea θ init, menos historias se combinan juntas, más veces de comparación se necesitan para la decisión de un nuevo evento. Por lo tanto, usamos θ init = 0.13, λ = 3, δ = 0.15 para los sistemas 3, 4 y 5. En esta configuración de parámetros, podemos obtener tanto costos normalizados mínimos bajos como menos tiempos de comparación. CONCLUSIÓN Hemos propuesto un procedimiento de detección basado en un árbol de indexación de noticias en nuestro modelo. Reduce los tiempos de comparación a aproximadamente una séptima parte del método tradicional sin afectar la <br>precisión de NED</br>. También hemos presentado dos extensiones al modelo básico TF-IDF. La primera extensión se realiza ajustando los pesos de los términos basados en las distribuciones de términos entre todo el corpus y un conjunto de historias de un clúster. Y la segunda extensión al modelo básico TF-IDF es un mejor uso de los tipos de términos (tipos de entidades nombradas y partes de la velocidad) de acuerdo con las categorías de noticias. Nuestros resultados experimentales en los conjuntos de datos TDT2 y TDT3 muestran que ambas extensiones contribuyen significativamente a la mejora en la precisión. No consideramos la información de tiempo de noticias como una pista para la tarea de NED, ya que la mayoría de los temas duran mucho tiempo y los conjuntos de datos de TDT solo abarcan un período relativamente corto (no más de 6 meses). Para el trabajo futuro, queremos recopilar un conjunto de noticias que abarque un período más largo de internet e integrar información temporal en la tarea de NED. Dado que el tema es un grupo de noticias relativamente grueso, también queremos refinar la granularidad del grupo a nivel de evento e identificar diferentes eventos y sus relaciones dentro de un tema. Agradecimientos: Este trabajo ha sido apoyado por la Fundación Nacional de Ciencias Naturales de China bajo la subvención No. 90604025. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son del autor(es) y no necesariamente reflejan las del patrocinador. REFERENCIAS [1] http://www.nist.gov/speech/tests/tdt/index.htm [2] En Detección y Seguimiento de Temas. Organización de la información basada en eventos. Kluwer Academic Publishers, 2002. .01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90 1 2 5 10 20 40 60 80 90 Probabilidad de Falsa Alarma (en %) Probabilidad de Omisión (en %) Curva Ponderada por Tema del SISTEMA1 Norma Mínima (Costo) del SISTEMA1 Curva Ponderada por Tema del SISTEMA2 Norma Mínima (Costo) del SISTEMA2 Curva Ponderada por Tema del SISTEMA3 Norma Mínima (Costo) del SISTEMA3 Curva Ponderada por Tema del SISTEMA4 Norma Mínima (Costo) del SISTEMA4 Curva Ponderada por Tema del SISTEMA5 Norma Mínima (Costo) del SISTEMA5 Rendimiento Aleatorio Figura 5. Curvas DET en TDT3 [3] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B.T. Archibald y X. Liu. Enfoques de aprendizaje para detectar y rastrear eventos de noticias. En el número especial de IEEE Intelligent Systems sobre Aplicaciones de Recuperación de Información Inteligente, volumen 14 (4), 1999, páginas 32-43. [4] Y. Yang, T. Pierce y J. Carbonell. Un estudio sobre la detección de eventos retrospectivos y en línea. En Actas de SIGIR-98, Melbourne, Australia, 1998, 28-36. [5] J. Allan, V. Lavrenko, D. Malin y R. Swan. Detecciones, límites y líneas de tiempo: Umass y tdt-3. En Actas del Taller de Detección y Seguimiento de Temas (TDT-3), Viena, VA, 2000, 167-174. [6] R. Papka y J. Allan. Detección de nuevos eventos en línea utilizando agrupamiento de un solo paso TÍTULO2:. Informe técnico UM-CS1998-021, 1998. [7] W. Lam, H. Meng, K. Wong y J. I'm sorry, but \"Yen\" is not a sentence. Can you please provide a sentence for me to translate into Spanish? Utilizando el Análisis Contextual para la Detección de Eventos de Noticias. Revista Internacional de Sistemas Inteligentes, 2001, 525-546. [8] B. Thorsten, C. Francine y F. Ayman. Un sistema para la detección de nuevos eventos. En Actas de la 26ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2003, 330-337. [9] S. Nicola y C. Joe. Combinando clasificadores semánticos y sintácticos de documentos para mejorar la detección de primeras noticias. En Actas de la 24ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell, and C. Jin. \n\nACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell y C. Jin. Detección de novedades condicionada al tema. En Actas de la 8ª Conferencia Internacional de ACM SIGKDD, ACM Press. 2002, 688-693. [11] M. Juha, A.M. Helena y S. Marko. Aplicando Clases Semánticas en la Detección y Seguimiento de Eventos. En Actas de la Conferencia Internacional sobre Procesamiento del Lenguaje Natural (ICON 2002), 2002, páginas 175-183. [12] M. Juha, A.M. Helena y S. Marko. Semántica simple en la detección y seguimiento de temas. Recuperación de información, 7(3-4): 2004, 347-368. [13] K. Giridhar y J. Allan. Clasificación de texto y entidades nombradas para la detección de nuevos eventos. En Actas de la 27ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2004, 297-304. [14] J. P. Callan, W. B. Croft, y S. M. Harding. El Sistema de Recuperación INQUERY. En Actas de DEXA-92, 3ra Conferencia Internacional sobre Aplicaciones de Bases de Datos y Sistemas Expertos, 1992, 78-83. [15] R. Krovetz. Viendo la morfología como un proceso de inferencia. En Actas de ACM SIGIR93, 1993, 61-81. [16] Y. Yang y J. Pedersen. Un estudio comparativo sobre la selección de características en la categorización de textos. En J. D. H. Fisher, editor, La Decimocuarta Conferencia Internacional sobre Aprendizaje Automático (ICML97), Morgan Kaufmann, 1997, 412-420. [17] T. M. Cover y J. A. I'm sorry, but \"Thomas\" is not a sentence. Can you provide more context or a complete sentence for me to translate into Spanish? Elementos de la teoría de la información. Wiley. 1991. [18] El consorcio de datos lingüísticos, http://www.ldc,upenn.edu/. [19] La definición de la tarea TDT 2001 y el plan de evaluación, http://www.nist.gov/speech/tests/tdt/tdt2001/evalplan.htm. [20] R. E. Schapire y Y. Cantante. Boostexter: Un sistema basado en Boosting para la categorización de texto. En Machine Learning 39(2/3):1, Kluwer Academic Publishers, 2000, 35-168. [21] K. Giridhar y J. Allan. 2005. Utilizando nombres y temas para la detección de nuevos eventos. En Actas de la Conferencia de Tecnología Humana y la Conferencia sobre Métodos Empíricos en Lenguaje Natural, Vancouver, 2005, 121-128 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "term weight": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "New Event Detection Based on Indexing-tree and Named Entity Zhang Kuo Tsinghua University Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Tsinghua University Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Tsinghua University Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn ABSTRACT New Event Detection (NED) aims at detecting from one or multiple streams of news stories that which one is reported on a new event (i.e.",
                "not reported previously).",
                "With the overwhelming volume of news available today, there is an increasing need for a NED system which is able to detect new events more efficiently and accurately.",
                "In this paper we propose a new NED model to speed up the NED task by using news indexing-tree dynamically.",
                "Moreover, based on the observation that terms of different types have different effects for NED task, two term reweighting approaches are proposed to improve NED accuracy.",
                "In the first approach, we propose to adjust term weights dynamically based on previous story clusters and in the second approach, we propose to employ statistics on training data to learn the named entity reweighting model for each class of stories.",
                "Experimental results on two Linguistic Data Consortium (LDC) datasets TDT2 and TDT3 show that the proposed model can improve both efficiency and accuracy of NED task significantly, compared to the baseline system and other existing systems.",
                "Categories and Subject Descriptors H.3.3 [Information Systems]: Information Search and Retrieval; H.4.2 [Information Systems Applications]: Types of Systemsdecision support.",
                "General Terms Algorithms, Performance, Experimentation 1.",
                "INTRODUCTION Topic Detection and Tracking (TDT) program aims to develop techniques which can effectively organize, search and structure news text materials from a variety of newswire and broadcast media [1].",
                "New Event Detection (NED) is one of the five tasks in TDT.",
                "It is the task of online identification of the earliest report for each topic as soon as that report arrives in the sequence of documents.",
                "A Topic is defined as a seminal event or activity, along with directly related events and activities [2].",
                "An Event is defined as something (non-trivial) happening in a certain place at a certain time [3].",
                "For instance, when a bomb explodes in a building, the exploding is the seminal event that triggers the topic, and other stories on the same topic would be those discussing salvaging efforts, the search for perpetrators, arrests and trial and so on.",
                "Useful news information is usually buried in a mass of data generated everyday.",
                "Therefore, NED systems are very useful for people who need to detect novel information from real-time news stream.",
                "These real-life needs often occur in domains like financial markets, news analysis, and intelligence gathering.",
                "In most of state-of-the-art (currently) NED systems, each news story on hand is compared to all the previous received stories.",
                "If all the similarities between them do not exceed a threshold, then the story triggers a new event.",
                "They are usually in the form of cosine similarity or Hellinger similarity metric.",
                "The core problem of NED is to identify whether two stories are on the same topic.",
                "Obviously, these systems cannot take advantage of topic information.",
                "Further more, it is not acceptable in real applications because of the large amount of computation required in the NED process.",
                "Other systems organize previous stories into clusters (each cluster corresponds to a topic), and new story is compared to the previous clusters instead of stories.",
                "This manner can reduce comparing times significantly.",
                "Nevertheless, it has been proved that this manner is less accurate [4, 5].",
                "This is because sometimes stories within a topic drift far away from each other, which could lead low similarity between a story and its topic.",
                "On the other hand, some proposed NED systems tried to improve accuracy by making better use of named entities [10, 11, 12, 13].",
                "However, none of the systems have considered that terms of different types (e.g.",
                "Noun, Verb or Person name) have different effects for different classes of stories in determining whether two stories are on the same topic.",
                "For example, the names of election candidates (Person name) are very important for stories of election class; the locations (Location name) where accidents happened are important for stories of accidents class.",
                "So, in NED, there still exist following three problems to be investigated: (1) How to speed up the detection procedure while do not decrease the detection accuracy? (2) How to make good use of cluster (topic) information to improve accuracy? (3) How to obtain better news story representation by better understanding of named entities.",
                "Driven by these problems, we have proposed three approaches in this paper. (1)To make the detection procedure faster, we propose a new NED procedure based on news indexing-tree created dynamically.",
                "Story indexing-tree is created by assembling similar stories together to form news clusters in different hierarchies according to their values of similarity.",
                "Comparisons between current story and previous clusters could help find the most similar story in less comparing times.",
                "The new procedure can reduce the amount of comparing times without hurting accuracy. (2)We use the clusters of the first floor in the indexing-tree as news topics, in which term weights are adjusted dynamically according to term distribution in the clusters.",
                "In this approach, cluster (topic) information is used properly, so the problem of theme decentralization is avoided. (3)Based on observations on the statistics obtained from training data, we found that terms of different types (e.g.",
                "Noun and Verb) have different effects for different classes of stories in determining whether two stories are on the same topic.",
                "And we propose to use statistics to optimize the weights of the terms of different types in a story according to the news class that the story belongs to.",
                "On TDT3 dataset, the new NED model just uses 14.9% comparing times of the basic model, while its minimum normalized cost is 0.5012, which is 0.0797 better than the basic model, and also better than any other results previously reported for this dataset [8, 13].",
                "The rest of the paper is organized as follows.",
                "We start off this paper by summarizing the previous work in NED in section 2.",
                "Section 3 presents the basic model for NED that most current systems use.",
                "Section 4 describes our new detection procedure based on news indexing-tree.",
                "In section 5, two term reweighting methods are proposed to improve NED accuracy.",
                "Section 6 gives our experimental data and evaluation metrics.",
                "We finally wrap up with the experimental results in Section 7, and the conclusions and future work in Section 8. 2.",
                "RELATED WORK Papka et al. proposed Single-Pass clustering on NED [6].",
                "When a new story was encountered, it was processed immediately to extract term features and a query representation of the storys content is built up.",
                "Then it was compared with all the previous queries.",
                "If the document did not trigger any queries by exceeding a threshold, it was marked as a new event.",
                "Lam et al build up previous query representations of story clusters, each of which corresponds to a topic [7].",
                "In this manner comparisons happen between stories and clusters.",
                "Recent years, most work focus on proposing better methods on comparison of stories and document representation.",
                "Brants et al. [8] extended a basic incremental TF-IDF model to include sourcespecific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, term reweighting based on inverse event frequencies, and segmentation of documents.",
                "Good improvements on TDT bench-marks were shown.",
                "Stokes et al. [9] utilized a combination of evidence from two distinct representations of a documents content.",
                "One of the representations was the usual free text vector, the other made use of lexical chains (created using WordNet) to build another term vector.",
                "Then the two representations are combined in a linear fashion.",
                "A marginal increase in effectiveness was achieved when the combined representation was used.",
                "Some efforts have been done on how to utilize named entities to improve NED.",
                "Yang et al. gave location named entities four times weight than other terms and named entities [10].",
                "DOREMI research group combined semantic similarities of person names, location names and time together with textual similarity [11][12].",
                "UMass [13] research group split document representation into two parts: named entities and non-named entities.",
                "And it was found that some classes of news could achieve better performance using named entity representation, while some other classes of news could achieve better performance using non-named entity representation.",
                "Both [10] and [13] used text categorization technique to classify news stories in advance.",
                "In [13] news stories are classified automatically at first, and then test sensitivities of names and non-name terms for NED for each class.",
                "In [10] frequent terms for each class are removed from document representation.",
                "For example, word election does not help identify different elections.",
                "In their work, effectiveness of different kinds of names (or terms with different POS) for NED in different news classes are not investigated.",
                "We use statistical analysis to reveal the fact and use it to improve NED performance. 3.",
                "BASIC MODEL In this section, we present the basic New Event Detection model which is similar to what most current systems apply.",
                "Then, we propose our new model by extending the basic model.",
                "New Event Detection systems use news story stream as input, in which stories are strictly time-ordered.",
                "Only previously received stories are available when dealing with current story.",
                "The output is a decision for whether the current story is on a new event or not and the confidence of the decision.",
                "Usually, a NED model consists of three parts: story representation, similarity calculation and detection procedure. 3.1 Story Representation Preprocessing is needed before generating story representation.",
                "For preprocessing, we tokenize words, recognize abbreviations, normalize abbreviations, add part-of-speech tags, remove stopwords included in the stop list used in InQuery [14], replace words with their stems using K-stem algorithm[15], and then generate word vector for each news story.",
                "We use incremental TF-IDF model for <br>term weight</br> calculation [4].",
                "In a TF-IDF model, term frequency in a news document is weighted by the inverse document frequency, which is generated from training corpus.",
                "When a new term occurs in testing process, there are two solutions: simply ignore the new term or set df of the term as a small const (e.g. df = 1).",
                "The new term receives too low weight in the first solution (0) and too high weight in the second solution.",
                "In incremental TF-IDF model, document frequencies are updated dynamically in each time step t: 1( ) ( ) ( )t t D tdf w df w df w−= + (1) where Dt represents news story set received in time t, and dfDt(w) means the number of documents that term w occurs in, and dft(w) means the total number of documents that term w occurs in before time t. In this work, each time window includes 50 news stories.",
                "Thus, each story d received in t is represented as follows: 1 2{ ( , , ), ( , , ),..., ( , , )}nd weight d t w weight d t w weight d t w→ where n means the number of distinct terms in story d, and ( , , )weight d t w means the weight of term w in story d at time t: log( ( , ) 1) log(( 1) /( ( ) 0.5)) ( , , ) log( ( , ) 1) log(( 1) /( ( ) 0.5)) t t t t w d tf d w N df w weight d t w tf d w N df w ∈ + + + = + + +∑ (2) where Nt means the total number of news stories before time t, and tf(d,w) means how many times term w occurs in news story d. 3.2 Similarity Calculation We use Hellinger distance for the calculation of similarity between two stories, for two stories d and d at time t, their similarity is defined as follows: , ( , , ) ( , , ) * ( , , ) w d d sim d d t weight d t w weight d t w ∈ = ∑ (3) 3.3 Detection Procedure For each story d received in time step t, the value ( ) ( ) ( ) ( ( , , )) time d time d n d max sim d d t < = (4) is a score used to determine whether d is a story about a new topic and at the same time is an indication of the confidence in our decision [8]. time(d) means the publication time of story d. If the score exceeds the thresholdθ new, then there exists a sufficiently similar document, thus d is a old story, otherwise, there is no sufficiently similar previous document, thus d is an new story. 4.",
                "New NED Procedure Traditional NED systems can be classified into two main types on the aspect of detection procedure: (1) S-S type, in which the story on hand is compared to each story received previously, and use the highest similarity to determine whether current story is about a new event; (2) S-C type, in which the story on hand is compared to all previous clusters each of which representing a topic, and the highest similarity is used for final decision for current story.",
                "If the highest similarity exceeds thresholdθ new, then it is an old story, and put it into the most similar cluster; otherwise it is a new story and create a new cluster.",
                "Previous work show that the first manner is more accurate than the second one [4][5].",
                "Since sometimes stories within a topic drift far away from each other, a story may have very low similarity with its topic.",
                "So using similarities between stories for determining new story is better than using similarities between story and clusters.",
                "Nevertheless, the first manner needs much more comparing times which means the first manner is low efficient.",
                "We propose a new detection procedure which uses comparisons with previous clusters to help find the most similar story in less comparing times, and the final new event decision is made according to the most similar story.",
                "Therefore, we can get both the accuracy of S-S type methods and the efficiency of S-C type methods.",
                "The new procedure creates a news indexing-tree dynamically, in which similar stories are put together to form a hierarchy of clusters.",
                "We index similar stories together by their common ancestor (a cluster node).",
                "Dissimilar stories are indexed in different clusters.",
                "When a story is coming, we use comparisons between the current story and previous hierarchical clusters to help find the most similar story which is useful for new event decision.",
                "After the new event decision is made, the current story is inserted to the indexing-tree for the following detection.",
                "The news indexing-tree is defined formally as follows: S-Tree = {r, NC , NS , E} where r is the root of S-Tree, NC is the set of all cluster nodes, NS is the set of all story nodes, and E is the set of all edges in S-Tree.",
                "We define a set of constraints for a S-Tree: ⅰ . , is an non-terminal node in the treeC i i N i∀ ∈ → ⅱ . , is a terminal node in the treeS i i N i∀ ∈ → ⅲ . , out degree of is at least 2C i i N i∀ ∈ → ⅳ . , is represented as the centroid of its desendantsC i i iN∀ ∈ → For a news story di, the comparison procedure and inserting procedure based on indexing-tree are defined as follows.",
                "An example is shown by Figure 1 and Figure 2.",
                "Figure 1.",
                "Comparison procedure Figure 2.",
                "Inserting procedure Comparison procedure: Step 1: compare di to all the direct child nodes of r and select λ nodes with highest similarities, e.g., C1 2 and C1 3 in Figure 1.",
                "Step 2: for each selected node in the last step, e.g.",
                "C1 2, compare di to all its direct child nodes, and select λ nodes with highest similarities, e.g.",
                "C2 2 and d8.",
                "Repeat step 2 for all non-terminal nodes.",
                "Step 3: record the terminal node with the highest similarty to di, e.g. s5, and the similarity value (0.20).",
                "Inserting di to the S-tree with r as root: Find the node n which is direct child of r in the path from r to the terminal node with highest similarity s, e.g.",
                "C1 2.",
                "If s is smaller than θ init+(h-1)δ , then add di to the tree as a direct child of r. Otherwise, if n is a terminal node, then create a cluster node instead of n, and add both n and di as its direct children; if n is an non-terminal node, then repeat this procedure and insert di to the sub-tree with n as root recursively.",
                "Here h is the length between n and the root of S-tree.",
                "The more the stories in a cluster similar to each other, the better the cluster represents the stories in it.",
                "Hence we add no constraints on the maximum of trees height and degree of a node.",
                "Therefore, we cannot give the complexity of this indexing-tree based procedure.",
                "But we will give the number of comparing times needed by the new procedure in our experiments in section7. 5.",
                "Term Reweighting Methods In this section, two term reweighting methods are proposed to improve NED accuracy.",
                "In the first method, a new way is explored for better using of cluster (topic) information.",
                "The second one finds a better way to make use of named entities based on news classification. 5.1 Term Reweighting Based on Distribution Distance TF-IDF is the most prevalent model used in information retrieval systems.",
                "The basic idea is that the fewer documents a term appears in, the more important the term is in discrimination of documents (relevant or not relevant to a query containing the term).",
                "Nevertheless, in TDT domain, we need to discriminate documents with regard to topics rather than queries.",
                "Intuitively, using cluster (topic) vectors to compare with subsequent news stories should outperform using story vectors.",
                "Unfortunately, the experimental results do not support this intuition [4][5].",
                "Based on observation on data, we find the reason is that a news topic usually contains many directly or indirectly related events, while they all have their own sub-subjects which are usually different with each other.",
                "Take the topic described in section 1 as an example, events like the explosion and salvage have very low similarities with events about criminal trial, therefore stories about trial would have low similarity with the topic vector built on its previous events.",
                "This section focuses on how to effectively make use of topic information and at the same time avoid the problem of content decentralization.",
                "At first, we classify terms into 5 classes to help analysis the needs of the modified model: Term class A: terms that occur frequently in the whole corpus, e.g., year and people.",
                "Terms of this class should be given low weights because they do not help much for topic discrimination.",
                "Term class B: terms that occur frequently within a news category, e.g., election, storm.",
                "They are useful to distinguish two stories in different news categories.",
                "However, they cannot provide information to determine whether two stories are on the same or different topics.",
                "In another words, term election and term storm are not helpful in differentiate two election campaigns and two storm disasters.",
                "Therefore, terms of this class should be assigned lower weights.",
                "Term class C: terms that occur frequently in a topic, and infrequently in other topics, e.g., the name of a crash plane, the name of a specific hurricane.",
                "News stories that belong to different topics rarely have overlap terms in this class.",
                "The more frequently a term appears in a topic, the more important the term is for a story belonging to the topic, therefore the term should be set higher weight.",
                "Term class D: terms that appear in a topic exclusively, but not frequently.",
                "For example, the name of a fireman who did very well in a salvage action, which may appears in only two or three stories but never appeared in other topics.",
                "Terms of this type should receive more weights than in TF-IDF model.",
                "However, since they are not popular in the topic, it is not appropriate to give them too high weights.",
                "Term class E: terms with low document frequency, and appear in different topics.",
                "Terms of this class should receive lower weights.",
                "Now we analyze whether TF-IDF model can give proper weights to the five classes of terms.",
                "Obviously, terms of class A are lowly weighted in TF-IDF model, which is conformable with the requirement described above.",
                "In TF-IDF model, terms of class B are highly dependant with the number of stories in a news class.",
                "TF-IDF model cannot provide low weights if the story containing the term belongs to a relative small news class.",
                "For a term of class C, the more frequently it appears in a topic, the less weight TFIDF model gives to it.",
                "This strongly conflicts with the requirement of terms in class C. For terms of class D, TF-IDF model gives them high weights correctly.",
                "But for terms of class E, TF-IDF model gives high weights to them which are not conformable with the requirement of low weights.",
                "To sum up, terms of class B, C, E cannot be properly weighted in TF-IDF model.",
                "So, we propose a modified model to resolve this problem.",
                "When θ init andθ new are set closely, we assume that most of the stories in a first-level cluster (a direct child node of root node) are on the same topic.",
                "Therefore, we make use of a first-level cluster to capture term distribution (df for all the terms within the cluster) within the topic dynamically.",
                "KL divergence of term distribution in a first-level cluster and the whole story set is used to adjust term weights:  ( , , ) * (1 * ( || )) ( , , ) ( , , ) * (1 * ( || )) cw tw cw tw w d D weight d t w KL P P weight d t w weight d t w KL P P γ γ ∈ + = +∑ (5) where ( ) ( ) ( ) ( ) 1,cw cw c c c c df w df w p y p y N N = = − (6) ( ) ( ) ( ) ( ) 1,t t tw tw t t df w df w p y p y N N = = − (7) where dfc(w) is the number of documents containing term w within cluster C, and Nc is the number of documents in cluster C, and Nt is the total number of documents that arrive before time step t. γ is a const parameter, now is manually set 3.",
                "KL divergence is defined as follows [17]: ( ) ( || ) ( ) log ( )x p x KL P Q p x q x = ∑ (8) The basic idea is: for a story in a topic, the more a term occurs within the topic, and the less it occurs in other topics, it should be assigned higher weights.",
                "Obviously, modified model can meet all the requirements of the five term classes listed above. 5.2 Term Reweighting Based on Term Type and Story Class Previous work found that some classes of news stories could achieve good improvements by giving extra weight to named entities.",
                "But we find that terms of different types should be given different amount of extra weight for different classes of news stories.",
                "We use open-NLP1 to recognize named entity types and part-ofspeech tags for terms that appear in news stories.",
                "Named entity types include person name, organization name, location name, date, time, money and percentage, and five POSs are selected: none (NN), verb (VB), adjective (JJ), adverb (RB) and cardinal number (CD).",
                "Statistical analysis shows topic-level discriminative terms types for different classes of stories.",
                "For the sake of convenience, named entity type and part-of-speech tags are uniformly called term type in subsequent sections.",
                "Determining whether two stories are about the same topic is a basic component for NED task.",
                "So at first we use 2 χ statistic to compute correlations between terms and topics.",
                "For a term t and a topic T, a contingence table is derived: Table 1.",
                "A 2×2 Contingence Table Doc Number belong to topic T not belong to topic T include t A B not include t C D The 2 χ statistic for a specific term t with respect to topic T is defined to be [16]: 2 2 ( , ) ( ) * ( ) ( ) * ( ) * ( ) * ( ) w T A B C D AD CB A C B D A B C D χ = + + + − + + + + (9) News topics for the TDT task are further classified into 11 rules of interpretations (ROIs) 2 .",
                "The ROI can be seen as a higher level class of stories.",
                "The average correlation between a term type and a topic ROI is computed as: 2 avg 2 ( , )( ( , ) )k m m km kT R w P w TP R p w T R P χ χ ∈ ∈ ∑ ∑（ , ）= 1 1 k=1…K, m=1…M (10) where K is the number of term types (set 12 constantly in the paper).",
                "M is the number news classes (ROIs, set 11 in the paper).",
                "Pk represents the set of all terms of type k, and Rm represents the set of all topics of class m, p(t,T) means the probability that t occurs in topic T. Because of limitation of space, only parts of the term types (9 term types) and parts of news classes (8 classes) are listed in table 2 with the average correlation values between them.",
                "The statistics is derived from labeled data in TDT2 corpus. (Results in table 2 are already normalized for convenience in comparison.)",
                "The statistics in table 2 indicates the usefulness of different term types in topic discrimination with respect to different news classes.",
                "We can see that, location name is the most useful term type for three news classes: Natural Disasters, Violence or War, Finances.",
                "And for three other categories Elections, Legal/Criminal Cases, Science and Discovery, person name is the most discriminative term type.",
                "For Scandals/Hearings, date is the most important information for topic discrimination.",
                "In addition, Legal/Criminal Cases and Finance topics have higher correlation with money terms, while Science and Discovery have higher correlation with percentage terms.",
                "Non-name terms are more stable for different classes. 1 . http://opennlp.sourceforge.net/ 2 . http://projects.ldc.upenn.edu/TDT3/Guide/label.html From the analysis of table 2, it is reasonable to adjust <br>term weight</br> according to their term type and the news class the story belongs to.",
                "New term weights are reweighted as follows: ( ) ( ) ( ) ( ) ( , , ) * ( , , ) ( , , ) * class d D type w T class d D type w w d weight d t w weight d t w weight d t w α α ∈ = ∑ (11) where type(w) represents the type of term w, and class(d) represents the class of story d, c kα is reweighting parameter for news class c and term type k. In the work, we just simply use statistics in table 2 as the reweighting parameters.",
                "Even thought using the statistics directly may not the best choice, we do not discuss how to automatically obtain the best parameters.",
                "We will try to use machine learning techniques to obtain the best parameters in the future work.",
                "In the work, we use BoosTexter [20] to classify all stories into one of the 11 ROIs.",
                "BoosTexter is a boosting based machine learning program, which creates a series of simple rules for building a classifier for text or attribute-value data.",
                "We use <br>term weight</br> generated using TF-IDF model as feature for story classification.",
                "We trained the model on the 12000 judged English stories in TDT2, and classify the rest of the stories in TDT2 and all stories in TDT3.",
                "Classification results are used for term reweighting in formula (11).",
                "Since the class labels of topic-off stories are not given in TDT datasets, we cannot give the classification accuracy here.",
                "Thus we do not discuss the effects of classification accuracy to NED performance in the paper. 6.",
                "EXPERIMENTAL SETUP 6.1 Datasets We used two LDC [18] datasets TDT2 and TDT3 for our experiments.",
                "TDT2 contains news stories from January to June 1998.",
                "It contains around 54,000 stories from sources like ABC, Associated Press, CNN, New York Times, Public Radio International, Voice of America etc.",
                "Only English stories in the collection were considered.",
                "TDT3 contains approximately 31,000 English stories collected from October to December 1998.",
                "In addition to the sources used in TDT2, it also contains stories from NBC and MSNBC TV broadcasts.",
                "We used transcribed versions of the TV and radio broadcasts besides textual news.",
                "TDT2 dataset is labeled with about 100 topics, and approximately 12,000 English stories belong to at least one of these topics.",
                "TDT3 dataset is labeled with about 120 topics, and approximately 8000 English stories belong to at least one of these topics.",
                "All the topics are classified into 11 Rules of Interpretation: (1)Elections, (2)Scandals/Hearings, (3)Legal/Criminal Cases, (4)Natural Disasters, (5)Accidents, (6)Ongoing Violence or War, (7)Science and Discovery News, (8)Finance, (9)New Law, (10)Sports News, (11)MISC.",
                "News. 6.2 Evaluation Metric TDT uses a cost function CDet that combines the probabilities of missing a new story and a false alarm [19]: * * * *Det Miss Miss Target FA FA NontargetC C P P C P P= + (12) Table 2.",
                "Average correlation between term types and news classes where CMiss means the cost of missing a new story, PMiss means the probability of missing a new story, and PTarget means the probability of seeing a new story in the data; CFA means the cost of a false alarm, PFA means the probability of a false alarm, and PNontarget means the probability of seeing an old story.",
                "The cost CDet is normalized such that a perfect system scores 0 and a trivial system, which is the better one of mark all stories as new or old, scores 1: ( ( * , * ) ) Det Det Miss Target FA Nontarget C Norm C min C P C P = (13) New event detection system gives two outputs for each story.",
                "The first part is yes or no indicating whether the story triggers a new event or not.",
                "The second part is a score indicating confidence of the first decision.",
                "Confidence scores can be used to plot DET curve, i.e., curves that plot false alarm vs. miss probabilities.",
                "Minimum normalized cost can be determined if optimal threshold on the score were chosen. 7.",
                "EXPERIMENTAL RESULTS 7.1 Main Results To test the approaches proposed in the model, we implemented and tested five systems: System-1: this system is used as baseline.",
                "It is implemented based on the basic model described in section 3, i.e., using incremental TF-IDF model to generate term weights, and using Hellinger distance to compute document similarity.",
                "Similarity score normalization is also employed [8].",
                "S-S detection procedure is used.",
                "System-2: this system is the same as system-1 except that S-C detection procedure is used.",
                "System-3: this system is the same as system-1 except that it uses the new detection procedure which is based on indexing-tree.",
                "System-4: implemented based on the approach presented in section 5.1, i.e., terms are reweighted according to the distance between term distributions in a cluster and all stories.",
                "The new detection procedure is used.",
                "System-5: implemented based on the approach presented in section 5.2, i.e., terms of different types are reweighted according to news class using trained parameters.",
                "The new detection procedure is used.",
                "The following are some other NED systems: System-6: [21] for each pair of stories, it computes three similarity values for named entity, non-named entity and all terms respectively.",
                "And employ Support Vector Machine to predict new or old using the similarity values as features.",
                "System-7: [8] it extended a basic incremental TF-IDF model to include source-specific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, etc.",
                "System-8: [13] it split document representation into two parts: named entities and non-named entities, and choose one effective part for each news class.",
                "Table 3 and table 4 show topic-weighted normalized costs and comparing times on TDT2 and TDT3 datasets respectively.",
                "Since no heldout data set for fine-tuning the threshold θ new was available for experiments on TDT2, we only report minimum normalized costs for our systems in table 3.",
                "System-5 outperforms all other systems including system-6, and it performs only 2.78e+8 comparing times in detection procedure which is only 13.4% of system-1.",
                "Table 3.",
                "NED results on TDT2 Systems Min Norm(CDet) Cmp times System-1 0.5749 2.08e+9 System-2① 0.6673 3.77e+8 System-3② 0.5765 2.81e+8 System-4② 0.5431 2.99e+8 System-5② 0.5089 2.78e+8 System-6 0.5300 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 When evaluating on the normalized costs on TDT3, we use the optimal thresholds obtained from TDT2 data set for all systems.",
                "System-2 reduces comparing times to 1.29e+9 which is just 18.3% of system-1, but at the same time it also gets a deteriorated minimum normalized cost which is 0.0499 higher than system-1.",
                "System-3 uses the new detection procedure based on news indexing-tree.",
                "It requires even less comparing times than system-2.",
                "This is because story-story comparisons usually yield greater similarities than story-cluster ones, so stories tend to be combined Location Person Date Organization Money Percentage NN JJ CD Elections 0.37 1 0.04 0.58 0.08 0.03 0.32 0.13 0.1 Scandals/Hearings 0.66 0.62 0.28 1 0.11 0.02 0.27 0.13 0.05 Legal/Criminal Cases 0.48 1 0.02 0.62 0.15 0 0.22 0.24 0.09 Natural Disasters 1 0.27 0 0.04 0.04 0 0.25 0.04 0.02 Violence or War 1 0.36 0.02 0.14 0.02 0.04 0.21 0.11 0.02 Science and Discovery 0.11 1 0.01 0.22 0.08 0.12 0.19 0.08 0.03 Finances 1 0.45 0.04 0.98 0.13 0.02 0.29 0.06 0.05 Sports 0.16 0.27 0.01 1 0.02 0 0.11 0.03 0.01 together in system-3.",
                "And system-3 is basically equivalent to system-1 in accuracy results.",
                "System-4 adjusts term weights based on the distance of term distributions between the whole corpus and cluster story set, yielding a good improvement by 0.0468 compared to system-1.",
                "The best system (system-5) has a minimum normalized cost 0.5012, which is 0.0797 better than system-1, and also better than any other results previously reported for this dataset [8, 13].",
                "Further more, system-5 only needs 1.05e+8 comparing times which is 14.9% of system-1.",
                "Table 4.",
                "NED results on TDT3 Systems Norm(CDet) Min Norm(CDet) Cmp times System-1 0.6159 0.5809 7.04e+8 System-2① 0.6493 0.6308 1.29e+8 System-3② 0.6197 0.5868 1.03e+8 System-4② 0.5601 0.5341 1.03e+8 System-5② 0.5413 0.5012 1.05e+8 System-7 -- 0.5783 -System-8 -- 0.5229 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 Figure5 shows the five DET curves for our systems on data set TDT3.",
                "System-5 achieves the minimum cost at a false alarm rate of 0.0157 and a miss rate of 0.4310.",
                "We can observe that System4 and System-5 obtain lower miss probability at regions of low false alarm probabilities.",
                "The hypothesis is that, more weight value is transferred to key terms of topics from non-key terms.",
                "Similarity score between two stories belonging to different topics are lower than before, because their overlapping terms are usually not key terms of their topics. 7.2 Parameter selection for indexing-tree detection Figure 3 shows the minimum normalized costs obtained by system-3 on TDT3 using different parameters.",
                "Theθ init parameter is tested on six values spanning from 0.03 to 0.18.",
                "And the λ parameter is tested on four values 1, 2, 3 and 4.",
                "We can see that, whenθ init is set to 0.12, which is the closest one toθ new, the costs are lower than others.",
                "This is easy to explain, because when stories belonging to the same topic are put in a cluster, it is more reasonable for the cluster to represent the stories in it.",
                "When parameter λ is set to 3 or 4, the costs are better than other cases, but there is no much difference between 3 and 4. 0 0.05 0.1 0.15 0.2 1 2 3 4 0.5 0.6 0.7 0.8 0.9 1 θ-initλ MinCost 0.6 0.65 0.7 0.75 0.8 0.85 0.9 Figure 3.",
                "Min Cost on TDT3 (δ =0.15) 0 0.05 0.1 0.15 0.2 1 2 3 4 0 0.5 1 1.5 2 2.5 x 10 8 θ-init λ Comparingtimes 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 8 Figure 4.",
                "Comparing times on TDT3 (δ =0.15) Figure 4 gives the comparing times used by system-3 on TDT3 with the same parameters as figure 3.",
                "The comparing times are strongly dependent onθ init.",
                "Because the greaterθ init is, the less stories combined together, the more comparing times are needed for new event decision.",
                "So we useθ init =0.13,λ =3,δ =0.15 for system-3, 4, and 5.",
                "In this parameter setting, we can get both low minimum normalized costs and less comparing times. 8.",
                "CONCLUSION We have proposed a news indexing-tree based detection procedure in our model.",
                "It reduces comparing times to about one seventh of traditional method without hurting NED accuracy.",
                "We also have presented two extensions to the basic TF-IDF model.",
                "The first extension is made by adjust term weights based on term distributions between the whole corpus and a cluster story set.",
                "And the second extension to basic TF-IDF model is better use of term types (named entities types and part-of-speed) according to news categories.",
                "Our experimental results on TDT2 and TDT3 datasets show that both of the two extensions contribute significantly to improvement in accuracy.",
                "We did not consider news time information as a clue for NED task, since most of the topics last for a long time and TDT data sets only span for a relative short period (no more than 6 months).",
                "For the future work, we want to collect news set which span for a longer period from internet, and integrate time information in NED task.",
                "Since topic is a relative coarse-grained news cluster, we also want to refine cluster granularity to event-level, and identify different events and their relations within a topic.",
                "Acknowledgments This work is supported by the National Natural Science Foundation of China under Grant No. 90604025.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsor. 9.",
                "REFERENCES [1] http://www.nist.gov/speech/tests/tdt/index.htm [2] In Topic Detection and Tracking.",
                "Event-based Information Organization.",
                "Kluwer Academic Publishers, 2002. .01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90 1 2 5 10 20 40 60 80 90 False Alarm Probability (in %) MissProbability(in%) SYSTEM1 Topic Weighted Curve SYSTEM1 Min Norm(Cost) SYSTEM2 Topic Weighted Curve SYSTEM2 Min Norm(Cost) SYSTEM3 Topic Weighted Curve SYSTEM3 Min Norm(Cost) SYSTEM4 Topic Weighted Curve SYSTEM4 Min Norm(Cost) SYSTEM5 Topic Weighted Curve SYSTEM5 Min Norm(Cost) Random Performance Figure 5.",
                "DET curves on TDT3 [3] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B.T.",
                "Archibald, and X. Liu.",
                "Learning Approaches for Detecting and Tracking News Events.",
                "In IEEE Intelligent Systems Special Issue on Applications of Intelligent Information Retrieval, volume 14 (4), 1999, 32-43. [4] Y. Yang, T. Pierce, and J. Carbonell.",
                "A Study on Retrospective and On-line Event Detection.",
                "In Proceedings of SIGIR-98, Melbourne, Australia, 1998, 28-36. [5] J. Allan, V. Lavrenko, D. Malin, and R. Swan.",
                "Detections, Bounds, and Timelines: Umass and tdt-3.",
                "In Proceedings of Topic Detection and Tracking Workshop (TDT-3), Vienna, VA, 2000, 167-174. [6] R. Papka and J. Allan.",
                "On-line New Event Detection Using Single Pass Clustering TITLE2:.",
                "Technical Report UM-CS1998-021, 1998. [7] W. Lam, H. Meng, K. Wong, and J.",
                "Yen.",
                "Using Contextual Analysis for News Event Detection.",
                "International Journal on Intelligent Systems, 2001, 525-546. [8] B. Thorsten, C. Francine, and F. Ayman.",
                "A System for New Event Detection.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2003, 330-337. [9] S. Nicola and C. Joe.",
                "Combining Semantic and Syntactic Document Classifiers to Improve First Story Detection.",
                "In Proceedings of the 24th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topicconditioned Novelty Detection.",
                "In Proceedings of the 8th ACM SIGKDD International Conference, ACM Press. 2002, 688-693. [11] M. Juha, A.M. Helena, and S. Marko.",
                "Applying Semantic Classes in Event Detection and Tracking.",
                "In Proceedings of International Conference on Natural Language Processing (ICON 2002), 2002, pages 175-183. [12] M. Juha, A.M. Helena, and S. Marko.",
                "Simple Semantics in Topic Detection and Tracking.",
                "Information Retrieval, 7(3-4): 2004, 347-368. [13] K. Giridhar and J. Allan.",
                "Text Classification and Named Entities for New Event Detection.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2004, 297-304. [14] J. P. Callan, W. B. Croft, and S. M. Harding.",
                "The INQUERY Retrieval System.",
                "In Proceedings of DEXA-92, 3rd International Conference on Database and Expert Systems Applications, 1992, 78-83. [15] R. Krovetz.",
                "Viewing Morphology as An Inference Process.",
                "In Proceedings of ACM SIGIR93, 1993, 61-81. [16] Y. Yang and J. Pedersen.",
                "A Comparative Study on Feature Selection in Text Categorization.",
                "In J. D. H. Fisher, editor, The Fourteenth International Conference on Machine Learning (ICML97), Morgan Kaufmann, 1997, 412-420. [17] T. M. Cover, and J.A.",
                "Thomas.",
                "Elements of Information Theory.",
                "Wiley. 1991. [18] The linguistic data consortium, http://www.ldc,upenn.edu/. [19] The 2001 TDT task definition and evaluation plan, http://www.nist.gov/speech/tests/tdt/tdt2001/evalplan.htm. [20] R. E. Schapire and Y.",
                "Singer.",
                "Boostexter: A Boosting-based System for Text Categorization.",
                "In Machine Learning 39(2/3):1, Kluwer Academic Publishers, 2000, 35-168. [21] K. Giridhar and J. Allan. 2005.",
                "Using Names and Topics for New Event Detection.",
                "In Proceedings of Human Technology Conference and Conference on Empirical Methods in Natural Language, Vancouver, 2005, 121-128"
            ],
            "original_annotated_samples": [
                "We use incremental TF-IDF model for <br>term weight</br> calculation [4].",
                "Non-name terms are more stable for different classes. 1 . http://opennlp.sourceforge.net/ 2 . http://projects.ldc.upenn.edu/TDT3/Guide/label.html From the analysis of table 2, it is reasonable to adjust <br>term weight</br> according to their term type and the news class the story belongs to.",
                "We use <br>term weight</br> generated using TF-IDF model as feature for story classification."
            ],
            "translated_annotated_samples": [
                "Utilizamos el modelo TF-IDF incremental para el cálculo del <br>peso de los términos</br> [4].",
                "Los términos no nominales son más estables para diferentes clases. 1. http://opennlp.sourceforge.net/ 2. http://projects.ldc.upenn.edu/TDT3/Guide/label.html A partir del análisis de la tabla 2, es razonable ajustar el <br>peso de los términos</br> según su tipo de término y la clase de noticias a la que pertenece la historia.",
                "Utilizamos el <br>peso del término</br> generado utilizando el modelo TF-IDF como característica para la clasificación de historias."
            ],
            "translated_text": "Detección de nuevos eventos basada en árbol de indexación y entidad nombrada Zhang Kuo Universidad Tsinghua Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Universidad Tsinghua Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Universidad Tsinghua Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn RESUMEN La Detección de Nuevos Eventos (NED) tiene como objetivo detectar de una o varias corrientes de noticias cuál se informa sobre un nuevo evento (es decir, no reportado previamente). Con el abrumador volumen de noticias disponible hoy en día, hay una creciente necesidad de un sistema NED que sea capaz de detectar eventos nuevos de manera más eficiente y precisa. En este artículo proponemos un nuevo modelo de NED para acelerar la tarea de NED mediante el uso dinámico de un árbol de indexación de noticias. Además, basándose en la observación de que los términos de diferentes tipos tienen diferentes efectos para la tarea de NED, se proponen dos enfoques de reponderación de términos para mejorar la precisión de NED. En el primer enfoque, proponemos ajustar dinámicamente los pesos de los términos basados en los clusters de historias previas y en el segundo enfoque, proponemos emplear estadísticas en los datos de entrenamiento para aprender el modelo de reajuste de entidades nombradas para cada clase de historias. Los resultados experimentales en dos conjuntos de datos del Consorcio de Datos Lingüísticos (LDC), TDT2 y TDT3, muestran que el modelo propuesto puede mejorar significativamente tanto la eficiencia como la precisión de la tarea de NED, en comparación con el sistema base y otros sistemas existentes. Categorías y Descriptores de Asignaturas H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información; H.4.2 [Aplicaciones de Sistemas de Información]: Tipos de Sistemas de Apoyo a Decisiones. Términos generales Algoritmos, Rendimiento, Experimentación 1. El programa de Detección y Seguimiento de Temas (TDT) tiene como objetivo desarrollar técnicas que puedan organizar, buscar y estructurar de manera efectiva materiales de texto de noticias de una variedad de agencias de noticias y medios de difusión [1]. La Detección de Eventos Nuevos (DEN) es una de las cinco tareas en TDT. Es la tarea de identificación en línea del informe más temprano para cada tema tan pronto como ese informe llegue en la secuencia de documentos. Un tema se define como un evento o actividad seminal, junto con eventos y actividades directamente relacionados [2]. Un evento se define como algo (no trivial) que sucede en un lugar y en un momento determinados [3]. Por ejemplo, cuando una bomba explota en un edificio, la explosión es el evento seminal que desencadena el tema, y otras historias sobre el mismo tema serían aquellas que discuten los esfuerzos de salvamento, la búsqueda de los perpetradores, arrestos y juicios, entre otros. La información útil de noticias suele estar enterrada en una masa de datos generados a diario. Por lo tanto, los sistemas NED son muy útiles para las personas que necesitan detectar información novedosa de un flujo de noticias en tiempo real. Estas necesidades de la vida real a menudo se presentan en ámbitos como los mercados financieros, el análisis de noticias y la recopilación de inteligencia. En la mayoría de los sistemas NED de última generación, cada noticia disponible se compara con todas las noticias previamente recibidas. Si todas las similitudes entre ellos no superan un umbral, entonces la historia desencadena un nuevo evento. Por lo general, se presentan en forma de similitud de coseno o métrica de similitud de Hellinger. El problema central de NED es identificar si dos historias tratan sobre el mismo tema. Obviamente, estos sistemas no pueden aprovechar la información del tema. Además, no es aceptable en aplicaciones reales debido a la gran cantidad de cálculos requeridos en el proceso de NED. Otros sistemas organizan historias anteriores en grupos (cada grupo corresponde a un tema), y la nueva historia se compara con los grupos anteriores en lugar de con historias. Esta forma puede reducir significativamente los tiempos de comparación. Sin embargo, se ha demostrado que este método es menos preciso [4, 5]. Esto se debe a que a veces las historias dentro de un tema se alejan mucho entre sí, lo que podría resultar en una baja similitud entre una historia y su tema. Por otro lado, algunos sistemas NED propuestos intentaron mejorar la precisión al hacer un mejor uso de las entidades nombradas [10, 11, 12, 13]. Sin embargo, ninguno de los sistemas ha considerado que los términos de diferentes tipos (por ejemplo, Los sustantivos, verbos o nombres de personas tienen diferentes efectos en diferentes clases de historias para determinar si dos historias tratan sobre el mismo tema. Por ejemplo, los nombres de los candidatos electorales (Nombre de la persona) son muy importantes para las historias de la clase de elecciones; las ubicaciones (Nombre de la ubicación) donde ocurrieron los accidentes son importantes para las historias de la clase de accidentes. Por lo tanto, en NED, todavía existen los siguientes tres problemas por investigar: (1) ¿Cómo acelerar el procedimiento de detección sin disminuir la precisión de la detección? (2) ¿Cómo aprovechar mejor la información de los grupos (temas) para mejorar la precisión? (3) ¿Cómo obtener una mejor representación de historias de noticias mediante una mejor comprensión de las entidades nombradas? Impulsados por estos problemas, hemos propuesto tres enfoques en este documento. (1) Para hacer que el procedimiento de detección sea más rápido, proponemos un nuevo procedimiento de NED basado en un árbol de indexación de noticias creado dinámicamente. El índice de historias en árbol se crea ensamblando historias similares para formar grupos de noticias en diferentes jerarquías según sus valores de similitud. Las comparaciones entre la historia actual y los grupos anteriores podrían ayudar a encontrar la historia más similar en menos tiempo de comparación. El nuevo procedimiento puede reducir la cantidad de veces de comparación sin afectar la precisión. Utilizamos los grupos del primer piso en el árbol de indexación como temas de noticias, en los cuales los pesos de los términos se ajustan dinámicamente según la distribución de términos en los grupos. En este enfoque, se utiliza adecuadamente la información de los grupos (temas), por lo que se evita el problema de la descentralización del tema. Basándonos en observaciones de las estadísticas obtenidas de los datos de entrenamiento, encontramos que los términos de diferentes tipos (por ejemplo, Los sustantivos y verbos tienen diferentes efectos para diferentes clases de historias al determinar si dos historias tratan sobre el mismo tema. Y proponemos utilizar estadísticas para optimizar los pesos de los términos de diferentes tipos en una historia de acuerdo a la clase de noticias a la que pertenece la historia. En el conjunto de datos TDT3, el nuevo modelo NED solo utiliza un 14.9% en comparación con los tiempos del modelo básico, mientras que su costo normalizado mínimo es de 0.5012, lo que es 0.0797 mejor que el modelo básico, y también mejor que cualquier otro resultado previamente reportado para este conjunto de datos [8, 13]. El resto del documento está organizado de la siguiente manera. Comenzamos este artículo resumiendo el trabajo previo en NED en la sección 2. La sección 3 presenta el modelo básico para NED que la mayoría de los sistemas actuales utilizan. La sección 4 describe nuestro nuevo procedimiento de detección basado en el índice de árbol de noticias. En la sección 5, se proponen dos métodos de reponderación de términos para mejorar la precisión de NED. La sección 6 presenta nuestros datos experimentales y métricas de evaluación. Finalmente concluimos con los resultados experimentales en la Sección 7, y las conclusiones y trabajo futuro en la Sección 8. TRABAJO RELACIONADO Papka et al. propusieron el agrupamiento de un solo paso en NED [6]. Cuando se encontraba una nueva historia, esta era procesada inmediatamente para extraer características de términos y se construía una representación de consulta del contenido de la historia. Luego se comparó con todas las consultas anteriores. Si el documento no generaba ninguna consulta al exceder un umbral, se marcaba como un nuevo evento. Lam et al construyen representaciones de consultas previas de grupos de historias, cada una de las cuales corresponde a un tema [7]. De esta manera se realizan comparaciones entre historias y grupos. En los últimos años, la mayoría de los trabajos se han centrado en proponer mejores métodos para la comparación de historias y la representación de documentos. Brants et al. [8] extendieron un modelo básico incremental de TF-IDF para incluir modelos específicos de fuente, normalización de puntajes de similitud basada en promedios específicos de documentos, normalización de puntajes de similitud basada en promedios específicos de pares de fuentes, reponderación de términos basada en frecuencias de eventos inversas y segmentación de documentos. Se mostraron buenas mejoras en los puntos de referencia de TDT. Stokes et al. [9] utilizaron una combinación de evidencia de dos representaciones distintas del contenido de un documento. Una de las representaciones era el vector de texto libre habitual, la otra hacía uso de cadenas léxicas (creadas utilizando WordNet) para construir otro vector de términos. Luego, las dos representaciones se combinan de forma lineal. Se logró un aumento marginal en la efectividad cuando se utilizó la representación combinada. Algunos esfuerzos se han realizado sobre cómo utilizar entidades nombradas para mejorar la desambiguación de entidades nombradas (NED). Yang et al. asignaron un peso cuatro veces mayor a las entidades con nombres de ubicación que a otros términos y entidades con nombres [10]. El grupo de investigación DOREMI combinó similitudes semánticas de nombres de personas, nombres de lugares y tiempo junto con similitud textual [11][12]. El grupo de investigación de UMass [13] dividió la representación del documento en dos partes: entidades nombradas y entidades no nombradas. Y se descubrió que algunas clases de noticias podían lograr un mejor rendimiento utilizando la representación de entidades nombradas, mientras que otras clases de noticias podían lograr un mejor rendimiento utilizando la representación sin entidades nombradas. Tanto [10] como [13] utilizaron la técnica de categorización de texto para clasificar previamente las noticias. En [13] las noticias se clasifican automáticamente al principio, y luego se prueban las sensibilidades de los nombres y términos que no son nombres para NED para cada clase. En [10], se eliminan los términos frecuentes de cada clase de la representación del documento. Por ejemplo, la palabra \"elección\" no ayuda a identificar diferentes elecciones. En su trabajo, no se investiga la efectividad de diferentes tipos de nombres (o términos con diferentes categorías gramaticales) para NED en diferentes clases de noticias. Utilizamos análisis estadístico para revelar el hecho y utilizarlo para mejorar el rendimiento de NED. 3. MODELO BÁSICO En esta sección, presentamos el modelo básico de Detección de Nuevos Eventos que es similar a lo que la mayoría de los sistemas actuales aplican. Luego, proponemos nuestro nuevo modelo ampliando el modelo básico. Los sistemas de detección de eventos nuevos utilizan flujos de noticias como entrada, en los cuales las historias están estrictamente ordenadas por tiempo. Solo están disponibles las historias recibidas previamente al tratar con la historia actual. La salida es una decisión sobre si la historia actual es un evento nuevo o no y la confianza de la decisión. Por lo general, un modelo NED consta de tres partes: representación de la historia, cálculo de similitud y procedimiento de detección. 3.1 Representación de la historia Se necesita preprocesamiento antes de generar la representación de la historia. Para el preprocesamiento, tokenizamos palabras, reconocemos abreviaturas, normalizamos abreviaturas, agregamos etiquetas de partes del discurso, eliminamos palabras vacías incluidas en la lista de paradas utilizada en InQuery [14], reemplazamos palabras con sus raíces utilizando el algoritmo K-stem[15], y luego generamos un vector de palabras para cada noticia. Utilizamos el modelo TF-IDF incremental para el cálculo del <br>peso de los términos</br> [4]. En un modelo TF-IDF, la frecuencia de término en un documento de noticias se pondera por la frecuencia inversa del documento, la cual se genera a partir del corpus de entrenamiento. Cuando surge un nuevo término en el proceso de prueba, hay dos soluciones: simplemente ignorar el nuevo término o establecer df del término como una constante pequeña (por ejemplo, df = 1). El nuevo término recibe un peso demasiado bajo en la primera solución (0) y un peso demasiado alto en la segunda solución. En el modelo TF-IDF incremental, las frecuencias de documentos se actualizan dinámicamente en cada paso de tiempo t: 1( ) ( ) ( )t t D tdf w df w df w−= + (1) donde Dt representa el conjunto de noticias recibidas en el tiempo t, y dfDt(w) significa el número de documentos en los que aparece el término w, y dft(w) significa el número total de documentos en los que aparece el término w antes del tiempo t. En este trabajo, cada ventana de tiempo incluye 50 noticias. Así, cada historia d recibida en t se representa de la siguiente manera: 1 2{ ( , , ), ( , , ),..., ( , , )}nd peso d t w peso d t w peso d t w→ donde n significa el número de términos distintos en la historia d, y ( , , )peso d t w significa el peso del término w en la historia d en el tiempo t: log( ( , ) 1) log(( 1) /( ( ) 0.5)) ( , , ) log( ( , ) 1) log(( 1) /( ( ) 0.5)) t t t t w d tf d w N df w peso d t w tf d w N df w ∈ + + + = + + +∑ (2) donde Nt significa el número total de historias de noticias antes del tiempo t, y tf(d,w) significa cuántas veces aparece el término w en la historia de noticias d. 3.2 Cálculo de Similitud Utilizamos la distancia de Hellinger para el cálculo de la similitud entre dos historias, para dos historias d y d en el tiempo t, su similitud se define de la siguiente manera: , ( , , ) ( , , ) * ( , , ) w d d sim d d t peso d t w peso d t w ∈ = ∑ (3) 3.3 Procedimiento de Detección Para cada historia d recibida en el paso de tiempo t, el valor ( ) ( ) ( ) ( ( , , )) tiempo d tiempo d n d max sim d d t < = (4) es un puntaje utilizado para determinar si d es una historia sobre un nuevo tema y al mismo tiempo es una indicación de la confianza en nuestra decisión [8]. tiempo(d) significa el tiempo de publicación de la historia d. Si el puntaje supera el umbral θ new, entonces existe un documento suficientemente similar, por lo tanto, d es una historia antigua, de lo contrario, no hay un documento previo suficientemente similar, por lo tanto, d es una historia nueva. 4. Nuevo procedimiento NED Los sistemas NED tradicionales se pueden clasificar en dos tipos principales en cuanto al procedimiento de detección: (1) tipo S-S, en el que la historia en cuestión se compara con cada historia recibida previamente, y se utiliza la mayor similitud para determinar si la historia actual trata sobre un evento nuevo; (2) tipo S-C, en el que la historia en cuestión se compara con todos los clusters anteriores, cada uno de los cuales representa un tema, y se utiliza la mayor similitud para la decisión final sobre la historia actual. Si la similitud más alta supera el umbral θ nuevo, entonces es una historia antigua y se coloca en el clúster más similar; de lo contrario, es una historia nueva y se crea un nuevo clúster. Trabajos anteriores muestran que la primera forma es más precisa que la segunda [4][5]. Dado que a veces las historias dentro de un tema se alejan mucho entre sí, una historia puede tener una similitud muy baja con su tema. Por lo tanto, utilizar similitudes entre historias para determinar una nueva historia es mejor que utilizar similitudes entre historias y grupos. Sin embargo, el primer método requiere mucho más tiempo de comparación, lo que significa que es menos eficiente. Proponemos un nuevo procedimiento de detección que utiliza comparaciones con clusters anteriores para ayudar a encontrar la historia más similar en menos tiempo de comparación, y la decisión final sobre el nuevo evento se toma de acuerdo con la historia más similar. Por lo tanto, podemos obtener tanto la precisión de los métodos de tipo S-S como la eficiencia de los métodos de tipo S-C. El nuevo procedimiento crea un árbol de indexación de noticias dinámicamente, en el cual se agrupan historias similares para formar una jerarquía de clusters. Indexamos historias similares juntas por su ancestro común (un nodo de clúster). Historias diferentes están indexadas en diferentes grupos. Cuando una historia está llegando, utilizamos comparaciones entre la historia actual y los grupos jerárquicos anteriores para ayudar a encontrar la historia más similar que es útil para la toma de decisiones sobre nuevos eventos. Después de que se tome la decisión sobre el nuevo evento, la historia actual se inserta en el árbol de indexación para la detección siguiente. El índice de noticias en forma de árbol se define formalmente de la siguiente manera: S-Tree = {r, NC, NS, E}, donde r es la raíz de S-Tree, NC es el conjunto de todos los nodos de clúster, NS es el conjunto de todos los nodos de historia, y E es el conjunto de todos los bordes en S-Tree. Definimos un conjunto de restricciones para un S-Árbol: ⅰ. , es un nodo no terminal en el árbolC i i N i∀ ∈ → ⅱ. , es un nodo terminal en el árbolS i i N i∀ ∈ → ⅲ. , el grado de salida de es al menos 2C i i N i∀ ∈ → ⅳ. , se representa como el centroide de sus descendientesC i i iN∀ ∈ → Para una noticia di, el procedimiento de comparación y el procedimiento de inserción basados en el árbol de indexación se definen de la siguiente manera. Un ejemplo se muestra en la Figura 1 y la Figura 2. Figura 1. Procedimiento de comparación Figura 2. Procedimiento de comparación de inserción: Paso 1: comparar di con todos los nodos hijos directos de r y seleccionar λ nodos con las similitudes más altas, por ejemplo, C1 2 y C1 3 en la Figura 1. Paso 2: para cada nodo seleccionado en el paso anterior, por ejemplo. C1 2, compara di con todos sus nodos hijos directos y selecciona λ nodos con las similitudes más altas, por ejemplo. C2 2 y d8. Repetir el paso 2 para todos los nodos no terminales. Paso 3: registrar el nodo terminal con la mayor similitud a di, por ejemplo, s5, y el valor de similitud (0.20). Insertando di en el S-árbol con r como raíz: Encuentra el nodo n que es hijo directo de r en el camino desde r hasta el nodo terminal con la similitud más alta s, por ejemplo. C1 2. Si s es menor que θ init + (h-1)δ, entonces agregar di al árbol como un hijo directo de r. De lo contrario, si n es un nodo terminal, crear un nodo de clúster en lugar de n, y agregar tanto n como di como sus hijos directos; si n es un nodo no terminal, repetir este procedimiento e insertar di en el subárbol con n como raíz de forma recursiva. Aquí h es la longitud entre n y la raíz del árbol S. Cuanto más similares sean las historias en un grupo, mejor representará el grupo a las historias en él. Por lo tanto, no imponemos restricciones en la altura máxima de los árboles y el grado de un nodo. Por lo tanto, no podemos dar la complejidad de este procedimiento basado en árboles de indexación. Pero proporcionaremos el número de veces de comparación necesarias por el nuevo procedimiento en nuestros experimentos en la sección 7.5. En esta sección, se proponen dos métodos de reponderación de términos para mejorar la precisión de NED. En el primer método, se explora una nueva forma de utilizar de manera más efectiva la información de los grupos (temas). El segundo encuentra una mejor manera de utilizar entidades nombradas basadas en la clasificación de noticias. 5.1 Reajuste de términos basado en la distancia de distribución TF-IDF es el modelo más prevalente utilizado en sistemas de recuperación de información. La idea básica es que cuanto menos documentos aparezca un término, más importante es el término en la discriminación de documentos (relevantes o no relevantes para una consulta que contenga el término). Sin embargo, en el dominio de TDT, necesitamos discriminar documentos en función de los temas en lugar de las consultas. Intuitivamente, el uso de vectores de clúster (tema) para comparar con historias de noticias posteriores debería tener un mejor rendimiento que el uso de vectores de historias. Desafortunadamente, los resultados experimentales no respaldan esta intuición [4][5]. Basándonos en la observación de los datos, encontramos que la razón es que un tema de noticias generalmente contiene muchos eventos directa o indirectamente relacionados, y todos ellos tienen sus propios subtemas que suelen ser diferentes entre sí. Tomemos el tema descrito en la sección 1 como ejemplo, eventos como la explosión y el rescate tienen similitudes muy bajas con eventos sobre juicios criminales, por lo tanto, las historias sobre juicios tendrían una baja similitud con el vector de tema construido en base a sus eventos previos. Esta sección se centra en cómo hacer un uso efectivo de la información del tema y al mismo tiempo evitar el problema de la descentralización del contenido. Al principio, clasificamos los términos en 5 clases para ayudar en el análisis de las necesidades del modelo modificado: Clase de términos A: términos que ocurren con frecuencia en todo el corpus, por ejemplo, año y personas. Los términos de esta clase deben recibir pesos bajos porque no ayudan mucho en la discriminación de temas. Clase B de términos: términos que ocurren con frecuencia dentro de una categoría de noticias, por ejemplo, elección, tormenta. Son útiles para distinguir dos historias en diferentes categorías de noticias. Sin embargo, no pueden proporcionar información para determinar si dos historias tratan sobre el mismo tema o temas diferentes. En otras palabras, los términos elección y tormenta no son útiles para diferenciar dos campañas electorales y dos desastres de tormenta. Por lo tanto, los términos de esta clase deberían asignarse pesos más bajos. Clase de término C: términos que ocurren con frecuencia en un tema y raramente en otros temas, por ejemplo, el nombre de un avión accidentado, el nombre de un huracán específico. Las noticias que pertenecen a diferentes temas rara vez tienen términos superpuestos en esta clase. Cuanto más frecuentemente aparezca un término en un tema, más importante es el término para una historia perteneciente al tema, por lo tanto, el término debería tener un peso más alto. Clase de término D: términos que aparecen en un tema de manera exclusiva, pero no con frecuencia. Por ejemplo, el nombre de un bombero que se desempeñó muy bien en una acción de salvamento, que puede aparecer en solo dos o tres historias pero nunca ha aparecido en otros temas. Los términos de este tipo deberían recibir más peso que en el modelo TF-IDF. Sin embargo, dado que no son populares en el tema, no es apropiado asignarles pesos demasiado altos. Clase de término E: términos con baja frecuencia en documentos y que aparecen en diferentes temas. Los términos de esta clase deberían recibir pesos más bajos. Ahora analizamos si el modelo TF-IDF puede asignar pesos adecuados a las cinco clases de términos. Obviamente, los términos de la clase A tienen un peso bajo en el modelo TF-IDF, lo cual es acorde con el requisito descrito anteriormente. En el modelo TF-IDF, los términos de la clase B dependen altamente del número de historias en una clase de noticias. El modelo TF-IDF no puede proporcionar pesos bajos si la historia que contiene el término pertenece a una clase de noticias relativamente pequeña. Para un término de clase C, cuanto más frecuentemente aparezca en un tema, menos peso le da el modelo TFIDF. Esto entra en conflicto directo con el requisito de términos en la clase C. Para los términos de la clase D, el modelo TF-IDF les asigna pesos altos de manera correcta. Pero para los términos de la clase E, el modelo TF-IDF asigna pesos altos a ellos que no son conformes con el requisito de pesos bajos. En resumen, los términos de clase B, C, E no pueden ser ponderados adecuadamente en el modelo TF-IDF. Por lo tanto, proponemos un modelo modificado para resolver este problema. Cuando θ init y θ new están configurados de cerca, asumimos que la mayoría de las historias en un clúster de primer nivel (un nodo hijo directo del nodo raíz) están en el mismo tema. Por lo tanto, utilizamos un clúster de primer nivel para capturar la distribución de términos (df para todos los términos dentro del clúster) dentro del tema de forma dinámica. La divergencia de Kullback-Leibler de la distribución de términos en un clúster de primer nivel y el conjunto completo de historias se utiliza para ajustar los pesos de los términos: ( , , ) * (1 * ( || )) ( , , ) ( , , ) * (1 * ( || )) cw tw cw tw w d D peso d t w KL P P peso d t w peso d t w KL P P γ γ ∈ + = +∑ (5) donde ( ) ( ) ( ) ( ) 1,cw cw c c c c df w df w p y p y N N = = − (6) ( ) ( ) ( ) ( ) 1,t t tw tw t t df w df w p y p y N N = = − (7) donde dfc(w) es el número de documentos que contienen el término w dentro del clúster C, y Nc es el número de documentos en el clúster C, y Nt es el número total de documentos que llegan antes del paso de tiempo t. γ es un parámetro constante, actualmente configurado manualmente en 3. La divergencia de Kullback-Leibler se define de la siguiente manera [17]: La idea básica es que, para una historia en un tema, cuanto más ocurre un término dentro del tema y menos ocurre en otros temas, se le deben asignar pesos más altos. Obviamente, el modelo modificado puede cumplir con todos los requisitos de las cinco clases de términos mencionadas anteriormente. 5.2 Reajuste de Términos Basado en el Tipo de Término y la Clase de Historia. Trabajos anteriores encontraron que algunas clases de historias de noticias podrían lograr mejoras significativas al dar un peso adicional a las entidades nombradas. Pero encontramos que los términos de diferentes tipos deben recibir una cantidad diferente de peso adicional para diferentes clases de noticias. Utilizamos open-NLP1 para reconocer tipos de entidades nombradas y etiquetas de partes del discurso para términos que aparecen en noticias. Los tipos de entidades nombradas incluyen nombre de persona, nombre de organización, nombre de ubicación, fecha, hora, dinero y porcentaje, y se seleccionan cinco partes del discurso: ninguno (NN), verbo (VB), adjetivo (JJ), adverbio (RB) y número cardinal (CD). El análisis estadístico muestra tipos de términos discriminativos a nivel de tema para diferentes clases de historias. Por conveniencia, los tipos de entidad nombrada y etiquetas de parte de la oración se llaman uniformemente tipo de término en las secciones siguientes. Determinar si dos historias tratan sobre el mismo tema es un componente básico para la tarea de NED. Por lo tanto, al principio usamos la estadística χ² para calcular las correlaciones entre términos y temas. Para un término t y un tema T, se deriva una tabla de contingencia: Tabla 1. Una tabla de contingencia de 2×2 con el número de documentos pertenecientes al tema T y no pertenecientes al tema T incluye t A B no incluye t C D. La estadística χ² para un término específico t con respecto al tema T se define como [16]: χ² = ( ) ( ) * ( ) ( ) * ( ) * ( ) * ( ) w T A B C D AD CB A C B D A B C D + + + − + + + + (9). Los temas de noticias para la tarea TDT se clasifican además en 11 reglas de interpretaciones (ROIs) 2. El ROI se puede ver como una clase de historias de nivel superior. El promedio de correlación entre un tipo de término y un ROI de tema se calcula como: 2 avg 2 ( , )( ( , ) )k m m km kT R w P w TP R p w T R P χ χ ∈ ∈ ∑ ∑（ , ）= 1 1 k=1…K, m=1…M (10) donde K es el número de tipos de términos (fijado constantemente en 12 en el artículo). M es el número de nuevas clases (ROIs, establecido en el conjunto 11 del artículo). Pk representa el conjunto de todos los términos de tipo k, y Rm representa el conjunto de todos los temas de la clase m, p(t,T) significa la probabilidad de que t ocurra en el tema T. Debido a limitaciones de espacio, solo se enumeran partes de los tipos de términos (9 tipos de términos) y partes de las clases de noticias (8 clases) en la tabla 2 con los valores promedio de correlación entre ellos. Las estadísticas se derivan de datos etiquetados en el corpus TDT2. (Los resultados en la tabla 2 ya están normalizados para mayor comodidad en la comparación). Las estadísticas en la tabla 2 indican la utilidad de diferentes tipos de términos en la discriminación de temas con respecto a diferentes clases de noticias. Podemos ver que el nombre de la ubicación es el tipo de término más útil para tres clases de noticias: Desastres Naturales, Violencia o Guerra, Finanzas. Y para otras tres categorías: Elecciones, Casos Legales/Criminales, Ciencia y Descubrimiento, el nombre de la persona es el tipo de término más discriminativo. Para Escándalos/Audiencias, la fecha es la información más importante para la discriminación de temas. Además, los casos legales/criminales y los temas financieros tienen una correlación más alta con términos relacionados con el dinero, mientras que la ciencia y el descubrimiento tienen una correlación más alta con términos de porcentaje. Los términos no nominales son más estables para diferentes clases. 1. http://opennlp.sourceforge.net/ 2. http://projects.ldc.upenn.edu/TDT3/Guide/label.html A partir del análisis de la tabla 2, es razonable ajustar el <br>peso de los términos</br> según su tipo de término y la clase de noticias a la que pertenece la historia. Los nuevos pesos de los términos se recalculan de la siguiente manera: ( ) ( ) ( ) ( ) ( , , ) * ( , , ) ( , , ) * clase d D tipo w T clase d D tipo w w d peso d t w peso d t w peso d t w α α ∈ = ∑ (11) donde type(w) representa el tipo del término w, y class(d) representa la clase de la historia d, c kα es el parámetro de recalibración para la clase de noticias c y el tipo de término k. En el trabajo, simplemente utilizamos las estadísticas de la tabla 2 como parámetros de recalibración. Aunque usar las estadísticas directamente puede que no sea la mejor opción, no discutimos cómo obtener automáticamente los mejores parámetros. Intentaremos utilizar técnicas de aprendizaje automático para obtener los mejores parámetros en el trabajo futuro. En el trabajo, utilizamos BoosTexter [20] para clasificar todas las historias en uno de los 11 ROIs. BoosTexter es un programa de aprendizaje automático basado en boosting, que crea una serie de reglas simples para construir un clasificador de datos de texto o atributos-valor. Utilizamos el <br>peso del término</br> generado utilizando el modelo TF-IDF como característica para la clasificación de historias. Entrenamos el modelo con las 12000 historias en inglés evaluadas en TDT2, y clasificamos el resto de las historias en TDT2 y todas las historias en TDT3. Los resultados de clasificación se utilizan para la reponderación de términos en la fórmula (11). Dado que las etiquetas de clase de las historias de tema apagado no se proporcionan en los conjuntos de datos de TDT, no podemos proporcionar la precisión de clasificación aquí. Por lo tanto, no discutimos los efectos de la precisión de la clasificación en el rendimiento de NED en el artículo. 6. CONFIGURACIÓN EXPERIMENTAL 6.1 Conjuntos de datos Utilizamos dos conjuntos de datos de LDC [18], TDT2 y TDT3, para nuestros experimentos. TDT2 contiene noticias desde enero hasta junio de 1998. Contiene alrededor de 54,000 historias de fuentes como ABC, Associated Press, CNN, New York Times, Public Radio International, Voice of America, etc. Solo se consideraron historias en inglés en la colección. TDT3 contiene aproximadamente 31,000 historias en inglés recopiladas de octubre a diciembre de 1998. Además de las fuentes utilizadas en TDT2, también contiene historias de las transmisiones de televisión de NBC y MSNBC. Utilizamos versiones transcritas de las emisiones de televisión y radio además de noticias textuales. El conjunto de datos TDT2 está etiquetado con alrededor de 100 temas, y aproximadamente 12,000 historias en inglés pertenecen al menos a uno de estos temas. El conjunto de datos TDT3 está etiquetado con alrededor de 120 temas, y aproximadamente 8000 historias en inglés pertenecen al menos a uno de estos temas. Todos los temas están clasificados en 11 Reglas de Interpretación: (1) Elecciones, (2) Escándalos/Audiencias, (3) Casos Legales/Criminales, (4) Desastres Naturales, (5) Accidentes, (6) Violencia o Guerra en Curso, (7) Noticias de Ciencia y Descubrimientos, (8) Finanzas, (9) Nueva Ley, (10) Noticias Deportivas, (11) Varios. Noticias. 6.2 Métrica de evaluación TDT utiliza una función de costo CDet que combina las probabilidades de perder una nueva historia y una falsa alarma [19]: * * * *Det Miss Miss Target FA FA NontargetC C P P C P P= + (12) Tabla 2. Correlación promedio entre tipos de términos y clases de noticias donde CMiss significa el costo de perder una nueva historia, PMiss significa la probabilidad de perder una nueva historia, y PTarget significa la probabilidad de ver una nueva historia en los datos; CFA significa el costo de una falsa alarma, PFA significa la probabilidad de una falsa alarma, y PNontarget significa la probabilidad de ver una historia antigua. El costo CDet se normaliza de tal manera que un sistema perfecto obtiene una puntuación de 0 y un sistema trivial, que es el mejor de marcar todas las historias como nuevas o antiguas, obtiene una puntuación de 1: ( ( * , * ) ) Det Det Miss Target FA Nontarget C Norm C min C P C P = (13) El sistema de detección de eventos nuevos proporciona dos salidas para cada historia. La primera parte es sí o no, indicando si la historia desencadena un nuevo evento o no. La segunda parte es una puntuación que indica la confianza de la primera decisión. Los puntajes de confianza se pueden utilizar para trazar la curva DET, es decir, curvas que representan la probabilidad de falsa alarma frente a la probabilidad de omisión. El costo normalizado mínimo se puede determinar si se elige el umbral óptimo en la puntuación. RESULTADOS EXPERIMENTALES 7.1 Resultados principales Para probar los enfoques propuestos en el modelo, implementamos y probamos cinco sistemas: Sistema-1: este sistema se utiliza como referencia. Se implementa basándose en el modelo básico descrito en la sección 3, es decir, utilizando el modelo TF-IDF incremental para generar pesos de términos y utilizando la distancia de Hellinger para calcular la similitud entre documentos. La normalización del puntaje de similitud también se emplea [8]. Se utiliza el procedimiento de detección S-S. Sistema-2: este sistema es igual que el sistema-1 excepto que se utiliza el procedimiento de detección S-C. Sistema-3: este sistema es igual que el sistema-1 excepto que utiliza el nuevo procedimiento de detección que se basa en un árbol de indexación. Sistema-4: implementado basado en el enfoque presentado en la sección 5.1, es decir, los términos se vuelven a ponderar según la distancia entre las distribuciones de términos en un grupo y todas las historias. Se utiliza el nuevo procedimiento de detección. Sistema-5: implementado basado en el enfoque presentado en la sección 5.2, es decir, los términos de diferentes tipos son reponderados según la clase de noticias utilizando parámetros entrenados. El nuevo procedimiento de detección se utiliza. Los siguientes son algunos otros sistemas NED: Sistema-6: [21] para cada par de historias, calcula tres valores de similitud para entidad nombrada, entidad no nombrada y todos los términos respectivamente. Y emplear Máquina de Vectores de Soporte para predecir nuevo o antiguo utilizando los valores de similitud como características. Sistema-7: [8] extendió un modelo básico incremental de TF-IDF para incluir modelos específicos de origen, normalización de puntuaciones de similitud basada en promedios específicos de documentos, normalización de puntuaciones de similitud basada en promedios específicos de pares de fuentes, etc. Sistema-8: [13] dividió la representación del documento en dos partes: entidades nombradas y entidades no nombradas, y eligió una parte efectiva para cada clase de noticias. La tabla 3 y la tabla 4 muestran los costos normalizados ponderados por tema y los tiempos de comparación en los conjuntos de datos TDT2 y TDT3 respectivamente. Dado que no se contaba con un conjunto de datos de validación para ajustar el umbral θ nuevo en los experimentos en TDT2, solo informamos los costos normalizados mínimos para nuestros sistemas en la tabla 3. El sistema-5 supera a todos los demás sistemas, incluido el sistema-6, y realiza solo 2.78e+8 veces la comparación de tiempos en el procedimiento de detección, lo que representa solo el 13.4% del sistema-1. Tabla 3. Los resultados de NED en los sistemas TDT2 Min Norm(CDet) tiempos de comparación son: Sistema-1 0.5749 2.08e+9, Sistema-2 0.6673 3.77e+8, Sistema-3 0.5765 2.81e+8, Sistema-4 0.5431 2.99e+8, Sistema-5 0.5089 2.78e+8, Sistema-6 0.5300. Al evaluar en los costos normalizados en TDT3, utilizamos los umbrales óptimos obtenidos del conjunto de datos de TDT2 para todos los sistemas. El sistema-2 reduce los tiempos de comparación a 1.29e+9, que es solo el 18.3% del sistema-1, pero al mismo tiempo también obtiene un costo normalizado mínimo deteriorado que es 0.0499 más alto que el sistema-1. El Sistema-3 utiliza el nuevo procedimiento de detección basado en el índice de noticias en árbol. Requiere incluso menos tiempo de comparación que el sistema-2. Esto se debe a que las comparaciones de historias suelen arrojar mayores similitudes que las de grupos de historias, por lo que las historias tienden a combinarse en el sistema-3. Y el sistema-3 es básicamente equivalente al sistema-1 en los resultados de precisión. El System-4 ajusta los pesos de los términos basándose en la distancia de las distribuciones de términos entre todo el corpus y el conjunto de historias del clúster, lo que resulta en una mejora significativa de 0.0468 en comparación con el system-1. El mejor sistema (sistema-5) tiene un costo normalizado mínimo de 0.5012, lo cual es 0.0797 mejor que el sistema-1, y también mejor que cualquier otro resultado previamente reportado para este conjunto de datos [8, 13]. Además, el sistema-5 solo necesita 1.05e+8 veces de comparación, lo que equivale al 14.9% del sistema-1. Tabla 4. Los resultados de NED en los sistemas TDT3 Norm(CDet) Norma mínima(CDet) Tiempos de comparación Sistema-1 0.6159 0.5809 7.04e+8 Sistema-2① 0.6493 0.6308 1.29e+8 Sistema-3② 0.6197 0.5868 1.03e+8 Sistema-4② 0.5601 0.5341 1.03e+8 Sistema-5② 0.5413 0.5012 1.05e+8 Sistema-7 -- 0.5783 -Sistema-8 -- 0.5229 -① θ nuevo=0.13 ② θ inicial=0.13, λ =3, δ =0.15 La Figura 5 muestra las cinco curvas DET para nuestros sistemas en el conjunto de datos TDT3. El Sistema-5 logra el costo mínimo con una tasa de falsas alarmas de 0.0157 y una tasa de omisión de 0.4310. Podemos observar que System4 y System-5 obtienen una menor probabilidad de error de omisión en regiones de baja probabilidad de falsa alarma. La hipótesis es que se transfiere más valor de peso a los términos clave de los temas que a los términos no clave. La puntuación de similitud entre dos historias pertenecientes a diferentes temas es más baja que antes, porque los términos superpuestos suelen no ser términos clave de sus temas. 7.2 Selección de parámetros para la detección de árboles de indexación. La Figura 3 muestra los costos normalizados mínimos obtenidos por el sistema-3 en TDT3 utilizando diferentes parámetros. El parámetro de inicio θ se prueba en seis valores que van desde 0.03 hasta 0.18. Y el parámetro λ se prueba en cuatro valores: 1, 2, 3 y 4. Podemos ver que, cuando θ inicial se establece en 0.12, que es el más cercano a θ nuevo, los costos son más bajos que los demás. Esto es fácil de explicar, porque cuando se agrupan historias pertenecientes al mismo tema en un conjunto, es más razonable que el conjunto represente las historias en él. Cuando el parámetro λ se establece en 3 o 4, los costos son mejores que en otros casos, pero no hay mucha diferencia entre 3 y 4. 0 0.05 0.1 0.15 0.2 1 2 3 4 0.5 0.6 0.7 0.8 0.9 1 θ-initλ MinCost 0.6 0.65 0.7 0.75 0.8 0.85 0.9 Figura 3. Costo mínimo en TDT3 (δ =0.15) 0 0.05 0.1 0.15 0.2 1 2 3 4 0 0.5 1 1.5 2 2.5 x 10 8 θ-inicio λ Comparando tiempos 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 8 Figura 4. Comparando los tiempos en TDT3 (δ =0.15), la Figura 4 muestra los tiempos de comparación utilizados por el sistema-3 en TDT3 con los mismos parámetros que la Figura 3. Los tiempos de comparación dependen fuertemente de θ init. Debido a que cuanto mayor sea θ init, menos historias se combinan juntas, más veces de comparación se necesitan para la decisión de un nuevo evento. Por lo tanto, usamos θ init = 0.13, λ = 3, δ = 0.15 para los sistemas 3, 4 y 5. En esta configuración de parámetros, podemos obtener tanto costos normalizados mínimos bajos como menos tiempos de comparación. CONCLUSIÓN Hemos propuesto un procedimiento de detección basado en un árbol de indexación de noticias en nuestro modelo. Reduce los tiempos de comparación a aproximadamente una séptima parte del método tradicional sin afectar la precisión de NED. También hemos presentado dos extensiones al modelo básico TF-IDF. La primera extensión se realiza ajustando los pesos de los términos basados en las distribuciones de términos entre todo el corpus y un conjunto de historias de un clúster. Y la segunda extensión al modelo básico TF-IDF es un mejor uso de los tipos de términos (tipos de entidades nombradas y partes de la velocidad) de acuerdo con las categorías de noticias. Nuestros resultados experimentales en los conjuntos de datos TDT2 y TDT3 muestran que ambas extensiones contribuyen significativamente a la mejora en la precisión. No consideramos la información de tiempo de noticias como una pista para la tarea de NED, ya que la mayoría de los temas duran mucho tiempo y los conjuntos de datos de TDT solo abarcan un período relativamente corto (no más de 6 meses). Para el trabajo futuro, queremos recopilar un conjunto de noticias que abarque un período más largo de internet e integrar información temporal en la tarea de NED. Dado que el tema es un grupo de noticias relativamente grueso, también queremos refinar la granularidad del grupo a nivel de evento e identificar diferentes eventos y sus relaciones dentro de un tema. Agradecimientos: Este trabajo ha sido apoyado por la Fundación Nacional de Ciencias Naturales de China bajo la subvención No. 90604025. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son del autor(es) y no necesariamente reflejan las del patrocinador. REFERENCIAS [1] http://www.nist.gov/speech/tests/tdt/index.htm [2] En Detección y Seguimiento de Temas. Organización de la información basada en eventos. Kluwer Academic Publishers, 2002. .01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90 1 2 5 10 20 40 60 80 90 Probabilidad de Falsa Alarma (en %) Probabilidad de Omisión (en %) Curva Ponderada por Tema del SISTEMA1 Norma Mínima (Costo) del SISTEMA1 Curva Ponderada por Tema del SISTEMA2 Norma Mínima (Costo) del SISTEMA2 Curva Ponderada por Tema del SISTEMA3 Norma Mínima (Costo) del SISTEMA3 Curva Ponderada por Tema del SISTEMA4 Norma Mínima (Costo) del SISTEMA4 Curva Ponderada por Tema del SISTEMA5 Norma Mínima (Costo) del SISTEMA5 Rendimiento Aleatorio Figura 5. Curvas DET en TDT3 [3] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B.T. Archibald y X. Liu. Enfoques de aprendizaje para detectar y rastrear eventos de noticias. En el número especial de IEEE Intelligent Systems sobre Aplicaciones de Recuperación de Información Inteligente, volumen 14 (4), 1999, páginas 32-43. [4] Y. Yang, T. Pierce y J. Carbonell. Un estudio sobre la detección de eventos retrospectivos y en línea. En Actas de SIGIR-98, Melbourne, Australia, 1998, 28-36. [5] J. Allan, V. Lavrenko, D. Malin y R. Swan. Detecciones, límites y líneas de tiempo: Umass y tdt-3. En Actas del Taller de Detección y Seguimiento de Temas (TDT-3), Viena, VA, 2000, 167-174. [6] R. Papka y J. Allan. Detección de nuevos eventos en línea utilizando agrupamiento de un solo paso TÍTULO2:. Informe técnico UM-CS1998-021, 1998. [7] W. Lam, H. Meng, K. Wong y J. I'm sorry, but \"Yen\" is not a sentence. Can you please provide a sentence for me to translate into Spanish? Utilizando el Análisis Contextual para la Detección de Eventos de Noticias. Revista Internacional de Sistemas Inteligentes, 2001, 525-546. [8] B. Thorsten, C. Francine y F. Ayman. Un sistema para la detección de nuevos eventos. En Actas de la 26ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2003, 330-337. [9] S. Nicola y C. Joe. Combinando clasificadores semánticos y sintácticos de documentos para mejorar la detección de primeras noticias. En Actas de la 24ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell, and C. Jin. \n\nACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell y C. Jin. Detección de novedades condicionada al tema. En Actas de la 8ª Conferencia Internacional de ACM SIGKDD, ACM Press. 2002, 688-693. [11] M. Juha, A.M. Helena y S. Marko. Aplicando Clases Semánticas en la Detección y Seguimiento de Eventos. En Actas de la Conferencia Internacional sobre Procesamiento del Lenguaje Natural (ICON 2002), 2002, páginas 175-183. [12] M. Juha, A.M. Helena y S. Marko. Semántica simple en la detección y seguimiento de temas. Recuperación de información, 7(3-4): 2004, 347-368. [13] K. Giridhar y J. Allan. Clasificación de texto y entidades nombradas para la detección de nuevos eventos. En Actas de la 27ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2004, 297-304. [14] J. P. Callan, W. B. Croft, y S. M. Harding. El Sistema de Recuperación INQUERY. En Actas de DEXA-92, 3ra Conferencia Internacional sobre Aplicaciones de Bases de Datos y Sistemas Expertos, 1992, 78-83. [15] R. Krovetz. Viendo la morfología como un proceso de inferencia. En Actas de ACM SIGIR93, 1993, 61-81. [16] Y. Yang y J. Pedersen. Un estudio comparativo sobre la selección de características en la categorización de textos. En J. D. H. Fisher, editor, La Decimocuarta Conferencia Internacional sobre Aprendizaje Automático (ICML97), Morgan Kaufmann, 1997, 412-420. [17] T. M. Cover y J. A. I'm sorry, but \"Thomas\" is not a sentence. Can you provide more context or a complete sentence for me to translate into Spanish? Elementos de la teoría de la información. Wiley. 1991. [18] El consorcio de datos lingüísticos, http://www.ldc,upenn.edu/. [19] La definición de la tarea TDT 2001 y el plan de evaluación, http://www.nist.gov/speech/tests/tdt/tdt2001/evalplan.htm. [20] R. E. Schapire y Y. Cantante. Boostexter: Un sistema basado en Boosting para la categorización de texto. En Machine Learning 39(2/3):1, Kluwer Academic Publishers, 2000, 35-168. [21] K. Giridhar y J. Allan. 2005. Utilizando nombres y temas para la detección de nuevos eventos. En Actas de la Conferencia de Tecnología Humana y la Conferencia sobre Métodos Empíricos en Lenguaje Natural, Vancouver, 2005, 121-128 ",
            "candidates": [],
            "error": [
                [
                    "peso de los términos",
                    "peso de los términos",
                    "peso del término"
                ]
            ]
        },
        "statistics": {
            "translated_key": "estadísticas",
            "is_in_text": true,
            "original_annotated_sentences": [
                "New Event Detection Based on Indexing-tree and Named Entity Zhang Kuo Tsinghua University Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Tsinghua University Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Tsinghua University Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn ABSTRACT New Event Detection (NED) aims at detecting from one or multiple streams of news stories that which one is reported on a new event (i.e.",
                "not reported previously).",
                "With the overwhelming volume of news available today, there is an increasing need for a NED system which is able to detect new events more efficiently and accurately.",
                "In this paper we propose a new NED model to speed up the NED task by using news indexing-tree dynamically.",
                "Moreover, based on the observation that terms of different types have different effects for NED task, two term reweighting approaches are proposed to improve NED accuracy.",
                "In the first approach, we propose to adjust term weights dynamically based on previous story clusters and in the second approach, we propose to employ <br>statistics</br> on training data to learn the named entity reweighting model for each class of stories.",
                "Experimental results on two Linguistic Data Consortium (LDC) datasets TDT2 and TDT3 show that the proposed model can improve both efficiency and accuracy of NED task significantly, compared to the baseline system and other existing systems.",
                "Categories and Subject Descriptors H.3.3 [Information Systems]: Information Search and Retrieval; H.4.2 [Information Systems Applications]: Types of Systemsdecision support.",
                "General Terms Algorithms, Performance, Experimentation 1.",
                "INTRODUCTION Topic Detection and Tracking (TDT) program aims to develop techniques which can effectively organize, search and structure news text materials from a variety of newswire and broadcast media [1].",
                "New Event Detection (NED) is one of the five tasks in TDT.",
                "It is the task of online identification of the earliest report for each topic as soon as that report arrives in the sequence of documents.",
                "A Topic is defined as a seminal event or activity, along with directly related events and activities [2].",
                "An Event is defined as something (non-trivial) happening in a certain place at a certain time [3].",
                "For instance, when a bomb explodes in a building, the exploding is the seminal event that triggers the topic, and other stories on the same topic would be those discussing salvaging efforts, the search for perpetrators, arrests and trial and so on.",
                "Useful news information is usually buried in a mass of data generated everyday.",
                "Therefore, NED systems are very useful for people who need to detect novel information from real-time news stream.",
                "These real-life needs often occur in domains like financial markets, news analysis, and intelligence gathering.",
                "In most of state-of-the-art (currently) NED systems, each news story on hand is compared to all the previous received stories.",
                "If all the similarities between them do not exceed a threshold, then the story triggers a new event.",
                "They are usually in the form of cosine similarity or Hellinger similarity metric.",
                "The core problem of NED is to identify whether two stories are on the same topic.",
                "Obviously, these systems cannot take advantage of topic information.",
                "Further more, it is not acceptable in real applications because of the large amount of computation required in the NED process.",
                "Other systems organize previous stories into clusters (each cluster corresponds to a topic), and new story is compared to the previous clusters instead of stories.",
                "This manner can reduce comparing times significantly.",
                "Nevertheless, it has been proved that this manner is less accurate [4, 5].",
                "This is because sometimes stories within a topic drift far away from each other, which could lead low similarity between a story and its topic.",
                "On the other hand, some proposed NED systems tried to improve accuracy by making better use of named entities [10, 11, 12, 13].",
                "However, none of the systems have considered that terms of different types (e.g.",
                "Noun, Verb or Person name) have different effects for different classes of stories in determining whether two stories are on the same topic.",
                "For example, the names of election candidates (Person name) are very important for stories of election class; the locations (Location name) where accidents happened are important for stories of accidents class.",
                "So, in NED, there still exist following three problems to be investigated: (1) How to speed up the detection procedure while do not decrease the detection accuracy? (2) How to make good use of cluster (topic) information to improve accuracy? (3) How to obtain better news story representation by better understanding of named entities.",
                "Driven by these problems, we have proposed three approaches in this paper. (1)To make the detection procedure faster, we propose a new NED procedure based on news indexing-tree created dynamically.",
                "Story indexing-tree is created by assembling similar stories together to form news clusters in different hierarchies according to their values of similarity.",
                "Comparisons between current story and previous clusters could help find the most similar story in less comparing times.",
                "The new procedure can reduce the amount of comparing times without hurting accuracy. (2)We use the clusters of the first floor in the indexing-tree as news topics, in which term weights are adjusted dynamically according to term distribution in the clusters.",
                "In this approach, cluster (topic) information is used properly, so the problem of theme decentralization is avoided. (3)Based on observations on the <br>statistics</br> obtained from training data, we found that terms of different types (e.g.",
                "Noun and Verb) have different effects for different classes of stories in determining whether two stories are on the same topic.",
                "And we propose to use <br>statistics</br> to optimize the weights of the terms of different types in a story according to the news class that the story belongs to.",
                "On TDT3 dataset, the new NED model just uses 14.9% comparing times of the basic model, while its minimum normalized cost is 0.5012, which is 0.0797 better than the basic model, and also better than any other results previously reported for this dataset [8, 13].",
                "The rest of the paper is organized as follows.",
                "We start off this paper by summarizing the previous work in NED in section 2.",
                "Section 3 presents the basic model for NED that most current systems use.",
                "Section 4 describes our new detection procedure based on news indexing-tree.",
                "In section 5, two term reweighting methods are proposed to improve NED accuracy.",
                "Section 6 gives our experimental data and evaluation metrics.",
                "We finally wrap up with the experimental results in Section 7, and the conclusions and future work in Section 8. 2.",
                "RELATED WORK Papka et al. proposed Single-Pass clustering on NED [6].",
                "When a new story was encountered, it was processed immediately to extract term features and a query representation of the storys content is built up.",
                "Then it was compared with all the previous queries.",
                "If the document did not trigger any queries by exceeding a threshold, it was marked as a new event.",
                "Lam et al build up previous query representations of story clusters, each of which corresponds to a topic [7].",
                "In this manner comparisons happen between stories and clusters.",
                "Recent years, most work focus on proposing better methods on comparison of stories and document representation.",
                "Brants et al. [8] extended a basic incremental TF-IDF model to include sourcespecific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, term reweighting based on inverse event frequencies, and segmentation of documents.",
                "Good improvements on TDT bench-marks were shown.",
                "Stokes et al. [9] utilized a combination of evidence from two distinct representations of a documents content.",
                "One of the representations was the usual free text vector, the other made use of lexical chains (created using WordNet) to build another term vector.",
                "Then the two representations are combined in a linear fashion.",
                "A marginal increase in effectiveness was achieved when the combined representation was used.",
                "Some efforts have been done on how to utilize named entities to improve NED.",
                "Yang et al. gave location named entities four times weight than other terms and named entities [10].",
                "DOREMI research group combined semantic similarities of person names, location names and time together with textual similarity [11][12].",
                "UMass [13] research group split document representation into two parts: named entities and non-named entities.",
                "And it was found that some classes of news could achieve better performance using named entity representation, while some other classes of news could achieve better performance using non-named entity representation.",
                "Both [10] and [13] used text categorization technique to classify news stories in advance.",
                "In [13] news stories are classified automatically at first, and then test sensitivities of names and non-name terms for NED for each class.",
                "In [10] frequent terms for each class are removed from document representation.",
                "For example, word election does not help identify different elections.",
                "In their work, effectiveness of different kinds of names (or terms with different POS) for NED in different news classes are not investigated.",
                "We use statistical analysis to reveal the fact and use it to improve NED performance. 3.",
                "BASIC MODEL In this section, we present the basic New Event Detection model which is similar to what most current systems apply.",
                "Then, we propose our new model by extending the basic model.",
                "New Event Detection systems use news story stream as input, in which stories are strictly time-ordered.",
                "Only previously received stories are available when dealing with current story.",
                "The output is a decision for whether the current story is on a new event or not and the confidence of the decision.",
                "Usually, a NED model consists of three parts: story representation, similarity calculation and detection procedure. 3.1 Story Representation Preprocessing is needed before generating story representation.",
                "For preprocessing, we tokenize words, recognize abbreviations, normalize abbreviations, add part-of-speech tags, remove stopwords included in the stop list used in InQuery [14], replace words with their stems using K-stem algorithm[15], and then generate word vector for each news story.",
                "We use incremental TF-IDF model for term weight calculation [4].",
                "In a TF-IDF model, term frequency in a news document is weighted by the inverse document frequency, which is generated from training corpus.",
                "When a new term occurs in testing process, there are two solutions: simply ignore the new term or set df of the term as a small const (e.g. df = 1).",
                "The new term receives too low weight in the first solution (0) and too high weight in the second solution.",
                "In incremental TF-IDF model, document frequencies are updated dynamically in each time step t: 1( ) ( ) ( )t t D tdf w df w df w−= + (1) where Dt represents news story set received in time t, and dfDt(w) means the number of documents that term w occurs in, and dft(w) means the total number of documents that term w occurs in before time t. In this work, each time window includes 50 news stories.",
                "Thus, each story d received in t is represented as follows: 1 2{ ( , , ), ( , , ),..., ( , , )}nd weight d t w weight d t w weight d t w→ where n means the number of distinct terms in story d, and ( , , )weight d t w means the weight of term w in story d at time t: log( ( , ) 1) log(( 1) /( ( ) 0.5)) ( , , ) log( ( , ) 1) log(( 1) /( ( ) 0.5)) t t t t w d tf d w N df w weight d t w tf d w N df w ∈ + + + = + + +∑ (2) where Nt means the total number of news stories before time t, and tf(d,w) means how many times term w occurs in news story d. 3.2 Similarity Calculation We use Hellinger distance for the calculation of similarity between two stories, for two stories d and d at time t, their similarity is defined as follows: , ( , , ) ( , , ) * ( , , ) w d d sim d d t weight d t w weight d t w ∈ = ∑ (3) 3.3 Detection Procedure For each story d received in time step t, the value ( ) ( ) ( ) ( ( , , )) time d time d n d max sim d d t < = (4) is a score used to determine whether d is a story about a new topic and at the same time is an indication of the confidence in our decision [8]. time(d) means the publication time of story d. If the score exceeds the thresholdθ new, then there exists a sufficiently similar document, thus d is a old story, otherwise, there is no sufficiently similar previous document, thus d is an new story. 4.",
                "New NED Procedure Traditional NED systems can be classified into two main types on the aspect of detection procedure: (1) S-S type, in which the story on hand is compared to each story received previously, and use the highest similarity to determine whether current story is about a new event; (2) S-C type, in which the story on hand is compared to all previous clusters each of which representing a topic, and the highest similarity is used for final decision for current story.",
                "If the highest similarity exceeds thresholdθ new, then it is an old story, and put it into the most similar cluster; otherwise it is a new story and create a new cluster.",
                "Previous work show that the first manner is more accurate than the second one [4][5].",
                "Since sometimes stories within a topic drift far away from each other, a story may have very low similarity with its topic.",
                "So using similarities between stories for determining new story is better than using similarities between story and clusters.",
                "Nevertheless, the first manner needs much more comparing times which means the first manner is low efficient.",
                "We propose a new detection procedure which uses comparisons with previous clusters to help find the most similar story in less comparing times, and the final new event decision is made according to the most similar story.",
                "Therefore, we can get both the accuracy of S-S type methods and the efficiency of S-C type methods.",
                "The new procedure creates a news indexing-tree dynamically, in which similar stories are put together to form a hierarchy of clusters.",
                "We index similar stories together by their common ancestor (a cluster node).",
                "Dissimilar stories are indexed in different clusters.",
                "When a story is coming, we use comparisons between the current story and previous hierarchical clusters to help find the most similar story which is useful for new event decision.",
                "After the new event decision is made, the current story is inserted to the indexing-tree for the following detection.",
                "The news indexing-tree is defined formally as follows: S-Tree = {r, NC , NS , E} where r is the root of S-Tree, NC is the set of all cluster nodes, NS is the set of all story nodes, and E is the set of all edges in S-Tree.",
                "We define a set of constraints for a S-Tree: ⅰ . , is an non-terminal node in the treeC i i N i∀ ∈ → ⅱ . , is a terminal node in the treeS i i N i∀ ∈ → ⅲ . , out degree of is at least 2C i i N i∀ ∈ → ⅳ . , is represented as the centroid of its desendantsC i i iN∀ ∈ → For a news story di, the comparison procedure and inserting procedure based on indexing-tree are defined as follows.",
                "An example is shown by Figure 1 and Figure 2.",
                "Figure 1.",
                "Comparison procedure Figure 2.",
                "Inserting procedure Comparison procedure: Step 1: compare di to all the direct child nodes of r and select λ nodes with highest similarities, e.g., C1 2 and C1 3 in Figure 1.",
                "Step 2: for each selected node in the last step, e.g.",
                "C1 2, compare di to all its direct child nodes, and select λ nodes with highest similarities, e.g.",
                "C2 2 and d8.",
                "Repeat step 2 for all non-terminal nodes.",
                "Step 3: record the terminal node with the highest similarty to di, e.g. s5, and the similarity value (0.20).",
                "Inserting di to the S-tree with r as root: Find the node n which is direct child of r in the path from r to the terminal node with highest similarity s, e.g.",
                "C1 2.",
                "If s is smaller than θ init+(h-1)δ , then add di to the tree as a direct child of r. Otherwise, if n is a terminal node, then create a cluster node instead of n, and add both n and di as its direct children; if n is an non-terminal node, then repeat this procedure and insert di to the sub-tree with n as root recursively.",
                "Here h is the length between n and the root of S-tree.",
                "The more the stories in a cluster similar to each other, the better the cluster represents the stories in it.",
                "Hence we add no constraints on the maximum of trees height and degree of a node.",
                "Therefore, we cannot give the complexity of this indexing-tree based procedure.",
                "But we will give the number of comparing times needed by the new procedure in our experiments in section7. 5.",
                "Term Reweighting Methods In this section, two term reweighting methods are proposed to improve NED accuracy.",
                "In the first method, a new way is explored for better using of cluster (topic) information.",
                "The second one finds a better way to make use of named entities based on news classification. 5.1 Term Reweighting Based on Distribution Distance TF-IDF is the most prevalent model used in information retrieval systems.",
                "The basic idea is that the fewer documents a term appears in, the more important the term is in discrimination of documents (relevant or not relevant to a query containing the term).",
                "Nevertheless, in TDT domain, we need to discriminate documents with regard to topics rather than queries.",
                "Intuitively, using cluster (topic) vectors to compare with subsequent news stories should outperform using story vectors.",
                "Unfortunately, the experimental results do not support this intuition [4][5].",
                "Based on observation on data, we find the reason is that a news topic usually contains many directly or indirectly related events, while they all have their own sub-subjects which are usually different with each other.",
                "Take the topic described in section 1 as an example, events like the explosion and salvage have very low similarities with events about criminal trial, therefore stories about trial would have low similarity with the topic vector built on its previous events.",
                "This section focuses on how to effectively make use of topic information and at the same time avoid the problem of content decentralization.",
                "At first, we classify terms into 5 classes to help analysis the needs of the modified model: Term class A: terms that occur frequently in the whole corpus, e.g., year and people.",
                "Terms of this class should be given low weights because they do not help much for topic discrimination.",
                "Term class B: terms that occur frequently within a news category, e.g., election, storm.",
                "They are useful to distinguish two stories in different news categories.",
                "However, they cannot provide information to determine whether two stories are on the same or different topics.",
                "In another words, term election and term storm are not helpful in differentiate two election campaigns and two storm disasters.",
                "Therefore, terms of this class should be assigned lower weights.",
                "Term class C: terms that occur frequently in a topic, and infrequently in other topics, e.g., the name of a crash plane, the name of a specific hurricane.",
                "News stories that belong to different topics rarely have overlap terms in this class.",
                "The more frequently a term appears in a topic, the more important the term is for a story belonging to the topic, therefore the term should be set higher weight.",
                "Term class D: terms that appear in a topic exclusively, but not frequently.",
                "For example, the name of a fireman who did very well in a salvage action, which may appears in only two or three stories but never appeared in other topics.",
                "Terms of this type should receive more weights than in TF-IDF model.",
                "However, since they are not popular in the topic, it is not appropriate to give them too high weights.",
                "Term class E: terms with low document frequency, and appear in different topics.",
                "Terms of this class should receive lower weights.",
                "Now we analyze whether TF-IDF model can give proper weights to the five classes of terms.",
                "Obviously, terms of class A are lowly weighted in TF-IDF model, which is conformable with the requirement described above.",
                "In TF-IDF model, terms of class B are highly dependant with the number of stories in a news class.",
                "TF-IDF model cannot provide low weights if the story containing the term belongs to a relative small news class.",
                "For a term of class C, the more frequently it appears in a topic, the less weight TFIDF model gives to it.",
                "This strongly conflicts with the requirement of terms in class C. For terms of class D, TF-IDF model gives them high weights correctly.",
                "But for terms of class E, TF-IDF model gives high weights to them which are not conformable with the requirement of low weights.",
                "To sum up, terms of class B, C, E cannot be properly weighted in TF-IDF model.",
                "So, we propose a modified model to resolve this problem.",
                "When θ init andθ new are set closely, we assume that most of the stories in a first-level cluster (a direct child node of root node) are on the same topic.",
                "Therefore, we make use of a first-level cluster to capture term distribution (df for all the terms within the cluster) within the topic dynamically.",
                "KL divergence of term distribution in a first-level cluster and the whole story set is used to adjust term weights:  ( , , ) * (1 * ( || )) ( , , ) ( , , ) * (1 * ( || )) cw tw cw tw w d D weight d t w KL P P weight d t w weight d t w KL P P γ γ ∈ + = +∑ (5) where ( ) ( ) ( ) ( ) 1,cw cw c c c c df w df w p y p y N N = = − (6) ( ) ( ) ( ) ( ) 1,t t tw tw t t df w df w p y p y N N = = − (7) where dfc(w) is the number of documents containing term w within cluster C, and Nc is the number of documents in cluster C, and Nt is the total number of documents that arrive before time step t. γ is a const parameter, now is manually set 3.",
                "KL divergence is defined as follows [17]: ( ) ( || ) ( ) log ( )x p x KL P Q p x q x = ∑ (8) The basic idea is: for a story in a topic, the more a term occurs within the topic, and the less it occurs in other topics, it should be assigned higher weights.",
                "Obviously, modified model can meet all the requirements of the five term classes listed above. 5.2 Term Reweighting Based on Term Type and Story Class Previous work found that some classes of news stories could achieve good improvements by giving extra weight to named entities.",
                "But we find that terms of different types should be given different amount of extra weight for different classes of news stories.",
                "We use open-NLP1 to recognize named entity types and part-ofspeech tags for terms that appear in news stories.",
                "Named entity types include person name, organization name, location name, date, time, money and percentage, and five POSs are selected: none (NN), verb (VB), adjective (JJ), adverb (RB) and cardinal number (CD).",
                "Statistical analysis shows topic-level discriminative terms types for different classes of stories.",
                "For the sake of convenience, named entity type and part-of-speech tags are uniformly called term type in subsequent sections.",
                "Determining whether two stories are about the same topic is a basic component for NED task.",
                "So at first we use 2 χ statistic to compute correlations between terms and topics.",
                "For a term t and a topic T, a contingence table is derived: Table 1.",
                "A 2×2 Contingence Table Doc Number belong to topic T not belong to topic T include t A B not include t C D The 2 χ statistic for a specific term t with respect to topic T is defined to be [16]: 2 2 ( , ) ( ) * ( ) ( ) * ( ) * ( ) * ( ) w T A B C D AD CB A C B D A B C D χ = + + + − + + + + (9) News topics for the TDT task are further classified into 11 rules of interpretations (ROIs) 2 .",
                "The ROI can be seen as a higher level class of stories.",
                "The average correlation between a term type and a topic ROI is computed as: 2 avg 2 ( , )( ( , ) )k m m km kT R w P w TP R p w T R P χ χ ∈ ∈ ∑ ∑（ , ）= 1 1 k=1…K, m=1…M (10) where K is the number of term types (set 12 constantly in the paper).",
                "M is the number news classes (ROIs, set 11 in the paper).",
                "Pk represents the set of all terms of type k, and Rm represents the set of all topics of class m, p(t,T) means the probability that t occurs in topic T. Because of limitation of space, only parts of the term types (9 term types) and parts of news classes (8 classes) are listed in table 2 with the average correlation values between them.",
                "The <br>statistics</br> is derived from labeled data in TDT2 corpus. (Results in table 2 are already normalized for convenience in comparison.)",
                "The <br>statistics</br> in table 2 indicates the usefulness of different term types in topic discrimination with respect to different news classes.",
                "We can see that, location name is the most useful term type for three news classes: Natural Disasters, Violence or War, Finances.",
                "And for three other categories Elections, Legal/Criminal Cases, Science and Discovery, person name is the most discriminative term type.",
                "For Scandals/Hearings, date is the most important information for topic discrimination.",
                "In addition, Legal/Criminal Cases and Finance topics have higher correlation with money terms, while Science and Discovery have higher correlation with percentage terms.",
                "Non-name terms are more stable for different classes. 1 . http://opennlp.sourceforge.net/ 2 . http://projects.ldc.upenn.edu/TDT3/Guide/label.html From the analysis of table 2, it is reasonable to adjust term weight according to their term type and the news class the story belongs to.",
                "New term weights are reweighted as follows: ( ) ( ) ( ) ( ) ( , , ) * ( , , ) ( , , ) * class d D type w T class d D type w w d weight d t w weight d t w weight d t w α α ∈ = ∑ (11) where type(w) represents the type of term w, and class(d) represents the class of story d, c kα is reweighting parameter for news class c and term type k. In the work, we just simply use <br>statistics</br> in table 2 as the reweighting parameters.",
                "Even thought using the <br>statistics</br> directly may not the best choice, we do not discuss how to automatically obtain the best parameters.",
                "We will try to use machine learning techniques to obtain the best parameters in the future work.",
                "In the work, we use BoosTexter [20] to classify all stories into one of the 11 ROIs.",
                "BoosTexter is a boosting based machine learning program, which creates a series of simple rules for building a classifier for text or attribute-value data.",
                "We use term weight generated using TF-IDF model as feature for story classification.",
                "We trained the model on the 12000 judged English stories in TDT2, and classify the rest of the stories in TDT2 and all stories in TDT3.",
                "Classification results are used for term reweighting in formula (11).",
                "Since the class labels of topic-off stories are not given in TDT datasets, we cannot give the classification accuracy here.",
                "Thus we do not discuss the effects of classification accuracy to NED performance in the paper. 6.",
                "EXPERIMENTAL SETUP 6.1 Datasets We used two LDC [18] datasets TDT2 and TDT3 for our experiments.",
                "TDT2 contains news stories from January to June 1998.",
                "It contains around 54,000 stories from sources like ABC, Associated Press, CNN, New York Times, Public Radio International, Voice of America etc.",
                "Only English stories in the collection were considered.",
                "TDT3 contains approximately 31,000 English stories collected from October to December 1998.",
                "In addition to the sources used in TDT2, it also contains stories from NBC and MSNBC TV broadcasts.",
                "We used transcribed versions of the TV and radio broadcasts besides textual news.",
                "TDT2 dataset is labeled with about 100 topics, and approximately 12,000 English stories belong to at least one of these topics.",
                "TDT3 dataset is labeled with about 120 topics, and approximately 8000 English stories belong to at least one of these topics.",
                "All the topics are classified into 11 Rules of Interpretation: (1)Elections, (2)Scandals/Hearings, (3)Legal/Criminal Cases, (4)Natural Disasters, (5)Accidents, (6)Ongoing Violence or War, (7)Science and Discovery News, (8)Finance, (9)New Law, (10)Sports News, (11)MISC.",
                "News. 6.2 Evaluation Metric TDT uses a cost function CDet that combines the probabilities of missing a new story and a false alarm [19]: * * * *Det Miss Miss Target FA FA NontargetC C P P C P P= + (12) Table 2.",
                "Average correlation between term types and news classes where CMiss means the cost of missing a new story, PMiss means the probability of missing a new story, and PTarget means the probability of seeing a new story in the data; CFA means the cost of a false alarm, PFA means the probability of a false alarm, and PNontarget means the probability of seeing an old story.",
                "The cost CDet is normalized such that a perfect system scores 0 and a trivial system, which is the better one of mark all stories as new or old, scores 1: ( ( * , * ) ) Det Det Miss Target FA Nontarget C Norm C min C P C P = (13) New event detection system gives two outputs for each story.",
                "The first part is yes or no indicating whether the story triggers a new event or not.",
                "The second part is a score indicating confidence of the first decision.",
                "Confidence scores can be used to plot DET curve, i.e., curves that plot false alarm vs. miss probabilities.",
                "Minimum normalized cost can be determined if optimal threshold on the score were chosen. 7.",
                "EXPERIMENTAL RESULTS 7.1 Main Results To test the approaches proposed in the model, we implemented and tested five systems: System-1: this system is used as baseline.",
                "It is implemented based on the basic model described in section 3, i.e., using incremental TF-IDF model to generate term weights, and using Hellinger distance to compute document similarity.",
                "Similarity score normalization is also employed [8].",
                "S-S detection procedure is used.",
                "System-2: this system is the same as system-1 except that S-C detection procedure is used.",
                "System-3: this system is the same as system-1 except that it uses the new detection procedure which is based on indexing-tree.",
                "System-4: implemented based on the approach presented in section 5.1, i.e., terms are reweighted according to the distance between term distributions in a cluster and all stories.",
                "The new detection procedure is used.",
                "System-5: implemented based on the approach presented in section 5.2, i.e., terms of different types are reweighted according to news class using trained parameters.",
                "The new detection procedure is used.",
                "The following are some other NED systems: System-6: [21] for each pair of stories, it computes three similarity values for named entity, non-named entity and all terms respectively.",
                "And employ Support Vector Machine to predict new or old using the similarity values as features.",
                "System-7: [8] it extended a basic incremental TF-IDF model to include source-specific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, etc.",
                "System-8: [13] it split document representation into two parts: named entities and non-named entities, and choose one effective part for each news class.",
                "Table 3 and table 4 show topic-weighted normalized costs and comparing times on TDT2 and TDT3 datasets respectively.",
                "Since no heldout data set for fine-tuning the threshold θ new was available for experiments on TDT2, we only report minimum normalized costs for our systems in table 3.",
                "System-5 outperforms all other systems including system-6, and it performs only 2.78e+8 comparing times in detection procedure which is only 13.4% of system-1.",
                "Table 3.",
                "NED results on TDT2 Systems Min Norm(CDet) Cmp times System-1 0.5749 2.08e+9 System-2① 0.6673 3.77e+8 System-3② 0.5765 2.81e+8 System-4② 0.5431 2.99e+8 System-5② 0.5089 2.78e+8 System-6 0.5300 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 When evaluating on the normalized costs on TDT3, we use the optimal thresholds obtained from TDT2 data set for all systems.",
                "System-2 reduces comparing times to 1.29e+9 which is just 18.3% of system-1, but at the same time it also gets a deteriorated minimum normalized cost which is 0.0499 higher than system-1.",
                "System-3 uses the new detection procedure based on news indexing-tree.",
                "It requires even less comparing times than system-2.",
                "This is because story-story comparisons usually yield greater similarities than story-cluster ones, so stories tend to be combined Location Person Date Organization Money Percentage NN JJ CD Elections 0.37 1 0.04 0.58 0.08 0.03 0.32 0.13 0.1 Scandals/Hearings 0.66 0.62 0.28 1 0.11 0.02 0.27 0.13 0.05 Legal/Criminal Cases 0.48 1 0.02 0.62 0.15 0 0.22 0.24 0.09 Natural Disasters 1 0.27 0 0.04 0.04 0 0.25 0.04 0.02 Violence or War 1 0.36 0.02 0.14 0.02 0.04 0.21 0.11 0.02 Science and Discovery 0.11 1 0.01 0.22 0.08 0.12 0.19 0.08 0.03 Finances 1 0.45 0.04 0.98 0.13 0.02 0.29 0.06 0.05 Sports 0.16 0.27 0.01 1 0.02 0 0.11 0.03 0.01 together in system-3.",
                "And system-3 is basically equivalent to system-1 in accuracy results.",
                "System-4 adjusts term weights based on the distance of term distributions between the whole corpus and cluster story set, yielding a good improvement by 0.0468 compared to system-1.",
                "The best system (system-5) has a minimum normalized cost 0.5012, which is 0.0797 better than system-1, and also better than any other results previously reported for this dataset [8, 13].",
                "Further more, system-5 only needs 1.05e+8 comparing times which is 14.9% of system-1.",
                "Table 4.",
                "NED results on TDT3 Systems Norm(CDet) Min Norm(CDet) Cmp times System-1 0.6159 0.5809 7.04e+8 System-2① 0.6493 0.6308 1.29e+8 System-3② 0.6197 0.5868 1.03e+8 System-4② 0.5601 0.5341 1.03e+8 System-5② 0.5413 0.5012 1.05e+8 System-7 -- 0.5783 -System-8 -- 0.5229 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 Figure5 shows the five DET curves for our systems on data set TDT3.",
                "System-5 achieves the minimum cost at a false alarm rate of 0.0157 and a miss rate of 0.4310.",
                "We can observe that System4 and System-5 obtain lower miss probability at regions of low false alarm probabilities.",
                "The hypothesis is that, more weight value is transferred to key terms of topics from non-key terms.",
                "Similarity score between two stories belonging to different topics are lower than before, because their overlapping terms are usually not key terms of their topics. 7.2 Parameter selection for indexing-tree detection Figure 3 shows the minimum normalized costs obtained by system-3 on TDT3 using different parameters.",
                "Theθ init parameter is tested on six values spanning from 0.03 to 0.18.",
                "And the λ parameter is tested on four values 1, 2, 3 and 4.",
                "We can see that, whenθ init is set to 0.12, which is the closest one toθ new, the costs are lower than others.",
                "This is easy to explain, because when stories belonging to the same topic are put in a cluster, it is more reasonable for the cluster to represent the stories in it.",
                "When parameter λ is set to 3 or 4, the costs are better than other cases, but there is no much difference between 3 and 4. 0 0.05 0.1 0.15 0.2 1 2 3 4 0.5 0.6 0.7 0.8 0.9 1 θ-initλ MinCost 0.6 0.65 0.7 0.75 0.8 0.85 0.9 Figure 3.",
                "Min Cost on TDT3 (δ =0.15) 0 0.05 0.1 0.15 0.2 1 2 3 4 0 0.5 1 1.5 2 2.5 x 10 8 θ-init λ Comparingtimes 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 8 Figure 4.",
                "Comparing times on TDT3 (δ =0.15) Figure 4 gives the comparing times used by system-3 on TDT3 with the same parameters as figure 3.",
                "The comparing times are strongly dependent onθ init.",
                "Because the greaterθ init is, the less stories combined together, the more comparing times are needed for new event decision.",
                "So we useθ init =0.13,λ =3,δ =0.15 for system-3, 4, and 5.",
                "In this parameter setting, we can get both low minimum normalized costs and less comparing times. 8.",
                "CONCLUSION We have proposed a news indexing-tree based detection procedure in our model.",
                "It reduces comparing times to about one seventh of traditional method without hurting NED accuracy.",
                "We also have presented two extensions to the basic TF-IDF model.",
                "The first extension is made by adjust term weights based on term distributions between the whole corpus and a cluster story set.",
                "And the second extension to basic TF-IDF model is better use of term types (named entities types and part-of-speed) according to news categories.",
                "Our experimental results on TDT2 and TDT3 datasets show that both of the two extensions contribute significantly to improvement in accuracy.",
                "We did not consider news time information as a clue for NED task, since most of the topics last for a long time and TDT data sets only span for a relative short period (no more than 6 months).",
                "For the future work, we want to collect news set which span for a longer period from internet, and integrate time information in NED task.",
                "Since topic is a relative coarse-grained news cluster, we also want to refine cluster granularity to event-level, and identify different events and their relations within a topic.",
                "Acknowledgments This work is supported by the National Natural Science Foundation of China under Grant No. 90604025.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsor. 9.",
                "REFERENCES [1] http://www.nist.gov/speech/tests/tdt/index.htm [2] In Topic Detection and Tracking.",
                "Event-based Information Organization.",
                "Kluwer Academic Publishers, 2002. .01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90 1 2 5 10 20 40 60 80 90 False Alarm Probability (in %) MissProbability(in%) SYSTEM1 Topic Weighted Curve SYSTEM1 Min Norm(Cost) SYSTEM2 Topic Weighted Curve SYSTEM2 Min Norm(Cost) SYSTEM3 Topic Weighted Curve SYSTEM3 Min Norm(Cost) SYSTEM4 Topic Weighted Curve SYSTEM4 Min Norm(Cost) SYSTEM5 Topic Weighted Curve SYSTEM5 Min Norm(Cost) Random Performance Figure 5.",
                "DET curves on TDT3 [3] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B.T.",
                "Archibald, and X. Liu.",
                "Learning Approaches for Detecting and Tracking News Events.",
                "In IEEE Intelligent Systems Special Issue on Applications of Intelligent Information Retrieval, volume 14 (4), 1999, 32-43. [4] Y. Yang, T. Pierce, and J. Carbonell.",
                "A Study on Retrospective and On-line Event Detection.",
                "In Proceedings of SIGIR-98, Melbourne, Australia, 1998, 28-36. [5] J. Allan, V. Lavrenko, D. Malin, and R. Swan.",
                "Detections, Bounds, and Timelines: Umass and tdt-3.",
                "In Proceedings of Topic Detection and Tracking Workshop (TDT-3), Vienna, VA, 2000, 167-174. [6] R. Papka and J. Allan.",
                "On-line New Event Detection Using Single Pass Clustering TITLE2:.",
                "Technical Report UM-CS1998-021, 1998. [7] W. Lam, H. Meng, K. Wong, and J.",
                "Yen.",
                "Using Contextual Analysis for News Event Detection.",
                "International Journal on Intelligent Systems, 2001, 525-546. [8] B. Thorsten, C. Francine, and F. Ayman.",
                "A System for New Event Detection.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2003, 330-337. [9] S. Nicola and C. Joe.",
                "Combining Semantic and Syntactic Document Classifiers to Improve First Story Detection.",
                "In Proceedings of the 24th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topicconditioned Novelty Detection.",
                "In Proceedings of the 8th ACM SIGKDD International Conference, ACM Press. 2002, 688-693. [11] M. Juha, A.M. Helena, and S. Marko.",
                "Applying Semantic Classes in Event Detection and Tracking.",
                "In Proceedings of International Conference on Natural Language Processing (ICON 2002), 2002, pages 175-183. [12] M. Juha, A.M. Helena, and S. Marko.",
                "Simple Semantics in Topic Detection and Tracking.",
                "Information Retrieval, 7(3-4): 2004, 347-368. [13] K. Giridhar and J. Allan.",
                "Text Classification and Named Entities for New Event Detection.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2004, 297-304. [14] J. P. Callan, W. B. Croft, and S. M. Harding.",
                "The INQUERY Retrieval System.",
                "In Proceedings of DEXA-92, 3rd International Conference on Database and Expert Systems Applications, 1992, 78-83. [15] R. Krovetz.",
                "Viewing Morphology as An Inference Process.",
                "In Proceedings of ACM SIGIR93, 1993, 61-81. [16] Y. Yang and J. Pedersen.",
                "A Comparative Study on Feature Selection in Text Categorization.",
                "In J. D. H. Fisher, editor, The Fourteenth International Conference on Machine Learning (ICML97), Morgan Kaufmann, 1997, 412-420. [17] T. M. Cover, and J.A.",
                "Thomas.",
                "Elements of Information Theory.",
                "Wiley. 1991. [18] The linguistic data consortium, http://www.ldc,upenn.edu/. [19] The 2001 TDT task definition and evaluation plan, http://www.nist.gov/speech/tests/tdt/tdt2001/evalplan.htm. [20] R. E. Schapire and Y.",
                "Singer.",
                "Boostexter: A Boosting-based System for Text Categorization.",
                "In Machine Learning 39(2/3):1, Kluwer Academic Publishers, 2000, 35-168. [21] K. Giridhar and J. Allan. 2005.",
                "Using Names and Topics for New Event Detection.",
                "In Proceedings of Human Technology Conference and Conference on Empirical Methods in Natural Language, Vancouver, 2005, 121-128"
            ],
            "original_annotated_samples": [
                "In the first approach, we propose to adjust term weights dynamically based on previous story clusters and in the second approach, we propose to employ <br>statistics</br> on training data to learn the named entity reweighting model for each class of stories.",
                "In this approach, cluster (topic) information is used properly, so the problem of theme decentralization is avoided. (3)Based on observations on the <br>statistics</br> obtained from training data, we found that terms of different types (e.g.",
                "And we propose to use <br>statistics</br> to optimize the weights of the terms of different types in a story according to the news class that the story belongs to.",
                "The <br>statistics</br> is derived from labeled data in TDT2 corpus. (Results in table 2 are already normalized for convenience in comparison.)",
                "The <br>statistics</br> in table 2 indicates the usefulness of different term types in topic discrimination with respect to different news classes."
            ],
            "translated_annotated_samples": [
                "En el primer enfoque, proponemos ajustar dinámicamente los pesos de los términos basados en los clusters de historias previas y en el segundo enfoque, proponemos emplear <br>estadísticas</br> en los datos de entrenamiento para aprender el modelo de reajuste de entidades nombradas para cada clase de historias.",
                "En este enfoque, se utiliza adecuadamente la información de los grupos (temas), por lo que se evita el problema de la descentralización del tema. Basándonos en observaciones de las <br>estadísticas</br> obtenidas de los datos de entrenamiento, encontramos que los términos de diferentes tipos (por ejemplo,",
                "Y proponemos utilizar <br>estadísticas</br> para optimizar los pesos de los términos de diferentes tipos en una historia de acuerdo a la clase de noticias a la que pertenece la historia.",
                "Las <br>estadísticas</br> se derivan de datos etiquetados en el corpus TDT2. (Los resultados en la tabla 2 ya están normalizados para mayor comodidad en la comparación).",
                "Las <br>estadísticas</br> en la tabla 2 indican la utilidad de diferentes tipos de términos en la discriminación de temas con respecto a diferentes clases de noticias."
            ],
            "translated_text": "Detección de nuevos eventos basada en árbol de indexación y entidad nombrada Zhang Kuo Universidad Tsinghua Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Universidad Tsinghua Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Universidad Tsinghua Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn RESUMEN La Detección de Nuevos Eventos (NED) tiene como objetivo detectar de una o varias corrientes de noticias cuál se informa sobre un nuevo evento (es decir, no reportado previamente). Con el abrumador volumen de noticias disponible hoy en día, hay una creciente necesidad de un sistema NED que sea capaz de detectar eventos nuevos de manera más eficiente y precisa. En este artículo proponemos un nuevo modelo de NED para acelerar la tarea de NED mediante el uso dinámico de un árbol de indexación de noticias. Además, basándose en la observación de que los términos de diferentes tipos tienen diferentes efectos para la tarea de NED, se proponen dos enfoques de reponderación de términos para mejorar la precisión de NED. En el primer enfoque, proponemos ajustar dinámicamente los pesos de los términos basados en los clusters de historias previas y en el segundo enfoque, proponemos emplear <br>estadísticas</br> en los datos de entrenamiento para aprender el modelo de reajuste de entidades nombradas para cada clase de historias. Los resultados experimentales en dos conjuntos de datos del Consorcio de Datos Lingüísticos (LDC), TDT2 y TDT3, muestran que el modelo propuesto puede mejorar significativamente tanto la eficiencia como la precisión de la tarea de NED, en comparación con el sistema base y otros sistemas existentes. Categorías y Descriptores de Asignaturas H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información; H.4.2 [Aplicaciones de Sistemas de Información]: Tipos de Sistemas de Apoyo a Decisiones. Términos generales Algoritmos, Rendimiento, Experimentación 1. El programa de Detección y Seguimiento de Temas (TDT) tiene como objetivo desarrollar técnicas que puedan organizar, buscar y estructurar de manera efectiva materiales de texto de noticias de una variedad de agencias de noticias y medios de difusión [1]. La Detección de Eventos Nuevos (DEN) es una de las cinco tareas en TDT. Es la tarea de identificación en línea del informe más temprano para cada tema tan pronto como ese informe llegue en la secuencia de documentos. Un tema se define como un evento o actividad seminal, junto con eventos y actividades directamente relacionados [2]. Un evento se define como algo (no trivial) que sucede en un lugar y en un momento determinados [3]. Por ejemplo, cuando una bomba explota en un edificio, la explosión es el evento seminal que desencadena el tema, y otras historias sobre el mismo tema serían aquellas que discuten los esfuerzos de salvamento, la búsqueda de los perpetradores, arrestos y juicios, entre otros. La información útil de noticias suele estar enterrada en una masa de datos generados a diario. Por lo tanto, los sistemas NED son muy útiles para las personas que necesitan detectar información novedosa de un flujo de noticias en tiempo real. Estas necesidades de la vida real a menudo se presentan en ámbitos como los mercados financieros, el análisis de noticias y la recopilación de inteligencia. En la mayoría de los sistemas NED de última generación, cada noticia disponible se compara con todas las noticias previamente recibidas. Si todas las similitudes entre ellos no superan un umbral, entonces la historia desencadena un nuevo evento. Por lo general, se presentan en forma de similitud de coseno o métrica de similitud de Hellinger. El problema central de NED es identificar si dos historias tratan sobre el mismo tema. Obviamente, estos sistemas no pueden aprovechar la información del tema. Además, no es aceptable en aplicaciones reales debido a la gran cantidad de cálculos requeridos en el proceso de NED. Otros sistemas organizan historias anteriores en grupos (cada grupo corresponde a un tema), y la nueva historia se compara con los grupos anteriores en lugar de con historias. Esta forma puede reducir significativamente los tiempos de comparación. Sin embargo, se ha demostrado que este método es menos preciso [4, 5]. Esto se debe a que a veces las historias dentro de un tema se alejan mucho entre sí, lo que podría resultar en una baja similitud entre una historia y su tema. Por otro lado, algunos sistemas NED propuestos intentaron mejorar la precisión al hacer un mejor uso de las entidades nombradas [10, 11, 12, 13]. Sin embargo, ninguno de los sistemas ha considerado que los términos de diferentes tipos (por ejemplo, Los sustantivos, verbos o nombres de personas tienen diferentes efectos en diferentes clases de historias para determinar si dos historias tratan sobre el mismo tema. Por ejemplo, los nombres de los candidatos electorales (Nombre de la persona) son muy importantes para las historias de la clase de elecciones; las ubicaciones (Nombre de la ubicación) donde ocurrieron los accidentes son importantes para las historias de la clase de accidentes. Por lo tanto, en NED, todavía existen los siguientes tres problemas por investigar: (1) ¿Cómo acelerar el procedimiento de detección sin disminuir la precisión de la detección? (2) ¿Cómo aprovechar mejor la información de los grupos (temas) para mejorar la precisión? (3) ¿Cómo obtener una mejor representación de historias de noticias mediante una mejor comprensión de las entidades nombradas? Impulsados por estos problemas, hemos propuesto tres enfoques en este documento. (1) Para hacer que el procedimiento de detección sea más rápido, proponemos un nuevo procedimiento de NED basado en un árbol de indexación de noticias creado dinámicamente. El índice de historias en árbol se crea ensamblando historias similares para formar grupos de noticias en diferentes jerarquías según sus valores de similitud. Las comparaciones entre la historia actual y los grupos anteriores podrían ayudar a encontrar la historia más similar en menos tiempo de comparación. El nuevo procedimiento puede reducir la cantidad de veces de comparación sin afectar la precisión. Utilizamos los grupos del primer piso en el árbol de indexación como temas de noticias, en los cuales los pesos de los términos se ajustan dinámicamente según la distribución de términos en los grupos. En este enfoque, se utiliza adecuadamente la información de los grupos (temas), por lo que se evita el problema de la descentralización del tema. Basándonos en observaciones de las <br>estadísticas</br> obtenidas de los datos de entrenamiento, encontramos que los términos de diferentes tipos (por ejemplo, Los sustantivos y verbos tienen diferentes efectos para diferentes clases de historias al determinar si dos historias tratan sobre el mismo tema. Y proponemos utilizar <br>estadísticas</br> para optimizar los pesos de los términos de diferentes tipos en una historia de acuerdo a la clase de noticias a la que pertenece la historia. En el conjunto de datos TDT3, el nuevo modelo NED solo utiliza un 14.9% en comparación con los tiempos del modelo básico, mientras que su costo normalizado mínimo es de 0.5012, lo que es 0.0797 mejor que el modelo básico, y también mejor que cualquier otro resultado previamente reportado para este conjunto de datos [8, 13]. El resto del documento está organizado de la siguiente manera. Comenzamos este artículo resumiendo el trabajo previo en NED en la sección 2. La sección 3 presenta el modelo básico para NED que la mayoría de los sistemas actuales utilizan. La sección 4 describe nuestro nuevo procedimiento de detección basado en el índice de árbol de noticias. En la sección 5, se proponen dos métodos de reponderación de términos para mejorar la precisión de NED. La sección 6 presenta nuestros datos experimentales y métricas de evaluación. Finalmente concluimos con los resultados experimentales en la Sección 7, y las conclusiones y trabajo futuro en la Sección 8. TRABAJO RELACIONADO Papka et al. propusieron el agrupamiento de un solo paso en NED [6]. Cuando se encontraba una nueva historia, esta era procesada inmediatamente para extraer características de términos y se construía una representación de consulta del contenido de la historia. Luego se comparó con todas las consultas anteriores. Si el documento no generaba ninguna consulta al exceder un umbral, se marcaba como un nuevo evento. Lam et al construyen representaciones de consultas previas de grupos de historias, cada una de las cuales corresponde a un tema [7]. De esta manera se realizan comparaciones entre historias y grupos. En los últimos años, la mayoría de los trabajos se han centrado en proponer mejores métodos para la comparación de historias y la representación de documentos. Brants et al. [8] extendieron un modelo básico incremental de TF-IDF para incluir modelos específicos de fuente, normalización de puntajes de similitud basada en promedios específicos de documentos, normalización de puntajes de similitud basada en promedios específicos de pares de fuentes, reponderación de términos basada en frecuencias de eventos inversas y segmentación de documentos. Se mostraron buenas mejoras en los puntos de referencia de TDT. Stokes et al. [9] utilizaron una combinación de evidencia de dos representaciones distintas del contenido de un documento. Una de las representaciones era el vector de texto libre habitual, la otra hacía uso de cadenas léxicas (creadas utilizando WordNet) para construir otro vector de términos. Luego, las dos representaciones se combinan de forma lineal. Se logró un aumento marginal en la efectividad cuando se utilizó la representación combinada. Algunos esfuerzos se han realizado sobre cómo utilizar entidades nombradas para mejorar la desambiguación de entidades nombradas (NED). Yang et al. asignaron un peso cuatro veces mayor a las entidades con nombres de ubicación que a otros términos y entidades con nombres [10]. El grupo de investigación DOREMI combinó similitudes semánticas de nombres de personas, nombres de lugares y tiempo junto con similitud textual [11][12]. El grupo de investigación de UMass [13] dividió la representación del documento en dos partes: entidades nombradas y entidades no nombradas. Y se descubrió que algunas clases de noticias podían lograr un mejor rendimiento utilizando la representación de entidades nombradas, mientras que otras clases de noticias podían lograr un mejor rendimiento utilizando la representación sin entidades nombradas. Tanto [10] como [13] utilizaron la técnica de categorización de texto para clasificar previamente las noticias. En [13] las noticias se clasifican automáticamente al principio, y luego se prueban las sensibilidades de los nombres y términos que no son nombres para NED para cada clase. En [10], se eliminan los términos frecuentes de cada clase de la representación del documento. Por ejemplo, la palabra \"elección\" no ayuda a identificar diferentes elecciones. En su trabajo, no se investiga la efectividad de diferentes tipos de nombres (o términos con diferentes categorías gramaticales) para NED en diferentes clases de noticias. Utilizamos análisis estadístico para revelar el hecho y utilizarlo para mejorar el rendimiento de NED. 3. MODELO BÁSICO En esta sección, presentamos el modelo básico de Detección de Nuevos Eventos que es similar a lo que la mayoría de los sistemas actuales aplican. Luego, proponemos nuestro nuevo modelo ampliando el modelo básico. Los sistemas de detección de eventos nuevos utilizan flujos de noticias como entrada, en los cuales las historias están estrictamente ordenadas por tiempo. Solo están disponibles las historias recibidas previamente al tratar con la historia actual. La salida es una decisión sobre si la historia actual es un evento nuevo o no y la confianza de la decisión. Por lo general, un modelo NED consta de tres partes: representación de la historia, cálculo de similitud y procedimiento de detección. 3.1 Representación de la historia Se necesita preprocesamiento antes de generar la representación de la historia. Para el preprocesamiento, tokenizamos palabras, reconocemos abreviaturas, normalizamos abreviaturas, agregamos etiquetas de partes del discurso, eliminamos palabras vacías incluidas en la lista de paradas utilizada en InQuery [14], reemplazamos palabras con sus raíces utilizando el algoritmo K-stem[15], y luego generamos un vector de palabras para cada noticia. Utilizamos el modelo TF-IDF incremental para el cálculo del peso de los términos [4]. En un modelo TF-IDF, la frecuencia de término en un documento de noticias se pondera por la frecuencia inversa del documento, la cual se genera a partir del corpus de entrenamiento. Cuando surge un nuevo término en el proceso de prueba, hay dos soluciones: simplemente ignorar el nuevo término o establecer df del término como una constante pequeña (por ejemplo, df = 1). El nuevo término recibe un peso demasiado bajo en la primera solución (0) y un peso demasiado alto en la segunda solución. En el modelo TF-IDF incremental, las frecuencias de documentos se actualizan dinámicamente en cada paso de tiempo t: 1( ) ( ) ( )t t D tdf w df w df w−= + (1) donde Dt representa el conjunto de noticias recibidas en el tiempo t, y dfDt(w) significa el número de documentos en los que aparece el término w, y dft(w) significa el número total de documentos en los que aparece el término w antes del tiempo t. En este trabajo, cada ventana de tiempo incluye 50 noticias. Así, cada historia d recibida en t se representa de la siguiente manera: 1 2{ ( , , ), ( , , ),..., ( , , )}nd peso d t w peso d t w peso d t w→ donde n significa el número de términos distintos en la historia d, y ( , , )peso d t w significa el peso del término w en la historia d en el tiempo t: log( ( , ) 1) log(( 1) /( ( ) 0.5)) ( , , ) log( ( , ) 1) log(( 1) /( ( ) 0.5)) t t t t w d tf d w N df w peso d t w tf d w N df w ∈ + + + = + + +∑ (2) donde Nt significa el número total de historias de noticias antes del tiempo t, y tf(d,w) significa cuántas veces aparece el término w en la historia de noticias d. 3.2 Cálculo de Similitud Utilizamos la distancia de Hellinger para el cálculo de la similitud entre dos historias, para dos historias d y d en el tiempo t, su similitud se define de la siguiente manera: , ( , , ) ( , , ) * ( , , ) w d d sim d d t peso d t w peso d t w ∈ = ∑ (3) 3.3 Procedimiento de Detección Para cada historia d recibida en el paso de tiempo t, el valor ( ) ( ) ( ) ( ( , , )) tiempo d tiempo d n d max sim d d t < = (4) es un puntaje utilizado para determinar si d es una historia sobre un nuevo tema y al mismo tiempo es una indicación de la confianza en nuestra decisión [8]. tiempo(d) significa el tiempo de publicación de la historia d. Si el puntaje supera el umbral θ new, entonces existe un documento suficientemente similar, por lo tanto, d es una historia antigua, de lo contrario, no hay un documento previo suficientemente similar, por lo tanto, d es una historia nueva. 4. Nuevo procedimiento NED Los sistemas NED tradicionales se pueden clasificar en dos tipos principales en cuanto al procedimiento de detección: (1) tipo S-S, en el que la historia en cuestión se compara con cada historia recibida previamente, y se utiliza la mayor similitud para determinar si la historia actual trata sobre un evento nuevo; (2) tipo S-C, en el que la historia en cuestión se compara con todos los clusters anteriores, cada uno de los cuales representa un tema, y se utiliza la mayor similitud para la decisión final sobre la historia actual. Si la similitud más alta supera el umbral θ nuevo, entonces es una historia antigua y se coloca en el clúster más similar; de lo contrario, es una historia nueva y se crea un nuevo clúster. Trabajos anteriores muestran que la primera forma es más precisa que la segunda [4][5]. Dado que a veces las historias dentro de un tema se alejan mucho entre sí, una historia puede tener una similitud muy baja con su tema. Por lo tanto, utilizar similitudes entre historias para determinar una nueva historia es mejor que utilizar similitudes entre historias y grupos. Sin embargo, el primer método requiere mucho más tiempo de comparación, lo que significa que es menos eficiente. Proponemos un nuevo procedimiento de detección que utiliza comparaciones con clusters anteriores para ayudar a encontrar la historia más similar en menos tiempo de comparación, y la decisión final sobre el nuevo evento se toma de acuerdo con la historia más similar. Por lo tanto, podemos obtener tanto la precisión de los métodos de tipo S-S como la eficiencia de los métodos de tipo S-C. El nuevo procedimiento crea un árbol de indexación de noticias dinámicamente, en el cual se agrupan historias similares para formar una jerarquía de clusters. Indexamos historias similares juntas por su ancestro común (un nodo de clúster). Historias diferentes están indexadas en diferentes grupos. Cuando una historia está llegando, utilizamos comparaciones entre la historia actual y los grupos jerárquicos anteriores para ayudar a encontrar la historia más similar que es útil para la toma de decisiones sobre nuevos eventos. Después de que se tome la decisión sobre el nuevo evento, la historia actual se inserta en el árbol de indexación para la detección siguiente. El índice de noticias en forma de árbol se define formalmente de la siguiente manera: S-Tree = {r, NC, NS, E}, donde r es la raíz de S-Tree, NC es el conjunto de todos los nodos de clúster, NS es el conjunto de todos los nodos de historia, y E es el conjunto de todos los bordes en S-Tree. Definimos un conjunto de restricciones para un S-Árbol: ⅰ. , es un nodo no terminal en el árbolC i i N i∀ ∈ → ⅱ. , es un nodo terminal en el árbolS i i N i∀ ∈ → ⅲ. , el grado de salida de es al menos 2C i i N i∀ ∈ → ⅳ. , se representa como el centroide de sus descendientesC i i iN∀ ∈ → Para una noticia di, el procedimiento de comparación y el procedimiento de inserción basados en el árbol de indexación se definen de la siguiente manera. Un ejemplo se muestra en la Figura 1 y la Figura 2. Figura 1. Procedimiento de comparación Figura 2. Procedimiento de comparación de inserción: Paso 1: comparar di con todos los nodos hijos directos de r y seleccionar λ nodos con las similitudes más altas, por ejemplo, C1 2 y C1 3 en la Figura 1. Paso 2: para cada nodo seleccionado en el paso anterior, por ejemplo. C1 2, compara di con todos sus nodos hijos directos y selecciona λ nodos con las similitudes más altas, por ejemplo. C2 2 y d8. Repetir el paso 2 para todos los nodos no terminales. Paso 3: registrar el nodo terminal con la mayor similitud a di, por ejemplo, s5, y el valor de similitud (0.20). Insertando di en el S-árbol con r como raíz: Encuentra el nodo n que es hijo directo de r en el camino desde r hasta el nodo terminal con la similitud más alta s, por ejemplo. C1 2. Si s es menor que θ init + (h-1)δ, entonces agregar di al árbol como un hijo directo de r. De lo contrario, si n es un nodo terminal, crear un nodo de clúster en lugar de n, y agregar tanto n como di como sus hijos directos; si n es un nodo no terminal, repetir este procedimiento e insertar di en el subárbol con n como raíz de forma recursiva. Aquí h es la longitud entre n y la raíz del árbol S. Cuanto más similares sean las historias en un grupo, mejor representará el grupo a las historias en él. Por lo tanto, no imponemos restricciones en la altura máxima de los árboles y el grado de un nodo. Por lo tanto, no podemos dar la complejidad de este procedimiento basado en árboles de indexación. Pero proporcionaremos el número de veces de comparación necesarias por el nuevo procedimiento en nuestros experimentos en la sección 7.5. En esta sección, se proponen dos métodos de reponderación de términos para mejorar la precisión de NED. En el primer método, se explora una nueva forma de utilizar de manera más efectiva la información de los grupos (temas). El segundo encuentra una mejor manera de utilizar entidades nombradas basadas en la clasificación de noticias. 5.1 Reajuste de términos basado en la distancia de distribución TF-IDF es el modelo más prevalente utilizado en sistemas de recuperación de información. La idea básica es que cuanto menos documentos aparezca un término, más importante es el término en la discriminación de documentos (relevantes o no relevantes para una consulta que contenga el término). Sin embargo, en el dominio de TDT, necesitamos discriminar documentos en función de los temas en lugar de las consultas. Intuitivamente, el uso de vectores de clúster (tema) para comparar con historias de noticias posteriores debería tener un mejor rendimiento que el uso de vectores de historias. Desafortunadamente, los resultados experimentales no respaldan esta intuición [4][5]. Basándonos en la observación de los datos, encontramos que la razón es que un tema de noticias generalmente contiene muchos eventos directa o indirectamente relacionados, y todos ellos tienen sus propios subtemas que suelen ser diferentes entre sí. Tomemos el tema descrito en la sección 1 como ejemplo, eventos como la explosión y el rescate tienen similitudes muy bajas con eventos sobre juicios criminales, por lo tanto, las historias sobre juicios tendrían una baja similitud con el vector de tema construido en base a sus eventos previos. Esta sección se centra en cómo hacer un uso efectivo de la información del tema y al mismo tiempo evitar el problema de la descentralización del contenido. Al principio, clasificamos los términos en 5 clases para ayudar en el análisis de las necesidades del modelo modificado: Clase de términos A: términos que ocurren con frecuencia en todo el corpus, por ejemplo, año y personas. Los términos de esta clase deben recibir pesos bajos porque no ayudan mucho en la discriminación de temas. Clase B de términos: términos que ocurren con frecuencia dentro de una categoría de noticias, por ejemplo, elección, tormenta. Son útiles para distinguir dos historias en diferentes categorías de noticias. Sin embargo, no pueden proporcionar información para determinar si dos historias tratan sobre el mismo tema o temas diferentes. En otras palabras, los términos elección y tormenta no son útiles para diferenciar dos campañas electorales y dos desastres de tormenta. Por lo tanto, los términos de esta clase deberían asignarse pesos más bajos. Clase de término C: términos que ocurren con frecuencia en un tema y raramente en otros temas, por ejemplo, el nombre de un avión accidentado, el nombre de un huracán específico. Las noticias que pertenecen a diferentes temas rara vez tienen términos superpuestos en esta clase. Cuanto más frecuentemente aparezca un término en un tema, más importante es el término para una historia perteneciente al tema, por lo tanto, el término debería tener un peso más alto. Clase de término D: términos que aparecen en un tema de manera exclusiva, pero no con frecuencia. Por ejemplo, el nombre de un bombero que se desempeñó muy bien en una acción de salvamento, que puede aparecer en solo dos o tres historias pero nunca ha aparecido en otros temas. Los términos de este tipo deberían recibir más peso que en el modelo TF-IDF. Sin embargo, dado que no son populares en el tema, no es apropiado asignarles pesos demasiado altos. Clase de término E: términos con baja frecuencia en documentos y que aparecen en diferentes temas. Los términos de esta clase deberían recibir pesos más bajos. Ahora analizamos si el modelo TF-IDF puede asignar pesos adecuados a las cinco clases de términos. Obviamente, los términos de la clase A tienen un peso bajo en el modelo TF-IDF, lo cual es acorde con el requisito descrito anteriormente. En el modelo TF-IDF, los términos de la clase B dependen altamente del número de historias en una clase de noticias. El modelo TF-IDF no puede proporcionar pesos bajos si la historia que contiene el término pertenece a una clase de noticias relativamente pequeña. Para un término de clase C, cuanto más frecuentemente aparezca en un tema, menos peso le da el modelo TFIDF. Esto entra en conflicto directo con el requisito de términos en la clase C. Para los términos de la clase D, el modelo TF-IDF les asigna pesos altos de manera correcta. Pero para los términos de la clase E, el modelo TF-IDF asigna pesos altos a ellos que no son conformes con el requisito de pesos bajos. En resumen, los términos de clase B, C, E no pueden ser ponderados adecuadamente en el modelo TF-IDF. Por lo tanto, proponemos un modelo modificado para resolver este problema. Cuando θ init y θ new están configurados de cerca, asumimos que la mayoría de las historias en un clúster de primer nivel (un nodo hijo directo del nodo raíz) están en el mismo tema. Por lo tanto, utilizamos un clúster de primer nivel para capturar la distribución de términos (df para todos los términos dentro del clúster) dentro del tema de forma dinámica. La divergencia de Kullback-Leibler de la distribución de términos en un clúster de primer nivel y el conjunto completo de historias se utiliza para ajustar los pesos de los términos: ( , , ) * (1 * ( || )) ( , , ) ( , , ) * (1 * ( || )) cw tw cw tw w d D peso d t w KL P P peso d t w peso d t w KL P P γ γ ∈ + = +∑ (5) donde ( ) ( ) ( ) ( ) 1,cw cw c c c c df w df w p y p y N N = = − (6) ( ) ( ) ( ) ( ) 1,t t tw tw t t df w df w p y p y N N = = − (7) donde dfc(w) es el número de documentos que contienen el término w dentro del clúster C, y Nc es el número de documentos en el clúster C, y Nt es el número total de documentos que llegan antes del paso de tiempo t. γ es un parámetro constante, actualmente configurado manualmente en 3. La divergencia de Kullback-Leibler se define de la siguiente manera [17]: La idea básica es que, para una historia en un tema, cuanto más ocurre un término dentro del tema y menos ocurre en otros temas, se le deben asignar pesos más altos. Obviamente, el modelo modificado puede cumplir con todos los requisitos de las cinco clases de términos mencionadas anteriormente. 5.2 Reajuste de Términos Basado en el Tipo de Término y la Clase de Historia. Trabajos anteriores encontraron que algunas clases de historias de noticias podrían lograr mejoras significativas al dar un peso adicional a las entidades nombradas. Pero encontramos que los términos de diferentes tipos deben recibir una cantidad diferente de peso adicional para diferentes clases de noticias. Utilizamos open-NLP1 para reconocer tipos de entidades nombradas y etiquetas de partes del discurso para términos que aparecen en noticias. Los tipos de entidades nombradas incluyen nombre de persona, nombre de organización, nombre de ubicación, fecha, hora, dinero y porcentaje, y se seleccionan cinco partes del discurso: ninguno (NN), verbo (VB), adjetivo (JJ), adverbio (RB) y número cardinal (CD). El análisis estadístico muestra tipos de términos discriminativos a nivel de tema para diferentes clases de historias. Por conveniencia, los tipos de entidad nombrada y etiquetas de parte de la oración se llaman uniformemente tipo de término en las secciones siguientes. Determinar si dos historias tratan sobre el mismo tema es un componente básico para la tarea de NED. Por lo tanto, al principio usamos la estadística χ² para calcular las correlaciones entre términos y temas. Para un término t y un tema T, se deriva una tabla de contingencia: Tabla 1. Una tabla de contingencia de 2×2 con el número de documentos pertenecientes al tema T y no pertenecientes al tema T incluye t A B no incluye t C D. La estadística χ² para un término específico t con respecto al tema T se define como [16]: χ² = ( ) ( ) * ( ) ( ) * ( ) * ( ) * ( ) w T A B C D AD CB A C B D A B C D + + + − + + + + (9). Los temas de noticias para la tarea TDT se clasifican además en 11 reglas de interpretaciones (ROIs) 2. El ROI se puede ver como una clase de historias de nivel superior. El promedio de correlación entre un tipo de término y un ROI de tema se calcula como: 2 avg 2 ( , )( ( , ) )k m m km kT R w P w TP R p w T R P χ χ ∈ ∈ ∑ ∑（ , ）= 1 1 k=1…K, m=1…M (10) donde K es el número de tipos de términos (fijado constantemente en 12 en el artículo). M es el número de nuevas clases (ROIs, establecido en el conjunto 11 del artículo). Pk representa el conjunto de todos los términos de tipo k, y Rm representa el conjunto de todos los temas de la clase m, p(t,T) significa la probabilidad de que t ocurra en el tema T. Debido a limitaciones de espacio, solo se enumeran partes de los tipos de términos (9 tipos de términos) y partes de las clases de noticias (8 clases) en la tabla 2 con los valores promedio de correlación entre ellos. Las <br>estadísticas</br> se derivan de datos etiquetados en el corpus TDT2. (Los resultados en la tabla 2 ya están normalizados para mayor comodidad en la comparación). Las <br>estadísticas</br> en la tabla 2 indican la utilidad de diferentes tipos de términos en la discriminación de temas con respecto a diferentes clases de noticias. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "training data": {
            "translated_key": "datos de entrenamiento",
            "is_in_text": true,
            "original_annotated_sentences": [
                "New Event Detection Based on Indexing-tree and Named Entity Zhang Kuo Tsinghua University Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Tsinghua University Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Tsinghua University Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn ABSTRACT New Event Detection (NED) aims at detecting from one or multiple streams of news stories that which one is reported on a new event (i.e.",
                "not reported previously).",
                "With the overwhelming volume of news available today, there is an increasing need for a NED system which is able to detect new events more efficiently and accurately.",
                "In this paper we propose a new NED model to speed up the NED task by using news indexing-tree dynamically.",
                "Moreover, based on the observation that terms of different types have different effects for NED task, two term reweighting approaches are proposed to improve NED accuracy.",
                "In the first approach, we propose to adjust term weights dynamically based on previous story clusters and in the second approach, we propose to employ statistics on <br>training data</br> to learn the named entity reweighting model for each class of stories.",
                "Experimental results on two Linguistic Data Consortium (LDC) datasets TDT2 and TDT3 show that the proposed model can improve both efficiency and accuracy of NED task significantly, compared to the baseline system and other existing systems.",
                "Categories and Subject Descriptors H.3.3 [Information Systems]: Information Search and Retrieval; H.4.2 [Information Systems Applications]: Types of Systemsdecision support.",
                "General Terms Algorithms, Performance, Experimentation 1.",
                "INTRODUCTION Topic Detection and Tracking (TDT) program aims to develop techniques which can effectively organize, search and structure news text materials from a variety of newswire and broadcast media [1].",
                "New Event Detection (NED) is one of the five tasks in TDT.",
                "It is the task of online identification of the earliest report for each topic as soon as that report arrives in the sequence of documents.",
                "A Topic is defined as a seminal event or activity, along with directly related events and activities [2].",
                "An Event is defined as something (non-trivial) happening in a certain place at a certain time [3].",
                "For instance, when a bomb explodes in a building, the exploding is the seminal event that triggers the topic, and other stories on the same topic would be those discussing salvaging efforts, the search for perpetrators, arrests and trial and so on.",
                "Useful news information is usually buried in a mass of data generated everyday.",
                "Therefore, NED systems are very useful for people who need to detect novel information from real-time news stream.",
                "These real-life needs often occur in domains like financial markets, news analysis, and intelligence gathering.",
                "In most of state-of-the-art (currently) NED systems, each news story on hand is compared to all the previous received stories.",
                "If all the similarities between them do not exceed a threshold, then the story triggers a new event.",
                "They are usually in the form of cosine similarity or Hellinger similarity metric.",
                "The core problem of NED is to identify whether two stories are on the same topic.",
                "Obviously, these systems cannot take advantage of topic information.",
                "Further more, it is not acceptable in real applications because of the large amount of computation required in the NED process.",
                "Other systems organize previous stories into clusters (each cluster corresponds to a topic), and new story is compared to the previous clusters instead of stories.",
                "This manner can reduce comparing times significantly.",
                "Nevertheless, it has been proved that this manner is less accurate [4, 5].",
                "This is because sometimes stories within a topic drift far away from each other, which could lead low similarity between a story and its topic.",
                "On the other hand, some proposed NED systems tried to improve accuracy by making better use of named entities [10, 11, 12, 13].",
                "However, none of the systems have considered that terms of different types (e.g.",
                "Noun, Verb or Person name) have different effects for different classes of stories in determining whether two stories are on the same topic.",
                "For example, the names of election candidates (Person name) are very important for stories of election class; the locations (Location name) where accidents happened are important for stories of accidents class.",
                "So, in NED, there still exist following three problems to be investigated: (1) How to speed up the detection procedure while do not decrease the detection accuracy? (2) How to make good use of cluster (topic) information to improve accuracy? (3) How to obtain better news story representation by better understanding of named entities.",
                "Driven by these problems, we have proposed three approaches in this paper. (1)To make the detection procedure faster, we propose a new NED procedure based on news indexing-tree created dynamically.",
                "Story indexing-tree is created by assembling similar stories together to form news clusters in different hierarchies according to their values of similarity.",
                "Comparisons between current story and previous clusters could help find the most similar story in less comparing times.",
                "The new procedure can reduce the amount of comparing times without hurting accuracy. (2)We use the clusters of the first floor in the indexing-tree as news topics, in which term weights are adjusted dynamically according to term distribution in the clusters.",
                "In this approach, cluster (topic) information is used properly, so the problem of theme decentralization is avoided. (3)Based on observations on the statistics obtained from <br>training data</br>, we found that terms of different types (e.g.",
                "Noun and Verb) have different effects for different classes of stories in determining whether two stories are on the same topic.",
                "And we propose to use statistics to optimize the weights of the terms of different types in a story according to the news class that the story belongs to.",
                "On TDT3 dataset, the new NED model just uses 14.9% comparing times of the basic model, while its minimum normalized cost is 0.5012, which is 0.0797 better than the basic model, and also better than any other results previously reported for this dataset [8, 13].",
                "The rest of the paper is organized as follows.",
                "We start off this paper by summarizing the previous work in NED in section 2.",
                "Section 3 presents the basic model for NED that most current systems use.",
                "Section 4 describes our new detection procedure based on news indexing-tree.",
                "In section 5, two term reweighting methods are proposed to improve NED accuracy.",
                "Section 6 gives our experimental data and evaluation metrics.",
                "We finally wrap up with the experimental results in Section 7, and the conclusions and future work in Section 8. 2.",
                "RELATED WORK Papka et al. proposed Single-Pass clustering on NED [6].",
                "When a new story was encountered, it was processed immediately to extract term features and a query representation of the storys content is built up.",
                "Then it was compared with all the previous queries.",
                "If the document did not trigger any queries by exceeding a threshold, it was marked as a new event.",
                "Lam et al build up previous query representations of story clusters, each of which corresponds to a topic [7].",
                "In this manner comparisons happen between stories and clusters.",
                "Recent years, most work focus on proposing better methods on comparison of stories and document representation.",
                "Brants et al. [8] extended a basic incremental TF-IDF model to include sourcespecific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, term reweighting based on inverse event frequencies, and segmentation of documents.",
                "Good improvements on TDT bench-marks were shown.",
                "Stokes et al. [9] utilized a combination of evidence from two distinct representations of a documents content.",
                "One of the representations was the usual free text vector, the other made use of lexical chains (created using WordNet) to build another term vector.",
                "Then the two representations are combined in a linear fashion.",
                "A marginal increase in effectiveness was achieved when the combined representation was used.",
                "Some efforts have been done on how to utilize named entities to improve NED.",
                "Yang et al. gave location named entities four times weight than other terms and named entities [10].",
                "DOREMI research group combined semantic similarities of person names, location names and time together with textual similarity [11][12].",
                "UMass [13] research group split document representation into two parts: named entities and non-named entities.",
                "And it was found that some classes of news could achieve better performance using named entity representation, while some other classes of news could achieve better performance using non-named entity representation.",
                "Both [10] and [13] used text categorization technique to classify news stories in advance.",
                "In [13] news stories are classified automatically at first, and then test sensitivities of names and non-name terms for NED for each class.",
                "In [10] frequent terms for each class are removed from document representation.",
                "For example, word election does not help identify different elections.",
                "In their work, effectiveness of different kinds of names (or terms with different POS) for NED in different news classes are not investigated.",
                "We use statistical analysis to reveal the fact and use it to improve NED performance. 3.",
                "BASIC MODEL In this section, we present the basic New Event Detection model which is similar to what most current systems apply.",
                "Then, we propose our new model by extending the basic model.",
                "New Event Detection systems use news story stream as input, in which stories are strictly time-ordered.",
                "Only previously received stories are available when dealing with current story.",
                "The output is a decision for whether the current story is on a new event or not and the confidence of the decision.",
                "Usually, a NED model consists of three parts: story representation, similarity calculation and detection procedure. 3.1 Story Representation Preprocessing is needed before generating story representation.",
                "For preprocessing, we tokenize words, recognize abbreviations, normalize abbreviations, add part-of-speech tags, remove stopwords included in the stop list used in InQuery [14], replace words with their stems using K-stem algorithm[15], and then generate word vector for each news story.",
                "We use incremental TF-IDF model for term weight calculation [4].",
                "In a TF-IDF model, term frequency in a news document is weighted by the inverse document frequency, which is generated from training corpus.",
                "When a new term occurs in testing process, there are two solutions: simply ignore the new term or set df of the term as a small const (e.g. df = 1).",
                "The new term receives too low weight in the first solution (0) and too high weight in the second solution.",
                "In incremental TF-IDF model, document frequencies are updated dynamically in each time step t: 1( ) ( ) ( )t t D tdf w df w df w−= + (1) where Dt represents news story set received in time t, and dfDt(w) means the number of documents that term w occurs in, and dft(w) means the total number of documents that term w occurs in before time t. In this work, each time window includes 50 news stories.",
                "Thus, each story d received in t is represented as follows: 1 2{ ( , , ), ( , , ),..., ( , , )}nd weight d t w weight d t w weight d t w→ where n means the number of distinct terms in story d, and ( , , )weight d t w means the weight of term w in story d at time t: log( ( , ) 1) log(( 1) /( ( ) 0.5)) ( , , ) log( ( , ) 1) log(( 1) /( ( ) 0.5)) t t t t w d tf d w N df w weight d t w tf d w N df w ∈ + + + = + + +∑ (2) where Nt means the total number of news stories before time t, and tf(d,w) means how many times term w occurs in news story d. 3.2 Similarity Calculation We use Hellinger distance for the calculation of similarity between two stories, for two stories d and d at time t, their similarity is defined as follows: , ( , , ) ( , , ) * ( , , ) w d d sim d d t weight d t w weight d t w ∈ = ∑ (3) 3.3 Detection Procedure For each story d received in time step t, the value ( ) ( ) ( ) ( ( , , )) time d time d n d max sim d d t < = (4) is a score used to determine whether d is a story about a new topic and at the same time is an indication of the confidence in our decision [8]. time(d) means the publication time of story d. If the score exceeds the thresholdθ new, then there exists a sufficiently similar document, thus d is a old story, otherwise, there is no sufficiently similar previous document, thus d is an new story. 4.",
                "New NED Procedure Traditional NED systems can be classified into two main types on the aspect of detection procedure: (1) S-S type, in which the story on hand is compared to each story received previously, and use the highest similarity to determine whether current story is about a new event; (2) S-C type, in which the story on hand is compared to all previous clusters each of which representing a topic, and the highest similarity is used for final decision for current story.",
                "If the highest similarity exceeds thresholdθ new, then it is an old story, and put it into the most similar cluster; otherwise it is a new story and create a new cluster.",
                "Previous work show that the first manner is more accurate than the second one [4][5].",
                "Since sometimes stories within a topic drift far away from each other, a story may have very low similarity with its topic.",
                "So using similarities between stories for determining new story is better than using similarities between story and clusters.",
                "Nevertheless, the first manner needs much more comparing times which means the first manner is low efficient.",
                "We propose a new detection procedure which uses comparisons with previous clusters to help find the most similar story in less comparing times, and the final new event decision is made according to the most similar story.",
                "Therefore, we can get both the accuracy of S-S type methods and the efficiency of S-C type methods.",
                "The new procedure creates a news indexing-tree dynamically, in which similar stories are put together to form a hierarchy of clusters.",
                "We index similar stories together by their common ancestor (a cluster node).",
                "Dissimilar stories are indexed in different clusters.",
                "When a story is coming, we use comparisons between the current story and previous hierarchical clusters to help find the most similar story which is useful for new event decision.",
                "After the new event decision is made, the current story is inserted to the indexing-tree for the following detection.",
                "The news indexing-tree is defined formally as follows: S-Tree = {r, NC , NS , E} where r is the root of S-Tree, NC is the set of all cluster nodes, NS is the set of all story nodes, and E is the set of all edges in S-Tree.",
                "We define a set of constraints for a S-Tree: ⅰ . , is an non-terminal node in the treeC i i N i∀ ∈ → ⅱ . , is a terminal node in the treeS i i N i∀ ∈ → ⅲ . , out degree of is at least 2C i i N i∀ ∈ → ⅳ . , is represented as the centroid of its desendantsC i i iN∀ ∈ → For a news story di, the comparison procedure and inserting procedure based on indexing-tree are defined as follows.",
                "An example is shown by Figure 1 and Figure 2.",
                "Figure 1.",
                "Comparison procedure Figure 2.",
                "Inserting procedure Comparison procedure: Step 1: compare di to all the direct child nodes of r and select λ nodes with highest similarities, e.g., C1 2 and C1 3 in Figure 1.",
                "Step 2: for each selected node in the last step, e.g.",
                "C1 2, compare di to all its direct child nodes, and select λ nodes with highest similarities, e.g.",
                "C2 2 and d8.",
                "Repeat step 2 for all non-terminal nodes.",
                "Step 3: record the terminal node with the highest similarty to di, e.g. s5, and the similarity value (0.20).",
                "Inserting di to the S-tree with r as root: Find the node n which is direct child of r in the path from r to the terminal node with highest similarity s, e.g.",
                "C1 2.",
                "If s is smaller than θ init+(h-1)δ , then add di to the tree as a direct child of r. Otherwise, if n is a terminal node, then create a cluster node instead of n, and add both n and di as its direct children; if n is an non-terminal node, then repeat this procedure and insert di to the sub-tree with n as root recursively.",
                "Here h is the length between n and the root of S-tree.",
                "The more the stories in a cluster similar to each other, the better the cluster represents the stories in it.",
                "Hence we add no constraints on the maximum of trees height and degree of a node.",
                "Therefore, we cannot give the complexity of this indexing-tree based procedure.",
                "But we will give the number of comparing times needed by the new procedure in our experiments in section7. 5.",
                "Term Reweighting Methods In this section, two term reweighting methods are proposed to improve NED accuracy.",
                "In the first method, a new way is explored for better using of cluster (topic) information.",
                "The second one finds a better way to make use of named entities based on news classification. 5.1 Term Reweighting Based on Distribution Distance TF-IDF is the most prevalent model used in information retrieval systems.",
                "The basic idea is that the fewer documents a term appears in, the more important the term is in discrimination of documents (relevant or not relevant to a query containing the term).",
                "Nevertheless, in TDT domain, we need to discriminate documents with regard to topics rather than queries.",
                "Intuitively, using cluster (topic) vectors to compare with subsequent news stories should outperform using story vectors.",
                "Unfortunately, the experimental results do not support this intuition [4][5].",
                "Based on observation on data, we find the reason is that a news topic usually contains many directly or indirectly related events, while they all have their own sub-subjects which are usually different with each other.",
                "Take the topic described in section 1 as an example, events like the explosion and salvage have very low similarities with events about criminal trial, therefore stories about trial would have low similarity with the topic vector built on its previous events.",
                "This section focuses on how to effectively make use of topic information and at the same time avoid the problem of content decentralization.",
                "At first, we classify terms into 5 classes to help analysis the needs of the modified model: Term class A: terms that occur frequently in the whole corpus, e.g., year and people.",
                "Terms of this class should be given low weights because they do not help much for topic discrimination.",
                "Term class B: terms that occur frequently within a news category, e.g., election, storm.",
                "They are useful to distinguish two stories in different news categories.",
                "However, they cannot provide information to determine whether two stories are on the same or different topics.",
                "In another words, term election and term storm are not helpful in differentiate two election campaigns and two storm disasters.",
                "Therefore, terms of this class should be assigned lower weights.",
                "Term class C: terms that occur frequently in a topic, and infrequently in other topics, e.g., the name of a crash plane, the name of a specific hurricane.",
                "News stories that belong to different topics rarely have overlap terms in this class.",
                "The more frequently a term appears in a topic, the more important the term is for a story belonging to the topic, therefore the term should be set higher weight.",
                "Term class D: terms that appear in a topic exclusively, but not frequently.",
                "For example, the name of a fireman who did very well in a salvage action, which may appears in only two or three stories but never appeared in other topics.",
                "Terms of this type should receive more weights than in TF-IDF model.",
                "However, since they are not popular in the topic, it is not appropriate to give them too high weights.",
                "Term class E: terms with low document frequency, and appear in different topics.",
                "Terms of this class should receive lower weights.",
                "Now we analyze whether TF-IDF model can give proper weights to the five classes of terms.",
                "Obviously, terms of class A are lowly weighted in TF-IDF model, which is conformable with the requirement described above.",
                "In TF-IDF model, terms of class B are highly dependant with the number of stories in a news class.",
                "TF-IDF model cannot provide low weights if the story containing the term belongs to a relative small news class.",
                "For a term of class C, the more frequently it appears in a topic, the less weight TFIDF model gives to it.",
                "This strongly conflicts with the requirement of terms in class C. For terms of class D, TF-IDF model gives them high weights correctly.",
                "But for terms of class E, TF-IDF model gives high weights to them which are not conformable with the requirement of low weights.",
                "To sum up, terms of class B, C, E cannot be properly weighted in TF-IDF model.",
                "So, we propose a modified model to resolve this problem.",
                "When θ init andθ new are set closely, we assume that most of the stories in a first-level cluster (a direct child node of root node) are on the same topic.",
                "Therefore, we make use of a first-level cluster to capture term distribution (df for all the terms within the cluster) within the topic dynamically.",
                "KL divergence of term distribution in a first-level cluster and the whole story set is used to adjust term weights:  ( , , ) * (1 * ( || )) ( , , ) ( , , ) * (1 * ( || )) cw tw cw tw w d D weight d t w KL P P weight d t w weight d t w KL P P γ γ ∈ + = +∑ (5) where ( ) ( ) ( ) ( ) 1,cw cw c c c c df w df w p y p y N N = = − (6) ( ) ( ) ( ) ( ) 1,t t tw tw t t df w df w p y p y N N = = − (7) where dfc(w) is the number of documents containing term w within cluster C, and Nc is the number of documents in cluster C, and Nt is the total number of documents that arrive before time step t. γ is a const parameter, now is manually set 3.",
                "KL divergence is defined as follows [17]: ( ) ( || ) ( ) log ( )x p x KL P Q p x q x = ∑ (8) The basic idea is: for a story in a topic, the more a term occurs within the topic, and the less it occurs in other topics, it should be assigned higher weights.",
                "Obviously, modified model can meet all the requirements of the five term classes listed above. 5.2 Term Reweighting Based on Term Type and Story Class Previous work found that some classes of news stories could achieve good improvements by giving extra weight to named entities.",
                "But we find that terms of different types should be given different amount of extra weight for different classes of news stories.",
                "We use open-NLP1 to recognize named entity types and part-ofspeech tags for terms that appear in news stories.",
                "Named entity types include person name, organization name, location name, date, time, money and percentage, and five POSs are selected: none (NN), verb (VB), adjective (JJ), adverb (RB) and cardinal number (CD).",
                "Statistical analysis shows topic-level discriminative terms types for different classes of stories.",
                "For the sake of convenience, named entity type and part-of-speech tags are uniformly called term type in subsequent sections.",
                "Determining whether two stories are about the same topic is a basic component for NED task.",
                "So at first we use 2 χ statistic to compute correlations between terms and topics.",
                "For a term t and a topic T, a contingence table is derived: Table 1.",
                "A 2×2 Contingence Table Doc Number belong to topic T not belong to topic T include t A B not include t C D The 2 χ statistic for a specific term t with respect to topic T is defined to be [16]: 2 2 ( , ) ( ) * ( ) ( ) * ( ) * ( ) * ( ) w T A B C D AD CB A C B D A B C D χ = + + + − + + + + (9) News topics for the TDT task are further classified into 11 rules of interpretations (ROIs) 2 .",
                "The ROI can be seen as a higher level class of stories.",
                "The average correlation between a term type and a topic ROI is computed as: 2 avg 2 ( , )( ( , ) )k m m km kT R w P w TP R p w T R P χ χ ∈ ∈ ∑ ∑（ , ）= 1 1 k=1…K, m=1…M (10) where K is the number of term types (set 12 constantly in the paper).",
                "M is the number news classes (ROIs, set 11 in the paper).",
                "Pk represents the set of all terms of type k, and Rm represents the set of all topics of class m, p(t,T) means the probability that t occurs in topic T. Because of limitation of space, only parts of the term types (9 term types) and parts of news classes (8 classes) are listed in table 2 with the average correlation values between them.",
                "The statistics is derived from labeled data in TDT2 corpus. (Results in table 2 are already normalized for convenience in comparison.)",
                "The statistics in table 2 indicates the usefulness of different term types in topic discrimination with respect to different news classes.",
                "We can see that, location name is the most useful term type for three news classes: Natural Disasters, Violence or War, Finances.",
                "And for three other categories Elections, Legal/Criminal Cases, Science and Discovery, person name is the most discriminative term type.",
                "For Scandals/Hearings, date is the most important information for topic discrimination.",
                "In addition, Legal/Criminal Cases and Finance topics have higher correlation with money terms, while Science and Discovery have higher correlation with percentage terms.",
                "Non-name terms are more stable for different classes. 1 . http://opennlp.sourceforge.net/ 2 . http://projects.ldc.upenn.edu/TDT3/Guide/label.html From the analysis of table 2, it is reasonable to adjust term weight according to their term type and the news class the story belongs to.",
                "New term weights are reweighted as follows: ( ) ( ) ( ) ( ) ( , , ) * ( , , ) ( , , ) * class d D type w T class d D type w w d weight d t w weight d t w weight d t w α α ∈ = ∑ (11) where type(w) represents the type of term w, and class(d) represents the class of story d, c kα is reweighting parameter for news class c and term type k. In the work, we just simply use statistics in table 2 as the reweighting parameters.",
                "Even thought using the statistics directly may not the best choice, we do not discuss how to automatically obtain the best parameters.",
                "We will try to use machine learning techniques to obtain the best parameters in the future work.",
                "In the work, we use BoosTexter [20] to classify all stories into one of the 11 ROIs.",
                "BoosTexter is a boosting based machine learning program, which creates a series of simple rules for building a classifier for text or attribute-value data.",
                "We use term weight generated using TF-IDF model as feature for story classification.",
                "We trained the model on the 12000 judged English stories in TDT2, and classify the rest of the stories in TDT2 and all stories in TDT3.",
                "Classification results are used for term reweighting in formula (11).",
                "Since the class labels of topic-off stories are not given in TDT datasets, we cannot give the classification accuracy here.",
                "Thus we do not discuss the effects of classification accuracy to NED performance in the paper. 6.",
                "EXPERIMENTAL SETUP 6.1 Datasets We used two LDC [18] datasets TDT2 and TDT3 for our experiments.",
                "TDT2 contains news stories from January to June 1998.",
                "It contains around 54,000 stories from sources like ABC, Associated Press, CNN, New York Times, Public Radio International, Voice of America etc.",
                "Only English stories in the collection were considered.",
                "TDT3 contains approximately 31,000 English stories collected from October to December 1998.",
                "In addition to the sources used in TDT2, it also contains stories from NBC and MSNBC TV broadcasts.",
                "We used transcribed versions of the TV and radio broadcasts besides textual news.",
                "TDT2 dataset is labeled with about 100 topics, and approximately 12,000 English stories belong to at least one of these topics.",
                "TDT3 dataset is labeled with about 120 topics, and approximately 8000 English stories belong to at least one of these topics.",
                "All the topics are classified into 11 Rules of Interpretation: (1)Elections, (2)Scandals/Hearings, (3)Legal/Criminal Cases, (4)Natural Disasters, (5)Accidents, (6)Ongoing Violence or War, (7)Science and Discovery News, (8)Finance, (9)New Law, (10)Sports News, (11)MISC.",
                "News. 6.2 Evaluation Metric TDT uses a cost function CDet that combines the probabilities of missing a new story and a false alarm [19]: * * * *Det Miss Miss Target FA FA NontargetC C P P C P P= + (12) Table 2.",
                "Average correlation between term types and news classes where CMiss means the cost of missing a new story, PMiss means the probability of missing a new story, and PTarget means the probability of seeing a new story in the data; CFA means the cost of a false alarm, PFA means the probability of a false alarm, and PNontarget means the probability of seeing an old story.",
                "The cost CDet is normalized such that a perfect system scores 0 and a trivial system, which is the better one of mark all stories as new or old, scores 1: ( ( * , * ) ) Det Det Miss Target FA Nontarget C Norm C min C P C P = (13) New event detection system gives two outputs for each story.",
                "The first part is yes or no indicating whether the story triggers a new event or not.",
                "The second part is a score indicating confidence of the first decision.",
                "Confidence scores can be used to plot DET curve, i.e., curves that plot false alarm vs. miss probabilities.",
                "Minimum normalized cost can be determined if optimal threshold on the score were chosen. 7.",
                "EXPERIMENTAL RESULTS 7.1 Main Results To test the approaches proposed in the model, we implemented and tested five systems: System-1: this system is used as baseline.",
                "It is implemented based on the basic model described in section 3, i.e., using incremental TF-IDF model to generate term weights, and using Hellinger distance to compute document similarity.",
                "Similarity score normalization is also employed [8].",
                "S-S detection procedure is used.",
                "System-2: this system is the same as system-1 except that S-C detection procedure is used.",
                "System-3: this system is the same as system-1 except that it uses the new detection procedure which is based on indexing-tree.",
                "System-4: implemented based on the approach presented in section 5.1, i.e., terms are reweighted according to the distance between term distributions in a cluster and all stories.",
                "The new detection procedure is used.",
                "System-5: implemented based on the approach presented in section 5.2, i.e., terms of different types are reweighted according to news class using trained parameters.",
                "The new detection procedure is used.",
                "The following are some other NED systems: System-6: [21] for each pair of stories, it computes three similarity values for named entity, non-named entity and all terms respectively.",
                "And employ Support Vector Machine to predict new or old using the similarity values as features.",
                "System-7: [8] it extended a basic incremental TF-IDF model to include source-specific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, etc.",
                "System-8: [13] it split document representation into two parts: named entities and non-named entities, and choose one effective part for each news class.",
                "Table 3 and table 4 show topic-weighted normalized costs and comparing times on TDT2 and TDT3 datasets respectively.",
                "Since no heldout data set for fine-tuning the threshold θ new was available for experiments on TDT2, we only report minimum normalized costs for our systems in table 3.",
                "System-5 outperforms all other systems including system-6, and it performs only 2.78e+8 comparing times in detection procedure which is only 13.4% of system-1.",
                "Table 3.",
                "NED results on TDT2 Systems Min Norm(CDet) Cmp times System-1 0.5749 2.08e+9 System-2① 0.6673 3.77e+8 System-3② 0.5765 2.81e+8 System-4② 0.5431 2.99e+8 System-5② 0.5089 2.78e+8 System-6 0.5300 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 When evaluating on the normalized costs on TDT3, we use the optimal thresholds obtained from TDT2 data set for all systems.",
                "System-2 reduces comparing times to 1.29e+9 which is just 18.3% of system-1, but at the same time it also gets a deteriorated minimum normalized cost which is 0.0499 higher than system-1.",
                "System-3 uses the new detection procedure based on news indexing-tree.",
                "It requires even less comparing times than system-2.",
                "This is because story-story comparisons usually yield greater similarities than story-cluster ones, so stories tend to be combined Location Person Date Organization Money Percentage NN JJ CD Elections 0.37 1 0.04 0.58 0.08 0.03 0.32 0.13 0.1 Scandals/Hearings 0.66 0.62 0.28 1 0.11 0.02 0.27 0.13 0.05 Legal/Criminal Cases 0.48 1 0.02 0.62 0.15 0 0.22 0.24 0.09 Natural Disasters 1 0.27 0 0.04 0.04 0 0.25 0.04 0.02 Violence or War 1 0.36 0.02 0.14 0.02 0.04 0.21 0.11 0.02 Science and Discovery 0.11 1 0.01 0.22 0.08 0.12 0.19 0.08 0.03 Finances 1 0.45 0.04 0.98 0.13 0.02 0.29 0.06 0.05 Sports 0.16 0.27 0.01 1 0.02 0 0.11 0.03 0.01 together in system-3.",
                "And system-3 is basically equivalent to system-1 in accuracy results.",
                "System-4 adjusts term weights based on the distance of term distributions between the whole corpus and cluster story set, yielding a good improvement by 0.0468 compared to system-1.",
                "The best system (system-5) has a minimum normalized cost 0.5012, which is 0.0797 better than system-1, and also better than any other results previously reported for this dataset [8, 13].",
                "Further more, system-5 only needs 1.05e+8 comparing times which is 14.9% of system-1.",
                "Table 4.",
                "NED results on TDT3 Systems Norm(CDet) Min Norm(CDet) Cmp times System-1 0.6159 0.5809 7.04e+8 System-2① 0.6493 0.6308 1.29e+8 System-3② 0.6197 0.5868 1.03e+8 System-4② 0.5601 0.5341 1.03e+8 System-5② 0.5413 0.5012 1.05e+8 System-7 -- 0.5783 -System-8 -- 0.5229 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 Figure5 shows the five DET curves for our systems on data set TDT3.",
                "System-5 achieves the minimum cost at a false alarm rate of 0.0157 and a miss rate of 0.4310.",
                "We can observe that System4 and System-5 obtain lower miss probability at regions of low false alarm probabilities.",
                "The hypothesis is that, more weight value is transferred to key terms of topics from non-key terms.",
                "Similarity score between two stories belonging to different topics are lower than before, because their overlapping terms are usually not key terms of their topics. 7.2 Parameter selection for indexing-tree detection Figure 3 shows the minimum normalized costs obtained by system-3 on TDT3 using different parameters.",
                "Theθ init parameter is tested on six values spanning from 0.03 to 0.18.",
                "And the λ parameter is tested on four values 1, 2, 3 and 4.",
                "We can see that, whenθ init is set to 0.12, which is the closest one toθ new, the costs are lower than others.",
                "This is easy to explain, because when stories belonging to the same topic are put in a cluster, it is more reasonable for the cluster to represent the stories in it.",
                "When parameter λ is set to 3 or 4, the costs are better than other cases, but there is no much difference between 3 and 4. 0 0.05 0.1 0.15 0.2 1 2 3 4 0.5 0.6 0.7 0.8 0.9 1 θ-initλ MinCost 0.6 0.65 0.7 0.75 0.8 0.85 0.9 Figure 3.",
                "Min Cost on TDT3 (δ =0.15) 0 0.05 0.1 0.15 0.2 1 2 3 4 0 0.5 1 1.5 2 2.5 x 10 8 θ-init λ Comparingtimes 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 8 Figure 4.",
                "Comparing times on TDT3 (δ =0.15) Figure 4 gives the comparing times used by system-3 on TDT3 with the same parameters as figure 3.",
                "The comparing times are strongly dependent onθ init.",
                "Because the greaterθ init is, the less stories combined together, the more comparing times are needed for new event decision.",
                "So we useθ init =0.13,λ =3,δ =0.15 for system-3, 4, and 5.",
                "In this parameter setting, we can get both low minimum normalized costs and less comparing times. 8.",
                "CONCLUSION We have proposed a news indexing-tree based detection procedure in our model.",
                "It reduces comparing times to about one seventh of traditional method without hurting NED accuracy.",
                "We also have presented two extensions to the basic TF-IDF model.",
                "The first extension is made by adjust term weights based on term distributions between the whole corpus and a cluster story set.",
                "And the second extension to basic TF-IDF model is better use of term types (named entities types and part-of-speed) according to news categories.",
                "Our experimental results on TDT2 and TDT3 datasets show that both of the two extensions contribute significantly to improvement in accuracy.",
                "We did not consider news time information as a clue for NED task, since most of the topics last for a long time and TDT data sets only span for a relative short period (no more than 6 months).",
                "For the future work, we want to collect news set which span for a longer period from internet, and integrate time information in NED task.",
                "Since topic is a relative coarse-grained news cluster, we also want to refine cluster granularity to event-level, and identify different events and their relations within a topic.",
                "Acknowledgments This work is supported by the National Natural Science Foundation of China under Grant No. 90604025.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsor. 9.",
                "REFERENCES [1] http://www.nist.gov/speech/tests/tdt/index.htm [2] In Topic Detection and Tracking.",
                "Event-based Information Organization.",
                "Kluwer Academic Publishers, 2002. .01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90 1 2 5 10 20 40 60 80 90 False Alarm Probability (in %) MissProbability(in%) SYSTEM1 Topic Weighted Curve SYSTEM1 Min Norm(Cost) SYSTEM2 Topic Weighted Curve SYSTEM2 Min Norm(Cost) SYSTEM3 Topic Weighted Curve SYSTEM3 Min Norm(Cost) SYSTEM4 Topic Weighted Curve SYSTEM4 Min Norm(Cost) SYSTEM5 Topic Weighted Curve SYSTEM5 Min Norm(Cost) Random Performance Figure 5.",
                "DET curves on TDT3 [3] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B.T.",
                "Archibald, and X. Liu.",
                "Learning Approaches for Detecting and Tracking News Events.",
                "In IEEE Intelligent Systems Special Issue on Applications of Intelligent Information Retrieval, volume 14 (4), 1999, 32-43. [4] Y. Yang, T. Pierce, and J. Carbonell.",
                "A Study on Retrospective and On-line Event Detection.",
                "In Proceedings of SIGIR-98, Melbourne, Australia, 1998, 28-36. [5] J. Allan, V. Lavrenko, D. Malin, and R. Swan.",
                "Detections, Bounds, and Timelines: Umass and tdt-3.",
                "In Proceedings of Topic Detection and Tracking Workshop (TDT-3), Vienna, VA, 2000, 167-174. [6] R. Papka and J. Allan.",
                "On-line New Event Detection Using Single Pass Clustering TITLE2:.",
                "Technical Report UM-CS1998-021, 1998. [7] W. Lam, H. Meng, K. Wong, and J.",
                "Yen.",
                "Using Contextual Analysis for News Event Detection.",
                "International Journal on Intelligent Systems, 2001, 525-546. [8] B. Thorsten, C. Francine, and F. Ayman.",
                "A System for New Event Detection.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2003, 330-337. [9] S. Nicola and C. Joe.",
                "Combining Semantic and Syntactic Document Classifiers to Improve First Story Detection.",
                "In Proceedings of the 24th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topicconditioned Novelty Detection.",
                "In Proceedings of the 8th ACM SIGKDD International Conference, ACM Press. 2002, 688-693. [11] M. Juha, A.M. Helena, and S. Marko.",
                "Applying Semantic Classes in Event Detection and Tracking.",
                "In Proceedings of International Conference on Natural Language Processing (ICON 2002), 2002, pages 175-183. [12] M. Juha, A.M. Helena, and S. Marko.",
                "Simple Semantics in Topic Detection and Tracking.",
                "Information Retrieval, 7(3-4): 2004, 347-368. [13] K. Giridhar and J. Allan.",
                "Text Classification and Named Entities for New Event Detection.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2004, 297-304. [14] J. P. Callan, W. B. Croft, and S. M. Harding.",
                "The INQUERY Retrieval System.",
                "In Proceedings of DEXA-92, 3rd International Conference on Database and Expert Systems Applications, 1992, 78-83. [15] R. Krovetz.",
                "Viewing Morphology as An Inference Process.",
                "In Proceedings of ACM SIGIR93, 1993, 61-81. [16] Y. Yang and J. Pedersen.",
                "A Comparative Study on Feature Selection in Text Categorization.",
                "In J. D. H. Fisher, editor, The Fourteenth International Conference on Machine Learning (ICML97), Morgan Kaufmann, 1997, 412-420. [17] T. M. Cover, and J.A.",
                "Thomas.",
                "Elements of Information Theory.",
                "Wiley. 1991. [18] The linguistic data consortium, http://www.ldc,upenn.edu/. [19] The 2001 TDT task definition and evaluation plan, http://www.nist.gov/speech/tests/tdt/tdt2001/evalplan.htm. [20] R. E. Schapire and Y.",
                "Singer.",
                "Boostexter: A Boosting-based System for Text Categorization.",
                "In Machine Learning 39(2/3):1, Kluwer Academic Publishers, 2000, 35-168. [21] K. Giridhar and J. Allan. 2005.",
                "Using Names and Topics for New Event Detection.",
                "In Proceedings of Human Technology Conference and Conference on Empirical Methods in Natural Language, Vancouver, 2005, 121-128"
            ],
            "original_annotated_samples": [
                "In the first approach, we propose to adjust term weights dynamically based on previous story clusters and in the second approach, we propose to employ statistics on <br>training data</br> to learn the named entity reweighting model for each class of stories.",
                "In this approach, cluster (topic) information is used properly, so the problem of theme decentralization is avoided. (3)Based on observations on the statistics obtained from <br>training data</br>, we found that terms of different types (e.g."
            ],
            "translated_annotated_samples": [
                "En el primer enfoque, proponemos ajustar dinámicamente los pesos de los términos basados en los clusters de historias previas y en el segundo enfoque, proponemos emplear estadísticas en los <br>datos de entrenamiento</br> para aprender el modelo de reajuste de entidades nombradas para cada clase de historias.",
                "En este enfoque, se utiliza adecuadamente la información de los grupos (temas), por lo que se evita el problema de la descentralización del tema. Basándonos en observaciones de las estadísticas obtenidas de los <br>datos de entrenamiento</br>, encontramos que los términos de diferentes tipos (por ejemplo,"
            ],
            "translated_text": "Detección de nuevos eventos basada en árbol de indexación y entidad nombrada Zhang Kuo Universidad Tsinghua Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Universidad Tsinghua Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Universidad Tsinghua Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn RESUMEN La Detección de Nuevos Eventos (NED) tiene como objetivo detectar de una o varias corrientes de noticias cuál se informa sobre un nuevo evento (es decir, no reportado previamente). Con el abrumador volumen de noticias disponible hoy en día, hay una creciente necesidad de un sistema NED que sea capaz de detectar eventos nuevos de manera más eficiente y precisa. En este artículo proponemos un nuevo modelo de NED para acelerar la tarea de NED mediante el uso dinámico de un árbol de indexación de noticias. Además, basándose en la observación de que los términos de diferentes tipos tienen diferentes efectos para la tarea de NED, se proponen dos enfoques de reponderación de términos para mejorar la precisión de NED. En el primer enfoque, proponemos ajustar dinámicamente los pesos de los términos basados en los clusters de historias previas y en el segundo enfoque, proponemos emplear estadísticas en los <br>datos de entrenamiento</br> para aprender el modelo de reajuste de entidades nombradas para cada clase de historias. Los resultados experimentales en dos conjuntos de datos del Consorcio de Datos Lingüísticos (LDC), TDT2 y TDT3, muestran que el modelo propuesto puede mejorar significativamente tanto la eficiencia como la precisión de la tarea de NED, en comparación con el sistema base y otros sistemas existentes. Categorías y Descriptores de Asignaturas H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información; H.4.2 [Aplicaciones de Sistemas de Información]: Tipos de Sistemas de Apoyo a Decisiones. Términos generales Algoritmos, Rendimiento, Experimentación 1. El programa de Detección y Seguimiento de Temas (TDT) tiene como objetivo desarrollar técnicas que puedan organizar, buscar y estructurar de manera efectiva materiales de texto de noticias de una variedad de agencias de noticias y medios de difusión [1]. La Detección de Eventos Nuevos (DEN) es una de las cinco tareas en TDT. Es la tarea de identificación en línea del informe más temprano para cada tema tan pronto como ese informe llegue en la secuencia de documentos. Un tema se define como un evento o actividad seminal, junto con eventos y actividades directamente relacionados [2]. Un evento se define como algo (no trivial) que sucede en un lugar y en un momento determinados [3]. Por ejemplo, cuando una bomba explota en un edificio, la explosión es el evento seminal que desencadena el tema, y otras historias sobre el mismo tema serían aquellas que discuten los esfuerzos de salvamento, la búsqueda de los perpetradores, arrestos y juicios, entre otros. La información útil de noticias suele estar enterrada en una masa de datos generados a diario. Por lo tanto, los sistemas NED son muy útiles para las personas que necesitan detectar información novedosa de un flujo de noticias en tiempo real. Estas necesidades de la vida real a menudo se presentan en ámbitos como los mercados financieros, el análisis de noticias y la recopilación de inteligencia. En la mayoría de los sistemas NED de última generación, cada noticia disponible se compara con todas las noticias previamente recibidas. Si todas las similitudes entre ellos no superan un umbral, entonces la historia desencadena un nuevo evento. Por lo general, se presentan en forma de similitud de coseno o métrica de similitud de Hellinger. El problema central de NED es identificar si dos historias tratan sobre el mismo tema. Obviamente, estos sistemas no pueden aprovechar la información del tema. Además, no es aceptable en aplicaciones reales debido a la gran cantidad de cálculos requeridos en el proceso de NED. Otros sistemas organizan historias anteriores en grupos (cada grupo corresponde a un tema), y la nueva historia se compara con los grupos anteriores en lugar de con historias. Esta forma puede reducir significativamente los tiempos de comparación. Sin embargo, se ha demostrado que este método es menos preciso [4, 5]. Esto se debe a que a veces las historias dentro de un tema se alejan mucho entre sí, lo que podría resultar en una baja similitud entre una historia y su tema. Por otro lado, algunos sistemas NED propuestos intentaron mejorar la precisión al hacer un mejor uso de las entidades nombradas [10, 11, 12, 13]. Sin embargo, ninguno de los sistemas ha considerado que los términos de diferentes tipos (por ejemplo, Los sustantivos, verbos o nombres de personas tienen diferentes efectos en diferentes clases de historias para determinar si dos historias tratan sobre el mismo tema. Por ejemplo, los nombres de los candidatos electorales (Nombre de la persona) son muy importantes para las historias de la clase de elecciones; las ubicaciones (Nombre de la ubicación) donde ocurrieron los accidentes son importantes para las historias de la clase de accidentes. Por lo tanto, en NED, todavía existen los siguientes tres problemas por investigar: (1) ¿Cómo acelerar el procedimiento de detección sin disminuir la precisión de la detección? (2) ¿Cómo aprovechar mejor la información de los grupos (temas) para mejorar la precisión? (3) ¿Cómo obtener una mejor representación de historias de noticias mediante una mejor comprensión de las entidades nombradas? Impulsados por estos problemas, hemos propuesto tres enfoques en este documento. (1) Para hacer que el procedimiento de detección sea más rápido, proponemos un nuevo procedimiento de NED basado en un árbol de indexación de noticias creado dinámicamente. El índice de historias en árbol se crea ensamblando historias similares para formar grupos de noticias en diferentes jerarquías según sus valores de similitud. Las comparaciones entre la historia actual y los grupos anteriores podrían ayudar a encontrar la historia más similar en menos tiempo de comparación. El nuevo procedimiento puede reducir la cantidad de veces de comparación sin afectar la precisión. Utilizamos los grupos del primer piso en el árbol de indexación como temas de noticias, en los cuales los pesos de los términos se ajustan dinámicamente según la distribución de términos en los grupos. En este enfoque, se utiliza adecuadamente la información de los grupos (temas), por lo que se evita el problema de la descentralización del tema. Basándonos en observaciones de las estadísticas obtenidas de los <br>datos de entrenamiento</br>, encontramos que los términos de diferentes tipos (por ejemplo, Los sustantivos y verbos tienen diferentes efectos para diferentes clases de historias al determinar si dos historias tratan sobre el mismo tema. Y proponemos utilizar estadísticas para optimizar los pesos de los términos de diferentes tipos en una historia de acuerdo a la clase de noticias a la que pertenece la historia. En el conjunto de datos TDT3, el nuevo modelo NED solo utiliza un 14.9% en comparación con los tiempos del modelo básico, mientras que su costo normalizado mínimo es de 0.5012, lo que es 0.0797 mejor que el modelo básico, y también mejor que cualquier otro resultado previamente reportado para este conjunto de datos [8, 13]. El resto del documento está organizado de la siguiente manera. Comenzamos este artículo resumiendo el trabajo previo en NED en la sección 2. La sección 3 presenta el modelo básico para NED que la mayoría de los sistemas actuales utilizan. La sección 4 describe nuestro nuevo procedimiento de detección basado en el índice de árbol de noticias. En la sección 5, se proponen dos métodos de reponderación de términos para mejorar la precisión de NED. La sección 6 presenta nuestros datos experimentales y métricas de evaluación. Finalmente concluimos con los resultados experimentales en la Sección 7, y las conclusiones y trabajo futuro en la Sección 8. TRABAJO RELACIONADO Papka et al. propusieron el agrupamiento de un solo paso en NED [6]. Cuando se encontraba una nueva historia, esta era procesada inmediatamente para extraer características de términos y se construía una representación de consulta del contenido de la historia. Luego se comparó con todas las consultas anteriores. Si el documento no generaba ninguna consulta al exceder un umbral, se marcaba como un nuevo evento. Lam et al construyen representaciones de consultas previas de grupos de historias, cada una de las cuales corresponde a un tema [7]. De esta manera se realizan comparaciones entre historias y grupos. En los últimos años, la mayoría de los trabajos se han centrado en proponer mejores métodos para la comparación de historias y la representación de documentos. Brants et al. [8] extendieron un modelo básico incremental de TF-IDF para incluir modelos específicos de fuente, normalización de puntajes de similitud basada en promedios específicos de documentos, normalización de puntajes de similitud basada en promedios específicos de pares de fuentes, reponderación de términos basada en frecuencias de eventos inversas y segmentación de documentos. Se mostraron buenas mejoras en los puntos de referencia de TDT. Stokes et al. [9] utilizaron una combinación de evidencia de dos representaciones distintas del contenido de un documento. Una de las representaciones era el vector de texto libre habitual, la otra hacía uso de cadenas léxicas (creadas utilizando WordNet) para construir otro vector de términos. Luego, las dos representaciones se combinan de forma lineal. Se logró un aumento marginal en la efectividad cuando se utilizó la representación combinada. Algunos esfuerzos se han realizado sobre cómo utilizar entidades nombradas para mejorar la desambiguación de entidades nombradas (NED). Yang et al. asignaron un peso cuatro veces mayor a las entidades con nombres de ubicación que a otros términos y entidades con nombres [10]. El grupo de investigación DOREMI combinó similitudes semánticas de nombres de personas, nombres de lugares y tiempo junto con similitud textual [11][12]. El grupo de investigación de UMass [13] dividió la representación del documento en dos partes: entidades nombradas y entidades no nombradas. Y se descubrió que algunas clases de noticias podían lograr un mejor rendimiento utilizando la representación de entidades nombradas, mientras que otras clases de noticias podían lograr un mejor rendimiento utilizando la representación sin entidades nombradas. Tanto [10] como [13] utilizaron la técnica de categorización de texto para clasificar previamente las noticias. En [13] las noticias se clasifican automáticamente al principio, y luego se prueban las sensibilidades de los nombres y términos que no son nombres para NED para cada clase. En [10], se eliminan los términos frecuentes de cada clase de la representación del documento. Por ejemplo, la palabra \"elección\" no ayuda a identificar diferentes elecciones. En su trabajo, no se investiga la efectividad de diferentes tipos de nombres (o términos con diferentes categorías gramaticales) para NED en diferentes clases de noticias. Utilizamos análisis estadístico para revelar el hecho y utilizarlo para mejorar el rendimiento de NED. 3. MODELO BÁSICO En esta sección, presentamos el modelo básico de Detección de Nuevos Eventos que es similar a lo que la mayoría de los sistemas actuales aplican. Luego, proponemos nuestro nuevo modelo ampliando el modelo básico. Los sistemas de detección de eventos nuevos utilizan flujos de noticias como entrada, en los cuales las historias están estrictamente ordenadas por tiempo. Solo están disponibles las historias recibidas previamente al tratar con la historia actual. La salida es una decisión sobre si la historia actual es un evento nuevo o no y la confianza de la decisión. Por lo general, un modelo NED consta de tres partes: representación de la historia, cálculo de similitud y procedimiento de detección. 3.1 Representación de la historia Se necesita preprocesamiento antes de generar la representación de la historia. Para el preprocesamiento, tokenizamos palabras, reconocemos abreviaturas, normalizamos abreviaturas, agregamos etiquetas de partes del discurso, eliminamos palabras vacías incluidas en la lista de paradas utilizada en InQuery [14], reemplazamos palabras con sus raíces utilizando el algoritmo K-stem[15], y luego generamos un vector de palabras para cada noticia. Utilizamos el modelo TF-IDF incremental para el cálculo del peso de los términos [4]. En un modelo TF-IDF, la frecuencia de término en un documento de noticias se pondera por la frecuencia inversa del documento, la cual se genera a partir del corpus de entrenamiento. Cuando surge un nuevo término en el proceso de prueba, hay dos soluciones: simplemente ignorar el nuevo término o establecer df del término como una constante pequeña (por ejemplo, df = 1). El nuevo término recibe un peso demasiado bajo en la primera solución (0) y un peso demasiado alto en la segunda solución. En el modelo TF-IDF incremental, las frecuencias de documentos se actualizan dinámicamente en cada paso de tiempo t: 1( ) ( ) ( )t t D tdf w df w df w−= + (1) donde Dt representa el conjunto de noticias recibidas en el tiempo t, y dfDt(w) significa el número de documentos en los que aparece el término w, y dft(w) significa el número total de documentos en los que aparece el término w antes del tiempo t. En este trabajo, cada ventana de tiempo incluye 50 noticias. Así, cada historia d recibida en t se representa de la siguiente manera: 1 2{ ( , , ), ( , , ),..., ( , , )}nd peso d t w peso d t w peso d t w→ donde n significa el número de términos distintos en la historia d, y ( , , )peso d t w significa el peso del término w en la historia d en el tiempo t: log( ( , ) 1) log(( 1) /( ( ) 0.5)) ( , , ) log( ( , ) 1) log(( 1) /( ( ) 0.5)) t t t t w d tf d w N df w peso d t w tf d w N df w ∈ + + + = + + +∑ (2) donde Nt significa el número total de historias de noticias antes del tiempo t, y tf(d,w) significa cuántas veces aparece el término w en la historia de noticias d. 3.2 Cálculo de Similitud Utilizamos la distancia de Hellinger para el cálculo de la similitud entre dos historias, para dos historias d y d en el tiempo t, su similitud se define de la siguiente manera: , ( , , ) ( , , ) * ( , , ) w d d sim d d t peso d t w peso d t w ∈ = ∑ (3) 3.3 Procedimiento de Detección Para cada historia d recibida en el paso de tiempo t, el valor ( ) ( ) ( ) ( ( , , )) tiempo d tiempo d n d max sim d d t < = (4) es un puntaje utilizado para determinar si d es una historia sobre un nuevo tema y al mismo tiempo es una indicación de la confianza en nuestra decisión [8]. tiempo(d) significa el tiempo de publicación de la historia d. Si el puntaje supera el umbral θ new, entonces existe un documento suficientemente similar, por lo tanto, d es una historia antigua, de lo contrario, no hay un documento previo suficientemente similar, por lo tanto, d es una historia nueva. 4. Nuevo procedimiento NED Los sistemas NED tradicionales se pueden clasificar en dos tipos principales en cuanto al procedimiento de detección: (1) tipo S-S, en el que la historia en cuestión se compara con cada historia recibida previamente, y se utiliza la mayor similitud para determinar si la historia actual trata sobre un evento nuevo; (2) tipo S-C, en el que la historia en cuestión se compara con todos los clusters anteriores, cada uno de los cuales representa un tema, y se utiliza la mayor similitud para la decisión final sobre la historia actual. Si la similitud más alta supera el umbral θ nuevo, entonces es una historia antigua y se coloca en el clúster más similar; de lo contrario, es una historia nueva y se crea un nuevo clúster. Trabajos anteriores muestran que la primera forma es más precisa que la segunda [4][5]. Dado que a veces las historias dentro de un tema se alejan mucho entre sí, una historia puede tener una similitud muy baja con su tema. Por lo tanto, utilizar similitudes entre historias para determinar una nueva historia es mejor que utilizar similitudes entre historias y grupos. Sin embargo, el primer método requiere mucho más tiempo de comparación, lo que significa que es menos eficiente. Proponemos un nuevo procedimiento de detección que utiliza comparaciones con clusters anteriores para ayudar a encontrar la historia más similar en menos tiempo de comparación, y la decisión final sobre el nuevo evento se toma de acuerdo con la historia más similar. Por lo tanto, podemos obtener tanto la precisión de los métodos de tipo S-S como la eficiencia de los métodos de tipo S-C. El nuevo procedimiento crea un árbol de indexación de noticias dinámicamente, en el cual se agrupan historias similares para formar una jerarquía de clusters. Indexamos historias similares juntas por su ancestro común (un nodo de clúster). Historias diferentes están indexadas en diferentes grupos. Cuando una historia está llegando, utilizamos comparaciones entre la historia actual y los grupos jerárquicos anteriores para ayudar a encontrar la historia más similar que es útil para la toma de decisiones sobre nuevos eventos. Después de que se tome la decisión sobre el nuevo evento, la historia actual se inserta en el árbol de indexación para la detección siguiente. El índice de noticias en forma de árbol se define formalmente de la siguiente manera: S-Tree = {r, NC, NS, E}, donde r es la raíz de S-Tree, NC es el conjunto de todos los nodos de clúster, NS es el conjunto de todos los nodos de historia, y E es el conjunto de todos los bordes en S-Tree. Definimos un conjunto de restricciones para un S-Árbol: ⅰ. , es un nodo no terminal en el árbolC i i N i∀ ∈ → ⅱ. , es un nodo terminal en el árbolS i i N i∀ ∈ → ⅲ. , el grado de salida de es al menos 2C i i N i∀ ∈ → ⅳ. , se representa como el centroide de sus descendientesC i i iN∀ ∈ → Para una noticia di, el procedimiento de comparación y el procedimiento de inserción basados en el árbol de indexación se definen de la siguiente manera. Un ejemplo se muestra en la Figura 1 y la Figura 2. Figura 1. Procedimiento de comparación Figura 2. Procedimiento de comparación de inserción: Paso 1: comparar di con todos los nodos hijos directos de r y seleccionar λ nodos con las similitudes más altas, por ejemplo, C1 2 y C1 3 en la Figura 1. Paso 2: para cada nodo seleccionado en el paso anterior, por ejemplo. C1 2, compara di con todos sus nodos hijos directos y selecciona λ nodos con las similitudes más altas, por ejemplo. C2 2 y d8. Repetir el paso 2 para todos los nodos no terminales. Paso 3: registrar el nodo terminal con la mayor similitud a di, por ejemplo, s5, y el valor de similitud (0.20). Insertando di en el S-árbol con r como raíz: Encuentra el nodo n que es hijo directo de r en el camino desde r hasta el nodo terminal con la similitud más alta s, por ejemplo. C1 2. Si s es menor que θ init + (h-1)δ, entonces agregar di al árbol como un hijo directo de r. De lo contrario, si n es un nodo terminal, crear un nodo de clúster en lugar de n, y agregar tanto n como di como sus hijos directos; si n es un nodo no terminal, repetir este procedimiento e insertar di en el subárbol con n como raíz de forma recursiva. Aquí h es la longitud entre n y la raíz del árbol S. Cuanto más similares sean las historias en un grupo, mejor representará el grupo a las historias en él. Por lo tanto, no imponemos restricciones en la altura máxima de los árboles y el grado de un nodo. Por lo tanto, no podemos dar la complejidad de este procedimiento basado en árboles de indexación. Pero proporcionaremos el número de veces de comparación necesarias por el nuevo procedimiento en nuestros experimentos en la sección 7.5. En esta sección, se proponen dos métodos de reponderación de términos para mejorar la precisión de NED. En el primer método, se explora una nueva forma de utilizar de manera más efectiva la información de los grupos (temas). El segundo encuentra una mejor manera de utilizar entidades nombradas basadas en la clasificación de noticias. 5.1 Reajuste de términos basado en la distancia de distribución TF-IDF es el modelo más prevalente utilizado en sistemas de recuperación de información. La idea básica es que cuanto menos documentos aparezca un término, más importante es el término en la discriminación de documentos (relevantes o no relevantes para una consulta que contenga el término). Sin embargo, en el dominio de TDT, necesitamos discriminar documentos en función de los temas en lugar de las consultas. Intuitivamente, el uso de vectores de clúster (tema) para comparar con historias de noticias posteriores debería tener un mejor rendimiento que el uso de vectores de historias. Desafortunadamente, los resultados experimentales no respaldan esta intuición [4][5]. Basándonos en la observación de los datos, encontramos que la razón es que un tema de noticias generalmente contiene muchos eventos directa o indirectamente relacionados, y todos ellos tienen sus propios subtemas que suelen ser diferentes entre sí. Tomemos el tema descrito en la sección 1 como ejemplo, eventos como la explosión y el rescate tienen similitudes muy bajas con eventos sobre juicios criminales, por lo tanto, las historias sobre juicios tendrían una baja similitud con el vector de tema construido en base a sus eventos previos. Esta sección se centra en cómo hacer un uso efectivo de la información del tema y al mismo tiempo evitar el problema de la descentralización del contenido. Al principio, clasificamos los términos en 5 clases para ayudar en el análisis de las necesidades del modelo modificado: Clase de términos A: términos que ocurren con frecuencia en todo el corpus, por ejemplo, año y personas. Los términos de esta clase deben recibir pesos bajos porque no ayudan mucho en la discriminación de temas. Clase B de términos: términos que ocurren con frecuencia dentro de una categoría de noticias, por ejemplo, elección, tormenta. Son útiles para distinguir dos historias en diferentes categorías de noticias. Sin embargo, no pueden proporcionar información para determinar si dos historias tratan sobre el mismo tema o temas diferentes. En otras palabras, los términos elección y tormenta no son útiles para diferenciar dos campañas electorales y dos desastres de tormenta. Por lo tanto, los términos de esta clase deberían asignarse pesos más bajos. Clase de término C: términos que ocurren con frecuencia en un tema y raramente en otros temas, por ejemplo, el nombre de un avión accidentado, el nombre de un huracán específico. Las noticias que pertenecen a diferentes temas rara vez tienen términos superpuestos en esta clase. Cuanto más frecuentemente aparezca un término en un tema, más importante es el término para una historia perteneciente al tema, por lo tanto, el término debería tener un peso más alto. Clase de término D: términos que aparecen en un tema de manera exclusiva, pero no con frecuencia. Por ejemplo, el nombre de un bombero que se desempeñó muy bien en una acción de salvamento, que puede aparecer en solo dos o tres historias pero nunca ha aparecido en otros temas. Los términos de este tipo deberían recibir más peso que en el modelo TF-IDF. Sin embargo, dado que no son populares en el tema, no es apropiado asignarles pesos demasiado altos. Clase de término E: términos con baja frecuencia en documentos y que aparecen en diferentes temas. Los términos de esta clase deberían recibir pesos más bajos. Ahora analizamos si el modelo TF-IDF puede asignar pesos adecuados a las cinco clases de términos. Obviamente, los términos de la clase A tienen un peso bajo en el modelo TF-IDF, lo cual es acorde con el requisito descrito anteriormente. En el modelo TF-IDF, los términos de la clase B dependen altamente del número de historias en una clase de noticias. El modelo TF-IDF no puede proporcionar pesos bajos si la historia que contiene el término pertenece a una clase de noticias relativamente pequeña. Para un término de clase C, cuanto más frecuentemente aparezca en un tema, menos peso le da el modelo TFIDF. Esto entra en conflicto directo con el requisito de términos en la clase C. Para los términos de la clase D, el modelo TF-IDF les asigna pesos altos de manera correcta. Pero para los términos de la clase E, el modelo TF-IDF asigna pesos altos a ellos que no son conformes con el requisito de pesos bajos. En resumen, los términos de clase B, C, E no pueden ser ponderados adecuadamente en el modelo TF-IDF. Por lo tanto, proponemos un modelo modificado para resolver este problema. Cuando θ init y θ new están configurados de cerca, asumimos que la mayoría de las historias en un clúster de primer nivel (un nodo hijo directo del nodo raíz) están en el mismo tema. Por lo tanto, utilizamos un clúster de primer nivel para capturar la distribución de términos (df para todos los términos dentro del clúster) dentro del tema de forma dinámica. La divergencia de Kullback-Leibler de la distribución de términos en un clúster de primer nivel y el conjunto completo de historias se utiliza para ajustar los pesos de los términos: ( , , ) * (1 * ( || )) ( , , ) ( , , ) * (1 * ( || )) cw tw cw tw w d D peso d t w KL P P peso d t w peso d t w KL P P γ γ ∈ + = +∑ (5) donde ( ) ( ) ( ) ( ) 1,cw cw c c c c df w df w p y p y N N = = − (6) ( ) ( ) ( ) ( ) 1,t t tw tw t t df w df w p y p y N N = = − (7) donde dfc(w) es el número de documentos que contienen el término w dentro del clúster C, y Nc es el número de documentos en el clúster C, y Nt es el número total de documentos que llegan antes del paso de tiempo t. γ es un parámetro constante, actualmente configurado manualmente en 3. La divergencia de Kullback-Leibler se define de la siguiente manera [17]: La idea básica es que, para una historia en un tema, cuanto más ocurre un término dentro del tema y menos ocurre en otros temas, se le deben asignar pesos más altos. Obviamente, el modelo modificado puede cumplir con todos los requisitos de las cinco clases de términos mencionadas anteriormente. 5.2 Reajuste de Términos Basado en el Tipo de Término y la Clase de Historia. Trabajos anteriores encontraron que algunas clases de historias de noticias podrían lograr mejoras significativas al dar un peso adicional a las entidades nombradas. Pero encontramos que los términos de diferentes tipos deben recibir una cantidad diferente de peso adicional para diferentes clases de noticias. Utilizamos open-NLP1 para reconocer tipos de entidades nombradas y etiquetas de partes del discurso para términos que aparecen en noticias. Los tipos de entidades nombradas incluyen nombre de persona, nombre de organización, nombre de ubicación, fecha, hora, dinero y porcentaje, y se seleccionan cinco partes del discurso: ninguno (NN), verbo (VB), adjetivo (JJ), adverbio (RB) y número cardinal (CD). El análisis estadístico muestra tipos de términos discriminativos a nivel de tema para diferentes clases de historias. Por conveniencia, los tipos de entidad nombrada y etiquetas de parte de la oración se llaman uniformemente tipo de término en las secciones siguientes. Determinar si dos historias tratan sobre el mismo tema es un componente básico para la tarea de NED. Por lo tanto, al principio usamos la estadística χ² para calcular las correlaciones entre términos y temas. Para un término t y un tema T, se deriva una tabla de contingencia: Tabla 1. Una tabla de contingencia de 2×2 con el número de documentos pertenecientes al tema T y no pertenecientes al tema T incluye t A B no incluye t C D. La estadística χ² para un término específico t con respecto al tema T se define como [16]: χ² = ( ) ( ) * ( ) ( ) * ( ) * ( ) * ( ) w T A B C D AD CB A C B D A B C D + + + − + + + + (9). Los temas de noticias para la tarea TDT se clasifican además en 11 reglas de interpretaciones (ROIs) 2. El ROI se puede ver como una clase de historias de nivel superior. El promedio de correlación entre un tipo de término y un ROI de tema se calcula como: 2 avg 2 ( , )( ( , ) )k m m km kT R w P w TP R p w T R P χ χ ∈ ∈ ∑ ∑（ , ）= 1 1 k=1…K, m=1…M (10) donde K es el número de tipos de términos (fijado constantemente en 12 en el artículo). M es el número de nuevas clases (ROIs, establecido en el conjunto 11 del artículo). Pk representa el conjunto de todos los términos de tipo k, y Rm representa el conjunto de todos los temas de la clase m, p(t,T) significa la probabilidad de que t ocurra en el tema T. Debido a limitaciones de espacio, solo se enumeran partes de los tipos de términos (9 tipos de términos) y partes de las clases de noticias (8 clases) en la tabla 2 con los valores promedio de correlación entre ellos. Las estadísticas se derivan de datos etiquetados en el corpus TDT2. (Los resultados en la tabla 2 ya están normalizados para mayor comodidad en la comparación). Las estadísticas en la tabla 2 indican la utilidad de diferentes tipos de términos en la discriminación de temas con respecto a diferentes clases de noticias. Podemos ver que el nombre de la ubicación es el tipo de término más útil para tres clases de noticias: Desastres Naturales, Violencia o Guerra, Finanzas. Y para otras tres categorías: Elecciones, Casos Legales/Criminales, Ciencia y Descubrimiento, el nombre de la persona es el tipo de término más discriminativo. Para Escándalos/Audiencias, la fecha es la información más importante para la discriminación de temas. Además, los casos legales/criminales y los temas financieros tienen una correlación más alta con términos relacionados con el dinero, mientras que la ciencia y el descubrimiento tienen una correlación más alta con términos de porcentaje. Los términos no nominales son más estables para diferentes clases. 1. http://opennlp.sourceforge.net/ 2. http://projects.ldc.upenn.edu/TDT3/Guide/label.html A partir del análisis de la tabla 2, es razonable ajustar el peso de los términos según su tipo de término y la clase de noticias a la que pertenece la historia. Los nuevos pesos de los términos se recalculan de la siguiente manera: ( ) ( ) ( ) ( ) ( , , ) * ( , , ) ( , , ) * clase d D tipo w T clase d D tipo w w d peso d t w peso d t w peso d t w α α ∈ = ∑ (11) donde type(w) representa el tipo del término w, y class(d) representa la clase de la historia d, c kα es el parámetro de recalibración para la clase de noticias c y el tipo de término k. En el trabajo, simplemente utilizamos las estadísticas de la tabla 2 como parámetros de recalibración. Aunque usar las estadísticas directamente puede que no sea la mejor opción, no discutimos cómo obtener automáticamente los mejores parámetros. Intentaremos utilizar técnicas de aprendizaje automático para obtener los mejores parámetros en el trabajo futuro. En el trabajo, utilizamos BoosTexter [20] para clasificar todas las historias en uno de los 11 ROIs. BoosTexter es un programa de aprendizaje automático basado en boosting, que crea una serie de reglas simples para construir un clasificador de datos de texto o atributos-valor. Utilizamos el peso del término generado utilizando el modelo TF-IDF como característica para la clasificación de historias. Entrenamos el modelo con las 12000 historias en inglés evaluadas en TDT2, y clasificamos el resto de las historias en TDT2 y todas las historias en TDT3. Los resultados de clasificación se utilizan para la reponderación de términos en la fórmula (11). Dado que las etiquetas de clase de las historias de tema apagado no se proporcionan en los conjuntos de datos de TDT, no podemos proporcionar la precisión de clasificación aquí. Por lo tanto, no discutimos los efectos de la precisión de la clasificación en el rendimiento de NED en el artículo. 6. CONFIGURACIÓN EXPERIMENTAL 6.1 Conjuntos de datos Utilizamos dos conjuntos de datos de LDC [18], TDT2 y TDT3, para nuestros experimentos. TDT2 contiene noticias desde enero hasta junio de 1998. Contiene alrededor de 54,000 historias de fuentes como ABC, Associated Press, CNN, New York Times, Public Radio International, Voice of America, etc. Solo se consideraron historias en inglés en la colección. TDT3 contiene aproximadamente 31,000 historias en inglés recopiladas de octubre a diciembre de 1998. Además de las fuentes utilizadas en TDT2, también contiene historias de las transmisiones de televisión de NBC y MSNBC. Utilizamos versiones transcritas de las emisiones de televisión y radio además de noticias textuales. El conjunto de datos TDT2 está etiquetado con alrededor de 100 temas, y aproximadamente 12,000 historias en inglés pertenecen al menos a uno de estos temas. El conjunto de datos TDT3 está etiquetado con alrededor de 120 temas, y aproximadamente 8000 historias en inglés pertenecen al menos a uno de estos temas. Todos los temas están clasificados en 11 Reglas de Interpretación: (1) Elecciones, (2) Escándalos/Audiencias, (3) Casos Legales/Criminales, (4) Desastres Naturales, (5) Accidentes, (6) Violencia o Guerra en Curso, (7) Noticias de Ciencia y Descubrimientos, (8) Finanzas, (9) Nueva Ley, (10) Noticias Deportivas, (11) Varios. Noticias. 6.2 Métrica de evaluación TDT utiliza una función de costo CDet que combina las probabilidades de perder una nueva historia y una falsa alarma [19]: * * * *Det Miss Miss Target FA FA NontargetC C P P C P P= + (12) Tabla 2. Correlación promedio entre tipos de términos y clases de noticias donde CMiss significa el costo de perder una nueva historia, PMiss significa la probabilidad de perder una nueva historia, y PTarget significa la probabilidad de ver una nueva historia en los datos; CFA significa el costo de una falsa alarma, PFA significa la probabilidad de una falsa alarma, y PNontarget significa la probabilidad de ver una historia antigua. El costo CDet se normaliza de tal manera que un sistema perfecto obtiene una puntuación de 0 y un sistema trivial, que es el mejor de marcar todas las historias como nuevas o antiguas, obtiene una puntuación de 1: ( ( * , * ) ) Det Det Miss Target FA Nontarget C Norm C min C P C P = (13) El sistema de detección de eventos nuevos proporciona dos salidas para cada historia. La primera parte es sí o no, indicando si la historia desencadena un nuevo evento o no. La segunda parte es una puntuación que indica la confianza de la primera decisión. Los puntajes de confianza se pueden utilizar para trazar la curva DET, es decir, curvas que representan la probabilidad de falsa alarma frente a la probabilidad de omisión. El costo normalizado mínimo se puede determinar si se elige el umbral óptimo en la puntuación. RESULTADOS EXPERIMENTALES 7.1 Resultados principales Para probar los enfoques propuestos en el modelo, implementamos y probamos cinco sistemas: Sistema-1: este sistema se utiliza como referencia. Se implementa basándose en el modelo básico descrito en la sección 3, es decir, utilizando el modelo TF-IDF incremental para generar pesos de términos y utilizando la distancia de Hellinger para calcular la similitud entre documentos. La normalización del puntaje de similitud también se emplea [8]. Se utiliza el procedimiento de detección S-S. Sistema-2: este sistema es igual que el sistema-1 excepto que se utiliza el procedimiento de detección S-C. Sistema-3: este sistema es igual que el sistema-1 excepto que utiliza el nuevo procedimiento de detección que se basa en un árbol de indexación. Sistema-4: implementado basado en el enfoque presentado en la sección 5.1, es decir, los términos se vuelven a ponderar según la distancia entre las distribuciones de términos en un grupo y todas las historias. Se utiliza el nuevo procedimiento de detección. Sistema-5: implementado basado en el enfoque presentado en la sección 5.2, es decir, los términos de diferentes tipos son reponderados según la clase de noticias utilizando parámetros entrenados. El nuevo procedimiento de detección se utiliza. Los siguientes son algunos otros sistemas NED: Sistema-6: [21] para cada par de historias, calcula tres valores de similitud para entidad nombrada, entidad no nombrada y todos los términos respectivamente. Y emplear Máquina de Vectores de Soporte para predecir nuevo o antiguo utilizando los valores de similitud como características. Sistema-7: [8] extendió un modelo básico incremental de TF-IDF para incluir modelos específicos de origen, normalización de puntuaciones de similitud basada en promedios específicos de documentos, normalización de puntuaciones de similitud basada en promedios específicos de pares de fuentes, etc. Sistema-8: [13] dividió la representación del documento en dos partes: entidades nombradas y entidades no nombradas, y eligió una parte efectiva para cada clase de noticias. La tabla 3 y la tabla 4 muestran los costos normalizados ponderados por tema y los tiempos de comparación en los conjuntos de datos TDT2 y TDT3 respectivamente. Dado que no se contaba con un conjunto de datos de validación para ajustar el umbral θ nuevo en los experimentos en TDT2, solo informamos los costos normalizados mínimos para nuestros sistemas en la tabla 3. El sistema-5 supera a todos los demás sistemas, incluido el sistema-6, y realiza solo 2.78e+8 veces la comparación de tiempos en el procedimiento de detección, lo que representa solo el 13.4% del sistema-1. Tabla 3. Los resultados de NED en los sistemas TDT2 Min Norm(CDet) tiempos de comparación son: Sistema-1 0.5749 2.08e+9, Sistema-2 0.6673 3.77e+8, Sistema-3 0.5765 2.81e+8, Sistema-4 0.5431 2.99e+8, Sistema-5 0.5089 2.78e+8, Sistema-6 0.5300. Al evaluar en los costos normalizados en TDT3, utilizamos los umbrales óptimos obtenidos del conjunto de datos de TDT2 para todos los sistemas. El sistema-2 reduce los tiempos de comparación a 1.29e+9, que es solo el 18.3% del sistema-1, pero al mismo tiempo también obtiene un costo normalizado mínimo deteriorado que es 0.0499 más alto que el sistema-1. El Sistema-3 utiliza el nuevo procedimiento de detección basado en el índice de noticias en árbol. Requiere incluso menos tiempo de comparación que el sistema-2. Esto se debe a que las comparaciones de historias suelen arrojar mayores similitudes que las de grupos de historias, por lo que las historias tienden a combinarse en el sistema-3. Y el sistema-3 es básicamente equivalente al sistema-1 en los resultados de precisión. El System-4 ajusta los pesos de los términos basándose en la distancia de las distribuciones de términos entre todo el corpus y el conjunto de historias del clúster, lo que resulta en una mejora significativa de 0.0468 en comparación con el system-1. El mejor sistema (sistema-5) tiene un costo normalizado mínimo de 0.5012, lo cual es 0.0797 mejor que el sistema-1, y también mejor que cualquier otro resultado previamente reportado para este conjunto de datos [8, 13]. Además, el sistema-5 solo necesita 1.05e+8 veces de comparación, lo que equivale al 14.9% del sistema-1. Tabla 4. Los resultados de NED en los sistemas TDT3 Norm(CDet) Norma mínima(CDet) Tiempos de comparación Sistema-1 0.6159 0.5809 7.04e+8 Sistema-2① 0.6493 0.6308 1.29e+8 Sistema-3② 0.6197 0.5868 1.03e+8 Sistema-4② 0.5601 0.5341 1.03e+8 Sistema-5② 0.5413 0.5012 1.05e+8 Sistema-7 -- 0.5783 -Sistema-8 -- 0.5229 -① θ nuevo=0.13 ② θ inicial=0.13, λ =3, δ =0.15 La Figura 5 muestra las cinco curvas DET para nuestros sistemas en el conjunto de datos TDT3. El Sistema-5 logra el costo mínimo con una tasa de falsas alarmas de 0.0157 y una tasa de omisión de 0.4310. Podemos observar que System4 y System-5 obtienen una menor probabilidad de error de omisión en regiones de baja probabilidad de falsa alarma. La hipótesis es que se transfiere más valor de peso a los términos clave de los temas que a los términos no clave. La puntuación de similitud entre dos historias pertenecientes a diferentes temas es más baja que antes, porque los términos superpuestos suelen no ser términos clave de sus temas. 7.2 Selección de parámetros para la detección de árboles de indexación. La Figura 3 muestra los costos normalizados mínimos obtenidos por el sistema-3 en TDT3 utilizando diferentes parámetros. El parámetro de inicio θ se prueba en seis valores que van desde 0.03 hasta 0.18. Y el parámetro λ se prueba en cuatro valores: 1, 2, 3 y 4. Podemos ver que, cuando θ inicial se establece en 0.12, que es el más cercano a θ nuevo, los costos son más bajos que los demás. Esto es fácil de explicar, porque cuando se agrupan historias pertenecientes al mismo tema en un conjunto, es más razonable que el conjunto represente las historias en él. Cuando el parámetro λ se establece en 3 o 4, los costos son mejores que en otros casos, pero no hay mucha diferencia entre 3 y 4. 0 0.05 0.1 0.15 0.2 1 2 3 4 0.5 0.6 0.7 0.8 0.9 1 θ-initλ MinCost 0.6 0.65 0.7 0.75 0.8 0.85 0.9 Figura 3. Costo mínimo en TDT3 (δ =0.15) 0 0.05 0.1 0.15 0.2 1 2 3 4 0 0.5 1 1.5 2 2.5 x 10 8 θ-inicio λ Comparando tiempos 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 8 Figura 4. Comparando los tiempos en TDT3 (δ =0.15), la Figura 4 muestra los tiempos de comparación utilizados por el sistema-3 en TDT3 con los mismos parámetros que la Figura 3. Los tiempos de comparación dependen fuertemente de θ init. Debido a que cuanto mayor sea θ init, menos historias se combinan juntas, más veces de comparación se necesitan para la decisión de un nuevo evento. Por lo tanto, usamos θ init = 0.13, λ = 3, δ = 0.15 para los sistemas 3, 4 y 5. En esta configuración de parámetros, podemos obtener tanto costos normalizados mínimos bajos como menos tiempos de comparación. CONCLUSIÓN Hemos propuesto un procedimiento de detección basado en un árbol de indexación de noticias en nuestro modelo. Reduce los tiempos de comparación a aproximadamente una séptima parte del método tradicional sin afectar la precisión de NED. También hemos presentado dos extensiones al modelo básico TF-IDF. La primera extensión se realiza ajustando los pesos de los términos basados en las distribuciones de términos entre todo el corpus y un conjunto de historias de un clúster. Y la segunda extensión al modelo básico TF-IDF es un mejor uso de los tipos de términos (tipos de entidades nombradas y partes de la velocidad) de acuerdo con las categorías de noticias. Nuestros resultados experimentales en los conjuntos de datos TDT2 y TDT3 muestran que ambas extensiones contribuyen significativamente a la mejora en la precisión. No consideramos la información de tiempo de noticias como una pista para la tarea de NED, ya que la mayoría de los temas duran mucho tiempo y los conjuntos de datos de TDT solo abarcan un período relativamente corto (no más de 6 meses). Para el trabajo futuro, queremos recopilar un conjunto de noticias que abarque un período más largo de internet e integrar información temporal en la tarea de NED. Dado que el tema es un grupo de noticias relativamente grueso, también queremos refinar la granularidad del grupo a nivel de evento e identificar diferentes eventos y sus relaciones dentro de un tema. Agradecimientos: Este trabajo ha sido apoyado por la Fundación Nacional de Ciencias Naturales de China bajo la subvención No. 90604025. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son del autor(es) y no necesariamente reflejan las del patrocinador. REFERENCIAS [1] http://www.nist.gov/speech/tests/tdt/index.htm [2] En Detección y Seguimiento de Temas. Organización de la información basada en eventos. Kluwer Academic Publishers, 2002. .01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90 1 2 5 10 20 40 60 80 90 Probabilidad de Falsa Alarma (en %) Probabilidad de Omisión (en %) Curva Ponderada por Tema del SISTEMA1 Norma Mínima (Costo) del SISTEMA1 Curva Ponderada por Tema del SISTEMA2 Norma Mínima (Costo) del SISTEMA2 Curva Ponderada por Tema del SISTEMA3 Norma Mínima (Costo) del SISTEMA3 Curva Ponderada por Tema del SISTEMA4 Norma Mínima (Costo) del SISTEMA4 Curva Ponderada por Tema del SISTEMA5 Norma Mínima (Costo) del SISTEMA5 Rendimiento Aleatorio Figura 5. Curvas DET en TDT3 [3] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B.T. Archibald y X. Liu. Enfoques de aprendizaje para detectar y rastrear eventos de noticias. En el número especial de IEEE Intelligent Systems sobre Aplicaciones de Recuperación de Información Inteligente, volumen 14 (4), 1999, páginas 32-43. [4] Y. Yang, T. Pierce y J. Carbonell. Un estudio sobre la detección de eventos retrospectivos y en línea. En Actas de SIGIR-98, Melbourne, Australia, 1998, 28-36. [5] J. Allan, V. Lavrenko, D. Malin y R. Swan. Detecciones, límites y líneas de tiempo: Umass y tdt-3. En Actas del Taller de Detección y Seguimiento de Temas (TDT-3), Viena, VA, 2000, 167-174. [6] R. Papka y J. Allan. Detección de nuevos eventos en línea utilizando agrupamiento de un solo paso TÍTULO2:. Informe técnico UM-CS1998-021, 1998. [7] W. Lam, H. Meng, K. Wong y J. I'm sorry, but \"Yen\" is not a sentence. Can you please provide a sentence for me to translate into Spanish? Utilizando el Análisis Contextual para la Detección de Eventos de Noticias. Revista Internacional de Sistemas Inteligentes, 2001, 525-546. [8] B. Thorsten, C. Francine y F. Ayman. Un sistema para la detección de nuevos eventos. En Actas de la 26ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2003, 330-337. [9] S. Nicola y C. Joe. Combinando clasificadores semánticos y sintácticos de documentos para mejorar la detección de primeras noticias. En Actas de la 24ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell, and C. Jin. \n\nACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell y C. Jin. Detección de novedades condicionada al tema. En Actas de la 8ª Conferencia Internacional de ACM SIGKDD, ACM Press. 2002, 688-693. [11] M. Juha, A.M. Helena y S. Marko. Aplicando Clases Semánticas en la Detección y Seguimiento de Eventos. En Actas de la Conferencia Internacional sobre Procesamiento del Lenguaje Natural (ICON 2002), 2002, páginas 175-183. [12] M. Juha, A.M. Helena y S. Marko. Semántica simple en la detección y seguimiento de temas. Recuperación de información, 7(3-4): 2004, 347-368. [13] K. Giridhar y J. Allan. Clasificación de texto y entidades nombradas para la detección de nuevos eventos. En Actas de la 27ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2004, 297-304. [14] J. P. Callan, W. B. Croft, y S. M. Harding. El Sistema de Recuperación INQUERY. En Actas de DEXA-92, 3ra Conferencia Internacional sobre Aplicaciones de Bases de Datos y Sistemas Expertos, 1992, 78-83. [15] R. Krovetz. Viendo la morfología como un proceso de inferencia. En Actas de ACM SIGIR93, 1993, 61-81. [16] Y. Yang y J. Pedersen. Un estudio comparativo sobre la selección de características en la categorización de textos. En J. D. H. Fisher, editor, La Decimocuarta Conferencia Internacional sobre Aprendizaje Automático (ICML97), Morgan Kaufmann, 1997, 412-420. [17] T. M. Cover y J. A. I'm sorry, but \"Thomas\" is not a sentence. Can you provide more context or a complete sentence for me to translate into Spanish? Elementos de la teoría de la información. Wiley. 1991. [18] El consorcio de datos lingüísticos, http://www.ldc,upenn.edu/. [19] La definición de la tarea TDT 2001 y el plan de evaluación, http://www.nist.gov/speech/tests/tdt/tdt2001/evalplan.htm. [20] R. E. Schapire y Y. Cantante. Boostexter: Un sistema basado en Boosting para la categorización de texto. En Machine Learning 39(2/3):1, Kluwer Academic Publishers, 2000, 35-168. [21] K. Giridhar y J. Allan. 2005. Utilizando nombres y temas para la detección de nuevos eventos. En Actas de la Conferencia de Tecnología Humana y la Conferencia sobre Métodos Empíricos en Lenguaje Natural, Vancouver, 2005, 121-128 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "named entity reweighting mode": {
            "translated_key": "modelo de reajuste de entidades nombradas",
            "is_in_text": true,
            "original_annotated_sentences": [
                "New Event Detection Based on Indexing-tree and Named Entity Zhang Kuo Tsinghua University Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Tsinghua University Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Tsinghua University Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn ABSTRACT New Event Detection (NED) aims at detecting from one or multiple streams of news stories that which one is reported on a new event (i.e.",
                "not reported previously).",
                "With the overwhelming volume of news available today, there is an increasing need for a NED system which is able to detect new events more efficiently and accurately.",
                "In this paper we propose a new NED model to speed up the NED task by using news indexing-tree dynamically.",
                "Moreover, based on the observation that terms of different types have different effects for NED task, two term reweighting approaches are proposed to improve NED accuracy.",
                "In the first approach, we propose to adjust term weights dynamically based on previous story clusters and in the second approach, we propose to employ statistics on training data to learn the <br>named entity reweighting mode</br>l for each class of stories.",
                "Experimental results on two Linguistic Data Consortium (LDC) datasets TDT2 and TDT3 show that the proposed model can improve both efficiency and accuracy of NED task significantly, compared to the baseline system and other existing systems.",
                "Categories and Subject Descriptors H.3.3 [Information Systems]: Information Search and Retrieval; H.4.2 [Information Systems Applications]: Types of Systemsdecision support.",
                "General Terms Algorithms, Performance, Experimentation 1.",
                "INTRODUCTION Topic Detection and Tracking (TDT) program aims to develop techniques which can effectively organize, search and structure news text materials from a variety of newswire and broadcast media [1].",
                "New Event Detection (NED) is one of the five tasks in TDT.",
                "It is the task of online identification of the earliest report for each topic as soon as that report arrives in the sequence of documents.",
                "A Topic is defined as a seminal event or activity, along with directly related events and activities [2].",
                "An Event is defined as something (non-trivial) happening in a certain place at a certain time [3].",
                "For instance, when a bomb explodes in a building, the exploding is the seminal event that triggers the topic, and other stories on the same topic would be those discussing salvaging efforts, the search for perpetrators, arrests and trial and so on.",
                "Useful news information is usually buried in a mass of data generated everyday.",
                "Therefore, NED systems are very useful for people who need to detect novel information from real-time news stream.",
                "These real-life needs often occur in domains like financial markets, news analysis, and intelligence gathering.",
                "In most of state-of-the-art (currently) NED systems, each news story on hand is compared to all the previous received stories.",
                "If all the similarities between them do not exceed a threshold, then the story triggers a new event.",
                "They are usually in the form of cosine similarity or Hellinger similarity metric.",
                "The core problem of NED is to identify whether two stories are on the same topic.",
                "Obviously, these systems cannot take advantage of topic information.",
                "Further more, it is not acceptable in real applications because of the large amount of computation required in the NED process.",
                "Other systems organize previous stories into clusters (each cluster corresponds to a topic), and new story is compared to the previous clusters instead of stories.",
                "This manner can reduce comparing times significantly.",
                "Nevertheless, it has been proved that this manner is less accurate [4, 5].",
                "This is because sometimes stories within a topic drift far away from each other, which could lead low similarity between a story and its topic.",
                "On the other hand, some proposed NED systems tried to improve accuracy by making better use of named entities [10, 11, 12, 13].",
                "However, none of the systems have considered that terms of different types (e.g.",
                "Noun, Verb or Person name) have different effects for different classes of stories in determining whether two stories are on the same topic.",
                "For example, the names of election candidates (Person name) are very important for stories of election class; the locations (Location name) where accidents happened are important for stories of accidents class.",
                "So, in NED, there still exist following three problems to be investigated: (1) How to speed up the detection procedure while do not decrease the detection accuracy? (2) How to make good use of cluster (topic) information to improve accuracy? (3) How to obtain better news story representation by better understanding of named entities.",
                "Driven by these problems, we have proposed three approaches in this paper. (1)To make the detection procedure faster, we propose a new NED procedure based on news indexing-tree created dynamically.",
                "Story indexing-tree is created by assembling similar stories together to form news clusters in different hierarchies according to their values of similarity.",
                "Comparisons between current story and previous clusters could help find the most similar story in less comparing times.",
                "The new procedure can reduce the amount of comparing times without hurting accuracy. (2)We use the clusters of the first floor in the indexing-tree as news topics, in which term weights are adjusted dynamically according to term distribution in the clusters.",
                "In this approach, cluster (topic) information is used properly, so the problem of theme decentralization is avoided. (3)Based on observations on the statistics obtained from training data, we found that terms of different types (e.g.",
                "Noun and Verb) have different effects for different classes of stories in determining whether two stories are on the same topic.",
                "And we propose to use statistics to optimize the weights of the terms of different types in a story according to the news class that the story belongs to.",
                "On TDT3 dataset, the new NED model just uses 14.9% comparing times of the basic model, while its minimum normalized cost is 0.5012, which is 0.0797 better than the basic model, and also better than any other results previously reported for this dataset [8, 13].",
                "The rest of the paper is organized as follows.",
                "We start off this paper by summarizing the previous work in NED in section 2.",
                "Section 3 presents the basic model for NED that most current systems use.",
                "Section 4 describes our new detection procedure based on news indexing-tree.",
                "In section 5, two term reweighting methods are proposed to improve NED accuracy.",
                "Section 6 gives our experimental data and evaluation metrics.",
                "We finally wrap up with the experimental results in Section 7, and the conclusions and future work in Section 8. 2.",
                "RELATED WORK Papka et al. proposed Single-Pass clustering on NED [6].",
                "When a new story was encountered, it was processed immediately to extract term features and a query representation of the storys content is built up.",
                "Then it was compared with all the previous queries.",
                "If the document did not trigger any queries by exceeding a threshold, it was marked as a new event.",
                "Lam et al build up previous query representations of story clusters, each of which corresponds to a topic [7].",
                "In this manner comparisons happen between stories and clusters.",
                "Recent years, most work focus on proposing better methods on comparison of stories and document representation.",
                "Brants et al. [8] extended a basic incremental TF-IDF model to include sourcespecific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, term reweighting based on inverse event frequencies, and segmentation of documents.",
                "Good improvements on TDT bench-marks were shown.",
                "Stokes et al. [9] utilized a combination of evidence from two distinct representations of a documents content.",
                "One of the representations was the usual free text vector, the other made use of lexical chains (created using WordNet) to build another term vector.",
                "Then the two representations are combined in a linear fashion.",
                "A marginal increase in effectiveness was achieved when the combined representation was used.",
                "Some efforts have been done on how to utilize named entities to improve NED.",
                "Yang et al. gave location named entities four times weight than other terms and named entities [10].",
                "DOREMI research group combined semantic similarities of person names, location names and time together with textual similarity [11][12].",
                "UMass [13] research group split document representation into two parts: named entities and non-named entities.",
                "And it was found that some classes of news could achieve better performance using named entity representation, while some other classes of news could achieve better performance using non-named entity representation.",
                "Both [10] and [13] used text categorization technique to classify news stories in advance.",
                "In [13] news stories are classified automatically at first, and then test sensitivities of names and non-name terms for NED for each class.",
                "In [10] frequent terms for each class are removed from document representation.",
                "For example, word election does not help identify different elections.",
                "In their work, effectiveness of different kinds of names (or terms with different POS) for NED in different news classes are not investigated.",
                "We use statistical analysis to reveal the fact and use it to improve NED performance. 3.",
                "BASIC MODEL In this section, we present the basic New Event Detection model which is similar to what most current systems apply.",
                "Then, we propose our new model by extending the basic model.",
                "New Event Detection systems use news story stream as input, in which stories are strictly time-ordered.",
                "Only previously received stories are available when dealing with current story.",
                "The output is a decision for whether the current story is on a new event or not and the confidence of the decision.",
                "Usually, a NED model consists of three parts: story representation, similarity calculation and detection procedure. 3.1 Story Representation Preprocessing is needed before generating story representation.",
                "For preprocessing, we tokenize words, recognize abbreviations, normalize abbreviations, add part-of-speech tags, remove stopwords included in the stop list used in InQuery [14], replace words with their stems using K-stem algorithm[15], and then generate word vector for each news story.",
                "We use incremental TF-IDF model for term weight calculation [4].",
                "In a TF-IDF model, term frequency in a news document is weighted by the inverse document frequency, which is generated from training corpus.",
                "When a new term occurs in testing process, there are two solutions: simply ignore the new term or set df of the term as a small const (e.g. df = 1).",
                "The new term receives too low weight in the first solution (0) and too high weight in the second solution.",
                "In incremental TF-IDF model, document frequencies are updated dynamically in each time step t: 1( ) ( ) ( )t t D tdf w df w df w−= + (1) where Dt represents news story set received in time t, and dfDt(w) means the number of documents that term w occurs in, and dft(w) means the total number of documents that term w occurs in before time t. In this work, each time window includes 50 news stories.",
                "Thus, each story d received in t is represented as follows: 1 2{ ( , , ), ( , , ),..., ( , , )}nd weight d t w weight d t w weight d t w→ where n means the number of distinct terms in story d, and ( , , )weight d t w means the weight of term w in story d at time t: log( ( , ) 1) log(( 1) /( ( ) 0.5)) ( , , ) log( ( , ) 1) log(( 1) /( ( ) 0.5)) t t t t w d tf d w N df w weight d t w tf d w N df w ∈ + + + = + + +∑ (2) where Nt means the total number of news stories before time t, and tf(d,w) means how many times term w occurs in news story d. 3.2 Similarity Calculation We use Hellinger distance for the calculation of similarity between two stories, for two stories d and d at time t, their similarity is defined as follows: , ( , , ) ( , , ) * ( , , ) w d d sim d d t weight d t w weight d t w ∈ = ∑ (3) 3.3 Detection Procedure For each story d received in time step t, the value ( ) ( ) ( ) ( ( , , )) time d time d n d max sim d d t < = (4) is a score used to determine whether d is a story about a new topic and at the same time is an indication of the confidence in our decision [8]. time(d) means the publication time of story d. If the score exceeds the thresholdθ new, then there exists a sufficiently similar document, thus d is a old story, otherwise, there is no sufficiently similar previous document, thus d is an new story. 4.",
                "New NED Procedure Traditional NED systems can be classified into two main types on the aspect of detection procedure: (1) S-S type, in which the story on hand is compared to each story received previously, and use the highest similarity to determine whether current story is about a new event; (2) S-C type, in which the story on hand is compared to all previous clusters each of which representing a topic, and the highest similarity is used for final decision for current story.",
                "If the highest similarity exceeds thresholdθ new, then it is an old story, and put it into the most similar cluster; otherwise it is a new story and create a new cluster.",
                "Previous work show that the first manner is more accurate than the second one [4][5].",
                "Since sometimes stories within a topic drift far away from each other, a story may have very low similarity with its topic.",
                "So using similarities between stories for determining new story is better than using similarities between story and clusters.",
                "Nevertheless, the first manner needs much more comparing times which means the first manner is low efficient.",
                "We propose a new detection procedure which uses comparisons with previous clusters to help find the most similar story in less comparing times, and the final new event decision is made according to the most similar story.",
                "Therefore, we can get both the accuracy of S-S type methods and the efficiency of S-C type methods.",
                "The new procedure creates a news indexing-tree dynamically, in which similar stories are put together to form a hierarchy of clusters.",
                "We index similar stories together by their common ancestor (a cluster node).",
                "Dissimilar stories are indexed in different clusters.",
                "When a story is coming, we use comparisons between the current story and previous hierarchical clusters to help find the most similar story which is useful for new event decision.",
                "After the new event decision is made, the current story is inserted to the indexing-tree for the following detection.",
                "The news indexing-tree is defined formally as follows: S-Tree = {r, NC , NS , E} where r is the root of S-Tree, NC is the set of all cluster nodes, NS is the set of all story nodes, and E is the set of all edges in S-Tree.",
                "We define a set of constraints for a S-Tree: ⅰ . , is an non-terminal node in the treeC i i N i∀ ∈ → ⅱ . , is a terminal node in the treeS i i N i∀ ∈ → ⅲ . , out degree of is at least 2C i i N i∀ ∈ → ⅳ . , is represented as the centroid of its desendantsC i i iN∀ ∈ → For a news story di, the comparison procedure and inserting procedure based on indexing-tree are defined as follows.",
                "An example is shown by Figure 1 and Figure 2.",
                "Figure 1.",
                "Comparison procedure Figure 2.",
                "Inserting procedure Comparison procedure: Step 1: compare di to all the direct child nodes of r and select λ nodes with highest similarities, e.g., C1 2 and C1 3 in Figure 1.",
                "Step 2: for each selected node in the last step, e.g.",
                "C1 2, compare di to all its direct child nodes, and select λ nodes with highest similarities, e.g.",
                "C2 2 and d8.",
                "Repeat step 2 for all non-terminal nodes.",
                "Step 3: record the terminal node with the highest similarty to di, e.g. s5, and the similarity value (0.20).",
                "Inserting di to the S-tree with r as root: Find the node n which is direct child of r in the path from r to the terminal node with highest similarity s, e.g.",
                "C1 2.",
                "If s is smaller than θ init+(h-1)δ , then add di to the tree as a direct child of r. Otherwise, if n is a terminal node, then create a cluster node instead of n, and add both n and di as its direct children; if n is an non-terminal node, then repeat this procedure and insert di to the sub-tree with n as root recursively.",
                "Here h is the length between n and the root of S-tree.",
                "The more the stories in a cluster similar to each other, the better the cluster represents the stories in it.",
                "Hence we add no constraints on the maximum of trees height and degree of a node.",
                "Therefore, we cannot give the complexity of this indexing-tree based procedure.",
                "But we will give the number of comparing times needed by the new procedure in our experiments in section7. 5.",
                "Term Reweighting Methods In this section, two term reweighting methods are proposed to improve NED accuracy.",
                "In the first method, a new way is explored for better using of cluster (topic) information.",
                "The second one finds a better way to make use of named entities based on news classification. 5.1 Term Reweighting Based on Distribution Distance TF-IDF is the most prevalent model used in information retrieval systems.",
                "The basic idea is that the fewer documents a term appears in, the more important the term is in discrimination of documents (relevant or not relevant to a query containing the term).",
                "Nevertheless, in TDT domain, we need to discriminate documents with regard to topics rather than queries.",
                "Intuitively, using cluster (topic) vectors to compare with subsequent news stories should outperform using story vectors.",
                "Unfortunately, the experimental results do not support this intuition [4][5].",
                "Based on observation on data, we find the reason is that a news topic usually contains many directly or indirectly related events, while they all have their own sub-subjects which are usually different with each other.",
                "Take the topic described in section 1 as an example, events like the explosion and salvage have very low similarities with events about criminal trial, therefore stories about trial would have low similarity with the topic vector built on its previous events.",
                "This section focuses on how to effectively make use of topic information and at the same time avoid the problem of content decentralization.",
                "At first, we classify terms into 5 classes to help analysis the needs of the modified model: Term class A: terms that occur frequently in the whole corpus, e.g., year and people.",
                "Terms of this class should be given low weights because they do not help much for topic discrimination.",
                "Term class B: terms that occur frequently within a news category, e.g., election, storm.",
                "They are useful to distinguish two stories in different news categories.",
                "However, they cannot provide information to determine whether two stories are on the same or different topics.",
                "In another words, term election and term storm are not helpful in differentiate two election campaigns and two storm disasters.",
                "Therefore, terms of this class should be assigned lower weights.",
                "Term class C: terms that occur frequently in a topic, and infrequently in other topics, e.g., the name of a crash plane, the name of a specific hurricane.",
                "News stories that belong to different topics rarely have overlap terms in this class.",
                "The more frequently a term appears in a topic, the more important the term is for a story belonging to the topic, therefore the term should be set higher weight.",
                "Term class D: terms that appear in a topic exclusively, but not frequently.",
                "For example, the name of a fireman who did very well in a salvage action, which may appears in only two or three stories but never appeared in other topics.",
                "Terms of this type should receive more weights than in TF-IDF model.",
                "However, since they are not popular in the topic, it is not appropriate to give them too high weights.",
                "Term class E: terms with low document frequency, and appear in different topics.",
                "Terms of this class should receive lower weights.",
                "Now we analyze whether TF-IDF model can give proper weights to the five classes of terms.",
                "Obviously, terms of class A are lowly weighted in TF-IDF model, which is conformable with the requirement described above.",
                "In TF-IDF model, terms of class B are highly dependant with the number of stories in a news class.",
                "TF-IDF model cannot provide low weights if the story containing the term belongs to a relative small news class.",
                "For a term of class C, the more frequently it appears in a topic, the less weight TFIDF model gives to it.",
                "This strongly conflicts with the requirement of terms in class C. For terms of class D, TF-IDF model gives them high weights correctly.",
                "But for terms of class E, TF-IDF model gives high weights to them which are not conformable with the requirement of low weights.",
                "To sum up, terms of class B, C, E cannot be properly weighted in TF-IDF model.",
                "So, we propose a modified model to resolve this problem.",
                "When θ init andθ new are set closely, we assume that most of the stories in a first-level cluster (a direct child node of root node) are on the same topic.",
                "Therefore, we make use of a first-level cluster to capture term distribution (df for all the terms within the cluster) within the topic dynamically.",
                "KL divergence of term distribution in a first-level cluster and the whole story set is used to adjust term weights:  ( , , ) * (1 * ( || )) ( , , ) ( , , ) * (1 * ( || )) cw tw cw tw w d D weight d t w KL P P weight d t w weight d t w KL P P γ γ ∈ + = +∑ (5) where ( ) ( ) ( ) ( ) 1,cw cw c c c c df w df w p y p y N N = = − (6) ( ) ( ) ( ) ( ) 1,t t tw tw t t df w df w p y p y N N = = − (7) where dfc(w) is the number of documents containing term w within cluster C, and Nc is the number of documents in cluster C, and Nt is the total number of documents that arrive before time step t. γ is a const parameter, now is manually set 3.",
                "KL divergence is defined as follows [17]: ( ) ( || ) ( ) log ( )x p x KL P Q p x q x = ∑ (8) The basic idea is: for a story in a topic, the more a term occurs within the topic, and the less it occurs in other topics, it should be assigned higher weights.",
                "Obviously, modified model can meet all the requirements of the five term classes listed above. 5.2 Term Reweighting Based on Term Type and Story Class Previous work found that some classes of news stories could achieve good improvements by giving extra weight to named entities.",
                "But we find that terms of different types should be given different amount of extra weight for different classes of news stories.",
                "We use open-NLP1 to recognize named entity types and part-ofspeech tags for terms that appear in news stories.",
                "Named entity types include person name, organization name, location name, date, time, money and percentage, and five POSs are selected: none (NN), verb (VB), adjective (JJ), adverb (RB) and cardinal number (CD).",
                "Statistical analysis shows topic-level discriminative terms types for different classes of stories.",
                "For the sake of convenience, named entity type and part-of-speech tags are uniformly called term type in subsequent sections.",
                "Determining whether two stories are about the same topic is a basic component for NED task.",
                "So at first we use 2 χ statistic to compute correlations between terms and topics.",
                "For a term t and a topic T, a contingence table is derived: Table 1.",
                "A 2×2 Contingence Table Doc Number belong to topic T not belong to topic T include t A B not include t C D The 2 χ statistic for a specific term t with respect to topic T is defined to be [16]: 2 2 ( , ) ( ) * ( ) ( ) * ( ) * ( ) * ( ) w T A B C D AD CB A C B D A B C D χ = + + + − + + + + (9) News topics for the TDT task are further classified into 11 rules of interpretations (ROIs) 2 .",
                "The ROI can be seen as a higher level class of stories.",
                "The average correlation between a term type and a topic ROI is computed as: 2 avg 2 ( , )( ( , ) )k m m km kT R w P w TP R p w T R P χ χ ∈ ∈ ∑ ∑（ , ）= 1 1 k=1…K, m=1…M (10) where K is the number of term types (set 12 constantly in the paper).",
                "M is the number news classes (ROIs, set 11 in the paper).",
                "Pk represents the set of all terms of type k, and Rm represents the set of all topics of class m, p(t,T) means the probability that t occurs in topic T. Because of limitation of space, only parts of the term types (9 term types) and parts of news classes (8 classes) are listed in table 2 with the average correlation values between them.",
                "The statistics is derived from labeled data in TDT2 corpus. (Results in table 2 are already normalized for convenience in comparison.)",
                "The statistics in table 2 indicates the usefulness of different term types in topic discrimination with respect to different news classes.",
                "We can see that, location name is the most useful term type for three news classes: Natural Disasters, Violence or War, Finances.",
                "And for three other categories Elections, Legal/Criminal Cases, Science and Discovery, person name is the most discriminative term type.",
                "For Scandals/Hearings, date is the most important information for topic discrimination.",
                "In addition, Legal/Criminal Cases and Finance topics have higher correlation with money terms, while Science and Discovery have higher correlation with percentage terms.",
                "Non-name terms are more stable for different classes. 1 . http://opennlp.sourceforge.net/ 2 . http://projects.ldc.upenn.edu/TDT3/Guide/label.html From the analysis of table 2, it is reasonable to adjust term weight according to their term type and the news class the story belongs to.",
                "New term weights are reweighted as follows: ( ) ( ) ( ) ( ) ( , , ) * ( , , ) ( , , ) * class d D type w T class d D type w w d weight d t w weight d t w weight d t w α α ∈ = ∑ (11) where type(w) represents the type of term w, and class(d) represents the class of story d, c kα is reweighting parameter for news class c and term type k. In the work, we just simply use statistics in table 2 as the reweighting parameters.",
                "Even thought using the statistics directly may not the best choice, we do not discuss how to automatically obtain the best parameters.",
                "We will try to use machine learning techniques to obtain the best parameters in the future work.",
                "In the work, we use BoosTexter [20] to classify all stories into one of the 11 ROIs.",
                "BoosTexter is a boosting based machine learning program, which creates a series of simple rules for building a classifier for text or attribute-value data.",
                "We use term weight generated using TF-IDF model as feature for story classification.",
                "We trained the model on the 12000 judged English stories in TDT2, and classify the rest of the stories in TDT2 and all stories in TDT3.",
                "Classification results are used for term reweighting in formula (11).",
                "Since the class labels of topic-off stories are not given in TDT datasets, we cannot give the classification accuracy here.",
                "Thus we do not discuss the effects of classification accuracy to NED performance in the paper. 6.",
                "EXPERIMENTAL SETUP 6.1 Datasets We used two LDC [18] datasets TDT2 and TDT3 for our experiments.",
                "TDT2 contains news stories from January to June 1998.",
                "It contains around 54,000 stories from sources like ABC, Associated Press, CNN, New York Times, Public Radio International, Voice of America etc.",
                "Only English stories in the collection were considered.",
                "TDT3 contains approximately 31,000 English stories collected from October to December 1998.",
                "In addition to the sources used in TDT2, it also contains stories from NBC and MSNBC TV broadcasts.",
                "We used transcribed versions of the TV and radio broadcasts besides textual news.",
                "TDT2 dataset is labeled with about 100 topics, and approximately 12,000 English stories belong to at least one of these topics.",
                "TDT3 dataset is labeled with about 120 topics, and approximately 8000 English stories belong to at least one of these topics.",
                "All the topics are classified into 11 Rules of Interpretation: (1)Elections, (2)Scandals/Hearings, (3)Legal/Criminal Cases, (4)Natural Disasters, (5)Accidents, (6)Ongoing Violence or War, (7)Science and Discovery News, (8)Finance, (9)New Law, (10)Sports News, (11)MISC.",
                "News. 6.2 Evaluation Metric TDT uses a cost function CDet that combines the probabilities of missing a new story and a false alarm [19]: * * * *Det Miss Miss Target FA FA NontargetC C P P C P P= + (12) Table 2.",
                "Average correlation between term types and news classes where CMiss means the cost of missing a new story, PMiss means the probability of missing a new story, and PTarget means the probability of seeing a new story in the data; CFA means the cost of a false alarm, PFA means the probability of a false alarm, and PNontarget means the probability of seeing an old story.",
                "The cost CDet is normalized such that a perfect system scores 0 and a trivial system, which is the better one of mark all stories as new or old, scores 1: ( ( * , * ) ) Det Det Miss Target FA Nontarget C Norm C min C P C P = (13) New event detection system gives two outputs for each story.",
                "The first part is yes or no indicating whether the story triggers a new event or not.",
                "The second part is a score indicating confidence of the first decision.",
                "Confidence scores can be used to plot DET curve, i.e., curves that plot false alarm vs. miss probabilities.",
                "Minimum normalized cost can be determined if optimal threshold on the score were chosen. 7.",
                "EXPERIMENTAL RESULTS 7.1 Main Results To test the approaches proposed in the model, we implemented and tested five systems: System-1: this system is used as baseline.",
                "It is implemented based on the basic model described in section 3, i.e., using incremental TF-IDF model to generate term weights, and using Hellinger distance to compute document similarity.",
                "Similarity score normalization is also employed [8].",
                "S-S detection procedure is used.",
                "System-2: this system is the same as system-1 except that S-C detection procedure is used.",
                "System-3: this system is the same as system-1 except that it uses the new detection procedure which is based on indexing-tree.",
                "System-4: implemented based on the approach presented in section 5.1, i.e., terms are reweighted according to the distance between term distributions in a cluster and all stories.",
                "The new detection procedure is used.",
                "System-5: implemented based on the approach presented in section 5.2, i.e., terms of different types are reweighted according to news class using trained parameters.",
                "The new detection procedure is used.",
                "The following are some other NED systems: System-6: [21] for each pair of stories, it computes three similarity values for named entity, non-named entity and all terms respectively.",
                "And employ Support Vector Machine to predict new or old using the similarity values as features.",
                "System-7: [8] it extended a basic incremental TF-IDF model to include source-specific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, etc.",
                "System-8: [13] it split document representation into two parts: named entities and non-named entities, and choose one effective part for each news class.",
                "Table 3 and table 4 show topic-weighted normalized costs and comparing times on TDT2 and TDT3 datasets respectively.",
                "Since no heldout data set for fine-tuning the threshold θ new was available for experiments on TDT2, we only report minimum normalized costs for our systems in table 3.",
                "System-5 outperforms all other systems including system-6, and it performs only 2.78e+8 comparing times in detection procedure which is only 13.4% of system-1.",
                "Table 3.",
                "NED results on TDT2 Systems Min Norm(CDet) Cmp times System-1 0.5749 2.08e+9 System-2① 0.6673 3.77e+8 System-3② 0.5765 2.81e+8 System-4② 0.5431 2.99e+8 System-5② 0.5089 2.78e+8 System-6 0.5300 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 When evaluating on the normalized costs on TDT3, we use the optimal thresholds obtained from TDT2 data set for all systems.",
                "System-2 reduces comparing times to 1.29e+9 which is just 18.3% of system-1, but at the same time it also gets a deteriorated minimum normalized cost which is 0.0499 higher than system-1.",
                "System-3 uses the new detection procedure based on news indexing-tree.",
                "It requires even less comparing times than system-2.",
                "This is because story-story comparisons usually yield greater similarities than story-cluster ones, so stories tend to be combined Location Person Date Organization Money Percentage NN JJ CD Elections 0.37 1 0.04 0.58 0.08 0.03 0.32 0.13 0.1 Scandals/Hearings 0.66 0.62 0.28 1 0.11 0.02 0.27 0.13 0.05 Legal/Criminal Cases 0.48 1 0.02 0.62 0.15 0 0.22 0.24 0.09 Natural Disasters 1 0.27 0 0.04 0.04 0 0.25 0.04 0.02 Violence or War 1 0.36 0.02 0.14 0.02 0.04 0.21 0.11 0.02 Science and Discovery 0.11 1 0.01 0.22 0.08 0.12 0.19 0.08 0.03 Finances 1 0.45 0.04 0.98 0.13 0.02 0.29 0.06 0.05 Sports 0.16 0.27 0.01 1 0.02 0 0.11 0.03 0.01 together in system-3.",
                "And system-3 is basically equivalent to system-1 in accuracy results.",
                "System-4 adjusts term weights based on the distance of term distributions between the whole corpus and cluster story set, yielding a good improvement by 0.0468 compared to system-1.",
                "The best system (system-5) has a minimum normalized cost 0.5012, which is 0.0797 better than system-1, and also better than any other results previously reported for this dataset [8, 13].",
                "Further more, system-5 only needs 1.05e+8 comparing times which is 14.9% of system-1.",
                "Table 4.",
                "NED results on TDT3 Systems Norm(CDet) Min Norm(CDet) Cmp times System-1 0.6159 0.5809 7.04e+8 System-2① 0.6493 0.6308 1.29e+8 System-3② 0.6197 0.5868 1.03e+8 System-4② 0.5601 0.5341 1.03e+8 System-5② 0.5413 0.5012 1.05e+8 System-7 -- 0.5783 -System-8 -- 0.5229 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 Figure5 shows the five DET curves for our systems on data set TDT3.",
                "System-5 achieves the minimum cost at a false alarm rate of 0.0157 and a miss rate of 0.4310.",
                "We can observe that System4 and System-5 obtain lower miss probability at regions of low false alarm probabilities.",
                "The hypothesis is that, more weight value is transferred to key terms of topics from non-key terms.",
                "Similarity score between two stories belonging to different topics are lower than before, because their overlapping terms are usually not key terms of their topics. 7.2 Parameter selection for indexing-tree detection Figure 3 shows the minimum normalized costs obtained by system-3 on TDT3 using different parameters.",
                "Theθ init parameter is tested on six values spanning from 0.03 to 0.18.",
                "And the λ parameter is tested on four values 1, 2, 3 and 4.",
                "We can see that, whenθ init is set to 0.12, which is the closest one toθ new, the costs are lower than others.",
                "This is easy to explain, because when stories belonging to the same topic are put in a cluster, it is more reasonable for the cluster to represent the stories in it.",
                "When parameter λ is set to 3 or 4, the costs are better than other cases, but there is no much difference between 3 and 4. 0 0.05 0.1 0.15 0.2 1 2 3 4 0.5 0.6 0.7 0.8 0.9 1 θ-initλ MinCost 0.6 0.65 0.7 0.75 0.8 0.85 0.9 Figure 3.",
                "Min Cost on TDT3 (δ =0.15) 0 0.05 0.1 0.15 0.2 1 2 3 4 0 0.5 1 1.5 2 2.5 x 10 8 θ-init λ Comparingtimes 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 8 Figure 4.",
                "Comparing times on TDT3 (δ =0.15) Figure 4 gives the comparing times used by system-3 on TDT3 with the same parameters as figure 3.",
                "The comparing times are strongly dependent onθ init.",
                "Because the greaterθ init is, the less stories combined together, the more comparing times are needed for new event decision.",
                "So we useθ init =0.13,λ =3,δ =0.15 for system-3, 4, and 5.",
                "In this parameter setting, we can get both low minimum normalized costs and less comparing times. 8.",
                "CONCLUSION We have proposed a news indexing-tree based detection procedure in our model.",
                "It reduces comparing times to about one seventh of traditional method without hurting NED accuracy.",
                "We also have presented two extensions to the basic TF-IDF model.",
                "The first extension is made by adjust term weights based on term distributions between the whole corpus and a cluster story set.",
                "And the second extension to basic TF-IDF model is better use of term types (named entities types and part-of-speed) according to news categories.",
                "Our experimental results on TDT2 and TDT3 datasets show that both of the two extensions contribute significantly to improvement in accuracy.",
                "We did not consider news time information as a clue for NED task, since most of the topics last for a long time and TDT data sets only span for a relative short period (no more than 6 months).",
                "For the future work, we want to collect news set which span for a longer period from internet, and integrate time information in NED task.",
                "Since topic is a relative coarse-grained news cluster, we also want to refine cluster granularity to event-level, and identify different events and their relations within a topic.",
                "Acknowledgments This work is supported by the National Natural Science Foundation of China under Grant No. 90604025.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsor. 9.",
                "REFERENCES [1] http://www.nist.gov/speech/tests/tdt/index.htm [2] In Topic Detection and Tracking.",
                "Event-based Information Organization.",
                "Kluwer Academic Publishers, 2002. .01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90 1 2 5 10 20 40 60 80 90 False Alarm Probability (in %) MissProbability(in%) SYSTEM1 Topic Weighted Curve SYSTEM1 Min Norm(Cost) SYSTEM2 Topic Weighted Curve SYSTEM2 Min Norm(Cost) SYSTEM3 Topic Weighted Curve SYSTEM3 Min Norm(Cost) SYSTEM4 Topic Weighted Curve SYSTEM4 Min Norm(Cost) SYSTEM5 Topic Weighted Curve SYSTEM5 Min Norm(Cost) Random Performance Figure 5.",
                "DET curves on TDT3 [3] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B.T.",
                "Archibald, and X. Liu.",
                "Learning Approaches for Detecting and Tracking News Events.",
                "In IEEE Intelligent Systems Special Issue on Applications of Intelligent Information Retrieval, volume 14 (4), 1999, 32-43. [4] Y. Yang, T. Pierce, and J. Carbonell.",
                "A Study on Retrospective and On-line Event Detection.",
                "In Proceedings of SIGIR-98, Melbourne, Australia, 1998, 28-36. [5] J. Allan, V. Lavrenko, D. Malin, and R. Swan.",
                "Detections, Bounds, and Timelines: Umass and tdt-3.",
                "In Proceedings of Topic Detection and Tracking Workshop (TDT-3), Vienna, VA, 2000, 167-174. [6] R. Papka and J. Allan.",
                "On-line New Event Detection Using Single Pass Clustering TITLE2:.",
                "Technical Report UM-CS1998-021, 1998. [7] W. Lam, H. Meng, K. Wong, and J.",
                "Yen.",
                "Using Contextual Analysis for News Event Detection.",
                "International Journal on Intelligent Systems, 2001, 525-546. [8] B. Thorsten, C. Francine, and F. Ayman.",
                "A System for New Event Detection.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2003, 330-337. [9] S. Nicola and C. Joe.",
                "Combining Semantic and Syntactic Document Classifiers to Improve First Story Detection.",
                "In Proceedings of the 24th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topicconditioned Novelty Detection.",
                "In Proceedings of the 8th ACM SIGKDD International Conference, ACM Press. 2002, 688-693. [11] M. Juha, A.M. Helena, and S. Marko.",
                "Applying Semantic Classes in Event Detection and Tracking.",
                "In Proceedings of International Conference on Natural Language Processing (ICON 2002), 2002, pages 175-183. [12] M. Juha, A.M. Helena, and S. Marko.",
                "Simple Semantics in Topic Detection and Tracking.",
                "Information Retrieval, 7(3-4): 2004, 347-368. [13] K. Giridhar and J. Allan.",
                "Text Classification and Named Entities for New Event Detection.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2004, 297-304. [14] J. P. Callan, W. B. Croft, and S. M. Harding.",
                "The INQUERY Retrieval System.",
                "In Proceedings of DEXA-92, 3rd International Conference on Database and Expert Systems Applications, 1992, 78-83. [15] R. Krovetz.",
                "Viewing Morphology as An Inference Process.",
                "In Proceedings of ACM SIGIR93, 1993, 61-81. [16] Y. Yang and J. Pedersen.",
                "A Comparative Study on Feature Selection in Text Categorization.",
                "In J. D. H. Fisher, editor, The Fourteenth International Conference on Machine Learning (ICML97), Morgan Kaufmann, 1997, 412-420. [17] T. M. Cover, and J.A.",
                "Thomas.",
                "Elements of Information Theory.",
                "Wiley. 1991. [18] The linguistic data consortium, http://www.ldc,upenn.edu/. [19] The 2001 TDT task definition and evaluation plan, http://www.nist.gov/speech/tests/tdt/tdt2001/evalplan.htm. [20] R. E. Schapire and Y.",
                "Singer.",
                "Boostexter: A Boosting-based System for Text Categorization.",
                "In Machine Learning 39(2/3):1, Kluwer Academic Publishers, 2000, 35-168. [21] K. Giridhar and J. Allan. 2005.",
                "Using Names and Topics for New Event Detection.",
                "In Proceedings of Human Technology Conference and Conference on Empirical Methods in Natural Language, Vancouver, 2005, 121-128"
            ],
            "original_annotated_samples": [
                "In the first approach, we propose to adjust term weights dynamically based on previous story clusters and in the second approach, we propose to employ statistics on training data to learn the <br>named entity reweighting mode</br>l for each class of stories."
            ],
            "translated_annotated_samples": [
                "En el primer enfoque, proponemos ajustar dinámicamente los pesos de los términos basados en los clusters de historias previas y en el segundo enfoque, proponemos emplear estadísticas en los datos de entrenamiento para aprender el <br>modelo de reajuste de entidades nombradas</br> para cada clase de historias."
            ],
            "translated_text": "Detección de nuevos eventos basada en árbol de indexación y entidad nombrada Zhang Kuo Universidad Tsinghua Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Universidad Tsinghua Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Universidad Tsinghua Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn RESUMEN La Detección de Nuevos Eventos (NED) tiene como objetivo detectar de una o varias corrientes de noticias cuál se informa sobre un nuevo evento (es decir, no reportado previamente). Con el abrumador volumen de noticias disponible hoy en día, hay una creciente necesidad de un sistema NED que sea capaz de detectar eventos nuevos de manera más eficiente y precisa. En este artículo proponemos un nuevo modelo de NED para acelerar la tarea de NED mediante el uso dinámico de un árbol de indexación de noticias. Además, basándose en la observación de que los términos de diferentes tipos tienen diferentes efectos para la tarea de NED, se proponen dos enfoques de reponderación de términos para mejorar la precisión de NED. En el primer enfoque, proponemos ajustar dinámicamente los pesos de los términos basados en los clusters de historias previas y en el segundo enfoque, proponemos emplear estadísticas en los datos de entrenamiento para aprender el <br>modelo de reajuste de entidades nombradas</br> para cada clase de historias. Los resultados experimentales en dos conjuntos de datos del Consorcio de Datos Lingüísticos (LDC), TDT2 y TDT3, muestran que el modelo propuesto puede mejorar significativamente tanto la eficiencia como la precisión de la tarea de NED, en comparación con el sistema base y otros sistemas existentes. Categorías y Descriptores de Asignaturas H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información; H.4.2 [Aplicaciones de Sistemas de Información]: Tipos de Sistemas de Apoyo a Decisiones. Términos generales Algoritmos, Rendimiento, Experimentación 1. El programa de Detección y Seguimiento de Temas (TDT) tiene como objetivo desarrollar técnicas que puedan organizar, buscar y estructurar de manera efectiva materiales de texto de noticias de una variedad de agencias de noticias y medios de difusión [1]. La Detección de Eventos Nuevos (DEN) es una de las cinco tareas en TDT. Es la tarea de identificación en línea del informe más temprano para cada tema tan pronto como ese informe llegue en la secuencia de documentos. Un tema se define como un evento o actividad seminal, junto con eventos y actividades directamente relacionados [2]. Un evento se define como algo (no trivial) que sucede en un lugar y en un momento determinados [3]. Por ejemplo, cuando una bomba explota en un edificio, la explosión es el evento seminal que desencadena el tema, y otras historias sobre el mismo tema serían aquellas que discuten los esfuerzos de salvamento, la búsqueda de los perpetradores, arrestos y juicios, entre otros. La información útil de noticias suele estar enterrada en una masa de datos generados a diario. Por lo tanto, los sistemas NED son muy útiles para las personas que necesitan detectar información novedosa de un flujo de noticias en tiempo real. Estas necesidades de la vida real a menudo se presentan en ámbitos como los mercados financieros, el análisis de noticias y la recopilación de inteligencia. En la mayoría de los sistemas NED de última generación, cada noticia disponible se compara con todas las noticias previamente recibidas. Si todas las similitudes entre ellos no superan un umbral, entonces la historia desencadena un nuevo evento. Por lo general, se presentan en forma de similitud de coseno o métrica de similitud de Hellinger. El problema central de NED es identificar si dos historias tratan sobre el mismo tema. Obviamente, estos sistemas no pueden aprovechar la información del tema. Además, no es aceptable en aplicaciones reales debido a la gran cantidad de cálculos requeridos en el proceso de NED. Otros sistemas organizan historias anteriores en grupos (cada grupo corresponde a un tema), y la nueva historia se compara con los grupos anteriores en lugar de con historias. Esta forma puede reducir significativamente los tiempos de comparación. Sin embargo, se ha demostrado que este método es menos preciso [4, 5]. Esto se debe a que a veces las historias dentro de un tema se alejan mucho entre sí, lo que podría resultar en una baja similitud entre una historia y su tema. Por otro lado, algunos sistemas NED propuestos intentaron mejorar la precisión al hacer un mejor uso de las entidades nombradas [10, 11, 12, 13]. Sin embargo, ninguno de los sistemas ha considerado que los términos de diferentes tipos (por ejemplo, Los sustantivos, verbos o nombres de personas tienen diferentes efectos en diferentes clases de historias para determinar si dos historias tratan sobre el mismo tema. Por ejemplo, los nombres de los candidatos electorales (Nombre de la persona) son muy importantes para las historias de la clase de elecciones; las ubicaciones (Nombre de la ubicación) donde ocurrieron los accidentes son importantes para las historias de la clase de accidentes. Por lo tanto, en NED, todavía existen los siguientes tres problemas por investigar: (1) ¿Cómo acelerar el procedimiento de detección sin disminuir la precisión de la detección? (2) ¿Cómo aprovechar mejor la información de los grupos (temas) para mejorar la precisión? (3) ¿Cómo obtener una mejor representación de historias de noticias mediante una mejor comprensión de las entidades nombradas? Impulsados por estos problemas, hemos propuesto tres enfoques en este documento. (1) Para hacer que el procedimiento de detección sea más rápido, proponemos un nuevo procedimiento de NED basado en un árbol de indexación de noticias creado dinámicamente. El índice de historias en árbol se crea ensamblando historias similares para formar grupos de noticias en diferentes jerarquías según sus valores de similitud. Las comparaciones entre la historia actual y los grupos anteriores podrían ayudar a encontrar la historia más similar en menos tiempo de comparación. El nuevo procedimiento puede reducir la cantidad de veces de comparación sin afectar la precisión. Utilizamos los grupos del primer piso en el árbol de indexación como temas de noticias, en los cuales los pesos de los términos se ajustan dinámicamente según la distribución de términos en los grupos. En este enfoque, se utiliza adecuadamente la información de los grupos (temas), por lo que se evita el problema de la descentralización del tema. Basándonos en observaciones de las estadísticas obtenidas de los datos de entrenamiento, encontramos que los términos de diferentes tipos (por ejemplo, Los sustantivos y verbos tienen diferentes efectos para diferentes clases de historias al determinar si dos historias tratan sobre el mismo tema. Y proponemos utilizar estadísticas para optimizar los pesos de los términos de diferentes tipos en una historia de acuerdo a la clase de noticias a la que pertenece la historia. En el conjunto de datos TDT3, el nuevo modelo NED solo utiliza un 14.9% en comparación con los tiempos del modelo básico, mientras que su costo normalizado mínimo es de 0.5012, lo que es 0.0797 mejor que el modelo básico, y también mejor que cualquier otro resultado previamente reportado para este conjunto de datos [8, 13]. El resto del documento está organizado de la siguiente manera. Comenzamos este artículo resumiendo el trabajo previo en NED en la sección 2. La sección 3 presenta el modelo básico para NED que la mayoría de los sistemas actuales utilizan. La sección 4 describe nuestro nuevo procedimiento de detección basado en el índice de árbol de noticias. En la sección 5, se proponen dos métodos de reponderación de términos para mejorar la precisión de NED. La sección 6 presenta nuestros datos experimentales y métricas de evaluación. Finalmente concluimos con los resultados experimentales en la Sección 7, y las conclusiones y trabajo futuro en la Sección 8. TRABAJO RELACIONADO Papka et al. propusieron el agrupamiento de un solo paso en NED [6]. Cuando se encontraba una nueva historia, esta era procesada inmediatamente para extraer características de términos y se construía una representación de consulta del contenido de la historia. Luego se comparó con todas las consultas anteriores. Si el documento no generaba ninguna consulta al exceder un umbral, se marcaba como un nuevo evento. Lam et al construyen representaciones de consultas previas de grupos de historias, cada una de las cuales corresponde a un tema [7]. De esta manera se realizan comparaciones entre historias y grupos. En los últimos años, la mayoría de los trabajos se han centrado en proponer mejores métodos para la comparación de historias y la representación de documentos. Brants et al. [8] extendieron un modelo básico incremental de TF-IDF para incluir modelos específicos de fuente, normalización de puntajes de similitud basada en promedios específicos de documentos, normalización de puntajes de similitud basada en promedios específicos de pares de fuentes, reponderación de términos basada en frecuencias de eventos inversas y segmentación de documentos. Se mostraron buenas mejoras en los puntos de referencia de TDT. Stokes et al. [9] utilizaron una combinación de evidencia de dos representaciones distintas del contenido de un documento. Una de las representaciones era el vector de texto libre habitual, la otra hacía uso de cadenas léxicas (creadas utilizando WordNet) para construir otro vector de términos. Luego, las dos representaciones se combinan de forma lineal. Se logró un aumento marginal en la efectividad cuando se utilizó la representación combinada. Algunos esfuerzos se han realizado sobre cómo utilizar entidades nombradas para mejorar la desambiguación de entidades nombradas (NED). Yang et al. asignaron un peso cuatro veces mayor a las entidades con nombres de ubicación que a otros términos y entidades con nombres [10]. El grupo de investigación DOREMI combinó similitudes semánticas de nombres de personas, nombres de lugares y tiempo junto con similitud textual [11][12]. El grupo de investigación de UMass [13] dividió la representación del documento en dos partes: entidades nombradas y entidades no nombradas. Y se descubrió que algunas clases de noticias podían lograr un mejor rendimiento utilizando la representación de entidades nombradas, mientras que otras clases de noticias podían lograr un mejor rendimiento utilizando la representación sin entidades nombradas. Tanto [10] como [13] utilizaron la técnica de categorización de texto para clasificar previamente las noticias. En [13] las noticias se clasifican automáticamente al principio, y luego se prueban las sensibilidades de los nombres y términos que no son nombres para NED para cada clase. En [10], se eliminan los términos frecuentes de cada clase de la representación del documento. Por ejemplo, la palabra \"elección\" no ayuda a identificar diferentes elecciones. En su trabajo, no se investiga la efectividad de diferentes tipos de nombres (o términos con diferentes categorías gramaticales) para NED en diferentes clases de noticias. Utilizamos análisis estadístico para revelar el hecho y utilizarlo para mejorar el rendimiento de NED. 3. MODELO BÁSICO En esta sección, presentamos el modelo básico de Detección de Nuevos Eventos que es similar a lo que la mayoría de los sistemas actuales aplican. Luego, proponemos nuestro nuevo modelo ampliando el modelo básico. Los sistemas de detección de eventos nuevos utilizan flujos de noticias como entrada, en los cuales las historias están estrictamente ordenadas por tiempo. Solo están disponibles las historias recibidas previamente al tratar con la historia actual. La salida es una decisión sobre si la historia actual es un evento nuevo o no y la confianza de la decisión. Por lo general, un modelo NED consta de tres partes: representación de la historia, cálculo de similitud y procedimiento de detección. 3.1 Representación de la historia Se necesita preprocesamiento antes de generar la representación de la historia. Para el preprocesamiento, tokenizamos palabras, reconocemos abreviaturas, normalizamos abreviaturas, agregamos etiquetas de partes del discurso, eliminamos palabras vacías incluidas en la lista de paradas utilizada en InQuery [14], reemplazamos palabras con sus raíces utilizando el algoritmo K-stem[15], y luego generamos un vector de palabras para cada noticia. Utilizamos el modelo TF-IDF incremental para el cálculo del peso de los términos [4]. En un modelo TF-IDF, la frecuencia de término en un documento de noticias se pondera por la frecuencia inversa del documento, la cual se genera a partir del corpus de entrenamiento. Cuando surge un nuevo término en el proceso de prueba, hay dos soluciones: simplemente ignorar el nuevo término o establecer df del término como una constante pequeña (por ejemplo, df = 1). El nuevo término recibe un peso demasiado bajo en la primera solución (0) y un peso demasiado alto en la segunda solución. En el modelo TF-IDF incremental, las frecuencias de documentos se actualizan dinámicamente en cada paso de tiempo t: 1( ) ( ) ( )t t D tdf w df w df w−= + (1) donde Dt representa el conjunto de noticias recibidas en el tiempo t, y dfDt(w) significa el número de documentos en los que aparece el término w, y dft(w) significa el número total de documentos en los que aparece el término w antes del tiempo t. En este trabajo, cada ventana de tiempo incluye 50 noticias. Así, cada historia d recibida en t se representa de la siguiente manera: 1 2{ ( , , ), ( , , ),..., ( , , )}nd peso d t w peso d t w peso d t w→ donde n significa el número de términos distintos en la historia d, y ( , , )peso d t w significa el peso del término w en la historia d en el tiempo t: log( ( , ) 1) log(( 1) /( ( ) 0.5)) ( , , ) log( ( , ) 1) log(( 1) /( ( ) 0.5)) t t t t w d tf d w N df w peso d t w tf d w N df w ∈ + + + = + + +∑ (2) donde Nt significa el número total de historias de noticias antes del tiempo t, y tf(d,w) significa cuántas veces aparece el término w en la historia de noticias d. 3.2 Cálculo de Similitud Utilizamos la distancia de Hellinger para el cálculo de la similitud entre dos historias, para dos historias d y d en el tiempo t, su similitud se define de la siguiente manera: , ( , , ) ( , , ) * ( , , ) w d d sim d d t peso d t w peso d t w ∈ = ∑ (3) 3.3 Procedimiento de Detección Para cada historia d recibida en el paso de tiempo t, el valor ( ) ( ) ( ) ( ( , , )) tiempo d tiempo d n d max sim d d t < = (4) es un puntaje utilizado para determinar si d es una historia sobre un nuevo tema y al mismo tiempo es una indicación de la confianza en nuestra decisión [8]. tiempo(d) significa el tiempo de publicación de la historia d. Si el puntaje supera el umbral θ new, entonces existe un documento suficientemente similar, por lo tanto, d es una historia antigua, de lo contrario, no hay un documento previo suficientemente similar, por lo tanto, d es una historia nueva. 4. Nuevo procedimiento NED Los sistemas NED tradicionales se pueden clasificar en dos tipos principales en cuanto al procedimiento de detección: (1) tipo S-S, en el que la historia en cuestión se compara con cada historia recibida previamente, y se utiliza la mayor similitud para determinar si la historia actual trata sobre un evento nuevo; (2) tipo S-C, en el que la historia en cuestión se compara con todos los clusters anteriores, cada uno de los cuales representa un tema, y se utiliza la mayor similitud para la decisión final sobre la historia actual. Si la similitud más alta supera el umbral θ nuevo, entonces es una historia antigua y se coloca en el clúster más similar; de lo contrario, es una historia nueva y se crea un nuevo clúster. Trabajos anteriores muestran que la primera forma es más precisa que la segunda [4][5]. Dado que a veces las historias dentro de un tema se alejan mucho entre sí, una historia puede tener una similitud muy baja con su tema. Por lo tanto, utilizar similitudes entre historias para determinar una nueva historia es mejor que utilizar similitudes entre historias y grupos. Sin embargo, el primer método requiere mucho más tiempo de comparación, lo que significa que es menos eficiente. Proponemos un nuevo procedimiento de detección que utiliza comparaciones con clusters anteriores para ayudar a encontrar la historia más similar en menos tiempo de comparación, y la decisión final sobre el nuevo evento se toma de acuerdo con la historia más similar. Por lo tanto, podemos obtener tanto la precisión de los métodos de tipo S-S como la eficiencia de los métodos de tipo S-C. El nuevo procedimiento crea un árbol de indexación de noticias dinámicamente, en el cual se agrupan historias similares para formar una jerarquía de clusters. Indexamos historias similares juntas por su ancestro común (un nodo de clúster). Historias diferentes están indexadas en diferentes grupos. Cuando una historia está llegando, utilizamos comparaciones entre la historia actual y los grupos jerárquicos anteriores para ayudar a encontrar la historia más similar que es útil para la toma de decisiones sobre nuevos eventos. Después de que se tome la decisión sobre el nuevo evento, la historia actual se inserta en el árbol de indexación para la detección siguiente. El índice de noticias en forma de árbol se define formalmente de la siguiente manera: S-Tree = {r, NC, NS, E}, donde r es la raíz de S-Tree, NC es el conjunto de todos los nodos de clúster, NS es el conjunto de todos los nodos de historia, y E es el conjunto de todos los bordes en S-Tree. Definimos un conjunto de restricciones para un S-Árbol: ⅰ. , es un nodo no terminal en el árbolC i i N i∀ ∈ → ⅱ. , es un nodo terminal en el árbolS i i N i∀ ∈ → ⅲ. , el grado de salida de es al menos 2C i i N i∀ ∈ → ⅳ. , se representa como el centroide de sus descendientesC i i iN∀ ∈ → Para una noticia di, el procedimiento de comparación y el procedimiento de inserción basados en el árbol de indexación se definen de la siguiente manera. Un ejemplo se muestra en la Figura 1 y la Figura 2. Figura 1. Procedimiento de comparación Figura 2. Procedimiento de comparación de inserción: Paso 1: comparar di con todos los nodos hijos directos de r y seleccionar λ nodos con las similitudes más altas, por ejemplo, C1 2 y C1 3 en la Figura 1. Paso 2: para cada nodo seleccionado en el paso anterior, por ejemplo. C1 2, compara di con todos sus nodos hijos directos y selecciona λ nodos con las similitudes más altas, por ejemplo. C2 2 y d8. Repetir el paso 2 para todos los nodos no terminales. Paso 3: registrar el nodo terminal con la mayor similitud a di, por ejemplo, s5, y el valor de similitud (0.20). Insertando di en el S-árbol con r como raíz: Encuentra el nodo n que es hijo directo de r en el camino desde r hasta el nodo terminal con la similitud más alta s, por ejemplo. C1 2. Si s es menor que θ init + (h-1)δ, entonces agregar di al árbol como un hijo directo de r. De lo contrario, si n es un nodo terminal, crear un nodo de clúster en lugar de n, y agregar tanto n como di como sus hijos directos; si n es un nodo no terminal, repetir este procedimiento e insertar di en el subárbol con n como raíz de forma recursiva. Aquí h es la longitud entre n y la raíz del árbol S. Cuanto más similares sean las historias en un grupo, mejor representará el grupo a las historias en él. Por lo tanto, no imponemos restricciones en la altura máxima de los árboles y el grado de un nodo. Por lo tanto, no podemos dar la complejidad de este procedimiento basado en árboles de indexación. Pero proporcionaremos el número de veces de comparación necesarias por el nuevo procedimiento en nuestros experimentos en la sección 7.5. En esta sección, se proponen dos métodos de reponderación de términos para mejorar la precisión de NED. En el primer método, se explora una nueva forma de utilizar de manera más efectiva la información de los grupos (temas). El segundo encuentra una mejor manera de utilizar entidades nombradas basadas en la clasificación de noticias. 5.1 Reajuste de términos basado en la distancia de distribución TF-IDF es el modelo más prevalente utilizado en sistemas de recuperación de información. La idea básica es que cuanto menos documentos aparezca un término, más importante es el término en la discriminación de documentos (relevantes o no relevantes para una consulta que contenga el término). Sin embargo, en el dominio de TDT, necesitamos discriminar documentos en función de los temas en lugar de las consultas. Intuitivamente, el uso de vectores de clúster (tema) para comparar con historias de noticias posteriores debería tener un mejor rendimiento que el uso de vectores de historias. Desafortunadamente, los resultados experimentales no respaldan esta intuición [4][5]. Basándonos en la observación de los datos, encontramos que la razón es que un tema de noticias generalmente contiene muchos eventos directa o indirectamente relacionados, y todos ellos tienen sus propios subtemas que suelen ser diferentes entre sí. Tomemos el tema descrito en la sección 1 como ejemplo, eventos como la explosión y el rescate tienen similitudes muy bajas con eventos sobre juicios criminales, por lo tanto, las historias sobre juicios tendrían una baja similitud con el vector de tema construido en base a sus eventos previos. Esta sección se centra en cómo hacer un uso efectivo de la información del tema y al mismo tiempo evitar el problema de la descentralización del contenido. Al principio, clasificamos los términos en 5 clases para ayudar en el análisis de las necesidades del modelo modificado: Clase de términos A: términos que ocurren con frecuencia en todo el corpus, por ejemplo, año y personas. Los términos de esta clase deben recibir pesos bajos porque no ayudan mucho en la discriminación de temas. Clase B de términos: términos que ocurren con frecuencia dentro de una categoría de noticias, por ejemplo, elección, tormenta. Son útiles para distinguir dos historias en diferentes categorías de noticias. Sin embargo, no pueden proporcionar información para determinar si dos historias tratan sobre el mismo tema o temas diferentes. En otras palabras, los términos elección y tormenta no son útiles para diferenciar dos campañas electorales y dos desastres de tormenta. Por lo tanto, los términos de esta clase deberían asignarse pesos más bajos. Clase de término C: términos que ocurren con frecuencia en un tema y raramente en otros temas, por ejemplo, el nombre de un avión accidentado, el nombre de un huracán específico. Las noticias que pertenecen a diferentes temas rara vez tienen términos superpuestos en esta clase. Cuanto más frecuentemente aparezca un término en un tema, más importante es el término para una historia perteneciente al tema, por lo tanto, el término debería tener un peso más alto. Clase de término D: términos que aparecen en un tema de manera exclusiva, pero no con frecuencia. Por ejemplo, el nombre de un bombero que se desempeñó muy bien en una acción de salvamento, que puede aparecer en solo dos o tres historias pero nunca ha aparecido en otros temas. Los términos de este tipo deberían recibir más peso que en el modelo TF-IDF. Sin embargo, dado que no son populares en el tema, no es apropiado asignarles pesos demasiado altos. Clase de término E: términos con baja frecuencia en documentos y que aparecen en diferentes temas. Los términos de esta clase deberían recibir pesos más bajos. Ahora analizamos si el modelo TF-IDF puede asignar pesos adecuados a las cinco clases de términos. Obviamente, los términos de la clase A tienen un peso bajo en el modelo TF-IDF, lo cual es acorde con el requisito descrito anteriormente. En el modelo TF-IDF, los términos de la clase B dependen altamente del número de historias en una clase de noticias. El modelo TF-IDF no puede proporcionar pesos bajos si la historia que contiene el término pertenece a una clase de noticias relativamente pequeña. Para un término de clase C, cuanto más frecuentemente aparezca en un tema, menos peso le da el modelo TFIDF. Esto entra en conflicto directo con el requisito de términos en la clase C. Para los términos de la clase D, el modelo TF-IDF les asigna pesos altos de manera correcta. Pero para los términos de la clase E, el modelo TF-IDF asigna pesos altos a ellos que no son conformes con el requisito de pesos bajos. En resumen, los términos de clase B, C, E no pueden ser ponderados adecuadamente en el modelo TF-IDF. Por lo tanto, proponemos un modelo modificado para resolver este problema. Cuando θ init y θ new están configurados de cerca, asumimos que la mayoría de las historias en un clúster de primer nivel (un nodo hijo directo del nodo raíz) están en el mismo tema. Por lo tanto, utilizamos un clúster de primer nivel para capturar la distribución de términos (df para todos los términos dentro del clúster) dentro del tema de forma dinámica. La divergencia de Kullback-Leibler de la distribución de términos en un clúster de primer nivel y el conjunto completo de historias se utiliza para ajustar los pesos de los términos: ( , , ) * (1 * ( || )) ( , , ) ( , , ) * (1 * ( || )) cw tw cw tw w d D peso d t w KL P P peso d t w peso d t w KL P P γ γ ∈ + = +∑ (5) donde ( ) ( ) ( ) ( ) 1,cw cw c c c c df w df w p y p y N N = = − (6) ( ) ( ) ( ) ( ) 1,t t tw tw t t df w df w p y p y N N = = − (7) donde dfc(w) es el número de documentos que contienen el término w dentro del clúster C, y Nc es el número de documentos en el clúster C, y Nt es el número total de documentos que llegan antes del paso de tiempo t. γ es un parámetro constante, actualmente configurado manualmente en 3. La divergencia de Kullback-Leibler se define de la siguiente manera [17]: La idea básica es que, para una historia en un tema, cuanto más ocurre un término dentro del tema y menos ocurre en otros temas, se le deben asignar pesos más altos. Obviamente, el modelo modificado puede cumplir con todos los requisitos de las cinco clases de términos mencionadas anteriormente. 5.2 Reajuste de Términos Basado en el Tipo de Término y la Clase de Historia. Trabajos anteriores encontraron que algunas clases de historias de noticias podrían lograr mejoras significativas al dar un peso adicional a las entidades nombradas. Pero encontramos que los términos de diferentes tipos deben recibir una cantidad diferente de peso adicional para diferentes clases de noticias. Utilizamos open-NLP1 para reconocer tipos de entidades nombradas y etiquetas de partes del discurso para términos que aparecen en noticias. Los tipos de entidades nombradas incluyen nombre de persona, nombre de organización, nombre de ubicación, fecha, hora, dinero y porcentaje, y se seleccionan cinco partes del discurso: ninguno (NN), verbo (VB), adjetivo (JJ), adverbio (RB) y número cardinal (CD). El análisis estadístico muestra tipos de términos discriminativos a nivel de tema para diferentes clases de historias. Por conveniencia, los tipos de entidad nombrada y etiquetas de parte de la oración se llaman uniformemente tipo de término en las secciones siguientes. Determinar si dos historias tratan sobre el mismo tema es un componente básico para la tarea de NED. Por lo tanto, al principio usamos la estadística χ² para calcular las correlaciones entre términos y temas. Para un término t y un tema T, se deriva una tabla de contingencia: Tabla 1. Una tabla de contingencia de 2×2 con el número de documentos pertenecientes al tema T y no pertenecientes al tema T incluye t A B no incluye t C D. La estadística χ² para un término específico t con respecto al tema T se define como [16]: χ² = ( ) ( ) * ( ) ( ) * ( ) * ( ) * ( ) w T A B C D AD CB A C B D A B C D + + + − + + + + (9). Los temas de noticias para la tarea TDT se clasifican además en 11 reglas de interpretaciones (ROIs) 2. El ROI se puede ver como una clase de historias de nivel superior. El promedio de correlación entre un tipo de término y un ROI de tema se calcula como: 2 avg 2 ( , )( ( , ) )k m m km kT R w P w TP R p w T R P χ χ ∈ ∈ ∑ ∑（ , ）= 1 1 k=1…K, m=1…M (10) donde K es el número de tipos de términos (fijado constantemente en 12 en el artículo). M es el número de nuevas clases (ROIs, establecido en el conjunto 11 del artículo). Pk representa el conjunto de todos los términos de tipo k, y Rm representa el conjunto de todos los temas de la clase m, p(t,T) significa la probabilidad de que t ocurra en el tema T. Debido a limitaciones de espacio, solo se enumeran partes de los tipos de términos (9 tipos de términos) y partes de las clases de noticias (8 clases) en la tabla 2 con los valores promedio de correlación entre ellos. Las estadísticas se derivan de datos etiquetados en el corpus TDT2. (Los resultados en la tabla 2 ya están normalizados para mayor comodidad en la comparación). Las estadísticas en la tabla 2 indican la utilidad de diferentes tipos de términos en la discriminación de temas con respecto a diferentes clases de noticias. Podemos ver que el nombre de la ubicación es el tipo de término más útil para tres clases de noticias: Desastres Naturales, Violencia o Guerra, Finanzas. Y para otras tres categorías: Elecciones, Casos Legales/Criminales, Ciencia y Descubrimiento, el nombre de la persona es el tipo de término más discriminativo. Para Escándalos/Audiencias, la fecha es la información más importante para la discriminación de temas. Además, los casos legales/criminales y los temas financieros tienen una correlación más alta con términos relacionados con el dinero, mientras que la ciencia y el descubrimiento tienen una correlación más alta con términos de porcentaje. Los términos no nominales son más estables para diferentes clases. 1. http://opennlp.sourceforge.net/ 2. http://projects.ldc.upenn.edu/TDT3/Guide/label.html A partir del análisis de la tabla 2, es razonable ajustar el peso de los términos según su tipo de término y la clase de noticias a la que pertenece la historia. Los nuevos pesos de los términos se recalculan de la siguiente manera: ( ) ( ) ( ) ( ) ( , , ) * ( , , ) ( , , ) * clase d D tipo w T clase d D tipo w w d peso d t w peso d t w peso d t w α α ∈ = ∑ (11) donde type(w) representa el tipo del término w, y class(d) representa la clase de la historia d, c kα es el parámetro de recalibración para la clase de noticias c y el tipo de término k. En el trabajo, simplemente utilizamos las estadísticas de la tabla 2 como parámetros de recalibración. Aunque usar las estadísticas directamente puede que no sea la mejor opción, no discutimos cómo obtener automáticamente los mejores parámetros. Intentaremos utilizar técnicas de aprendizaje automático para obtener los mejores parámetros en el trabajo futuro. En el trabajo, utilizamos BoosTexter [20] para clasificar todas las historias en uno de los 11 ROIs. BoosTexter es un programa de aprendizaje automático basado en boosting, que crea una serie de reglas simples para construir un clasificador de datos de texto o atributos-valor. Utilizamos el peso del término generado utilizando el modelo TF-IDF como característica para la clasificación de historias. Entrenamos el modelo con las 12000 historias en inglés evaluadas en TDT2, y clasificamos el resto de las historias en TDT2 y todas las historias en TDT3. Los resultados de clasificación se utilizan para la reponderación de términos en la fórmula (11). Dado que las etiquetas de clase de las historias de tema apagado no se proporcionan en los conjuntos de datos de TDT, no podemos proporcionar la precisión de clasificación aquí. Por lo tanto, no discutimos los efectos de la precisión de la clasificación en el rendimiento de NED en el artículo. 6. CONFIGURACIÓN EXPERIMENTAL 6.1 Conjuntos de datos Utilizamos dos conjuntos de datos de LDC [18], TDT2 y TDT3, para nuestros experimentos. TDT2 contiene noticias desde enero hasta junio de 1998. Contiene alrededor de 54,000 historias de fuentes como ABC, Associated Press, CNN, New York Times, Public Radio International, Voice of America, etc. Solo se consideraron historias en inglés en la colección. TDT3 contiene aproximadamente 31,000 historias en inglés recopiladas de octubre a diciembre de 1998. Además de las fuentes utilizadas en TDT2, también contiene historias de las transmisiones de televisión de NBC y MSNBC. Utilizamos versiones transcritas de las emisiones de televisión y radio además de noticias textuales. El conjunto de datos TDT2 está etiquetado con alrededor de 100 temas, y aproximadamente 12,000 historias en inglés pertenecen al menos a uno de estos temas. El conjunto de datos TDT3 está etiquetado con alrededor de 120 temas, y aproximadamente 8000 historias en inglés pertenecen al menos a uno de estos temas. Todos los temas están clasificados en 11 Reglas de Interpretación: (1) Elecciones, (2) Escándalos/Audiencias, (3) Casos Legales/Criminales, (4) Desastres Naturales, (5) Accidentes, (6) Violencia o Guerra en Curso, (7) Noticias de Ciencia y Descubrimientos, (8) Finanzas, (9) Nueva Ley, (10) Noticias Deportivas, (11) Varios. Noticias. 6.2 Métrica de evaluación TDT utiliza una función de costo CDet que combina las probabilidades de perder una nueva historia y una falsa alarma [19]: * * * *Det Miss Miss Target FA FA NontargetC C P P C P P= + (12) Tabla 2. Correlación promedio entre tipos de términos y clases de noticias donde CMiss significa el costo de perder una nueva historia, PMiss significa la probabilidad de perder una nueva historia, y PTarget significa la probabilidad de ver una nueva historia en los datos; CFA significa el costo de una falsa alarma, PFA significa la probabilidad de una falsa alarma, y PNontarget significa la probabilidad de ver una historia antigua. El costo CDet se normaliza de tal manera que un sistema perfecto obtiene una puntuación de 0 y un sistema trivial, que es el mejor de marcar todas las historias como nuevas o antiguas, obtiene una puntuación de 1: ( ( * , * ) ) Det Det Miss Target FA Nontarget C Norm C min C P C P = (13) El sistema de detección de eventos nuevos proporciona dos salidas para cada historia. La primera parte es sí o no, indicando si la historia desencadena un nuevo evento o no. La segunda parte es una puntuación que indica la confianza de la primera decisión. Los puntajes de confianza se pueden utilizar para trazar la curva DET, es decir, curvas que representan la probabilidad de falsa alarma frente a la probabilidad de omisión. El costo normalizado mínimo se puede determinar si se elige el umbral óptimo en la puntuación. RESULTADOS EXPERIMENTALES 7.1 Resultados principales Para probar los enfoques propuestos en el modelo, implementamos y probamos cinco sistemas: Sistema-1: este sistema se utiliza como referencia. Se implementa basándose en el modelo básico descrito en la sección 3, es decir, utilizando el modelo TF-IDF incremental para generar pesos de términos y utilizando la distancia de Hellinger para calcular la similitud entre documentos. La normalización del puntaje de similitud también se emplea [8]. Se utiliza el procedimiento de detección S-S. Sistema-2: este sistema es igual que el sistema-1 excepto que se utiliza el procedimiento de detección S-C. Sistema-3: este sistema es igual que el sistema-1 excepto que utiliza el nuevo procedimiento de detección que se basa en un árbol de indexación. Sistema-4: implementado basado en el enfoque presentado en la sección 5.1, es decir, los términos se vuelven a ponderar según la distancia entre las distribuciones de términos en un grupo y todas las historias. Se utiliza el nuevo procedimiento de detección. Sistema-5: implementado basado en el enfoque presentado en la sección 5.2, es decir, los términos de diferentes tipos son reponderados según la clase de noticias utilizando parámetros entrenados. El nuevo procedimiento de detección se utiliza. Los siguientes son algunos otros sistemas NED: Sistema-6: [21] para cada par de historias, calcula tres valores de similitud para entidad nombrada, entidad no nombrada y todos los términos respectivamente. Y emplear Máquina de Vectores de Soporte para predecir nuevo o antiguo utilizando los valores de similitud como características. Sistema-7: [8] extendió un modelo básico incremental de TF-IDF para incluir modelos específicos de origen, normalización de puntuaciones de similitud basada en promedios específicos de documentos, normalización de puntuaciones de similitud basada en promedios específicos de pares de fuentes, etc. Sistema-8: [13] dividió la representación del documento en dos partes: entidades nombradas y entidades no nombradas, y eligió una parte efectiva para cada clase de noticias. La tabla 3 y la tabla 4 muestran los costos normalizados ponderados por tema y los tiempos de comparación en los conjuntos de datos TDT2 y TDT3 respectivamente. Dado que no se contaba con un conjunto de datos de validación para ajustar el umbral θ nuevo en los experimentos en TDT2, solo informamos los costos normalizados mínimos para nuestros sistemas en la tabla 3. El sistema-5 supera a todos los demás sistemas, incluido el sistema-6, y realiza solo 2.78e+8 veces la comparación de tiempos en el procedimiento de detección, lo que representa solo el 13.4% del sistema-1. Tabla 3. Los resultados de NED en los sistemas TDT2 Min Norm(CDet) tiempos de comparación son: Sistema-1 0.5749 2.08e+9, Sistema-2 0.6673 3.77e+8, Sistema-3 0.5765 2.81e+8, Sistema-4 0.5431 2.99e+8, Sistema-5 0.5089 2.78e+8, Sistema-6 0.5300. Al evaluar en los costos normalizados en TDT3, utilizamos los umbrales óptimos obtenidos del conjunto de datos de TDT2 para todos los sistemas. El sistema-2 reduce los tiempos de comparación a 1.29e+9, que es solo el 18.3% del sistema-1, pero al mismo tiempo también obtiene un costo normalizado mínimo deteriorado que es 0.0499 más alto que el sistema-1. El Sistema-3 utiliza el nuevo procedimiento de detección basado en el índice de noticias en árbol. Requiere incluso menos tiempo de comparación que el sistema-2. Esto se debe a que las comparaciones de historias suelen arrojar mayores similitudes que las de grupos de historias, por lo que las historias tienden a combinarse en el sistema-3. Y el sistema-3 es básicamente equivalente al sistema-1 en los resultados de precisión. El System-4 ajusta los pesos de los términos basándose en la distancia de las distribuciones de términos entre todo el corpus y el conjunto de historias del clúster, lo que resulta en una mejora significativa de 0.0468 en comparación con el system-1. El mejor sistema (sistema-5) tiene un costo normalizado mínimo de 0.5012, lo cual es 0.0797 mejor que el sistema-1, y también mejor que cualquier otro resultado previamente reportado para este conjunto de datos [8, 13]. Además, el sistema-5 solo necesita 1.05e+8 veces de comparación, lo que equivale al 14.9% del sistema-1. Tabla 4. Los resultados de NED en los sistemas TDT3 Norm(CDet) Norma mínima(CDet) Tiempos de comparación Sistema-1 0.6159 0.5809 7.04e+8 Sistema-2① 0.6493 0.6308 1.29e+8 Sistema-3② 0.6197 0.5868 1.03e+8 Sistema-4② 0.5601 0.5341 1.03e+8 Sistema-5② 0.5413 0.5012 1.05e+8 Sistema-7 -- 0.5783 -Sistema-8 -- 0.5229 -① θ nuevo=0.13 ② θ inicial=0.13, λ =3, δ =0.15 La Figura 5 muestra las cinco curvas DET para nuestros sistemas en el conjunto de datos TDT3. El Sistema-5 logra el costo mínimo con una tasa de falsas alarmas de 0.0157 y una tasa de omisión de 0.4310. Podemos observar que System4 y System-5 obtienen una menor probabilidad de error de omisión en regiones de baja probabilidad de falsa alarma. La hipótesis es que se transfiere más valor de peso a los términos clave de los temas que a los términos no clave. La puntuación de similitud entre dos historias pertenecientes a diferentes temas es más baja que antes, porque los términos superpuestos suelen no ser términos clave de sus temas. 7.2 Selección de parámetros para la detección de árboles de indexación. La Figura 3 muestra los costos normalizados mínimos obtenidos por el sistema-3 en TDT3 utilizando diferentes parámetros. El parámetro de inicio θ se prueba en seis valores que van desde 0.03 hasta 0.18. Y el parámetro λ se prueba en cuatro valores: 1, 2, 3 y 4. Podemos ver que, cuando θ inicial se establece en 0.12, que es el más cercano a θ nuevo, los costos son más bajos que los demás. Esto es fácil de explicar, porque cuando se agrupan historias pertenecientes al mismo tema en un conjunto, es más razonable que el conjunto represente las historias en él. Cuando el parámetro λ se establece en 3 o 4, los costos son mejores que en otros casos, pero no hay mucha diferencia entre 3 y 4. 0 0.05 0.1 0.15 0.2 1 2 3 4 0.5 0.6 0.7 0.8 0.9 1 θ-initλ MinCost 0.6 0.65 0.7 0.75 0.8 0.85 0.9 Figura 3. Costo mínimo en TDT3 (δ =0.15) 0 0.05 0.1 0.15 0.2 1 2 3 4 0 0.5 1 1.5 2 2.5 x 10 8 θ-inicio λ Comparando tiempos 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 8 Figura 4. Comparando los tiempos en TDT3 (δ =0.15), la Figura 4 muestra los tiempos de comparación utilizados por el sistema-3 en TDT3 con los mismos parámetros que la Figura 3. Los tiempos de comparación dependen fuertemente de θ init. Debido a que cuanto mayor sea θ init, menos historias se combinan juntas, más veces de comparación se necesitan para la decisión de un nuevo evento. Por lo tanto, usamos θ init = 0.13, λ = 3, δ = 0.15 para los sistemas 3, 4 y 5. En esta configuración de parámetros, podemos obtener tanto costos normalizados mínimos bajos como menos tiempos de comparación. CONCLUSIÓN Hemos propuesto un procedimiento de detección basado en un árbol de indexación de noticias en nuestro modelo. Reduce los tiempos de comparación a aproximadamente una séptima parte del método tradicional sin afectar la precisión de NED. También hemos presentado dos extensiones al modelo básico TF-IDF. La primera extensión se realiza ajustando los pesos de los términos basados en las distribuciones de términos entre todo el corpus y un conjunto de historias de un clúster. Y la segunda extensión al modelo básico TF-IDF es un mejor uso de los tipos de términos (tipos de entidades nombradas y partes de la velocidad) de acuerdo con las categorías de noticias. Nuestros resultados experimentales en los conjuntos de datos TDT2 y TDT3 muestran que ambas extensiones contribuyen significativamente a la mejora en la precisión. No consideramos la información de tiempo de noticias como una pista para la tarea de NED, ya que la mayoría de los temas duran mucho tiempo y los conjuntos de datos de TDT solo abarcan un período relativamente corto (no más de 6 meses). Para el trabajo futuro, queremos recopilar un conjunto de noticias que abarque un período más largo de internet e integrar información temporal en la tarea de NED. Dado que el tema es un grupo de noticias relativamente grueso, también queremos refinar la granularidad del grupo a nivel de evento e identificar diferentes eventos y sus relaciones dentro de un tema. Agradecimientos: Este trabajo ha sido apoyado por la Fundación Nacional de Ciencias Naturales de China bajo la subvención No. 90604025. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son del autor(es) y no necesariamente reflejan las del patrocinador. REFERENCIAS [1] http://www.nist.gov/speech/tests/tdt/index.htm [2] En Detección y Seguimiento de Temas. Organización de la información basada en eventos. Kluwer Academic Publishers, 2002. .01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90 1 2 5 10 20 40 60 80 90 Probabilidad de Falsa Alarma (en %) Probabilidad de Omisión (en %) Curva Ponderada por Tema del SISTEMA1 Norma Mínima (Costo) del SISTEMA1 Curva Ponderada por Tema del SISTEMA2 Norma Mínima (Costo) del SISTEMA2 Curva Ponderada por Tema del SISTEMA3 Norma Mínima (Costo) del SISTEMA3 Curva Ponderada por Tema del SISTEMA4 Norma Mínima (Costo) del SISTEMA4 Curva Ponderada por Tema del SISTEMA5 Norma Mínima (Costo) del SISTEMA5 Rendimiento Aleatorio Figura 5. Curvas DET en TDT3 [3] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B.T. Archibald y X. Liu. Enfoques de aprendizaje para detectar y rastrear eventos de noticias. En el número especial de IEEE Intelligent Systems sobre Aplicaciones de Recuperación de Información Inteligente, volumen 14 (4), 1999, páginas 32-43. [4] Y. Yang, T. Pierce y J. Carbonell. Un estudio sobre la detección de eventos retrospectivos y en línea. En Actas de SIGIR-98, Melbourne, Australia, 1998, 28-36. [5] J. Allan, V. Lavrenko, D. Malin y R. Swan. Detecciones, límites y líneas de tiempo: Umass y tdt-3. En Actas del Taller de Detección y Seguimiento de Temas (TDT-3), Viena, VA, 2000, 167-174. [6] R. Papka y J. Allan. Detección de nuevos eventos en línea utilizando agrupamiento de un solo paso TÍTULO2:. Informe técnico UM-CS1998-021, 1998. [7] W. Lam, H. Meng, K. Wong y J. I'm sorry, but \"Yen\" is not a sentence. Can you please provide a sentence for me to translate into Spanish? Utilizando el Análisis Contextual para la Detección de Eventos de Noticias. Revista Internacional de Sistemas Inteligentes, 2001, 525-546. [8] B. Thorsten, C. Francine y F. Ayman. Un sistema para la detección de nuevos eventos. En Actas de la 26ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2003, 330-337. [9] S. Nicola y C. Joe. Combinando clasificadores semánticos y sintácticos de documentos para mejorar la detección de primeras noticias. En Actas de la 24ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell, and C. Jin. \n\nACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell y C. Jin. Detección de novedades condicionada al tema. En Actas de la 8ª Conferencia Internacional de ACM SIGKDD, ACM Press. 2002, 688-693. [11] M. Juha, A.M. Helena y S. Marko. Aplicando Clases Semánticas en la Detección y Seguimiento de Eventos. En Actas de la Conferencia Internacional sobre Procesamiento del Lenguaje Natural (ICON 2002), 2002, páginas 175-183. [12] M. Juha, A.M. Helena y S. Marko. Semántica simple en la detección y seguimiento de temas. Recuperación de información, 7(3-4): 2004, 347-368. [13] K. Giridhar y J. Allan. Clasificación de texto y entidades nombradas para la detección de nuevos eventos. En Actas de la 27ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2004, 297-304. [14] J. P. Callan, W. B. Croft, y S. M. Harding. El Sistema de Recuperación INQUERY. En Actas de DEXA-92, 3ra Conferencia Internacional sobre Aplicaciones de Bases de Datos y Sistemas Expertos, 1992, 78-83. [15] R. Krovetz. Viendo la morfología como un proceso de inferencia. En Actas de ACM SIGIR93, 1993, 61-81. [16] Y. Yang y J. Pedersen. Un estudio comparativo sobre la selección de características en la categorización de textos. En J. D. H. Fisher, editor, La Decimocuarta Conferencia Internacional sobre Aprendizaje Automático (ICML97), Morgan Kaufmann, 1997, 412-420. [17] T. M. Cover y J. A. I'm sorry, but \"Thomas\" is not a sentence. Can you provide more context or a complete sentence for me to translate into Spanish? Elementos de la teoría de la información. Wiley. 1991. [18] El consorcio de datos lingüísticos, http://www.ldc,upenn.edu/. [19] La definición de la tarea TDT 2001 y el plan de evaluación, http://www.nist.gov/speech/tests/tdt/tdt2001/evalplan.htm. [20] R. E. Schapire y Y. Cantante. Boostexter: Un sistema basado en Boosting para la categorización de texto. En Machine Learning 39(2/3):1, Kluwer Academic Publishers, 2000, 35-168. [21] K. Giridhar y J. Allan. 2005. Utilizando nombres y temas para la detección de nuevos eventos. En Actas de la Conferencia de Tecnología Humana y la Conferencia sobre Métodos Empíricos en Lenguaje Natural, Vancouver, 2005, 121-128 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "class of story": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "New Event Detection Based on Indexing-tree and Named Entity Zhang Kuo Tsinghua University Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Tsinghua University Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Tsinghua University Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn ABSTRACT New Event Detection (NED) aims at detecting from one or multiple streams of news stories that which one is reported on a new event (i.e.",
                "not reported previously).",
                "With the overwhelming volume of news available today, there is an increasing need for a NED system which is able to detect new events more efficiently and accurately.",
                "In this paper we propose a new NED model to speed up the NED task by using news indexing-tree dynamically.",
                "Moreover, based on the observation that terms of different types have different effects for NED task, two term reweighting approaches are proposed to improve NED accuracy.",
                "In the first approach, we propose to adjust term weights dynamically based on previous story clusters and in the second approach, we propose to employ statistics on training data to learn the named entity reweighting model for each class of stories.",
                "Experimental results on two Linguistic Data Consortium (LDC) datasets TDT2 and TDT3 show that the proposed model can improve both efficiency and accuracy of NED task significantly, compared to the baseline system and other existing systems.",
                "Categories and Subject Descriptors H.3.3 [Information Systems]: Information Search and Retrieval; H.4.2 [Information Systems Applications]: Types of Systemsdecision support.",
                "General Terms Algorithms, Performance, Experimentation 1.",
                "INTRODUCTION Topic Detection and Tracking (TDT) program aims to develop techniques which can effectively organize, search and structure news text materials from a variety of newswire and broadcast media [1].",
                "New Event Detection (NED) is one of the five tasks in TDT.",
                "It is the task of online identification of the earliest report for each topic as soon as that report arrives in the sequence of documents.",
                "A Topic is defined as a seminal event or activity, along with directly related events and activities [2].",
                "An Event is defined as something (non-trivial) happening in a certain place at a certain time [3].",
                "For instance, when a bomb explodes in a building, the exploding is the seminal event that triggers the topic, and other stories on the same topic would be those discussing salvaging efforts, the search for perpetrators, arrests and trial and so on.",
                "Useful news information is usually buried in a mass of data generated everyday.",
                "Therefore, NED systems are very useful for people who need to detect novel information from real-time news stream.",
                "These real-life needs often occur in domains like financial markets, news analysis, and intelligence gathering.",
                "In most of state-of-the-art (currently) NED systems, each news story on hand is compared to all the previous received stories.",
                "If all the similarities between them do not exceed a threshold, then the story triggers a new event.",
                "They are usually in the form of cosine similarity or Hellinger similarity metric.",
                "The core problem of NED is to identify whether two stories are on the same topic.",
                "Obviously, these systems cannot take advantage of topic information.",
                "Further more, it is not acceptable in real applications because of the large amount of computation required in the NED process.",
                "Other systems organize previous stories into clusters (each cluster corresponds to a topic), and new story is compared to the previous clusters instead of stories.",
                "This manner can reduce comparing times significantly.",
                "Nevertheless, it has been proved that this manner is less accurate [4, 5].",
                "This is because sometimes stories within a topic drift far away from each other, which could lead low similarity between a story and its topic.",
                "On the other hand, some proposed NED systems tried to improve accuracy by making better use of named entities [10, 11, 12, 13].",
                "However, none of the systems have considered that terms of different types (e.g.",
                "Noun, Verb or Person name) have different effects for different classes of stories in determining whether two stories are on the same topic.",
                "For example, the names of election candidates (Person name) are very important for stories of election class; the locations (Location name) where accidents happened are important for stories of accidents class.",
                "So, in NED, there still exist following three problems to be investigated: (1) How to speed up the detection procedure while do not decrease the detection accuracy? (2) How to make good use of cluster (topic) information to improve accuracy? (3) How to obtain better news story representation by better understanding of named entities.",
                "Driven by these problems, we have proposed three approaches in this paper. (1)To make the detection procedure faster, we propose a new NED procedure based on news indexing-tree created dynamically.",
                "Story indexing-tree is created by assembling similar stories together to form news clusters in different hierarchies according to their values of similarity.",
                "Comparisons between current story and previous clusters could help find the most similar story in less comparing times.",
                "The new procedure can reduce the amount of comparing times without hurting accuracy. (2)We use the clusters of the first floor in the indexing-tree as news topics, in which term weights are adjusted dynamically according to term distribution in the clusters.",
                "In this approach, cluster (topic) information is used properly, so the problem of theme decentralization is avoided. (3)Based on observations on the statistics obtained from training data, we found that terms of different types (e.g.",
                "Noun and Verb) have different effects for different classes of stories in determining whether two stories are on the same topic.",
                "And we propose to use statistics to optimize the weights of the terms of different types in a story according to the news class that the story belongs to.",
                "On TDT3 dataset, the new NED model just uses 14.9% comparing times of the basic model, while its minimum normalized cost is 0.5012, which is 0.0797 better than the basic model, and also better than any other results previously reported for this dataset [8, 13].",
                "The rest of the paper is organized as follows.",
                "We start off this paper by summarizing the previous work in NED in section 2.",
                "Section 3 presents the basic model for NED that most current systems use.",
                "Section 4 describes our new detection procedure based on news indexing-tree.",
                "In section 5, two term reweighting methods are proposed to improve NED accuracy.",
                "Section 6 gives our experimental data and evaluation metrics.",
                "We finally wrap up with the experimental results in Section 7, and the conclusions and future work in Section 8. 2.",
                "RELATED WORK Papka et al. proposed Single-Pass clustering on NED [6].",
                "When a new story was encountered, it was processed immediately to extract term features and a query representation of the storys content is built up.",
                "Then it was compared with all the previous queries.",
                "If the document did not trigger any queries by exceeding a threshold, it was marked as a new event.",
                "Lam et al build up previous query representations of story clusters, each of which corresponds to a topic [7].",
                "In this manner comparisons happen between stories and clusters.",
                "Recent years, most work focus on proposing better methods on comparison of stories and document representation.",
                "Brants et al. [8] extended a basic incremental TF-IDF model to include sourcespecific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, term reweighting based on inverse event frequencies, and segmentation of documents.",
                "Good improvements on TDT bench-marks were shown.",
                "Stokes et al. [9] utilized a combination of evidence from two distinct representations of a documents content.",
                "One of the representations was the usual free text vector, the other made use of lexical chains (created using WordNet) to build another term vector.",
                "Then the two representations are combined in a linear fashion.",
                "A marginal increase in effectiveness was achieved when the combined representation was used.",
                "Some efforts have been done on how to utilize named entities to improve NED.",
                "Yang et al. gave location named entities four times weight than other terms and named entities [10].",
                "DOREMI research group combined semantic similarities of person names, location names and time together with textual similarity [11][12].",
                "UMass [13] research group split document representation into two parts: named entities and non-named entities.",
                "And it was found that some classes of news could achieve better performance using named entity representation, while some other classes of news could achieve better performance using non-named entity representation.",
                "Both [10] and [13] used text categorization technique to classify news stories in advance.",
                "In [13] news stories are classified automatically at first, and then test sensitivities of names and non-name terms for NED for each class.",
                "In [10] frequent terms for each class are removed from document representation.",
                "For example, word election does not help identify different elections.",
                "In their work, effectiveness of different kinds of names (or terms with different POS) for NED in different news classes are not investigated.",
                "We use statistical analysis to reveal the fact and use it to improve NED performance. 3.",
                "BASIC MODEL In this section, we present the basic New Event Detection model which is similar to what most current systems apply.",
                "Then, we propose our new model by extending the basic model.",
                "New Event Detection systems use news story stream as input, in which stories are strictly time-ordered.",
                "Only previously received stories are available when dealing with current story.",
                "The output is a decision for whether the current story is on a new event or not and the confidence of the decision.",
                "Usually, a NED model consists of three parts: story representation, similarity calculation and detection procedure. 3.1 Story Representation Preprocessing is needed before generating story representation.",
                "For preprocessing, we tokenize words, recognize abbreviations, normalize abbreviations, add part-of-speech tags, remove stopwords included in the stop list used in InQuery [14], replace words with their stems using K-stem algorithm[15], and then generate word vector for each news story.",
                "We use incremental TF-IDF model for term weight calculation [4].",
                "In a TF-IDF model, term frequency in a news document is weighted by the inverse document frequency, which is generated from training corpus.",
                "When a new term occurs in testing process, there are two solutions: simply ignore the new term or set df of the term as a small const (e.g. df = 1).",
                "The new term receives too low weight in the first solution (0) and too high weight in the second solution.",
                "In incremental TF-IDF model, document frequencies are updated dynamically in each time step t: 1( ) ( ) ( )t t D tdf w df w df w−= + (1) where Dt represents news story set received in time t, and dfDt(w) means the number of documents that term w occurs in, and dft(w) means the total number of documents that term w occurs in before time t. In this work, each time window includes 50 news stories.",
                "Thus, each story d received in t is represented as follows: 1 2{ ( , , ), ( , , ),..., ( , , )}nd weight d t w weight d t w weight d t w→ where n means the number of distinct terms in story d, and ( , , )weight d t w means the weight of term w in story d at time t: log( ( , ) 1) log(( 1) /( ( ) 0.5)) ( , , ) log( ( , ) 1) log(( 1) /( ( ) 0.5)) t t t t w d tf d w N df w weight d t w tf d w N df w ∈ + + + = + + +∑ (2) where Nt means the total number of news stories before time t, and tf(d,w) means how many times term w occurs in news story d. 3.2 Similarity Calculation We use Hellinger distance for the calculation of similarity between two stories, for two stories d and d at time t, their similarity is defined as follows: , ( , , ) ( , , ) * ( , , ) w d d sim d d t weight d t w weight d t w ∈ = ∑ (3) 3.3 Detection Procedure For each story d received in time step t, the value ( ) ( ) ( ) ( ( , , )) time d time d n d max sim d d t < = (4) is a score used to determine whether d is a story about a new topic and at the same time is an indication of the confidence in our decision [8]. time(d) means the publication time of story d. If the score exceeds the thresholdθ new, then there exists a sufficiently similar document, thus d is a old story, otherwise, there is no sufficiently similar previous document, thus d is an new story. 4.",
                "New NED Procedure Traditional NED systems can be classified into two main types on the aspect of detection procedure: (1) S-S type, in which the story on hand is compared to each story received previously, and use the highest similarity to determine whether current story is about a new event; (2) S-C type, in which the story on hand is compared to all previous clusters each of which representing a topic, and the highest similarity is used for final decision for current story.",
                "If the highest similarity exceeds thresholdθ new, then it is an old story, and put it into the most similar cluster; otherwise it is a new story and create a new cluster.",
                "Previous work show that the first manner is more accurate than the second one [4][5].",
                "Since sometimes stories within a topic drift far away from each other, a story may have very low similarity with its topic.",
                "So using similarities between stories for determining new story is better than using similarities between story and clusters.",
                "Nevertheless, the first manner needs much more comparing times which means the first manner is low efficient.",
                "We propose a new detection procedure which uses comparisons with previous clusters to help find the most similar story in less comparing times, and the final new event decision is made according to the most similar story.",
                "Therefore, we can get both the accuracy of S-S type methods and the efficiency of S-C type methods.",
                "The new procedure creates a news indexing-tree dynamically, in which similar stories are put together to form a hierarchy of clusters.",
                "We index similar stories together by their common ancestor (a cluster node).",
                "Dissimilar stories are indexed in different clusters.",
                "When a story is coming, we use comparisons between the current story and previous hierarchical clusters to help find the most similar story which is useful for new event decision.",
                "After the new event decision is made, the current story is inserted to the indexing-tree for the following detection.",
                "The news indexing-tree is defined formally as follows: S-Tree = {r, NC , NS , E} where r is the root of S-Tree, NC is the set of all cluster nodes, NS is the set of all story nodes, and E is the set of all edges in S-Tree.",
                "We define a set of constraints for a S-Tree: ⅰ . , is an non-terminal node in the treeC i i N i∀ ∈ → ⅱ . , is a terminal node in the treeS i i N i∀ ∈ → ⅲ . , out degree of is at least 2C i i N i∀ ∈ → ⅳ . , is represented as the centroid of its desendantsC i i iN∀ ∈ → For a news story di, the comparison procedure and inserting procedure based on indexing-tree are defined as follows.",
                "An example is shown by Figure 1 and Figure 2.",
                "Figure 1.",
                "Comparison procedure Figure 2.",
                "Inserting procedure Comparison procedure: Step 1: compare di to all the direct child nodes of r and select λ nodes with highest similarities, e.g., C1 2 and C1 3 in Figure 1.",
                "Step 2: for each selected node in the last step, e.g.",
                "C1 2, compare di to all its direct child nodes, and select λ nodes with highest similarities, e.g.",
                "C2 2 and d8.",
                "Repeat step 2 for all non-terminal nodes.",
                "Step 3: record the terminal node with the highest similarty to di, e.g. s5, and the similarity value (0.20).",
                "Inserting di to the S-tree with r as root: Find the node n which is direct child of r in the path from r to the terminal node with highest similarity s, e.g.",
                "C1 2.",
                "If s is smaller than θ init+(h-1)δ , then add di to the tree as a direct child of r. Otherwise, if n is a terminal node, then create a cluster node instead of n, and add both n and di as its direct children; if n is an non-terminal node, then repeat this procedure and insert di to the sub-tree with n as root recursively.",
                "Here h is the length between n and the root of S-tree.",
                "The more the stories in a cluster similar to each other, the better the cluster represents the stories in it.",
                "Hence we add no constraints on the maximum of trees height and degree of a node.",
                "Therefore, we cannot give the complexity of this indexing-tree based procedure.",
                "But we will give the number of comparing times needed by the new procedure in our experiments in section7. 5.",
                "Term Reweighting Methods In this section, two term reweighting methods are proposed to improve NED accuracy.",
                "In the first method, a new way is explored for better using of cluster (topic) information.",
                "The second one finds a better way to make use of named entities based on news classification. 5.1 Term Reweighting Based on Distribution Distance TF-IDF is the most prevalent model used in information retrieval systems.",
                "The basic idea is that the fewer documents a term appears in, the more important the term is in discrimination of documents (relevant or not relevant to a query containing the term).",
                "Nevertheless, in TDT domain, we need to discriminate documents with regard to topics rather than queries.",
                "Intuitively, using cluster (topic) vectors to compare with subsequent news stories should outperform using story vectors.",
                "Unfortunately, the experimental results do not support this intuition [4][5].",
                "Based on observation on data, we find the reason is that a news topic usually contains many directly or indirectly related events, while they all have their own sub-subjects which are usually different with each other.",
                "Take the topic described in section 1 as an example, events like the explosion and salvage have very low similarities with events about criminal trial, therefore stories about trial would have low similarity with the topic vector built on its previous events.",
                "This section focuses on how to effectively make use of topic information and at the same time avoid the problem of content decentralization.",
                "At first, we classify terms into 5 classes to help analysis the needs of the modified model: Term class A: terms that occur frequently in the whole corpus, e.g., year and people.",
                "Terms of this class should be given low weights because they do not help much for topic discrimination.",
                "Term class B: terms that occur frequently within a news category, e.g., election, storm.",
                "They are useful to distinguish two stories in different news categories.",
                "However, they cannot provide information to determine whether two stories are on the same or different topics.",
                "In another words, term election and term storm are not helpful in differentiate two election campaigns and two storm disasters.",
                "Therefore, terms of this class should be assigned lower weights.",
                "Term class C: terms that occur frequently in a topic, and infrequently in other topics, e.g., the name of a crash plane, the name of a specific hurricane.",
                "News stories that belong to different topics rarely have overlap terms in this class.",
                "The more frequently a term appears in a topic, the more important the term is for a story belonging to the topic, therefore the term should be set higher weight.",
                "Term class D: terms that appear in a topic exclusively, but not frequently.",
                "For example, the name of a fireman who did very well in a salvage action, which may appears in only two or three stories but never appeared in other topics.",
                "Terms of this type should receive more weights than in TF-IDF model.",
                "However, since they are not popular in the topic, it is not appropriate to give them too high weights.",
                "Term class E: terms with low document frequency, and appear in different topics.",
                "Terms of this class should receive lower weights.",
                "Now we analyze whether TF-IDF model can give proper weights to the five classes of terms.",
                "Obviously, terms of class A are lowly weighted in TF-IDF model, which is conformable with the requirement described above.",
                "In TF-IDF model, terms of class B are highly dependant with the number of stories in a news class.",
                "TF-IDF model cannot provide low weights if the story containing the term belongs to a relative small news class.",
                "For a term of class C, the more frequently it appears in a topic, the less weight TFIDF model gives to it.",
                "This strongly conflicts with the requirement of terms in class C. For terms of class D, TF-IDF model gives them high weights correctly.",
                "But for terms of class E, TF-IDF model gives high weights to them which are not conformable with the requirement of low weights.",
                "To sum up, terms of class B, C, E cannot be properly weighted in TF-IDF model.",
                "So, we propose a modified model to resolve this problem.",
                "When θ init andθ new are set closely, we assume that most of the stories in a first-level cluster (a direct child node of root node) are on the same topic.",
                "Therefore, we make use of a first-level cluster to capture term distribution (df for all the terms within the cluster) within the topic dynamically.",
                "KL divergence of term distribution in a first-level cluster and the whole story set is used to adjust term weights:  ( , , ) * (1 * ( || )) ( , , ) ( , , ) * (1 * ( || )) cw tw cw tw w d D weight d t w KL P P weight d t w weight d t w KL P P γ γ ∈ + = +∑ (5) where ( ) ( ) ( ) ( ) 1,cw cw c c c c df w df w p y p y N N = = − (6) ( ) ( ) ( ) ( ) 1,t t tw tw t t df w df w p y p y N N = = − (7) where dfc(w) is the number of documents containing term w within cluster C, and Nc is the number of documents in cluster C, and Nt is the total number of documents that arrive before time step t. γ is a const parameter, now is manually set 3.",
                "KL divergence is defined as follows [17]: ( ) ( || ) ( ) log ( )x p x KL P Q p x q x = ∑ (8) The basic idea is: for a story in a topic, the more a term occurs within the topic, and the less it occurs in other topics, it should be assigned higher weights.",
                "Obviously, modified model can meet all the requirements of the five term classes listed above. 5.2 Term Reweighting Based on Term Type and Story Class Previous work found that some classes of news stories could achieve good improvements by giving extra weight to named entities.",
                "But we find that terms of different types should be given different amount of extra weight for different classes of news stories.",
                "We use open-NLP1 to recognize named entity types and part-ofspeech tags for terms that appear in news stories.",
                "Named entity types include person name, organization name, location name, date, time, money and percentage, and five POSs are selected: none (NN), verb (VB), adjective (JJ), adverb (RB) and cardinal number (CD).",
                "Statistical analysis shows topic-level discriminative terms types for different classes of stories.",
                "For the sake of convenience, named entity type and part-of-speech tags are uniformly called term type in subsequent sections.",
                "Determining whether two stories are about the same topic is a basic component for NED task.",
                "So at first we use 2 χ statistic to compute correlations between terms and topics.",
                "For a term t and a topic T, a contingence table is derived: Table 1.",
                "A 2×2 Contingence Table Doc Number belong to topic T not belong to topic T include t A B not include t C D The 2 χ statistic for a specific term t with respect to topic T is defined to be [16]: 2 2 ( , ) ( ) * ( ) ( ) * ( ) * ( ) * ( ) w T A B C D AD CB A C B D A B C D χ = + + + − + + + + (9) News topics for the TDT task are further classified into 11 rules of interpretations (ROIs) 2 .",
                "The ROI can be seen as a higher level class of stories.",
                "The average correlation between a term type and a topic ROI is computed as: 2 avg 2 ( , )( ( , ) )k m m km kT R w P w TP R p w T R P χ χ ∈ ∈ ∑ ∑（ , ）= 1 1 k=1…K, m=1…M (10) where K is the number of term types (set 12 constantly in the paper).",
                "M is the number news classes (ROIs, set 11 in the paper).",
                "Pk represents the set of all terms of type k, and Rm represents the set of all topics of class m, p(t,T) means the probability that t occurs in topic T. Because of limitation of space, only parts of the term types (9 term types) and parts of news classes (8 classes) are listed in table 2 with the average correlation values between them.",
                "The statistics is derived from labeled data in TDT2 corpus. (Results in table 2 are already normalized for convenience in comparison.)",
                "The statistics in table 2 indicates the usefulness of different term types in topic discrimination with respect to different news classes.",
                "We can see that, location name is the most useful term type for three news classes: Natural Disasters, Violence or War, Finances.",
                "And for three other categories Elections, Legal/Criminal Cases, Science and Discovery, person name is the most discriminative term type.",
                "For Scandals/Hearings, date is the most important information for topic discrimination.",
                "In addition, Legal/Criminal Cases and Finance topics have higher correlation with money terms, while Science and Discovery have higher correlation with percentage terms.",
                "Non-name terms are more stable for different classes. 1 . http://opennlp.sourceforge.net/ 2 . http://projects.ldc.upenn.edu/TDT3/Guide/label.html From the analysis of table 2, it is reasonable to adjust term weight according to their term type and the news class the story belongs to.",
                "New term weights are reweighted as follows: ( ) ( ) ( ) ( ) ( , , ) * ( , , ) ( , , ) * class d D type w T class d D type w w d weight d t w weight d t w weight d t w α α ∈ = ∑ (11) where type(w) represents the type of term w, and class(d) represents the <br>class of story</br> d, c kα is reweighting parameter for news class c and term type k. In the work, we just simply use statistics in table 2 as the reweighting parameters.",
                "Even thought using the statistics directly may not the best choice, we do not discuss how to automatically obtain the best parameters.",
                "We will try to use machine learning techniques to obtain the best parameters in the future work.",
                "In the work, we use BoosTexter [20] to classify all stories into one of the 11 ROIs.",
                "BoosTexter is a boosting based machine learning program, which creates a series of simple rules for building a classifier for text or attribute-value data.",
                "We use term weight generated using TF-IDF model as feature for story classification.",
                "We trained the model on the 12000 judged English stories in TDT2, and classify the rest of the stories in TDT2 and all stories in TDT3.",
                "Classification results are used for term reweighting in formula (11).",
                "Since the class labels of topic-off stories are not given in TDT datasets, we cannot give the classification accuracy here.",
                "Thus we do not discuss the effects of classification accuracy to NED performance in the paper. 6.",
                "EXPERIMENTAL SETUP 6.1 Datasets We used two LDC [18] datasets TDT2 and TDT3 for our experiments.",
                "TDT2 contains news stories from January to June 1998.",
                "It contains around 54,000 stories from sources like ABC, Associated Press, CNN, New York Times, Public Radio International, Voice of America etc.",
                "Only English stories in the collection were considered.",
                "TDT3 contains approximately 31,000 English stories collected from October to December 1998.",
                "In addition to the sources used in TDT2, it also contains stories from NBC and MSNBC TV broadcasts.",
                "We used transcribed versions of the TV and radio broadcasts besides textual news.",
                "TDT2 dataset is labeled with about 100 topics, and approximately 12,000 English stories belong to at least one of these topics.",
                "TDT3 dataset is labeled with about 120 topics, and approximately 8000 English stories belong to at least one of these topics.",
                "All the topics are classified into 11 Rules of Interpretation: (1)Elections, (2)Scandals/Hearings, (3)Legal/Criminal Cases, (4)Natural Disasters, (5)Accidents, (6)Ongoing Violence or War, (7)Science and Discovery News, (8)Finance, (9)New Law, (10)Sports News, (11)MISC.",
                "News. 6.2 Evaluation Metric TDT uses a cost function CDet that combines the probabilities of missing a new story and a false alarm [19]: * * * *Det Miss Miss Target FA FA NontargetC C P P C P P= + (12) Table 2.",
                "Average correlation between term types and news classes where CMiss means the cost of missing a new story, PMiss means the probability of missing a new story, and PTarget means the probability of seeing a new story in the data; CFA means the cost of a false alarm, PFA means the probability of a false alarm, and PNontarget means the probability of seeing an old story.",
                "The cost CDet is normalized such that a perfect system scores 0 and a trivial system, which is the better one of mark all stories as new or old, scores 1: ( ( * , * ) ) Det Det Miss Target FA Nontarget C Norm C min C P C P = (13) New event detection system gives two outputs for each story.",
                "The first part is yes or no indicating whether the story triggers a new event or not.",
                "The second part is a score indicating confidence of the first decision.",
                "Confidence scores can be used to plot DET curve, i.e., curves that plot false alarm vs. miss probabilities.",
                "Minimum normalized cost can be determined if optimal threshold on the score were chosen. 7.",
                "EXPERIMENTAL RESULTS 7.1 Main Results To test the approaches proposed in the model, we implemented and tested five systems: System-1: this system is used as baseline.",
                "It is implemented based on the basic model described in section 3, i.e., using incremental TF-IDF model to generate term weights, and using Hellinger distance to compute document similarity.",
                "Similarity score normalization is also employed [8].",
                "S-S detection procedure is used.",
                "System-2: this system is the same as system-1 except that S-C detection procedure is used.",
                "System-3: this system is the same as system-1 except that it uses the new detection procedure which is based on indexing-tree.",
                "System-4: implemented based on the approach presented in section 5.1, i.e., terms are reweighted according to the distance between term distributions in a cluster and all stories.",
                "The new detection procedure is used.",
                "System-5: implemented based on the approach presented in section 5.2, i.e., terms of different types are reweighted according to news class using trained parameters.",
                "The new detection procedure is used.",
                "The following are some other NED systems: System-6: [21] for each pair of stories, it computes three similarity values for named entity, non-named entity and all terms respectively.",
                "And employ Support Vector Machine to predict new or old using the similarity values as features.",
                "System-7: [8] it extended a basic incremental TF-IDF model to include source-specific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, etc.",
                "System-8: [13] it split document representation into two parts: named entities and non-named entities, and choose one effective part for each news class.",
                "Table 3 and table 4 show topic-weighted normalized costs and comparing times on TDT2 and TDT3 datasets respectively.",
                "Since no heldout data set for fine-tuning the threshold θ new was available for experiments on TDT2, we only report minimum normalized costs for our systems in table 3.",
                "System-5 outperforms all other systems including system-6, and it performs only 2.78e+8 comparing times in detection procedure which is only 13.4% of system-1.",
                "Table 3.",
                "NED results on TDT2 Systems Min Norm(CDet) Cmp times System-1 0.5749 2.08e+9 System-2① 0.6673 3.77e+8 System-3② 0.5765 2.81e+8 System-4② 0.5431 2.99e+8 System-5② 0.5089 2.78e+8 System-6 0.5300 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 When evaluating on the normalized costs on TDT3, we use the optimal thresholds obtained from TDT2 data set for all systems.",
                "System-2 reduces comparing times to 1.29e+9 which is just 18.3% of system-1, but at the same time it also gets a deteriorated minimum normalized cost which is 0.0499 higher than system-1.",
                "System-3 uses the new detection procedure based on news indexing-tree.",
                "It requires even less comparing times than system-2.",
                "This is because story-story comparisons usually yield greater similarities than story-cluster ones, so stories tend to be combined Location Person Date Organization Money Percentage NN JJ CD Elections 0.37 1 0.04 0.58 0.08 0.03 0.32 0.13 0.1 Scandals/Hearings 0.66 0.62 0.28 1 0.11 0.02 0.27 0.13 0.05 Legal/Criminal Cases 0.48 1 0.02 0.62 0.15 0 0.22 0.24 0.09 Natural Disasters 1 0.27 0 0.04 0.04 0 0.25 0.04 0.02 Violence or War 1 0.36 0.02 0.14 0.02 0.04 0.21 0.11 0.02 Science and Discovery 0.11 1 0.01 0.22 0.08 0.12 0.19 0.08 0.03 Finances 1 0.45 0.04 0.98 0.13 0.02 0.29 0.06 0.05 Sports 0.16 0.27 0.01 1 0.02 0 0.11 0.03 0.01 together in system-3.",
                "And system-3 is basically equivalent to system-1 in accuracy results.",
                "System-4 adjusts term weights based on the distance of term distributions between the whole corpus and cluster story set, yielding a good improvement by 0.0468 compared to system-1.",
                "The best system (system-5) has a minimum normalized cost 0.5012, which is 0.0797 better than system-1, and also better than any other results previously reported for this dataset [8, 13].",
                "Further more, system-5 only needs 1.05e+8 comparing times which is 14.9% of system-1.",
                "Table 4.",
                "NED results on TDT3 Systems Norm(CDet) Min Norm(CDet) Cmp times System-1 0.6159 0.5809 7.04e+8 System-2① 0.6493 0.6308 1.29e+8 System-3② 0.6197 0.5868 1.03e+8 System-4② 0.5601 0.5341 1.03e+8 System-5② 0.5413 0.5012 1.05e+8 System-7 -- 0.5783 -System-8 -- 0.5229 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 Figure5 shows the five DET curves for our systems on data set TDT3.",
                "System-5 achieves the minimum cost at a false alarm rate of 0.0157 and a miss rate of 0.4310.",
                "We can observe that System4 and System-5 obtain lower miss probability at regions of low false alarm probabilities.",
                "The hypothesis is that, more weight value is transferred to key terms of topics from non-key terms.",
                "Similarity score between two stories belonging to different topics are lower than before, because their overlapping terms are usually not key terms of their topics. 7.2 Parameter selection for indexing-tree detection Figure 3 shows the minimum normalized costs obtained by system-3 on TDT3 using different parameters.",
                "Theθ init parameter is tested on six values spanning from 0.03 to 0.18.",
                "And the λ parameter is tested on four values 1, 2, 3 and 4.",
                "We can see that, whenθ init is set to 0.12, which is the closest one toθ new, the costs are lower than others.",
                "This is easy to explain, because when stories belonging to the same topic are put in a cluster, it is more reasonable for the cluster to represent the stories in it.",
                "When parameter λ is set to 3 or 4, the costs are better than other cases, but there is no much difference between 3 and 4. 0 0.05 0.1 0.15 0.2 1 2 3 4 0.5 0.6 0.7 0.8 0.9 1 θ-initλ MinCost 0.6 0.65 0.7 0.75 0.8 0.85 0.9 Figure 3.",
                "Min Cost on TDT3 (δ =0.15) 0 0.05 0.1 0.15 0.2 1 2 3 4 0 0.5 1 1.5 2 2.5 x 10 8 θ-init λ Comparingtimes 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 8 Figure 4.",
                "Comparing times on TDT3 (δ =0.15) Figure 4 gives the comparing times used by system-3 on TDT3 with the same parameters as figure 3.",
                "The comparing times are strongly dependent onθ init.",
                "Because the greaterθ init is, the less stories combined together, the more comparing times are needed for new event decision.",
                "So we useθ init =0.13,λ =3,δ =0.15 for system-3, 4, and 5.",
                "In this parameter setting, we can get both low minimum normalized costs and less comparing times. 8.",
                "CONCLUSION We have proposed a news indexing-tree based detection procedure in our model.",
                "It reduces comparing times to about one seventh of traditional method without hurting NED accuracy.",
                "We also have presented two extensions to the basic TF-IDF model.",
                "The first extension is made by adjust term weights based on term distributions between the whole corpus and a cluster story set.",
                "And the second extension to basic TF-IDF model is better use of term types (named entities types and part-of-speed) according to news categories.",
                "Our experimental results on TDT2 and TDT3 datasets show that both of the two extensions contribute significantly to improvement in accuracy.",
                "We did not consider news time information as a clue for NED task, since most of the topics last for a long time and TDT data sets only span for a relative short period (no more than 6 months).",
                "For the future work, we want to collect news set which span for a longer period from internet, and integrate time information in NED task.",
                "Since topic is a relative coarse-grained news cluster, we also want to refine cluster granularity to event-level, and identify different events and their relations within a topic.",
                "Acknowledgments This work is supported by the National Natural Science Foundation of China under Grant No. 90604025.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsor. 9.",
                "REFERENCES [1] http://www.nist.gov/speech/tests/tdt/index.htm [2] In Topic Detection and Tracking.",
                "Event-based Information Organization.",
                "Kluwer Academic Publishers, 2002. .01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90 1 2 5 10 20 40 60 80 90 False Alarm Probability (in %) MissProbability(in%) SYSTEM1 Topic Weighted Curve SYSTEM1 Min Norm(Cost) SYSTEM2 Topic Weighted Curve SYSTEM2 Min Norm(Cost) SYSTEM3 Topic Weighted Curve SYSTEM3 Min Norm(Cost) SYSTEM4 Topic Weighted Curve SYSTEM4 Min Norm(Cost) SYSTEM5 Topic Weighted Curve SYSTEM5 Min Norm(Cost) Random Performance Figure 5.",
                "DET curves on TDT3 [3] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B.T.",
                "Archibald, and X. Liu.",
                "Learning Approaches for Detecting and Tracking News Events.",
                "In IEEE Intelligent Systems Special Issue on Applications of Intelligent Information Retrieval, volume 14 (4), 1999, 32-43. [4] Y. Yang, T. Pierce, and J. Carbonell.",
                "A Study on Retrospective and On-line Event Detection.",
                "In Proceedings of SIGIR-98, Melbourne, Australia, 1998, 28-36. [5] J. Allan, V. Lavrenko, D. Malin, and R. Swan.",
                "Detections, Bounds, and Timelines: Umass and tdt-3.",
                "In Proceedings of Topic Detection and Tracking Workshop (TDT-3), Vienna, VA, 2000, 167-174. [6] R. Papka and J. Allan.",
                "On-line New Event Detection Using Single Pass Clustering TITLE2:.",
                "Technical Report UM-CS1998-021, 1998. [7] W. Lam, H. Meng, K. Wong, and J.",
                "Yen.",
                "Using Contextual Analysis for News Event Detection.",
                "International Journal on Intelligent Systems, 2001, 525-546. [8] B. Thorsten, C. Francine, and F. Ayman.",
                "A System for New Event Detection.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2003, 330-337. [9] S. Nicola and C. Joe.",
                "Combining Semantic and Syntactic Document Classifiers to Improve First Story Detection.",
                "In Proceedings of the 24th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topicconditioned Novelty Detection.",
                "In Proceedings of the 8th ACM SIGKDD International Conference, ACM Press. 2002, 688-693. [11] M. Juha, A.M. Helena, and S. Marko.",
                "Applying Semantic Classes in Event Detection and Tracking.",
                "In Proceedings of International Conference on Natural Language Processing (ICON 2002), 2002, pages 175-183. [12] M. Juha, A.M. Helena, and S. Marko.",
                "Simple Semantics in Topic Detection and Tracking.",
                "Information Retrieval, 7(3-4): 2004, 347-368. [13] K. Giridhar and J. Allan.",
                "Text Classification and Named Entities for New Event Detection.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2004, 297-304. [14] J. P. Callan, W. B. Croft, and S. M. Harding.",
                "The INQUERY Retrieval System.",
                "In Proceedings of DEXA-92, 3rd International Conference on Database and Expert Systems Applications, 1992, 78-83. [15] R. Krovetz.",
                "Viewing Morphology as An Inference Process.",
                "In Proceedings of ACM SIGIR93, 1993, 61-81. [16] Y. Yang and J. Pedersen.",
                "A Comparative Study on Feature Selection in Text Categorization.",
                "In J. D. H. Fisher, editor, The Fourteenth International Conference on Machine Learning (ICML97), Morgan Kaufmann, 1997, 412-420. [17] T. M. Cover, and J.A.",
                "Thomas.",
                "Elements of Information Theory.",
                "Wiley. 1991. [18] The linguistic data consortium, http://www.ldc,upenn.edu/. [19] The 2001 TDT task definition and evaluation plan, http://www.nist.gov/speech/tests/tdt/tdt2001/evalplan.htm. [20] R. E. Schapire and Y.",
                "Singer.",
                "Boostexter: A Boosting-based System for Text Categorization.",
                "In Machine Learning 39(2/3):1, Kluwer Academic Publishers, 2000, 35-168. [21] K. Giridhar and J. Allan. 2005.",
                "Using Names and Topics for New Event Detection.",
                "In Proceedings of Human Technology Conference and Conference on Empirical Methods in Natural Language, Vancouver, 2005, 121-128"
            ],
            "original_annotated_samples": [
                "New term weights are reweighted as follows: ( ) ( ) ( ) ( ) ( , , ) * ( , , ) ( , , ) * class d D type w T class d D type w w d weight d t w weight d t w weight d t w α α ∈ = ∑ (11) where type(w) represents the type of term w, and class(d) represents the <br>class of story</br> d, c kα is reweighting parameter for news class c and term type k. In the work, we just simply use statistics in table 2 as the reweighting parameters."
            ],
            "translated_annotated_samples": [
                "Los nuevos pesos de los términos se recalculan de la siguiente manera: ( ) ( ) ( ) ( ) ( , , ) * ( , , ) ( , , ) * clase d D tipo w T clase d D tipo w w d peso d t w peso d t w peso d t w α α ∈ = ∑ (11) donde type(w) representa el tipo del término w, y class(d) representa la clase de la historia d, c kα es el parámetro de recalibración para la clase de noticias c y el tipo de término k. En el trabajo, simplemente utilizamos las estadísticas de la tabla 2 como parámetros de recalibración."
            ],
            "translated_text": "Detección de nuevos eventos basada en árbol de indexación y entidad nombrada Zhang Kuo Universidad Tsinghua Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Universidad Tsinghua Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Universidad Tsinghua Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn RESUMEN La Detección de Nuevos Eventos (NED) tiene como objetivo detectar de una o varias corrientes de noticias cuál se informa sobre un nuevo evento (es decir, no reportado previamente). Con el abrumador volumen de noticias disponible hoy en día, hay una creciente necesidad de un sistema NED que sea capaz de detectar eventos nuevos de manera más eficiente y precisa. En este artículo proponemos un nuevo modelo de NED para acelerar la tarea de NED mediante el uso dinámico de un árbol de indexación de noticias. Además, basándose en la observación de que los términos de diferentes tipos tienen diferentes efectos para la tarea de NED, se proponen dos enfoques de reponderación de términos para mejorar la precisión de NED. En el primer enfoque, proponemos ajustar dinámicamente los pesos de los términos basados en los clusters de historias previas y en el segundo enfoque, proponemos emplear estadísticas en los datos de entrenamiento para aprender el modelo de reajuste de entidades nombradas para cada clase de historias. Los resultados experimentales en dos conjuntos de datos del Consorcio de Datos Lingüísticos (LDC), TDT2 y TDT3, muestran que el modelo propuesto puede mejorar significativamente tanto la eficiencia como la precisión de la tarea de NED, en comparación con el sistema base y otros sistemas existentes. Categorías y Descriptores de Asignaturas H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información; H.4.2 [Aplicaciones de Sistemas de Información]: Tipos de Sistemas de Apoyo a Decisiones. Términos generales Algoritmos, Rendimiento, Experimentación 1. El programa de Detección y Seguimiento de Temas (TDT) tiene como objetivo desarrollar técnicas que puedan organizar, buscar y estructurar de manera efectiva materiales de texto de noticias de una variedad de agencias de noticias y medios de difusión [1]. La Detección de Eventos Nuevos (DEN) es una de las cinco tareas en TDT. Es la tarea de identificación en línea del informe más temprano para cada tema tan pronto como ese informe llegue en la secuencia de documentos. Un tema se define como un evento o actividad seminal, junto con eventos y actividades directamente relacionados [2]. Un evento se define como algo (no trivial) que sucede en un lugar y en un momento determinados [3]. Por ejemplo, cuando una bomba explota en un edificio, la explosión es el evento seminal que desencadena el tema, y otras historias sobre el mismo tema serían aquellas que discuten los esfuerzos de salvamento, la búsqueda de los perpetradores, arrestos y juicios, entre otros. La información útil de noticias suele estar enterrada en una masa de datos generados a diario. Por lo tanto, los sistemas NED son muy útiles para las personas que necesitan detectar información novedosa de un flujo de noticias en tiempo real. Estas necesidades de la vida real a menudo se presentan en ámbitos como los mercados financieros, el análisis de noticias y la recopilación de inteligencia. En la mayoría de los sistemas NED de última generación, cada noticia disponible se compara con todas las noticias previamente recibidas. Si todas las similitudes entre ellos no superan un umbral, entonces la historia desencadena un nuevo evento. Por lo general, se presentan en forma de similitud de coseno o métrica de similitud de Hellinger. El problema central de NED es identificar si dos historias tratan sobre el mismo tema. Obviamente, estos sistemas no pueden aprovechar la información del tema. Además, no es aceptable en aplicaciones reales debido a la gran cantidad de cálculos requeridos en el proceso de NED. Otros sistemas organizan historias anteriores en grupos (cada grupo corresponde a un tema), y la nueva historia se compara con los grupos anteriores en lugar de con historias. Esta forma puede reducir significativamente los tiempos de comparación. Sin embargo, se ha demostrado que este método es menos preciso [4, 5]. Esto se debe a que a veces las historias dentro de un tema se alejan mucho entre sí, lo que podría resultar en una baja similitud entre una historia y su tema. Por otro lado, algunos sistemas NED propuestos intentaron mejorar la precisión al hacer un mejor uso de las entidades nombradas [10, 11, 12, 13]. Sin embargo, ninguno de los sistemas ha considerado que los términos de diferentes tipos (por ejemplo, Los sustantivos, verbos o nombres de personas tienen diferentes efectos en diferentes clases de historias para determinar si dos historias tratan sobre el mismo tema. Por ejemplo, los nombres de los candidatos electorales (Nombre de la persona) son muy importantes para las historias de la clase de elecciones; las ubicaciones (Nombre de la ubicación) donde ocurrieron los accidentes son importantes para las historias de la clase de accidentes. Por lo tanto, en NED, todavía existen los siguientes tres problemas por investigar: (1) ¿Cómo acelerar el procedimiento de detección sin disminuir la precisión de la detección? (2) ¿Cómo aprovechar mejor la información de los grupos (temas) para mejorar la precisión? (3) ¿Cómo obtener una mejor representación de historias de noticias mediante una mejor comprensión de las entidades nombradas? Impulsados por estos problemas, hemos propuesto tres enfoques en este documento. (1) Para hacer que el procedimiento de detección sea más rápido, proponemos un nuevo procedimiento de NED basado en un árbol de indexación de noticias creado dinámicamente. El índice de historias en árbol se crea ensamblando historias similares para formar grupos de noticias en diferentes jerarquías según sus valores de similitud. Las comparaciones entre la historia actual y los grupos anteriores podrían ayudar a encontrar la historia más similar en menos tiempo de comparación. El nuevo procedimiento puede reducir la cantidad de veces de comparación sin afectar la precisión. Utilizamos los grupos del primer piso en el árbol de indexación como temas de noticias, en los cuales los pesos de los términos se ajustan dinámicamente según la distribución de términos en los grupos. En este enfoque, se utiliza adecuadamente la información de los grupos (temas), por lo que se evita el problema de la descentralización del tema. Basándonos en observaciones de las estadísticas obtenidas de los datos de entrenamiento, encontramos que los términos de diferentes tipos (por ejemplo, Los sustantivos y verbos tienen diferentes efectos para diferentes clases de historias al determinar si dos historias tratan sobre el mismo tema. Y proponemos utilizar estadísticas para optimizar los pesos de los términos de diferentes tipos en una historia de acuerdo a la clase de noticias a la que pertenece la historia. En el conjunto de datos TDT3, el nuevo modelo NED solo utiliza un 14.9% en comparación con los tiempos del modelo básico, mientras que su costo normalizado mínimo es de 0.5012, lo que es 0.0797 mejor que el modelo básico, y también mejor que cualquier otro resultado previamente reportado para este conjunto de datos [8, 13]. El resto del documento está organizado de la siguiente manera. Comenzamos este artículo resumiendo el trabajo previo en NED en la sección 2. La sección 3 presenta el modelo básico para NED que la mayoría de los sistemas actuales utilizan. La sección 4 describe nuestro nuevo procedimiento de detección basado en el índice de árbol de noticias. En la sección 5, se proponen dos métodos de reponderación de términos para mejorar la precisión de NED. La sección 6 presenta nuestros datos experimentales y métricas de evaluación. Finalmente concluimos con los resultados experimentales en la Sección 7, y las conclusiones y trabajo futuro en la Sección 8. TRABAJO RELACIONADO Papka et al. propusieron el agrupamiento de un solo paso en NED [6]. Cuando se encontraba una nueva historia, esta era procesada inmediatamente para extraer características de términos y se construía una representación de consulta del contenido de la historia. Luego se comparó con todas las consultas anteriores. Si el documento no generaba ninguna consulta al exceder un umbral, se marcaba como un nuevo evento. Lam et al construyen representaciones de consultas previas de grupos de historias, cada una de las cuales corresponde a un tema [7]. De esta manera se realizan comparaciones entre historias y grupos. En los últimos años, la mayoría de los trabajos se han centrado en proponer mejores métodos para la comparación de historias y la representación de documentos. Brants et al. [8] extendieron un modelo básico incremental de TF-IDF para incluir modelos específicos de fuente, normalización de puntajes de similitud basada en promedios específicos de documentos, normalización de puntajes de similitud basada en promedios específicos de pares de fuentes, reponderación de términos basada en frecuencias de eventos inversas y segmentación de documentos. Se mostraron buenas mejoras en los puntos de referencia de TDT. Stokes et al. [9] utilizaron una combinación de evidencia de dos representaciones distintas del contenido de un documento. Una de las representaciones era el vector de texto libre habitual, la otra hacía uso de cadenas léxicas (creadas utilizando WordNet) para construir otro vector de términos. Luego, las dos representaciones se combinan de forma lineal. Se logró un aumento marginal en la efectividad cuando se utilizó la representación combinada. Algunos esfuerzos se han realizado sobre cómo utilizar entidades nombradas para mejorar la desambiguación de entidades nombradas (NED). Yang et al. asignaron un peso cuatro veces mayor a las entidades con nombres de ubicación que a otros términos y entidades con nombres [10]. El grupo de investigación DOREMI combinó similitudes semánticas de nombres de personas, nombres de lugares y tiempo junto con similitud textual [11][12]. El grupo de investigación de UMass [13] dividió la representación del documento en dos partes: entidades nombradas y entidades no nombradas. Y se descubrió que algunas clases de noticias podían lograr un mejor rendimiento utilizando la representación de entidades nombradas, mientras que otras clases de noticias podían lograr un mejor rendimiento utilizando la representación sin entidades nombradas. Tanto [10] como [13] utilizaron la técnica de categorización de texto para clasificar previamente las noticias. En [13] las noticias se clasifican automáticamente al principio, y luego se prueban las sensibilidades de los nombres y términos que no son nombres para NED para cada clase. En [10], se eliminan los términos frecuentes de cada clase de la representación del documento. Por ejemplo, la palabra \"elección\" no ayuda a identificar diferentes elecciones. En su trabajo, no se investiga la efectividad de diferentes tipos de nombres (o términos con diferentes categorías gramaticales) para NED en diferentes clases de noticias. Utilizamos análisis estadístico para revelar el hecho y utilizarlo para mejorar el rendimiento de NED. 3. MODELO BÁSICO En esta sección, presentamos el modelo básico de Detección de Nuevos Eventos que es similar a lo que la mayoría de los sistemas actuales aplican. Luego, proponemos nuestro nuevo modelo ampliando el modelo básico. Los sistemas de detección de eventos nuevos utilizan flujos de noticias como entrada, en los cuales las historias están estrictamente ordenadas por tiempo. Solo están disponibles las historias recibidas previamente al tratar con la historia actual. La salida es una decisión sobre si la historia actual es un evento nuevo o no y la confianza de la decisión. Por lo general, un modelo NED consta de tres partes: representación de la historia, cálculo de similitud y procedimiento de detección. 3.1 Representación de la historia Se necesita preprocesamiento antes de generar la representación de la historia. Para el preprocesamiento, tokenizamos palabras, reconocemos abreviaturas, normalizamos abreviaturas, agregamos etiquetas de partes del discurso, eliminamos palabras vacías incluidas en la lista de paradas utilizada en InQuery [14], reemplazamos palabras con sus raíces utilizando el algoritmo K-stem[15], y luego generamos un vector de palabras para cada noticia. Utilizamos el modelo TF-IDF incremental para el cálculo del peso de los términos [4]. En un modelo TF-IDF, la frecuencia de término en un documento de noticias se pondera por la frecuencia inversa del documento, la cual se genera a partir del corpus de entrenamiento. Cuando surge un nuevo término en el proceso de prueba, hay dos soluciones: simplemente ignorar el nuevo término o establecer df del término como una constante pequeña (por ejemplo, df = 1). El nuevo término recibe un peso demasiado bajo en la primera solución (0) y un peso demasiado alto en la segunda solución. En el modelo TF-IDF incremental, las frecuencias de documentos se actualizan dinámicamente en cada paso de tiempo t: 1( ) ( ) ( )t t D tdf w df w df w−= + (1) donde Dt representa el conjunto de noticias recibidas en el tiempo t, y dfDt(w) significa el número de documentos en los que aparece el término w, y dft(w) significa el número total de documentos en los que aparece el término w antes del tiempo t. En este trabajo, cada ventana de tiempo incluye 50 noticias. Así, cada historia d recibida en t se representa de la siguiente manera: 1 2{ ( , , ), ( , , ),..., ( , , )}nd peso d t w peso d t w peso d t w→ donde n significa el número de términos distintos en la historia d, y ( , , )peso d t w significa el peso del término w en la historia d en el tiempo t: log( ( , ) 1) log(( 1) /( ( ) 0.5)) ( , , ) log( ( , ) 1) log(( 1) /( ( ) 0.5)) t t t t w d tf d w N df w peso d t w tf d w N df w ∈ + + + = + + +∑ (2) donde Nt significa el número total de historias de noticias antes del tiempo t, y tf(d,w) significa cuántas veces aparece el término w en la historia de noticias d. 3.2 Cálculo de Similitud Utilizamos la distancia de Hellinger para el cálculo de la similitud entre dos historias, para dos historias d y d en el tiempo t, su similitud se define de la siguiente manera: , ( , , ) ( , , ) * ( , , ) w d d sim d d t peso d t w peso d t w ∈ = ∑ (3) 3.3 Procedimiento de Detección Para cada historia d recibida en el paso de tiempo t, el valor ( ) ( ) ( ) ( ( , , )) tiempo d tiempo d n d max sim d d t < = (4) es un puntaje utilizado para determinar si d es una historia sobre un nuevo tema y al mismo tiempo es una indicación de la confianza en nuestra decisión [8]. tiempo(d) significa el tiempo de publicación de la historia d. Si el puntaje supera el umbral θ new, entonces existe un documento suficientemente similar, por lo tanto, d es una historia antigua, de lo contrario, no hay un documento previo suficientemente similar, por lo tanto, d es una historia nueva. 4. Nuevo procedimiento NED Los sistemas NED tradicionales se pueden clasificar en dos tipos principales en cuanto al procedimiento de detección: (1) tipo S-S, en el que la historia en cuestión se compara con cada historia recibida previamente, y se utiliza la mayor similitud para determinar si la historia actual trata sobre un evento nuevo; (2) tipo S-C, en el que la historia en cuestión se compara con todos los clusters anteriores, cada uno de los cuales representa un tema, y se utiliza la mayor similitud para la decisión final sobre la historia actual. Si la similitud más alta supera el umbral θ nuevo, entonces es una historia antigua y se coloca en el clúster más similar; de lo contrario, es una historia nueva y se crea un nuevo clúster. Trabajos anteriores muestran que la primera forma es más precisa que la segunda [4][5]. Dado que a veces las historias dentro de un tema se alejan mucho entre sí, una historia puede tener una similitud muy baja con su tema. Por lo tanto, utilizar similitudes entre historias para determinar una nueva historia es mejor que utilizar similitudes entre historias y grupos. Sin embargo, el primer método requiere mucho más tiempo de comparación, lo que significa que es menos eficiente. Proponemos un nuevo procedimiento de detección que utiliza comparaciones con clusters anteriores para ayudar a encontrar la historia más similar en menos tiempo de comparación, y la decisión final sobre el nuevo evento se toma de acuerdo con la historia más similar. Por lo tanto, podemos obtener tanto la precisión de los métodos de tipo S-S como la eficiencia de los métodos de tipo S-C. El nuevo procedimiento crea un árbol de indexación de noticias dinámicamente, en el cual se agrupan historias similares para formar una jerarquía de clusters. Indexamos historias similares juntas por su ancestro común (un nodo de clúster). Historias diferentes están indexadas en diferentes grupos. Cuando una historia está llegando, utilizamos comparaciones entre la historia actual y los grupos jerárquicos anteriores para ayudar a encontrar la historia más similar que es útil para la toma de decisiones sobre nuevos eventos. Después de que se tome la decisión sobre el nuevo evento, la historia actual se inserta en el árbol de indexación para la detección siguiente. El índice de noticias en forma de árbol se define formalmente de la siguiente manera: S-Tree = {r, NC, NS, E}, donde r es la raíz de S-Tree, NC es el conjunto de todos los nodos de clúster, NS es el conjunto de todos los nodos de historia, y E es el conjunto de todos los bordes en S-Tree. Definimos un conjunto de restricciones para un S-Árbol: ⅰ. , es un nodo no terminal en el árbolC i i N i∀ ∈ → ⅱ. , es un nodo terminal en el árbolS i i N i∀ ∈ → ⅲ. , el grado de salida de es al menos 2C i i N i∀ ∈ → ⅳ. , se representa como el centroide de sus descendientesC i i iN∀ ∈ → Para una noticia di, el procedimiento de comparación y el procedimiento de inserción basados en el árbol de indexación se definen de la siguiente manera. Un ejemplo se muestra en la Figura 1 y la Figura 2. Figura 1. Procedimiento de comparación Figura 2. Procedimiento de comparación de inserción: Paso 1: comparar di con todos los nodos hijos directos de r y seleccionar λ nodos con las similitudes más altas, por ejemplo, C1 2 y C1 3 en la Figura 1. Paso 2: para cada nodo seleccionado en el paso anterior, por ejemplo. C1 2, compara di con todos sus nodos hijos directos y selecciona λ nodos con las similitudes más altas, por ejemplo. C2 2 y d8. Repetir el paso 2 para todos los nodos no terminales. Paso 3: registrar el nodo terminal con la mayor similitud a di, por ejemplo, s5, y el valor de similitud (0.20). Insertando di en el S-árbol con r como raíz: Encuentra el nodo n que es hijo directo de r en el camino desde r hasta el nodo terminal con la similitud más alta s, por ejemplo. C1 2. Si s es menor que θ init + (h-1)δ, entonces agregar di al árbol como un hijo directo de r. De lo contrario, si n es un nodo terminal, crear un nodo de clúster en lugar de n, y agregar tanto n como di como sus hijos directos; si n es un nodo no terminal, repetir este procedimiento e insertar di en el subárbol con n como raíz de forma recursiva. Aquí h es la longitud entre n y la raíz del árbol S. Cuanto más similares sean las historias en un grupo, mejor representará el grupo a las historias en él. Por lo tanto, no imponemos restricciones en la altura máxima de los árboles y el grado de un nodo. Por lo tanto, no podemos dar la complejidad de este procedimiento basado en árboles de indexación. Pero proporcionaremos el número de veces de comparación necesarias por el nuevo procedimiento en nuestros experimentos en la sección 7.5. En esta sección, se proponen dos métodos de reponderación de términos para mejorar la precisión de NED. En el primer método, se explora una nueva forma de utilizar de manera más efectiva la información de los grupos (temas). El segundo encuentra una mejor manera de utilizar entidades nombradas basadas en la clasificación de noticias. 5.1 Reajuste de términos basado en la distancia de distribución TF-IDF es el modelo más prevalente utilizado en sistemas de recuperación de información. La idea básica es que cuanto menos documentos aparezca un término, más importante es el término en la discriminación de documentos (relevantes o no relevantes para una consulta que contenga el término). Sin embargo, en el dominio de TDT, necesitamos discriminar documentos en función de los temas en lugar de las consultas. Intuitivamente, el uso de vectores de clúster (tema) para comparar con historias de noticias posteriores debería tener un mejor rendimiento que el uso de vectores de historias. Desafortunadamente, los resultados experimentales no respaldan esta intuición [4][5]. Basándonos en la observación de los datos, encontramos que la razón es que un tema de noticias generalmente contiene muchos eventos directa o indirectamente relacionados, y todos ellos tienen sus propios subtemas que suelen ser diferentes entre sí. Tomemos el tema descrito en la sección 1 como ejemplo, eventos como la explosión y el rescate tienen similitudes muy bajas con eventos sobre juicios criminales, por lo tanto, las historias sobre juicios tendrían una baja similitud con el vector de tema construido en base a sus eventos previos. Esta sección se centra en cómo hacer un uso efectivo de la información del tema y al mismo tiempo evitar el problema de la descentralización del contenido. Al principio, clasificamos los términos en 5 clases para ayudar en el análisis de las necesidades del modelo modificado: Clase de términos A: términos que ocurren con frecuencia en todo el corpus, por ejemplo, año y personas. Los términos de esta clase deben recibir pesos bajos porque no ayudan mucho en la discriminación de temas. Clase B de términos: términos que ocurren con frecuencia dentro de una categoría de noticias, por ejemplo, elección, tormenta. Son útiles para distinguir dos historias en diferentes categorías de noticias. Sin embargo, no pueden proporcionar información para determinar si dos historias tratan sobre el mismo tema o temas diferentes. En otras palabras, los términos elección y tormenta no son útiles para diferenciar dos campañas electorales y dos desastres de tormenta. Por lo tanto, los términos de esta clase deberían asignarse pesos más bajos. Clase de término C: términos que ocurren con frecuencia en un tema y raramente en otros temas, por ejemplo, el nombre de un avión accidentado, el nombre de un huracán específico. Las noticias que pertenecen a diferentes temas rara vez tienen términos superpuestos en esta clase. Cuanto más frecuentemente aparezca un término en un tema, más importante es el término para una historia perteneciente al tema, por lo tanto, el término debería tener un peso más alto. Clase de término D: términos que aparecen en un tema de manera exclusiva, pero no con frecuencia. Por ejemplo, el nombre de un bombero que se desempeñó muy bien en una acción de salvamento, que puede aparecer en solo dos o tres historias pero nunca ha aparecido en otros temas. Los términos de este tipo deberían recibir más peso que en el modelo TF-IDF. Sin embargo, dado que no son populares en el tema, no es apropiado asignarles pesos demasiado altos. Clase de término E: términos con baja frecuencia en documentos y que aparecen en diferentes temas. Los términos de esta clase deberían recibir pesos más bajos. Ahora analizamos si el modelo TF-IDF puede asignar pesos adecuados a las cinco clases de términos. Obviamente, los términos de la clase A tienen un peso bajo en el modelo TF-IDF, lo cual es acorde con el requisito descrito anteriormente. En el modelo TF-IDF, los términos de la clase B dependen altamente del número de historias en una clase de noticias. El modelo TF-IDF no puede proporcionar pesos bajos si la historia que contiene el término pertenece a una clase de noticias relativamente pequeña. Para un término de clase C, cuanto más frecuentemente aparezca en un tema, menos peso le da el modelo TFIDF. Esto entra en conflicto directo con el requisito de términos en la clase C. Para los términos de la clase D, el modelo TF-IDF les asigna pesos altos de manera correcta. Pero para los términos de la clase E, el modelo TF-IDF asigna pesos altos a ellos que no son conformes con el requisito de pesos bajos. En resumen, los términos de clase B, C, E no pueden ser ponderados adecuadamente en el modelo TF-IDF. Por lo tanto, proponemos un modelo modificado para resolver este problema. Cuando θ init y θ new están configurados de cerca, asumimos que la mayoría de las historias en un clúster de primer nivel (un nodo hijo directo del nodo raíz) están en el mismo tema. Por lo tanto, utilizamos un clúster de primer nivel para capturar la distribución de términos (df para todos los términos dentro del clúster) dentro del tema de forma dinámica. La divergencia de Kullback-Leibler de la distribución de términos en un clúster de primer nivel y el conjunto completo de historias se utiliza para ajustar los pesos de los términos: ( , , ) * (1 * ( || )) ( , , ) ( , , ) * (1 * ( || )) cw tw cw tw w d D peso d t w KL P P peso d t w peso d t w KL P P γ γ ∈ + = +∑ (5) donde ( ) ( ) ( ) ( ) 1,cw cw c c c c df w df w p y p y N N = = − (6) ( ) ( ) ( ) ( ) 1,t t tw tw t t df w df w p y p y N N = = − (7) donde dfc(w) es el número de documentos que contienen el término w dentro del clúster C, y Nc es el número de documentos en el clúster C, y Nt es el número total de documentos que llegan antes del paso de tiempo t. γ es un parámetro constante, actualmente configurado manualmente en 3. La divergencia de Kullback-Leibler se define de la siguiente manera [17]: La idea básica es que, para una historia en un tema, cuanto más ocurre un término dentro del tema y menos ocurre en otros temas, se le deben asignar pesos más altos. Obviamente, el modelo modificado puede cumplir con todos los requisitos de las cinco clases de términos mencionadas anteriormente. 5.2 Reajuste de Términos Basado en el Tipo de Término y la Clase de Historia. Trabajos anteriores encontraron que algunas clases de historias de noticias podrían lograr mejoras significativas al dar un peso adicional a las entidades nombradas. Pero encontramos que los términos de diferentes tipos deben recibir una cantidad diferente de peso adicional para diferentes clases de noticias. Utilizamos open-NLP1 para reconocer tipos de entidades nombradas y etiquetas de partes del discurso para términos que aparecen en noticias. Los tipos de entidades nombradas incluyen nombre de persona, nombre de organización, nombre de ubicación, fecha, hora, dinero y porcentaje, y se seleccionan cinco partes del discurso: ninguno (NN), verbo (VB), adjetivo (JJ), adverbio (RB) y número cardinal (CD). El análisis estadístico muestra tipos de términos discriminativos a nivel de tema para diferentes clases de historias. Por conveniencia, los tipos de entidad nombrada y etiquetas de parte de la oración se llaman uniformemente tipo de término en las secciones siguientes. Determinar si dos historias tratan sobre el mismo tema es un componente básico para la tarea de NED. Por lo tanto, al principio usamos la estadística χ² para calcular las correlaciones entre términos y temas. Para un término t y un tema T, se deriva una tabla de contingencia: Tabla 1. Una tabla de contingencia de 2×2 con el número de documentos pertenecientes al tema T y no pertenecientes al tema T incluye t A B no incluye t C D. La estadística χ² para un término específico t con respecto al tema T se define como [16]: χ² = ( ) ( ) * ( ) ( ) * ( ) * ( ) * ( ) w T A B C D AD CB A C B D A B C D + + + − + + + + (9). Los temas de noticias para la tarea TDT se clasifican además en 11 reglas de interpretaciones (ROIs) 2. El ROI se puede ver como una clase de historias de nivel superior. El promedio de correlación entre un tipo de término y un ROI de tema se calcula como: 2 avg 2 ( , )( ( , ) )k m m km kT R w P w TP R p w T R P χ χ ∈ ∈ ∑ ∑（ , ）= 1 1 k=1…K, m=1…M (10) donde K es el número de tipos de términos (fijado constantemente en 12 en el artículo). M es el número de nuevas clases (ROIs, establecido en el conjunto 11 del artículo). Pk representa el conjunto de todos los términos de tipo k, y Rm representa el conjunto de todos los temas de la clase m, p(t,T) significa la probabilidad de que t ocurra en el tema T. Debido a limitaciones de espacio, solo se enumeran partes de los tipos de términos (9 tipos de términos) y partes de las clases de noticias (8 clases) en la tabla 2 con los valores promedio de correlación entre ellos. Las estadísticas se derivan de datos etiquetados en el corpus TDT2. (Los resultados en la tabla 2 ya están normalizados para mayor comodidad en la comparación). Las estadísticas en la tabla 2 indican la utilidad de diferentes tipos de términos en la discriminación de temas con respecto a diferentes clases de noticias. Podemos ver que el nombre de la ubicación es el tipo de término más útil para tres clases de noticias: Desastres Naturales, Violencia o Guerra, Finanzas. Y para otras tres categorías: Elecciones, Casos Legales/Criminales, Ciencia y Descubrimiento, el nombre de la persona es el tipo de término más discriminativo. Para Escándalos/Audiencias, la fecha es la información más importante para la discriminación de temas. Además, los casos legales/criminales y los temas financieros tienen una correlación más alta con términos relacionados con el dinero, mientras que la ciencia y el descubrimiento tienen una correlación más alta con términos de porcentaje. Los términos no nominales son más estables para diferentes clases. 1. http://opennlp.sourceforge.net/ 2. http://projects.ldc.upenn.edu/TDT3/Guide/label.html A partir del análisis de la tabla 2, es razonable ajustar el peso de los términos según su tipo de término y la clase de noticias a la que pertenece la historia. Los nuevos pesos de los términos se recalculan de la siguiente manera: ( ) ( ) ( ) ( ) ( , , ) * ( , , ) ( , , ) * clase d D tipo w T clase d D tipo w w d peso d t w peso d t w peso d t w α α ∈ = ∑ (11) donde type(w) representa el tipo del término w, y class(d) representa la clase de la historia d, c kα es el parámetro de recalibración para la clase de noticias c y el tipo de término k. En el trabajo, simplemente utilizamos las estadísticas de la tabla 2 como parámetros de recalibración. Aunque usar las estadísticas directamente puede que no sea la mejor opción, no discutimos cómo obtener automáticamente los mejores parámetros. Intentaremos utilizar técnicas de aprendizaje automático para obtener los mejores parámetros en el trabajo futuro. En el trabajo, utilizamos BoosTexter [20] para clasificar todas las historias en uno de los 11 ROIs. BoosTexter es un programa de aprendizaje automático basado en boosting, que crea una serie de reglas simples para construir un clasificador de datos de texto o atributos-valor. Utilizamos el peso del término generado utilizando el modelo TF-IDF como característica para la clasificación de historias. Entrenamos el modelo con las 12000 historias en inglés evaluadas en TDT2, y clasificamos el resto de las historias en TDT2 y todas las historias en TDT3. Los resultados de clasificación se utilizan para la reponderación de términos en la fórmula (11). Dado que las etiquetas de clase de las historias de tema apagado no se proporcionan en los conjuntos de datos de TDT, no podemos proporcionar la precisión de clasificación aquí. Por lo tanto, no discutimos los efectos de la precisión de la clasificación en el rendimiento de NED en el artículo. 6. CONFIGURACIÓN EXPERIMENTAL 6.1 Conjuntos de datos Utilizamos dos conjuntos de datos de LDC [18], TDT2 y TDT3, para nuestros experimentos. TDT2 contiene noticias desde enero hasta junio de 1998. Contiene alrededor de 54,000 historias de fuentes como ABC, Associated Press, CNN, New York Times, Public Radio International, Voice of America, etc. Solo se consideraron historias en inglés en la colección. TDT3 contiene aproximadamente 31,000 historias en inglés recopiladas de octubre a diciembre de 1998. Además de las fuentes utilizadas en TDT2, también contiene historias de las transmisiones de televisión de NBC y MSNBC. Utilizamos versiones transcritas de las emisiones de televisión y radio además de noticias textuales. El conjunto de datos TDT2 está etiquetado con alrededor de 100 temas, y aproximadamente 12,000 historias en inglés pertenecen al menos a uno de estos temas. El conjunto de datos TDT3 está etiquetado con alrededor de 120 temas, y aproximadamente 8000 historias en inglés pertenecen al menos a uno de estos temas. Todos los temas están clasificados en 11 Reglas de Interpretación: (1) Elecciones, (2) Escándalos/Audiencias, (3) Casos Legales/Criminales, (4) Desastres Naturales, (5) Accidentes, (6) Violencia o Guerra en Curso, (7) Noticias de Ciencia y Descubrimientos, (8) Finanzas, (9) Nueva Ley, (10) Noticias Deportivas, (11) Varios. Noticias. 6.2 Métrica de evaluación TDT utiliza una función de costo CDet que combina las probabilidades de perder una nueva historia y una falsa alarma [19]: * * * *Det Miss Miss Target FA FA NontargetC C P P C P P= + (12) Tabla 2. Correlación promedio entre tipos de términos y clases de noticias donde CMiss significa el costo de perder una nueva historia, PMiss significa la probabilidad de perder una nueva historia, y PTarget significa la probabilidad de ver una nueva historia en los datos; CFA significa el costo de una falsa alarma, PFA significa la probabilidad de una falsa alarma, y PNontarget significa la probabilidad de ver una historia antigua. El costo CDet se normaliza de tal manera que un sistema perfecto obtiene una puntuación de 0 y un sistema trivial, que es el mejor de marcar todas las historias como nuevas o antiguas, obtiene una puntuación de 1: ( ( * , * ) ) Det Det Miss Target FA Nontarget C Norm C min C P C P = (13) El sistema de detección de eventos nuevos proporciona dos salidas para cada historia. La primera parte es sí o no, indicando si la historia desencadena un nuevo evento o no. La segunda parte es una puntuación que indica la confianza de la primera decisión. Los puntajes de confianza se pueden utilizar para trazar la curva DET, es decir, curvas que representan la probabilidad de falsa alarma frente a la probabilidad de omisión. El costo normalizado mínimo se puede determinar si se elige el umbral óptimo en la puntuación. RESULTADOS EXPERIMENTALES 7.1 Resultados principales Para probar los enfoques propuestos en el modelo, implementamos y probamos cinco sistemas: Sistema-1: este sistema se utiliza como referencia. Se implementa basándose en el modelo básico descrito en la sección 3, es decir, utilizando el modelo TF-IDF incremental para generar pesos de términos y utilizando la distancia de Hellinger para calcular la similitud entre documentos. La normalización del puntaje de similitud también se emplea [8]. Se utiliza el procedimiento de detección S-S. Sistema-2: este sistema es igual que el sistema-1 excepto que se utiliza el procedimiento de detección S-C. Sistema-3: este sistema es igual que el sistema-1 excepto que utiliza el nuevo procedimiento de detección que se basa en un árbol de indexación. Sistema-4: implementado basado en el enfoque presentado en la sección 5.1, es decir, los términos se vuelven a ponderar según la distancia entre las distribuciones de términos en un grupo y todas las historias. Se utiliza el nuevo procedimiento de detección. Sistema-5: implementado basado en el enfoque presentado en la sección 5.2, es decir, los términos de diferentes tipos son reponderados según la clase de noticias utilizando parámetros entrenados. El nuevo procedimiento de detección se utiliza. Los siguientes son algunos otros sistemas NED: Sistema-6: [21] para cada par de historias, calcula tres valores de similitud para entidad nombrada, entidad no nombrada y todos los términos respectivamente. Y emplear Máquina de Vectores de Soporte para predecir nuevo o antiguo utilizando los valores de similitud como características. Sistema-7: [8] extendió un modelo básico incremental de TF-IDF para incluir modelos específicos de origen, normalización de puntuaciones de similitud basada en promedios específicos de documentos, normalización de puntuaciones de similitud basada en promedios específicos de pares de fuentes, etc. Sistema-8: [13] dividió la representación del documento en dos partes: entidades nombradas y entidades no nombradas, y eligió una parte efectiva para cada clase de noticias. La tabla 3 y la tabla 4 muestran los costos normalizados ponderados por tema y los tiempos de comparación en los conjuntos de datos TDT2 y TDT3 respectivamente. Dado que no se contaba con un conjunto de datos de validación para ajustar el umbral θ nuevo en los experimentos en TDT2, solo informamos los costos normalizados mínimos para nuestros sistemas en la tabla 3. El sistema-5 supera a todos los demás sistemas, incluido el sistema-6, y realiza solo 2.78e+8 veces la comparación de tiempos en el procedimiento de detección, lo que representa solo el 13.4% del sistema-1. Tabla 3. Los resultados de NED en los sistemas TDT2 Min Norm(CDet) tiempos de comparación son: Sistema-1 0.5749 2.08e+9, Sistema-2 0.6673 3.77e+8, Sistema-3 0.5765 2.81e+8, Sistema-4 0.5431 2.99e+8, Sistema-5 0.5089 2.78e+8, Sistema-6 0.5300. Al evaluar en los costos normalizados en TDT3, utilizamos los umbrales óptimos obtenidos del conjunto de datos de TDT2 para todos los sistemas. El sistema-2 reduce los tiempos de comparación a 1.29e+9, que es solo el 18.3% del sistema-1, pero al mismo tiempo también obtiene un costo normalizado mínimo deteriorado que es 0.0499 más alto que el sistema-1. El Sistema-3 utiliza el nuevo procedimiento de detección basado en el índice de noticias en árbol. Requiere incluso menos tiempo de comparación que el sistema-2. Esto se debe a que las comparaciones de historias suelen arrojar mayores similitudes que las de grupos de historias, por lo que las historias tienden a combinarse en el sistema-3. Y el sistema-3 es básicamente equivalente al sistema-1 en los resultados de precisión. El System-4 ajusta los pesos de los términos basándose en la distancia de las distribuciones de términos entre todo el corpus y el conjunto de historias del clúster, lo que resulta en una mejora significativa de 0.0468 en comparación con el system-1. El mejor sistema (sistema-5) tiene un costo normalizado mínimo de 0.5012, lo cual es 0.0797 mejor que el sistema-1, y también mejor que cualquier otro resultado previamente reportado para este conjunto de datos [8, 13]. Además, el sistema-5 solo necesita 1.05e+8 veces de comparación, lo que equivale al 14.9% del sistema-1. Tabla 4. Los resultados de NED en los sistemas TDT3 Norm(CDet) Norma mínima(CDet) Tiempos de comparación Sistema-1 0.6159 0.5809 7.04e+8 Sistema-2① 0.6493 0.6308 1.29e+8 Sistema-3② 0.6197 0.5868 1.03e+8 Sistema-4② 0.5601 0.5341 1.03e+8 Sistema-5② 0.5413 0.5012 1.05e+8 Sistema-7 -- 0.5783 -Sistema-8 -- 0.5229 -① θ nuevo=0.13 ② θ inicial=0.13, λ =3, δ =0.15 La Figura 5 muestra las cinco curvas DET para nuestros sistemas en el conjunto de datos TDT3. El Sistema-5 logra el costo mínimo con una tasa de falsas alarmas de 0.0157 y una tasa de omisión de 0.4310. Podemos observar que System4 y System-5 obtienen una menor probabilidad de error de omisión en regiones de baja probabilidad de falsa alarma. La hipótesis es que se transfiere más valor de peso a los términos clave de los temas que a los términos no clave. La puntuación de similitud entre dos historias pertenecientes a diferentes temas es más baja que antes, porque los términos superpuestos suelen no ser términos clave de sus temas. 7.2 Selección de parámetros para la detección de árboles de indexación. La Figura 3 muestra los costos normalizados mínimos obtenidos por el sistema-3 en TDT3 utilizando diferentes parámetros. El parámetro de inicio θ se prueba en seis valores que van desde 0.03 hasta 0.18. Y el parámetro λ se prueba en cuatro valores: 1, 2, 3 y 4. Podemos ver que, cuando θ inicial se establece en 0.12, que es el más cercano a θ nuevo, los costos son más bajos que los demás. Esto es fácil de explicar, porque cuando se agrupan historias pertenecientes al mismo tema en un conjunto, es más razonable que el conjunto represente las historias en él. Cuando el parámetro λ se establece en 3 o 4, los costos son mejores que en otros casos, pero no hay mucha diferencia entre 3 y 4. 0 0.05 0.1 0.15 0.2 1 2 3 4 0.5 0.6 0.7 0.8 0.9 1 θ-initλ MinCost 0.6 0.65 0.7 0.75 0.8 0.85 0.9 Figura 3. Costo mínimo en TDT3 (δ =0.15) 0 0.05 0.1 0.15 0.2 1 2 3 4 0 0.5 1 1.5 2 2.5 x 10 8 θ-inicio λ Comparando tiempos 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 8 Figura 4. Comparando los tiempos en TDT3 (δ =0.15), la Figura 4 muestra los tiempos de comparación utilizados por el sistema-3 en TDT3 con los mismos parámetros que la Figura 3. Los tiempos de comparación dependen fuertemente de θ init. Debido a que cuanto mayor sea θ init, menos historias se combinan juntas, más veces de comparación se necesitan para la decisión de un nuevo evento. Por lo tanto, usamos θ init = 0.13, λ = 3, δ = 0.15 para los sistemas 3, 4 y 5. En esta configuración de parámetros, podemos obtener tanto costos normalizados mínimos bajos como menos tiempos de comparación. CONCLUSIÓN Hemos propuesto un procedimiento de detección basado en un árbol de indexación de noticias en nuestro modelo. Reduce los tiempos de comparación a aproximadamente una séptima parte del método tradicional sin afectar la precisión de NED. También hemos presentado dos extensiones al modelo básico TF-IDF. La primera extensión se realiza ajustando los pesos de los términos basados en las distribuciones de términos entre todo el corpus y un conjunto de historias de un clúster. Y la segunda extensión al modelo básico TF-IDF es un mejor uso de los tipos de términos (tipos de entidades nombradas y partes de la velocidad) de acuerdo con las categorías de noticias. Nuestros resultados experimentales en los conjuntos de datos TDT2 y TDT3 muestran que ambas extensiones contribuyen significativamente a la mejora en la precisión. No consideramos la información de tiempo de noticias como una pista para la tarea de NED, ya que la mayoría de los temas duran mucho tiempo y los conjuntos de datos de TDT solo abarcan un período relativamente corto (no más de 6 meses). Para el trabajo futuro, queremos recopilar un conjunto de noticias que abarque un período más largo de internet e integrar información temporal en la tarea de NED. Dado que el tema es un grupo de noticias relativamente grueso, también queremos refinar la granularidad del grupo a nivel de evento e identificar diferentes eventos y sus relaciones dentro de un tema. Agradecimientos: Este trabajo ha sido apoyado por la Fundación Nacional de Ciencias Naturales de China bajo la subvención No. 90604025. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son del autor(es) y no necesariamente reflejan las del patrocinador. REFERENCIAS [1] http://www.nist.gov/speech/tests/tdt/index.htm [2] En Detección y Seguimiento de Temas. Organización de la información basada en eventos. Kluwer Academic Publishers, 2002. .01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90 1 2 5 10 20 40 60 80 90 Probabilidad de Falsa Alarma (en %) Probabilidad de Omisión (en %) Curva Ponderada por Tema del SISTEMA1 Norma Mínima (Costo) del SISTEMA1 Curva Ponderada por Tema del SISTEMA2 Norma Mínima (Costo) del SISTEMA2 Curva Ponderada por Tema del SISTEMA3 Norma Mínima (Costo) del SISTEMA3 Curva Ponderada por Tema del SISTEMA4 Norma Mínima (Costo) del SISTEMA4 Curva Ponderada por Tema del SISTEMA5 Norma Mínima (Costo) del SISTEMA5 Rendimiento Aleatorio Figura 5. Curvas DET en TDT3 [3] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B.T. Archibald y X. Liu. Enfoques de aprendizaje para detectar y rastrear eventos de noticias. En el número especial de IEEE Intelligent Systems sobre Aplicaciones de Recuperación de Información Inteligente, volumen 14 (4), 1999, páginas 32-43. [4] Y. Yang, T. Pierce y J. Carbonell. Un estudio sobre la detección de eventos retrospectivos y en línea. En Actas de SIGIR-98, Melbourne, Australia, 1998, 28-36. [5] J. Allan, V. Lavrenko, D. Malin y R. Swan. Detecciones, límites y líneas de tiempo: Umass y tdt-3. En Actas del Taller de Detección y Seguimiento de Temas (TDT-3), Viena, VA, 2000, 167-174. [6] R. Papka y J. Allan. Detección de nuevos eventos en línea utilizando agrupamiento de un solo paso TÍTULO2:. Informe técnico UM-CS1998-021, 1998. [7] W. Lam, H. Meng, K. Wong y J. I'm sorry, but \"Yen\" is not a sentence. Can you please provide a sentence for me to translate into Spanish? Utilizando el Análisis Contextual para la Detección de Eventos de Noticias. Revista Internacional de Sistemas Inteligentes, 2001, 525-546. [8] B. Thorsten, C. Francine y F. Ayman. Un sistema para la detección de nuevos eventos. En Actas de la 26ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2003, 330-337. [9] S. Nicola y C. Joe. Combinando clasificadores semánticos y sintácticos de documentos para mejorar la detección de primeras noticias. En Actas de la 24ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell, and C. Jin. \n\nACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell y C. Jin. Detección de novedades condicionada al tema. En Actas de la 8ª Conferencia Internacional de ACM SIGKDD, ACM Press. 2002, 688-693. [11] M. Juha, A.M. Helena y S. Marko. Aplicando Clases Semánticas en la Detección y Seguimiento de Eventos. En Actas de la Conferencia Internacional sobre Procesamiento del Lenguaje Natural (ICON 2002), 2002, páginas 175-183. [12] M. Juha, A.M. Helena y S. Marko. Semántica simple en la detección y seguimiento de temas. Recuperación de información, 7(3-4): 2004, 347-368. [13] K. Giridhar y J. Allan. Clasificación de texto y entidades nombradas para la detección de nuevos eventos. En Actas de la 27ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2004, 297-304. [14] J. P. Callan, W. B. Croft, y S. M. Harding. El Sistema de Recuperación INQUERY. En Actas de DEXA-92, 3ra Conferencia Internacional sobre Aplicaciones de Bases de Datos y Sistemas Expertos, 1992, 78-83. [15] R. Krovetz. Viendo la morfología como un proceso de inferencia. En Actas de ACM SIGIR93, 1993, 61-81. [16] Y. Yang y J. Pedersen. Un estudio comparativo sobre la selección de características en la categorización de textos. En J. D. H. Fisher, editor, La Decimocuarta Conferencia Internacional sobre Aprendizaje Automático (ICML97), Morgan Kaufmann, 1997, 412-420. [17] T. M. Cover y J. A. I'm sorry, but \"Thomas\" is not a sentence. Can you provide more context or a complete sentence for me to translate into Spanish? Elementos de la teoría de la información. Wiley. 1991. [18] El consorcio de datos lingüísticos, http://www.ldc,upenn.edu/. [19] La definición de la tarea TDT 2001 y el plan de evaluación, http://www.nist.gov/speech/tests/tdt/tdt2001/evalplan.htm. [20] R. E. Schapire y Y. Cantante. Boostexter: Un sistema basado en Boosting para la categorización de texto. En Machine Learning 39(2/3):1, Kluwer Academic Publishers, 2000, 35-168. [21] K. Giridhar y J. Allan. 2005. Utilizando nombres y temas para la detección de nuevos eventos. En Actas de la Conferencia de Tecnología Humana y la Conferencia sobre Métodos Empíricos en Lenguaje Natural, Vancouver, 2005, 121-128 ",
            "candidates": [],
            "error": [
                []
            ]
        },
        "story class": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "New Event Detection Based on Indexing-tree and Named Entity Zhang Kuo Tsinghua University Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Tsinghua University Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Tsinghua University Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn ABSTRACT New Event Detection (NED) aims at detecting from one or multiple streams of news stories that which one is reported on a new event (i.e.",
                "not reported previously).",
                "With the overwhelming volume of news available today, there is an increasing need for a NED system which is able to detect new events more efficiently and accurately.",
                "In this paper we propose a new NED model to speed up the NED task by using news indexing-tree dynamically.",
                "Moreover, based on the observation that terms of different types have different effects for NED task, two term reweighting approaches are proposed to improve NED accuracy.",
                "In the first approach, we propose to adjust term weights dynamically based on previous story clusters and in the second approach, we propose to employ statistics on training data to learn the named entity reweighting model for each class of stories.",
                "Experimental results on two Linguistic Data Consortium (LDC) datasets TDT2 and TDT3 show that the proposed model can improve both efficiency and accuracy of NED task significantly, compared to the baseline system and other existing systems.",
                "Categories and Subject Descriptors H.3.3 [Information Systems]: Information Search and Retrieval; H.4.2 [Information Systems Applications]: Types of Systemsdecision support.",
                "General Terms Algorithms, Performance, Experimentation 1.",
                "INTRODUCTION Topic Detection and Tracking (TDT) program aims to develop techniques which can effectively organize, search and structure news text materials from a variety of newswire and broadcast media [1].",
                "New Event Detection (NED) is one of the five tasks in TDT.",
                "It is the task of online identification of the earliest report for each topic as soon as that report arrives in the sequence of documents.",
                "A Topic is defined as a seminal event or activity, along with directly related events and activities [2].",
                "An Event is defined as something (non-trivial) happening in a certain place at a certain time [3].",
                "For instance, when a bomb explodes in a building, the exploding is the seminal event that triggers the topic, and other stories on the same topic would be those discussing salvaging efforts, the search for perpetrators, arrests and trial and so on.",
                "Useful news information is usually buried in a mass of data generated everyday.",
                "Therefore, NED systems are very useful for people who need to detect novel information from real-time news stream.",
                "These real-life needs often occur in domains like financial markets, news analysis, and intelligence gathering.",
                "In most of state-of-the-art (currently) NED systems, each news story on hand is compared to all the previous received stories.",
                "If all the similarities between them do not exceed a threshold, then the story triggers a new event.",
                "They are usually in the form of cosine similarity or Hellinger similarity metric.",
                "The core problem of NED is to identify whether two stories are on the same topic.",
                "Obviously, these systems cannot take advantage of topic information.",
                "Further more, it is not acceptable in real applications because of the large amount of computation required in the NED process.",
                "Other systems organize previous stories into clusters (each cluster corresponds to a topic), and new story is compared to the previous clusters instead of stories.",
                "This manner can reduce comparing times significantly.",
                "Nevertheless, it has been proved that this manner is less accurate [4, 5].",
                "This is because sometimes stories within a topic drift far away from each other, which could lead low similarity between a story and its topic.",
                "On the other hand, some proposed NED systems tried to improve accuracy by making better use of named entities [10, 11, 12, 13].",
                "However, none of the systems have considered that terms of different types (e.g.",
                "Noun, Verb or Person name) have different effects for different classes of stories in determining whether two stories are on the same topic.",
                "For example, the names of election candidates (Person name) are very important for stories of election class; the locations (Location name) where accidents happened are important for stories of accidents class.",
                "So, in NED, there still exist following three problems to be investigated: (1) How to speed up the detection procedure while do not decrease the detection accuracy? (2) How to make good use of cluster (topic) information to improve accuracy? (3) How to obtain better news story representation by better understanding of named entities.",
                "Driven by these problems, we have proposed three approaches in this paper. (1)To make the detection procedure faster, we propose a new NED procedure based on news indexing-tree created dynamically.",
                "Story indexing-tree is created by assembling similar stories together to form news clusters in different hierarchies according to their values of similarity.",
                "Comparisons between current story and previous clusters could help find the most similar story in less comparing times.",
                "The new procedure can reduce the amount of comparing times without hurting accuracy. (2)We use the clusters of the first floor in the indexing-tree as news topics, in which term weights are adjusted dynamically according to term distribution in the clusters.",
                "In this approach, cluster (topic) information is used properly, so the problem of theme decentralization is avoided. (3)Based on observations on the statistics obtained from training data, we found that terms of different types (e.g.",
                "Noun and Verb) have different effects for different classes of stories in determining whether two stories are on the same topic.",
                "And we propose to use statistics to optimize the weights of the terms of different types in a story according to the news class that the story belongs to.",
                "On TDT3 dataset, the new NED model just uses 14.9% comparing times of the basic model, while its minimum normalized cost is 0.5012, which is 0.0797 better than the basic model, and also better than any other results previously reported for this dataset [8, 13].",
                "The rest of the paper is organized as follows.",
                "We start off this paper by summarizing the previous work in NED in section 2.",
                "Section 3 presents the basic model for NED that most current systems use.",
                "Section 4 describes our new detection procedure based on news indexing-tree.",
                "In section 5, two term reweighting methods are proposed to improve NED accuracy.",
                "Section 6 gives our experimental data and evaluation metrics.",
                "We finally wrap up with the experimental results in Section 7, and the conclusions and future work in Section 8. 2.",
                "RELATED WORK Papka et al. proposed Single-Pass clustering on NED [6].",
                "When a new story was encountered, it was processed immediately to extract term features and a query representation of the storys content is built up.",
                "Then it was compared with all the previous queries.",
                "If the document did not trigger any queries by exceeding a threshold, it was marked as a new event.",
                "Lam et al build up previous query representations of story clusters, each of which corresponds to a topic [7].",
                "In this manner comparisons happen between stories and clusters.",
                "Recent years, most work focus on proposing better methods on comparison of stories and document representation.",
                "Brants et al. [8] extended a basic incremental TF-IDF model to include sourcespecific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, term reweighting based on inverse event frequencies, and segmentation of documents.",
                "Good improvements on TDT bench-marks were shown.",
                "Stokes et al. [9] utilized a combination of evidence from two distinct representations of a documents content.",
                "One of the representations was the usual free text vector, the other made use of lexical chains (created using WordNet) to build another term vector.",
                "Then the two representations are combined in a linear fashion.",
                "A marginal increase in effectiveness was achieved when the combined representation was used.",
                "Some efforts have been done on how to utilize named entities to improve NED.",
                "Yang et al. gave location named entities four times weight than other terms and named entities [10].",
                "DOREMI research group combined semantic similarities of person names, location names and time together with textual similarity [11][12].",
                "UMass [13] research group split document representation into two parts: named entities and non-named entities.",
                "And it was found that some classes of news could achieve better performance using named entity representation, while some other classes of news could achieve better performance using non-named entity representation.",
                "Both [10] and [13] used text categorization technique to classify news stories in advance.",
                "In [13] news stories are classified automatically at first, and then test sensitivities of names and non-name terms for NED for each class.",
                "In [10] frequent terms for each class are removed from document representation.",
                "For example, word election does not help identify different elections.",
                "In their work, effectiveness of different kinds of names (or terms with different POS) for NED in different news classes are not investigated.",
                "We use statistical analysis to reveal the fact and use it to improve NED performance. 3.",
                "BASIC MODEL In this section, we present the basic New Event Detection model which is similar to what most current systems apply.",
                "Then, we propose our new model by extending the basic model.",
                "New Event Detection systems use news story stream as input, in which stories are strictly time-ordered.",
                "Only previously received stories are available when dealing with current story.",
                "The output is a decision for whether the current story is on a new event or not and the confidence of the decision.",
                "Usually, a NED model consists of three parts: story representation, similarity calculation and detection procedure. 3.1 Story Representation Preprocessing is needed before generating story representation.",
                "For preprocessing, we tokenize words, recognize abbreviations, normalize abbreviations, add part-of-speech tags, remove stopwords included in the stop list used in InQuery [14], replace words with their stems using K-stem algorithm[15], and then generate word vector for each news story.",
                "We use incremental TF-IDF model for term weight calculation [4].",
                "In a TF-IDF model, term frequency in a news document is weighted by the inverse document frequency, which is generated from training corpus.",
                "When a new term occurs in testing process, there are two solutions: simply ignore the new term or set df of the term as a small const (e.g. df = 1).",
                "The new term receives too low weight in the first solution (0) and too high weight in the second solution.",
                "In incremental TF-IDF model, document frequencies are updated dynamically in each time step t: 1( ) ( ) ( )t t D tdf w df w df w−= + (1) where Dt represents news story set received in time t, and dfDt(w) means the number of documents that term w occurs in, and dft(w) means the total number of documents that term w occurs in before time t. In this work, each time window includes 50 news stories.",
                "Thus, each story d received in t is represented as follows: 1 2{ ( , , ), ( , , ),..., ( , , )}nd weight d t w weight d t w weight d t w→ where n means the number of distinct terms in story d, and ( , , )weight d t w means the weight of term w in story d at time t: log( ( , ) 1) log(( 1) /( ( ) 0.5)) ( , , ) log( ( , ) 1) log(( 1) /( ( ) 0.5)) t t t t w d tf d w N df w weight d t w tf d w N df w ∈ + + + = + + +∑ (2) where Nt means the total number of news stories before time t, and tf(d,w) means how many times term w occurs in news story d. 3.2 Similarity Calculation We use Hellinger distance for the calculation of similarity between two stories, for two stories d and d at time t, their similarity is defined as follows: , ( , , ) ( , , ) * ( , , ) w d d sim d d t weight d t w weight d t w ∈ = ∑ (3) 3.3 Detection Procedure For each story d received in time step t, the value ( ) ( ) ( ) ( ( , , )) time d time d n d max sim d d t < = (4) is a score used to determine whether d is a story about a new topic and at the same time is an indication of the confidence in our decision [8]. time(d) means the publication time of story d. If the score exceeds the thresholdθ new, then there exists a sufficiently similar document, thus d is a old story, otherwise, there is no sufficiently similar previous document, thus d is an new story. 4.",
                "New NED Procedure Traditional NED systems can be classified into two main types on the aspect of detection procedure: (1) S-S type, in which the story on hand is compared to each story received previously, and use the highest similarity to determine whether current story is about a new event; (2) S-C type, in which the story on hand is compared to all previous clusters each of which representing a topic, and the highest similarity is used for final decision for current story.",
                "If the highest similarity exceeds thresholdθ new, then it is an old story, and put it into the most similar cluster; otherwise it is a new story and create a new cluster.",
                "Previous work show that the first manner is more accurate than the second one [4][5].",
                "Since sometimes stories within a topic drift far away from each other, a story may have very low similarity with its topic.",
                "So using similarities between stories for determining new story is better than using similarities between story and clusters.",
                "Nevertheless, the first manner needs much more comparing times which means the first manner is low efficient.",
                "We propose a new detection procedure which uses comparisons with previous clusters to help find the most similar story in less comparing times, and the final new event decision is made according to the most similar story.",
                "Therefore, we can get both the accuracy of S-S type methods and the efficiency of S-C type methods.",
                "The new procedure creates a news indexing-tree dynamically, in which similar stories are put together to form a hierarchy of clusters.",
                "We index similar stories together by their common ancestor (a cluster node).",
                "Dissimilar stories are indexed in different clusters.",
                "When a story is coming, we use comparisons between the current story and previous hierarchical clusters to help find the most similar story which is useful for new event decision.",
                "After the new event decision is made, the current story is inserted to the indexing-tree for the following detection.",
                "The news indexing-tree is defined formally as follows: S-Tree = {r, NC , NS , E} where r is the root of S-Tree, NC is the set of all cluster nodes, NS is the set of all story nodes, and E is the set of all edges in S-Tree.",
                "We define a set of constraints for a S-Tree: ⅰ . , is an non-terminal node in the treeC i i N i∀ ∈ → ⅱ . , is a terminal node in the treeS i i N i∀ ∈ → ⅲ . , out degree of is at least 2C i i N i∀ ∈ → ⅳ . , is represented as the centroid of its desendantsC i i iN∀ ∈ → For a news story di, the comparison procedure and inserting procedure based on indexing-tree are defined as follows.",
                "An example is shown by Figure 1 and Figure 2.",
                "Figure 1.",
                "Comparison procedure Figure 2.",
                "Inserting procedure Comparison procedure: Step 1: compare di to all the direct child nodes of r and select λ nodes with highest similarities, e.g., C1 2 and C1 3 in Figure 1.",
                "Step 2: for each selected node in the last step, e.g.",
                "C1 2, compare di to all its direct child nodes, and select λ nodes with highest similarities, e.g.",
                "C2 2 and d8.",
                "Repeat step 2 for all non-terminal nodes.",
                "Step 3: record the terminal node with the highest similarty to di, e.g. s5, and the similarity value (0.20).",
                "Inserting di to the S-tree with r as root: Find the node n which is direct child of r in the path from r to the terminal node with highest similarity s, e.g.",
                "C1 2.",
                "If s is smaller than θ init+(h-1)δ , then add di to the tree as a direct child of r. Otherwise, if n is a terminal node, then create a cluster node instead of n, and add both n and di as its direct children; if n is an non-terminal node, then repeat this procedure and insert di to the sub-tree with n as root recursively.",
                "Here h is the length between n and the root of S-tree.",
                "The more the stories in a cluster similar to each other, the better the cluster represents the stories in it.",
                "Hence we add no constraints on the maximum of trees height and degree of a node.",
                "Therefore, we cannot give the complexity of this indexing-tree based procedure.",
                "But we will give the number of comparing times needed by the new procedure in our experiments in section7. 5.",
                "Term Reweighting Methods In this section, two term reweighting methods are proposed to improve NED accuracy.",
                "In the first method, a new way is explored for better using of cluster (topic) information.",
                "The second one finds a better way to make use of named entities based on news classification. 5.1 Term Reweighting Based on Distribution Distance TF-IDF is the most prevalent model used in information retrieval systems.",
                "The basic idea is that the fewer documents a term appears in, the more important the term is in discrimination of documents (relevant or not relevant to a query containing the term).",
                "Nevertheless, in TDT domain, we need to discriminate documents with regard to topics rather than queries.",
                "Intuitively, using cluster (topic) vectors to compare with subsequent news stories should outperform using story vectors.",
                "Unfortunately, the experimental results do not support this intuition [4][5].",
                "Based on observation on data, we find the reason is that a news topic usually contains many directly or indirectly related events, while they all have their own sub-subjects which are usually different with each other.",
                "Take the topic described in section 1 as an example, events like the explosion and salvage have very low similarities with events about criminal trial, therefore stories about trial would have low similarity with the topic vector built on its previous events.",
                "This section focuses on how to effectively make use of topic information and at the same time avoid the problem of content decentralization.",
                "At first, we classify terms into 5 classes to help analysis the needs of the modified model: Term class A: terms that occur frequently in the whole corpus, e.g., year and people.",
                "Terms of this class should be given low weights because they do not help much for topic discrimination.",
                "Term class B: terms that occur frequently within a news category, e.g., election, storm.",
                "They are useful to distinguish two stories in different news categories.",
                "However, they cannot provide information to determine whether two stories are on the same or different topics.",
                "In another words, term election and term storm are not helpful in differentiate two election campaigns and two storm disasters.",
                "Therefore, terms of this class should be assigned lower weights.",
                "Term class C: terms that occur frequently in a topic, and infrequently in other topics, e.g., the name of a crash plane, the name of a specific hurricane.",
                "News stories that belong to different topics rarely have overlap terms in this class.",
                "The more frequently a term appears in a topic, the more important the term is for a story belonging to the topic, therefore the term should be set higher weight.",
                "Term class D: terms that appear in a topic exclusively, but not frequently.",
                "For example, the name of a fireman who did very well in a salvage action, which may appears in only two or three stories but never appeared in other topics.",
                "Terms of this type should receive more weights than in TF-IDF model.",
                "However, since they are not popular in the topic, it is not appropriate to give them too high weights.",
                "Term class E: terms with low document frequency, and appear in different topics.",
                "Terms of this class should receive lower weights.",
                "Now we analyze whether TF-IDF model can give proper weights to the five classes of terms.",
                "Obviously, terms of class A are lowly weighted in TF-IDF model, which is conformable with the requirement described above.",
                "In TF-IDF model, terms of class B are highly dependant with the number of stories in a news class.",
                "TF-IDF model cannot provide low weights if the story containing the term belongs to a relative small news class.",
                "For a term of class C, the more frequently it appears in a topic, the less weight TFIDF model gives to it.",
                "This strongly conflicts with the requirement of terms in class C. For terms of class D, TF-IDF model gives them high weights correctly.",
                "But for terms of class E, TF-IDF model gives high weights to them which are not conformable with the requirement of low weights.",
                "To sum up, terms of class B, C, E cannot be properly weighted in TF-IDF model.",
                "So, we propose a modified model to resolve this problem.",
                "When θ init andθ new are set closely, we assume that most of the stories in a first-level cluster (a direct child node of root node) are on the same topic.",
                "Therefore, we make use of a first-level cluster to capture term distribution (df for all the terms within the cluster) within the topic dynamically.",
                "KL divergence of term distribution in a first-level cluster and the whole story set is used to adjust term weights:  ( , , ) * (1 * ( || )) ( , , ) ( , , ) * (1 * ( || )) cw tw cw tw w d D weight d t w KL P P weight d t w weight d t w KL P P γ γ ∈ + = +∑ (5) where ( ) ( ) ( ) ( ) 1,cw cw c c c c df w df w p y p y N N = = − (6) ( ) ( ) ( ) ( ) 1,t t tw tw t t df w df w p y p y N N = = − (7) where dfc(w) is the number of documents containing term w within cluster C, and Nc is the number of documents in cluster C, and Nt is the total number of documents that arrive before time step t. γ is a const parameter, now is manually set 3.",
                "KL divergence is defined as follows [17]: ( ) ( || ) ( ) log ( )x p x KL P Q p x q x = ∑ (8) The basic idea is: for a story in a topic, the more a term occurs within the topic, and the less it occurs in other topics, it should be assigned higher weights.",
                "Obviously, modified model can meet all the requirements of the five term classes listed above. 5.2 Term Reweighting Based on Term Type and <br>story class</br> Previous work found that some classes of news stories could achieve good improvements by giving extra weight to named entities.",
                "But we find that terms of different types should be given different amount of extra weight for different classes of news stories.",
                "We use open-NLP1 to recognize named entity types and part-ofspeech tags for terms that appear in news stories.",
                "Named entity types include person name, organization name, location name, date, time, money and percentage, and five POSs are selected: none (NN), verb (VB), adjective (JJ), adverb (RB) and cardinal number (CD).",
                "Statistical analysis shows topic-level discriminative terms types for different classes of stories.",
                "For the sake of convenience, named entity type and part-of-speech tags are uniformly called term type in subsequent sections.",
                "Determining whether two stories are about the same topic is a basic component for NED task.",
                "So at first we use 2 χ statistic to compute correlations between terms and topics.",
                "For a term t and a topic T, a contingence table is derived: Table 1.",
                "A 2×2 Contingence Table Doc Number belong to topic T not belong to topic T include t A B not include t C D The 2 χ statistic for a specific term t with respect to topic T is defined to be [16]: 2 2 ( , ) ( ) * ( ) ( ) * ( ) * ( ) * ( ) w T A B C D AD CB A C B D A B C D χ = + + + − + + + + (9) News topics for the TDT task are further classified into 11 rules of interpretations (ROIs) 2 .",
                "The ROI can be seen as a higher level class of stories.",
                "The average correlation between a term type and a topic ROI is computed as: 2 avg 2 ( , )( ( , ) )k m m km kT R w P w TP R p w T R P χ χ ∈ ∈ ∑ ∑（ , ）= 1 1 k=1…K, m=1…M (10) where K is the number of term types (set 12 constantly in the paper).",
                "M is the number news classes (ROIs, set 11 in the paper).",
                "Pk represents the set of all terms of type k, and Rm represents the set of all topics of class m, p(t,T) means the probability that t occurs in topic T. Because of limitation of space, only parts of the term types (9 term types) and parts of news classes (8 classes) are listed in table 2 with the average correlation values between them.",
                "The statistics is derived from labeled data in TDT2 corpus. (Results in table 2 are already normalized for convenience in comparison.)",
                "The statistics in table 2 indicates the usefulness of different term types in topic discrimination with respect to different news classes.",
                "We can see that, location name is the most useful term type for three news classes: Natural Disasters, Violence or War, Finances.",
                "And for three other categories Elections, Legal/Criminal Cases, Science and Discovery, person name is the most discriminative term type.",
                "For Scandals/Hearings, date is the most important information for topic discrimination.",
                "In addition, Legal/Criminal Cases and Finance topics have higher correlation with money terms, while Science and Discovery have higher correlation with percentage terms.",
                "Non-name terms are more stable for different classes. 1 . http://opennlp.sourceforge.net/ 2 . http://projects.ldc.upenn.edu/TDT3/Guide/label.html From the analysis of table 2, it is reasonable to adjust term weight according to their term type and the news class the story belongs to.",
                "New term weights are reweighted as follows: ( ) ( ) ( ) ( ) ( , , ) * ( , , ) ( , , ) * class d D type w T class d D type w w d weight d t w weight d t w weight d t w α α ∈ = ∑ (11) where type(w) represents the type of term w, and class(d) represents the class of story d, c kα is reweighting parameter for news class c and term type k. In the work, we just simply use statistics in table 2 as the reweighting parameters.",
                "Even thought using the statistics directly may not the best choice, we do not discuss how to automatically obtain the best parameters.",
                "We will try to use machine learning techniques to obtain the best parameters in the future work.",
                "In the work, we use BoosTexter [20] to classify all stories into one of the 11 ROIs.",
                "BoosTexter is a boosting based machine learning program, which creates a series of simple rules for building a classifier for text or attribute-value data.",
                "We use term weight generated using TF-IDF model as feature for story classification.",
                "We trained the model on the 12000 judged English stories in TDT2, and classify the rest of the stories in TDT2 and all stories in TDT3.",
                "Classification results are used for term reweighting in formula (11).",
                "Since the class labels of topic-off stories are not given in TDT datasets, we cannot give the classification accuracy here.",
                "Thus we do not discuss the effects of classification accuracy to NED performance in the paper. 6.",
                "EXPERIMENTAL SETUP 6.1 Datasets We used two LDC [18] datasets TDT2 and TDT3 for our experiments.",
                "TDT2 contains news stories from January to June 1998.",
                "It contains around 54,000 stories from sources like ABC, Associated Press, CNN, New York Times, Public Radio International, Voice of America etc.",
                "Only English stories in the collection were considered.",
                "TDT3 contains approximately 31,000 English stories collected from October to December 1998.",
                "In addition to the sources used in TDT2, it also contains stories from NBC and MSNBC TV broadcasts.",
                "We used transcribed versions of the TV and radio broadcasts besides textual news.",
                "TDT2 dataset is labeled with about 100 topics, and approximately 12,000 English stories belong to at least one of these topics.",
                "TDT3 dataset is labeled with about 120 topics, and approximately 8000 English stories belong to at least one of these topics.",
                "All the topics are classified into 11 Rules of Interpretation: (1)Elections, (2)Scandals/Hearings, (3)Legal/Criminal Cases, (4)Natural Disasters, (5)Accidents, (6)Ongoing Violence or War, (7)Science and Discovery News, (8)Finance, (9)New Law, (10)Sports News, (11)MISC.",
                "News. 6.2 Evaluation Metric TDT uses a cost function CDet that combines the probabilities of missing a new story and a false alarm [19]: * * * *Det Miss Miss Target FA FA NontargetC C P P C P P= + (12) Table 2.",
                "Average correlation between term types and news classes where CMiss means the cost of missing a new story, PMiss means the probability of missing a new story, and PTarget means the probability of seeing a new story in the data; CFA means the cost of a false alarm, PFA means the probability of a false alarm, and PNontarget means the probability of seeing an old story.",
                "The cost CDet is normalized such that a perfect system scores 0 and a trivial system, which is the better one of mark all stories as new or old, scores 1: ( ( * , * ) ) Det Det Miss Target FA Nontarget C Norm C min C P C P = (13) New event detection system gives two outputs for each story.",
                "The first part is yes or no indicating whether the story triggers a new event or not.",
                "The second part is a score indicating confidence of the first decision.",
                "Confidence scores can be used to plot DET curve, i.e., curves that plot false alarm vs. miss probabilities.",
                "Minimum normalized cost can be determined if optimal threshold on the score were chosen. 7.",
                "EXPERIMENTAL RESULTS 7.1 Main Results To test the approaches proposed in the model, we implemented and tested five systems: System-1: this system is used as baseline.",
                "It is implemented based on the basic model described in section 3, i.e., using incremental TF-IDF model to generate term weights, and using Hellinger distance to compute document similarity.",
                "Similarity score normalization is also employed [8].",
                "S-S detection procedure is used.",
                "System-2: this system is the same as system-1 except that S-C detection procedure is used.",
                "System-3: this system is the same as system-1 except that it uses the new detection procedure which is based on indexing-tree.",
                "System-4: implemented based on the approach presented in section 5.1, i.e., terms are reweighted according to the distance between term distributions in a cluster and all stories.",
                "The new detection procedure is used.",
                "System-5: implemented based on the approach presented in section 5.2, i.e., terms of different types are reweighted according to news class using trained parameters.",
                "The new detection procedure is used.",
                "The following are some other NED systems: System-6: [21] for each pair of stories, it computes three similarity values for named entity, non-named entity and all terms respectively.",
                "And employ Support Vector Machine to predict new or old using the similarity values as features.",
                "System-7: [8] it extended a basic incremental TF-IDF model to include source-specific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, etc.",
                "System-8: [13] it split document representation into two parts: named entities and non-named entities, and choose one effective part for each news class.",
                "Table 3 and table 4 show topic-weighted normalized costs and comparing times on TDT2 and TDT3 datasets respectively.",
                "Since no heldout data set for fine-tuning the threshold θ new was available for experiments on TDT2, we only report minimum normalized costs for our systems in table 3.",
                "System-5 outperforms all other systems including system-6, and it performs only 2.78e+8 comparing times in detection procedure which is only 13.4% of system-1.",
                "Table 3.",
                "NED results on TDT2 Systems Min Norm(CDet) Cmp times System-1 0.5749 2.08e+9 System-2① 0.6673 3.77e+8 System-3② 0.5765 2.81e+8 System-4② 0.5431 2.99e+8 System-5② 0.5089 2.78e+8 System-6 0.5300 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 When evaluating on the normalized costs on TDT3, we use the optimal thresholds obtained from TDT2 data set for all systems.",
                "System-2 reduces comparing times to 1.29e+9 which is just 18.3% of system-1, but at the same time it also gets a deteriorated minimum normalized cost which is 0.0499 higher than system-1.",
                "System-3 uses the new detection procedure based on news indexing-tree.",
                "It requires even less comparing times than system-2.",
                "This is because story-story comparisons usually yield greater similarities than story-cluster ones, so stories tend to be combined Location Person Date Organization Money Percentage NN JJ CD Elections 0.37 1 0.04 0.58 0.08 0.03 0.32 0.13 0.1 Scandals/Hearings 0.66 0.62 0.28 1 0.11 0.02 0.27 0.13 0.05 Legal/Criminal Cases 0.48 1 0.02 0.62 0.15 0 0.22 0.24 0.09 Natural Disasters 1 0.27 0 0.04 0.04 0 0.25 0.04 0.02 Violence or War 1 0.36 0.02 0.14 0.02 0.04 0.21 0.11 0.02 Science and Discovery 0.11 1 0.01 0.22 0.08 0.12 0.19 0.08 0.03 Finances 1 0.45 0.04 0.98 0.13 0.02 0.29 0.06 0.05 Sports 0.16 0.27 0.01 1 0.02 0 0.11 0.03 0.01 together in system-3.",
                "And system-3 is basically equivalent to system-1 in accuracy results.",
                "System-4 adjusts term weights based on the distance of term distributions between the whole corpus and cluster story set, yielding a good improvement by 0.0468 compared to system-1.",
                "The best system (system-5) has a minimum normalized cost 0.5012, which is 0.0797 better than system-1, and also better than any other results previously reported for this dataset [8, 13].",
                "Further more, system-5 only needs 1.05e+8 comparing times which is 14.9% of system-1.",
                "Table 4.",
                "NED results on TDT3 Systems Norm(CDet) Min Norm(CDet) Cmp times System-1 0.6159 0.5809 7.04e+8 System-2① 0.6493 0.6308 1.29e+8 System-3② 0.6197 0.5868 1.03e+8 System-4② 0.5601 0.5341 1.03e+8 System-5② 0.5413 0.5012 1.05e+8 System-7 -- 0.5783 -System-8 -- 0.5229 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 Figure5 shows the five DET curves for our systems on data set TDT3.",
                "System-5 achieves the minimum cost at a false alarm rate of 0.0157 and a miss rate of 0.4310.",
                "We can observe that System4 and System-5 obtain lower miss probability at regions of low false alarm probabilities.",
                "The hypothesis is that, more weight value is transferred to key terms of topics from non-key terms.",
                "Similarity score between two stories belonging to different topics are lower than before, because their overlapping terms are usually not key terms of their topics. 7.2 Parameter selection for indexing-tree detection Figure 3 shows the minimum normalized costs obtained by system-3 on TDT3 using different parameters.",
                "Theθ init parameter is tested on six values spanning from 0.03 to 0.18.",
                "And the λ parameter is tested on four values 1, 2, 3 and 4.",
                "We can see that, whenθ init is set to 0.12, which is the closest one toθ new, the costs are lower than others.",
                "This is easy to explain, because when stories belonging to the same topic are put in a cluster, it is more reasonable for the cluster to represent the stories in it.",
                "When parameter λ is set to 3 or 4, the costs are better than other cases, but there is no much difference between 3 and 4. 0 0.05 0.1 0.15 0.2 1 2 3 4 0.5 0.6 0.7 0.8 0.9 1 θ-initλ MinCost 0.6 0.65 0.7 0.75 0.8 0.85 0.9 Figure 3.",
                "Min Cost on TDT3 (δ =0.15) 0 0.05 0.1 0.15 0.2 1 2 3 4 0 0.5 1 1.5 2 2.5 x 10 8 θ-init λ Comparingtimes 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 8 Figure 4.",
                "Comparing times on TDT3 (δ =0.15) Figure 4 gives the comparing times used by system-3 on TDT3 with the same parameters as figure 3.",
                "The comparing times are strongly dependent onθ init.",
                "Because the greaterθ init is, the less stories combined together, the more comparing times are needed for new event decision.",
                "So we useθ init =0.13,λ =3,δ =0.15 for system-3, 4, and 5.",
                "In this parameter setting, we can get both low minimum normalized costs and less comparing times. 8.",
                "CONCLUSION We have proposed a news indexing-tree based detection procedure in our model.",
                "It reduces comparing times to about one seventh of traditional method without hurting NED accuracy.",
                "We also have presented two extensions to the basic TF-IDF model.",
                "The first extension is made by adjust term weights based on term distributions between the whole corpus and a cluster story set.",
                "And the second extension to basic TF-IDF model is better use of term types (named entities types and part-of-speed) according to news categories.",
                "Our experimental results on TDT2 and TDT3 datasets show that both of the two extensions contribute significantly to improvement in accuracy.",
                "We did not consider news time information as a clue for NED task, since most of the topics last for a long time and TDT data sets only span for a relative short period (no more than 6 months).",
                "For the future work, we want to collect news set which span for a longer period from internet, and integrate time information in NED task.",
                "Since topic is a relative coarse-grained news cluster, we also want to refine cluster granularity to event-level, and identify different events and their relations within a topic.",
                "Acknowledgments This work is supported by the National Natural Science Foundation of China under Grant No. 90604025.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsor. 9.",
                "REFERENCES [1] http://www.nist.gov/speech/tests/tdt/index.htm [2] In Topic Detection and Tracking.",
                "Event-based Information Organization.",
                "Kluwer Academic Publishers, 2002. .01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90 1 2 5 10 20 40 60 80 90 False Alarm Probability (in %) MissProbability(in%) SYSTEM1 Topic Weighted Curve SYSTEM1 Min Norm(Cost) SYSTEM2 Topic Weighted Curve SYSTEM2 Min Norm(Cost) SYSTEM3 Topic Weighted Curve SYSTEM3 Min Norm(Cost) SYSTEM4 Topic Weighted Curve SYSTEM4 Min Norm(Cost) SYSTEM5 Topic Weighted Curve SYSTEM5 Min Norm(Cost) Random Performance Figure 5.",
                "DET curves on TDT3 [3] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B.T.",
                "Archibald, and X. Liu.",
                "Learning Approaches for Detecting and Tracking News Events.",
                "In IEEE Intelligent Systems Special Issue on Applications of Intelligent Information Retrieval, volume 14 (4), 1999, 32-43. [4] Y. Yang, T. Pierce, and J. Carbonell.",
                "A Study on Retrospective and On-line Event Detection.",
                "In Proceedings of SIGIR-98, Melbourne, Australia, 1998, 28-36. [5] J. Allan, V. Lavrenko, D. Malin, and R. Swan.",
                "Detections, Bounds, and Timelines: Umass and tdt-3.",
                "In Proceedings of Topic Detection and Tracking Workshop (TDT-3), Vienna, VA, 2000, 167-174. [6] R. Papka and J. Allan.",
                "On-line New Event Detection Using Single Pass Clustering TITLE2:.",
                "Technical Report UM-CS1998-021, 1998. [7] W. Lam, H. Meng, K. Wong, and J.",
                "Yen.",
                "Using Contextual Analysis for News Event Detection.",
                "International Journal on Intelligent Systems, 2001, 525-546. [8] B. Thorsten, C. Francine, and F. Ayman.",
                "A System for New Event Detection.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2003, 330-337. [9] S. Nicola and C. Joe.",
                "Combining Semantic and Syntactic Document Classifiers to Improve First Story Detection.",
                "In Proceedings of the 24th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topicconditioned Novelty Detection.",
                "In Proceedings of the 8th ACM SIGKDD International Conference, ACM Press. 2002, 688-693. [11] M. Juha, A.M. Helena, and S. Marko.",
                "Applying Semantic Classes in Event Detection and Tracking.",
                "In Proceedings of International Conference on Natural Language Processing (ICON 2002), 2002, pages 175-183. [12] M. Juha, A.M. Helena, and S. Marko.",
                "Simple Semantics in Topic Detection and Tracking.",
                "Information Retrieval, 7(3-4): 2004, 347-368. [13] K. Giridhar and J. Allan.",
                "Text Classification and Named Entities for New Event Detection.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2004, 297-304. [14] J. P. Callan, W. B. Croft, and S. M. Harding.",
                "The INQUERY Retrieval System.",
                "In Proceedings of DEXA-92, 3rd International Conference on Database and Expert Systems Applications, 1992, 78-83. [15] R. Krovetz.",
                "Viewing Morphology as An Inference Process.",
                "In Proceedings of ACM SIGIR93, 1993, 61-81. [16] Y. Yang and J. Pedersen.",
                "A Comparative Study on Feature Selection in Text Categorization.",
                "In J. D. H. Fisher, editor, The Fourteenth International Conference on Machine Learning (ICML97), Morgan Kaufmann, 1997, 412-420. [17] T. M. Cover, and J.A.",
                "Thomas.",
                "Elements of Information Theory.",
                "Wiley. 1991. [18] The linguistic data consortium, http://www.ldc,upenn.edu/. [19] The 2001 TDT task definition and evaluation plan, http://www.nist.gov/speech/tests/tdt/tdt2001/evalplan.htm. [20] R. E. Schapire and Y.",
                "Singer.",
                "Boostexter: A Boosting-based System for Text Categorization.",
                "In Machine Learning 39(2/3):1, Kluwer Academic Publishers, 2000, 35-168. [21] K. Giridhar and J. Allan. 2005.",
                "Using Names and Topics for New Event Detection.",
                "In Proceedings of Human Technology Conference and Conference on Empirical Methods in Natural Language, Vancouver, 2005, 121-128"
            ],
            "original_annotated_samples": [
                "Obviously, modified model can meet all the requirements of the five term classes listed above. 5.2 Term Reweighting Based on Term Type and <br>story class</br> Previous work found that some classes of news stories could achieve good improvements by giving extra weight to named entities."
            ],
            "translated_annotated_samples": [
                "Obviamente, el modelo modificado puede cumplir con todos los requisitos de las cinco clases de términos mencionadas anteriormente. 5.2 Reajuste de Términos Basado en el Tipo de Término y la Clase de Historia. Trabajos anteriores encontraron que algunas clases de historias de noticias podrían lograr mejoras significativas al dar un peso adicional a las entidades nombradas."
            ],
            "translated_text": "Detección de nuevos eventos basada en árbol de indexación y entidad nombrada Zhang Kuo Universidad Tsinghua Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Universidad Tsinghua Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Universidad Tsinghua Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn RESUMEN La Detección de Nuevos Eventos (NED) tiene como objetivo detectar de una o varias corrientes de noticias cuál se informa sobre un nuevo evento (es decir, no reportado previamente). Con el abrumador volumen de noticias disponible hoy en día, hay una creciente necesidad de un sistema NED que sea capaz de detectar eventos nuevos de manera más eficiente y precisa. En este artículo proponemos un nuevo modelo de NED para acelerar la tarea de NED mediante el uso dinámico de un árbol de indexación de noticias. Además, basándose en la observación de que los términos de diferentes tipos tienen diferentes efectos para la tarea de NED, se proponen dos enfoques de reponderación de términos para mejorar la precisión de NED. En el primer enfoque, proponemos ajustar dinámicamente los pesos de los términos basados en los clusters de historias previas y en el segundo enfoque, proponemos emplear estadísticas en los datos de entrenamiento para aprender el modelo de reajuste de entidades nombradas para cada clase de historias. Los resultados experimentales en dos conjuntos de datos del Consorcio de Datos Lingüísticos (LDC), TDT2 y TDT3, muestran que el modelo propuesto puede mejorar significativamente tanto la eficiencia como la precisión de la tarea de NED, en comparación con el sistema base y otros sistemas existentes. Categorías y Descriptores de Asignaturas H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información; H.4.2 [Aplicaciones de Sistemas de Información]: Tipos de Sistemas de Apoyo a Decisiones. Términos generales Algoritmos, Rendimiento, Experimentación 1. El programa de Detección y Seguimiento de Temas (TDT) tiene como objetivo desarrollar técnicas que puedan organizar, buscar y estructurar de manera efectiva materiales de texto de noticias de una variedad de agencias de noticias y medios de difusión [1]. La Detección de Eventos Nuevos (DEN) es una de las cinco tareas en TDT. Es la tarea de identificación en línea del informe más temprano para cada tema tan pronto como ese informe llegue en la secuencia de documentos. Un tema se define como un evento o actividad seminal, junto con eventos y actividades directamente relacionados [2]. Un evento se define como algo (no trivial) que sucede en un lugar y en un momento determinados [3]. Por ejemplo, cuando una bomba explota en un edificio, la explosión es el evento seminal que desencadena el tema, y otras historias sobre el mismo tema serían aquellas que discuten los esfuerzos de salvamento, la búsqueda de los perpetradores, arrestos y juicios, entre otros. La información útil de noticias suele estar enterrada en una masa de datos generados a diario. Por lo tanto, los sistemas NED son muy útiles para las personas que necesitan detectar información novedosa de un flujo de noticias en tiempo real. Estas necesidades de la vida real a menudo se presentan en ámbitos como los mercados financieros, el análisis de noticias y la recopilación de inteligencia. En la mayoría de los sistemas NED de última generación, cada noticia disponible se compara con todas las noticias previamente recibidas. Si todas las similitudes entre ellos no superan un umbral, entonces la historia desencadena un nuevo evento. Por lo general, se presentan en forma de similitud de coseno o métrica de similitud de Hellinger. El problema central de NED es identificar si dos historias tratan sobre el mismo tema. Obviamente, estos sistemas no pueden aprovechar la información del tema. Además, no es aceptable en aplicaciones reales debido a la gran cantidad de cálculos requeridos en el proceso de NED. Otros sistemas organizan historias anteriores en grupos (cada grupo corresponde a un tema), y la nueva historia se compara con los grupos anteriores en lugar de con historias. Esta forma puede reducir significativamente los tiempos de comparación. Sin embargo, se ha demostrado que este método es menos preciso [4, 5]. Esto se debe a que a veces las historias dentro de un tema se alejan mucho entre sí, lo que podría resultar en una baja similitud entre una historia y su tema. Por otro lado, algunos sistemas NED propuestos intentaron mejorar la precisión al hacer un mejor uso de las entidades nombradas [10, 11, 12, 13]. Sin embargo, ninguno de los sistemas ha considerado que los términos de diferentes tipos (por ejemplo, Los sustantivos, verbos o nombres de personas tienen diferentes efectos en diferentes clases de historias para determinar si dos historias tratan sobre el mismo tema. Por ejemplo, los nombres de los candidatos electorales (Nombre de la persona) son muy importantes para las historias de la clase de elecciones; las ubicaciones (Nombre de la ubicación) donde ocurrieron los accidentes son importantes para las historias de la clase de accidentes. Por lo tanto, en NED, todavía existen los siguientes tres problemas por investigar: (1) ¿Cómo acelerar el procedimiento de detección sin disminuir la precisión de la detección? (2) ¿Cómo aprovechar mejor la información de los grupos (temas) para mejorar la precisión? (3) ¿Cómo obtener una mejor representación de historias de noticias mediante una mejor comprensión de las entidades nombradas? Impulsados por estos problemas, hemos propuesto tres enfoques en este documento. (1) Para hacer que el procedimiento de detección sea más rápido, proponemos un nuevo procedimiento de NED basado en un árbol de indexación de noticias creado dinámicamente. El índice de historias en árbol se crea ensamblando historias similares para formar grupos de noticias en diferentes jerarquías según sus valores de similitud. Las comparaciones entre la historia actual y los grupos anteriores podrían ayudar a encontrar la historia más similar en menos tiempo de comparación. El nuevo procedimiento puede reducir la cantidad de veces de comparación sin afectar la precisión. Utilizamos los grupos del primer piso en el árbol de indexación como temas de noticias, en los cuales los pesos de los términos se ajustan dinámicamente según la distribución de términos en los grupos. En este enfoque, se utiliza adecuadamente la información de los grupos (temas), por lo que se evita el problema de la descentralización del tema. Basándonos en observaciones de las estadísticas obtenidas de los datos de entrenamiento, encontramos que los términos de diferentes tipos (por ejemplo, Los sustantivos y verbos tienen diferentes efectos para diferentes clases de historias al determinar si dos historias tratan sobre el mismo tema. Y proponemos utilizar estadísticas para optimizar los pesos de los términos de diferentes tipos en una historia de acuerdo a la clase de noticias a la que pertenece la historia. En el conjunto de datos TDT3, el nuevo modelo NED solo utiliza un 14.9% en comparación con los tiempos del modelo básico, mientras que su costo normalizado mínimo es de 0.5012, lo que es 0.0797 mejor que el modelo básico, y también mejor que cualquier otro resultado previamente reportado para este conjunto de datos [8, 13]. El resto del documento está organizado de la siguiente manera. Comenzamos este artículo resumiendo el trabajo previo en NED en la sección 2. La sección 3 presenta el modelo básico para NED que la mayoría de los sistemas actuales utilizan. La sección 4 describe nuestro nuevo procedimiento de detección basado en el índice de árbol de noticias. En la sección 5, se proponen dos métodos de reponderación de términos para mejorar la precisión de NED. La sección 6 presenta nuestros datos experimentales y métricas de evaluación. Finalmente concluimos con los resultados experimentales en la Sección 7, y las conclusiones y trabajo futuro en la Sección 8. TRABAJO RELACIONADO Papka et al. propusieron el agrupamiento de un solo paso en NED [6]. Cuando se encontraba una nueva historia, esta era procesada inmediatamente para extraer características de términos y se construía una representación de consulta del contenido de la historia. Luego se comparó con todas las consultas anteriores. Si el documento no generaba ninguna consulta al exceder un umbral, se marcaba como un nuevo evento. Lam et al construyen representaciones de consultas previas de grupos de historias, cada una de las cuales corresponde a un tema [7]. De esta manera se realizan comparaciones entre historias y grupos. En los últimos años, la mayoría de los trabajos se han centrado en proponer mejores métodos para la comparación de historias y la representación de documentos. Brants et al. [8] extendieron un modelo básico incremental de TF-IDF para incluir modelos específicos de fuente, normalización de puntajes de similitud basada en promedios específicos de documentos, normalización de puntajes de similitud basada en promedios específicos de pares de fuentes, reponderación de términos basada en frecuencias de eventos inversas y segmentación de documentos. Se mostraron buenas mejoras en los puntos de referencia de TDT. Stokes et al. [9] utilizaron una combinación de evidencia de dos representaciones distintas del contenido de un documento. Una de las representaciones era el vector de texto libre habitual, la otra hacía uso de cadenas léxicas (creadas utilizando WordNet) para construir otro vector de términos. Luego, las dos representaciones se combinan de forma lineal. Se logró un aumento marginal en la efectividad cuando se utilizó la representación combinada. Algunos esfuerzos se han realizado sobre cómo utilizar entidades nombradas para mejorar la desambiguación de entidades nombradas (NED). Yang et al. asignaron un peso cuatro veces mayor a las entidades con nombres de ubicación que a otros términos y entidades con nombres [10]. El grupo de investigación DOREMI combinó similitudes semánticas de nombres de personas, nombres de lugares y tiempo junto con similitud textual [11][12]. El grupo de investigación de UMass [13] dividió la representación del documento en dos partes: entidades nombradas y entidades no nombradas. Y se descubrió que algunas clases de noticias podían lograr un mejor rendimiento utilizando la representación de entidades nombradas, mientras que otras clases de noticias podían lograr un mejor rendimiento utilizando la representación sin entidades nombradas. Tanto [10] como [13] utilizaron la técnica de categorización de texto para clasificar previamente las noticias. En [13] las noticias se clasifican automáticamente al principio, y luego se prueban las sensibilidades de los nombres y términos que no son nombres para NED para cada clase. En [10], se eliminan los términos frecuentes de cada clase de la representación del documento. Por ejemplo, la palabra \"elección\" no ayuda a identificar diferentes elecciones. En su trabajo, no se investiga la efectividad de diferentes tipos de nombres (o términos con diferentes categorías gramaticales) para NED en diferentes clases de noticias. Utilizamos análisis estadístico para revelar el hecho y utilizarlo para mejorar el rendimiento de NED. 3. MODELO BÁSICO En esta sección, presentamos el modelo básico de Detección de Nuevos Eventos que es similar a lo que la mayoría de los sistemas actuales aplican. Luego, proponemos nuestro nuevo modelo ampliando el modelo básico. Los sistemas de detección de eventos nuevos utilizan flujos de noticias como entrada, en los cuales las historias están estrictamente ordenadas por tiempo. Solo están disponibles las historias recibidas previamente al tratar con la historia actual. La salida es una decisión sobre si la historia actual es un evento nuevo o no y la confianza de la decisión. Por lo general, un modelo NED consta de tres partes: representación de la historia, cálculo de similitud y procedimiento de detección. 3.1 Representación de la historia Se necesita preprocesamiento antes de generar la representación de la historia. Para el preprocesamiento, tokenizamos palabras, reconocemos abreviaturas, normalizamos abreviaturas, agregamos etiquetas de partes del discurso, eliminamos palabras vacías incluidas en la lista de paradas utilizada en InQuery [14], reemplazamos palabras con sus raíces utilizando el algoritmo K-stem[15], y luego generamos un vector de palabras para cada noticia. Utilizamos el modelo TF-IDF incremental para el cálculo del peso de los términos [4]. En un modelo TF-IDF, la frecuencia de término en un documento de noticias se pondera por la frecuencia inversa del documento, la cual se genera a partir del corpus de entrenamiento. Cuando surge un nuevo término en el proceso de prueba, hay dos soluciones: simplemente ignorar el nuevo término o establecer df del término como una constante pequeña (por ejemplo, df = 1). El nuevo término recibe un peso demasiado bajo en la primera solución (0) y un peso demasiado alto en la segunda solución. En el modelo TF-IDF incremental, las frecuencias de documentos se actualizan dinámicamente en cada paso de tiempo t: 1( ) ( ) ( )t t D tdf w df w df w−= + (1) donde Dt representa el conjunto de noticias recibidas en el tiempo t, y dfDt(w) significa el número de documentos en los que aparece el término w, y dft(w) significa el número total de documentos en los que aparece el término w antes del tiempo t. En este trabajo, cada ventana de tiempo incluye 50 noticias. Así, cada historia d recibida en t se representa de la siguiente manera: 1 2{ ( , , ), ( , , ),..., ( , , )}nd peso d t w peso d t w peso d t w→ donde n significa el número de términos distintos en la historia d, y ( , , )peso d t w significa el peso del término w en la historia d en el tiempo t: log( ( , ) 1) log(( 1) /( ( ) 0.5)) ( , , ) log( ( , ) 1) log(( 1) /( ( ) 0.5)) t t t t w d tf d w N df w peso d t w tf d w N df w ∈ + + + = + + +∑ (2) donde Nt significa el número total de historias de noticias antes del tiempo t, y tf(d,w) significa cuántas veces aparece el término w en la historia de noticias d. 3.2 Cálculo de Similitud Utilizamos la distancia de Hellinger para el cálculo de la similitud entre dos historias, para dos historias d y d en el tiempo t, su similitud se define de la siguiente manera: , ( , , ) ( , , ) * ( , , ) w d d sim d d t peso d t w peso d t w ∈ = ∑ (3) 3.3 Procedimiento de Detección Para cada historia d recibida en el paso de tiempo t, el valor ( ) ( ) ( ) ( ( , , )) tiempo d tiempo d n d max sim d d t < = (4) es un puntaje utilizado para determinar si d es una historia sobre un nuevo tema y al mismo tiempo es una indicación de la confianza en nuestra decisión [8]. tiempo(d) significa el tiempo de publicación de la historia d. Si el puntaje supera el umbral θ new, entonces existe un documento suficientemente similar, por lo tanto, d es una historia antigua, de lo contrario, no hay un documento previo suficientemente similar, por lo tanto, d es una historia nueva. 4. Nuevo procedimiento NED Los sistemas NED tradicionales se pueden clasificar en dos tipos principales en cuanto al procedimiento de detección: (1) tipo S-S, en el que la historia en cuestión se compara con cada historia recibida previamente, y se utiliza la mayor similitud para determinar si la historia actual trata sobre un evento nuevo; (2) tipo S-C, en el que la historia en cuestión se compara con todos los clusters anteriores, cada uno de los cuales representa un tema, y se utiliza la mayor similitud para la decisión final sobre la historia actual. Si la similitud más alta supera el umbral θ nuevo, entonces es una historia antigua y se coloca en el clúster más similar; de lo contrario, es una historia nueva y se crea un nuevo clúster. Trabajos anteriores muestran que la primera forma es más precisa que la segunda [4][5]. Dado que a veces las historias dentro de un tema se alejan mucho entre sí, una historia puede tener una similitud muy baja con su tema. Por lo tanto, utilizar similitudes entre historias para determinar una nueva historia es mejor que utilizar similitudes entre historias y grupos. Sin embargo, el primer método requiere mucho más tiempo de comparación, lo que significa que es menos eficiente. Proponemos un nuevo procedimiento de detección que utiliza comparaciones con clusters anteriores para ayudar a encontrar la historia más similar en menos tiempo de comparación, y la decisión final sobre el nuevo evento se toma de acuerdo con la historia más similar. Por lo tanto, podemos obtener tanto la precisión de los métodos de tipo S-S como la eficiencia de los métodos de tipo S-C. El nuevo procedimiento crea un árbol de indexación de noticias dinámicamente, en el cual se agrupan historias similares para formar una jerarquía de clusters. Indexamos historias similares juntas por su ancestro común (un nodo de clúster). Historias diferentes están indexadas en diferentes grupos. Cuando una historia está llegando, utilizamos comparaciones entre la historia actual y los grupos jerárquicos anteriores para ayudar a encontrar la historia más similar que es útil para la toma de decisiones sobre nuevos eventos. Después de que se tome la decisión sobre el nuevo evento, la historia actual se inserta en el árbol de indexación para la detección siguiente. El índice de noticias en forma de árbol se define formalmente de la siguiente manera: S-Tree = {r, NC, NS, E}, donde r es la raíz de S-Tree, NC es el conjunto de todos los nodos de clúster, NS es el conjunto de todos los nodos de historia, y E es el conjunto de todos los bordes en S-Tree. Definimos un conjunto de restricciones para un S-Árbol: ⅰ. , es un nodo no terminal en el árbolC i i N i∀ ∈ → ⅱ. , es un nodo terminal en el árbolS i i N i∀ ∈ → ⅲ. , el grado de salida de es al menos 2C i i N i∀ ∈ → ⅳ. , se representa como el centroide de sus descendientesC i i iN∀ ∈ → Para una noticia di, el procedimiento de comparación y el procedimiento de inserción basados en el árbol de indexación se definen de la siguiente manera. Un ejemplo se muestra en la Figura 1 y la Figura 2. Figura 1. Procedimiento de comparación Figura 2. Procedimiento de comparación de inserción: Paso 1: comparar di con todos los nodos hijos directos de r y seleccionar λ nodos con las similitudes más altas, por ejemplo, C1 2 y C1 3 en la Figura 1. Paso 2: para cada nodo seleccionado en el paso anterior, por ejemplo. C1 2, compara di con todos sus nodos hijos directos y selecciona λ nodos con las similitudes más altas, por ejemplo. C2 2 y d8. Repetir el paso 2 para todos los nodos no terminales. Paso 3: registrar el nodo terminal con la mayor similitud a di, por ejemplo, s5, y el valor de similitud (0.20). Insertando di en el S-árbol con r como raíz: Encuentra el nodo n que es hijo directo de r en el camino desde r hasta el nodo terminal con la similitud más alta s, por ejemplo. C1 2. Si s es menor que θ init + (h-1)δ, entonces agregar di al árbol como un hijo directo de r. De lo contrario, si n es un nodo terminal, crear un nodo de clúster en lugar de n, y agregar tanto n como di como sus hijos directos; si n es un nodo no terminal, repetir este procedimiento e insertar di en el subárbol con n como raíz de forma recursiva. Aquí h es la longitud entre n y la raíz del árbol S. Cuanto más similares sean las historias en un grupo, mejor representará el grupo a las historias en él. Por lo tanto, no imponemos restricciones en la altura máxima de los árboles y el grado de un nodo. Por lo tanto, no podemos dar la complejidad de este procedimiento basado en árboles de indexación. Pero proporcionaremos el número de veces de comparación necesarias por el nuevo procedimiento en nuestros experimentos en la sección 7.5. En esta sección, se proponen dos métodos de reponderación de términos para mejorar la precisión de NED. En el primer método, se explora una nueva forma de utilizar de manera más efectiva la información de los grupos (temas). El segundo encuentra una mejor manera de utilizar entidades nombradas basadas en la clasificación de noticias. 5.1 Reajuste de términos basado en la distancia de distribución TF-IDF es el modelo más prevalente utilizado en sistemas de recuperación de información. La idea básica es que cuanto menos documentos aparezca un término, más importante es el término en la discriminación de documentos (relevantes o no relevantes para una consulta que contenga el término). Sin embargo, en el dominio de TDT, necesitamos discriminar documentos en función de los temas en lugar de las consultas. Intuitivamente, el uso de vectores de clúster (tema) para comparar con historias de noticias posteriores debería tener un mejor rendimiento que el uso de vectores de historias. Desafortunadamente, los resultados experimentales no respaldan esta intuición [4][5]. Basándonos en la observación de los datos, encontramos que la razón es que un tema de noticias generalmente contiene muchos eventos directa o indirectamente relacionados, y todos ellos tienen sus propios subtemas que suelen ser diferentes entre sí. Tomemos el tema descrito en la sección 1 como ejemplo, eventos como la explosión y el rescate tienen similitudes muy bajas con eventos sobre juicios criminales, por lo tanto, las historias sobre juicios tendrían una baja similitud con el vector de tema construido en base a sus eventos previos. Esta sección se centra en cómo hacer un uso efectivo de la información del tema y al mismo tiempo evitar el problema de la descentralización del contenido. Al principio, clasificamos los términos en 5 clases para ayudar en el análisis de las necesidades del modelo modificado: Clase de términos A: términos que ocurren con frecuencia en todo el corpus, por ejemplo, año y personas. Los términos de esta clase deben recibir pesos bajos porque no ayudan mucho en la discriminación de temas. Clase B de términos: términos que ocurren con frecuencia dentro de una categoría de noticias, por ejemplo, elección, tormenta. Son útiles para distinguir dos historias en diferentes categorías de noticias. Sin embargo, no pueden proporcionar información para determinar si dos historias tratan sobre el mismo tema o temas diferentes. En otras palabras, los términos elección y tormenta no son útiles para diferenciar dos campañas electorales y dos desastres de tormenta. Por lo tanto, los términos de esta clase deberían asignarse pesos más bajos. Clase de término C: términos que ocurren con frecuencia en un tema y raramente en otros temas, por ejemplo, el nombre de un avión accidentado, el nombre de un huracán específico. Las noticias que pertenecen a diferentes temas rara vez tienen términos superpuestos en esta clase. Cuanto más frecuentemente aparezca un término en un tema, más importante es el término para una historia perteneciente al tema, por lo tanto, el término debería tener un peso más alto. Clase de término D: términos que aparecen en un tema de manera exclusiva, pero no con frecuencia. Por ejemplo, el nombre de un bombero que se desempeñó muy bien en una acción de salvamento, que puede aparecer en solo dos o tres historias pero nunca ha aparecido en otros temas. Los términos de este tipo deberían recibir más peso que en el modelo TF-IDF. Sin embargo, dado que no son populares en el tema, no es apropiado asignarles pesos demasiado altos. Clase de término E: términos con baja frecuencia en documentos y que aparecen en diferentes temas. Los términos de esta clase deberían recibir pesos más bajos. Ahora analizamos si el modelo TF-IDF puede asignar pesos adecuados a las cinco clases de términos. Obviamente, los términos de la clase A tienen un peso bajo en el modelo TF-IDF, lo cual es acorde con el requisito descrito anteriormente. En el modelo TF-IDF, los términos de la clase B dependen altamente del número de historias en una clase de noticias. El modelo TF-IDF no puede proporcionar pesos bajos si la historia que contiene el término pertenece a una clase de noticias relativamente pequeña. Para un término de clase C, cuanto más frecuentemente aparezca en un tema, menos peso le da el modelo TFIDF. Esto entra en conflicto directo con el requisito de términos en la clase C. Para los términos de la clase D, el modelo TF-IDF les asigna pesos altos de manera correcta. Pero para los términos de la clase E, el modelo TF-IDF asigna pesos altos a ellos que no son conformes con el requisito de pesos bajos. En resumen, los términos de clase B, C, E no pueden ser ponderados adecuadamente en el modelo TF-IDF. Por lo tanto, proponemos un modelo modificado para resolver este problema. Cuando θ init y θ new están configurados de cerca, asumimos que la mayoría de las historias en un clúster de primer nivel (un nodo hijo directo del nodo raíz) están en el mismo tema. Por lo tanto, utilizamos un clúster de primer nivel para capturar la distribución de términos (df para todos los términos dentro del clúster) dentro del tema de forma dinámica. La divergencia de Kullback-Leibler de la distribución de términos en un clúster de primer nivel y el conjunto completo de historias se utiliza para ajustar los pesos de los términos: ( , , ) * (1 * ( || )) ( , , ) ( , , ) * (1 * ( || )) cw tw cw tw w d D peso d t w KL P P peso d t w peso d t w KL P P γ γ ∈ + = +∑ (5) donde ( ) ( ) ( ) ( ) 1,cw cw c c c c df w df w p y p y N N = = − (6) ( ) ( ) ( ) ( ) 1,t t tw tw t t df w df w p y p y N N = = − (7) donde dfc(w) es el número de documentos que contienen el término w dentro del clúster C, y Nc es el número de documentos en el clúster C, y Nt es el número total de documentos que llegan antes del paso de tiempo t. γ es un parámetro constante, actualmente configurado manualmente en 3. La divergencia de Kullback-Leibler se define de la siguiente manera [17]: La idea básica es que, para una historia en un tema, cuanto más ocurre un término dentro del tema y menos ocurre en otros temas, se le deben asignar pesos más altos. Obviamente, el modelo modificado puede cumplir con todos los requisitos de las cinco clases de términos mencionadas anteriormente. 5.2 Reajuste de Términos Basado en el Tipo de Término y la Clase de Historia. Trabajos anteriores encontraron que algunas clases de historias de noticias podrían lograr mejoras significativas al dar un peso adicional a las entidades nombradas. Pero encontramos que los términos de diferentes tipos deben recibir una cantidad diferente de peso adicional para diferentes clases de noticias. Utilizamos open-NLP1 para reconocer tipos de entidades nombradas y etiquetas de partes del discurso para términos que aparecen en noticias. Los tipos de entidades nombradas incluyen nombre de persona, nombre de organización, nombre de ubicación, fecha, hora, dinero y porcentaje, y se seleccionan cinco partes del discurso: ninguno (NN), verbo (VB), adjetivo (JJ), adverbio (RB) y número cardinal (CD). El análisis estadístico muestra tipos de términos discriminativos a nivel de tema para diferentes clases de historias. Por conveniencia, los tipos de entidad nombrada y etiquetas de parte de la oración se llaman uniformemente tipo de término en las secciones siguientes. Determinar si dos historias tratan sobre el mismo tema es un componente básico para la tarea de NED. Por lo tanto, al principio usamos la estadística χ² para calcular las correlaciones entre términos y temas. Para un término t y un tema T, se deriva una tabla de contingencia: Tabla 1. Una tabla de contingencia de 2×2 con el número de documentos pertenecientes al tema T y no pertenecientes al tema T incluye t A B no incluye t C D. La estadística χ² para un término específico t con respecto al tema T se define como [16]: χ² = ( ) ( ) * ( ) ( ) * ( ) * ( ) * ( ) w T A B C D AD CB A C B D A B C D + + + − + + + + (9). Los temas de noticias para la tarea TDT se clasifican además en 11 reglas de interpretaciones (ROIs) 2. El ROI se puede ver como una clase de historias de nivel superior. El promedio de correlación entre un tipo de término y un ROI de tema se calcula como: 2 avg 2 ( , )( ( , ) )k m m km kT R w P w TP R p w T R P χ χ ∈ ∈ ∑ ∑（ , ）= 1 1 k=1…K, m=1…M (10) donde K es el número de tipos de términos (fijado constantemente en 12 en el artículo). M es el número de nuevas clases (ROIs, establecido en el conjunto 11 del artículo). Pk representa el conjunto de todos los términos de tipo k, y Rm representa el conjunto de todos los temas de la clase m, p(t,T) significa la probabilidad de que t ocurra en el tema T. Debido a limitaciones de espacio, solo se enumeran partes de los tipos de términos (9 tipos de términos) y partes de las clases de noticias (8 clases) en la tabla 2 con los valores promedio de correlación entre ellos. Las estadísticas se derivan de datos etiquetados en el corpus TDT2. (Los resultados en la tabla 2 ya están normalizados para mayor comodidad en la comparación). Las estadísticas en la tabla 2 indican la utilidad de diferentes tipos de términos en la discriminación de temas con respecto a diferentes clases de noticias. Podemos ver que el nombre de la ubicación es el tipo de término más útil para tres clases de noticias: Desastres Naturales, Violencia o Guerra, Finanzas. Y para otras tres categorías: Elecciones, Casos Legales/Criminales, Ciencia y Descubrimiento, el nombre de la persona es el tipo de término más discriminativo. Para Escándalos/Audiencias, la fecha es la información más importante para la discriminación de temas. Además, los casos legales/criminales y los temas financieros tienen una correlación más alta con términos relacionados con el dinero, mientras que la ciencia y el descubrimiento tienen una correlación más alta con términos de porcentaje. Los términos no nominales son más estables para diferentes clases. 1. http://opennlp.sourceforge.net/ 2. http://projects.ldc.upenn.edu/TDT3/Guide/label.html A partir del análisis de la tabla 2, es razonable ajustar el peso de los términos según su tipo de término y la clase de noticias a la que pertenece la historia. Los nuevos pesos de los términos se recalculan de la siguiente manera: ( ) ( ) ( ) ( ) ( , , ) * ( , , ) ( , , ) * clase d D tipo w T clase d D tipo w w d peso d t w peso d t w peso d t w α α ∈ = ∑ (11) donde type(w) representa el tipo del término w, y class(d) representa la clase de la historia d, c kα es el parámetro de recalibración para la clase de noticias c y el tipo de término k. En el trabajo, simplemente utilizamos las estadísticas de la tabla 2 como parámetros de recalibración. Aunque usar las estadísticas directamente puede que no sea la mejor opción, no discutimos cómo obtener automáticamente los mejores parámetros. Intentaremos utilizar técnicas de aprendizaje automático para obtener los mejores parámetros en el trabajo futuro. En el trabajo, utilizamos BoosTexter [20] para clasificar todas las historias en uno de los 11 ROIs. BoosTexter es un programa de aprendizaje automático basado en boosting, que crea una serie de reglas simples para construir un clasificador de datos de texto o atributos-valor. Utilizamos el peso del término generado utilizando el modelo TF-IDF como característica para la clasificación de historias. Entrenamos el modelo con las 12000 historias en inglés evaluadas en TDT2, y clasificamos el resto de las historias en TDT2 y todas las historias en TDT3. Los resultados de clasificación se utilizan para la reponderación de términos en la fórmula (11). Dado que las etiquetas de clase de las historias de tema apagado no se proporcionan en los conjuntos de datos de TDT, no podemos proporcionar la precisión de clasificación aquí. Por lo tanto, no discutimos los efectos de la precisión de la clasificación en el rendimiento de NED en el artículo. 6. CONFIGURACIÓN EXPERIMENTAL 6.1 Conjuntos de datos Utilizamos dos conjuntos de datos de LDC [18], TDT2 y TDT3, para nuestros experimentos. TDT2 contiene noticias desde enero hasta junio de 1998. Contiene alrededor de 54,000 historias de fuentes como ABC, Associated Press, CNN, New York Times, Public Radio International, Voice of America, etc. Solo se consideraron historias en inglés en la colección. TDT3 contiene aproximadamente 31,000 historias en inglés recopiladas de octubre a diciembre de 1998. Además de las fuentes utilizadas en TDT2, también contiene historias de las transmisiones de televisión de NBC y MSNBC. Utilizamos versiones transcritas de las emisiones de televisión y radio además de noticias textuales. El conjunto de datos TDT2 está etiquetado con alrededor de 100 temas, y aproximadamente 12,000 historias en inglés pertenecen al menos a uno de estos temas. El conjunto de datos TDT3 está etiquetado con alrededor de 120 temas, y aproximadamente 8000 historias en inglés pertenecen al menos a uno de estos temas. Todos los temas están clasificados en 11 Reglas de Interpretación: (1) Elecciones, (2) Escándalos/Audiencias, (3) Casos Legales/Criminales, (4) Desastres Naturales, (5) Accidentes, (6) Violencia o Guerra en Curso, (7) Noticias de Ciencia y Descubrimientos, (8) Finanzas, (9) Nueva Ley, (10) Noticias Deportivas, (11) Varios. Noticias. 6.2 Métrica de evaluación TDT utiliza una función de costo CDet que combina las probabilidades de perder una nueva historia y una falsa alarma [19]: * * * *Det Miss Miss Target FA FA NontargetC C P P C P P= + (12) Tabla 2. Correlación promedio entre tipos de términos y clases de noticias donde CMiss significa el costo de perder una nueva historia, PMiss significa la probabilidad de perder una nueva historia, y PTarget significa la probabilidad de ver una nueva historia en los datos; CFA significa el costo de una falsa alarma, PFA significa la probabilidad de una falsa alarma, y PNontarget significa la probabilidad de ver una historia antigua. El costo CDet se normaliza de tal manera que un sistema perfecto obtiene una puntuación de 0 y un sistema trivial, que es el mejor de marcar todas las historias como nuevas o antiguas, obtiene una puntuación de 1: ( ( * , * ) ) Det Det Miss Target FA Nontarget C Norm C min C P C P = (13) El sistema de detección de eventos nuevos proporciona dos salidas para cada historia. La primera parte es sí o no, indicando si la historia desencadena un nuevo evento o no. La segunda parte es una puntuación que indica la confianza de la primera decisión. Los puntajes de confianza se pueden utilizar para trazar la curva DET, es decir, curvas que representan la probabilidad de falsa alarma frente a la probabilidad de omisión. El costo normalizado mínimo se puede determinar si se elige el umbral óptimo en la puntuación. RESULTADOS EXPERIMENTALES 7.1 Resultados principales Para probar los enfoques propuestos en el modelo, implementamos y probamos cinco sistemas: Sistema-1: este sistema se utiliza como referencia. Se implementa basándose en el modelo básico descrito en la sección 3, es decir, utilizando el modelo TF-IDF incremental para generar pesos de términos y utilizando la distancia de Hellinger para calcular la similitud entre documentos. La normalización del puntaje de similitud también se emplea [8]. Se utiliza el procedimiento de detección S-S. Sistema-2: este sistema es igual que el sistema-1 excepto que se utiliza el procedimiento de detección S-C. Sistema-3: este sistema es igual que el sistema-1 excepto que utiliza el nuevo procedimiento de detección que se basa en un árbol de indexación. Sistema-4: implementado basado en el enfoque presentado en la sección 5.1, es decir, los términos se vuelven a ponderar según la distancia entre las distribuciones de términos en un grupo y todas las historias. Se utiliza el nuevo procedimiento de detección. Sistema-5: implementado basado en el enfoque presentado en la sección 5.2, es decir, los términos de diferentes tipos son reponderados según la clase de noticias utilizando parámetros entrenados. El nuevo procedimiento de detección se utiliza. Los siguientes son algunos otros sistemas NED: Sistema-6: [21] para cada par de historias, calcula tres valores de similitud para entidad nombrada, entidad no nombrada y todos los términos respectivamente. Y emplear Máquina de Vectores de Soporte para predecir nuevo o antiguo utilizando los valores de similitud como características. Sistema-7: [8] extendió un modelo básico incremental de TF-IDF para incluir modelos específicos de origen, normalización de puntuaciones de similitud basada en promedios específicos de documentos, normalización de puntuaciones de similitud basada en promedios específicos de pares de fuentes, etc. Sistema-8: [13] dividió la representación del documento en dos partes: entidades nombradas y entidades no nombradas, y eligió una parte efectiva para cada clase de noticias. La tabla 3 y la tabla 4 muestran los costos normalizados ponderados por tema y los tiempos de comparación en los conjuntos de datos TDT2 y TDT3 respectivamente. Dado que no se contaba con un conjunto de datos de validación para ajustar el umbral θ nuevo en los experimentos en TDT2, solo informamos los costos normalizados mínimos para nuestros sistemas en la tabla 3. El sistema-5 supera a todos los demás sistemas, incluido el sistema-6, y realiza solo 2.78e+8 veces la comparación de tiempos en el procedimiento de detección, lo que representa solo el 13.4% del sistema-1. Tabla 3. Los resultados de NED en los sistemas TDT2 Min Norm(CDet) tiempos de comparación son: Sistema-1 0.5749 2.08e+9, Sistema-2 0.6673 3.77e+8, Sistema-3 0.5765 2.81e+8, Sistema-4 0.5431 2.99e+8, Sistema-5 0.5089 2.78e+8, Sistema-6 0.5300. Al evaluar en los costos normalizados en TDT3, utilizamos los umbrales óptimos obtenidos del conjunto de datos de TDT2 para todos los sistemas. El sistema-2 reduce los tiempos de comparación a 1.29e+9, que es solo el 18.3% del sistema-1, pero al mismo tiempo también obtiene un costo normalizado mínimo deteriorado que es 0.0499 más alto que el sistema-1. El Sistema-3 utiliza el nuevo procedimiento de detección basado en el índice de noticias en árbol. Requiere incluso menos tiempo de comparación que el sistema-2. Esto se debe a que las comparaciones de historias suelen arrojar mayores similitudes que las de grupos de historias, por lo que las historias tienden a combinarse en el sistema-3. Y el sistema-3 es básicamente equivalente al sistema-1 en los resultados de precisión. El System-4 ajusta los pesos de los términos basándose en la distancia de las distribuciones de términos entre todo el corpus y el conjunto de historias del clúster, lo que resulta en una mejora significativa de 0.0468 en comparación con el system-1. El mejor sistema (sistema-5) tiene un costo normalizado mínimo de 0.5012, lo cual es 0.0797 mejor que el sistema-1, y también mejor que cualquier otro resultado previamente reportado para este conjunto de datos [8, 13]. Además, el sistema-5 solo necesita 1.05e+8 veces de comparación, lo que equivale al 14.9% del sistema-1. Tabla 4. Los resultados de NED en los sistemas TDT3 Norm(CDet) Norma mínima(CDet) Tiempos de comparación Sistema-1 0.6159 0.5809 7.04e+8 Sistema-2① 0.6493 0.6308 1.29e+8 Sistema-3② 0.6197 0.5868 1.03e+8 Sistema-4② 0.5601 0.5341 1.03e+8 Sistema-5② 0.5413 0.5012 1.05e+8 Sistema-7 -- 0.5783 -Sistema-8 -- 0.5229 -① θ nuevo=0.13 ② θ inicial=0.13, λ =3, δ =0.15 La Figura 5 muestra las cinco curvas DET para nuestros sistemas en el conjunto de datos TDT3. El Sistema-5 logra el costo mínimo con una tasa de falsas alarmas de 0.0157 y una tasa de omisión de 0.4310. Podemos observar que System4 y System-5 obtienen una menor probabilidad de error de omisión en regiones de baja probabilidad de falsa alarma. La hipótesis es que se transfiere más valor de peso a los términos clave de los temas que a los términos no clave. La puntuación de similitud entre dos historias pertenecientes a diferentes temas es más baja que antes, porque los términos superpuestos suelen no ser términos clave de sus temas. 7.2 Selección de parámetros para la detección de árboles de indexación. La Figura 3 muestra los costos normalizados mínimos obtenidos por el sistema-3 en TDT3 utilizando diferentes parámetros. El parámetro de inicio θ se prueba en seis valores que van desde 0.03 hasta 0.18. Y el parámetro λ se prueba en cuatro valores: 1, 2, 3 y 4. Podemos ver que, cuando θ inicial se establece en 0.12, que es el más cercano a θ nuevo, los costos son más bajos que los demás. Esto es fácil de explicar, porque cuando se agrupan historias pertenecientes al mismo tema en un conjunto, es más razonable que el conjunto represente las historias en él. Cuando el parámetro λ se establece en 3 o 4, los costos son mejores que en otros casos, pero no hay mucha diferencia entre 3 y 4. 0 0.05 0.1 0.15 0.2 1 2 3 4 0.5 0.6 0.7 0.8 0.9 1 θ-initλ MinCost 0.6 0.65 0.7 0.75 0.8 0.85 0.9 Figura 3. Costo mínimo en TDT3 (δ =0.15) 0 0.05 0.1 0.15 0.2 1 2 3 4 0 0.5 1 1.5 2 2.5 x 10 8 θ-inicio λ Comparando tiempos 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 8 Figura 4. Comparando los tiempos en TDT3 (δ =0.15), la Figura 4 muestra los tiempos de comparación utilizados por el sistema-3 en TDT3 con los mismos parámetros que la Figura 3. Los tiempos de comparación dependen fuertemente de θ init. Debido a que cuanto mayor sea θ init, menos historias se combinan juntas, más veces de comparación se necesitan para la decisión de un nuevo evento. Por lo tanto, usamos θ init = 0.13, λ = 3, δ = 0.15 para los sistemas 3, 4 y 5. En esta configuración de parámetros, podemos obtener tanto costos normalizados mínimos bajos como menos tiempos de comparación. CONCLUSIÓN Hemos propuesto un procedimiento de detección basado en un árbol de indexación de noticias en nuestro modelo. Reduce los tiempos de comparación a aproximadamente una séptima parte del método tradicional sin afectar la precisión de NED. También hemos presentado dos extensiones al modelo básico TF-IDF. La primera extensión se realiza ajustando los pesos de los términos basados en las distribuciones de términos entre todo el corpus y un conjunto de historias de un clúster. Y la segunda extensión al modelo básico TF-IDF es un mejor uso de los tipos de términos (tipos de entidades nombradas y partes de la velocidad) de acuerdo con las categorías de noticias. Nuestros resultados experimentales en los conjuntos de datos TDT2 y TDT3 muestran que ambas extensiones contribuyen significativamente a la mejora en la precisión. No consideramos la información de tiempo de noticias como una pista para la tarea de NED, ya que la mayoría de los temas duran mucho tiempo y los conjuntos de datos de TDT solo abarcan un período relativamente corto (no más de 6 meses). Para el trabajo futuro, queremos recopilar un conjunto de noticias que abarque un período más largo de internet e integrar información temporal en la tarea de NED. Dado que el tema es un grupo de noticias relativamente grueso, también queremos refinar la granularidad del grupo a nivel de evento e identificar diferentes eventos y sus relaciones dentro de un tema. Agradecimientos: Este trabajo ha sido apoyado por la Fundación Nacional de Ciencias Naturales de China bajo la subvención No. 90604025. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son del autor(es) y no necesariamente reflejan las del patrocinador. REFERENCIAS [1] http://www.nist.gov/speech/tests/tdt/index.htm [2] En Detección y Seguimiento de Temas. Organización de la información basada en eventos. Kluwer Academic Publishers, 2002. .01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90 1 2 5 10 20 40 60 80 90 Probabilidad de Falsa Alarma (en %) Probabilidad de Omisión (en %) Curva Ponderada por Tema del SISTEMA1 Norma Mínima (Costo) del SISTEMA1 Curva Ponderada por Tema del SISTEMA2 Norma Mínima (Costo) del SISTEMA2 Curva Ponderada por Tema del SISTEMA3 Norma Mínima (Costo) del SISTEMA3 Curva Ponderada por Tema del SISTEMA4 Norma Mínima (Costo) del SISTEMA4 Curva Ponderada por Tema del SISTEMA5 Norma Mínima (Costo) del SISTEMA5 Rendimiento Aleatorio Figura 5. Curvas DET en TDT3 [3] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B.T. Archibald y X. Liu. Enfoques de aprendizaje para detectar y rastrear eventos de noticias. En el número especial de IEEE Intelligent Systems sobre Aplicaciones de Recuperación de Información Inteligente, volumen 14 (4), 1999, páginas 32-43. [4] Y. Yang, T. Pierce y J. Carbonell. Un estudio sobre la detección de eventos retrospectivos y en línea. En Actas de SIGIR-98, Melbourne, Australia, 1998, 28-36. [5] J. Allan, V. Lavrenko, D. Malin y R. Swan. Detecciones, límites y líneas de tiempo: Umass y tdt-3. En Actas del Taller de Detección y Seguimiento de Temas (TDT-3), Viena, VA, 2000, 167-174. [6] R. Papka y J. Allan. Detección de nuevos eventos en línea utilizando agrupamiento de un solo paso TÍTULO2:. Informe técnico UM-CS1998-021, 1998. [7] W. Lam, H. Meng, K. Wong y J. I'm sorry, but \"Yen\" is not a sentence. Can you please provide a sentence for me to translate into Spanish? Utilizando el Análisis Contextual para la Detección de Eventos de Noticias. Revista Internacional de Sistemas Inteligentes, 2001, 525-546. [8] B. Thorsten, C. Francine y F. Ayman. Un sistema para la detección de nuevos eventos. En Actas de la 26ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2003, 330-337. [9] S. Nicola y C. Joe. Combinando clasificadores semánticos y sintácticos de documentos para mejorar la detección de primeras noticias. En Actas de la 24ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell, and C. Jin. \n\nACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell y C. Jin. Detección de novedades condicionada al tema. En Actas de la 8ª Conferencia Internacional de ACM SIGKDD, ACM Press. 2002, 688-693. [11] M. Juha, A.M. Helena y S. Marko. Aplicando Clases Semánticas en la Detección y Seguimiento de Eventos. En Actas de la Conferencia Internacional sobre Procesamiento del Lenguaje Natural (ICON 2002), 2002, páginas 175-183. [12] M. Juha, A.M. Helena y S. Marko. Semántica simple en la detección y seguimiento de temas. Recuperación de información, 7(3-4): 2004, 347-368. [13] K. Giridhar y J. Allan. Clasificación de texto y entidades nombradas para la detección de nuevos eventos. En Actas de la 27ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2004, 297-304. [14] J. P. Callan, W. B. Croft, y S. M. Harding. El Sistema de Recuperación INQUERY. En Actas de DEXA-92, 3ra Conferencia Internacional sobre Aplicaciones de Bases de Datos y Sistemas Expertos, 1992, 78-83. [15] R. Krovetz. Viendo la morfología como un proceso de inferencia. En Actas de ACM SIGIR93, 1993, 61-81. [16] Y. Yang y J. Pedersen. Un estudio comparativo sobre la selección de características en la categorización de textos. En J. D. H. Fisher, editor, La Decimocuarta Conferencia Internacional sobre Aprendizaje Automático (ICML97), Morgan Kaufmann, 1997, 412-420. [17] T. M. Cover y J. A. I'm sorry, but \"Thomas\" is not a sentence. Can you provide more context or a complete sentence for me to translate into Spanish? Elementos de la teoría de la información. Wiley. 1991. [18] El consorcio de datos lingüísticos, http://www.ldc,upenn.edu/. [19] La definición de la tarea TDT 2001 y el plan de evaluación, http://www.nist.gov/speech/tests/tdt/tdt2001/evalplan.htm. [20] R. E. Schapire y Y. Cantante. Boostexter: Un sistema basado en Boosting para la categorización de texto. En Machine Learning 39(2/3):1, Kluwer Academic Publishers, 2000, 35-168. [21] K. Giridhar y J. Allan. 2005. Utilizando nombres y temas para la detección de nuevos eventos. En Actas de la Conferencia de Tecnología Humana y la Conferencia sobre Métodos Empíricos en Lenguaje Natural, Vancouver, 2005, 121-128 ",
            "candidates": [],
            "error": [
                []
            ]
        },
        "linguistic data consortium": {
            "translated_key": "consorcio de datos lingüísticos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "New Event Detection Based on Indexing-tree and Named Entity Zhang Kuo Tsinghua University Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Tsinghua University Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Tsinghua University Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn ABSTRACT New Event Detection (NED) aims at detecting from one or multiple streams of news stories that which one is reported on a new event (i.e.",
                "not reported previously).",
                "With the overwhelming volume of news available today, there is an increasing need for a NED system which is able to detect new events more efficiently and accurately.",
                "In this paper we propose a new NED model to speed up the NED task by using news indexing-tree dynamically.",
                "Moreover, based on the observation that terms of different types have different effects for NED task, two term reweighting approaches are proposed to improve NED accuracy.",
                "In the first approach, we propose to adjust term weights dynamically based on previous story clusters and in the second approach, we propose to employ statistics on training data to learn the named entity reweighting model for each class of stories.",
                "Experimental results on two <br>linguistic data consortium</br> (LDC) datasets TDT2 and TDT3 show that the proposed model can improve both efficiency and accuracy of NED task significantly, compared to the baseline system and other existing systems.",
                "Categories and Subject Descriptors H.3.3 [Information Systems]: Information Search and Retrieval; H.4.2 [Information Systems Applications]: Types of Systemsdecision support.",
                "General Terms Algorithms, Performance, Experimentation 1.",
                "INTRODUCTION Topic Detection and Tracking (TDT) program aims to develop techniques which can effectively organize, search and structure news text materials from a variety of newswire and broadcast media [1].",
                "New Event Detection (NED) is one of the five tasks in TDT.",
                "It is the task of online identification of the earliest report for each topic as soon as that report arrives in the sequence of documents.",
                "A Topic is defined as a seminal event or activity, along with directly related events and activities [2].",
                "An Event is defined as something (non-trivial) happening in a certain place at a certain time [3].",
                "For instance, when a bomb explodes in a building, the exploding is the seminal event that triggers the topic, and other stories on the same topic would be those discussing salvaging efforts, the search for perpetrators, arrests and trial and so on.",
                "Useful news information is usually buried in a mass of data generated everyday.",
                "Therefore, NED systems are very useful for people who need to detect novel information from real-time news stream.",
                "These real-life needs often occur in domains like financial markets, news analysis, and intelligence gathering.",
                "In most of state-of-the-art (currently) NED systems, each news story on hand is compared to all the previous received stories.",
                "If all the similarities between them do not exceed a threshold, then the story triggers a new event.",
                "They are usually in the form of cosine similarity or Hellinger similarity metric.",
                "The core problem of NED is to identify whether two stories are on the same topic.",
                "Obviously, these systems cannot take advantage of topic information.",
                "Further more, it is not acceptable in real applications because of the large amount of computation required in the NED process.",
                "Other systems organize previous stories into clusters (each cluster corresponds to a topic), and new story is compared to the previous clusters instead of stories.",
                "This manner can reduce comparing times significantly.",
                "Nevertheless, it has been proved that this manner is less accurate [4, 5].",
                "This is because sometimes stories within a topic drift far away from each other, which could lead low similarity between a story and its topic.",
                "On the other hand, some proposed NED systems tried to improve accuracy by making better use of named entities [10, 11, 12, 13].",
                "However, none of the systems have considered that terms of different types (e.g.",
                "Noun, Verb or Person name) have different effects for different classes of stories in determining whether two stories are on the same topic.",
                "For example, the names of election candidates (Person name) are very important for stories of election class; the locations (Location name) where accidents happened are important for stories of accidents class.",
                "So, in NED, there still exist following three problems to be investigated: (1) How to speed up the detection procedure while do not decrease the detection accuracy? (2) How to make good use of cluster (topic) information to improve accuracy? (3) How to obtain better news story representation by better understanding of named entities.",
                "Driven by these problems, we have proposed three approaches in this paper. (1)To make the detection procedure faster, we propose a new NED procedure based on news indexing-tree created dynamically.",
                "Story indexing-tree is created by assembling similar stories together to form news clusters in different hierarchies according to their values of similarity.",
                "Comparisons between current story and previous clusters could help find the most similar story in less comparing times.",
                "The new procedure can reduce the amount of comparing times without hurting accuracy. (2)We use the clusters of the first floor in the indexing-tree as news topics, in which term weights are adjusted dynamically according to term distribution in the clusters.",
                "In this approach, cluster (topic) information is used properly, so the problem of theme decentralization is avoided. (3)Based on observations on the statistics obtained from training data, we found that terms of different types (e.g.",
                "Noun and Verb) have different effects for different classes of stories in determining whether two stories are on the same topic.",
                "And we propose to use statistics to optimize the weights of the terms of different types in a story according to the news class that the story belongs to.",
                "On TDT3 dataset, the new NED model just uses 14.9% comparing times of the basic model, while its minimum normalized cost is 0.5012, which is 0.0797 better than the basic model, and also better than any other results previously reported for this dataset [8, 13].",
                "The rest of the paper is organized as follows.",
                "We start off this paper by summarizing the previous work in NED in section 2.",
                "Section 3 presents the basic model for NED that most current systems use.",
                "Section 4 describes our new detection procedure based on news indexing-tree.",
                "In section 5, two term reweighting methods are proposed to improve NED accuracy.",
                "Section 6 gives our experimental data and evaluation metrics.",
                "We finally wrap up with the experimental results in Section 7, and the conclusions and future work in Section 8. 2.",
                "RELATED WORK Papka et al. proposed Single-Pass clustering on NED [6].",
                "When a new story was encountered, it was processed immediately to extract term features and a query representation of the storys content is built up.",
                "Then it was compared with all the previous queries.",
                "If the document did not trigger any queries by exceeding a threshold, it was marked as a new event.",
                "Lam et al build up previous query representations of story clusters, each of which corresponds to a topic [7].",
                "In this manner comparisons happen between stories and clusters.",
                "Recent years, most work focus on proposing better methods on comparison of stories and document representation.",
                "Brants et al. [8] extended a basic incremental TF-IDF model to include sourcespecific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, term reweighting based on inverse event frequencies, and segmentation of documents.",
                "Good improvements on TDT bench-marks were shown.",
                "Stokes et al. [9] utilized a combination of evidence from two distinct representations of a documents content.",
                "One of the representations was the usual free text vector, the other made use of lexical chains (created using WordNet) to build another term vector.",
                "Then the two representations are combined in a linear fashion.",
                "A marginal increase in effectiveness was achieved when the combined representation was used.",
                "Some efforts have been done on how to utilize named entities to improve NED.",
                "Yang et al. gave location named entities four times weight than other terms and named entities [10].",
                "DOREMI research group combined semantic similarities of person names, location names and time together with textual similarity [11][12].",
                "UMass [13] research group split document representation into two parts: named entities and non-named entities.",
                "And it was found that some classes of news could achieve better performance using named entity representation, while some other classes of news could achieve better performance using non-named entity representation.",
                "Both [10] and [13] used text categorization technique to classify news stories in advance.",
                "In [13] news stories are classified automatically at first, and then test sensitivities of names and non-name terms for NED for each class.",
                "In [10] frequent terms for each class are removed from document representation.",
                "For example, word election does not help identify different elections.",
                "In their work, effectiveness of different kinds of names (or terms with different POS) for NED in different news classes are not investigated.",
                "We use statistical analysis to reveal the fact and use it to improve NED performance. 3.",
                "BASIC MODEL In this section, we present the basic New Event Detection model which is similar to what most current systems apply.",
                "Then, we propose our new model by extending the basic model.",
                "New Event Detection systems use news story stream as input, in which stories are strictly time-ordered.",
                "Only previously received stories are available when dealing with current story.",
                "The output is a decision for whether the current story is on a new event or not and the confidence of the decision.",
                "Usually, a NED model consists of three parts: story representation, similarity calculation and detection procedure. 3.1 Story Representation Preprocessing is needed before generating story representation.",
                "For preprocessing, we tokenize words, recognize abbreviations, normalize abbreviations, add part-of-speech tags, remove stopwords included in the stop list used in InQuery [14], replace words with their stems using K-stem algorithm[15], and then generate word vector for each news story.",
                "We use incremental TF-IDF model for term weight calculation [4].",
                "In a TF-IDF model, term frequency in a news document is weighted by the inverse document frequency, which is generated from training corpus.",
                "When a new term occurs in testing process, there are two solutions: simply ignore the new term or set df of the term as a small const (e.g. df = 1).",
                "The new term receives too low weight in the first solution (0) and too high weight in the second solution.",
                "In incremental TF-IDF model, document frequencies are updated dynamically in each time step t: 1( ) ( ) ( )t t D tdf w df w df w−= + (1) where Dt represents news story set received in time t, and dfDt(w) means the number of documents that term w occurs in, and dft(w) means the total number of documents that term w occurs in before time t. In this work, each time window includes 50 news stories.",
                "Thus, each story d received in t is represented as follows: 1 2{ ( , , ), ( , , ),..., ( , , )}nd weight d t w weight d t w weight d t w→ where n means the number of distinct terms in story d, and ( , , )weight d t w means the weight of term w in story d at time t: log( ( , ) 1) log(( 1) /( ( ) 0.5)) ( , , ) log( ( , ) 1) log(( 1) /( ( ) 0.5)) t t t t w d tf d w N df w weight d t w tf d w N df w ∈ + + + = + + +∑ (2) where Nt means the total number of news stories before time t, and tf(d,w) means how many times term w occurs in news story d. 3.2 Similarity Calculation We use Hellinger distance for the calculation of similarity between two stories, for two stories d and d at time t, their similarity is defined as follows: , ( , , ) ( , , ) * ( , , ) w d d sim d d t weight d t w weight d t w ∈ = ∑ (3) 3.3 Detection Procedure For each story d received in time step t, the value ( ) ( ) ( ) ( ( , , )) time d time d n d max sim d d t < = (4) is a score used to determine whether d is a story about a new topic and at the same time is an indication of the confidence in our decision [8]. time(d) means the publication time of story d. If the score exceeds the thresholdθ new, then there exists a sufficiently similar document, thus d is a old story, otherwise, there is no sufficiently similar previous document, thus d is an new story. 4.",
                "New NED Procedure Traditional NED systems can be classified into two main types on the aspect of detection procedure: (1) S-S type, in which the story on hand is compared to each story received previously, and use the highest similarity to determine whether current story is about a new event; (2) S-C type, in which the story on hand is compared to all previous clusters each of which representing a topic, and the highest similarity is used for final decision for current story.",
                "If the highest similarity exceeds thresholdθ new, then it is an old story, and put it into the most similar cluster; otherwise it is a new story and create a new cluster.",
                "Previous work show that the first manner is more accurate than the second one [4][5].",
                "Since sometimes stories within a topic drift far away from each other, a story may have very low similarity with its topic.",
                "So using similarities between stories for determining new story is better than using similarities between story and clusters.",
                "Nevertheless, the first manner needs much more comparing times which means the first manner is low efficient.",
                "We propose a new detection procedure which uses comparisons with previous clusters to help find the most similar story in less comparing times, and the final new event decision is made according to the most similar story.",
                "Therefore, we can get both the accuracy of S-S type methods and the efficiency of S-C type methods.",
                "The new procedure creates a news indexing-tree dynamically, in which similar stories are put together to form a hierarchy of clusters.",
                "We index similar stories together by their common ancestor (a cluster node).",
                "Dissimilar stories are indexed in different clusters.",
                "When a story is coming, we use comparisons between the current story and previous hierarchical clusters to help find the most similar story which is useful for new event decision.",
                "After the new event decision is made, the current story is inserted to the indexing-tree for the following detection.",
                "The news indexing-tree is defined formally as follows: S-Tree = {r, NC , NS , E} where r is the root of S-Tree, NC is the set of all cluster nodes, NS is the set of all story nodes, and E is the set of all edges in S-Tree.",
                "We define a set of constraints for a S-Tree: ⅰ . , is an non-terminal node in the treeC i i N i∀ ∈ → ⅱ . , is a terminal node in the treeS i i N i∀ ∈ → ⅲ . , out degree of is at least 2C i i N i∀ ∈ → ⅳ . , is represented as the centroid of its desendantsC i i iN∀ ∈ → For a news story di, the comparison procedure and inserting procedure based on indexing-tree are defined as follows.",
                "An example is shown by Figure 1 and Figure 2.",
                "Figure 1.",
                "Comparison procedure Figure 2.",
                "Inserting procedure Comparison procedure: Step 1: compare di to all the direct child nodes of r and select λ nodes with highest similarities, e.g., C1 2 and C1 3 in Figure 1.",
                "Step 2: for each selected node in the last step, e.g.",
                "C1 2, compare di to all its direct child nodes, and select λ nodes with highest similarities, e.g.",
                "C2 2 and d8.",
                "Repeat step 2 for all non-terminal nodes.",
                "Step 3: record the terminal node with the highest similarty to di, e.g. s5, and the similarity value (0.20).",
                "Inserting di to the S-tree with r as root: Find the node n which is direct child of r in the path from r to the terminal node with highest similarity s, e.g.",
                "C1 2.",
                "If s is smaller than θ init+(h-1)δ , then add di to the tree as a direct child of r. Otherwise, if n is a terminal node, then create a cluster node instead of n, and add both n and di as its direct children; if n is an non-terminal node, then repeat this procedure and insert di to the sub-tree with n as root recursively.",
                "Here h is the length between n and the root of S-tree.",
                "The more the stories in a cluster similar to each other, the better the cluster represents the stories in it.",
                "Hence we add no constraints on the maximum of trees height and degree of a node.",
                "Therefore, we cannot give the complexity of this indexing-tree based procedure.",
                "But we will give the number of comparing times needed by the new procedure in our experiments in section7. 5.",
                "Term Reweighting Methods In this section, two term reweighting methods are proposed to improve NED accuracy.",
                "In the first method, a new way is explored for better using of cluster (topic) information.",
                "The second one finds a better way to make use of named entities based on news classification. 5.1 Term Reweighting Based on Distribution Distance TF-IDF is the most prevalent model used in information retrieval systems.",
                "The basic idea is that the fewer documents a term appears in, the more important the term is in discrimination of documents (relevant or not relevant to a query containing the term).",
                "Nevertheless, in TDT domain, we need to discriminate documents with regard to topics rather than queries.",
                "Intuitively, using cluster (topic) vectors to compare with subsequent news stories should outperform using story vectors.",
                "Unfortunately, the experimental results do not support this intuition [4][5].",
                "Based on observation on data, we find the reason is that a news topic usually contains many directly or indirectly related events, while they all have their own sub-subjects which are usually different with each other.",
                "Take the topic described in section 1 as an example, events like the explosion and salvage have very low similarities with events about criminal trial, therefore stories about trial would have low similarity with the topic vector built on its previous events.",
                "This section focuses on how to effectively make use of topic information and at the same time avoid the problem of content decentralization.",
                "At first, we classify terms into 5 classes to help analysis the needs of the modified model: Term class A: terms that occur frequently in the whole corpus, e.g., year and people.",
                "Terms of this class should be given low weights because they do not help much for topic discrimination.",
                "Term class B: terms that occur frequently within a news category, e.g., election, storm.",
                "They are useful to distinguish two stories in different news categories.",
                "However, they cannot provide information to determine whether two stories are on the same or different topics.",
                "In another words, term election and term storm are not helpful in differentiate two election campaigns and two storm disasters.",
                "Therefore, terms of this class should be assigned lower weights.",
                "Term class C: terms that occur frequently in a topic, and infrequently in other topics, e.g., the name of a crash plane, the name of a specific hurricane.",
                "News stories that belong to different topics rarely have overlap terms in this class.",
                "The more frequently a term appears in a topic, the more important the term is for a story belonging to the topic, therefore the term should be set higher weight.",
                "Term class D: terms that appear in a topic exclusively, but not frequently.",
                "For example, the name of a fireman who did very well in a salvage action, which may appears in only two or three stories but never appeared in other topics.",
                "Terms of this type should receive more weights than in TF-IDF model.",
                "However, since they are not popular in the topic, it is not appropriate to give them too high weights.",
                "Term class E: terms with low document frequency, and appear in different topics.",
                "Terms of this class should receive lower weights.",
                "Now we analyze whether TF-IDF model can give proper weights to the five classes of terms.",
                "Obviously, terms of class A are lowly weighted in TF-IDF model, which is conformable with the requirement described above.",
                "In TF-IDF model, terms of class B are highly dependant with the number of stories in a news class.",
                "TF-IDF model cannot provide low weights if the story containing the term belongs to a relative small news class.",
                "For a term of class C, the more frequently it appears in a topic, the less weight TFIDF model gives to it.",
                "This strongly conflicts with the requirement of terms in class C. For terms of class D, TF-IDF model gives them high weights correctly.",
                "But for terms of class E, TF-IDF model gives high weights to them which are not conformable with the requirement of low weights.",
                "To sum up, terms of class B, C, E cannot be properly weighted in TF-IDF model.",
                "So, we propose a modified model to resolve this problem.",
                "When θ init andθ new are set closely, we assume that most of the stories in a first-level cluster (a direct child node of root node) are on the same topic.",
                "Therefore, we make use of a first-level cluster to capture term distribution (df for all the terms within the cluster) within the topic dynamically.",
                "KL divergence of term distribution in a first-level cluster and the whole story set is used to adjust term weights:  ( , , ) * (1 * ( || )) ( , , ) ( , , ) * (1 * ( || )) cw tw cw tw w d D weight d t w KL P P weight d t w weight d t w KL P P γ γ ∈ + = +∑ (5) where ( ) ( ) ( ) ( ) 1,cw cw c c c c df w df w p y p y N N = = − (6) ( ) ( ) ( ) ( ) 1,t t tw tw t t df w df w p y p y N N = = − (7) where dfc(w) is the number of documents containing term w within cluster C, and Nc is the number of documents in cluster C, and Nt is the total number of documents that arrive before time step t. γ is a const parameter, now is manually set 3.",
                "KL divergence is defined as follows [17]: ( ) ( || ) ( ) log ( )x p x KL P Q p x q x = ∑ (8) The basic idea is: for a story in a topic, the more a term occurs within the topic, and the less it occurs in other topics, it should be assigned higher weights.",
                "Obviously, modified model can meet all the requirements of the five term classes listed above. 5.2 Term Reweighting Based on Term Type and Story Class Previous work found that some classes of news stories could achieve good improvements by giving extra weight to named entities.",
                "But we find that terms of different types should be given different amount of extra weight for different classes of news stories.",
                "We use open-NLP1 to recognize named entity types and part-ofspeech tags for terms that appear in news stories.",
                "Named entity types include person name, organization name, location name, date, time, money and percentage, and five POSs are selected: none (NN), verb (VB), adjective (JJ), adverb (RB) and cardinal number (CD).",
                "Statistical analysis shows topic-level discriminative terms types for different classes of stories.",
                "For the sake of convenience, named entity type and part-of-speech tags are uniformly called term type in subsequent sections.",
                "Determining whether two stories are about the same topic is a basic component for NED task.",
                "So at first we use 2 χ statistic to compute correlations between terms and topics.",
                "For a term t and a topic T, a contingence table is derived: Table 1.",
                "A 2×2 Contingence Table Doc Number belong to topic T not belong to topic T include t A B not include t C D The 2 χ statistic for a specific term t with respect to topic T is defined to be [16]: 2 2 ( , ) ( ) * ( ) ( ) * ( ) * ( ) * ( ) w T A B C D AD CB A C B D A B C D χ = + + + − + + + + (9) News topics for the TDT task are further classified into 11 rules of interpretations (ROIs) 2 .",
                "The ROI can be seen as a higher level class of stories.",
                "The average correlation between a term type and a topic ROI is computed as: 2 avg 2 ( , )( ( , ) )k m m km kT R w P w TP R p w T R P χ χ ∈ ∈ ∑ ∑（ , ）= 1 1 k=1…K, m=1…M (10) where K is the number of term types (set 12 constantly in the paper).",
                "M is the number news classes (ROIs, set 11 in the paper).",
                "Pk represents the set of all terms of type k, and Rm represents the set of all topics of class m, p(t,T) means the probability that t occurs in topic T. Because of limitation of space, only parts of the term types (9 term types) and parts of news classes (8 classes) are listed in table 2 with the average correlation values between them.",
                "The statistics is derived from labeled data in TDT2 corpus. (Results in table 2 are already normalized for convenience in comparison.)",
                "The statistics in table 2 indicates the usefulness of different term types in topic discrimination with respect to different news classes.",
                "We can see that, location name is the most useful term type for three news classes: Natural Disasters, Violence or War, Finances.",
                "And for three other categories Elections, Legal/Criminal Cases, Science and Discovery, person name is the most discriminative term type.",
                "For Scandals/Hearings, date is the most important information for topic discrimination.",
                "In addition, Legal/Criminal Cases and Finance topics have higher correlation with money terms, while Science and Discovery have higher correlation with percentage terms.",
                "Non-name terms are more stable for different classes. 1 . http://opennlp.sourceforge.net/ 2 . http://projects.ldc.upenn.edu/TDT3/Guide/label.html From the analysis of table 2, it is reasonable to adjust term weight according to their term type and the news class the story belongs to.",
                "New term weights are reweighted as follows: ( ) ( ) ( ) ( ) ( , , ) * ( , , ) ( , , ) * class d D type w T class d D type w w d weight d t w weight d t w weight d t w α α ∈ = ∑ (11) where type(w) represents the type of term w, and class(d) represents the class of story d, c kα is reweighting parameter for news class c and term type k. In the work, we just simply use statistics in table 2 as the reweighting parameters.",
                "Even thought using the statistics directly may not the best choice, we do not discuss how to automatically obtain the best parameters.",
                "We will try to use machine learning techniques to obtain the best parameters in the future work.",
                "In the work, we use BoosTexter [20] to classify all stories into one of the 11 ROIs.",
                "BoosTexter is a boosting based machine learning program, which creates a series of simple rules for building a classifier for text or attribute-value data.",
                "We use term weight generated using TF-IDF model as feature for story classification.",
                "We trained the model on the 12000 judged English stories in TDT2, and classify the rest of the stories in TDT2 and all stories in TDT3.",
                "Classification results are used for term reweighting in formula (11).",
                "Since the class labels of topic-off stories are not given in TDT datasets, we cannot give the classification accuracy here.",
                "Thus we do not discuss the effects of classification accuracy to NED performance in the paper. 6.",
                "EXPERIMENTAL SETUP 6.1 Datasets We used two LDC [18] datasets TDT2 and TDT3 for our experiments.",
                "TDT2 contains news stories from January to June 1998.",
                "It contains around 54,000 stories from sources like ABC, Associated Press, CNN, New York Times, Public Radio International, Voice of America etc.",
                "Only English stories in the collection were considered.",
                "TDT3 contains approximately 31,000 English stories collected from October to December 1998.",
                "In addition to the sources used in TDT2, it also contains stories from NBC and MSNBC TV broadcasts.",
                "We used transcribed versions of the TV and radio broadcasts besides textual news.",
                "TDT2 dataset is labeled with about 100 topics, and approximately 12,000 English stories belong to at least one of these topics.",
                "TDT3 dataset is labeled with about 120 topics, and approximately 8000 English stories belong to at least one of these topics.",
                "All the topics are classified into 11 Rules of Interpretation: (1)Elections, (2)Scandals/Hearings, (3)Legal/Criminal Cases, (4)Natural Disasters, (5)Accidents, (6)Ongoing Violence or War, (7)Science and Discovery News, (8)Finance, (9)New Law, (10)Sports News, (11)MISC.",
                "News. 6.2 Evaluation Metric TDT uses a cost function CDet that combines the probabilities of missing a new story and a false alarm [19]: * * * *Det Miss Miss Target FA FA NontargetC C P P C P P= + (12) Table 2.",
                "Average correlation between term types and news classes where CMiss means the cost of missing a new story, PMiss means the probability of missing a new story, and PTarget means the probability of seeing a new story in the data; CFA means the cost of a false alarm, PFA means the probability of a false alarm, and PNontarget means the probability of seeing an old story.",
                "The cost CDet is normalized such that a perfect system scores 0 and a trivial system, which is the better one of mark all stories as new or old, scores 1: ( ( * , * ) ) Det Det Miss Target FA Nontarget C Norm C min C P C P = (13) New event detection system gives two outputs for each story.",
                "The first part is yes or no indicating whether the story triggers a new event or not.",
                "The second part is a score indicating confidence of the first decision.",
                "Confidence scores can be used to plot DET curve, i.e., curves that plot false alarm vs. miss probabilities.",
                "Minimum normalized cost can be determined if optimal threshold on the score were chosen. 7.",
                "EXPERIMENTAL RESULTS 7.1 Main Results To test the approaches proposed in the model, we implemented and tested five systems: System-1: this system is used as baseline.",
                "It is implemented based on the basic model described in section 3, i.e., using incremental TF-IDF model to generate term weights, and using Hellinger distance to compute document similarity.",
                "Similarity score normalization is also employed [8].",
                "S-S detection procedure is used.",
                "System-2: this system is the same as system-1 except that S-C detection procedure is used.",
                "System-3: this system is the same as system-1 except that it uses the new detection procedure which is based on indexing-tree.",
                "System-4: implemented based on the approach presented in section 5.1, i.e., terms are reweighted according to the distance between term distributions in a cluster and all stories.",
                "The new detection procedure is used.",
                "System-5: implemented based on the approach presented in section 5.2, i.e., terms of different types are reweighted according to news class using trained parameters.",
                "The new detection procedure is used.",
                "The following are some other NED systems: System-6: [21] for each pair of stories, it computes three similarity values for named entity, non-named entity and all terms respectively.",
                "And employ Support Vector Machine to predict new or old using the similarity values as features.",
                "System-7: [8] it extended a basic incremental TF-IDF model to include source-specific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, etc.",
                "System-8: [13] it split document representation into two parts: named entities and non-named entities, and choose one effective part for each news class.",
                "Table 3 and table 4 show topic-weighted normalized costs and comparing times on TDT2 and TDT3 datasets respectively.",
                "Since no heldout data set for fine-tuning the threshold θ new was available for experiments on TDT2, we only report minimum normalized costs for our systems in table 3.",
                "System-5 outperforms all other systems including system-6, and it performs only 2.78e+8 comparing times in detection procedure which is only 13.4% of system-1.",
                "Table 3.",
                "NED results on TDT2 Systems Min Norm(CDet) Cmp times System-1 0.5749 2.08e+9 System-2① 0.6673 3.77e+8 System-3② 0.5765 2.81e+8 System-4② 0.5431 2.99e+8 System-5② 0.5089 2.78e+8 System-6 0.5300 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 When evaluating on the normalized costs on TDT3, we use the optimal thresholds obtained from TDT2 data set for all systems.",
                "System-2 reduces comparing times to 1.29e+9 which is just 18.3% of system-1, but at the same time it also gets a deteriorated minimum normalized cost which is 0.0499 higher than system-1.",
                "System-3 uses the new detection procedure based on news indexing-tree.",
                "It requires even less comparing times than system-2.",
                "This is because story-story comparisons usually yield greater similarities than story-cluster ones, so stories tend to be combined Location Person Date Organization Money Percentage NN JJ CD Elections 0.37 1 0.04 0.58 0.08 0.03 0.32 0.13 0.1 Scandals/Hearings 0.66 0.62 0.28 1 0.11 0.02 0.27 0.13 0.05 Legal/Criminal Cases 0.48 1 0.02 0.62 0.15 0 0.22 0.24 0.09 Natural Disasters 1 0.27 0 0.04 0.04 0 0.25 0.04 0.02 Violence or War 1 0.36 0.02 0.14 0.02 0.04 0.21 0.11 0.02 Science and Discovery 0.11 1 0.01 0.22 0.08 0.12 0.19 0.08 0.03 Finances 1 0.45 0.04 0.98 0.13 0.02 0.29 0.06 0.05 Sports 0.16 0.27 0.01 1 0.02 0 0.11 0.03 0.01 together in system-3.",
                "And system-3 is basically equivalent to system-1 in accuracy results.",
                "System-4 adjusts term weights based on the distance of term distributions between the whole corpus and cluster story set, yielding a good improvement by 0.0468 compared to system-1.",
                "The best system (system-5) has a minimum normalized cost 0.5012, which is 0.0797 better than system-1, and also better than any other results previously reported for this dataset [8, 13].",
                "Further more, system-5 only needs 1.05e+8 comparing times which is 14.9% of system-1.",
                "Table 4.",
                "NED results on TDT3 Systems Norm(CDet) Min Norm(CDet) Cmp times System-1 0.6159 0.5809 7.04e+8 System-2① 0.6493 0.6308 1.29e+8 System-3② 0.6197 0.5868 1.03e+8 System-4② 0.5601 0.5341 1.03e+8 System-5② 0.5413 0.5012 1.05e+8 System-7 -- 0.5783 -System-8 -- 0.5229 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 Figure5 shows the five DET curves for our systems on data set TDT3.",
                "System-5 achieves the minimum cost at a false alarm rate of 0.0157 and a miss rate of 0.4310.",
                "We can observe that System4 and System-5 obtain lower miss probability at regions of low false alarm probabilities.",
                "The hypothesis is that, more weight value is transferred to key terms of topics from non-key terms.",
                "Similarity score between two stories belonging to different topics are lower than before, because their overlapping terms are usually not key terms of their topics. 7.2 Parameter selection for indexing-tree detection Figure 3 shows the minimum normalized costs obtained by system-3 on TDT3 using different parameters.",
                "Theθ init parameter is tested on six values spanning from 0.03 to 0.18.",
                "And the λ parameter is tested on four values 1, 2, 3 and 4.",
                "We can see that, whenθ init is set to 0.12, which is the closest one toθ new, the costs are lower than others.",
                "This is easy to explain, because when stories belonging to the same topic are put in a cluster, it is more reasonable for the cluster to represent the stories in it.",
                "When parameter λ is set to 3 or 4, the costs are better than other cases, but there is no much difference between 3 and 4. 0 0.05 0.1 0.15 0.2 1 2 3 4 0.5 0.6 0.7 0.8 0.9 1 θ-initλ MinCost 0.6 0.65 0.7 0.75 0.8 0.85 0.9 Figure 3.",
                "Min Cost on TDT3 (δ =0.15) 0 0.05 0.1 0.15 0.2 1 2 3 4 0 0.5 1 1.5 2 2.5 x 10 8 θ-init λ Comparingtimes 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 8 Figure 4.",
                "Comparing times on TDT3 (δ =0.15) Figure 4 gives the comparing times used by system-3 on TDT3 with the same parameters as figure 3.",
                "The comparing times are strongly dependent onθ init.",
                "Because the greaterθ init is, the less stories combined together, the more comparing times are needed for new event decision.",
                "So we useθ init =0.13,λ =3,δ =0.15 for system-3, 4, and 5.",
                "In this parameter setting, we can get both low minimum normalized costs and less comparing times. 8.",
                "CONCLUSION We have proposed a news indexing-tree based detection procedure in our model.",
                "It reduces comparing times to about one seventh of traditional method without hurting NED accuracy.",
                "We also have presented two extensions to the basic TF-IDF model.",
                "The first extension is made by adjust term weights based on term distributions between the whole corpus and a cluster story set.",
                "And the second extension to basic TF-IDF model is better use of term types (named entities types and part-of-speed) according to news categories.",
                "Our experimental results on TDT2 and TDT3 datasets show that both of the two extensions contribute significantly to improvement in accuracy.",
                "We did not consider news time information as a clue for NED task, since most of the topics last for a long time and TDT data sets only span for a relative short period (no more than 6 months).",
                "For the future work, we want to collect news set which span for a longer period from internet, and integrate time information in NED task.",
                "Since topic is a relative coarse-grained news cluster, we also want to refine cluster granularity to event-level, and identify different events and their relations within a topic.",
                "Acknowledgments This work is supported by the National Natural Science Foundation of China under Grant No. 90604025.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsor. 9.",
                "REFERENCES [1] http://www.nist.gov/speech/tests/tdt/index.htm [2] In Topic Detection and Tracking.",
                "Event-based Information Organization.",
                "Kluwer Academic Publishers, 2002. .01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90 1 2 5 10 20 40 60 80 90 False Alarm Probability (in %) MissProbability(in%) SYSTEM1 Topic Weighted Curve SYSTEM1 Min Norm(Cost) SYSTEM2 Topic Weighted Curve SYSTEM2 Min Norm(Cost) SYSTEM3 Topic Weighted Curve SYSTEM3 Min Norm(Cost) SYSTEM4 Topic Weighted Curve SYSTEM4 Min Norm(Cost) SYSTEM5 Topic Weighted Curve SYSTEM5 Min Norm(Cost) Random Performance Figure 5.",
                "DET curves on TDT3 [3] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B.T.",
                "Archibald, and X. Liu.",
                "Learning Approaches for Detecting and Tracking News Events.",
                "In IEEE Intelligent Systems Special Issue on Applications of Intelligent Information Retrieval, volume 14 (4), 1999, 32-43. [4] Y. Yang, T. Pierce, and J. Carbonell.",
                "A Study on Retrospective and On-line Event Detection.",
                "In Proceedings of SIGIR-98, Melbourne, Australia, 1998, 28-36. [5] J. Allan, V. Lavrenko, D. Malin, and R. Swan.",
                "Detections, Bounds, and Timelines: Umass and tdt-3.",
                "In Proceedings of Topic Detection and Tracking Workshop (TDT-3), Vienna, VA, 2000, 167-174. [6] R. Papka and J. Allan.",
                "On-line New Event Detection Using Single Pass Clustering TITLE2:.",
                "Technical Report UM-CS1998-021, 1998. [7] W. Lam, H. Meng, K. Wong, and J.",
                "Yen.",
                "Using Contextual Analysis for News Event Detection.",
                "International Journal on Intelligent Systems, 2001, 525-546. [8] B. Thorsten, C. Francine, and F. Ayman.",
                "A System for New Event Detection.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2003, 330-337. [9] S. Nicola and C. Joe.",
                "Combining Semantic and Syntactic Document Classifiers to Improve First Story Detection.",
                "In Proceedings of the 24th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topicconditioned Novelty Detection.",
                "In Proceedings of the 8th ACM SIGKDD International Conference, ACM Press. 2002, 688-693. [11] M. Juha, A.M. Helena, and S. Marko.",
                "Applying Semantic Classes in Event Detection and Tracking.",
                "In Proceedings of International Conference on Natural Language Processing (ICON 2002), 2002, pages 175-183. [12] M. Juha, A.M. Helena, and S. Marko.",
                "Simple Semantics in Topic Detection and Tracking.",
                "Information Retrieval, 7(3-4): 2004, 347-368. [13] K. Giridhar and J. Allan.",
                "Text Classification and Named Entities for New Event Detection.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2004, 297-304. [14] J. P. Callan, W. B. Croft, and S. M. Harding.",
                "The INQUERY Retrieval System.",
                "In Proceedings of DEXA-92, 3rd International Conference on Database and Expert Systems Applications, 1992, 78-83. [15] R. Krovetz.",
                "Viewing Morphology as An Inference Process.",
                "In Proceedings of ACM SIGIR93, 1993, 61-81. [16] Y. Yang and J. Pedersen.",
                "A Comparative Study on Feature Selection in Text Categorization.",
                "In J. D. H. Fisher, editor, The Fourteenth International Conference on Machine Learning (ICML97), Morgan Kaufmann, 1997, 412-420. [17] T. M. Cover, and J.A.",
                "Thomas.",
                "Elements of Information Theory.",
                "Wiley. 1991. [18] The <br>linguistic data consortium</br>, http://www.ldc,upenn.edu/. [19] The 2001 TDT task definition and evaluation plan, http://www.nist.gov/speech/tests/tdt/tdt2001/evalplan.htm. [20] R. E. Schapire and Y.",
                "Singer.",
                "Boostexter: A Boosting-based System for Text Categorization.",
                "In Machine Learning 39(2/3):1, Kluwer Academic Publishers, 2000, 35-168. [21] K. Giridhar and J. Allan. 2005.",
                "Using Names and Topics for New Event Detection.",
                "In Proceedings of Human Technology Conference and Conference on Empirical Methods in Natural Language, Vancouver, 2005, 121-128"
            ],
            "original_annotated_samples": [
                "Experimental results on two <br>linguistic data consortium</br> (LDC) datasets TDT2 and TDT3 show that the proposed model can improve both efficiency and accuracy of NED task significantly, compared to the baseline system and other existing systems.",
                "Wiley. 1991. [18] The <br>linguistic data consortium</br>, http://www.ldc,upenn.edu/. [19] The 2001 TDT task definition and evaluation plan, http://www.nist.gov/speech/tests/tdt/tdt2001/evalplan.htm. [20] R. E. Schapire and Y."
            ],
            "translated_annotated_samples": [
                "Los resultados experimentales en dos conjuntos de datos del Consorcio de Datos Lingüísticos (LDC), TDT2 y TDT3, muestran que el modelo propuesto puede mejorar significativamente tanto la eficiencia como la precisión de la tarea de NED, en comparación con el sistema base y otros sistemas existentes.",
                "Wiley. 1991. [18] El <br>consorcio de datos lingüísticos</br>, http://www.ldc,upenn.edu/. [19] La definición de la tarea TDT 2001 y el plan de evaluación, http://www.nist.gov/speech/tests/tdt/tdt2001/evalplan.htm. [20] R. E. Schapire y Y."
            ],
            "translated_text": "Detección de nuevos eventos basada en árbol de indexación y entidad nombrada Zhang Kuo Universidad Tsinghua Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Universidad Tsinghua Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Universidad Tsinghua Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn RESUMEN La Detección de Nuevos Eventos (NED) tiene como objetivo detectar de una o varias corrientes de noticias cuál se informa sobre un nuevo evento (es decir, no reportado previamente). Con el abrumador volumen de noticias disponible hoy en día, hay una creciente necesidad de un sistema NED que sea capaz de detectar eventos nuevos de manera más eficiente y precisa. En este artículo proponemos un nuevo modelo de NED para acelerar la tarea de NED mediante el uso dinámico de un árbol de indexación de noticias. Además, basándose en la observación de que los términos de diferentes tipos tienen diferentes efectos para la tarea de NED, se proponen dos enfoques de reponderación de términos para mejorar la precisión de NED. En el primer enfoque, proponemos ajustar dinámicamente los pesos de los términos basados en los clusters de historias previas y en el segundo enfoque, proponemos emplear estadísticas en los datos de entrenamiento para aprender el modelo de reajuste de entidades nombradas para cada clase de historias. Los resultados experimentales en dos conjuntos de datos del Consorcio de Datos Lingüísticos (LDC), TDT2 y TDT3, muestran que el modelo propuesto puede mejorar significativamente tanto la eficiencia como la precisión de la tarea de NED, en comparación con el sistema base y otros sistemas existentes. Categorías y Descriptores de Asignaturas H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información; H.4.2 [Aplicaciones de Sistemas de Información]: Tipos de Sistemas de Apoyo a Decisiones. Términos generales Algoritmos, Rendimiento, Experimentación 1. El programa de Detección y Seguimiento de Temas (TDT) tiene como objetivo desarrollar técnicas que puedan organizar, buscar y estructurar de manera efectiva materiales de texto de noticias de una variedad de agencias de noticias y medios de difusión [1]. La Detección de Eventos Nuevos (DEN) es una de las cinco tareas en TDT. Es la tarea de identificación en línea del informe más temprano para cada tema tan pronto como ese informe llegue en la secuencia de documentos. Un tema se define como un evento o actividad seminal, junto con eventos y actividades directamente relacionados [2]. Un evento se define como algo (no trivial) que sucede en un lugar y en un momento determinados [3]. Por ejemplo, cuando una bomba explota en un edificio, la explosión es el evento seminal que desencadena el tema, y otras historias sobre el mismo tema serían aquellas que discuten los esfuerzos de salvamento, la búsqueda de los perpetradores, arrestos y juicios, entre otros. La información útil de noticias suele estar enterrada en una masa de datos generados a diario. Por lo tanto, los sistemas NED son muy útiles para las personas que necesitan detectar información novedosa de un flujo de noticias en tiempo real. Estas necesidades de la vida real a menudo se presentan en ámbitos como los mercados financieros, el análisis de noticias y la recopilación de inteligencia. En la mayoría de los sistemas NED de última generación, cada noticia disponible se compara con todas las noticias previamente recibidas. Si todas las similitudes entre ellos no superan un umbral, entonces la historia desencadena un nuevo evento. Por lo general, se presentan en forma de similitud de coseno o métrica de similitud de Hellinger. El problema central de NED es identificar si dos historias tratan sobre el mismo tema. Obviamente, estos sistemas no pueden aprovechar la información del tema. Además, no es aceptable en aplicaciones reales debido a la gran cantidad de cálculos requeridos en el proceso de NED. Otros sistemas organizan historias anteriores en grupos (cada grupo corresponde a un tema), y la nueva historia se compara con los grupos anteriores en lugar de con historias. Esta forma puede reducir significativamente los tiempos de comparación. Sin embargo, se ha demostrado que este método es menos preciso [4, 5]. Esto se debe a que a veces las historias dentro de un tema se alejan mucho entre sí, lo que podría resultar en una baja similitud entre una historia y su tema. Por otro lado, algunos sistemas NED propuestos intentaron mejorar la precisión al hacer un mejor uso de las entidades nombradas [10, 11, 12, 13]. Sin embargo, ninguno de los sistemas ha considerado que los términos de diferentes tipos (por ejemplo, Los sustantivos, verbos o nombres de personas tienen diferentes efectos en diferentes clases de historias para determinar si dos historias tratan sobre el mismo tema. Por ejemplo, los nombres de los candidatos electorales (Nombre de la persona) son muy importantes para las historias de la clase de elecciones; las ubicaciones (Nombre de la ubicación) donde ocurrieron los accidentes son importantes para las historias de la clase de accidentes. Por lo tanto, en NED, todavía existen los siguientes tres problemas por investigar: (1) ¿Cómo acelerar el procedimiento de detección sin disminuir la precisión de la detección? (2) ¿Cómo aprovechar mejor la información de los grupos (temas) para mejorar la precisión? (3) ¿Cómo obtener una mejor representación de historias de noticias mediante una mejor comprensión de las entidades nombradas? Impulsados por estos problemas, hemos propuesto tres enfoques en este documento. (1) Para hacer que el procedimiento de detección sea más rápido, proponemos un nuevo procedimiento de NED basado en un árbol de indexación de noticias creado dinámicamente. El índice de historias en árbol se crea ensamblando historias similares para formar grupos de noticias en diferentes jerarquías según sus valores de similitud. Las comparaciones entre la historia actual y los grupos anteriores podrían ayudar a encontrar la historia más similar en menos tiempo de comparación. El nuevo procedimiento puede reducir la cantidad de veces de comparación sin afectar la precisión. Utilizamos los grupos del primer piso en el árbol de indexación como temas de noticias, en los cuales los pesos de los términos se ajustan dinámicamente según la distribución de términos en los grupos. En este enfoque, se utiliza adecuadamente la información de los grupos (temas), por lo que se evita el problema de la descentralización del tema. Basándonos en observaciones de las estadísticas obtenidas de los datos de entrenamiento, encontramos que los términos de diferentes tipos (por ejemplo, Los sustantivos y verbos tienen diferentes efectos para diferentes clases de historias al determinar si dos historias tratan sobre el mismo tema. Y proponemos utilizar estadísticas para optimizar los pesos de los términos de diferentes tipos en una historia de acuerdo a la clase de noticias a la que pertenece la historia. En el conjunto de datos TDT3, el nuevo modelo NED solo utiliza un 14.9% en comparación con los tiempos del modelo básico, mientras que su costo normalizado mínimo es de 0.5012, lo que es 0.0797 mejor que el modelo básico, y también mejor que cualquier otro resultado previamente reportado para este conjunto de datos [8, 13]. El resto del documento está organizado de la siguiente manera. Comenzamos este artículo resumiendo el trabajo previo en NED en la sección 2. La sección 3 presenta el modelo básico para NED que la mayoría de los sistemas actuales utilizan. La sección 4 describe nuestro nuevo procedimiento de detección basado en el índice de árbol de noticias. En la sección 5, se proponen dos métodos de reponderación de términos para mejorar la precisión de NED. La sección 6 presenta nuestros datos experimentales y métricas de evaluación. Finalmente concluimos con los resultados experimentales en la Sección 7, y las conclusiones y trabajo futuro en la Sección 8. TRABAJO RELACIONADO Papka et al. propusieron el agrupamiento de un solo paso en NED [6]. Cuando se encontraba una nueva historia, esta era procesada inmediatamente para extraer características de términos y se construía una representación de consulta del contenido de la historia. Luego se comparó con todas las consultas anteriores. Si el documento no generaba ninguna consulta al exceder un umbral, se marcaba como un nuevo evento. Lam et al construyen representaciones de consultas previas de grupos de historias, cada una de las cuales corresponde a un tema [7]. De esta manera se realizan comparaciones entre historias y grupos. En los últimos años, la mayoría de los trabajos se han centrado en proponer mejores métodos para la comparación de historias y la representación de documentos. Brants et al. [8] extendieron un modelo básico incremental de TF-IDF para incluir modelos específicos de fuente, normalización de puntajes de similitud basada en promedios específicos de documentos, normalización de puntajes de similitud basada en promedios específicos de pares de fuentes, reponderación de términos basada en frecuencias de eventos inversas y segmentación de documentos. Se mostraron buenas mejoras en los puntos de referencia de TDT. Stokes et al. [9] utilizaron una combinación de evidencia de dos representaciones distintas del contenido de un documento. Una de las representaciones era el vector de texto libre habitual, la otra hacía uso de cadenas léxicas (creadas utilizando WordNet) para construir otro vector de términos. Luego, las dos representaciones se combinan de forma lineal. Se logró un aumento marginal en la efectividad cuando se utilizó la representación combinada. Algunos esfuerzos se han realizado sobre cómo utilizar entidades nombradas para mejorar la desambiguación de entidades nombradas (NED). Yang et al. asignaron un peso cuatro veces mayor a las entidades con nombres de ubicación que a otros términos y entidades con nombres [10]. El grupo de investigación DOREMI combinó similitudes semánticas de nombres de personas, nombres de lugares y tiempo junto con similitud textual [11][12]. El grupo de investigación de UMass [13] dividió la representación del documento en dos partes: entidades nombradas y entidades no nombradas. Y se descubrió que algunas clases de noticias podían lograr un mejor rendimiento utilizando la representación de entidades nombradas, mientras que otras clases de noticias podían lograr un mejor rendimiento utilizando la representación sin entidades nombradas. Tanto [10] como [13] utilizaron la técnica de categorización de texto para clasificar previamente las noticias. En [13] las noticias se clasifican automáticamente al principio, y luego se prueban las sensibilidades de los nombres y términos que no son nombres para NED para cada clase. En [10], se eliminan los términos frecuentes de cada clase de la representación del documento. Por ejemplo, la palabra \"elección\" no ayuda a identificar diferentes elecciones. En su trabajo, no se investiga la efectividad de diferentes tipos de nombres (o términos con diferentes categorías gramaticales) para NED en diferentes clases de noticias. Utilizamos análisis estadístico para revelar el hecho y utilizarlo para mejorar el rendimiento de NED. 3. MODELO BÁSICO En esta sección, presentamos el modelo básico de Detección de Nuevos Eventos que es similar a lo que la mayoría de los sistemas actuales aplican. Luego, proponemos nuestro nuevo modelo ampliando el modelo básico. Los sistemas de detección de eventos nuevos utilizan flujos de noticias como entrada, en los cuales las historias están estrictamente ordenadas por tiempo. Solo están disponibles las historias recibidas previamente al tratar con la historia actual. La salida es una decisión sobre si la historia actual es un evento nuevo o no y la confianza de la decisión. Por lo general, un modelo NED consta de tres partes: representación de la historia, cálculo de similitud y procedimiento de detección. 3.1 Representación de la historia Se necesita preprocesamiento antes de generar la representación de la historia. Para el preprocesamiento, tokenizamos palabras, reconocemos abreviaturas, normalizamos abreviaturas, agregamos etiquetas de partes del discurso, eliminamos palabras vacías incluidas en la lista de paradas utilizada en InQuery [14], reemplazamos palabras con sus raíces utilizando el algoritmo K-stem[15], y luego generamos un vector de palabras para cada noticia. Utilizamos el modelo TF-IDF incremental para el cálculo del peso de los términos [4]. En un modelo TF-IDF, la frecuencia de término en un documento de noticias se pondera por la frecuencia inversa del documento, la cual se genera a partir del corpus de entrenamiento. Cuando surge un nuevo término en el proceso de prueba, hay dos soluciones: simplemente ignorar el nuevo término o establecer df del término como una constante pequeña (por ejemplo, df = 1). El nuevo término recibe un peso demasiado bajo en la primera solución (0) y un peso demasiado alto en la segunda solución. En el modelo TF-IDF incremental, las frecuencias de documentos se actualizan dinámicamente en cada paso de tiempo t: 1( ) ( ) ( )t t D tdf w df w df w−= + (1) donde Dt representa el conjunto de noticias recibidas en el tiempo t, y dfDt(w) significa el número de documentos en los que aparece el término w, y dft(w) significa el número total de documentos en los que aparece el término w antes del tiempo t. En este trabajo, cada ventana de tiempo incluye 50 noticias. Así, cada historia d recibida en t se representa de la siguiente manera: 1 2{ ( , , ), ( , , ),..., ( , , )}nd peso d t w peso d t w peso d t w→ donde n significa el número de términos distintos en la historia d, y ( , , )peso d t w significa el peso del término w en la historia d en el tiempo t: log( ( , ) 1) log(( 1) /( ( ) 0.5)) ( , , ) log( ( , ) 1) log(( 1) /( ( ) 0.5)) t t t t w d tf d w N df w peso d t w tf d w N df w ∈ + + + = + + +∑ (2) donde Nt significa el número total de historias de noticias antes del tiempo t, y tf(d,w) significa cuántas veces aparece el término w en la historia de noticias d. 3.2 Cálculo de Similitud Utilizamos la distancia de Hellinger para el cálculo de la similitud entre dos historias, para dos historias d y d en el tiempo t, su similitud se define de la siguiente manera: , ( , , ) ( , , ) * ( , , ) w d d sim d d t peso d t w peso d t w ∈ = ∑ (3) 3.3 Procedimiento de Detección Para cada historia d recibida en el paso de tiempo t, el valor ( ) ( ) ( ) ( ( , , )) tiempo d tiempo d n d max sim d d t < = (4) es un puntaje utilizado para determinar si d es una historia sobre un nuevo tema y al mismo tiempo es una indicación de la confianza en nuestra decisión [8]. tiempo(d) significa el tiempo de publicación de la historia d. Si el puntaje supera el umbral θ new, entonces existe un documento suficientemente similar, por lo tanto, d es una historia antigua, de lo contrario, no hay un documento previo suficientemente similar, por lo tanto, d es una historia nueva. 4. Nuevo procedimiento NED Los sistemas NED tradicionales se pueden clasificar en dos tipos principales en cuanto al procedimiento de detección: (1) tipo S-S, en el que la historia en cuestión se compara con cada historia recibida previamente, y se utiliza la mayor similitud para determinar si la historia actual trata sobre un evento nuevo; (2) tipo S-C, en el que la historia en cuestión se compara con todos los clusters anteriores, cada uno de los cuales representa un tema, y se utiliza la mayor similitud para la decisión final sobre la historia actual. Si la similitud más alta supera el umbral θ nuevo, entonces es una historia antigua y se coloca en el clúster más similar; de lo contrario, es una historia nueva y se crea un nuevo clúster. Trabajos anteriores muestran que la primera forma es más precisa que la segunda [4][5]. Dado que a veces las historias dentro de un tema se alejan mucho entre sí, una historia puede tener una similitud muy baja con su tema. Por lo tanto, utilizar similitudes entre historias para determinar una nueva historia es mejor que utilizar similitudes entre historias y grupos. Sin embargo, el primer método requiere mucho más tiempo de comparación, lo que significa que es menos eficiente. Proponemos un nuevo procedimiento de detección que utiliza comparaciones con clusters anteriores para ayudar a encontrar la historia más similar en menos tiempo de comparación, y la decisión final sobre el nuevo evento se toma de acuerdo con la historia más similar. Por lo tanto, podemos obtener tanto la precisión de los métodos de tipo S-S como la eficiencia de los métodos de tipo S-C. El nuevo procedimiento crea un árbol de indexación de noticias dinámicamente, en el cual se agrupan historias similares para formar una jerarquía de clusters. Indexamos historias similares juntas por su ancestro común (un nodo de clúster). Historias diferentes están indexadas en diferentes grupos. Cuando una historia está llegando, utilizamos comparaciones entre la historia actual y los grupos jerárquicos anteriores para ayudar a encontrar la historia más similar que es útil para la toma de decisiones sobre nuevos eventos. Después de que se tome la decisión sobre el nuevo evento, la historia actual se inserta en el árbol de indexación para la detección siguiente. El índice de noticias en forma de árbol se define formalmente de la siguiente manera: S-Tree = {r, NC, NS, E}, donde r es la raíz de S-Tree, NC es el conjunto de todos los nodos de clúster, NS es el conjunto de todos los nodos de historia, y E es el conjunto de todos los bordes en S-Tree. Definimos un conjunto de restricciones para un S-Árbol: ⅰ. , es un nodo no terminal en el árbolC i i N i∀ ∈ → ⅱ. , es un nodo terminal en el árbolS i i N i∀ ∈ → ⅲ. , el grado de salida de es al menos 2C i i N i∀ ∈ → ⅳ. , se representa como el centroide de sus descendientesC i i iN∀ ∈ → Para una noticia di, el procedimiento de comparación y el procedimiento de inserción basados en el árbol de indexación se definen de la siguiente manera. Un ejemplo se muestra en la Figura 1 y la Figura 2. Figura 1. Procedimiento de comparación Figura 2. Procedimiento de comparación de inserción: Paso 1: comparar di con todos los nodos hijos directos de r y seleccionar λ nodos con las similitudes más altas, por ejemplo, C1 2 y C1 3 en la Figura 1. Paso 2: para cada nodo seleccionado en el paso anterior, por ejemplo. C1 2, compara di con todos sus nodos hijos directos y selecciona λ nodos con las similitudes más altas, por ejemplo. C2 2 y d8. Repetir el paso 2 para todos los nodos no terminales. Paso 3: registrar el nodo terminal con la mayor similitud a di, por ejemplo, s5, y el valor de similitud (0.20). Insertando di en el S-árbol con r como raíz: Encuentra el nodo n que es hijo directo de r en el camino desde r hasta el nodo terminal con la similitud más alta s, por ejemplo. C1 2. Si s es menor que θ init + (h-1)δ, entonces agregar di al árbol como un hijo directo de r. De lo contrario, si n es un nodo terminal, crear un nodo de clúster en lugar de n, y agregar tanto n como di como sus hijos directos; si n es un nodo no terminal, repetir este procedimiento e insertar di en el subárbol con n como raíz de forma recursiva. Aquí h es la longitud entre n y la raíz del árbol S. Cuanto más similares sean las historias en un grupo, mejor representará el grupo a las historias en él. Por lo tanto, no imponemos restricciones en la altura máxima de los árboles y el grado de un nodo. Por lo tanto, no podemos dar la complejidad de este procedimiento basado en árboles de indexación. Pero proporcionaremos el número de veces de comparación necesarias por el nuevo procedimiento en nuestros experimentos en la sección 7.5. En esta sección, se proponen dos métodos de reponderación de términos para mejorar la precisión de NED. En el primer método, se explora una nueva forma de utilizar de manera más efectiva la información de los grupos (temas). El segundo encuentra una mejor manera de utilizar entidades nombradas basadas en la clasificación de noticias. 5.1 Reajuste de términos basado en la distancia de distribución TF-IDF es el modelo más prevalente utilizado en sistemas de recuperación de información. La idea básica es que cuanto menos documentos aparezca un término, más importante es el término en la discriminación de documentos (relevantes o no relevantes para una consulta que contenga el término). Sin embargo, en el dominio de TDT, necesitamos discriminar documentos en función de los temas en lugar de las consultas. Intuitivamente, el uso de vectores de clúster (tema) para comparar con historias de noticias posteriores debería tener un mejor rendimiento que el uso de vectores de historias. Desafortunadamente, los resultados experimentales no respaldan esta intuición [4][5]. Basándonos en la observación de los datos, encontramos que la razón es que un tema de noticias generalmente contiene muchos eventos directa o indirectamente relacionados, y todos ellos tienen sus propios subtemas que suelen ser diferentes entre sí. Tomemos el tema descrito en la sección 1 como ejemplo, eventos como la explosión y el rescate tienen similitudes muy bajas con eventos sobre juicios criminales, por lo tanto, las historias sobre juicios tendrían una baja similitud con el vector de tema construido en base a sus eventos previos. Esta sección se centra en cómo hacer un uso efectivo de la información del tema y al mismo tiempo evitar el problema de la descentralización del contenido. Al principio, clasificamos los términos en 5 clases para ayudar en el análisis de las necesidades del modelo modificado: Clase de términos A: términos que ocurren con frecuencia en todo el corpus, por ejemplo, año y personas. Los términos de esta clase deben recibir pesos bajos porque no ayudan mucho en la discriminación de temas. Clase B de términos: términos que ocurren con frecuencia dentro de una categoría de noticias, por ejemplo, elección, tormenta. Son útiles para distinguir dos historias en diferentes categorías de noticias. Sin embargo, no pueden proporcionar información para determinar si dos historias tratan sobre el mismo tema o temas diferentes. En otras palabras, los términos elección y tormenta no son útiles para diferenciar dos campañas electorales y dos desastres de tormenta. Por lo tanto, los términos de esta clase deberían asignarse pesos más bajos. Clase de término C: términos que ocurren con frecuencia en un tema y raramente en otros temas, por ejemplo, el nombre de un avión accidentado, el nombre de un huracán específico. Las noticias que pertenecen a diferentes temas rara vez tienen términos superpuestos en esta clase. Cuanto más frecuentemente aparezca un término en un tema, más importante es el término para una historia perteneciente al tema, por lo tanto, el término debería tener un peso más alto. Clase de término D: términos que aparecen en un tema de manera exclusiva, pero no con frecuencia. Por ejemplo, el nombre de un bombero que se desempeñó muy bien en una acción de salvamento, que puede aparecer en solo dos o tres historias pero nunca ha aparecido en otros temas. Los términos de este tipo deberían recibir más peso que en el modelo TF-IDF. Sin embargo, dado que no son populares en el tema, no es apropiado asignarles pesos demasiado altos. Clase de término E: términos con baja frecuencia en documentos y que aparecen en diferentes temas. Los términos de esta clase deberían recibir pesos más bajos. Ahora analizamos si el modelo TF-IDF puede asignar pesos adecuados a las cinco clases de términos. Obviamente, los términos de la clase A tienen un peso bajo en el modelo TF-IDF, lo cual es acorde con el requisito descrito anteriormente. En el modelo TF-IDF, los términos de la clase B dependen altamente del número de historias en una clase de noticias. El modelo TF-IDF no puede proporcionar pesos bajos si la historia que contiene el término pertenece a una clase de noticias relativamente pequeña. Para un término de clase C, cuanto más frecuentemente aparezca en un tema, menos peso le da el modelo TFIDF. Esto entra en conflicto directo con el requisito de términos en la clase C. Para los términos de la clase D, el modelo TF-IDF les asigna pesos altos de manera correcta. Pero para los términos de la clase E, el modelo TF-IDF asigna pesos altos a ellos que no son conformes con el requisito de pesos bajos. En resumen, los términos de clase B, C, E no pueden ser ponderados adecuadamente en el modelo TF-IDF. Por lo tanto, proponemos un modelo modificado para resolver este problema. Cuando θ init y θ new están configurados de cerca, asumimos que la mayoría de las historias en un clúster de primer nivel (un nodo hijo directo del nodo raíz) están en el mismo tema. Por lo tanto, utilizamos un clúster de primer nivel para capturar la distribución de términos (df para todos los términos dentro del clúster) dentro del tema de forma dinámica. La divergencia de Kullback-Leibler de la distribución de términos en un clúster de primer nivel y el conjunto completo de historias se utiliza para ajustar los pesos de los términos: ( , , ) * (1 * ( || )) ( , , ) ( , , ) * (1 * ( || )) cw tw cw tw w d D peso d t w KL P P peso d t w peso d t w KL P P γ γ ∈ + = +∑ (5) donde ( ) ( ) ( ) ( ) 1,cw cw c c c c df w df w p y p y N N = = − (6) ( ) ( ) ( ) ( ) 1,t t tw tw t t df w df w p y p y N N = = − (7) donde dfc(w) es el número de documentos que contienen el término w dentro del clúster C, y Nc es el número de documentos en el clúster C, y Nt es el número total de documentos que llegan antes del paso de tiempo t. γ es un parámetro constante, actualmente configurado manualmente en 3. La divergencia de Kullback-Leibler se define de la siguiente manera [17]: La idea básica es que, para una historia en un tema, cuanto más ocurre un término dentro del tema y menos ocurre en otros temas, se le deben asignar pesos más altos. Obviamente, el modelo modificado puede cumplir con todos los requisitos de las cinco clases de términos mencionadas anteriormente. 5.2 Reajuste de Términos Basado en el Tipo de Término y la Clase de Historia. Trabajos anteriores encontraron que algunas clases de historias de noticias podrían lograr mejoras significativas al dar un peso adicional a las entidades nombradas. Pero encontramos que los términos de diferentes tipos deben recibir una cantidad diferente de peso adicional para diferentes clases de noticias. Utilizamos open-NLP1 para reconocer tipos de entidades nombradas y etiquetas de partes del discurso para términos que aparecen en noticias. Los tipos de entidades nombradas incluyen nombre de persona, nombre de organización, nombre de ubicación, fecha, hora, dinero y porcentaje, y se seleccionan cinco partes del discurso: ninguno (NN), verbo (VB), adjetivo (JJ), adverbio (RB) y número cardinal (CD). El análisis estadístico muestra tipos de términos discriminativos a nivel de tema para diferentes clases de historias. Por conveniencia, los tipos de entidad nombrada y etiquetas de parte de la oración se llaman uniformemente tipo de término en las secciones siguientes. Determinar si dos historias tratan sobre el mismo tema es un componente básico para la tarea de NED. Por lo tanto, al principio usamos la estadística χ² para calcular las correlaciones entre términos y temas. Para un término t y un tema T, se deriva una tabla de contingencia: Tabla 1. Una tabla de contingencia de 2×2 con el número de documentos pertenecientes al tema T y no pertenecientes al tema T incluye t A B no incluye t C D. La estadística χ² para un término específico t con respecto al tema T se define como [16]: χ² = ( ) ( ) * ( ) ( ) * ( ) * ( ) * ( ) w T A B C D AD CB A C B D A B C D + + + − + + + + (9). Los temas de noticias para la tarea TDT se clasifican además en 11 reglas de interpretaciones (ROIs) 2. El ROI se puede ver como una clase de historias de nivel superior. El promedio de correlación entre un tipo de término y un ROI de tema se calcula como: 2 avg 2 ( , )( ( , ) )k m m km kT R w P w TP R p w T R P χ χ ∈ ∈ ∑ ∑（ , ）= 1 1 k=1…K, m=1…M (10) donde K es el número de tipos de términos (fijado constantemente en 12 en el artículo). M es el número de nuevas clases (ROIs, establecido en el conjunto 11 del artículo). Pk representa el conjunto de todos los términos de tipo k, y Rm representa el conjunto de todos los temas de la clase m, p(t,T) significa la probabilidad de que t ocurra en el tema T. Debido a limitaciones de espacio, solo se enumeran partes de los tipos de términos (9 tipos de términos) y partes de las clases de noticias (8 clases) en la tabla 2 con los valores promedio de correlación entre ellos. Las estadísticas se derivan de datos etiquetados en el corpus TDT2. (Los resultados en la tabla 2 ya están normalizados para mayor comodidad en la comparación). Las estadísticas en la tabla 2 indican la utilidad de diferentes tipos de términos en la discriminación de temas con respecto a diferentes clases de noticias. Podemos ver que el nombre de la ubicación es el tipo de término más útil para tres clases de noticias: Desastres Naturales, Violencia o Guerra, Finanzas. Y para otras tres categorías: Elecciones, Casos Legales/Criminales, Ciencia y Descubrimiento, el nombre de la persona es el tipo de término más discriminativo. Para Escándalos/Audiencias, la fecha es la información más importante para la discriminación de temas. Además, los casos legales/criminales y los temas financieros tienen una correlación más alta con términos relacionados con el dinero, mientras que la ciencia y el descubrimiento tienen una correlación más alta con términos de porcentaje. Los términos no nominales son más estables para diferentes clases. 1. http://opennlp.sourceforge.net/ 2. http://projects.ldc.upenn.edu/TDT3/Guide/label.html A partir del análisis de la tabla 2, es razonable ajustar el peso de los términos según su tipo de término y la clase de noticias a la que pertenece la historia. Los nuevos pesos de los términos se recalculan de la siguiente manera: ( ) ( ) ( ) ( ) ( , , ) * ( , , ) ( , , ) * clase d D tipo w T clase d D tipo w w d peso d t w peso d t w peso d t w α α ∈ = ∑ (11) donde type(w) representa el tipo del término w, y class(d) representa la clase de la historia d, c kα es el parámetro de recalibración para la clase de noticias c y el tipo de término k. En el trabajo, simplemente utilizamos las estadísticas de la tabla 2 como parámetros de recalibración. Aunque usar las estadísticas directamente puede que no sea la mejor opción, no discutimos cómo obtener automáticamente los mejores parámetros. Intentaremos utilizar técnicas de aprendizaje automático para obtener los mejores parámetros en el trabajo futuro. En el trabajo, utilizamos BoosTexter [20] para clasificar todas las historias en uno de los 11 ROIs. BoosTexter es un programa de aprendizaje automático basado en boosting, que crea una serie de reglas simples para construir un clasificador de datos de texto o atributos-valor. Utilizamos el peso del término generado utilizando el modelo TF-IDF como característica para la clasificación de historias. Entrenamos el modelo con las 12000 historias en inglés evaluadas en TDT2, y clasificamos el resto de las historias en TDT2 y todas las historias en TDT3. Los resultados de clasificación se utilizan para la reponderación de términos en la fórmula (11). Dado que las etiquetas de clase de las historias de tema apagado no se proporcionan en los conjuntos de datos de TDT, no podemos proporcionar la precisión de clasificación aquí. Por lo tanto, no discutimos los efectos de la precisión de la clasificación en el rendimiento de NED en el artículo. 6. CONFIGURACIÓN EXPERIMENTAL 6.1 Conjuntos de datos Utilizamos dos conjuntos de datos de LDC [18], TDT2 y TDT3, para nuestros experimentos. TDT2 contiene noticias desde enero hasta junio de 1998. Contiene alrededor de 54,000 historias de fuentes como ABC, Associated Press, CNN, New York Times, Public Radio International, Voice of America, etc. Solo se consideraron historias en inglés en la colección. TDT3 contiene aproximadamente 31,000 historias en inglés recopiladas de octubre a diciembre de 1998. Además de las fuentes utilizadas en TDT2, también contiene historias de las transmisiones de televisión de NBC y MSNBC. Utilizamos versiones transcritas de las emisiones de televisión y radio además de noticias textuales. El conjunto de datos TDT2 está etiquetado con alrededor de 100 temas, y aproximadamente 12,000 historias en inglés pertenecen al menos a uno de estos temas. El conjunto de datos TDT3 está etiquetado con alrededor de 120 temas, y aproximadamente 8000 historias en inglés pertenecen al menos a uno de estos temas. Todos los temas están clasificados en 11 Reglas de Interpretación: (1) Elecciones, (2) Escándalos/Audiencias, (3) Casos Legales/Criminales, (4) Desastres Naturales, (5) Accidentes, (6) Violencia o Guerra en Curso, (7) Noticias de Ciencia y Descubrimientos, (8) Finanzas, (9) Nueva Ley, (10) Noticias Deportivas, (11) Varios. Noticias. 6.2 Métrica de evaluación TDT utiliza una función de costo CDet que combina las probabilidades de perder una nueva historia y una falsa alarma [19]: * * * *Det Miss Miss Target FA FA NontargetC C P P C P P= + (12) Tabla 2. Correlación promedio entre tipos de términos y clases de noticias donde CMiss significa el costo de perder una nueva historia, PMiss significa la probabilidad de perder una nueva historia, y PTarget significa la probabilidad de ver una nueva historia en los datos; CFA significa el costo de una falsa alarma, PFA significa la probabilidad de una falsa alarma, y PNontarget significa la probabilidad de ver una historia antigua. El costo CDet se normaliza de tal manera que un sistema perfecto obtiene una puntuación de 0 y un sistema trivial, que es el mejor de marcar todas las historias como nuevas o antiguas, obtiene una puntuación de 1: ( ( * , * ) ) Det Det Miss Target FA Nontarget C Norm C min C P C P = (13) El sistema de detección de eventos nuevos proporciona dos salidas para cada historia. La primera parte es sí o no, indicando si la historia desencadena un nuevo evento o no. La segunda parte es una puntuación que indica la confianza de la primera decisión. Los puntajes de confianza se pueden utilizar para trazar la curva DET, es decir, curvas que representan la probabilidad de falsa alarma frente a la probabilidad de omisión. El costo normalizado mínimo se puede determinar si se elige el umbral óptimo en la puntuación. RESULTADOS EXPERIMENTALES 7.1 Resultados principales Para probar los enfoques propuestos en el modelo, implementamos y probamos cinco sistemas: Sistema-1: este sistema se utiliza como referencia. Se implementa basándose en el modelo básico descrito en la sección 3, es decir, utilizando el modelo TF-IDF incremental para generar pesos de términos y utilizando la distancia de Hellinger para calcular la similitud entre documentos. La normalización del puntaje de similitud también se emplea [8]. Se utiliza el procedimiento de detección S-S. Sistema-2: este sistema es igual que el sistema-1 excepto que se utiliza el procedimiento de detección S-C. Sistema-3: este sistema es igual que el sistema-1 excepto que utiliza el nuevo procedimiento de detección que se basa en un árbol de indexación. Sistema-4: implementado basado en el enfoque presentado en la sección 5.1, es decir, los términos se vuelven a ponderar según la distancia entre las distribuciones de términos en un grupo y todas las historias. Se utiliza el nuevo procedimiento de detección. Sistema-5: implementado basado en el enfoque presentado en la sección 5.2, es decir, los términos de diferentes tipos son reponderados según la clase de noticias utilizando parámetros entrenados. El nuevo procedimiento de detección se utiliza. Los siguientes son algunos otros sistemas NED: Sistema-6: [21] para cada par de historias, calcula tres valores de similitud para entidad nombrada, entidad no nombrada y todos los términos respectivamente. Y emplear Máquina de Vectores de Soporte para predecir nuevo o antiguo utilizando los valores de similitud como características. Sistema-7: [8] extendió un modelo básico incremental de TF-IDF para incluir modelos específicos de origen, normalización de puntuaciones de similitud basada en promedios específicos de documentos, normalización de puntuaciones de similitud basada en promedios específicos de pares de fuentes, etc. Sistema-8: [13] dividió la representación del documento en dos partes: entidades nombradas y entidades no nombradas, y eligió una parte efectiva para cada clase de noticias. La tabla 3 y la tabla 4 muestran los costos normalizados ponderados por tema y los tiempos de comparación en los conjuntos de datos TDT2 y TDT3 respectivamente. Dado que no se contaba con un conjunto de datos de validación para ajustar el umbral θ nuevo en los experimentos en TDT2, solo informamos los costos normalizados mínimos para nuestros sistemas en la tabla 3. El sistema-5 supera a todos los demás sistemas, incluido el sistema-6, y realiza solo 2.78e+8 veces la comparación de tiempos en el procedimiento de detección, lo que representa solo el 13.4% del sistema-1. Tabla 3. Los resultados de NED en los sistemas TDT2 Min Norm(CDet) tiempos de comparación son: Sistema-1 0.5749 2.08e+9, Sistema-2 0.6673 3.77e+8, Sistema-3 0.5765 2.81e+8, Sistema-4 0.5431 2.99e+8, Sistema-5 0.5089 2.78e+8, Sistema-6 0.5300. Al evaluar en los costos normalizados en TDT3, utilizamos los umbrales óptimos obtenidos del conjunto de datos de TDT2 para todos los sistemas. El sistema-2 reduce los tiempos de comparación a 1.29e+9, que es solo el 18.3% del sistema-1, pero al mismo tiempo también obtiene un costo normalizado mínimo deteriorado que es 0.0499 más alto que el sistema-1. El Sistema-3 utiliza el nuevo procedimiento de detección basado en el índice de noticias en árbol. Requiere incluso menos tiempo de comparación que el sistema-2. Esto se debe a que las comparaciones de historias suelen arrojar mayores similitudes que las de grupos de historias, por lo que las historias tienden a combinarse en el sistema-3. Y el sistema-3 es básicamente equivalente al sistema-1 en los resultados de precisión. El System-4 ajusta los pesos de los términos basándose en la distancia de las distribuciones de términos entre todo el corpus y el conjunto de historias del clúster, lo que resulta en una mejora significativa de 0.0468 en comparación con el system-1. El mejor sistema (sistema-5) tiene un costo normalizado mínimo de 0.5012, lo cual es 0.0797 mejor que el sistema-1, y también mejor que cualquier otro resultado previamente reportado para este conjunto de datos [8, 13]. Además, el sistema-5 solo necesita 1.05e+8 veces de comparación, lo que equivale al 14.9% del sistema-1. Tabla 4. Los resultados de NED en los sistemas TDT3 Norm(CDet) Norma mínima(CDet) Tiempos de comparación Sistema-1 0.6159 0.5809 7.04e+8 Sistema-2① 0.6493 0.6308 1.29e+8 Sistema-3② 0.6197 0.5868 1.03e+8 Sistema-4② 0.5601 0.5341 1.03e+8 Sistema-5② 0.5413 0.5012 1.05e+8 Sistema-7 -- 0.5783 -Sistema-8 -- 0.5229 -① θ nuevo=0.13 ② θ inicial=0.13, λ =3, δ =0.15 La Figura 5 muestra las cinco curvas DET para nuestros sistemas en el conjunto de datos TDT3. El Sistema-5 logra el costo mínimo con una tasa de falsas alarmas de 0.0157 y una tasa de omisión de 0.4310. Podemos observar que System4 y System-5 obtienen una menor probabilidad de error de omisión en regiones de baja probabilidad de falsa alarma. La hipótesis es que se transfiere más valor de peso a los términos clave de los temas que a los términos no clave. La puntuación de similitud entre dos historias pertenecientes a diferentes temas es más baja que antes, porque los términos superpuestos suelen no ser términos clave de sus temas. 7.2 Selección de parámetros para la detección de árboles de indexación. La Figura 3 muestra los costos normalizados mínimos obtenidos por el sistema-3 en TDT3 utilizando diferentes parámetros. El parámetro de inicio θ se prueba en seis valores que van desde 0.03 hasta 0.18. Y el parámetro λ se prueba en cuatro valores: 1, 2, 3 y 4. Podemos ver que, cuando θ inicial se establece en 0.12, que es el más cercano a θ nuevo, los costos son más bajos que los demás. Esto es fácil de explicar, porque cuando se agrupan historias pertenecientes al mismo tema en un conjunto, es más razonable que el conjunto represente las historias en él. Cuando el parámetro λ se establece en 3 o 4, los costos son mejores que en otros casos, pero no hay mucha diferencia entre 3 y 4. 0 0.05 0.1 0.15 0.2 1 2 3 4 0.5 0.6 0.7 0.8 0.9 1 θ-initλ MinCost 0.6 0.65 0.7 0.75 0.8 0.85 0.9 Figura 3. Costo mínimo en TDT3 (δ =0.15) 0 0.05 0.1 0.15 0.2 1 2 3 4 0 0.5 1 1.5 2 2.5 x 10 8 θ-inicio λ Comparando tiempos 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 8 Figura 4. Comparando los tiempos en TDT3 (δ =0.15), la Figura 4 muestra los tiempos de comparación utilizados por el sistema-3 en TDT3 con los mismos parámetros que la Figura 3. Los tiempos de comparación dependen fuertemente de θ init. Debido a que cuanto mayor sea θ init, menos historias se combinan juntas, más veces de comparación se necesitan para la decisión de un nuevo evento. Por lo tanto, usamos θ init = 0.13, λ = 3, δ = 0.15 para los sistemas 3, 4 y 5. En esta configuración de parámetros, podemos obtener tanto costos normalizados mínimos bajos como menos tiempos de comparación. CONCLUSIÓN Hemos propuesto un procedimiento de detección basado en un árbol de indexación de noticias en nuestro modelo. Reduce los tiempos de comparación a aproximadamente una séptima parte del método tradicional sin afectar la precisión de NED. También hemos presentado dos extensiones al modelo básico TF-IDF. La primera extensión se realiza ajustando los pesos de los términos basados en las distribuciones de términos entre todo el corpus y un conjunto de historias de un clúster. Y la segunda extensión al modelo básico TF-IDF es un mejor uso de los tipos de términos (tipos de entidades nombradas y partes de la velocidad) de acuerdo con las categorías de noticias. Nuestros resultados experimentales en los conjuntos de datos TDT2 y TDT3 muestran que ambas extensiones contribuyen significativamente a la mejora en la precisión. No consideramos la información de tiempo de noticias como una pista para la tarea de NED, ya que la mayoría de los temas duran mucho tiempo y los conjuntos de datos de TDT solo abarcan un período relativamente corto (no más de 6 meses). Para el trabajo futuro, queremos recopilar un conjunto de noticias que abarque un período más largo de internet e integrar información temporal en la tarea de NED. Dado que el tema es un grupo de noticias relativamente grueso, también queremos refinar la granularidad del grupo a nivel de evento e identificar diferentes eventos y sus relaciones dentro de un tema. Agradecimientos: Este trabajo ha sido apoyado por la Fundación Nacional de Ciencias Naturales de China bajo la subvención No. 90604025. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son del autor(es) y no necesariamente reflejan las del patrocinador. REFERENCIAS [1] http://www.nist.gov/speech/tests/tdt/index.htm [2] En Detección y Seguimiento de Temas. Organización de la información basada en eventos. Kluwer Academic Publishers, 2002. .01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90 1 2 5 10 20 40 60 80 90 Probabilidad de Falsa Alarma (en %) Probabilidad de Omisión (en %) Curva Ponderada por Tema del SISTEMA1 Norma Mínima (Costo) del SISTEMA1 Curva Ponderada por Tema del SISTEMA2 Norma Mínima (Costo) del SISTEMA2 Curva Ponderada por Tema del SISTEMA3 Norma Mínima (Costo) del SISTEMA3 Curva Ponderada por Tema del SISTEMA4 Norma Mínima (Costo) del SISTEMA4 Curva Ponderada por Tema del SISTEMA5 Norma Mínima (Costo) del SISTEMA5 Rendimiento Aleatorio Figura 5. Curvas DET en TDT3 [3] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B.T. Archibald y X. Liu. Enfoques de aprendizaje para detectar y rastrear eventos de noticias. En el número especial de IEEE Intelligent Systems sobre Aplicaciones de Recuperación de Información Inteligente, volumen 14 (4), 1999, páginas 32-43. [4] Y. Yang, T. Pierce y J. Carbonell. Un estudio sobre la detección de eventos retrospectivos y en línea. En Actas de SIGIR-98, Melbourne, Australia, 1998, 28-36. [5] J. Allan, V. Lavrenko, D. Malin y R. Swan. Detecciones, límites y líneas de tiempo: Umass y tdt-3. En Actas del Taller de Detección y Seguimiento de Temas (TDT-3), Viena, VA, 2000, 167-174. [6] R. Papka y J. Allan. Detección de nuevos eventos en línea utilizando agrupamiento de un solo paso TÍTULO2:. Informe técnico UM-CS1998-021, 1998. [7] W. Lam, H. Meng, K. Wong y J. I'm sorry, but \"Yen\" is not a sentence. Can you please provide a sentence for me to translate into Spanish? Utilizando el Análisis Contextual para la Detección de Eventos de Noticias. Revista Internacional de Sistemas Inteligentes, 2001, 525-546. [8] B. Thorsten, C. Francine y F. Ayman. Un sistema para la detección de nuevos eventos. En Actas de la 26ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2003, 330-337. [9] S. Nicola y C. Joe. Combinando clasificadores semánticos y sintácticos de documentos para mejorar la detección de primeras noticias. En Actas de la 24ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell, and C. Jin. \n\nACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell y C. Jin. Detección de novedades condicionada al tema. En Actas de la 8ª Conferencia Internacional de ACM SIGKDD, ACM Press. 2002, 688-693. [11] M. Juha, A.M. Helena y S. Marko. Aplicando Clases Semánticas en la Detección y Seguimiento de Eventos. En Actas de la Conferencia Internacional sobre Procesamiento del Lenguaje Natural (ICON 2002), 2002, páginas 175-183. [12] M. Juha, A.M. Helena y S. Marko. Semántica simple en la detección y seguimiento de temas. Recuperación de información, 7(3-4): 2004, 347-368. [13] K. Giridhar y J. Allan. Clasificación de texto y entidades nombradas para la detección de nuevos eventos. En Actas de la 27ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2004, 297-304. [14] J. P. Callan, W. B. Croft, y S. M. Harding. El Sistema de Recuperación INQUERY. En Actas de DEXA-92, 3ra Conferencia Internacional sobre Aplicaciones de Bases de Datos y Sistemas Expertos, 1992, 78-83. [15] R. Krovetz. Viendo la morfología como un proceso de inferencia. En Actas de ACM SIGIR93, 1993, 61-81. [16] Y. Yang y J. Pedersen. Un estudio comparativo sobre la selección de características en la categorización de textos. En J. D. H. Fisher, editor, La Decimocuarta Conferencia Internacional sobre Aprendizaje Automático (ICML97), Morgan Kaufmann, 1997, 412-420. [17] T. M. Cover y J. A. I'm sorry, but \"Thomas\" is not a sentence. Can you provide more context or a complete sentence for me to translate into Spanish? Elementos de la teoría de la información. Wiley. 1991. [18] El <br>consorcio de datos lingüísticos</br>, http://www.ldc,upenn.edu/. [19] La definición de la tarea TDT 2001 y el plan de evaluación, http://www.nist.gov/speech/tests/tdt/tdt2001/evalplan.htm. [20] R. E. Schapire y Y. Cantante. Boostexter: Un sistema basado en Boosting para la categorización de texto. En Machine Learning 39(2/3):1, Kluwer Academic Publishers, 2000, 35-168. [21] K. Giridhar y J. Allan. 2005. Utilizando nombres y temas para la detección de nuevos eventos. En Actas de la Conferencia de Tecnología Humana y la Conferencia sobre Métodos Empíricos en Lenguaje Natural, Vancouver, 2005, 121-128 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "baseline system": {
            "translated_key": "sistema base",
            "is_in_text": true,
            "original_annotated_sentences": [
                "New Event Detection Based on Indexing-tree and Named Entity Zhang Kuo Tsinghua University Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Tsinghua University Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Tsinghua University Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn ABSTRACT New Event Detection (NED) aims at detecting from one or multiple streams of news stories that which one is reported on a new event (i.e.",
                "not reported previously).",
                "With the overwhelming volume of news available today, there is an increasing need for a NED system which is able to detect new events more efficiently and accurately.",
                "In this paper we propose a new NED model to speed up the NED task by using news indexing-tree dynamically.",
                "Moreover, based on the observation that terms of different types have different effects for NED task, two term reweighting approaches are proposed to improve NED accuracy.",
                "In the first approach, we propose to adjust term weights dynamically based on previous story clusters and in the second approach, we propose to employ statistics on training data to learn the named entity reweighting model for each class of stories.",
                "Experimental results on two Linguistic Data Consortium (LDC) datasets TDT2 and TDT3 show that the proposed model can improve both efficiency and accuracy of NED task significantly, compared to the <br>baseline system</br> and other existing systems.",
                "Categories and Subject Descriptors H.3.3 [Information Systems]: Information Search and Retrieval; H.4.2 [Information Systems Applications]: Types of Systemsdecision support.",
                "General Terms Algorithms, Performance, Experimentation 1.",
                "INTRODUCTION Topic Detection and Tracking (TDT) program aims to develop techniques which can effectively organize, search and structure news text materials from a variety of newswire and broadcast media [1].",
                "New Event Detection (NED) is one of the five tasks in TDT.",
                "It is the task of online identification of the earliest report for each topic as soon as that report arrives in the sequence of documents.",
                "A Topic is defined as a seminal event or activity, along with directly related events and activities [2].",
                "An Event is defined as something (non-trivial) happening in a certain place at a certain time [3].",
                "For instance, when a bomb explodes in a building, the exploding is the seminal event that triggers the topic, and other stories on the same topic would be those discussing salvaging efforts, the search for perpetrators, arrests and trial and so on.",
                "Useful news information is usually buried in a mass of data generated everyday.",
                "Therefore, NED systems are very useful for people who need to detect novel information from real-time news stream.",
                "These real-life needs often occur in domains like financial markets, news analysis, and intelligence gathering.",
                "In most of state-of-the-art (currently) NED systems, each news story on hand is compared to all the previous received stories.",
                "If all the similarities between them do not exceed a threshold, then the story triggers a new event.",
                "They are usually in the form of cosine similarity or Hellinger similarity metric.",
                "The core problem of NED is to identify whether two stories are on the same topic.",
                "Obviously, these systems cannot take advantage of topic information.",
                "Further more, it is not acceptable in real applications because of the large amount of computation required in the NED process.",
                "Other systems organize previous stories into clusters (each cluster corresponds to a topic), and new story is compared to the previous clusters instead of stories.",
                "This manner can reduce comparing times significantly.",
                "Nevertheless, it has been proved that this manner is less accurate [4, 5].",
                "This is because sometimes stories within a topic drift far away from each other, which could lead low similarity between a story and its topic.",
                "On the other hand, some proposed NED systems tried to improve accuracy by making better use of named entities [10, 11, 12, 13].",
                "However, none of the systems have considered that terms of different types (e.g.",
                "Noun, Verb or Person name) have different effects for different classes of stories in determining whether two stories are on the same topic.",
                "For example, the names of election candidates (Person name) are very important for stories of election class; the locations (Location name) where accidents happened are important for stories of accidents class.",
                "So, in NED, there still exist following three problems to be investigated: (1) How to speed up the detection procedure while do not decrease the detection accuracy? (2) How to make good use of cluster (topic) information to improve accuracy? (3) How to obtain better news story representation by better understanding of named entities.",
                "Driven by these problems, we have proposed three approaches in this paper. (1)To make the detection procedure faster, we propose a new NED procedure based on news indexing-tree created dynamically.",
                "Story indexing-tree is created by assembling similar stories together to form news clusters in different hierarchies according to their values of similarity.",
                "Comparisons between current story and previous clusters could help find the most similar story in less comparing times.",
                "The new procedure can reduce the amount of comparing times without hurting accuracy. (2)We use the clusters of the first floor in the indexing-tree as news topics, in which term weights are adjusted dynamically according to term distribution in the clusters.",
                "In this approach, cluster (topic) information is used properly, so the problem of theme decentralization is avoided. (3)Based on observations on the statistics obtained from training data, we found that terms of different types (e.g.",
                "Noun and Verb) have different effects for different classes of stories in determining whether two stories are on the same topic.",
                "And we propose to use statistics to optimize the weights of the terms of different types in a story according to the news class that the story belongs to.",
                "On TDT3 dataset, the new NED model just uses 14.9% comparing times of the basic model, while its minimum normalized cost is 0.5012, which is 0.0797 better than the basic model, and also better than any other results previously reported for this dataset [8, 13].",
                "The rest of the paper is organized as follows.",
                "We start off this paper by summarizing the previous work in NED in section 2.",
                "Section 3 presents the basic model for NED that most current systems use.",
                "Section 4 describes our new detection procedure based on news indexing-tree.",
                "In section 5, two term reweighting methods are proposed to improve NED accuracy.",
                "Section 6 gives our experimental data and evaluation metrics.",
                "We finally wrap up with the experimental results in Section 7, and the conclusions and future work in Section 8. 2.",
                "RELATED WORK Papka et al. proposed Single-Pass clustering on NED [6].",
                "When a new story was encountered, it was processed immediately to extract term features and a query representation of the storys content is built up.",
                "Then it was compared with all the previous queries.",
                "If the document did not trigger any queries by exceeding a threshold, it was marked as a new event.",
                "Lam et al build up previous query representations of story clusters, each of which corresponds to a topic [7].",
                "In this manner comparisons happen between stories and clusters.",
                "Recent years, most work focus on proposing better methods on comparison of stories and document representation.",
                "Brants et al. [8] extended a basic incremental TF-IDF model to include sourcespecific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, term reweighting based on inverse event frequencies, and segmentation of documents.",
                "Good improvements on TDT bench-marks were shown.",
                "Stokes et al. [9] utilized a combination of evidence from two distinct representations of a documents content.",
                "One of the representations was the usual free text vector, the other made use of lexical chains (created using WordNet) to build another term vector.",
                "Then the two representations are combined in a linear fashion.",
                "A marginal increase in effectiveness was achieved when the combined representation was used.",
                "Some efforts have been done on how to utilize named entities to improve NED.",
                "Yang et al. gave location named entities four times weight than other terms and named entities [10].",
                "DOREMI research group combined semantic similarities of person names, location names and time together with textual similarity [11][12].",
                "UMass [13] research group split document representation into two parts: named entities and non-named entities.",
                "And it was found that some classes of news could achieve better performance using named entity representation, while some other classes of news could achieve better performance using non-named entity representation.",
                "Both [10] and [13] used text categorization technique to classify news stories in advance.",
                "In [13] news stories are classified automatically at first, and then test sensitivities of names and non-name terms for NED for each class.",
                "In [10] frequent terms for each class are removed from document representation.",
                "For example, word election does not help identify different elections.",
                "In their work, effectiveness of different kinds of names (or terms with different POS) for NED in different news classes are not investigated.",
                "We use statistical analysis to reveal the fact and use it to improve NED performance. 3.",
                "BASIC MODEL In this section, we present the basic New Event Detection model which is similar to what most current systems apply.",
                "Then, we propose our new model by extending the basic model.",
                "New Event Detection systems use news story stream as input, in which stories are strictly time-ordered.",
                "Only previously received stories are available when dealing with current story.",
                "The output is a decision for whether the current story is on a new event or not and the confidence of the decision.",
                "Usually, a NED model consists of three parts: story representation, similarity calculation and detection procedure. 3.1 Story Representation Preprocessing is needed before generating story representation.",
                "For preprocessing, we tokenize words, recognize abbreviations, normalize abbreviations, add part-of-speech tags, remove stopwords included in the stop list used in InQuery [14], replace words with their stems using K-stem algorithm[15], and then generate word vector for each news story.",
                "We use incremental TF-IDF model for term weight calculation [4].",
                "In a TF-IDF model, term frequency in a news document is weighted by the inverse document frequency, which is generated from training corpus.",
                "When a new term occurs in testing process, there are two solutions: simply ignore the new term or set df of the term as a small const (e.g. df = 1).",
                "The new term receives too low weight in the first solution (0) and too high weight in the second solution.",
                "In incremental TF-IDF model, document frequencies are updated dynamically in each time step t: 1( ) ( ) ( )t t D tdf w df w df w−= + (1) where Dt represents news story set received in time t, and dfDt(w) means the number of documents that term w occurs in, and dft(w) means the total number of documents that term w occurs in before time t. In this work, each time window includes 50 news stories.",
                "Thus, each story d received in t is represented as follows: 1 2{ ( , , ), ( , , ),..., ( , , )}nd weight d t w weight d t w weight d t w→ where n means the number of distinct terms in story d, and ( , , )weight d t w means the weight of term w in story d at time t: log( ( , ) 1) log(( 1) /( ( ) 0.5)) ( , , ) log( ( , ) 1) log(( 1) /( ( ) 0.5)) t t t t w d tf d w N df w weight d t w tf d w N df w ∈ + + + = + + +∑ (2) where Nt means the total number of news stories before time t, and tf(d,w) means how many times term w occurs in news story d. 3.2 Similarity Calculation We use Hellinger distance for the calculation of similarity between two stories, for two stories d and d at time t, their similarity is defined as follows: , ( , , ) ( , , ) * ( , , ) w d d sim d d t weight d t w weight d t w ∈ = ∑ (3) 3.3 Detection Procedure For each story d received in time step t, the value ( ) ( ) ( ) ( ( , , )) time d time d n d max sim d d t < = (4) is a score used to determine whether d is a story about a new topic and at the same time is an indication of the confidence in our decision [8]. time(d) means the publication time of story d. If the score exceeds the thresholdθ new, then there exists a sufficiently similar document, thus d is a old story, otherwise, there is no sufficiently similar previous document, thus d is an new story. 4.",
                "New NED Procedure Traditional NED systems can be classified into two main types on the aspect of detection procedure: (1) S-S type, in which the story on hand is compared to each story received previously, and use the highest similarity to determine whether current story is about a new event; (2) S-C type, in which the story on hand is compared to all previous clusters each of which representing a topic, and the highest similarity is used for final decision for current story.",
                "If the highest similarity exceeds thresholdθ new, then it is an old story, and put it into the most similar cluster; otherwise it is a new story and create a new cluster.",
                "Previous work show that the first manner is more accurate than the second one [4][5].",
                "Since sometimes stories within a topic drift far away from each other, a story may have very low similarity with its topic.",
                "So using similarities between stories for determining new story is better than using similarities between story and clusters.",
                "Nevertheless, the first manner needs much more comparing times which means the first manner is low efficient.",
                "We propose a new detection procedure which uses comparisons with previous clusters to help find the most similar story in less comparing times, and the final new event decision is made according to the most similar story.",
                "Therefore, we can get both the accuracy of S-S type methods and the efficiency of S-C type methods.",
                "The new procedure creates a news indexing-tree dynamically, in which similar stories are put together to form a hierarchy of clusters.",
                "We index similar stories together by their common ancestor (a cluster node).",
                "Dissimilar stories are indexed in different clusters.",
                "When a story is coming, we use comparisons between the current story and previous hierarchical clusters to help find the most similar story which is useful for new event decision.",
                "After the new event decision is made, the current story is inserted to the indexing-tree for the following detection.",
                "The news indexing-tree is defined formally as follows: S-Tree = {r, NC , NS , E} where r is the root of S-Tree, NC is the set of all cluster nodes, NS is the set of all story nodes, and E is the set of all edges in S-Tree.",
                "We define a set of constraints for a S-Tree: ⅰ . , is an non-terminal node in the treeC i i N i∀ ∈ → ⅱ . , is a terminal node in the treeS i i N i∀ ∈ → ⅲ . , out degree of is at least 2C i i N i∀ ∈ → ⅳ . , is represented as the centroid of its desendantsC i i iN∀ ∈ → For a news story di, the comparison procedure and inserting procedure based on indexing-tree are defined as follows.",
                "An example is shown by Figure 1 and Figure 2.",
                "Figure 1.",
                "Comparison procedure Figure 2.",
                "Inserting procedure Comparison procedure: Step 1: compare di to all the direct child nodes of r and select λ nodes with highest similarities, e.g., C1 2 and C1 3 in Figure 1.",
                "Step 2: for each selected node in the last step, e.g.",
                "C1 2, compare di to all its direct child nodes, and select λ nodes with highest similarities, e.g.",
                "C2 2 and d8.",
                "Repeat step 2 for all non-terminal nodes.",
                "Step 3: record the terminal node with the highest similarty to di, e.g. s5, and the similarity value (0.20).",
                "Inserting di to the S-tree with r as root: Find the node n which is direct child of r in the path from r to the terminal node with highest similarity s, e.g.",
                "C1 2.",
                "If s is smaller than θ init+(h-1)δ , then add di to the tree as a direct child of r. Otherwise, if n is a terminal node, then create a cluster node instead of n, and add both n and di as its direct children; if n is an non-terminal node, then repeat this procedure and insert di to the sub-tree with n as root recursively.",
                "Here h is the length between n and the root of S-tree.",
                "The more the stories in a cluster similar to each other, the better the cluster represents the stories in it.",
                "Hence we add no constraints on the maximum of trees height and degree of a node.",
                "Therefore, we cannot give the complexity of this indexing-tree based procedure.",
                "But we will give the number of comparing times needed by the new procedure in our experiments in section7. 5.",
                "Term Reweighting Methods In this section, two term reweighting methods are proposed to improve NED accuracy.",
                "In the first method, a new way is explored for better using of cluster (topic) information.",
                "The second one finds a better way to make use of named entities based on news classification. 5.1 Term Reweighting Based on Distribution Distance TF-IDF is the most prevalent model used in information retrieval systems.",
                "The basic idea is that the fewer documents a term appears in, the more important the term is in discrimination of documents (relevant or not relevant to a query containing the term).",
                "Nevertheless, in TDT domain, we need to discriminate documents with regard to topics rather than queries.",
                "Intuitively, using cluster (topic) vectors to compare with subsequent news stories should outperform using story vectors.",
                "Unfortunately, the experimental results do not support this intuition [4][5].",
                "Based on observation on data, we find the reason is that a news topic usually contains many directly or indirectly related events, while they all have their own sub-subjects which are usually different with each other.",
                "Take the topic described in section 1 as an example, events like the explosion and salvage have very low similarities with events about criminal trial, therefore stories about trial would have low similarity with the topic vector built on its previous events.",
                "This section focuses on how to effectively make use of topic information and at the same time avoid the problem of content decentralization.",
                "At first, we classify terms into 5 classes to help analysis the needs of the modified model: Term class A: terms that occur frequently in the whole corpus, e.g., year and people.",
                "Terms of this class should be given low weights because they do not help much for topic discrimination.",
                "Term class B: terms that occur frequently within a news category, e.g., election, storm.",
                "They are useful to distinguish two stories in different news categories.",
                "However, they cannot provide information to determine whether two stories are on the same or different topics.",
                "In another words, term election and term storm are not helpful in differentiate two election campaigns and two storm disasters.",
                "Therefore, terms of this class should be assigned lower weights.",
                "Term class C: terms that occur frequently in a topic, and infrequently in other topics, e.g., the name of a crash plane, the name of a specific hurricane.",
                "News stories that belong to different topics rarely have overlap terms in this class.",
                "The more frequently a term appears in a topic, the more important the term is for a story belonging to the topic, therefore the term should be set higher weight.",
                "Term class D: terms that appear in a topic exclusively, but not frequently.",
                "For example, the name of a fireman who did very well in a salvage action, which may appears in only two or three stories but never appeared in other topics.",
                "Terms of this type should receive more weights than in TF-IDF model.",
                "However, since they are not popular in the topic, it is not appropriate to give them too high weights.",
                "Term class E: terms with low document frequency, and appear in different topics.",
                "Terms of this class should receive lower weights.",
                "Now we analyze whether TF-IDF model can give proper weights to the five classes of terms.",
                "Obviously, terms of class A are lowly weighted in TF-IDF model, which is conformable with the requirement described above.",
                "In TF-IDF model, terms of class B are highly dependant with the number of stories in a news class.",
                "TF-IDF model cannot provide low weights if the story containing the term belongs to a relative small news class.",
                "For a term of class C, the more frequently it appears in a topic, the less weight TFIDF model gives to it.",
                "This strongly conflicts with the requirement of terms in class C. For terms of class D, TF-IDF model gives them high weights correctly.",
                "But for terms of class E, TF-IDF model gives high weights to them which are not conformable with the requirement of low weights.",
                "To sum up, terms of class B, C, E cannot be properly weighted in TF-IDF model.",
                "So, we propose a modified model to resolve this problem.",
                "When θ init andθ new are set closely, we assume that most of the stories in a first-level cluster (a direct child node of root node) are on the same topic.",
                "Therefore, we make use of a first-level cluster to capture term distribution (df for all the terms within the cluster) within the topic dynamically.",
                "KL divergence of term distribution in a first-level cluster and the whole story set is used to adjust term weights:  ( , , ) * (1 * ( || )) ( , , ) ( , , ) * (1 * ( || )) cw tw cw tw w d D weight d t w KL P P weight d t w weight d t w KL P P γ γ ∈ + = +∑ (5) where ( ) ( ) ( ) ( ) 1,cw cw c c c c df w df w p y p y N N = = − (6) ( ) ( ) ( ) ( ) 1,t t tw tw t t df w df w p y p y N N = = − (7) where dfc(w) is the number of documents containing term w within cluster C, and Nc is the number of documents in cluster C, and Nt is the total number of documents that arrive before time step t. γ is a const parameter, now is manually set 3.",
                "KL divergence is defined as follows [17]: ( ) ( || ) ( ) log ( )x p x KL P Q p x q x = ∑ (8) The basic idea is: for a story in a topic, the more a term occurs within the topic, and the less it occurs in other topics, it should be assigned higher weights.",
                "Obviously, modified model can meet all the requirements of the five term classes listed above. 5.2 Term Reweighting Based on Term Type and Story Class Previous work found that some classes of news stories could achieve good improvements by giving extra weight to named entities.",
                "But we find that terms of different types should be given different amount of extra weight for different classes of news stories.",
                "We use open-NLP1 to recognize named entity types and part-ofspeech tags for terms that appear in news stories.",
                "Named entity types include person name, organization name, location name, date, time, money and percentage, and five POSs are selected: none (NN), verb (VB), adjective (JJ), adverb (RB) and cardinal number (CD).",
                "Statistical analysis shows topic-level discriminative terms types for different classes of stories.",
                "For the sake of convenience, named entity type and part-of-speech tags are uniformly called term type in subsequent sections.",
                "Determining whether two stories are about the same topic is a basic component for NED task.",
                "So at first we use 2 χ statistic to compute correlations between terms and topics.",
                "For a term t and a topic T, a contingence table is derived: Table 1.",
                "A 2×2 Contingence Table Doc Number belong to topic T not belong to topic T include t A B not include t C D The 2 χ statistic for a specific term t with respect to topic T is defined to be [16]: 2 2 ( , ) ( ) * ( ) ( ) * ( ) * ( ) * ( ) w T A B C D AD CB A C B D A B C D χ = + + + − + + + + (9) News topics for the TDT task are further classified into 11 rules of interpretations (ROIs) 2 .",
                "The ROI can be seen as a higher level class of stories.",
                "The average correlation between a term type and a topic ROI is computed as: 2 avg 2 ( , )( ( , ) )k m m km kT R w P w TP R p w T R P χ χ ∈ ∈ ∑ ∑（ , ）= 1 1 k=1…K, m=1…M (10) where K is the number of term types (set 12 constantly in the paper).",
                "M is the number news classes (ROIs, set 11 in the paper).",
                "Pk represents the set of all terms of type k, and Rm represents the set of all topics of class m, p(t,T) means the probability that t occurs in topic T. Because of limitation of space, only parts of the term types (9 term types) and parts of news classes (8 classes) are listed in table 2 with the average correlation values between them.",
                "The statistics is derived from labeled data in TDT2 corpus. (Results in table 2 are already normalized for convenience in comparison.)",
                "The statistics in table 2 indicates the usefulness of different term types in topic discrimination with respect to different news classes.",
                "We can see that, location name is the most useful term type for three news classes: Natural Disasters, Violence or War, Finances.",
                "And for three other categories Elections, Legal/Criminal Cases, Science and Discovery, person name is the most discriminative term type.",
                "For Scandals/Hearings, date is the most important information for topic discrimination.",
                "In addition, Legal/Criminal Cases and Finance topics have higher correlation with money terms, while Science and Discovery have higher correlation with percentage terms.",
                "Non-name terms are more stable for different classes. 1 . http://opennlp.sourceforge.net/ 2 . http://projects.ldc.upenn.edu/TDT3/Guide/label.html From the analysis of table 2, it is reasonable to adjust term weight according to their term type and the news class the story belongs to.",
                "New term weights are reweighted as follows: ( ) ( ) ( ) ( ) ( , , ) * ( , , ) ( , , ) * class d D type w T class d D type w w d weight d t w weight d t w weight d t w α α ∈ = ∑ (11) where type(w) represents the type of term w, and class(d) represents the class of story d, c kα is reweighting parameter for news class c and term type k. In the work, we just simply use statistics in table 2 as the reweighting parameters.",
                "Even thought using the statistics directly may not the best choice, we do not discuss how to automatically obtain the best parameters.",
                "We will try to use machine learning techniques to obtain the best parameters in the future work.",
                "In the work, we use BoosTexter [20] to classify all stories into one of the 11 ROIs.",
                "BoosTexter is a boosting based machine learning program, which creates a series of simple rules for building a classifier for text or attribute-value data.",
                "We use term weight generated using TF-IDF model as feature for story classification.",
                "We trained the model on the 12000 judged English stories in TDT2, and classify the rest of the stories in TDT2 and all stories in TDT3.",
                "Classification results are used for term reweighting in formula (11).",
                "Since the class labels of topic-off stories are not given in TDT datasets, we cannot give the classification accuracy here.",
                "Thus we do not discuss the effects of classification accuracy to NED performance in the paper. 6.",
                "EXPERIMENTAL SETUP 6.1 Datasets We used two LDC [18] datasets TDT2 and TDT3 for our experiments.",
                "TDT2 contains news stories from January to June 1998.",
                "It contains around 54,000 stories from sources like ABC, Associated Press, CNN, New York Times, Public Radio International, Voice of America etc.",
                "Only English stories in the collection were considered.",
                "TDT3 contains approximately 31,000 English stories collected from October to December 1998.",
                "In addition to the sources used in TDT2, it also contains stories from NBC and MSNBC TV broadcasts.",
                "We used transcribed versions of the TV and radio broadcasts besides textual news.",
                "TDT2 dataset is labeled with about 100 topics, and approximately 12,000 English stories belong to at least one of these topics.",
                "TDT3 dataset is labeled with about 120 topics, and approximately 8000 English stories belong to at least one of these topics.",
                "All the topics are classified into 11 Rules of Interpretation: (1)Elections, (2)Scandals/Hearings, (3)Legal/Criminal Cases, (4)Natural Disasters, (5)Accidents, (6)Ongoing Violence or War, (7)Science and Discovery News, (8)Finance, (9)New Law, (10)Sports News, (11)MISC.",
                "News. 6.2 Evaluation Metric TDT uses a cost function CDet that combines the probabilities of missing a new story and a false alarm [19]: * * * *Det Miss Miss Target FA FA NontargetC C P P C P P= + (12) Table 2.",
                "Average correlation between term types and news classes where CMiss means the cost of missing a new story, PMiss means the probability of missing a new story, and PTarget means the probability of seeing a new story in the data; CFA means the cost of a false alarm, PFA means the probability of a false alarm, and PNontarget means the probability of seeing an old story.",
                "The cost CDet is normalized such that a perfect system scores 0 and a trivial system, which is the better one of mark all stories as new or old, scores 1: ( ( * , * ) ) Det Det Miss Target FA Nontarget C Norm C min C P C P = (13) New event detection system gives two outputs for each story.",
                "The first part is yes or no indicating whether the story triggers a new event or not.",
                "The second part is a score indicating confidence of the first decision.",
                "Confidence scores can be used to plot DET curve, i.e., curves that plot false alarm vs. miss probabilities.",
                "Minimum normalized cost can be determined if optimal threshold on the score were chosen. 7.",
                "EXPERIMENTAL RESULTS 7.1 Main Results To test the approaches proposed in the model, we implemented and tested five systems: System-1: this system is used as baseline.",
                "It is implemented based on the basic model described in section 3, i.e., using incremental TF-IDF model to generate term weights, and using Hellinger distance to compute document similarity.",
                "Similarity score normalization is also employed [8].",
                "S-S detection procedure is used.",
                "System-2: this system is the same as system-1 except that S-C detection procedure is used.",
                "System-3: this system is the same as system-1 except that it uses the new detection procedure which is based on indexing-tree.",
                "System-4: implemented based on the approach presented in section 5.1, i.e., terms are reweighted according to the distance between term distributions in a cluster and all stories.",
                "The new detection procedure is used.",
                "System-5: implemented based on the approach presented in section 5.2, i.e., terms of different types are reweighted according to news class using trained parameters.",
                "The new detection procedure is used.",
                "The following are some other NED systems: System-6: [21] for each pair of stories, it computes three similarity values for named entity, non-named entity and all terms respectively.",
                "And employ Support Vector Machine to predict new or old using the similarity values as features.",
                "System-7: [8] it extended a basic incremental TF-IDF model to include source-specific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, etc.",
                "System-8: [13] it split document representation into two parts: named entities and non-named entities, and choose one effective part for each news class.",
                "Table 3 and table 4 show topic-weighted normalized costs and comparing times on TDT2 and TDT3 datasets respectively.",
                "Since no heldout data set for fine-tuning the threshold θ new was available for experiments on TDT2, we only report minimum normalized costs for our systems in table 3.",
                "System-5 outperforms all other systems including system-6, and it performs only 2.78e+8 comparing times in detection procedure which is only 13.4% of system-1.",
                "Table 3.",
                "NED results on TDT2 Systems Min Norm(CDet) Cmp times System-1 0.5749 2.08e+9 System-2① 0.6673 3.77e+8 System-3② 0.5765 2.81e+8 System-4② 0.5431 2.99e+8 System-5② 0.5089 2.78e+8 System-6 0.5300 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 When evaluating on the normalized costs on TDT3, we use the optimal thresholds obtained from TDT2 data set for all systems.",
                "System-2 reduces comparing times to 1.29e+9 which is just 18.3% of system-1, but at the same time it also gets a deteriorated minimum normalized cost which is 0.0499 higher than system-1.",
                "System-3 uses the new detection procedure based on news indexing-tree.",
                "It requires even less comparing times than system-2.",
                "This is because story-story comparisons usually yield greater similarities than story-cluster ones, so stories tend to be combined Location Person Date Organization Money Percentage NN JJ CD Elections 0.37 1 0.04 0.58 0.08 0.03 0.32 0.13 0.1 Scandals/Hearings 0.66 0.62 0.28 1 0.11 0.02 0.27 0.13 0.05 Legal/Criminal Cases 0.48 1 0.02 0.62 0.15 0 0.22 0.24 0.09 Natural Disasters 1 0.27 0 0.04 0.04 0 0.25 0.04 0.02 Violence or War 1 0.36 0.02 0.14 0.02 0.04 0.21 0.11 0.02 Science and Discovery 0.11 1 0.01 0.22 0.08 0.12 0.19 0.08 0.03 Finances 1 0.45 0.04 0.98 0.13 0.02 0.29 0.06 0.05 Sports 0.16 0.27 0.01 1 0.02 0 0.11 0.03 0.01 together in system-3.",
                "And system-3 is basically equivalent to system-1 in accuracy results.",
                "System-4 adjusts term weights based on the distance of term distributions between the whole corpus and cluster story set, yielding a good improvement by 0.0468 compared to system-1.",
                "The best system (system-5) has a minimum normalized cost 0.5012, which is 0.0797 better than system-1, and also better than any other results previously reported for this dataset [8, 13].",
                "Further more, system-5 only needs 1.05e+8 comparing times which is 14.9% of system-1.",
                "Table 4.",
                "NED results on TDT3 Systems Norm(CDet) Min Norm(CDet) Cmp times System-1 0.6159 0.5809 7.04e+8 System-2① 0.6493 0.6308 1.29e+8 System-3② 0.6197 0.5868 1.03e+8 System-4② 0.5601 0.5341 1.03e+8 System-5② 0.5413 0.5012 1.05e+8 System-7 -- 0.5783 -System-8 -- 0.5229 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 Figure5 shows the five DET curves for our systems on data set TDT3.",
                "System-5 achieves the minimum cost at a false alarm rate of 0.0157 and a miss rate of 0.4310.",
                "We can observe that System4 and System-5 obtain lower miss probability at regions of low false alarm probabilities.",
                "The hypothesis is that, more weight value is transferred to key terms of topics from non-key terms.",
                "Similarity score between two stories belonging to different topics are lower than before, because their overlapping terms are usually not key terms of their topics. 7.2 Parameter selection for indexing-tree detection Figure 3 shows the minimum normalized costs obtained by system-3 on TDT3 using different parameters.",
                "Theθ init parameter is tested on six values spanning from 0.03 to 0.18.",
                "And the λ parameter is tested on four values 1, 2, 3 and 4.",
                "We can see that, whenθ init is set to 0.12, which is the closest one toθ new, the costs are lower than others.",
                "This is easy to explain, because when stories belonging to the same topic are put in a cluster, it is more reasonable for the cluster to represent the stories in it.",
                "When parameter λ is set to 3 or 4, the costs are better than other cases, but there is no much difference between 3 and 4. 0 0.05 0.1 0.15 0.2 1 2 3 4 0.5 0.6 0.7 0.8 0.9 1 θ-initλ MinCost 0.6 0.65 0.7 0.75 0.8 0.85 0.9 Figure 3.",
                "Min Cost on TDT3 (δ =0.15) 0 0.05 0.1 0.15 0.2 1 2 3 4 0 0.5 1 1.5 2 2.5 x 10 8 θ-init λ Comparingtimes 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 8 Figure 4.",
                "Comparing times on TDT3 (δ =0.15) Figure 4 gives the comparing times used by system-3 on TDT3 with the same parameters as figure 3.",
                "The comparing times are strongly dependent onθ init.",
                "Because the greaterθ init is, the less stories combined together, the more comparing times are needed for new event decision.",
                "So we useθ init =0.13,λ =3,δ =0.15 for system-3, 4, and 5.",
                "In this parameter setting, we can get both low minimum normalized costs and less comparing times. 8.",
                "CONCLUSION We have proposed a news indexing-tree based detection procedure in our model.",
                "It reduces comparing times to about one seventh of traditional method without hurting NED accuracy.",
                "We also have presented two extensions to the basic TF-IDF model.",
                "The first extension is made by adjust term weights based on term distributions between the whole corpus and a cluster story set.",
                "And the second extension to basic TF-IDF model is better use of term types (named entities types and part-of-speed) according to news categories.",
                "Our experimental results on TDT2 and TDT3 datasets show that both of the two extensions contribute significantly to improvement in accuracy.",
                "We did not consider news time information as a clue for NED task, since most of the topics last for a long time and TDT data sets only span for a relative short period (no more than 6 months).",
                "For the future work, we want to collect news set which span for a longer period from internet, and integrate time information in NED task.",
                "Since topic is a relative coarse-grained news cluster, we also want to refine cluster granularity to event-level, and identify different events and their relations within a topic.",
                "Acknowledgments This work is supported by the National Natural Science Foundation of China under Grant No. 90604025.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsor. 9.",
                "REFERENCES [1] http://www.nist.gov/speech/tests/tdt/index.htm [2] In Topic Detection and Tracking.",
                "Event-based Information Organization.",
                "Kluwer Academic Publishers, 2002. .01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90 1 2 5 10 20 40 60 80 90 False Alarm Probability (in %) MissProbability(in%) SYSTEM1 Topic Weighted Curve SYSTEM1 Min Norm(Cost) SYSTEM2 Topic Weighted Curve SYSTEM2 Min Norm(Cost) SYSTEM3 Topic Weighted Curve SYSTEM3 Min Norm(Cost) SYSTEM4 Topic Weighted Curve SYSTEM4 Min Norm(Cost) SYSTEM5 Topic Weighted Curve SYSTEM5 Min Norm(Cost) Random Performance Figure 5.",
                "DET curves on TDT3 [3] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B.T.",
                "Archibald, and X. Liu.",
                "Learning Approaches for Detecting and Tracking News Events.",
                "In IEEE Intelligent Systems Special Issue on Applications of Intelligent Information Retrieval, volume 14 (4), 1999, 32-43. [4] Y. Yang, T. Pierce, and J. Carbonell.",
                "A Study on Retrospective and On-line Event Detection.",
                "In Proceedings of SIGIR-98, Melbourne, Australia, 1998, 28-36. [5] J. Allan, V. Lavrenko, D. Malin, and R. Swan.",
                "Detections, Bounds, and Timelines: Umass and tdt-3.",
                "In Proceedings of Topic Detection and Tracking Workshop (TDT-3), Vienna, VA, 2000, 167-174. [6] R. Papka and J. Allan.",
                "On-line New Event Detection Using Single Pass Clustering TITLE2:.",
                "Technical Report UM-CS1998-021, 1998. [7] W. Lam, H. Meng, K. Wong, and J.",
                "Yen.",
                "Using Contextual Analysis for News Event Detection.",
                "International Journal on Intelligent Systems, 2001, 525-546. [8] B. Thorsten, C. Francine, and F. Ayman.",
                "A System for New Event Detection.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2003, 330-337. [9] S. Nicola and C. Joe.",
                "Combining Semantic and Syntactic Document Classifiers to Improve First Story Detection.",
                "In Proceedings of the 24th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topicconditioned Novelty Detection.",
                "In Proceedings of the 8th ACM SIGKDD International Conference, ACM Press. 2002, 688-693. [11] M. Juha, A.M. Helena, and S. Marko.",
                "Applying Semantic Classes in Event Detection and Tracking.",
                "In Proceedings of International Conference on Natural Language Processing (ICON 2002), 2002, pages 175-183. [12] M. Juha, A.M. Helena, and S. Marko.",
                "Simple Semantics in Topic Detection and Tracking.",
                "Information Retrieval, 7(3-4): 2004, 347-368. [13] K. Giridhar and J. Allan.",
                "Text Classification and Named Entities for New Event Detection.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2004, 297-304. [14] J. P. Callan, W. B. Croft, and S. M. Harding.",
                "The INQUERY Retrieval System.",
                "In Proceedings of DEXA-92, 3rd International Conference on Database and Expert Systems Applications, 1992, 78-83. [15] R. Krovetz.",
                "Viewing Morphology as An Inference Process.",
                "In Proceedings of ACM SIGIR93, 1993, 61-81. [16] Y. Yang and J. Pedersen.",
                "A Comparative Study on Feature Selection in Text Categorization.",
                "In J. D. H. Fisher, editor, The Fourteenth International Conference on Machine Learning (ICML97), Morgan Kaufmann, 1997, 412-420. [17] T. M. Cover, and J.A.",
                "Thomas.",
                "Elements of Information Theory.",
                "Wiley. 1991. [18] The linguistic data consortium, http://www.ldc,upenn.edu/. [19] The 2001 TDT task definition and evaluation plan, http://www.nist.gov/speech/tests/tdt/tdt2001/evalplan.htm. [20] R. E. Schapire and Y.",
                "Singer.",
                "Boostexter: A Boosting-based System for Text Categorization.",
                "In Machine Learning 39(2/3):1, Kluwer Academic Publishers, 2000, 35-168. [21] K. Giridhar and J. Allan. 2005.",
                "Using Names and Topics for New Event Detection.",
                "In Proceedings of Human Technology Conference and Conference on Empirical Methods in Natural Language, Vancouver, 2005, 121-128"
            ],
            "original_annotated_samples": [
                "Experimental results on two Linguistic Data Consortium (LDC) datasets TDT2 and TDT3 show that the proposed model can improve both efficiency and accuracy of NED task significantly, compared to the <br>baseline system</br> and other existing systems."
            ],
            "translated_annotated_samples": [
                "Los resultados experimentales en dos conjuntos de datos del Consorcio de Datos Lingüísticos (LDC), TDT2 y TDT3, muestran que el modelo propuesto puede mejorar significativamente tanto la eficiencia como la precisión de la tarea de NED, en comparación con el <br>sistema base</br> y otros sistemas existentes."
            ],
            "translated_text": "Detección de nuevos eventos basada en árbol de indexación y entidad nombrada Zhang Kuo Universidad Tsinghua Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Universidad Tsinghua Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Universidad Tsinghua Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn RESUMEN La Detección de Nuevos Eventos (NED) tiene como objetivo detectar de una o varias corrientes de noticias cuál se informa sobre un nuevo evento (es decir, no reportado previamente). Con el abrumador volumen de noticias disponible hoy en día, hay una creciente necesidad de un sistema NED que sea capaz de detectar eventos nuevos de manera más eficiente y precisa. En este artículo proponemos un nuevo modelo de NED para acelerar la tarea de NED mediante el uso dinámico de un árbol de indexación de noticias. Además, basándose en la observación de que los términos de diferentes tipos tienen diferentes efectos para la tarea de NED, se proponen dos enfoques de reponderación de términos para mejorar la precisión de NED. En el primer enfoque, proponemos ajustar dinámicamente los pesos de los términos basados en los clusters de historias previas y en el segundo enfoque, proponemos emplear estadísticas en los datos de entrenamiento para aprender el modelo de reajuste de entidades nombradas para cada clase de historias. Los resultados experimentales en dos conjuntos de datos del Consorcio de Datos Lingüísticos (LDC), TDT2 y TDT3, muestran que el modelo propuesto puede mejorar significativamente tanto la eficiencia como la precisión de la tarea de NED, en comparación con el <br>sistema base</br> y otros sistemas existentes. Categorías y Descriptores de Asignaturas H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información; H.4.2 [Aplicaciones de Sistemas de Información]: Tipos de Sistemas de Apoyo a Decisiones. Términos generales Algoritmos, Rendimiento, Experimentación 1. El programa de Detección y Seguimiento de Temas (TDT) tiene como objetivo desarrollar técnicas que puedan organizar, buscar y estructurar de manera efectiva materiales de texto de noticias de una variedad de agencias de noticias y medios de difusión [1]. La Detección de Eventos Nuevos (DEN) es una de las cinco tareas en TDT. Es la tarea de identificación en línea del informe más temprano para cada tema tan pronto como ese informe llegue en la secuencia de documentos. Un tema se define como un evento o actividad seminal, junto con eventos y actividades directamente relacionados [2]. Un evento se define como algo (no trivial) que sucede en un lugar y en un momento determinados [3]. Por ejemplo, cuando una bomba explota en un edificio, la explosión es el evento seminal que desencadena el tema, y otras historias sobre el mismo tema serían aquellas que discuten los esfuerzos de salvamento, la búsqueda de los perpetradores, arrestos y juicios, entre otros. La información útil de noticias suele estar enterrada en una masa de datos generados a diario. Por lo tanto, los sistemas NED son muy útiles para las personas que necesitan detectar información novedosa de un flujo de noticias en tiempo real. Estas necesidades de la vida real a menudo se presentan en ámbitos como los mercados financieros, el análisis de noticias y la recopilación de inteligencia. En la mayoría de los sistemas NED de última generación, cada noticia disponible se compara con todas las noticias previamente recibidas. Si todas las similitudes entre ellos no superan un umbral, entonces la historia desencadena un nuevo evento. Por lo general, se presentan en forma de similitud de coseno o métrica de similitud de Hellinger. El problema central de NED es identificar si dos historias tratan sobre el mismo tema. Obviamente, estos sistemas no pueden aprovechar la información del tema. Además, no es aceptable en aplicaciones reales debido a la gran cantidad de cálculos requeridos en el proceso de NED. Otros sistemas organizan historias anteriores en grupos (cada grupo corresponde a un tema), y la nueva historia se compara con los grupos anteriores en lugar de con historias. Esta forma puede reducir significativamente los tiempos de comparación. Sin embargo, se ha demostrado que este método es menos preciso [4, 5]. Esto se debe a que a veces las historias dentro de un tema se alejan mucho entre sí, lo que podría resultar en una baja similitud entre una historia y su tema. Por otro lado, algunos sistemas NED propuestos intentaron mejorar la precisión al hacer un mejor uso de las entidades nombradas [10, 11, 12, 13]. Sin embargo, ninguno de los sistemas ha considerado que los términos de diferentes tipos (por ejemplo, Los sustantivos, verbos o nombres de personas tienen diferentes efectos en diferentes clases de historias para determinar si dos historias tratan sobre el mismo tema. Por ejemplo, los nombres de los candidatos electorales (Nombre de la persona) son muy importantes para las historias de la clase de elecciones; las ubicaciones (Nombre de la ubicación) donde ocurrieron los accidentes son importantes para las historias de la clase de accidentes. Por lo tanto, en NED, todavía existen los siguientes tres problemas por investigar: (1) ¿Cómo acelerar el procedimiento de detección sin disminuir la precisión de la detección? (2) ¿Cómo aprovechar mejor la información de los grupos (temas) para mejorar la precisión? (3) ¿Cómo obtener una mejor representación de historias de noticias mediante una mejor comprensión de las entidades nombradas? Impulsados por estos problemas, hemos propuesto tres enfoques en este documento. (1) Para hacer que el procedimiento de detección sea más rápido, proponemos un nuevo procedimiento de NED basado en un árbol de indexación de noticias creado dinámicamente. El índice de historias en árbol se crea ensamblando historias similares para formar grupos de noticias en diferentes jerarquías según sus valores de similitud. Las comparaciones entre la historia actual y los grupos anteriores podrían ayudar a encontrar la historia más similar en menos tiempo de comparación. El nuevo procedimiento puede reducir la cantidad de veces de comparación sin afectar la precisión. Utilizamos los grupos del primer piso en el árbol de indexación como temas de noticias, en los cuales los pesos de los términos se ajustan dinámicamente según la distribución de términos en los grupos. En este enfoque, se utiliza adecuadamente la información de los grupos (temas), por lo que se evita el problema de la descentralización del tema. Basándonos en observaciones de las estadísticas obtenidas de los datos de entrenamiento, encontramos que los términos de diferentes tipos (por ejemplo, Los sustantivos y verbos tienen diferentes efectos para diferentes clases de historias al determinar si dos historias tratan sobre el mismo tema. Y proponemos utilizar estadísticas para optimizar los pesos de los términos de diferentes tipos en una historia de acuerdo a la clase de noticias a la que pertenece la historia. En el conjunto de datos TDT3, el nuevo modelo NED solo utiliza un 14.9% en comparación con los tiempos del modelo básico, mientras que su costo normalizado mínimo es de 0.5012, lo que es 0.0797 mejor que el modelo básico, y también mejor que cualquier otro resultado previamente reportado para este conjunto de datos [8, 13]. El resto del documento está organizado de la siguiente manera. Comenzamos este artículo resumiendo el trabajo previo en NED en la sección 2. La sección 3 presenta el modelo básico para NED que la mayoría de los sistemas actuales utilizan. La sección 4 describe nuestro nuevo procedimiento de detección basado en el índice de árbol de noticias. En la sección 5, se proponen dos métodos de reponderación de términos para mejorar la precisión de NED. La sección 6 presenta nuestros datos experimentales y métricas de evaluación. Finalmente concluimos con los resultados experimentales en la Sección 7, y las conclusiones y trabajo futuro en la Sección 8. TRABAJO RELACIONADO Papka et al. propusieron el agrupamiento de un solo paso en NED [6]. Cuando se encontraba una nueva historia, esta era procesada inmediatamente para extraer características de términos y se construía una representación de consulta del contenido de la historia. Luego se comparó con todas las consultas anteriores. Si el documento no generaba ninguna consulta al exceder un umbral, se marcaba como un nuevo evento. Lam et al construyen representaciones de consultas previas de grupos de historias, cada una de las cuales corresponde a un tema [7]. De esta manera se realizan comparaciones entre historias y grupos. En los últimos años, la mayoría de los trabajos se han centrado en proponer mejores métodos para la comparación de historias y la representación de documentos. Brants et al. [8] extendieron un modelo básico incremental de TF-IDF para incluir modelos específicos de fuente, normalización de puntajes de similitud basada en promedios específicos de documentos, normalización de puntajes de similitud basada en promedios específicos de pares de fuentes, reponderación de términos basada en frecuencias de eventos inversas y segmentación de documentos. Se mostraron buenas mejoras en los puntos de referencia de TDT. Stokes et al. [9] utilizaron una combinación de evidencia de dos representaciones distintas del contenido de un documento. Una de las representaciones era el vector de texto libre habitual, la otra hacía uso de cadenas léxicas (creadas utilizando WordNet) para construir otro vector de términos. Luego, las dos representaciones se combinan de forma lineal. Se logró un aumento marginal en la efectividad cuando se utilizó la representación combinada. Algunos esfuerzos se han realizado sobre cómo utilizar entidades nombradas para mejorar la desambiguación de entidades nombradas (NED). Yang et al. asignaron un peso cuatro veces mayor a las entidades con nombres de ubicación que a otros términos y entidades con nombres [10]. El grupo de investigación DOREMI combinó similitudes semánticas de nombres de personas, nombres de lugares y tiempo junto con similitud textual [11][12]. El grupo de investigación de UMass [13] dividió la representación del documento en dos partes: entidades nombradas y entidades no nombradas. Y se descubrió que algunas clases de noticias podían lograr un mejor rendimiento utilizando la representación de entidades nombradas, mientras que otras clases de noticias podían lograr un mejor rendimiento utilizando la representación sin entidades nombradas. Tanto [10] como [13] utilizaron la técnica de categorización de texto para clasificar previamente las noticias. En [13] las noticias se clasifican automáticamente al principio, y luego se prueban las sensibilidades de los nombres y términos que no son nombres para NED para cada clase. En [10], se eliminan los términos frecuentes de cada clase de la representación del documento. Por ejemplo, la palabra \"elección\" no ayuda a identificar diferentes elecciones. En su trabajo, no se investiga la efectividad de diferentes tipos de nombres (o términos con diferentes categorías gramaticales) para NED en diferentes clases de noticias. Utilizamos análisis estadístico para revelar el hecho y utilizarlo para mejorar el rendimiento de NED. 3. MODELO BÁSICO En esta sección, presentamos el modelo básico de Detección de Nuevos Eventos que es similar a lo que la mayoría de los sistemas actuales aplican. Luego, proponemos nuestro nuevo modelo ampliando el modelo básico. Los sistemas de detección de eventos nuevos utilizan flujos de noticias como entrada, en los cuales las historias están estrictamente ordenadas por tiempo. Solo están disponibles las historias recibidas previamente al tratar con la historia actual. La salida es una decisión sobre si la historia actual es un evento nuevo o no y la confianza de la decisión. Por lo general, un modelo NED consta de tres partes: representación de la historia, cálculo de similitud y procedimiento de detección. 3.1 Representación de la historia Se necesita preprocesamiento antes de generar la representación de la historia. Para el preprocesamiento, tokenizamos palabras, reconocemos abreviaturas, normalizamos abreviaturas, agregamos etiquetas de partes del discurso, eliminamos palabras vacías incluidas en la lista de paradas utilizada en InQuery [14], reemplazamos palabras con sus raíces utilizando el algoritmo K-stem[15], y luego generamos un vector de palabras para cada noticia. Utilizamos el modelo TF-IDF incremental para el cálculo del peso de los términos [4]. En un modelo TF-IDF, la frecuencia de término en un documento de noticias se pondera por la frecuencia inversa del documento, la cual se genera a partir del corpus de entrenamiento. Cuando surge un nuevo término en el proceso de prueba, hay dos soluciones: simplemente ignorar el nuevo término o establecer df del término como una constante pequeña (por ejemplo, df = 1). El nuevo término recibe un peso demasiado bajo en la primera solución (0) y un peso demasiado alto en la segunda solución. En el modelo TF-IDF incremental, las frecuencias de documentos se actualizan dinámicamente en cada paso de tiempo t: 1( ) ( ) ( )t t D tdf w df w df w−= + (1) donde Dt representa el conjunto de noticias recibidas en el tiempo t, y dfDt(w) significa el número de documentos en los que aparece el término w, y dft(w) significa el número total de documentos en los que aparece el término w antes del tiempo t. En este trabajo, cada ventana de tiempo incluye 50 noticias. Así, cada historia d recibida en t se representa de la siguiente manera: 1 2{ ( , , ), ( , , ),..., ( , , )}nd peso d t w peso d t w peso d t w→ donde n significa el número de términos distintos en la historia d, y ( , , )peso d t w significa el peso del término w en la historia d en el tiempo t: log( ( , ) 1) log(( 1) /( ( ) 0.5)) ( , , ) log( ( , ) 1) log(( 1) /( ( ) 0.5)) t t t t w d tf d w N df w peso d t w tf d w N df w ∈ + + + = + + +∑ (2) donde Nt significa el número total de historias de noticias antes del tiempo t, y tf(d,w) significa cuántas veces aparece el término w en la historia de noticias d. 3.2 Cálculo de Similitud Utilizamos la distancia de Hellinger para el cálculo de la similitud entre dos historias, para dos historias d y d en el tiempo t, su similitud se define de la siguiente manera: , ( , , ) ( , , ) * ( , , ) w d d sim d d t peso d t w peso d t w ∈ = ∑ (3) 3.3 Procedimiento de Detección Para cada historia d recibida en el paso de tiempo t, el valor ( ) ( ) ( ) ( ( , , )) tiempo d tiempo d n d max sim d d t < = (4) es un puntaje utilizado para determinar si d es una historia sobre un nuevo tema y al mismo tiempo es una indicación de la confianza en nuestra decisión [8]. tiempo(d) significa el tiempo de publicación de la historia d. Si el puntaje supera el umbral θ new, entonces existe un documento suficientemente similar, por lo tanto, d es una historia antigua, de lo contrario, no hay un documento previo suficientemente similar, por lo tanto, d es una historia nueva. 4. Nuevo procedimiento NED Los sistemas NED tradicionales se pueden clasificar en dos tipos principales en cuanto al procedimiento de detección: (1) tipo S-S, en el que la historia en cuestión se compara con cada historia recibida previamente, y se utiliza la mayor similitud para determinar si la historia actual trata sobre un evento nuevo; (2) tipo S-C, en el que la historia en cuestión se compara con todos los clusters anteriores, cada uno de los cuales representa un tema, y se utiliza la mayor similitud para la decisión final sobre la historia actual. Si la similitud más alta supera el umbral θ nuevo, entonces es una historia antigua y se coloca en el clúster más similar; de lo contrario, es una historia nueva y se crea un nuevo clúster. Trabajos anteriores muestran que la primera forma es más precisa que la segunda [4][5]. Dado que a veces las historias dentro de un tema se alejan mucho entre sí, una historia puede tener una similitud muy baja con su tema. Por lo tanto, utilizar similitudes entre historias para determinar una nueva historia es mejor que utilizar similitudes entre historias y grupos. Sin embargo, el primer método requiere mucho más tiempo de comparación, lo que significa que es menos eficiente. Proponemos un nuevo procedimiento de detección que utiliza comparaciones con clusters anteriores para ayudar a encontrar la historia más similar en menos tiempo de comparación, y la decisión final sobre el nuevo evento se toma de acuerdo con la historia más similar. Por lo tanto, podemos obtener tanto la precisión de los métodos de tipo S-S como la eficiencia de los métodos de tipo S-C. El nuevo procedimiento crea un árbol de indexación de noticias dinámicamente, en el cual se agrupan historias similares para formar una jerarquía de clusters. Indexamos historias similares juntas por su ancestro común (un nodo de clúster). Historias diferentes están indexadas en diferentes grupos. Cuando una historia está llegando, utilizamos comparaciones entre la historia actual y los grupos jerárquicos anteriores para ayudar a encontrar la historia más similar que es útil para la toma de decisiones sobre nuevos eventos. Después de que se tome la decisión sobre el nuevo evento, la historia actual se inserta en el árbol de indexación para la detección siguiente. El índice de noticias en forma de árbol se define formalmente de la siguiente manera: S-Tree = {r, NC, NS, E}, donde r es la raíz de S-Tree, NC es el conjunto de todos los nodos de clúster, NS es el conjunto de todos los nodos de historia, y E es el conjunto de todos los bordes en S-Tree. Definimos un conjunto de restricciones para un S-Árbol: ⅰ. , es un nodo no terminal en el árbolC i i N i∀ ∈ → ⅱ. , es un nodo terminal en el árbolS i i N i∀ ∈ → ⅲ. , el grado de salida de es al menos 2C i i N i∀ ∈ → ⅳ. , se representa como el centroide de sus descendientesC i i iN∀ ∈ → Para una noticia di, el procedimiento de comparación y el procedimiento de inserción basados en el árbol de indexación se definen de la siguiente manera. Un ejemplo se muestra en la Figura 1 y la Figura 2. Figura 1. Procedimiento de comparación Figura 2. Procedimiento de comparación de inserción: Paso 1: comparar di con todos los nodos hijos directos de r y seleccionar λ nodos con las similitudes más altas, por ejemplo, C1 2 y C1 3 en la Figura 1. Paso 2: para cada nodo seleccionado en el paso anterior, por ejemplo. C1 2, compara di con todos sus nodos hijos directos y selecciona λ nodos con las similitudes más altas, por ejemplo. C2 2 y d8. Repetir el paso 2 para todos los nodos no terminales. Paso 3: registrar el nodo terminal con la mayor similitud a di, por ejemplo, s5, y el valor de similitud (0.20). Insertando di en el S-árbol con r como raíz: Encuentra el nodo n que es hijo directo de r en el camino desde r hasta el nodo terminal con la similitud más alta s, por ejemplo. C1 2. Si s es menor que θ init + (h-1)δ, entonces agregar di al árbol como un hijo directo de r. De lo contrario, si n es un nodo terminal, crear un nodo de clúster en lugar de n, y agregar tanto n como di como sus hijos directos; si n es un nodo no terminal, repetir este procedimiento e insertar di en el subárbol con n como raíz de forma recursiva. Aquí h es la longitud entre n y la raíz del árbol S. Cuanto más similares sean las historias en un grupo, mejor representará el grupo a las historias en él. Por lo tanto, no imponemos restricciones en la altura máxima de los árboles y el grado de un nodo. Por lo tanto, no podemos dar la complejidad de este procedimiento basado en árboles de indexación. Pero proporcionaremos el número de veces de comparación necesarias por el nuevo procedimiento en nuestros experimentos en la sección 7.5. En esta sección, se proponen dos métodos de reponderación de términos para mejorar la precisión de NED. En el primer método, se explora una nueva forma de utilizar de manera más efectiva la información de los grupos (temas). El segundo encuentra una mejor manera de utilizar entidades nombradas basadas en la clasificación de noticias. 5.1 Reajuste de términos basado en la distancia de distribución TF-IDF es el modelo más prevalente utilizado en sistemas de recuperación de información. La idea básica es que cuanto menos documentos aparezca un término, más importante es el término en la discriminación de documentos (relevantes o no relevantes para una consulta que contenga el término). Sin embargo, en el dominio de TDT, necesitamos discriminar documentos en función de los temas en lugar de las consultas. Intuitivamente, el uso de vectores de clúster (tema) para comparar con historias de noticias posteriores debería tener un mejor rendimiento que el uso de vectores de historias. Desafortunadamente, los resultados experimentales no respaldan esta intuición [4][5]. Basándonos en la observación de los datos, encontramos que la razón es que un tema de noticias generalmente contiene muchos eventos directa o indirectamente relacionados, y todos ellos tienen sus propios subtemas que suelen ser diferentes entre sí. Tomemos el tema descrito en la sección 1 como ejemplo, eventos como la explosión y el rescate tienen similitudes muy bajas con eventos sobre juicios criminales, por lo tanto, las historias sobre juicios tendrían una baja similitud con el vector de tema construido en base a sus eventos previos. Esta sección se centra en cómo hacer un uso efectivo de la información del tema y al mismo tiempo evitar el problema de la descentralización del contenido. Al principio, clasificamos los términos en 5 clases para ayudar en el análisis de las necesidades del modelo modificado: Clase de términos A: términos que ocurren con frecuencia en todo el corpus, por ejemplo, año y personas. Los términos de esta clase deben recibir pesos bajos porque no ayudan mucho en la discriminación de temas. Clase B de términos: términos que ocurren con frecuencia dentro de una categoría de noticias, por ejemplo, elección, tormenta. Son útiles para distinguir dos historias en diferentes categorías de noticias. Sin embargo, no pueden proporcionar información para determinar si dos historias tratan sobre el mismo tema o temas diferentes. En otras palabras, los términos elección y tormenta no son útiles para diferenciar dos campañas electorales y dos desastres de tormenta. Por lo tanto, los términos de esta clase deberían asignarse pesos más bajos. Clase de término C: términos que ocurren con frecuencia en un tema y raramente en otros temas, por ejemplo, el nombre de un avión accidentado, el nombre de un huracán específico. Las noticias que pertenecen a diferentes temas rara vez tienen términos superpuestos en esta clase. Cuanto más frecuentemente aparezca un término en un tema, más importante es el término para una historia perteneciente al tema, por lo tanto, el término debería tener un peso más alto. Clase de término D: términos que aparecen en un tema de manera exclusiva, pero no con frecuencia. Por ejemplo, el nombre de un bombero que se desempeñó muy bien en una acción de salvamento, que puede aparecer en solo dos o tres historias pero nunca ha aparecido en otros temas. Los términos de este tipo deberían recibir más peso que en el modelo TF-IDF. Sin embargo, dado que no son populares en el tema, no es apropiado asignarles pesos demasiado altos. Clase de término E: términos con baja frecuencia en documentos y que aparecen en diferentes temas. Los términos de esta clase deberían recibir pesos más bajos. Ahora analizamos si el modelo TF-IDF puede asignar pesos adecuados a las cinco clases de términos. Obviamente, los términos de la clase A tienen un peso bajo en el modelo TF-IDF, lo cual es acorde con el requisito descrito anteriormente. En el modelo TF-IDF, los términos de la clase B dependen altamente del número de historias en una clase de noticias. El modelo TF-IDF no puede proporcionar pesos bajos si la historia que contiene el término pertenece a una clase de noticias relativamente pequeña. Para un término de clase C, cuanto más frecuentemente aparezca en un tema, menos peso le da el modelo TFIDF. Esto entra en conflicto directo con el requisito de términos en la clase C. Para los términos de la clase D, el modelo TF-IDF les asigna pesos altos de manera correcta. Pero para los términos de la clase E, el modelo TF-IDF asigna pesos altos a ellos que no son conformes con el requisito de pesos bajos. En resumen, los términos de clase B, C, E no pueden ser ponderados adecuadamente en el modelo TF-IDF. Por lo tanto, proponemos un modelo modificado para resolver este problema. Cuando θ init y θ new están configurados de cerca, asumimos que la mayoría de las historias en un clúster de primer nivel (un nodo hijo directo del nodo raíz) están en el mismo tema. Por lo tanto, utilizamos un clúster de primer nivel para capturar la distribución de términos (df para todos los términos dentro del clúster) dentro del tema de forma dinámica. La divergencia de Kullback-Leibler de la distribución de términos en un clúster de primer nivel y el conjunto completo de historias se utiliza para ajustar los pesos de los términos: ( , , ) * (1 * ( || )) ( , , ) ( , , ) * (1 * ( || )) cw tw cw tw w d D peso d t w KL P P peso d t w peso d t w KL P P γ γ ∈ + = +∑ (5) donde ( ) ( ) ( ) ( ) 1,cw cw c c c c df w df w p y p y N N = = − (6) ( ) ( ) ( ) ( ) 1,t t tw tw t t df w df w p y p y N N = = − (7) donde dfc(w) es el número de documentos que contienen el término w dentro del clúster C, y Nc es el número de documentos en el clúster C, y Nt es el número total de documentos que llegan antes del paso de tiempo t. γ es un parámetro constante, actualmente configurado manualmente en 3. La divergencia de Kullback-Leibler se define de la siguiente manera [17]: La idea básica es que, para una historia en un tema, cuanto más ocurre un término dentro del tema y menos ocurre en otros temas, se le deben asignar pesos más altos. Obviamente, el modelo modificado puede cumplir con todos los requisitos de las cinco clases de términos mencionadas anteriormente. 5.2 Reajuste de Términos Basado en el Tipo de Término y la Clase de Historia. Trabajos anteriores encontraron que algunas clases de historias de noticias podrían lograr mejoras significativas al dar un peso adicional a las entidades nombradas. Pero encontramos que los términos de diferentes tipos deben recibir una cantidad diferente de peso adicional para diferentes clases de noticias. Utilizamos open-NLP1 para reconocer tipos de entidades nombradas y etiquetas de partes del discurso para términos que aparecen en noticias. Los tipos de entidades nombradas incluyen nombre de persona, nombre de organización, nombre de ubicación, fecha, hora, dinero y porcentaje, y se seleccionan cinco partes del discurso: ninguno (NN), verbo (VB), adjetivo (JJ), adverbio (RB) y número cardinal (CD). El análisis estadístico muestra tipos de términos discriminativos a nivel de tema para diferentes clases de historias. Por conveniencia, los tipos de entidad nombrada y etiquetas de parte de la oración se llaman uniformemente tipo de término en las secciones siguientes. Determinar si dos historias tratan sobre el mismo tema es un componente básico para la tarea de NED. Por lo tanto, al principio usamos la estadística χ² para calcular las correlaciones entre términos y temas. Para un término t y un tema T, se deriva una tabla de contingencia: Tabla 1. Una tabla de contingencia de 2×2 con el número de documentos pertenecientes al tema T y no pertenecientes al tema T incluye t A B no incluye t C D. La estadística χ² para un término específico t con respecto al tema T se define como [16]: χ² = ( ) ( ) * ( ) ( ) * ( ) * ( ) * ( ) w T A B C D AD CB A C B D A B C D + + + − + + + + (9). Los temas de noticias para la tarea TDT se clasifican además en 11 reglas de interpretaciones (ROIs) 2. El ROI se puede ver como una clase de historias de nivel superior. El promedio de correlación entre un tipo de término y un ROI de tema se calcula como: 2 avg 2 ( , )( ( , ) )k m m km kT R w P w TP R p w T R P χ χ ∈ ∈ ∑ ∑（ , ）= 1 1 k=1…K, m=1…M (10) donde K es el número de tipos de términos (fijado constantemente en 12 en el artículo). M es el número de nuevas clases (ROIs, establecido en el conjunto 11 del artículo). Pk representa el conjunto de todos los términos de tipo k, y Rm representa el conjunto de todos los temas de la clase m, p(t,T) significa la probabilidad de que t ocurra en el tema T. Debido a limitaciones de espacio, solo se enumeran partes de los tipos de términos (9 tipos de términos) y partes de las clases de noticias (8 clases) en la tabla 2 con los valores promedio de correlación entre ellos. Las estadísticas se derivan de datos etiquetados en el corpus TDT2. (Los resultados en la tabla 2 ya están normalizados para mayor comodidad en la comparación). Las estadísticas en la tabla 2 indican la utilidad de diferentes tipos de términos en la discriminación de temas con respecto a diferentes clases de noticias. Podemos ver que el nombre de la ubicación es el tipo de término más útil para tres clases de noticias: Desastres Naturales, Violencia o Guerra, Finanzas. Y para otras tres categorías: Elecciones, Casos Legales/Criminales, Ciencia y Descubrimiento, el nombre de la persona es el tipo de término más discriminativo. Para Escándalos/Audiencias, la fecha es la información más importante para la discriminación de temas. Además, los casos legales/criminales y los temas financieros tienen una correlación más alta con términos relacionados con el dinero, mientras que la ciencia y el descubrimiento tienen una correlación más alta con términos de porcentaje. Los términos no nominales son más estables para diferentes clases. 1. http://opennlp.sourceforge.net/ 2. http://projects.ldc.upenn.edu/TDT3/Guide/label.html A partir del análisis de la tabla 2, es razonable ajustar el peso de los términos según su tipo de término y la clase de noticias a la que pertenece la historia. Los nuevos pesos de los términos se recalculan de la siguiente manera: ( ) ( ) ( ) ( ) ( , , ) * ( , , ) ( , , ) * clase d D tipo w T clase d D tipo w w d peso d t w peso d t w peso d t w α α ∈ = ∑ (11) donde type(w) representa el tipo del término w, y class(d) representa la clase de la historia d, c kα es el parámetro de recalibración para la clase de noticias c y el tipo de término k. En el trabajo, simplemente utilizamos las estadísticas de la tabla 2 como parámetros de recalibración. Aunque usar las estadísticas directamente puede que no sea la mejor opción, no discutimos cómo obtener automáticamente los mejores parámetros. Intentaremos utilizar técnicas de aprendizaje automático para obtener los mejores parámetros en el trabajo futuro. En el trabajo, utilizamos BoosTexter [20] para clasificar todas las historias en uno de los 11 ROIs. BoosTexter es un programa de aprendizaje automático basado en boosting, que crea una serie de reglas simples para construir un clasificador de datos de texto o atributos-valor. Utilizamos el peso del término generado utilizando el modelo TF-IDF como característica para la clasificación de historias. Entrenamos el modelo con las 12000 historias en inglés evaluadas en TDT2, y clasificamos el resto de las historias en TDT2 y todas las historias en TDT3. Los resultados de clasificación se utilizan para la reponderación de términos en la fórmula (11). Dado que las etiquetas de clase de las historias de tema apagado no se proporcionan en los conjuntos de datos de TDT, no podemos proporcionar la precisión de clasificación aquí. Por lo tanto, no discutimos los efectos de la precisión de la clasificación en el rendimiento de NED en el artículo. 6. CONFIGURACIÓN EXPERIMENTAL 6.1 Conjuntos de datos Utilizamos dos conjuntos de datos de LDC [18], TDT2 y TDT3, para nuestros experimentos. TDT2 contiene noticias desde enero hasta junio de 1998. Contiene alrededor de 54,000 historias de fuentes como ABC, Associated Press, CNN, New York Times, Public Radio International, Voice of America, etc. Solo se consideraron historias en inglés en la colección. TDT3 contiene aproximadamente 31,000 historias en inglés recopiladas de octubre a diciembre de 1998. Además de las fuentes utilizadas en TDT2, también contiene historias de las transmisiones de televisión de NBC y MSNBC. Utilizamos versiones transcritas de las emisiones de televisión y radio además de noticias textuales. El conjunto de datos TDT2 está etiquetado con alrededor de 100 temas, y aproximadamente 12,000 historias en inglés pertenecen al menos a uno de estos temas. El conjunto de datos TDT3 está etiquetado con alrededor de 120 temas, y aproximadamente 8000 historias en inglés pertenecen al menos a uno de estos temas. Todos los temas están clasificados en 11 Reglas de Interpretación: (1) Elecciones, (2) Escándalos/Audiencias, (3) Casos Legales/Criminales, (4) Desastres Naturales, (5) Accidentes, (6) Violencia o Guerra en Curso, (7) Noticias de Ciencia y Descubrimientos, (8) Finanzas, (9) Nueva Ley, (10) Noticias Deportivas, (11) Varios. Noticias. 6.2 Métrica de evaluación TDT utiliza una función de costo CDet que combina las probabilidades de perder una nueva historia y una falsa alarma [19]: * * * *Det Miss Miss Target FA FA NontargetC C P P C P P= + (12) Tabla 2. Correlación promedio entre tipos de términos y clases de noticias donde CMiss significa el costo de perder una nueva historia, PMiss significa la probabilidad de perder una nueva historia, y PTarget significa la probabilidad de ver una nueva historia en los datos; CFA significa el costo de una falsa alarma, PFA significa la probabilidad de una falsa alarma, y PNontarget significa la probabilidad de ver una historia antigua. El costo CDet se normaliza de tal manera que un sistema perfecto obtiene una puntuación de 0 y un sistema trivial, que es el mejor de marcar todas las historias como nuevas o antiguas, obtiene una puntuación de 1: ( ( * , * ) ) Det Det Miss Target FA Nontarget C Norm C min C P C P = (13) El sistema de detección de eventos nuevos proporciona dos salidas para cada historia. La primera parte es sí o no, indicando si la historia desencadena un nuevo evento o no. La segunda parte es una puntuación que indica la confianza de la primera decisión. Los puntajes de confianza se pueden utilizar para trazar la curva DET, es decir, curvas que representan la probabilidad de falsa alarma frente a la probabilidad de omisión. El costo normalizado mínimo se puede determinar si se elige el umbral óptimo en la puntuación. RESULTADOS EXPERIMENTALES 7.1 Resultados principales Para probar los enfoques propuestos en el modelo, implementamos y probamos cinco sistemas: Sistema-1: este sistema se utiliza como referencia. Se implementa basándose en el modelo básico descrito en la sección 3, es decir, utilizando el modelo TF-IDF incremental para generar pesos de términos y utilizando la distancia de Hellinger para calcular la similitud entre documentos. La normalización del puntaje de similitud también se emplea [8]. Se utiliza el procedimiento de detección S-S. Sistema-2: este sistema es igual que el sistema-1 excepto que se utiliza el procedimiento de detección S-C. Sistema-3: este sistema es igual que el sistema-1 excepto que utiliza el nuevo procedimiento de detección que se basa en un árbol de indexación. Sistema-4: implementado basado en el enfoque presentado en la sección 5.1, es decir, los términos se vuelven a ponderar según la distancia entre las distribuciones de términos en un grupo y todas las historias. Se utiliza el nuevo procedimiento de detección. Sistema-5: implementado basado en el enfoque presentado en la sección 5.2, es decir, los términos de diferentes tipos son reponderados según la clase de noticias utilizando parámetros entrenados. El nuevo procedimiento de detección se utiliza. Los siguientes son algunos otros sistemas NED: Sistema-6: [21] para cada par de historias, calcula tres valores de similitud para entidad nombrada, entidad no nombrada y todos los términos respectivamente. Y emplear Máquina de Vectores de Soporte para predecir nuevo o antiguo utilizando los valores de similitud como características. Sistema-7: [8] extendió un modelo básico incremental de TF-IDF para incluir modelos específicos de origen, normalización de puntuaciones de similitud basada en promedios específicos de documentos, normalización de puntuaciones de similitud basada en promedios específicos de pares de fuentes, etc. Sistema-8: [13] dividió la representación del documento en dos partes: entidades nombradas y entidades no nombradas, y eligió una parte efectiva para cada clase de noticias. La tabla 3 y la tabla 4 muestran los costos normalizados ponderados por tema y los tiempos de comparación en los conjuntos de datos TDT2 y TDT3 respectivamente. Dado que no se contaba con un conjunto de datos de validación para ajustar el umbral θ nuevo en los experimentos en TDT2, solo informamos los costos normalizados mínimos para nuestros sistemas en la tabla 3. El sistema-5 supera a todos los demás sistemas, incluido el sistema-6, y realiza solo 2.78e+8 veces la comparación de tiempos en el procedimiento de detección, lo que representa solo el 13.4% del sistema-1. Tabla 3. Los resultados de NED en los sistemas TDT2 Min Norm(CDet) tiempos de comparación son: Sistema-1 0.5749 2.08e+9, Sistema-2 0.6673 3.77e+8, Sistema-3 0.5765 2.81e+8, Sistema-4 0.5431 2.99e+8, Sistema-5 0.5089 2.78e+8, Sistema-6 0.5300. Al evaluar en los costos normalizados en TDT3, utilizamos los umbrales óptimos obtenidos del conjunto de datos de TDT2 para todos los sistemas. El sistema-2 reduce los tiempos de comparación a 1.29e+9, que es solo el 18.3% del sistema-1, pero al mismo tiempo también obtiene un costo normalizado mínimo deteriorado que es 0.0499 más alto que el sistema-1. El Sistema-3 utiliza el nuevo procedimiento de detección basado en el índice de noticias en árbol. Requiere incluso menos tiempo de comparación que el sistema-2. Esto se debe a que las comparaciones de historias suelen arrojar mayores similitudes que las de grupos de historias, por lo que las historias tienden a combinarse en el sistema-3. Y el sistema-3 es básicamente equivalente al sistema-1 en los resultados de precisión. El System-4 ajusta los pesos de los términos basándose en la distancia de las distribuciones de términos entre todo el corpus y el conjunto de historias del clúster, lo que resulta en una mejora significativa de 0.0468 en comparación con el system-1. El mejor sistema (sistema-5) tiene un costo normalizado mínimo de 0.5012, lo cual es 0.0797 mejor que el sistema-1, y también mejor que cualquier otro resultado previamente reportado para este conjunto de datos [8, 13]. Además, el sistema-5 solo necesita 1.05e+8 veces de comparación, lo que equivale al 14.9% del sistema-1. Tabla 4. Los resultados de NED en los sistemas TDT3 Norm(CDet) Norma mínima(CDet) Tiempos de comparación Sistema-1 0.6159 0.5809 7.04e+8 Sistema-2① 0.6493 0.6308 1.29e+8 Sistema-3② 0.6197 0.5868 1.03e+8 Sistema-4② 0.5601 0.5341 1.03e+8 Sistema-5② 0.5413 0.5012 1.05e+8 Sistema-7 -- 0.5783 -Sistema-8 -- 0.5229 -① θ nuevo=0.13 ② θ inicial=0.13, λ =3, δ =0.15 La Figura 5 muestra las cinco curvas DET para nuestros sistemas en el conjunto de datos TDT3. El Sistema-5 logra el costo mínimo con una tasa de falsas alarmas de 0.0157 y una tasa de omisión de 0.4310. Podemos observar que System4 y System-5 obtienen una menor probabilidad de error de omisión en regiones de baja probabilidad de falsa alarma. La hipótesis es que se transfiere más valor de peso a los términos clave de los temas que a los términos no clave. La puntuación de similitud entre dos historias pertenecientes a diferentes temas es más baja que antes, porque los términos superpuestos suelen no ser términos clave de sus temas. 7.2 Selección de parámetros para la detección de árboles de indexación. La Figura 3 muestra los costos normalizados mínimos obtenidos por el sistema-3 en TDT3 utilizando diferentes parámetros. El parámetro de inicio θ se prueba en seis valores que van desde 0.03 hasta 0.18. Y el parámetro λ se prueba en cuatro valores: 1, 2, 3 y 4. Podemos ver que, cuando θ inicial se establece en 0.12, que es el más cercano a θ nuevo, los costos son más bajos que los demás. Esto es fácil de explicar, porque cuando se agrupan historias pertenecientes al mismo tema en un conjunto, es más razonable que el conjunto represente las historias en él. Cuando el parámetro λ se establece en 3 o 4, los costos son mejores que en otros casos, pero no hay mucha diferencia entre 3 y 4. 0 0.05 0.1 0.15 0.2 1 2 3 4 0.5 0.6 0.7 0.8 0.9 1 θ-initλ MinCost 0.6 0.65 0.7 0.75 0.8 0.85 0.9 Figura 3. Costo mínimo en TDT3 (δ =0.15) 0 0.05 0.1 0.15 0.2 1 2 3 4 0 0.5 1 1.5 2 2.5 x 10 8 θ-inicio λ Comparando tiempos 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 8 Figura 4. Comparando los tiempos en TDT3 (δ =0.15), la Figura 4 muestra los tiempos de comparación utilizados por el sistema-3 en TDT3 con los mismos parámetros que la Figura 3. Los tiempos de comparación dependen fuertemente de θ init. Debido a que cuanto mayor sea θ init, menos historias se combinan juntas, más veces de comparación se necesitan para la decisión de un nuevo evento. Por lo tanto, usamos θ init = 0.13, λ = 3, δ = 0.15 para los sistemas 3, 4 y 5. En esta configuración de parámetros, podemos obtener tanto costos normalizados mínimos bajos como menos tiempos de comparación. CONCLUSIÓN Hemos propuesto un procedimiento de detección basado en un árbol de indexación de noticias en nuestro modelo. Reduce los tiempos de comparación a aproximadamente una séptima parte del método tradicional sin afectar la precisión de NED. También hemos presentado dos extensiones al modelo básico TF-IDF. La primera extensión se realiza ajustando los pesos de los términos basados en las distribuciones de términos entre todo el corpus y un conjunto de historias de un clúster. Y la segunda extensión al modelo básico TF-IDF es un mejor uso de los tipos de términos (tipos de entidades nombradas y partes de la velocidad) de acuerdo con las categorías de noticias. Nuestros resultados experimentales en los conjuntos de datos TDT2 y TDT3 muestran que ambas extensiones contribuyen significativamente a la mejora en la precisión. No consideramos la información de tiempo de noticias como una pista para la tarea de NED, ya que la mayoría de los temas duran mucho tiempo y los conjuntos de datos de TDT solo abarcan un período relativamente corto (no más de 6 meses). Para el trabajo futuro, queremos recopilar un conjunto de noticias que abarque un período más largo de internet e integrar información temporal en la tarea de NED. Dado que el tema es un grupo de noticias relativamente grueso, también queremos refinar la granularidad del grupo a nivel de evento e identificar diferentes eventos y sus relaciones dentro de un tema. Agradecimientos: Este trabajo ha sido apoyado por la Fundación Nacional de Ciencias Naturales de China bajo la subvención No. 90604025. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son del autor(es) y no necesariamente reflejan las del patrocinador. REFERENCIAS [1] http://www.nist.gov/speech/tests/tdt/index.htm [2] En Detección y Seguimiento de Temas. Organización de la información basada en eventos. Kluwer Academic Publishers, 2002. .01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90 1 2 5 10 20 40 60 80 90 Probabilidad de Falsa Alarma (en %) Probabilidad de Omisión (en %) Curva Ponderada por Tema del SISTEMA1 Norma Mínima (Costo) del SISTEMA1 Curva Ponderada por Tema del SISTEMA2 Norma Mínima (Costo) del SISTEMA2 Curva Ponderada por Tema del SISTEMA3 Norma Mínima (Costo) del SISTEMA3 Curva Ponderada por Tema del SISTEMA4 Norma Mínima (Costo) del SISTEMA4 Curva Ponderada por Tema del SISTEMA5 Norma Mínima (Costo) del SISTEMA5 Rendimiento Aleatorio Figura 5. Curvas DET en TDT3 [3] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B.T. Archibald y X. Liu. Enfoques de aprendizaje para detectar y rastrear eventos de noticias. En el número especial de IEEE Intelligent Systems sobre Aplicaciones de Recuperación de Información Inteligente, volumen 14 (4), 1999, páginas 32-43. [4] Y. Yang, T. Pierce y J. Carbonell. Un estudio sobre la detección de eventos retrospectivos y en línea. En Actas de SIGIR-98, Melbourne, Australia, 1998, 28-36. [5] J. Allan, V. Lavrenko, D. Malin y R. Swan. Detecciones, límites y líneas de tiempo: Umass y tdt-3. En Actas del Taller de Detección y Seguimiento de Temas (TDT-3), Viena, VA, 2000, 167-174. [6] R. Papka y J. Allan. Detección de nuevos eventos en línea utilizando agrupamiento de un solo paso TÍTULO2:. Informe técnico UM-CS1998-021, 1998. [7] W. Lam, H. Meng, K. Wong y J. I'm sorry, but \"Yen\" is not a sentence. Can you please provide a sentence for me to translate into Spanish? Utilizando el Análisis Contextual para la Detección de Eventos de Noticias. Revista Internacional de Sistemas Inteligentes, 2001, 525-546. [8] B. Thorsten, C. Francine y F. Ayman. Un sistema para la detección de nuevos eventos. En Actas de la 26ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2003, 330-337. [9] S. Nicola y C. Joe. Combinando clasificadores semánticos y sintácticos de documentos para mejorar la detección de primeras noticias. En Actas de la 24ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell, and C. Jin. \n\nACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell y C. Jin. Detección de novedades condicionada al tema. En Actas de la 8ª Conferencia Internacional de ACM SIGKDD, ACM Press. 2002, 688-693. [11] M. Juha, A.M. Helena y S. Marko. Aplicando Clases Semánticas en la Detección y Seguimiento de Eventos. En Actas de la Conferencia Internacional sobre Procesamiento del Lenguaje Natural (ICON 2002), 2002, páginas 175-183. [12] M. Juha, A.M. Helena y S. Marko. Semántica simple en la detección y seguimiento de temas. Recuperación de información, 7(3-4): 2004, 347-368. [13] K. Giridhar y J. Allan. Clasificación de texto y entidades nombradas para la detección de nuevos eventos. En Actas de la 27ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2004, 297-304. [14] J. P. Callan, W. B. Croft, y S. M. Harding. El Sistema de Recuperación INQUERY. En Actas de DEXA-92, 3ra Conferencia Internacional sobre Aplicaciones de Bases de Datos y Sistemas Expertos, 1992, 78-83. [15] R. Krovetz. Viendo la morfología como un proceso de inferencia. En Actas de ACM SIGIR93, 1993, 61-81. [16] Y. Yang y J. Pedersen. Un estudio comparativo sobre la selección de características en la categorización de textos. En J. D. H. Fisher, editor, La Decimocuarta Conferencia Internacional sobre Aprendizaje Automático (ICML97), Morgan Kaufmann, 1997, 412-420. [17] T. M. Cover y J. A. I'm sorry, but \"Thomas\" is not a sentence. Can you provide more context or a complete sentence for me to translate into Spanish? Elementos de la teoría de la información. Wiley. 1991. [18] El consorcio de datos lingüísticos, http://www.ldc,upenn.edu/. [19] La definición de la tarea TDT 2001 y el plan de evaluación, http://www.nist.gov/speech/tests/tdt/tdt2001/evalplan.htm. [20] R. E. Schapire y Y. Cantante. Boostexter: Un sistema basado en Boosting para la categorización de texto. En Machine Learning 39(2/3):1, Kluwer Academic Publishers, 2000, 35-168. [21] K. Giridhar y J. Allan. 2005. Utilizando nombres y temas para la detección de nuevos eventos. En Actas de la Conferencia de Tecnología Humana y la Conferencia sobre Métodos Empíricos en Lenguaje Natural, Vancouver, 2005, 121-128 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "existing system": {
            "translated_key": "sistemas existentes",
            "is_in_text": true,
            "original_annotated_sentences": [
                "New Event Detection Based on Indexing-tree and Named Entity Zhang Kuo Tsinghua University Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Tsinghua University Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Tsinghua University Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn ABSTRACT New Event Detection (NED) aims at detecting from one or multiple streams of news stories that which one is reported on a new event (i.e.",
                "not reported previously).",
                "With the overwhelming volume of news available today, there is an increasing need for a NED system which is able to detect new events more efficiently and accurately.",
                "In this paper we propose a new NED model to speed up the NED task by using news indexing-tree dynamically.",
                "Moreover, based on the observation that terms of different types have different effects for NED task, two term reweighting approaches are proposed to improve NED accuracy.",
                "In the first approach, we propose to adjust term weights dynamically based on previous story clusters and in the second approach, we propose to employ statistics on training data to learn the named entity reweighting model for each class of stories.",
                "Experimental results on two Linguistic Data Consortium (LDC) datasets TDT2 and TDT3 show that the proposed model can improve both efficiency and accuracy of NED task significantly, compared to the baseline system and other <br>existing system</br>s.",
                "Categories and Subject Descriptors H.3.3 [Information Systems]: Information Search and Retrieval; H.4.2 [Information Systems Applications]: Types of Systemsdecision support.",
                "General Terms Algorithms, Performance, Experimentation 1.",
                "INTRODUCTION Topic Detection and Tracking (TDT) program aims to develop techniques which can effectively organize, search and structure news text materials from a variety of newswire and broadcast media [1].",
                "New Event Detection (NED) is one of the five tasks in TDT.",
                "It is the task of online identification of the earliest report for each topic as soon as that report arrives in the sequence of documents.",
                "A Topic is defined as a seminal event or activity, along with directly related events and activities [2].",
                "An Event is defined as something (non-trivial) happening in a certain place at a certain time [3].",
                "For instance, when a bomb explodes in a building, the exploding is the seminal event that triggers the topic, and other stories on the same topic would be those discussing salvaging efforts, the search for perpetrators, arrests and trial and so on.",
                "Useful news information is usually buried in a mass of data generated everyday.",
                "Therefore, NED systems are very useful for people who need to detect novel information from real-time news stream.",
                "These real-life needs often occur in domains like financial markets, news analysis, and intelligence gathering.",
                "In most of state-of-the-art (currently) NED systems, each news story on hand is compared to all the previous received stories.",
                "If all the similarities between them do not exceed a threshold, then the story triggers a new event.",
                "They are usually in the form of cosine similarity or Hellinger similarity metric.",
                "The core problem of NED is to identify whether two stories are on the same topic.",
                "Obviously, these systems cannot take advantage of topic information.",
                "Further more, it is not acceptable in real applications because of the large amount of computation required in the NED process.",
                "Other systems organize previous stories into clusters (each cluster corresponds to a topic), and new story is compared to the previous clusters instead of stories.",
                "This manner can reduce comparing times significantly.",
                "Nevertheless, it has been proved that this manner is less accurate [4, 5].",
                "This is because sometimes stories within a topic drift far away from each other, which could lead low similarity between a story and its topic.",
                "On the other hand, some proposed NED systems tried to improve accuracy by making better use of named entities [10, 11, 12, 13].",
                "However, none of the systems have considered that terms of different types (e.g.",
                "Noun, Verb or Person name) have different effects for different classes of stories in determining whether two stories are on the same topic.",
                "For example, the names of election candidates (Person name) are very important for stories of election class; the locations (Location name) where accidents happened are important for stories of accidents class.",
                "So, in NED, there still exist following three problems to be investigated: (1) How to speed up the detection procedure while do not decrease the detection accuracy? (2) How to make good use of cluster (topic) information to improve accuracy? (3) How to obtain better news story representation by better understanding of named entities.",
                "Driven by these problems, we have proposed three approaches in this paper. (1)To make the detection procedure faster, we propose a new NED procedure based on news indexing-tree created dynamically.",
                "Story indexing-tree is created by assembling similar stories together to form news clusters in different hierarchies according to their values of similarity.",
                "Comparisons between current story and previous clusters could help find the most similar story in less comparing times.",
                "The new procedure can reduce the amount of comparing times without hurting accuracy. (2)We use the clusters of the first floor in the indexing-tree as news topics, in which term weights are adjusted dynamically according to term distribution in the clusters.",
                "In this approach, cluster (topic) information is used properly, so the problem of theme decentralization is avoided. (3)Based on observations on the statistics obtained from training data, we found that terms of different types (e.g.",
                "Noun and Verb) have different effects for different classes of stories in determining whether two stories are on the same topic.",
                "And we propose to use statistics to optimize the weights of the terms of different types in a story according to the news class that the story belongs to.",
                "On TDT3 dataset, the new NED model just uses 14.9% comparing times of the basic model, while its minimum normalized cost is 0.5012, which is 0.0797 better than the basic model, and also better than any other results previously reported for this dataset [8, 13].",
                "The rest of the paper is organized as follows.",
                "We start off this paper by summarizing the previous work in NED in section 2.",
                "Section 3 presents the basic model for NED that most current systems use.",
                "Section 4 describes our new detection procedure based on news indexing-tree.",
                "In section 5, two term reweighting methods are proposed to improve NED accuracy.",
                "Section 6 gives our experimental data and evaluation metrics.",
                "We finally wrap up with the experimental results in Section 7, and the conclusions and future work in Section 8. 2.",
                "RELATED WORK Papka et al. proposed Single-Pass clustering on NED [6].",
                "When a new story was encountered, it was processed immediately to extract term features and a query representation of the storys content is built up.",
                "Then it was compared with all the previous queries.",
                "If the document did not trigger any queries by exceeding a threshold, it was marked as a new event.",
                "Lam et al build up previous query representations of story clusters, each of which corresponds to a topic [7].",
                "In this manner comparisons happen between stories and clusters.",
                "Recent years, most work focus on proposing better methods on comparison of stories and document representation.",
                "Brants et al. [8] extended a basic incremental TF-IDF model to include sourcespecific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, term reweighting based on inverse event frequencies, and segmentation of documents.",
                "Good improvements on TDT bench-marks were shown.",
                "Stokes et al. [9] utilized a combination of evidence from two distinct representations of a documents content.",
                "One of the representations was the usual free text vector, the other made use of lexical chains (created using WordNet) to build another term vector.",
                "Then the two representations are combined in a linear fashion.",
                "A marginal increase in effectiveness was achieved when the combined representation was used.",
                "Some efforts have been done on how to utilize named entities to improve NED.",
                "Yang et al. gave location named entities four times weight than other terms and named entities [10].",
                "DOREMI research group combined semantic similarities of person names, location names and time together with textual similarity [11][12].",
                "UMass [13] research group split document representation into two parts: named entities and non-named entities.",
                "And it was found that some classes of news could achieve better performance using named entity representation, while some other classes of news could achieve better performance using non-named entity representation.",
                "Both [10] and [13] used text categorization technique to classify news stories in advance.",
                "In [13] news stories are classified automatically at first, and then test sensitivities of names and non-name terms for NED for each class.",
                "In [10] frequent terms for each class are removed from document representation.",
                "For example, word election does not help identify different elections.",
                "In their work, effectiveness of different kinds of names (or terms with different POS) for NED in different news classes are not investigated.",
                "We use statistical analysis to reveal the fact and use it to improve NED performance. 3.",
                "BASIC MODEL In this section, we present the basic New Event Detection model which is similar to what most current systems apply.",
                "Then, we propose our new model by extending the basic model.",
                "New Event Detection systems use news story stream as input, in which stories are strictly time-ordered.",
                "Only previously received stories are available when dealing with current story.",
                "The output is a decision for whether the current story is on a new event or not and the confidence of the decision.",
                "Usually, a NED model consists of three parts: story representation, similarity calculation and detection procedure. 3.1 Story Representation Preprocessing is needed before generating story representation.",
                "For preprocessing, we tokenize words, recognize abbreviations, normalize abbreviations, add part-of-speech tags, remove stopwords included in the stop list used in InQuery [14], replace words with their stems using K-stem algorithm[15], and then generate word vector for each news story.",
                "We use incremental TF-IDF model for term weight calculation [4].",
                "In a TF-IDF model, term frequency in a news document is weighted by the inverse document frequency, which is generated from training corpus.",
                "When a new term occurs in testing process, there are two solutions: simply ignore the new term or set df of the term as a small const (e.g. df = 1).",
                "The new term receives too low weight in the first solution (0) and too high weight in the second solution.",
                "In incremental TF-IDF model, document frequencies are updated dynamically in each time step t: 1( ) ( ) ( )t t D tdf w df w df w−= + (1) where Dt represents news story set received in time t, and dfDt(w) means the number of documents that term w occurs in, and dft(w) means the total number of documents that term w occurs in before time t. In this work, each time window includes 50 news stories.",
                "Thus, each story d received in t is represented as follows: 1 2{ ( , , ), ( , , ),..., ( , , )}nd weight d t w weight d t w weight d t w→ where n means the number of distinct terms in story d, and ( , , )weight d t w means the weight of term w in story d at time t: log( ( , ) 1) log(( 1) /( ( ) 0.5)) ( , , ) log( ( , ) 1) log(( 1) /( ( ) 0.5)) t t t t w d tf d w N df w weight d t w tf d w N df w ∈ + + + = + + +∑ (2) where Nt means the total number of news stories before time t, and tf(d,w) means how many times term w occurs in news story d. 3.2 Similarity Calculation We use Hellinger distance for the calculation of similarity between two stories, for two stories d and d at time t, their similarity is defined as follows: , ( , , ) ( , , ) * ( , , ) w d d sim d d t weight d t w weight d t w ∈ = ∑ (3) 3.3 Detection Procedure For each story d received in time step t, the value ( ) ( ) ( ) ( ( , , )) time d time d n d max sim d d t < = (4) is a score used to determine whether d is a story about a new topic and at the same time is an indication of the confidence in our decision [8]. time(d) means the publication time of story d. If the score exceeds the thresholdθ new, then there exists a sufficiently similar document, thus d is a old story, otherwise, there is no sufficiently similar previous document, thus d is an new story. 4.",
                "New NED Procedure Traditional NED systems can be classified into two main types on the aspect of detection procedure: (1) S-S type, in which the story on hand is compared to each story received previously, and use the highest similarity to determine whether current story is about a new event; (2) S-C type, in which the story on hand is compared to all previous clusters each of which representing a topic, and the highest similarity is used for final decision for current story.",
                "If the highest similarity exceeds thresholdθ new, then it is an old story, and put it into the most similar cluster; otherwise it is a new story and create a new cluster.",
                "Previous work show that the first manner is more accurate than the second one [4][5].",
                "Since sometimes stories within a topic drift far away from each other, a story may have very low similarity with its topic.",
                "So using similarities between stories for determining new story is better than using similarities between story and clusters.",
                "Nevertheless, the first manner needs much more comparing times which means the first manner is low efficient.",
                "We propose a new detection procedure which uses comparisons with previous clusters to help find the most similar story in less comparing times, and the final new event decision is made according to the most similar story.",
                "Therefore, we can get both the accuracy of S-S type methods and the efficiency of S-C type methods.",
                "The new procedure creates a news indexing-tree dynamically, in which similar stories are put together to form a hierarchy of clusters.",
                "We index similar stories together by their common ancestor (a cluster node).",
                "Dissimilar stories are indexed in different clusters.",
                "When a story is coming, we use comparisons between the current story and previous hierarchical clusters to help find the most similar story which is useful for new event decision.",
                "After the new event decision is made, the current story is inserted to the indexing-tree for the following detection.",
                "The news indexing-tree is defined formally as follows: S-Tree = {r, NC , NS , E} where r is the root of S-Tree, NC is the set of all cluster nodes, NS is the set of all story nodes, and E is the set of all edges in S-Tree.",
                "We define a set of constraints for a S-Tree: ⅰ . , is an non-terminal node in the treeC i i N i∀ ∈ → ⅱ . , is a terminal node in the treeS i i N i∀ ∈ → ⅲ . , out degree of is at least 2C i i N i∀ ∈ → ⅳ . , is represented as the centroid of its desendantsC i i iN∀ ∈ → For a news story di, the comparison procedure and inserting procedure based on indexing-tree are defined as follows.",
                "An example is shown by Figure 1 and Figure 2.",
                "Figure 1.",
                "Comparison procedure Figure 2.",
                "Inserting procedure Comparison procedure: Step 1: compare di to all the direct child nodes of r and select λ nodes with highest similarities, e.g., C1 2 and C1 3 in Figure 1.",
                "Step 2: for each selected node in the last step, e.g.",
                "C1 2, compare di to all its direct child nodes, and select λ nodes with highest similarities, e.g.",
                "C2 2 and d8.",
                "Repeat step 2 for all non-terminal nodes.",
                "Step 3: record the terminal node with the highest similarty to di, e.g. s5, and the similarity value (0.20).",
                "Inserting di to the S-tree with r as root: Find the node n which is direct child of r in the path from r to the terminal node with highest similarity s, e.g.",
                "C1 2.",
                "If s is smaller than θ init+(h-1)δ , then add di to the tree as a direct child of r. Otherwise, if n is a terminal node, then create a cluster node instead of n, and add both n and di as its direct children; if n is an non-terminal node, then repeat this procedure and insert di to the sub-tree with n as root recursively.",
                "Here h is the length between n and the root of S-tree.",
                "The more the stories in a cluster similar to each other, the better the cluster represents the stories in it.",
                "Hence we add no constraints on the maximum of trees height and degree of a node.",
                "Therefore, we cannot give the complexity of this indexing-tree based procedure.",
                "But we will give the number of comparing times needed by the new procedure in our experiments in section7. 5.",
                "Term Reweighting Methods In this section, two term reweighting methods are proposed to improve NED accuracy.",
                "In the first method, a new way is explored for better using of cluster (topic) information.",
                "The second one finds a better way to make use of named entities based on news classification. 5.1 Term Reweighting Based on Distribution Distance TF-IDF is the most prevalent model used in information retrieval systems.",
                "The basic idea is that the fewer documents a term appears in, the more important the term is in discrimination of documents (relevant or not relevant to a query containing the term).",
                "Nevertheless, in TDT domain, we need to discriminate documents with regard to topics rather than queries.",
                "Intuitively, using cluster (topic) vectors to compare with subsequent news stories should outperform using story vectors.",
                "Unfortunately, the experimental results do not support this intuition [4][5].",
                "Based on observation on data, we find the reason is that a news topic usually contains many directly or indirectly related events, while they all have their own sub-subjects which are usually different with each other.",
                "Take the topic described in section 1 as an example, events like the explosion and salvage have very low similarities with events about criminal trial, therefore stories about trial would have low similarity with the topic vector built on its previous events.",
                "This section focuses on how to effectively make use of topic information and at the same time avoid the problem of content decentralization.",
                "At first, we classify terms into 5 classes to help analysis the needs of the modified model: Term class A: terms that occur frequently in the whole corpus, e.g., year and people.",
                "Terms of this class should be given low weights because they do not help much for topic discrimination.",
                "Term class B: terms that occur frequently within a news category, e.g., election, storm.",
                "They are useful to distinguish two stories in different news categories.",
                "However, they cannot provide information to determine whether two stories are on the same or different topics.",
                "In another words, term election and term storm are not helpful in differentiate two election campaigns and two storm disasters.",
                "Therefore, terms of this class should be assigned lower weights.",
                "Term class C: terms that occur frequently in a topic, and infrequently in other topics, e.g., the name of a crash plane, the name of a specific hurricane.",
                "News stories that belong to different topics rarely have overlap terms in this class.",
                "The more frequently a term appears in a topic, the more important the term is for a story belonging to the topic, therefore the term should be set higher weight.",
                "Term class D: terms that appear in a topic exclusively, but not frequently.",
                "For example, the name of a fireman who did very well in a salvage action, which may appears in only two or three stories but never appeared in other topics.",
                "Terms of this type should receive more weights than in TF-IDF model.",
                "However, since they are not popular in the topic, it is not appropriate to give them too high weights.",
                "Term class E: terms with low document frequency, and appear in different topics.",
                "Terms of this class should receive lower weights.",
                "Now we analyze whether TF-IDF model can give proper weights to the five classes of terms.",
                "Obviously, terms of class A are lowly weighted in TF-IDF model, which is conformable with the requirement described above.",
                "In TF-IDF model, terms of class B are highly dependant with the number of stories in a news class.",
                "TF-IDF model cannot provide low weights if the story containing the term belongs to a relative small news class.",
                "For a term of class C, the more frequently it appears in a topic, the less weight TFIDF model gives to it.",
                "This strongly conflicts with the requirement of terms in class C. For terms of class D, TF-IDF model gives them high weights correctly.",
                "But for terms of class E, TF-IDF model gives high weights to them which are not conformable with the requirement of low weights.",
                "To sum up, terms of class B, C, E cannot be properly weighted in TF-IDF model.",
                "So, we propose a modified model to resolve this problem.",
                "When θ init andθ new are set closely, we assume that most of the stories in a first-level cluster (a direct child node of root node) are on the same topic.",
                "Therefore, we make use of a first-level cluster to capture term distribution (df for all the terms within the cluster) within the topic dynamically.",
                "KL divergence of term distribution in a first-level cluster and the whole story set is used to adjust term weights:  ( , , ) * (1 * ( || )) ( , , ) ( , , ) * (1 * ( || )) cw tw cw tw w d D weight d t w KL P P weight d t w weight d t w KL P P γ γ ∈ + = +∑ (5) where ( ) ( ) ( ) ( ) 1,cw cw c c c c df w df w p y p y N N = = − (6) ( ) ( ) ( ) ( ) 1,t t tw tw t t df w df w p y p y N N = = − (7) where dfc(w) is the number of documents containing term w within cluster C, and Nc is the number of documents in cluster C, and Nt is the total number of documents that arrive before time step t. γ is a const parameter, now is manually set 3.",
                "KL divergence is defined as follows [17]: ( ) ( || ) ( ) log ( )x p x KL P Q p x q x = ∑ (8) The basic idea is: for a story in a topic, the more a term occurs within the topic, and the less it occurs in other topics, it should be assigned higher weights.",
                "Obviously, modified model can meet all the requirements of the five term classes listed above. 5.2 Term Reweighting Based on Term Type and Story Class Previous work found that some classes of news stories could achieve good improvements by giving extra weight to named entities.",
                "But we find that terms of different types should be given different amount of extra weight for different classes of news stories.",
                "We use open-NLP1 to recognize named entity types and part-ofspeech tags for terms that appear in news stories.",
                "Named entity types include person name, organization name, location name, date, time, money and percentage, and five POSs are selected: none (NN), verb (VB), adjective (JJ), adverb (RB) and cardinal number (CD).",
                "Statistical analysis shows topic-level discriminative terms types for different classes of stories.",
                "For the sake of convenience, named entity type and part-of-speech tags are uniformly called term type in subsequent sections.",
                "Determining whether two stories are about the same topic is a basic component for NED task.",
                "So at first we use 2 χ statistic to compute correlations between terms and topics.",
                "For a term t and a topic T, a contingence table is derived: Table 1.",
                "A 2×2 Contingence Table Doc Number belong to topic T not belong to topic T include t A B not include t C D The 2 χ statistic for a specific term t with respect to topic T is defined to be [16]: 2 2 ( , ) ( ) * ( ) ( ) * ( ) * ( ) * ( ) w T A B C D AD CB A C B D A B C D χ = + + + − + + + + (9) News topics for the TDT task are further classified into 11 rules of interpretations (ROIs) 2 .",
                "The ROI can be seen as a higher level class of stories.",
                "The average correlation between a term type and a topic ROI is computed as: 2 avg 2 ( , )( ( , ) )k m m km kT R w P w TP R p w T R P χ χ ∈ ∈ ∑ ∑（ , ）= 1 1 k=1…K, m=1…M (10) where K is the number of term types (set 12 constantly in the paper).",
                "M is the number news classes (ROIs, set 11 in the paper).",
                "Pk represents the set of all terms of type k, and Rm represents the set of all topics of class m, p(t,T) means the probability that t occurs in topic T. Because of limitation of space, only parts of the term types (9 term types) and parts of news classes (8 classes) are listed in table 2 with the average correlation values between them.",
                "The statistics is derived from labeled data in TDT2 corpus. (Results in table 2 are already normalized for convenience in comparison.)",
                "The statistics in table 2 indicates the usefulness of different term types in topic discrimination with respect to different news classes.",
                "We can see that, location name is the most useful term type for three news classes: Natural Disasters, Violence or War, Finances.",
                "And for three other categories Elections, Legal/Criminal Cases, Science and Discovery, person name is the most discriminative term type.",
                "For Scandals/Hearings, date is the most important information for topic discrimination.",
                "In addition, Legal/Criminal Cases and Finance topics have higher correlation with money terms, while Science and Discovery have higher correlation with percentage terms.",
                "Non-name terms are more stable for different classes. 1 . http://opennlp.sourceforge.net/ 2 . http://projects.ldc.upenn.edu/TDT3/Guide/label.html From the analysis of table 2, it is reasonable to adjust term weight according to their term type and the news class the story belongs to.",
                "New term weights are reweighted as follows: ( ) ( ) ( ) ( ) ( , , ) * ( , , ) ( , , ) * class d D type w T class d D type w w d weight d t w weight d t w weight d t w α α ∈ = ∑ (11) where type(w) represents the type of term w, and class(d) represents the class of story d, c kα is reweighting parameter for news class c and term type k. In the work, we just simply use statistics in table 2 as the reweighting parameters.",
                "Even thought using the statistics directly may not the best choice, we do not discuss how to automatically obtain the best parameters.",
                "We will try to use machine learning techniques to obtain the best parameters in the future work.",
                "In the work, we use BoosTexter [20] to classify all stories into one of the 11 ROIs.",
                "BoosTexter is a boosting based machine learning program, which creates a series of simple rules for building a classifier for text or attribute-value data.",
                "We use term weight generated using TF-IDF model as feature for story classification.",
                "We trained the model on the 12000 judged English stories in TDT2, and classify the rest of the stories in TDT2 and all stories in TDT3.",
                "Classification results are used for term reweighting in formula (11).",
                "Since the class labels of topic-off stories are not given in TDT datasets, we cannot give the classification accuracy here.",
                "Thus we do not discuss the effects of classification accuracy to NED performance in the paper. 6.",
                "EXPERIMENTAL SETUP 6.1 Datasets We used two LDC [18] datasets TDT2 and TDT3 for our experiments.",
                "TDT2 contains news stories from January to June 1998.",
                "It contains around 54,000 stories from sources like ABC, Associated Press, CNN, New York Times, Public Radio International, Voice of America etc.",
                "Only English stories in the collection were considered.",
                "TDT3 contains approximately 31,000 English stories collected from October to December 1998.",
                "In addition to the sources used in TDT2, it also contains stories from NBC and MSNBC TV broadcasts.",
                "We used transcribed versions of the TV and radio broadcasts besides textual news.",
                "TDT2 dataset is labeled with about 100 topics, and approximately 12,000 English stories belong to at least one of these topics.",
                "TDT3 dataset is labeled with about 120 topics, and approximately 8000 English stories belong to at least one of these topics.",
                "All the topics are classified into 11 Rules of Interpretation: (1)Elections, (2)Scandals/Hearings, (3)Legal/Criminal Cases, (4)Natural Disasters, (5)Accidents, (6)Ongoing Violence or War, (7)Science and Discovery News, (8)Finance, (9)New Law, (10)Sports News, (11)MISC.",
                "News. 6.2 Evaluation Metric TDT uses a cost function CDet that combines the probabilities of missing a new story and a false alarm [19]: * * * *Det Miss Miss Target FA FA NontargetC C P P C P P= + (12) Table 2.",
                "Average correlation between term types and news classes where CMiss means the cost of missing a new story, PMiss means the probability of missing a new story, and PTarget means the probability of seeing a new story in the data; CFA means the cost of a false alarm, PFA means the probability of a false alarm, and PNontarget means the probability of seeing an old story.",
                "The cost CDet is normalized such that a perfect system scores 0 and a trivial system, which is the better one of mark all stories as new or old, scores 1: ( ( * , * ) ) Det Det Miss Target FA Nontarget C Norm C min C P C P = (13) New event detection system gives two outputs for each story.",
                "The first part is yes or no indicating whether the story triggers a new event or not.",
                "The second part is a score indicating confidence of the first decision.",
                "Confidence scores can be used to plot DET curve, i.e., curves that plot false alarm vs. miss probabilities.",
                "Minimum normalized cost can be determined if optimal threshold on the score were chosen. 7.",
                "EXPERIMENTAL RESULTS 7.1 Main Results To test the approaches proposed in the model, we implemented and tested five systems: System-1: this system is used as baseline.",
                "It is implemented based on the basic model described in section 3, i.e., using incremental TF-IDF model to generate term weights, and using Hellinger distance to compute document similarity.",
                "Similarity score normalization is also employed [8].",
                "S-S detection procedure is used.",
                "System-2: this system is the same as system-1 except that S-C detection procedure is used.",
                "System-3: this system is the same as system-1 except that it uses the new detection procedure which is based on indexing-tree.",
                "System-4: implemented based on the approach presented in section 5.1, i.e., terms are reweighted according to the distance between term distributions in a cluster and all stories.",
                "The new detection procedure is used.",
                "System-5: implemented based on the approach presented in section 5.2, i.e., terms of different types are reweighted according to news class using trained parameters.",
                "The new detection procedure is used.",
                "The following are some other NED systems: System-6: [21] for each pair of stories, it computes three similarity values for named entity, non-named entity and all terms respectively.",
                "And employ Support Vector Machine to predict new or old using the similarity values as features.",
                "System-7: [8] it extended a basic incremental TF-IDF model to include source-specific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, etc.",
                "System-8: [13] it split document representation into two parts: named entities and non-named entities, and choose one effective part for each news class.",
                "Table 3 and table 4 show topic-weighted normalized costs and comparing times on TDT2 and TDT3 datasets respectively.",
                "Since no heldout data set for fine-tuning the threshold θ new was available for experiments on TDT2, we only report minimum normalized costs for our systems in table 3.",
                "System-5 outperforms all other systems including system-6, and it performs only 2.78e+8 comparing times in detection procedure which is only 13.4% of system-1.",
                "Table 3.",
                "NED results on TDT2 Systems Min Norm(CDet) Cmp times System-1 0.5749 2.08e+9 System-2① 0.6673 3.77e+8 System-3② 0.5765 2.81e+8 System-4② 0.5431 2.99e+8 System-5② 0.5089 2.78e+8 System-6 0.5300 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 When evaluating on the normalized costs on TDT3, we use the optimal thresholds obtained from TDT2 data set for all systems.",
                "System-2 reduces comparing times to 1.29e+9 which is just 18.3% of system-1, but at the same time it also gets a deteriorated minimum normalized cost which is 0.0499 higher than system-1.",
                "System-3 uses the new detection procedure based on news indexing-tree.",
                "It requires even less comparing times than system-2.",
                "This is because story-story comparisons usually yield greater similarities than story-cluster ones, so stories tend to be combined Location Person Date Organization Money Percentage NN JJ CD Elections 0.37 1 0.04 0.58 0.08 0.03 0.32 0.13 0.1 Scandals/Hearings 0.66 0.62 0.28 1 0.11 0.02 0.27 0.13 0.05 Legal/Criminal Cases 0.48 1 0.02 0.62 0.15 0 0.22 0.24 0.09 Natural Disasters 1 0.27 0 0.04 0.04 0 0.25 0.04 0.02 Violence or War 1 0.36 0.02 0.14 0.02 0.04 0.21 0.11 0.02 Science and Discovery 0.11 1 0.01 0.22 0.08 0.12 0.19 0.08 0.03 Finances 1 0.45 0.04 0.98 0.13 0.02 0.29 0.06 0.05 Sports 0.16 0.27 0.01 1 0.02 0 0.11 0.03 0.01 together in system-3.",
                "And system-3 is basically equivalent to system-1 in accuracy results.",
                "System-4 adjusts term weights based on the distance of term distributions between the whole corpus and cluster story set, yielding a good improvement by 0.0468 compared to system-1.",
                "The best system (system-5) has a minimum normalized cost 0.5012, which is 0.0797 better than system-1, and also better than any other results previously reported for this dataset [8, 13].",
                "Further more, system-5 only needs 1.05e+8 comparing times which is 14.9% of system-1.",
                "Table 4.",
                "NED results on TDT3 Systems Norm(CDet) Min Norm(CDet) Cmp times System-1 0.6159 0.5809 7.04e+8 System-2① 0.6493 0.6308 1.29e+8 System-3② 0.6197 0.5868 1.03e+8 System-4② 0.5601 0.5341 1.03e+8 System-5② 0.5413 0.5012 1.05e+8 System-7 -- 0.5783 -System-8 -- 0.5229 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 Figure5 shows the five DET curves for our systems on data set TDT3.",
                "System-5 achieves the minimum cost at a false alarm rate of 0.0157 and a miss rate of 0.4310.",
                "We can observe that System4 and System-5 obtain lower miss probability at regions of low false alarm probabilities.",
                "The hypothesis is that, more weight value is transferred to key terms of topics from non-key terms.",
                "Similarity score between two stories belonging to different topics are lower than before, because their overlapping terms are usually not key terms of their topics. 7.2 Parameter selection for indexing-tree detection Figure 3 shows the minimum normalized costs obtained by system-3 on TDT3 using different parameters.",
                "Theθ init parameter is tested on six values spanning from 0.03 to 0.18.",
                "And the λ parameter is tested on four values 1, 2, 3 and 4.",
                "We can see that, whenθ init is set to 0.12, which is the closest one toθ new, the costs are lower than others.",
                "This is easy to explain, because when stories belonging to the same topic are put in a cluster, it is more reasonable for the cluster to represent the stories in it.",
                "When parameter λ is set to 3 or 4, the costs are better than other cases, but there is no much difference between 3 and 4. 0 0.05 0.1 0.15 0.2 1 2 3 4 0.5 0.6 0.7 0.8 0.9 1 θ-initλ MinCost 0.6 0.65 0.7 0.75 0.8 0.85 0.9 Figure 3.",
                "Min Cost on TDT3 (δ =0.15) 0 0.05 0.1 0.15 0.2 1 2 3 4 0 0.5 1 1.5 2 2.5 x 10 8 θ-init λ Comparingtimes 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 8 Figure 4.",
                "Comparing times on TDT3 (δ =0.15) Figure 4 gives the comparing times used by system-3 on TDT3 with the same parameters as figure 3.",
                "The comparing times are strongly dependent onθ init.",
                "Because the greaterθ init is, the less stories combined together, the more comparing times are needed for new event decision.",
                "So we useθ init =0.13,λ =3,δ =0.15 for system-3, 4, and 5.",
                "In this parameter setting, we can get both low minimum normalized costs and less comparing times. 8.",
                "CONCLUSION We have proposed a news indexing-tree based detection procedure in our model.",
                "It reduces comparing times to about one seventh of traditional method without hurting NED accuracy.",
                "We also have presented two extensions to the basic TF-IDF model.",
                "The first extension is made by adjust term weights based on term distributions between the whole corpus and a cluster story set.",
                "And the second extension to basic TF-IDF model is better use of term types (named entities types and part-of-speed) according to news categories.",
                "Our experimental results on TDT2 and TDT3 datasets show that both of the two extensions contribute significantly to improvement in accuracy.",
                "We did not consider news time information as a clue for NED task, since most of the topics last for a long time and TDT data sets only span for a relative short period (no more than 6 months).",
                "For the future work, we want to collect news set which span for a longer period from internet, and integrate time information in NED task.",
                "Since topic is a relative coarse-grained news cluster, we also want to refine cluster granularity to event-level, and identify different events and their relations within a topic.",
                "Acknowledgments This work is supported by the National Natural Science Foundation of China under Grant No. 90604025.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsor. 9.",
                "REFERENCES [1] http://www.nist.gov/speech/tests/tdt/index.htm [2] In Topic Detection and Tracking.",
                "Event-based Information Organization.",
                "Kluwer Academic Publishers, 2002. .01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90 1 2 5 10 20 40 60 80 90 False Alarm Probability (in %) MissProbability(in%) SYSTEM1 Topic Weighted Curve SYSTEM1 Min Norm(Cost) SYSTEM2 Topic Weighted Curve SYSTEM2 Min Norm(Cost) SYSTEM3 Topic Weighted Curve SYSTEM3 Min Norm(Cost) SYSTEM4 Topic Weighted Curve SYSTEM4 Min Norm(Cost) SYSTEM5 Topic Weighted Curve SYSTEM5 Min Norm(Cost) Random Performance Figure 5.",
                "DET curves on TDT3 [3] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B.T.",
                "Archibald, and X. Liu.",
                "Learning Approaches for Detecting and Tracking News Events.",
                "In IEEE Intelligent Systems Special Issue on Applications of Intelligent Information Retrieval, volume 14 (4), 1999, 32-43. [4] Y. Yang, T. Pierce, and J. Carbonell.",
                "A Study on Retrospective and On-line Event Detection.",
                "In Proceedings of SIGIR-98, Melbourne, Australia, 1998, 28-36. [5] J. Allan, V. Lavrenko, D. Malin, and R. Swan.",
                "Detections, Bounds, and Timelines: Umass and tdt-3.",
                "In Proceedings of Topic Detection and Tracking Workshop (TDT-3), Vienna, VA, 2000, 167-174. [6] R. Papka and J. Allan.",
                "On-line New Event Detection Using Single Pass Clustering TITLE2:.",
                "Technical Report UM-CS1998-021, 1998. [7] W. Lam, H. Meng, K. Wong, and J.",
                "Yen.",
                "Using Contextual Analysis for News Event Detection.",
                "International Journal on Intelligent Systems, 2001, 525-546. [8] B. Thorsten, C. Francine, and F. Ayman.",
                "A System for New Event Detection.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2003, 330-337. [9] S. Nicola and C. Joe.",
                "Combining Semantic and Syntactic Document Classifiers to Improve First Story Detection.",
                "In Proceedings of the 24th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topicconditioned Novelty Detection.",
                "In Proceedings of the 8th ACM SIGKDD International Conference, ACM Press. 2002, 688-693. [11] M. Juha, A.M. Helena, and S. Marko.",
                "Applying Semantic Classes in Event Detection and Tracking.",
                "In Proceedings of International Conference on Natural Language Processing (ICON 2002), 2002, pages 175-183. [12] M. Juha, A.M. Helena, and S. Marko.",
                "Simple Semantics in Topic Detection and Tracking.",
                "Information Retrieval, 7(3-4): 2004, 347-368. [13] K. Giridhar and J. Allan.",
                "Text Classification and Named Entities for New Event Detection.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2004, 297-304. [14] J. P. Callan, W. B. Croft, and S. M. Harding.",
                "The INQUERY Retrieval System.",
                "In Proceedings of DEXA-92, 3rd International Conference on Database and Expert Systems Applications, 1992, 78-83. [15] R. Krovetz.",
                "Viewing Morphology as An Inference Process.",
                "In Proceedings of ACM SIGIR93, 1993, 61-81. [16] Y. Yang and J. Pedersen.",
                "A Comparative Study on Feature Selection in Text Categorization.",
                "In J. D. H. Fisher, editor, The Fourteenth International Conference on Machine Learning (ICML97), Morgan Kaufmann, 1997, 412-420. [17] T. M. Cover, and J.A.",
                "Thomas.",
                "Elements of Information Theory.",
                "Wiley. 1991. [18] The linguistic data consortium, http://www.ldc,upenn.edu/. [19] The 2001 TDT task definition and evaluation plan, http://www.nist.gov/speech/tests/tdt/tdt2001/evalplan.htm. [20] R. E. Schapire and Y.",
                "Singer.",
                "Boostexter: A Boosting-based System for Text Categorization.",
                "In Machine Learning 39(2/3):1, Kluwer Academic Publishers, 2000, 35-168. [21] K. Giridhar and J. Allan. 2005.",
                "Using Names and Topics for New Event Detection.",
                "In Proceedings of Human Technology Conference and Conference on Empirical Methods in Natural Language, Vancouver, 2005, 121-128"
            ],
            "original_annotated_samples": [
                "Experimental results on two Linguistic Data Consortium (LDC) datasets TDT2 and TDT3 show that the proposed model can improve both efficiency and accuracy of NED task significantly, compared to the baseline system and other <br>existing system</br>s."
            ],
            "translated_annotated_samples": [
                "Los resultados experimentales en dos conjuntos de datos del Consorcio de Datos Lingüísticos (LDC), TDT2 y TDT3, muestran que el modelo propuesto puede mejorar significativamente tanto la eficiencia como la precisión de la tarea de NED, en comparación con el sistema base y otros <br>sistemas existentes</br>."
            ],
            "translated_text": "Detección de nuevos eventos basada en árbol de indexación y entidad nombrada Zhang Kuo Universidad Tsinghua Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Universidad Tsinghua Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Universidad Tsinghua Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn RESUMEN La Detección de Nuevos Eventos (NED) tiene como objetivo detectar de una o varias corrientes de noticias cuál se informa sobre un nuevo evento (es decir, no reportado previamente). Con el abrumador volumen de noticias disponible hoy en día, hay una creciente necesidad de un sistema NED que sea capaz de detectar eventos nuevos de manera más eficiente y precisa. En este artículo proponemos un nuevo modelo de NED para acelerar la tarea de NED mediante el uso dinámico de un árbol de indexación de noticias. Además, basándose en la observación de que los términos de diferentes tipos tienen diferentes efectos para la tarea de NED, se proponen dos enfoques de reponderación de términos para mejorar la precisión de NED. En el primer enfoque, proponemos ajustar dinámicamente los pesos de los términos basados en los clusters de historias previas y en el segundo enfoque, proponemos emplear estadísticas en los datos de entrenamiento para aprender el modelo de reajuste de entidades nombradas para cada clase de historias. Los resultados experimentales en dos conjuntos de datos del Consorcio de Datos Lingüísticos (LDC), TDT2 y TDT3, muestran que el modelo propuesto puede mejorar significativamente tanto la eficiencia como la precisión de la tarea de NED, en comparación con el sistema base y otros <br>sistemas existentes</br>. Categorías y Descriptores de Asignaturas H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información; H.4.2 [Aplicaciones de Sistemas de Información]: Tipos de Sistemas de Apoyo a Decisiones. Términos generales Algoritmos, Rendimiento, Experimentación 1. El programa de Detección y Seguimiento de Temas (TDT) tiene como objetivo desarrollar técnicas que puedan organizar, buscar y estructurar de manera efectiva materiales de texto de noticias de una variedad de agencias de noticias y medios de difusión [1]. La Detección de Eventos Nuevos (DEN) es una de las cinco tareas en TDT. Es la tarea de identificación en línea del informe más temprano para cada tema tan pronto como ese informe llegue en la secuencia de documentos. Un tema se define como un evento o actividad seminal, junto con eventos y actividades directamente relacionados [2]. Un evento se define como algo (no trivial) que sucede en un lugar y en un momento determinados [3]. Por ejemplo, cuando una bomba explota en un edificio, la explosión es el evento seminal que desencadena el tema, y otras historias sobre el mismo tema serían aquellas que discuten los esfuerzos de salvamento, la búsqueda de los perpetradores, arrestos y juicios, entre otros. La información útil de noticias suele estar enterrada en una masa de datos generados a diario. Por lo tanto, los sistemas NED son muy útiles para las personas que necesitan detectar información novedosa de un flujo de noticias en tiempo real. Estas necesidades de la vida real a menudo se presentan en ámbitos como los mercados financieros, el análisis de noticias y la recopilación de inteligencia. En la mayoría de los sistemas NED de última generación, cada noticia disponible se compara con todas las noticias previamente recibidas. Si todas las similitudes entre ellos no superan un umbral, entonces la historia desencadena un nuevo evento. Por lo general, se presentan en forma de similitud de coseno o métrica de similitud de Hellinger. El problema central de NED es identificar si dos historias tratan sobre el mismo tema. Obviamente, estos sistemas no pueden aprovechar la información del tema. Además, no es aceptable en aplicaciones reales debido a la gran cantidad de cálculos requeridos en el proceso de NED. Otros sistemas organizan historias anteriores en grupos (cada grupo corresponde a un tema), y la nueva historia se compara con los grupos anteriores en lugar de con historias. Esta forma puede reducir significativamente los tiempos de comparación. Sin embargo, se ha demostrado que este método es menos preciso [4, 5]. Esto se debe a que a veces las historias dentro de un tema se alejan mucho entre sí, lo que podría resultar en una baja similitud entre una historia y su tema. Por otro lado, algunos sistemas NED propuestos intentaron mejorar la precisión al hacer un mejor uso de las entidades nombradas [10, 11, 12, 13]. Sin embargo, ninguno de los sistemas ha considerado que los términos de diferentes tipos (por ejemplo, Los sustantivos, verbos o nombres de personas tienen diferentes efectos en diferentes clases de historias para determinar si dos historias tratan sobre el mismo tema. Por ejemplo, los nombres de los candidatos electorales (Nombre de la persona) son muy importantes para las historias de la clase de elecciones; las ubicaciones (Nombre de la ubicación) donde ocurrieron los accidentes son importantes para las historias de la clase de accidentes. Por lo tanto, en NED, todavía existen los siguientes tres problemas por investigar: (1) ¿Cómo acelerar el procedimiento de detección sin disminuir la precisión de la detección? (2) ¿Cómo aprovechar mejor la información de los grupos (temas) para mejorar la precisión? (3) ¿Cómo obtener una mejor representación de historias de noticias mediante una mejor comprensión de las entidades nombradas? Impulsados por estos problemas, hemos propuesto tres enfoques en este documento. (1) Para hacer que el procedimiento de detección sea más rápido, proponemos un nuevo procedimiento de NED basado en un árbol de indexación de noticias creado dinámicamente. El índice de historias en árbol se crea ensamblando historias similares para formar grupos de noticias en diferentes jerarquías según sus valores de similitud. Las comparaciones entre la historia actual y los grupos anteriores podrían ayudar a encontrar la historia más similar en menos tiempo de comparación. El nuevo procedimiento puede reducir la cantidad de veces de comparación sin afectar la precisión. Utilizamos los grupos del primer piso en el árbol de indexación como temas de noticias, en los cuales los pesos de los términos se ajustan dinámicamente según la distribución de términos en los grupos. En este enfoque, se utiliza adecuadamente la información de los grupos (temas), por lo que se evita el problema de la descentralización del tema. Basándonos en observaciones de las estadísticas obtenidas de los datos de entrenamiento, encontramos que los términos de diferentes tipos (por ejemplo, Los sustantivos y verbos tienen diferentes efectos para diferentes clases de historias al determinar si dos historias tratan sobre el mismo tema. Y proponemos utilizar estadísticas para optimizar los pesos de los términos de diferentes tipos en una historia de acuerdo a la clase de noticias a la que pertenece la historia. En el conjunto de datos TDT3, el nuevo modelo NED solo utiliza un 14.9% en comparación con los tiempos del modelo básico, mientras que su costo normalizado mínimo es de 0.5012, lo que es 0.0797 mejor que el modelo básico, y también mejor que cualquier otro resultado previamente reportado para este conjunto de datos [8, 13]. El resto del documento está organizado de la siguiente manera. Comenzamos este artículo resumiendo el trabajo previo en NED en la sección 2. La sección 3 presenta el modelo básico para NED que la mayoría de los sistemas actuales utilizan. La sección 4 describe nuestro nuevo procedimiento de detección basado en el índice de árbol de noticias. En la sección 5, se proponen dos métodos de reponderación de términos para mejorar la precisión de NED. La sección 6 presenta nuestros datos experimentales y métricas de evaluación. Finalmente concluimos con los resultados experimentales en la Sección 7, y las conclusiones y trabajo futuro en la Sección 8. TRABAJO RELACIONADO Papka et al. propusieron el agrupamiento de un solo paso en NED [6]. Cuando se encontraba una nueva historia, esta era procesada inmediatamente para extraer características de términos y se construía una representación de consulta del contenido de la historia. Luego se comparó con todas las consultas anteriores. Si el documento no generaba ninguna consulta al exceder un umbral, se marcaba como un nuevo evento. Lam et al construyen representaciones de consultas previas de grupos de historias, cada una de las cuales corresponde a un tema [7]. De esta manera se realizan comparaciones entre historias y grupos. En los últimos años, la mayoría de los trabajos se han centrado en proponer mejores métodos para la comparación de historias y la representación de documentos. Brants et al. [8] extendieron un modelo básico incremental de TF-IDF para incluir modelos específicos de fuente, normalización de puntajes de similitud basada en promedios específicos de documentos, normalización de puntajes de similitud basada en promedios específicos de pares de fuentes, reponderación de términos basada en frecuencias de eventos inversas y segmentación de documentos. Se mostraron buenas mejoras en los puntos de referencia de TDT. Stokes et al. [9] utilizaron una combinación de evidencia de dos representaciones distintas del contenido de un documento. Una de las representaciones era el vector de texto libre habitual, la otra hacía uso de cadenas léxicas (creadas utilizando WordNet) para construir otro vector de términos. Luego, las dos representaciones se combinan de forma lineal. Se logró un aumento marginal en la efectividad cuando se utilizó la representación combinada. Algunos esfuerzos se han realizado sobre cómo utilizar entidades nombradas para mejorar la desambiguación de entidades nombradas (NED). Yang et al. asignaron un peso cuatro veces mayor a las entidades con nombres de ubicación que a otros términos y entidades con nombres [10]. El grupo de investigación DOREMI combinó similitudes semánticas de nombres de personas, nombres de lugares y tiempo junto con similitud textual [11][12]. El grupo de investigación de UMass [13] dividió la representación del documento en dos partes: entidades nombradas y entidades no nombradas. Y se descubrió que algunas clases de noticias podían lograr un mejor rendimiento utilizando la representación de entidades nombradas, mientras que otras clases de noticias podían lograr un mejor rendimiento utilizando la representación sin entidades nombradas. Tanto [10] como [13] utilizaron la técnica de categorización de texto para clasificar previamente las noticias. En [13] las noticias se clasifican automáticamente al principio, y luego se prueban las sensibilidades de los nombres y términos que no son nombres para NED para cada clase. En [10], se eliminan los términos frecuentes de cada clase de la representación del documento. Por ejemplo, la palabra \"elección\" no ayuda a identificar diferentes elecciones. En su trabajo, no se investiga la efectividad de diferentes tipos de nombres (o términos con diferentes categorías gramaticales) para NED en diferentes clases de noticias. Utilizamos análisis estadístico para revelar el hecho y utilizarlo para mejorar el rendimiento de NED. 3. MODELO BÁSICO En esta sección, presentamos el modelo básico de Detección de Nuevos Eventos que es similar a lo que la mayoría de los sistemas actuales aplican. Luego, proponemos nuestro nuevo modelo ampliando el modelo básico. Los sistemas de detección de eventos nuevos utilizan flujos de noticias como entrada, en los cuales las historias están estrictamente ordenadas por tiempo. Solo están disponibles las historias recibidas previamente al tratar con la historia actual. La salida es una decisión sobre si la historia actual es un evento nuevo o no y la confianza de la decisión. Por lo general, un modelo NED consta de tres partes: representación de la historia, cálculo de similitud y procedimiento de detección. 3.1 Representación de la historia Se necesita preprocesamiento antes de generar la representación de la historia. Para el preprocesamiento, tokenizamos palabras, reconocemos abreviaturas, normalizamos abreviaturas, agregamos etiquetas de partes del discurso, eliminamos palabras vacías incluidas en la lista de paradas utilizada en InQuery [14], reemplazamos palabras con sus raíces utilizando el algoritmo K-stem[15], y luego generamos un vector de palabras para cada noticia. Utilizamos el modelo TF-IDF incremental para el cálculo del peso de los términos [4]. En un modelo TF-IDF, la frecuencia de término en un documento de noticias se pondera por la frecuencia inversa del documento, la cual se genera a partir del corpus de entrenamiento. Cuando surge un nuevo término en el proceso de prueba, hay dos soluciones: simplemente ignorar el nuevo término o establecer df del término como una constante pequeña (por ejemplo, df = 1). El nuevo término recibe un peso demasiado bajo en la primera solución (0) y un peso demasiado alto en la segunda solución. En el modelo TF-IDF incremental, las frecuencias de documentos se actualizan dinámicamente en cada paso de tiempo t: 1( ) ( ) ( )t t D tdf w df w df w−= + (1) donde Dt representa el conjunto de noticias recibidas en el tiempo t, y dfDt(w) significa el número de documentos en los que aparece el término w, y dft(w) significa el número total de documentos en los que aparece el término w antes del tiempo t. En este trabajo, cada ventana de tiempo incluye 50 noticias. Así, cada historia d recibida en t se representa de la siguiente manera: 1 2{ ( , , ), ( , , ),..., ( , , )}nd peso d t w peso d t w peso d t w→ donde n significa el número de términos distintos en la historia d, y ( , , )peso d t w significa el peso del término w en la historia d en el tiempo t: log( ( , ) 1) log(( 1) /( ( ) 0.5)) ( , , ) log( ( , ) 1) log(( 1) /( ( ) 0.5)) t t t t w d tf d w N df w peso d t w tf d w N df w ∈ + + + = + + +∑ (2) donde Nt significa el número total de historias de noticias antes del tiempo t, y tf(d,w) significa cuántas veces aparece el término w en la historia de noticias d. 3.2 Cálculo de Similitud Utilizamos la distancia de Hellinger para el cálculo de la similitud entre dos historias, para dos historias d y d en el tiempo t, su similitud se define de la siguiente manera: , ( , , ) ( , , ) * ( , , ) w d d sim d d t peso d t w peso d t w ∈ = ∑ (3) 3.3 Procedimiento de Detección Para cada historia d recibida en el paso de tiempo t, el valor ( ) ( ) ( ) ( ( , , )) tiempo d tiempo d n d max sim d d t < = (4) es un puntaje utilizado para determinar si d es una historia sobre un nuevo tema y al mismo tiempo es una indicación de la confianza en nuestra decisión [8]. tiempo(d) significa el tiempo de publicación de la historia d. Si el puntaje supera el umbral θ new, entonces existe un documento suficientemente similar, por lo tanto, d es una historia antigua, de lo contrario, no hay un documento previo suficientemente similar, por lo tanto, d es una historia nueva. 4. Nuevo procedimiento NED Los sistemas NED tradicionales se pueden clasificar en dos tipos principales en cuanto al procedimiento de detección: (1) tipo S-S, en el que la historia en cuestión se compara con cada historia recibida previamente, y se utiliza la mayor similitud para determinar si la historia actual trata sobre un evento nuevo; (2) tipo S-C, en el que la historia en cuestión se compara con todos los clusters anteriores, cada uno de los cuales representa un tema, y se utiliza la mayor similitud para la decisión final sobre la historia actual. Si la similitud más alta supera el umbral θ nuevo, entonces es una historia antigua y se coloca en el clúster más similar; de lo contrario, es una historia nueva y se crea un nuevo clúster. Trabajos anteriores muestran que la primera forma es más precisa que la segunda [4][5]. Dado que a veces las historias dentro de un tema se alejan mucho entre sí, una historia puede tener una similitud muy baja con su tema. Por lo tanto, utilizar similitudes entre historias para determinar una nueva historia es mejor que utilizar similitudes entre historias y grupos. Sin embargo, el primer método requiere mucho más tiempo de comparación, lo que significa que es menos eficiente. Proponemos un nuevo procedimiento de detección que utiliza comparaciones con clusters anteriores para ayudar a encontrar la historia más similar en menos tiempo de comparación, y la decisión final sobre el nuevo evento se toma de acuerdo con la historia más similar. Por lo tanto, podemos obtener tanto la precisión de los métodos de tipo S-S como la eficiencia de los métodos de tipo S-C. El nuevo procedimiento crea un árbol de indexación de noticias dinámicamente, en el cual se agrupan historias similares para formar una jerarquía de clusters. Indexamos historias similares juntas por su ancestro común (un nodo de clúster). Historias diferentes están indexadas en diferentes grupos. Cuando una historia está llegando, utilizamos comparaciones entre la historia actual y los grupos jerárquicos anteriores para ayudar a encontrar la historia más similar que es útil para la toma de decisiones sobre nuevos eventos. Después de que se tome la decisión sobre el nuevo evento, la historia actual se inserta en el árbol de indexación para la detección siguiente. El índice de noticias en forma de árbol se define formalmente de la siguiente manera: S-Tree = {r, NC, NS, E}, donde r es la raíz de S-Tree, NC es el conjunto de todos los nodos de clúster, NS es el conjunto de todos los nodos de historia, y E es el conjunto de todos los bordes en S-Tree. Definimos un conjunto de restricciones para un S-Árbol: ⅰ. , es un nodo no terminal en el árbolC i i N i∀ ∈ → ⅱ. , es un nodo terminal en el árbolS i i N i∀ ∈ → ⅲ. , el grado de salida de es al menos 2C i i N i∀ ∈ → ⅳ. , se representa como el centroide de sus descendientesC i i iN∀ ∈ → Para una noticia di, el procedimiento de comparación y el procedimiento de inserción basados en el árbol de indexación se definen de la siguiente manera. Un ejemplo se muestra en la Figura 1 y la Figura 2. Figura 1. Procedimiento de comparación Figura 2. Procedimiento de comparación de inserción: Paso 1: comparar di con todos los nodos hijos directos de r y seleccionar λ nodos con las similitudes más altas, por ejemplo, C1 2 y C1 3 en la Figura 1. Paso 2: para cada nodo seleccionado en el paso anterior, por ejemplo. C1 2, compara di con todos sus nodos hijos directos y selecciona λ nodos con las similitudes más altas, por ejemplo. C2 2 y d8. Repetir el paso 2 para todos los nodos no terminales. Paso 3: registrar el nodo terminal con la mayor similitud a di, por ejemplo, s5, y el valor de similitud (0.20). Insertando di en el S-árbol con r como raíz: Encuentra el nodo n que es hijo directo de r en el camino desde r hasta el nodo terminal con la similitud más alta s, por ejemplo. C1 2. Si s es menor que θ init + (h-1)δ, entonces agregar di al árbol como un hijo directo de r. De lo contrario, si n es un nodo terminal, crear un nodo de clúster en lugar de n, y agregar tanto n como di como sus hijos directos; si n es un nodo no terminal, repetir este procedimiento e insertar di en el subárbol con n como raíz de forma recursiva. Aquí h es la longitud entre n y la raíz del árbol S. Cuanto más similares sean las historias en un grupo, mejor representará el grupo a las historias en él. Por lo tanto, no imponemos restricciones en la altura máxima de los árboles y el grado de un nodo. Por lo tanto, no podemos dar la complejidad de este procedimiento basado en árboles de indexación. Pero proporcionaremos el número de veces de comparación necesarias por el nuevo procedimiento en nuestros experimentos en la sección 7.5. En esta sección, se proponen dos métodos de reponderación de términos para mejorar la precisión de NED. En el primer método, se explora una nueva forma de utilizar de manera más efectiva la información de los grupos (temas). El segundo encuentra una mejor manera de utilizar entidades nombradas basadas en la clasificación de noticias. 5.1 Reajuste de términos basado en la distancia de distribución TF-IDF es el modelo más prevalente utilizado en sistemas de recuperación de información. La idea básica es que cuanto menos documentos aparezca un término, más importante es el término en la discriminación de documentos (relevantes o no relevantes para una consulta que contenga el término). Sin embargo, en el dominio de TDT, necesitamos discriminar documentos en función de los temas en lugar de las consultas. Intuitivamente, el uso de vectores de clúster (tema) para comparar con historias de noticias posteriores debería tener un mejor rendimiento que el uso de vectores de historias. Desafortunadamente, los resultados experimentales no respaldan esta intuición [4][5]. Basándonos en la observación de los datos, encontramos que la razón es que un tema de noticias generalmente contiene muchos eventos directa o indirectamente relacionados, y todos ellos tienen sus propios subtemas que suelen ser diferentes entre sí. Tomemos el tema descrito en la sección 1 como ejemplo, eventos como la explosión y el rescate tienen similitudes muy bajas con eventos sobre juicios criminales, por lo tanto, las historias sobre juicios tendrían una baja similitud con el vector de tema construido en base a sus eventos previos. Esta sección se centra en cómo hacer un uso efectivo de la información del tema y al mismo tiempo evitar el problema de la descentralización del contenido. Al principio, clasificamos los términos en 5 clases para ayudar en el análisis de las necesidades del modelo modificado: Clase de términos A: términos que ocurren con frecuencia en todo el corpus, por ejemplo, año y personas. Los términos de esta clase deben recibir pesos bajos porque no ayudan mucho en la discriminación de temas. Clase B de términos: términos que ocurren con frecuencia dentro de una categoría de noticias, por ejemplo, elección, tormenta. Son útiles para distinguir dos historias en diferentes categorías de noticias. Sin embargo, no pueden proporcionar información para determinar si dos historias tratan sobre el mismo tema o temas diferentes. En otras palabras, los términos elección y tormenta no son útiles para diferenciar dos campañas electorales y dos desastres de tormenta. Por lo tanto, los términos de esta clase deberían asignarse pesos más bajos. Clase de término C: términos que ocurren con frecuencia en un tema y raramente en otros temas, por ejemplo, el nombre de un avión accidentado, el nombre de un huracán específico. Las noticias que pertenecen a diferentes temas rara vez tienen términos superpuestos en esta clase. Cuanto más frecuentemente aparezca un término en un tema, más importante es el término para una historia perteneciente al tema, por lo tanto, el término debería tener un peso más alto. Clase de término D: términos que aparecen en un tema de manera exclusiva, pero no con frecuencia. Por ejemplo, el nombre de un bombero que se desempeñó muy bien en una acción de salvamento, que puede aparecer en solo dos o tres historias pero nunca ha aparecido en otros temas. Los términos de este tipo deberían recibir más peso que en el modelo TF-IDF. Sin embargo, dado que no son populares en el tema, no es apropiado asignarles pesos demasiado altos. Clase de término E: términos con baja frecuencia en documentos y que aparecen en diferentes temas. Los términos de esta clase deberían recibir pesos más bajos. Ahora analizamos si el modelo TF-IDF puede asignar pesos adecuados a las cinco clases de términos. Obviamente, los términos de la clase A tienen un peso bajo en el modelo TF-IDF, lo cual es acorde con el requisito descrito anteriormente. En el modelo TF-IDF, los términos de la clase B dependen altamente del número de historias en una clase de noticias. El modelo TF-IDF no puede proporcionar pesos bajos si la historia que contiene el término pertenece a una clase de noticias relativamente pequeña. Para un término de clase C, cuanto más frecuentemente aparezca en un tema, menos peso le da el modelo TFIDF. Esto entra en conflicto directo con el requisito de términos en la clase C. Para los términos de la clase D, el modelo TF-IDF les asigna pesos altos de manera correcta. Pero para los términos de la clase E, el modelo TF-IDF asigna pesos altos a ellos que no son conformes con el requisito de pesos bajos. En resumen, los términos de clase B, C, E no pueden ser ponderados adecuadamente en el modelo TF-IDF. Por lo tanto, proponemos un modelo modificado para resolver este problema. Cuando θ init y θ new están configurados de cerca, asumimos que la mayoría de las historias en un clúster de primer nivel (un nodo hijo directo del nodo raíz) están en el mismo tema. Por lo tanto, utilizamos un clúster de primer nivel para capturar la distribución de términos (df para todos los términos dentro del clúster) dentro del tema de forma dinámica. La divergencia de Kullback-Leibler de la distribución de términos en un clúster de primer nivel y el conjunto completo de historias se utiliza para ajustar los pesos de los términos: ( , , ) * (1 * ( || )) ( , , ) ( , , ) * (1 * ( || )) cw tw cw tw w d D peso d t w KL P P peso d t w peso d t w KL P P γ γ ∈ + = +∑ (5) donde ( ) ( ) ( ) ( ) 1,cw cw c c c c df w df w p y p y N N = = − (6) ( ) ( ) ( ) ( ) 1,t t tw tw t t df w df w p y p y N N = = − (7) donde dfc(w) es el número de documentos que contienen el término w dentro del clúster C, y Nc es el número de documentos en el clúster C, y Nt es el número total de documentos que llegan antes del paso de tiempo t. γ es un parámetro constante, actualmente configurado manualmente en 3. La divergencia de Kullback-Leibler se define de la siguiente manera [17]: La idea básica es que, para una historia en un tema, cuanto más ocurre un término dentro del tema y menos ocurre en otros temas, se le deben asignar pesos más altos. Obviamente, el modelo modificado puede cumplir con todos los requisitos de las cinco clases de términos mencionadas anteriormente. 5.2 Reajuste de Términos Basado en el Tipo de Término y la Clase de Historia. Trabajos anteriores encontraron que algunas clases de historias de noticias podrían lograr mejoras significativas al dar un peso adicional a las entidades nombradas. Pero encontramos que los términos de diferentes tipos deben recibir una cantidad diferente de peso adicional para diferentes clases de noticias. Utilizamos open-NLP1 para reconocer tipos de entidades nombradas y etiquetas de partes del discurso para términos que aparecen en noticias. Los tipos de entidades nombradas incluyen nombre de persona, nombre de organización, nombre de ubicación, fecha, hora, dinero y porcentaje, y se seleccionan cinco partes del discurso: ninguno (NN), verbo (VB), adjetivo (JJ), adverbio (RB) y número cardinal (CD). El análisis estadístico muestra tipos de términos discriminativos a nivel de tema para diferentes clases de historias. Por conveniencia, los tipos de entidad nombrada y etiquetas de parte de la oración se llaman uniformemente tipo de término en las secciones siguientes. Determinar si dos historias tratan sobre el mismo tema es un componente básico para la tarea de NED. Por lo tanto, al principio usamos la estadística χ² para calcular las correlaciones entre términos y temas. Para un término t y un tema T, se deriva una tabla de contingencia: Tabla 1. Una tabla de contingencia de 2×2 con el número de documentos pertenecientes al tema T y no pertenecientes al tema T incluye t A B no incluye t C D. La estadística χ² para un término específico t con respecto al tema T se define como [16]: χ² = ( ) ( ) * ( ) ( ) * ( ) * ( ) * ( ) w T A B C D AD CB A C B D A B C D + + + − + + + + (9). Los temas de noticias para la tarea TDT se clasifican además en 11 reglas de interpretaciones (ROIs) 2. El ROI se puede ver como una clase de historias de nivel superior. El promedio de correlación entre un tipo de término y un ROI de tema se calcula como: 2 avg 2 ( , )( ( , ) )k m m km kT R w P w TP R p w T R P χ χ ∈ ∈ ∑ ∑（ , ）= 1 1 k=1…K, m=1…M (10) donde K es el número de tipos de términos (fijado constantemente en 12 en el artículo). M es el número de nuevas clases (ROIs, establecido en el conjunto 11 del artículo). Pk representa el conjunto de todos los términos de tipo k, y Rm representa el conjunto de todos los temas de la clase m, p(t,T) significa la probabilidad de que t ocurra en el tema T. Debido a limitaciones de espacio, solo se enumeran partes de los tipos de términos (9 tipos de términos) y partes de las clases de noticias (8 clases) en la tabla 2 con los valores promedio de correlación entre ellos. Las estadísticas se derivan de datos etiquetados en el corpus TDT2. (Los resultados en la tabla 2 ya están normalizados para mayor comodidad en la comparación). Las estadísticas en la tabla 2 indican la utilidad de diferentes tipos de términos en la discriminación de temas con respecto a diferentes clases de noticias. Podemos ver que el nombre de la ubicación es el tipo de término más útil para tres clases de noticias: Desastres Naturales, Violencia o Guerra, Finanzas. Y para otras tres categorías: Elecciones, Casos Legales/Criminales, Ciencia y Descubrimiento, el nombre de la persona es el tipo de término más discriminativo. Para Escándalos/Audiencias, la fecha es la información más importante para la discriminación de temas. Además, los casos legales/criminales y los temas financieros tienen una correlación más alta con términos relacionados con el dinero, mientras que la ciencia y el descubrimiento tienen una correlación más alta con términos de porcentaje. Los términos no nominales son más estables para diferentes clases. 1. http://opennlp.sourceforge.net/ 2. http://projects.ldc.upenn.edu/TDT3/Guide/label.html A partir del análisis de la tabla 2, es razonable ajustar el peso de los términos según su tipo de término y la clase de noticias a la que pertenece la historia. Los nuevos pesos de los términos se recalculan de la siguiente manera: ( ) ( ) ( ) ( ) ( , , ) * ( , , ) ( , , ) * clase d D tipo w T clase d D tipo w w d peso d t w peso d t w peso d t w α α ∈ = ∑ (11) donde type(w) representa el tipo del término w, y class(d) representa la clase de la historia d, c kα es el parámetro de recalibración para la clase de noticias c y el tipo de término k. En el trabajo, simplemente utilizamos las estadísticas de la tabla 2 como parámetros de recalibración. Aunque usar las estadísticas directamente puede que no sea la mejor opción, no discutimos cómo obtener automáticamente los mejores parámetros. Intentaremos utilizar técnicas de aprendizaje automático para obtener los mejores parámetros en el trabajo futuro. En el trabajo, utilizamos BoosTexter [20] para clasificar todas las historias en uno de los 11 ROIs. BoosTexter es un programa de aprendizaje automático basado en boosting, que crea una serie de reglas simples para construir un clasificador de datos de texto o atributos-valor. Utilizamos el peso del término generado utilizando el modelo TF-IDF como característica para la clasificación de historias. Entrenamos el modelo con las 12000 historias en inglés evaluadas en TDT2, y clasificamos el resto de las historias en TDT2 y todas las historias en TDT3. Los resultados de clasificación se utilizan para la reponderación de términos en la fórmula (11). Dado que las etiquetas de clase de las historias de tema apagado no se proporcionan en los conjuntos de datos de TDT, no podemos proporcionar la precisión de clasificación aquí. Por lo tanto, no discutimos los efectos de la precisión de la clasificación en el rendimiento de NED en el artículo. 6. CONFIGURACIÓN EXPERIMENTAL 6.1 Conjuntos de datos Utilizamos dos conjuntos de datos de LDC [18], TDT2 y TDT3, para nuestros experimentos. TDT2 contiene noticias desde enero hasta junio de 1998. Contiene alrededor de 54,000 historias de fuentes como ABC, Associated Press, CNN, New York Times, Public Radio International, Voice of America, etc. Solo se consideraron historias en inglés en la colección. TDT3 contiene aproximadamente 31,000 historias en inglés recopiladas de octubre a diciembre de 1998. Además de las fuentes utilizadas en TDT2, también contiene historias de las transmisiones de televisión de NBC y MSNBC. Utilizamos versiones transcritas de las emisiones de televisión y radio además de noticias textuales. El conjunto de datos TDT2 está etiquetado con alrededor de 100 temas, y aproximadamente 12,000 historias en inglés pertenecen al menos a uno de estos temas. El conjunto de datos TDT3 está etiquetado con alrededor de 120 temas, y aproximadamente 8000 historias en inglés pertenecen al menos a uno de estos temas. Todos los temas están clasificados en 11 Reglas de Interpretación: (1) Elecciones, (2) Escándalos/Audiencias, (3) Casos Legales/Criminales, (4) Desastres Naturales, (5) Accidentes, (6) Violencia o Guerra en Curso, (7) Noticias de Ciencia y Descubrimientos, (8) Finanzas, (9) Nueva Ley, (10) Noticias Deportivas, (11) Varios. Noticias. 6.2 Métrica de evaluación TDT utiliza una función de costo CDet que combina las probabilidades de perder una nueva historia y una falsa alarma [19]: * * * *Det Miss Miss Target FA FA NontargetC C P P C P P= + (12) Tabla 2. Correlación promedio entre tipos de términos y clases de noticias donde CMiss significa el costo de perder una nueva historia, PMiss significa la probabilidad de perder una nueva historia, y PTarget significa la probabilidad de ver una nueva historia en los datos; CFA significa el costo de una falsa alarma, PFA significa la probabilidad de una falsa alarma, y PNontarget significa la probabilidad de ver una historia antigua. El costo CDet se normaliza de tal manera que un sistema perfecto obtiene una puntuación de 0 y un sistema trivial, que es el mejor de marcar todas las historias como nuevas o antiguas, obtiene una puntuación de 1: ( ( * , * ) ) Det Det Miss Target FA Nontarget C Norm C min C P C P = (13) El sistema de detección de eventos nuevos proporciona dos salidas para cada historia. La primera parte es sí o no, indicando si la historia desencadena un nuevo evento o no. La segunda parte es una puntuación que indica la confianza de la primera decisión. Los puntajes de confianza se pueden utilizar para trazar la curva DET, es decir, curvas que representan la probabilidad de falsa alarma frente a la probabilidad de omisión. El costo normalizado mínimo se puede determinar si se elige el umbral óptimo en la puntuación. RESULTADOS EXPERIMENTALES 7.1 Resultados principales Para probar los enfoques propuestos en el modelo, implementamos y probamos cinco sistemas: Sistema-1: este sistema se utiliza como referencia. Se implementa basándose en el modelo básico descrito en la sección 3, es decir, utilizando el modelo TF-IDF incremental para generar pesos de términos y utilizando la distancia de Hellinger para calcular la similitud entre documentos. La normalización del puntaje de similitud también se emplea [8]. Se utiliza el procedimiento de detección S-S. Sistema-2: este sistema es igual que el sistema-1 excepto que se utiliza el procedimiento de detección S-C. Sistema-3: este sistema es igual que el sistema-1 excepto que utiliza el nuevo procedimiento de detección que se basa en un árbol de indexación. Sistema-4: implementado basado en el enfoque presentado en la sección 5.1, es decir, los términos se vuelven a ponderar según la distancia entre las distribuciones de términos en un grupo y todas las historias. Se utiliza el nuevo procedimiento de detección. Sistema-5: implementado basado en el enfoque presentado en la sección 5.2, es decir, los términos de diferentes tipos son reponderados según la clase de noticias utilizando parámetros entrenados. El nuevo procedimiento de detección se utiliza. Los siguientes son algunos otros sistemas NED: Sistema-6: [21] para cada par de historias, calcula tres valores de similitud para entidad nombrada, entidad no nombrada y todos los términos respectivamente. Y emplear Máquina de Vectores de Soporte para predecir nuevo o antiguo utilizando los valores de similitud como características. Sistema-7: [8] extendió un modelo básico incremental de TF-IDF para incluir modelos específicos de origen, normalización de puntuaciones de similitud basada en promedios específicos de documentos, normalización de puntuaciones de similitud basada en promedios específicos de pares de fuentes, etc. Sistema-8: [13] dividió la representación del documento en dos partes: entidades nombradas y entidades no nombradas, y eligió una parte efectiva para cada clase de noticias. La tabla 3 y la tabla 4 muestran los costos normalizados ponderados por tema y los tiempos de comparación en los conjuntos de datos TDT2 y TDT3 respectivamente. Dado que no se contaba con un conjunto de datos de validación para ajustar el umbral θ nuevo en los experimentos en TDT2, solo informamos los costos normalizados mínimos para nuestros sistemas en la tabla 3. El sistema-5 supera a todos los demás sistemas, incluido el sistema-6, y realiza solo 2.78e+8 veces la comparación de tiempos en el procedimiento de detección, lo que representa solo el 13.4% del sistema-1. Tabla 3. Los resultados de NED en los sistemas TDT2 Min Norm(CDet) tiempos de comparación son: Sistema-1 0.5749 2.08e+9, Sistema-2 0.6673 3.77e+8, Sistema-3 0.5765 2.81e+8, Sistema-4 0.5431 2.99e+8, Sistema-5 0.5089 2.78e+8, Sistema-6 0.5300. Al evaluar en los costos normalizados en TDT3, utilizamos los umbrales óptimos obtenidos del conjunto de datos de TDT2 para todos los sistemas. El sistema-2 reduce los tiempos de comparación a 1.29e+9, que es solo el 18.3% del sistema-1, pero al mismo tiempo también obtiene un costo normalizado mínimo deteriorado que es 0.0499 más alto que el sistema-1. El Sistema-3 utiliza el nuevo procedimiento de detección basado en el índice de noticias en árbol. Requiere incluso menos tiempo de comparación que el sistema-2. Esto se debe a que las comparaciones de historias suelen arrojar mayores similitudes que las de grupos de historias, por lo que las historias tienden a combinarse en el sistema-3. Y el sistema-3 es básicamente equivalente al sistema-1 en los resultados de precisión. El System-4 ajusta los pesos de los términos basándose en la distancia de las distribuciones de términos entre todo el corpus y el conjunto de historias del clúster, lo que resulta en una mejora significativa de 0.0468 en comparación con el system-1. El mejor sistema (sistema-5) tiene un costo normalizado mínimo de 0.5012, lo cual es 0.0797 mejor que el sistema-1, y también mejor que cualquier otro resultado previamente reportado para este conjunto de datos [8, 13]. Además, el sistema-5 solo necesita 1.05e+8 veces de comparación, lo que equivale al 14.9% del sistema-1. Tabla 4. Los resultados de NED en los sistemas TDT3 Norm(CDet) Norma mínima(CDet) Tiempos de comparación Sistema-1 0.6159 0.5809 7.04e+8 Sistema-2① 0.6493 0.6308 1.29e+8 Sistema-3② 0.6197 0.5868 1.03e+8 Sistema-4② 0.5601 0.5341 1.03e+8 Sistema-5② 0.5413 0.5012 1.05e+8 Sistema-7 -- 0.5783 -Sistema-8 -- 0.5229 -① θ nuevo=0.13 ② θ inicial=0.13, λ =3, δ =0.15 La Figura 5 muestra las cinco curvas DET para nuestros sistemas en el conjunto de datos TDT3. El Sistema-5 logra el costo mínimo con una tasa de falsas alarmas de 0.0157 y una tasa de omisión de 0.4310. Podemos observar que System4 y System-5 obtienen una menor probabilidad de error de omisión en regiones de baja probabilidad de falsa alarma. La hipótesis es que se transfiere más valor de peso a los términos clave de los temas que a los términos no clave. La puntuación de similitud entre dos historias pertenecientes a diferentes temas es más baja que antes, porque los términos superpuestos suelen no ser términos clave de sus temas. 7.2 Selección de parámetros para la detección de árboles de indexación. La Figura 3 muestra los costos normalizados mínimos obtenidos por el sistema-3 en TDT3 utilizando diferentes parámetros. El parámetro de inicio θ se prueba en seis valores que van desde 0.03 hasta 0.18. Y el parámetro λ se prueba en cuatro valores: 1, 2, 3 y 4. Podemos ver que, cuando θ inicial se establece en 0.12, que es el más cercano a θ nuevo, los costos son más bajos que los demás. Esto es fácil de explicar, porque cuando se agrupan historias pertenecientes al mismo tema en un conjunto, es más razonable que el conjunto represente las historias en él. Cuando el parámetro λ se establece en 3 o 4, los costos son mejores que en otros casos, pero no hay mucha diferencia entre 3 y 4. 0 0.05 0.1 0.15 0.2 1 2 3 4 0.5 0.6 0.7 0.8 0.9 1 θ-initλ MinCost 0.6 0.65 0.7 0.75 0.8 0.85 0.9 Figura 3. Costo mínimo en TDT3 (δ =0.15) 0 0.05 0.1 0.15 0.2 1 2 3 4 0 0.5 1 1.5 2 2.5 x 10 8 θ-inicio λ Comparando tiempos 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 8 Figura 4. Comparando los tiempos en TDT3 (δ =0.15), la Figura 4 muestra los tiempos de comparación utilizados por el sistema-3 en TDT3 con los mismos parámetros que la Figura 3. Los tiempos de comparación dependen fuertemente de θ init. Debido a que cuanto mayor sea θ init, menos historias se combinan juntas, más veces de comparación se necesitan para la decisión de un nuevo evento. Por lo tanto, usamos θ init = 0.13, λ = 3, δ = 0.15 para los sistemas 3, 4 y 5. En esta configuración de parámetros, podemos obtener tanto costos normalizados mínimos bajos como menos tiempos de comparación. CONCLUSIÓN Hemos propuesto un procedimiento de detección basado en un árbol de indexación de noticias en nuestro modelo. Reduce los tiempos de comparación a aproximadamente una séptima parte del método tradicional sin afectar la precisión de NED. También hemos presentado dos extensiones al modelo básico TF-IDF. La primera extensión se realiza ajustando los pesos de los términos basados en las distribuciones de términos entre todo el corpus y un conjunto de historias de un clúster. Y la segunda extensión al modelo básico TF-IDF es un mejor uso de los tipos de términos (tipos de entidades nombradas y partes de la velocidad) de acuerdo con las categorías de noticias. Nuestros resultados experimentales en los conjuntos de datos TDT2 y TDT3 muestran que ambas extensiones contribuyen significativamente a la mejora en la precisión. No consideramos la información de tiempo de noticias como una pista para la tarea de NED, ya que la mayoría de los temas duran mucho tiempo y los conjuntos de datos de TDT solo abarcan un período relativamente corto (no más de 6 meses). Para el trabajo futuro, queremos recopilar un conjunto de noticias que abarque un período más largo de internet e integrar información temporal en la tarea de NED. Dado que el tema es un grupo de noticias relativamente grueso, también queremos refinar la granularidad del grupo a nivel de evento e identificar diferentes eventos y sus relaciones dentro de un tema. Agradecimientos: Este trabajo ha sido apoyado por la Fundación Nacional de Ciencias Naturales de China bajo la subvención No. 90604025. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son del autor(es) y no necesariamente reflejan las del patrocinador. REFERENCIAS [1] http://www.nist.gov/speech/tests/tdt/index.htm [2] En Detección y Seguimiento de Temas. Organización de la información basada en eventos. Kluwer Academic Publishers, 2002. .01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90 1 2 5 10 20 40 60 80 90 Probabilidad de Falsa Alarma (en %) Probabilidad de Omisión (en %) Curva Ponderada por Tema del SISTEMA1 Norma Mínima (Costo) del SISTEMA1 Curva Ponderada por Tema del SISTEMA2 Norma Mínima (Costo) del SISTEMA2 Curva Ponderada por Tema del SISTEMA3 Norma Mínima (Costo) del SISTEMA3 Curva Ponderada por Tema del SISTEMA4 Norma Mínima (Costo) del SISTEMA4 Curva Ponderada por Tema del SISTEMA5 Norma Mínima (Costo) del SISTEMA5 Rendimiento Aleatorio Figura 5. Curvas DET en TDT3 [3] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B.T. Archibald y X. Liu. Enfoques de aprendizaje para detectar y rastrear eventos de noticias. En el número especial de IEEE Intelligent Systems sobre Aplicaciones de Recuperación de Información Inteligente, volumen 14 (4), 1999, páginas 32-43. [4] Y. Yang, T. Pierce y J. Carbonell. Un estudio sobre la detección de eventos retrospectivos y en línea. En Actas de SIGIR-98, Melbourne, Australia, 1998, 28-36. [5] J. Allan, V. Lavrenko, D. Malin y R. Swan. Detecciones, límites y líneas de tiempo: Umass y tdt-3. En Actas del Taller de Detección y Seguimiento de Temas (TDT-3), Viena, VA, 2000, 167-174. [6] R. Papka y J. Allan. Detección de nuevos eventos en línea utilizando agrupamiento de un solo paso TÍTULO2:. Informe técnico UM-CS1998-021, 1998. [7] W. Lam, H. Meng, K. Wong y J. I'm sorry, but \"Yen\" is not a sentence. Can you please provide a sentence for me to translate into Spanish? Utilizando el Análisis Contextual para la Detección de Eventos de Noticias. Revista Internacional de Sistemas Inteligentes, 2001, 525-546. [8] B. Thorsten, C. Francine y F. Ayman. Un sistema para la detección de nuevos eventos. En Actas de la 26ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2003, 330-337. [9] S. Nicola y C. Joe. Combinando clasificadores semánticos y sintácticos de documentos para mejorar la detección de primeras noticias. En Actas de la 24ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell, and C. Jin. \n\nACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell y C. Jin. Detección de novedades condicionada al tema. En Actas de la 8ª Conferencia Internacional de ACM SIGKDD, ACM Press. 2002, 688-693. [11] M. Juha, A.M. Helena y S. Marko. Aplicando Clases Semánticas en la Detección y Seguimiento de Eventos. En Actas de la Conferencia Internacional sobre Procesamiento del Lenguaje Natural (ICON 2002), 2002, páginas 175-183. [12] M. Juha, A.M. Helena y S. Marko. Semántica simple en la detección y seguimiento de temas. Recuperación de información, 7(3-4): 2004, 347-368. [13] K. Giridhar y J. Allan. Clasificación de texto y entidades nombradas para la detección de nuevos eventos. En Actas de la 27ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2004, 297-304. [14] J. P. Callan, W. B. Croft, y S. M. Harding. El Sistema de Recuperación INQUERY. En Actas de DEXA-92, 3ra Conferencia Internacional sobre Aplicaciones de Bases de Datos y Sistemas Expertos, 1992, 78-83. [15] R. Krovetz. Viendo la morfología como un proceso de inferencia. En Actas de ACM SIGIR93, 1993, 61-81. [16] Y. Yang y J. Pedersen. Un estudio comparativo sobre la selección de características en la categorización de textos. En J. D. H. Fisher, editor, La Decimocuarta Conferencia Internacional sobre Aprendizaje Automático (ICML97), Morgan Kaufmann, 1997, 412-420. [17] T. M. Cover y J. A. I'm sorry, but \"Thomas\" is not a sentence. Can you provide more context or a complete sentence for me to translate into Spanish? Elementos de la teoría de la información. Wiley. 1991. [18] El consorcio de datos lingüísticos, http://www.ldc,upenn.edu/. [19] La definición de la tarea TDT 2001 y el plan de evaluación, http://www.nist.gov/speech/tests/tdt/tdt2001/evalplan.htm. [20] R. E. Schapire y Y. Cantante. Boostexter: Un sistema basado en Boosting para la categorización de texto. En Machine Learning 39(2/3):1, Kluwer Academic Publishers, 2000, 35-168. [21] K. Giridhar y J. Allan. 2005. Utilizando nombres y temas para la detección de nuevos eventos. En Actas de la Conferencia de Tecnología Humana y la Conferencia sobre Métodos Empíricos en Lenguaje Natural, Vancouver, 2005, 121-128 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "topic detection and track": {
            "translated_key": "detección y seguimiento de temas",
            "is_in_text": false,
            "original_annotated_sentences": [
                "New Event Detection Based on Indexing-tree and Named Entity Zhang Kuo Tsinghua University Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Tsinghua University Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Tsinghua University Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn ABSTRACT New Event Detection (NED) aims at detecting from one or multiple streams of news stories that which one is reported on a new event (i.e.",
                "not reported previously).",
                "With the overwhelming volume of news available today, there is an increasing need for a NED system which is able to detect new events more efficiently and accurately.",
                "In this paper we propose a new NED model to speed up the NED task by using news indexing-tree dynamically.",
                "Moreover, based on the observation that terms of different types have different effects for NED task, two term reweighting approaches are proposed to improve NED accuracy.",
                "In the first approach, we propose to adjust term weights dynamically based on previous story clusters and in the second approach, we propose to employ statistics on training data to learn the named entity reweighting model for each class of stories.",
                "Experimental results on two Linguistic Data Consortium (LDC) datasets TDT2 and TDT3 show that the proposed model can improve both efficiency and accuracy of NED task significantly, compared to the baseline system and other existing systems.",
                "Categories and Subject Descriptors H.3.3 [Information Systems]: Information Search and Retrieval; H.4.2 [Information Systems Applications]: Types of Systemsdecision support.",
                "General Terms Algorithms, Performance, Experimentation 1.",
                "INTRODUCTION Topic Detection and Tracking (TDT) program aims to develop techniques which can effectively organize, search and structure news text materials from a variety of newswire and broadcast media [1].",
                "New Event Detection (NED) is one of the five tasks in TDT.",
                "It is the task of online identification of the earliest report for each topic as soon as that report arrives in the sequence of documents.",
                "A Topic is defined as a seminal event or activity, along with directly related events and activities [2].",
                "An Event is defined as something (non-trivial) happening in a certain place at a certain time [3].",
                "For instance, when a bomb explodes in a building, the exploding is the seminal event that triggers the topic, and other stories on the same topic would be those discussing salvaging efforts, the search for perpetrators, arrests and trial and so on.",
                "Useful news information is usually buried in a mass of data generated everyday.",
                "Therefore, NED systems are very useful for people who need to detect novel information from real-time news stream.",
                "These real-life needs often occur in domains like financial markets, news analysis, and intelligence gathering.",
                "In most of state-of-the-art (currently) NED systems, each news story on hand is compared to all the previous received stories.",
                "If all the similarities between them do not exceed a threshold, then the story triggers a new event.",
                "They are usually in the form of cosine similarity or Hellinger similarity metric.",
                "The core problem of NED is to identify whether two stories are on the same topic.",
                "Obviously, these systems cannot take advantage of topic information.",
                "Further more, it is not acceptable in real applications because of the large amount of computation required in the NED process.",
                "Other systems organize previous stories into clusters (each cluster corresponds to a topic), and new story is compared to the previous clusters instead of stories.",
                "This manner can reduce comparing times significantly.",
                "Nevertheless, it has been proved that this manner is less accurate [4, 5].",
                "This is because sometimes stories within a topic drift far away from each other, which could lead low similarity between a story and its topic.",
                "On the other hand, some proposed NED systems tried to improve accuracy by making better use of named entities [10, 11, 12, 13].",
                "However, none of the systems have considered that terms of different types (e.g.",
                "Noun, Verb or Person name) have different effects for different classes of stories in determining whether two stories are on the same topic.",
                "For example, the names of election candidates (Person name) are very important for stories of election class; the locations (Location name) where accidents happened are important for stories of accidents class.",
                "So, in NED, there still exist following three problems to be investigated: (1) How to speed up the detection procedure while do not decrease the detection accuracy? (2) How to make good use of cluster (topic) information to improve accuracy? (3) How to obtain better news story representation by better understanding of named entities.",
                "Driven by these problems, we have proposed three approaches in this paper. (1)To make the detection procedure faster, we propose a new NED procedure based on news indexing-tree created dynamically.",
                "Story indexing-tree is created by assembling similar stories together to form news clusters in different hierarchies according to their values of similarity.",
                "Comparisons between current story and previous clusters could help find the most similar story in less comparing times.",
                "The new procedure can reduce the amount of comparing times without hurting accuracy. (2)We use the clusters of the first floor in the indexing-tree as news topics, in which term weights are adjusted dynamically according to term distribution in the clusters.",
                "In this approach, cluster (topic) information is used properly, so the problem of theme decentralization is avoided. (3)Based on observations on the statistics obtained from training data, we found that terms of different types (e.g.",
                "Noun and Verb) have different effects for different classes of stories in determining whether two stories are on the same topic.",
                "And we propose to use statistics to optimize the weights of the terms of different types in a story according to the news class that the story belongs to.",
                "On TDT3 dataset, the new NED model just uses 14.9% comparing times of the basic model, while its minimum normalized cost is 0.5012, which is 0.0797 better than the basic model, and also better than any other results previously reported for this dataset [8, 13].",
                "The rest of the paper is organized as follows.",
                "We start off this paper by summarizing the previous work in NED in section 2.",
                "Section 3 presents the basic model for NED that most current systems use.",
                "Section 4 describes our new detection procedure based on news indexing-tree.",
                "In section 5, two term reweighting methods are proposed to improve NED accuracy.",
                "Section 6 gives our experimental data and evaluation metrics.",
                "We finally wrap up with the experimental results in Section 7, and the conclusions and future work in Section 8. 2.",
                "RELATED WORK Papka et al. proposed Single-Pass clustering on NED [6].",
                "When a new story was encountered, it was processed immediately to extract term features and a query representation of the storys content is built up.",
                "Then it was compared with all the previous queries.",
                "If the document did not trigger any queries by exceeding a threshold, it was marked as a new event.",
                "Lam et al build up previous query representations of story clusters, each of which corresponds to a topic [7].",
                "In this manner comparisons happen between stories and clusters.",
                "Recent years, most work focus on proposing better methods on comparison of stories and document representation.",
                "Brants et al. [8] extended a basic incremental TF-IDF model to include sourcespecific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, term reweighting based on inverse event frequencies, and segmentation of documents.",
                "Good improvements on TDT bench-marks were shown.",
                "Stokes et al. [9] utilized a combination of evidence from two distinct representations of a documents content.",
                "One of the representations was the usual free text vector, the other made use of lexical chains (created using WordNet) to build another term vector.",
                "Then the two representations are combined in a linear fashion.",
                "A marginal increase in effectiveness was achieved when the combined representation was used.",
                "Some efforts have been done on how to utilize named entities to improve NED.",
                "Yang et al. gave location named entities four times weight than other terms and named entities [10].",
                "DOREMI research group combined semantic similarities of person names, location names and time together with textual similarity [11][12].",
                "UMass [13] research group split document representation into two parts: named entities and non-named entities.",
                "And it was found that some classes of news could achieve better performance using named entity representation, while some other classes of news could achieve better performance using non-named entity representation.",
                "Both [10] and [13] used text categorization technique to classify news stories in advance.",
                "In [13] news stories are classified automatically at first, and then test sensitivities of names and non-name terms for NED for each class.",
                "In [10] frequent terms for each class are removed from document representation.",
                "For example, word election does not help identify different elections.",
                "In their work, effectiveness of different kinds of names (or terms with different POS) for NED in different news classes are not investigated.",
                "We use statistical analysis to reveal the fact and use it to improve NED performance. 3.",
                "BASIC MODEL In this section, we present the basic New Event Detection model which is similar to what most current systems apply.",
                "Then, we propose our new model by extending the basic model.",
                "New Event Detection systems use news story stream as input, in which stories are strictly time-ordered.",
                "Only previously received stories are available when dealing with current story.",
                "The output is a decision for whether the current story is on a new event or not and the confidence of the decision.",
                "Usually, a NED model consists of three parts: story representation, similarity calculation and detection procedure. 3.1 Story Representation Preprocessing is needed before generating story representation.",
                "For preprocessing, we tokenize words, recognize abbreviations, normalize abbreviations, add part-of-speech tags, remove stopwords included in the stop list used in InQuery [14], replace words with their stems using K-stem algorithm[15], and then generate word vector for each news story.",
                "We use incremental TF-IDF model for term weight calculation [4].",
                "In a TF-IDF model, term frequency in a news document is weighted by the inverse document frequency, which is generated from training corpus.",
                "When a new term occurs in testing process, there are two solutions: simply ignore the new term or set df of the term as a small const (e.g. df = 1).",
                "The new term receives too low weight in the first solution (0) and too high weight in the second solution.",
                "In incremental TF-IDF model, document frequencies are updated dynamically in each time step t: 1( ) ( ) ( )t t D tdf w df w df w−= + (1) where Dt represents news story set received in time t, and dfDt(w) means the number of documents that term w occurs in, and dft(w) means the total number of documents that term w occurs in before time t. In this work, each time window includes 50 news stories.",
                "Thus, each story d received in t is represented as follows: 1 2{ ( , , ), ( , , ),..., ( , , )}nd weight d t w weight d t w weight d t w→ where n means the number of distinct terms in story d, and ( , , )weight d t w means the weight of term w in story d at time t: log( ( , ) 1) log(( 1) /( ( ) 0.5)) ( , , ) log( ( , ) 1) log(( 1) /( ( ) 0.5)) t t t t w d tf d w N df w weight d t w tf d w N df w ∈ + + + = + + +∑ (2) where Nt means the total number of news stories before time t, and tf(d,w) means how many times term w occurs in news story d. 3.2 Similarity Calculation We use Hellinger distance for the calculation of similarity between two stories, for two stories d and d at time t, their similarity is defined as follows: , ( , , ) ( , , ) * ( , , ) w d d sim d d t weight d t w weight d t w ∈ = ∑ (3) 3.3 Detection Procedure For each story d received in time step t, the value ( ) ( ) ( ) ( ( , , )) time d time d n d max sim d d t < = (4) is a score used to determine whether d is a story about a new topic and at the same time is an indication of the confidence in our decision [8]. time(d) means the publication time of story d. If the score exceeds the thresholdθ new, then there exists a sufficiently similar document, thus d is a old story, otherwise, there is no sufficiently similar previous document, thus d is an new story. 4.",
                "New NED Procedure Traditional NED systems can be classified into two main types on the aspect of detection procedure: (1) S-S type, in which the story on hand is compared to each story received previously, and use the highest similarity to determine whether current story is about a new event; (2) S-C type, in which the story on hand is compared to all previous clusters each of which representing a topic, and the highest similarity is used for final decision for current story.",
                "If the highest similarity exceeds thresholdθ new, then it is an old story, and put it into the most similar cluster; otherwise it is a new story and create a new cluster.",
                "Previous work show that the first manner is more accurate than the second one [4][5].",
                "Since sometimes stories within a topic drift far away from each other, a story may have very low similarity with its topic.",
                "So using similarities between stories for determining new story is better than using similarities between story and clusters.",
                "Nevertheless, the first manner needs much more comparing times which means the first manner is low efficient.",
                "We propose a new detection procedure which uses comparisons with previous clusters to help find the most similar story in less comparing times, and the final new event decision is made according to the most similar story.",
                "Therefore, we can get both the accuracy of S-S type methods and the efficiency of S-C type methods.",
                "The new procedure creates a news indexing-tree dynamically, in which similar stories are put together to form a hierarchy of clusters.",
                "We index similar stories together by their common ancestor (a cluster node).",
                "Dissimilar stories are indexed in different clusters.",
                "When a story is coming, we use comparisons between the current story and previous hierarchical clusters to help find the most similar story which is useful for new event decision.",
                "After the new event decision is made, the current story is inserted to the indexing-tree for the following detection.",
                "The news indexing-tree is defined formally as follows: S-Tree = {r, NC , NS , E} where r is the root of S-Tree, NC is the set of all cluster nodes, NS is the set of all story nodes, and E is the set of all edges in S-Tree.",
                "We define a set of constraints for a S-Tree: ⅰ . , is an non-terminal node in the treeC i i N i∀ ∈ → ⅱ . , is a terminal node in the treeS i i N i∀ ∈ → ⅲ . , out degree of is at least 2C i i N i∀ ∈ → ⅳ . , is represented as the centroid of its desendantsC i i iN∀ ∈ → For a news story di, the comparison procedure and inserting procedure based on indexing-tree are defined as follows.",
                "An example is shown by Figure 1 and Figure 2.",
                "Figure 1.",
                "Comparison procedure Figure 2.",
                "Inserting procedure Comparison procedure: Step 1: compare di to all the direct child nodes of r and select λ nodes with highest similarities, e.g., C1 2 and C1 3 in Figure 1.",
                "Step 2: for each selected node in the last step, e.g.",
                "C1 2, compare di to all its direct child nodes, and select λ nodes with highest similarities, e.g.",
                "C2 2 and d8.",
                "Repeat step 2 for all non-terminal nodes.",
                "Step 3: record the terminal node with the highest similarty to di, e.g. s5, and the similarity value (0.20).",
                "Inserting di to the S-tree with r as root: Find the node n which is direct child of r in the path from r to the terminal node with highest similarity s, e.g.",
                "C1 2.",
                "If s is smaller than θ init+(h-1)δ , then add di to the tree as a direct child of r. Otherwise, if n is a terminal node, then create a cluster node instead of n, and add both n and di as its direct children; if n is an non-terminal node, then repeat this procedure and insert di to the sub-tree with n as root recursively.",
                "Here h is the length between n and the root of S-tree.",
                "The more the stories in a cluster similar to each other, the better the cluster represents the stories in it.",
                "Hence we add no constraints on the maximum of trees height and degree of a node.",
                "Therefore, we cannot give the complexity of this indexing-tree based procedure.",
                "But we will give the number of comparing times needed by the new procedure in our experiments in section7. 5.",
                "Term Reweighting Methods In this section, two term reweighting methods are proposed to improve NED accuracy.",
                "In the first method, a new way is explored for better using of cluster (topic) information.",
                "The second one finds a better way to make use of named entities based on news classification. 5.1 Term Reweighting Based on Distribution Distance TF-IDF is the most prevalent model used in information retrieval systems.",
                "The basic idea is that the fewer documents a term appears in, the more important the term is in discrimination of documents (relevant or not relevant to a query containing the term).",
                "Nevertheless, in TDT domain, we need to discriminate documents with regard to topics rather than queries.",
                "Intuitively, using cluster (topic) vectors to compare with subsequent news stories should outperform using story vectors.",
                "Unfortunately, the experimental results do not support this intuition [4][5].",
                "Based on observation on data, we find the reason is that a news topic usually contains many directly or indirectly related events, while they all have their own sub-subjects which are usually different with each other.",
                "Take the topic described in section 1 as an example, events like the explosion and salvage have very low similarities with events about criminal trial, therefore stories about trial would have low similarity with the topic vector built on its previous events.",
                "This section focuses on how to effectively make use of topic information and at the same time avoid the problem of content decentralization.",
                "At first, we classify terms into 5 classes to help analysis the needs of the modified model: Term class A: terms that occur frequently in the whole corpus, e.g., year and people.",
                "Terms of this class should be given low weights because they do not help much for topic discrimination.",
                "Term class B: terms that occur frequently within a news category, e.g., election, storm.",
                "They are useful to distinguish two stories in different news categories.",
                "However, they cannot provide information to determine whether two stories are on the same or different topics.",
                "In another words, term election and term storm are not helpful in differentiate two election campaigns and two storm disasters.",
                "Therefore, terms of this class should be assigned lower weights.",
                "Term class C: terms that occur frequently in a topic, and infrequently in other topics, e.g., the name of a crash plane, the name of a specific hurricane.",
                "News stories that belong to different topics rarely have overlap terms in this class.",
                "The more frequently a term appears in a topic, the more important the term is for a story belonging to the topic, therefore the term should be set higher weight.",
                "Term class D: terms that appear in a topic exclusively, but not frequently.",
                "For example, the name of a fireman who did very well in a salvage action, which may appears in only two or three stories but never appeared in other topics.",
                "Terms of this type should receive more weights than in TF-IDF model.",
                "However, since they are not popular in the topic, it is not appropriate to give them too high weights.",
                "Term class E: terms with low document frequency, and appear in different topics.",
                "Terms of this class should receive lower weights.",
                "Now we analyze whether TF-IDF model can give proper weights to the five classes of terms.",
                "Obviously, terms of class A are lowly weighted in TF-IDF model, which is conformable with the requirement described above.",
                "In TF-IDF model, terms of class B are highly dependant with the number of stories in a news class.",
                "TF-IDF model cannot provide low weights if the story containing the term belongs to a relative small news class.",
                "For a term of class C, the more frequently it appears in a topic, the less weight TFIDF model gives to it.",
                "This strongly conflicts with the requirement of terms in class C. For terms of class D, TF-IDF model gives them high weights correctly.",
                "But for terms of class E, TF-IDF model gives high weights to them which are not conformable with the requirement of low weights.",
                "To sum up, terms of class B, C, E cannot be properly weighted in TF-IDF model.",
                "So, we propose a modified model to resolve this problem.",
                "When θ init andθ new are set closely, we assume that most of the stories in a first-level cluster (a direct child node of root node) are on the same topic.",
                "Therefore, we make use of a first-level cluster to capture term distribution (df for all the terms within the cluster) within the topic dynamically.",
                "KL divergence of term distribution in a first-level cluster and the whole story set is used to adjust term weights:  ( , , ) * (1 * ( || )) ( , , ) ( , , ) * (1 * ( || )) cw tw cw tw w d D weight d t w KL P P weight d t w weight d t w KL P P γ γ ∈ + = +∑ (5) where ( ) ( ) ( ) ( ) 1,cw cw c c c c df w df w p y p y N N = = − (6) ( ) ( ) ( ) ( ) 1,t t tw tw t t df w df w p y p y N N = = − (7) where dfc(w) is the number of documents containing term w within cluster C, and Nc is the number of documents in cluster C, and Nt is the total number of documents that arrive before time step t. γ is a const parameter, now is manually set 3.",
                "KL divergence is defined as follows [17]: ( ) ( || ) ( ) log ( )x p x KL P Q p x q x = ∑ (8) The basic idea is: for a story in a topic, the more a term occurs within the topic, and the less it occurs in other topics, it should be assigned higher weights.",
                "Obviously, modified model can meet all the requirements of the five term classes listed above. 5.2 Term Reweighting Based on Term Type and Story Class Previous work found that some classes of news stories could achieve good improvements by giving extra weight to named entities.",
                "But we find that terms of different types should be given different amount of extra weight for different classes of news stories.",
                "We use open-NLP1 to recognize named entity types and part-ofspeech tags for terms that appear in news stories.",
                "Named entity types include person name, organization name, location name, date, time, money and percentage, and five POSs are selected: none (NN), verb (VB), adjective (JJ), adverb (RB) and cardinal number (CD).",
                "Statistical analysis shows topic-level discriminative terms types for different classes of stories.",
                "For the sake of convenience, named entity type and part-of-speech tags are uniformly called term type in subsequent sections.",
                "Determining whether two stories are about the same topic is a basic component for NED task.",
                "So at first we use 2 χ statistic to compute correlations between terms and topics.",
                "For a term t and a topic T, a contingence table is derived: Table 1.",
                "A 2×2 Contingence Table Doc Number belong to topic T not belong to topic T include t A B not include t C D The 2 χ statistic for a specific term t with respect to topic T is defined to be [16]: 2 2 ( , ) ( ) * ( ) ( ) * ( ) * ( ) * ( ) w T A B C D AD CB A C B D A B C D χ = + + + − + + + + (9) News topics for the TDT task are further classified into 11 rules of interpretations (ROIs) 2 .",
                "The ROI can be seen as a higher level class of stories.",
                "The average correlation between a term type and a topic ROI is computed as: 2 avg 2 ( , )( ( , ) )k m m km kT R w P w TP R p w T R P χ χ ∈ ∈ ∑ ∑（ , ）= 1 1 k=1…K, m=1…M (10) where K is the number of term types (set 12 constantly in the paper).",
                "M is the number news classes (ROIs, set 11 in the paper).",
                "Pk represents the set of all terms of type k, and Rm represents the set of all topics of class m, p(t,T) means the probability that t occurs in topic T. Because of limitation of space, only parts of the term types (9 term types) and parts of news classes (8 classes) are listed in table 2 with the average correlation values between them.",
                "The statistics is derived from labeled data in TDT2 corpus. (Results in table 2 are already normalized for convenience in comparison.)",
                "The statistics in table 2 indicates the usefulness of different term types in topic discrimination with respect to different news classes.",
                "We can see that, location name is the most useful term type for three news classes: Natural Disasters, Violence or War, Finances.",
                "And for three other categories Elections, Legal/Criminal Cases, Science and Discovery, person name is the most discriminative term type.",
                "For Scandals/Hearings, date is the most important information for topic discrimination.",
                "In addition, Legal/Criminal Cases and Finance topics have higher correlation with money terms, while Science and Discovery have higher correlation with percentage terms.",
                "Non-name terms are more stable for different classes. 1 . http://opennlp.sourceforge.net/ 2 . http://projects.ldc.upenn.edu/TDT3/Guide/label.html From the analysis of table 2, it is reasonable to adjust term weight according to their term type and the news class the story belongs to.",
                "New term weights are reweighted as follows: ( ) ( ) ( ) ( ) ( , , ) * ( , , ) ( , , ) * class d D type w T class d D type w w d weight d t w weight d t w weight d t w α α ∈ = ∑ (11) where type(w) represents the type of term w, and class(d) represents the class of story d, c kα is reweighting parameter for news class c and term type k. In the work, we just simply use statistics in table 2 as the reweighting parameters.",
                "Even thought using the statistics directly may not the best choice, we do not discuss how to automatically obtain the best parameters.",
                "We will try to use machine learning techniques to obtain the best parameters in the future work.",
                "In the work, we use BoosTexter [20] to classify all stories into one of the 11 ROIs.",
                "BoosTexter is a boosting based machine learning program, which creates a series of simple rules for building a classifier for text or attribute-value data.",
                "We use term weight generated using TF-IDF model as feature for story classification.",
                "We trained the model on the 12000 judged English stories in TDT2, and classify the rest of the stories in TDT2 and all stories in TDT3.",
                "Classification results are used for term reweighting in formula (11).",
                "Since the class labels of topic-off stories are not given in TDT datasets, we cannot give the classification accuracy here.",
                "Thus we do not discuss the effects of classification accuracy to NED performance in the paper. 6.",
                "EXPERIMENTAL SETUP 6.1 Datasets We used two LDC [18] datasets TDT2 and TDT3 for our experiments.",
                "TDT2 contains news stories from January to June 1998.",
                "It contains around 54,000 stories from sources like ABC, Associated Press, CNN, New York Times, Public Radio International, Voice of America etc.",
                "Only English stories in the collection were considered.",
                "TDT3 contains approximately 31,000 English stories collected from October to December 1998.",
                "In addition to the sources used in TDT2, it also contains stories from NBC and MSNBC TV broadcasts.",
                "We used transcribed versions of the TV and radio broadcasts besides textual news.",
                "TDT2 dataset is labeled with about 100 topics, and approximately 12,000 English stories belong to at least one of these topics.",
                "TDT3 dataset is labeled with about 120 topics, and approximately 8000 English stories belong to at least one of these topics.",
                "All the topics are classified into 11 Rules of Interpretation: (1)Elections, (2)Scandals/Hearings, (3)Legal/Criminal Cases, (4)Natural Disasters, (5)Accidents, (6)Ongoing Violence or War, (7)Science and Discovery News, (8)Finance, (9)New Law, (10)Sports News, (11)MISC.",
                "News. 6.2 Evaluation Metric TDT uses a cost function CDet that combines the probabilities of missing a new story and a false alarm [19]: * * * *Det Miss Miss Target FA FA NontargetC C P P C P P= + (12) Table 2.",
                "Average correlation between term types and news classes where CMiss means the cost of missing a new story, PMiss means the probability of missing a new story, and PTarget means the probability of seeing a new story in the data; CFA means the cost of a false alarm, PFA means the probability of a false alarm, and PNontarget means the probability of seeing an old story.",
                "The cost CDet is normalized such that a perfect system scores 0 and a trivial system, which is the better one of mark all stories as new or old, scores 1: ( ( * , * ) ) Det Det Miss Target FA Nontarget C Norm C min C P C P = (13) New event detection system gives two outputs for each story.",
                "The first part is yes or no indicating whether the story triggers a new event or not.",
                "The second part is a score indicating confidence of the first decision.",
                "Confidence scores can be used to plot DET curve, i.e., curves that plot false alarm vs. miss probabilities.",
                "Minimum normalized cost can be determined if optimal threshold on the score were chosen. 7.",
                "EXPERIMENTAL RESULTS 7.1 Main Results To test the approaches proposed in the model, we implemented and tested five systems: System-1: this system is used as baseline.",
                "It is implemented based on the basic model described in section 3, i.e., using incremental TF-IDF model to generate term weights, and using Hellinger distance to compute document similarity.",
                "Similarity score normalization is also employed [8].",
                "S-S detection procedure is used.",
                "System-2: this system is the same as system-1 except that S-C detection procedure is used.",
                "System-3: this system is the same as system-1 except that it uses the new detection procedure which is based on indexing-tree.",
                "System-4: implemented based on the approach presented in section 5.1, i.e., terms are reweighted according to the distance between term distributions in a cluster and all stories.",
                "The new detection procedure is used.",
                "System-5: implemented based on the approach presented in section 5.2, i.e., terms of different types are reweighted according to news class using trained parameters.",
                "The new detection procedure is used.",
                "The following are some other NED systems: System-6: [21] for each pair of stories, it computes three similarity values for named entity, non-named entity and all terms respectively.",
                "And employ Support Vector Machine to predict new or old using the similarity values as features.",
                "System-7: [8] it extended a basic incremental TF-IDF model to include source-specific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, etc.",
                "System-8: [13] it split document representation into two parts: named entities and non-named entities, and choose one effective part for each news class.",
                "Table 3 and table 4 show topic-weighted normalized costs and comparing times on TDT2 and TDT3 datasets respectively.",
                "Since no heldout data set for fine-tuning the threshold θ new was available for experiments on TDT2, we only report minimum normalized costs for our systems in table 3.",
                "System-5 outperforms all other systems including system-6, and it performs only 2.78e+8 comparing times in detection procedure which is only 13.4% of system-1.",
                "Table 3.",
                "NED results on TDT2 Systems Min Norm(CDet) Cmp times System-1 0.5749 2.08e+9 System-2① 0.6673 3.77e+8 System-3② 0.5765 2.81e+8 System-4② 0.5431 2.99e+8 System-5② 0.5089 2.78e+8 System-6 0.5300 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 When evaluating on the normalized costs on TDT3, we use the optimal thresholds obtained from TDT2 data set for all systems.",
                "System-2 reduces comparing times to 1.29e+9 which is just 18.3% of system-1, but at the same time it also gets a deteriorated minimum normalized cost which is 0.0499 higher than system-1.",
                "System-3 uses the new detection procedure based on news indexing-tree.",
                "It requires even less comparing times than system-2.",
                "This is because story-story comparisons usually yield greater similarities than story-cluster ones, so stories tend to be combined Location Person Date Organization Money Percentage NN JJ CD Elections 0.37 1 0.04 0.58 0.08 0.03 0.32 0.13 0.1 Scandals/Hearings 0.66 0.62 0.28 1 0.11 0.02 0.27 0.13 0.05 Legal/Criminal Cases 0.48 1 0.02 0.62 0.15 0 0.22 0.24 0.09 Natural Disasters 1 0.27 0 0.04 0.04 0 0.25 0.04 0.02 Violence or War 1 0.36 0.02 0.14 0.02 0.04 0.21 0.11 0.02 Science and Discovery 0.11 1 0.01 0.22 0.08 0.12 0.19 0.08 0.03 Finances 1 0.45 0.04 0.98 0.13 0.02 0.29 0.06 0.05 Sports 0.16 0.27 0.01 1 0.02 0 0.11 0.03 0.01 together in system-3.",
                "And system-3 is basically equivalent to system-1 in accuracy results.",
                "System-4 adjusts term weights based on the distance of term distributions between the whole corpus and cluster story set, yielding a good improvement by 0.0468 compared to system-1.",
                "The best system (system-5) has a minimum normalized cost 0.5012, which is 0.0797 better than system-1, and also better than any other results previously reported for this dataset [8, 13].",
                "Further more, system-5 only needs 1.05e+8 comparing times which is 14.9% of system-1.",
                "Table 4.",
                "NED results on TDT3 Systems Norm(CDet) Min Norm(CDet) Cmp times System-1 0.6159 0.5809 7.04e+8 System-2① 0.6493 0.6308 1.29e+8 System-3② 0.6197 0.5868 1.03e+8 System-4② 0.5601 0.5341 1.03e+8 System-5② 0.5413 0.5012 1.05e+8 System-7 -- 0.5783 -System-8 -- 0.5229 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 Figure5 shows the five DET curves for our systems on data set TDT3.",
                "System-5 achieves the minimum cost at a false alarm rate of 0.0157 and a miss rate of 0.4310.",
                "We can observe that System4 and System-5 obtain lower miss probability at regions of low false alarm probabilities.",
                "The hypothesis is that, more weight value is transferred to key terms of topics from non-key terms.",
                "Similarity score between two stories belonging to different topics are lower than before, because their overlapping terms are usually not key terms of their topics. 7.2 Parameter selection for indexing-tree detection Figure 3 shows the minimum normalized costs obtained by system-3 on TDT3 using different parameters.",
                "Theθ init parameter is tested on six values spanning from 0.03 to 0.18.",
                "And the λ parameter is tested on four values 1, 2, 3 and 4.",
                "We can see that, whenθ init is set to 0.12, which is the closest one toθ new, the costs are lower than others.",
                "This is easy to explain, because when stories belonging to the same topic are put in a cluster, it is more reasonable for the cluster to represent the stories in it.",
                "When parameter λ is set to 3 or 4, the costs are better than other cases, but there is no much difference between 3 and 4. 0 0.05 0.1 0.15 0.2 1 2 3 4 0.5 0.6 0.7 0.8 0.9 1 θ-initλ MinCost 0.6 0.65 0.7 0.75 0.8 0.85 0.9 Figure 3.",
                "Min Cost on TDT3 (δ =0.15) 0 0.05 0.1 0.15 0.2 1 2 3 4 0 0.5 1 1.5 2 2.5 x 10 8 θ-init λ Comparingtimes 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 8 Figure 4.",
                "Comparing times on TDT3 (δ =0.15) Figure 4 gives the comparing times used by system-3 on TDT3 with the same parameters as figure 3.",
                "The comparing times are strongly dependent onθ init.",
                "Because the greaterθ init is, the less stories combined together, the more comparing times are needed for new event decision.",
                "So we useθ init =0.13,λ =3,δ =0.15 for system-3, 4, and 5.",
                "In this parameter setting, we can get both low minimum normalized costs and less comparing times. 8.",
                "CONCLUSION We have proposed a news indexing-tree based detection procedure in our model.",
                "It reduces comparing times to about one seventh of traditional method without hurting NED accuracy.",
                "We also have presented two extensions to the basic TF-IDF model.",
                "The first extension is made by adjust term weights based on term distributions between the whole corpus and a cluster story set.",
                "And the second extension to basic TF-IDF model is better use of term types (named entities types and part-of-speed) according to news categories.",
                "Our experimental results on TDT2 and TDT3 datasets show that both of the two extensions contribute significantly to improvement in accuracy.",
                "We did not consider news time information as a clue for NED task, since most of the topics last for a long time and TDT data sets only span for a relative short period (no more than 6 months).",
                "For the future work, we want to collect news set which span for a longer period from internet, and integrate time information in NED task.",
                "Since topic is a relative coarse-grained news cluster, we also want to refine cluster granularity to event-level, and identify different events and their relations within a topic.",
                "Acknowledgments This work is supported by the National Natural Science Foundation of China under Grant No. 90604025.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsor. 9.",
                "REFERENCES [1] http://www.nist.gov/speech/tests/tdt/index.htm [2] In Topic Detection and Tracking.",
                "Event-based Information Organization.",
                "Kluwer Academic Publishers, 2002. .01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90 1 2 5 10 20 40 60 80 90 False Alarm Probability (in %) MissProbability(in%) SYSTEM1 Topic Weighted Curve SYSTEM1 Min Norm(Cost) SYSTEM2 Topic Weighted Curve SYSTEM2 Min Norm(Cost) SYSTEM3 Topic Weighted Curve SYSTEM3 Min Norm(Cost) SYSTEM4 Topic Weighted Curve SYSTEM4 Min Norm(Cost) SYSTEM5 Topic Weighted Curve SYSTEM5 Min Norm(Cost) Random Performance Figure 5.",
                "DET curves on TDT3 [3] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B.T.",
                "Archibald, and X. Liu.",
                "Learning Approaches for Detecting and Tracking News Events.",
                "In IEEE Intelligent Systems Special Issue on Applications of Intelligent Information Retrieval, volume 14 (4), 1999, 32-43. [4] Y. Yang, T. Pierce, and J. Carbonell.",
                "A Study on Retrospective and On-line Event Detection.",
                "In Proceedings of SIGIR-98, Melbourne, Australia, 1998, 28-36. [5] J. Allan, V. Lavrenko, D. Malin, and R. Swan.",
                "Detections, Bounds, and Timelines: Umass and tdt-3.",
                "In Proceedings of Topic Detection and Tracking Workshop (TDT-3), Vienna, VA, 2000, 167-174. [6] R. Papka and J. Allan.",
                "On-line New Event Detection Using Single Pass Clustering TITLE2:.",
                "Technical Report UM-CS1998-021, 1998. [7] W. Lam, H. Meng, K. Wong, and J.",
                "Yen.",
                "Using Contextual Analysis for News Event Detection.",
                "International Journal on Intelligent Systems, 2001, 525-546. [8] B. Thorsten, C. Francine, and F. Ayman.",
                "A System for New Event Detection.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2003, 330-337. [9] S. Nicola and C. Joe.",
                "Combining Semantic and Syntactic Document Classifiers to Improve First Story Detection.",
                "In Proceedings of the 24th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topicconditioned Novelty Detection.",
                "In Proceedings of the 8th ACM SIGKDD International Conference, ACM Press. 2002, 688-693. [11] M. Juha, A.M. Helena, and S. Marko.",
                "Applying Semantic Classes in Event Detection and Tracking.",
                "In Proceedings of International Conference on Natural Language Processing (ICON 2002), 2002, pages 175-183. [12] M. Juha, A.M. Helena, and S. Marko.",
                "Simple Semantics in Topic Detection and Tracking.",
                "Information Retrieval, 7(3-4): 2004, 347-368. [13] K. Giridhar and J. Allan.",
                "Text Classification and Named Entities for New Event Detection.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2004, 297-304. [14] J. P. Callan, W. B. Croft, and S. M. Harding.",
                "The INQUERY Retrieval System.",
                "In Proceedings of DEXA-92, 3rd International Conference on Database and Expert Systems Applications, 1992, 78-83. [15] R. Krovetz.",
                "Viewing Morphology as An Inference Process.",
                "In Proceedings of ACM SIGIR93, 1993, 61-81. [16] Y. Yang and J. Pedersen.",
                "A Comparative Study on Feature Selection in Text Categorization.",
                "In J. D. H. Fisher, editor, The Fourteenth International Conference on Machine Learning (ICML97), Morgan Kaufmann, 1997, 412-420. [17] T. M. Cover, and J.A.",
                "Thomas.",
                "Elements of Information Theory.",
                "Wiley. 1991. [18] The linguistic data consortium, http://www.ldc,upenn.edu/. [19] The 2001 TDT task definition and evaluation plan, http://www.nist.gov/speech/tests/tdt/tdt2001/evalplan.htm. [20] R. E. Schapire and Y.",
                "Singer.",
                "Boostexter: A Boosting-based System for Text Categorization.",
                "In Machine Learning 39(2/3):1, Kluwer Academic Publishers, 2000, 35-168. [21] K. Giridhar and J. Allan. 2005.",
                "Using Names and Topics for New Event Detection.",
                "In Proceedings of Human Technology Conference and Conference on Empirical Methods in Natural Language, Vancouver, 2005, 121-128"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "name entity": {
            "translated_key": "entidad de nombre",
            "is_in_text": false,
            "original_annotated_sentences": [
                "New Event Detection Based on Indexing-tree and Named Entity Zhang Kuo Tsinghua University Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Tsinghua University Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Tsinghua University Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn ABSTRACT New Event Detection (NED) aims at detecting from one or multiple streams of news stories that which one is reported on a new event (i.e.",
                "not reported previously).",
                "With the overwhelming volume of news available today, there is an increasing need for a NED system which is able to detect new events more efficiently and accurately.",
                "In this paper we propose a new NED model to speed up the NED task by using news indexing-tree dynamically.",
                "Moreover, based on the observation that terms of different types have different effects for NED task, two term reweighting approaches are proposed to improve NED accuracy.",
                "In the first approach, we propose to adjust term weights dynamically based on previous story clusters and in the second approach, we propose to employ statistics on training data to learn the named entity reweighting model for each class of stories.",
                "Experimental results on two Linguistic Data Consortium (LDC) datasets TDT2 and TDT3 show that the proposed model can improve both efficiency and accuracy of NED task significantly, compared to the baseline system and other existing systems.",
                "Categories and Subject Descriptors H.3.3 [Information Systems]: Information Search and Retrieval; H.4.2 [Information Systems Applications]: Types of Systemsdecision support.",
                "General Terms Algorithms, Performance, Experimentation 1.",
                "INTRODUCTION Topic Detection and Tracking (TDT) program aims to develop techniques which can effectively organize, search and structure news text materials from a variety of newswire and broadcast media [1].",
                "New Event Detection (NED) is one of the five tasks in TDT.",
                "It is the task of online identification of the earliest report for each topic as soon as that report arrives in the sequence of documents.",
                "A Topic is defined as a seminal event or activity, along with directly related events and activities [2].",
                "An Event is defined as something (non-trivial) happening in a certain place at a certain time [3].",
                "For instance, when a bomb explodes in a building, the exploding is the seminal event that triggers the topic, and other stories on the same topic would be those discussing salvaging efforts, the search for perpetrators, arrests and trial and so on.",
                "Useful news information is usually buried in a mass of data generated everyday.",
                "Therefore, NED systems are very useful for people who need to detect novel information from real-time news stream.",
                "These real-life needs often occur in domains like financial markets, news analysis, and intelligence gathering.",
                "In most of state-of-the-art (currently) NED systems, each news story on hand is compared to all the previous received stories.",
                "If all the similarities between them do not exceed a threshold, then the story triggers a new event.",
                "They are usually in the form of cosine similarity or Hellinger similarity metric.",
                "The core problem of NED is to identify whether two stories are on the same topic.",
                "Obviously, these systems cannot take advantage of topic information.",
                "Further more, it is not acceptable in real applications because of the large amount of computation required in the NED process.",
                "Other systems organize previous stories into clusters (each cluster corresponds to a topic), and new story is compared to the previous clusters instead of stories.",
                "This manner can reduce comparing times significantly.",
                "Nevertheless, it has been proved that this manner is less accurate [4, 5].",
                "This is because sometimes stories within a topic drift far away from each other, which could lead low similarity between a story and its topic.",
                "On the other hand, some proposed NED systems tried to improve accuracy by making better use of named entities [10, 11, 12, 13].",
                "However, none of the systems have considered that terms of different types (e.g.",
                "Noun, Verb or Person name) have different effects for different classes of stories in determining whether two stories are on the same topic.",
                "For example, the names of election candidates (Person name) are very important for stories of election class; the locations (Location name) where accidents happened are important for stories of accidents class.",
                "So, in NED, there still exist following three problems to be investigated: (1) How to speed up the detection procedure while do not decrease the detection accuracy? (2) How to make good use of cluster (topic) information to improve accuracy? (3) How to obtain better news story representation by better understanding of named entities.",
                "Driven by these problems, we have proposed three approaches in this paper. (1)To make the detection procedure faster, we propose a new NED procedure based on news indexing-tree created dynamically.",
                "Story indexing-tree is created by assembling similar stories together to form news clusters in different hierarchies according to their values of similarity.",
                "Comparisons between current story and previous clusters could help find the most similar story in less comparing times.",
                "The new procedure can reduce the amount of comparing times without hurting accuracy. (2)We use the clusters of the first floor in the indexing-tree as news topics, in which term weights are adjusted dynamically according to term distribution in the clusters.",
                "In this approach, cluster (topic) information is used properly, so the problem of theme decentralization is avoided. (3)Based on observations on the statistics obtained from training data, we found that terms of different types (e.g.",
                "Noun and Verb) have different effects for different classes of stories in determining whether two stories are on the same topic.",
                "And we propose to use statistics to optimize the weights of the terms of different types in a story according to the news class that the story belongs to.",
                "On TDT3 dataset, the new NED model just uses 14.9% comparing times of the basic model, while its minimum normalized cost is 0.5012, which is 0.0797 better than the basic model, and also better than any other results previously reported for this dataset [8, 13].",
                "The rest of the paper is organized as follows.",
                "We start off this paper by summarizing the previous work in NED in section 2.",
                "Section 3 presents the basic model for NED that most current systems use.",
                "Section 4 describes our new detection procedure based on news indexing-tree.",
                "In section 5, two term reweighting methods are proposed to improve NED accuracy.",
                "Section 6 gives our experimental data and evaluation metrics.",
                "We finally wrap up with the experimental results in Section 7, and the conclusions and future work in Section 8. 2.",
                "RELATED WORK Papka et al. proposed Single-Pass clustering on NED [6].",
                "When a new story was encountered, it was processed immediately to extract term features and a query representation of the storys content is built up.",
                "Then it was compared with all the previous queries.",
                "If the document did not trigger any queries by exceeding a threshold, it was marked as a new event.",
                "Lam et al build up previous query representations of story clusters, each of which corresponds to a topic [7].",
                "In this manner comparisons happen between stories and clusters.",
                "Recent years, most work focus on proposing better methods on comparison of stories and document representation.",
                "Brants et al. [8] extended a basic incremental TF-IDF model to include sourcespecific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, term reweighting based on inverse event frequencies, and segmentation of documents.",
                "Good improvements on TDT bench-marks were shown.",
                "Stokes et al. [9] utilized a combination of evidence from two distinct representations of a documents content.",
                "One of the representations was the usual free text vector, the other made use of lexical chains (created using WordNet) to build another term vector.",
                "Then the two representations are combined in a linear fashion.",
                "A marginal increase in effectiveness was achieved when the combined representation was used.",
                "Some efforts have been done on how to utilize named entities to improve NED.",
                "Yang et al. gave location named entities four times weight than other terms and named entities [10].",
                "DOREMI research group combined semantic similarities of person names, location names and time together with textual similarity [11][12].",
                "UMass [13] research group split document representation into two parts: named entities and non-named entities.",
                "And it was found that some classes of news could achieve better performance using named entity representation, while some other classes of news could achieve better performance using non-named entity representation.",
                "Both [10] and [13] used text categorization technique to classify news stories in advance.",
                "In [13] news stories are classified automatically at first, and then test sensitivities of names and non-name terms for NED for each class.",
                "In [10] frequent terms for each class are removed from document representation.",
                "For example, word election does not help identify different elections.",
                "In their work, effectiveness of different kinds of names (or terms with different POS) for NED in different news classes are not investigated.",
                "We use statistical analysis to reveal the fact and use it to improve NED performance. 3.",
                "BASIC MODEL In this section, we present the basic New Event Detection model which is similar to what most current systems apply.",
                "Then, we propose our new model by extending the basic model.",
                "New Event Detection systems use news story stream as input, in which stories are strictly time-ordered.",
                "Only previously received stories are available when dealing with current story.",
                "The output is a decision for whether the current story is on a new event or not and the confidence of the decision.",
                "Usually, a NED model consists of three parts: story representation, similarity calculation and detection procedure. 3.1 Story Representation Preprocessing is needed before generating story representation.",
                "For preprocessing, we tokenize words, recognize abbreviations, normalize abbreviations, add part-of-speech tags, remove stopwords included in the stop list used in InQuery [14], replace words with their stems using K-stem algorithm[15], and then generate word vector for each news story.",
                "We use incremental TF-IDF model for term weight calculation [4].",
                "In a TF-IDF model, term frequency in a news document is weighted by the inverse document frequency, which is generated from training corpus.",
                "When a new term occurs in testing process, there are two solutions: simply ignore the new term or set df of the term as a small const (e.g. df = 1).",
                "The new term receives too low weight in the first solution (0) and too high weight in the second solution.",
                "In incremental TF-IDF model, document frequencies are updated dynamically in each time step t: 1( ) ( ) ( )t t D tdf w df w df w−= + (1) where Dt represents news story set received in time t, and dfDt(w) means the number of documents that term w occurs in, and dft(w) means the total number of documents that term w occurs in before time t. In this work, each time window includes 50 news stories.",
                "Thus, each story d received in t is represented as follows: 1 2{ ( , , ), ( , , ),..., ( , , )}nd weight d t w weight d t w weight d t w→ where n means the number of distinct terms in story d, and ( , , )weight d t w means the weight of term w in story d at time t: log( ( , ) 1) log(( 1) /( ( ) 0.5)) ( , , ) log( ( , ) 1) log(( 1) /( ( ) 0.5)) t t t t w d tf d w N df w weight d t w tf d w N df w ∈ + + + = + + +∑ (2) where Nt means the total number of news stories before time t, and tf(d,w) means how many times term w occurs in news story d. 3.2 Similarity Calculation We use Hellinger distance for the calculation of similarity between two stories, for two stories d and d at time t, their similarity is defined as follows: , ( , , ) ( , , ) * ( , , ) w d d sim d d t weight d t w weight d t w ∈ = ∑ (3) 3.3 Detection Procedure For each story d received in time step t, the value ( ) ( ) ( ) ( ( , , )) time d time d n d max sim d d t < = (4) is a score used to determine whether d is a story about a new topic and at the same time is an indication of the confidence in our decision [8]. time(d) means the publication time of story d. If the score exceeds the thresholdθ new, then there exists a sufficiently similar document, thus d is a old story, otherwise, there is no sufficiently similar previous document, thus d is an new story. 4.",
                "New NED Procedure Traditional NED systems can be classified into two main types on the aspect of detection procedure: (1) S-S type, in which the story on hand is compared to each story received previously, and use the highest similarity to determine whether current story is about a new event; (2) S-C type, in which the story on hand is compared to all previous clusters each of which representing a topic, and the highest similarity is used for final decision for current story.",
                "If the highest similarity exceeds thresholdθ new, then it is an old story, and put it into the most similar cluster; otherwise it is a new story and create a new cluster.",
                "Previous work show that the first manner is more accurate than the second one [4][5].",
                "Since sometimes stories within a topic drift far away from each other, a story may have very low similarity with its topic.",
                "So using similarities between stories for determining new story is better than using similarities between story and clusters.",
                "Nevertheless, the first manner needs much more comparing times which means the first manner is low efficient.",
                "We propose a new detection procedure which uses comparisons with previous clusters to help find the most similar story in less comparing times, and the final new event decision is made according to the most similar story.",
                "Therefore, we can get both the accuracy of S-S type methods and the efficiency of S-C type methods.",
                "The new procedure creates a news indexing-tree dynamically, in which similar stories are put together to form a hierarchy of clusters.",
                "We index similar stories together by their common ancestor (a cluster node).",
                "Dissimilar stories are indexed in different clusters.",
                "When a story is coming, we use comparisons between the current story and previous hierarchical clusters to help find the most similar story which is useful for new event decision.",
                "After the new event decision is made, the current story is inserted to the indexing-tree for the following detection.",
                "The news indexing-tree is defined formally as follows: S-Tree = {r, NC , NS , E} where r is the root of S-Tree, NC is the set of all cluster nodes, NS is the set of all story nodes, and E is the set of all edges in S-Tree.",
                "We define a set of constraints for a S-Tree: ⅰ . , is an non-terminal node in the treeC i i N i∀ ∈ → ⅱ . , is a terminal node in the treeS i i N i∀ ∈ → ⅲ . , out degree of is at least 2C i i N i∀ ∈ → ⅳ . , is represented as the centroid of its desendantsC i i iN∀ ∈ → For a news story di, the comparison procedure and inserting procedure based on indexing-tree are defined as follows.",
                "An example is shown by Figure 1 and Figure 2.",
                "Figure 1.",
                "Comparison procedure Figure 2.",
                "Inserting procedure Comparison procedure: Step 1: compare di to all the direct child nodes of r and select λ nodes with highest similarities, e.g., C1 2 and C1 3 in Figure 1.",
                "Step 2: for each selected node in the last step, e.g.",
                "C1 2, compare di to all its direct child nodes, and select λ nodes with highest similarities, e.g.",
                "C2 2 and d8.",
                "Repeat step 2 for all non-terminal nodes.",
                "Step 3: record the terminal node with the highest similarty to di, e.g. s5, and the similarity value (0.20).",
                "Inserting di to the S-tree with r as root: Find the node n which is direct child of r in the path from r to the terminal node with highest similarity s, e.g.",
                "C1 2.",
                "If s is smaller than θ init+(h-1)δ , then add di to the tree as a direct child of r. Otherwise, if n is a terminal node, then create a cluster node instead of n, and add both n and di as its direct children; if n is an non-terminal node, then repeat this procedure and insert di to the sub-tree with n as root recursively.",
                "Here h is the length between n and the root of S-tree.",
                "The more the stories in a cluster similar to each other, the better the cluster represents the stories in it.",
                "Hence we add no constraints on the maximum of trees height and degree of a node.",
                "Therefore, we cannot give the complexity of this indexing-tree based procedure.",
                "But we will give the number of comparing times needed by the new procedure in our experiments in section7. 5.",
                "Term Reweighting Methods In this section, two term reweighting methods are proposed to improve NED accuracy.",
                "In the first method, a new way is explored for better using of cluster (topic) information.",
                "The second one finds a better way to make use of named entities based on news classification. 5.1 Term Reweighting Based on Distribution Distance TF-IDF is the most prevalent model used in information retrieval systems.",
                "The basic idea is that the fewer documents a term appears in, the more important the term is in discrimination of documents (relevant or not relevant to a query containing the term).",
                "Nevertheless, in TDT domain, we need to discriminate documents with regard to topics rather than queries.",
                "Intuitively, using cluster (topic) vectors to compare with subsequent news stories should outperform using story vectors.",
                "Unfortunately, the experimental results do not support this intuition [4][5].",
                "Based on observation on data, we find the reason is that a news topic usually contains many directly or indirectly related events, while they all have their own sub-subjects which are usually different with each other.",
                "Take the topic described in section 1 as an example, events like the explosion and salvage have very low similarities with events about criminal trial, therefore stories about trial would have low similarity with the topic vector built on its previous events.",
                "This section focuses on how to effectively make use of topic information and at the same time avoid the problem of content decentralization.",
                "At first, we classify terms into 5 classes to help analysis the needs of the modified model: Term class A: terms that occur frequently in the whole corpus, e.g., year and people.",
                "Terms of this class should be given low weights because they do not help much for topic discrimination.",
                "Term class B: terms that occur frequently within a news category, e.g., election, storm.",
                "They are useful to distinguish two stories in different news categories.",
                "However, they cannot provide information to determine whether two stories are on the same or different topics.",
                "In another words, term election and term storm are not helpful in differentiate two election campaigns and two storm disasters.",
                "Therefore, terms of this class should be assigned lower weights.",
                "Term class C: terms that occur frequently in a topic, and infrequently in other topics, e.g., the name of a crash plane, the name of a specific hurricane.",
                "News stories that belong to different topics rarely have overlap terms in this class.",
                "The more frequently a term appears in a topic, the more important the term is for a story belonging to the topic, therefore the term should be set higher weight.",
                "Term class D: terms that appear in a topic exclusively, but not frequently.",
                "For example, the name of a fireman who did very well in a salvage action, which may appears in only two or three stories but never appeared in other topics.",
                "Terms of this type should receive more weights than in TF-IDF model.",
                "However, since they are not popular in the topic, it is not appropriate to give them too high weights.",
                "Term class E: terms with low document frequency, and appear in different topics.",
                "Terms of this class should receive lower weights.",
                "Now we analyze whether TF-IDF model can give proper weights to the five classes of terms.",
                "Obviously, terms of class A are lowly weighted in TF-IDF model, which is conformable with the requirement described above.",
                "In TF-IDF model, terms of class B are highly dependant with the number of stories in a news class.",
                "TF-IDF model cannot provide low weights if the story containing the term belongs to a relative small news class.",
                "For a term of class C, the more frequently it appears in a topic, the less weight TFIDF model gives to it.",
                "This strongly conflicts with the requirement of terms in class C. For terms of class D, TF-IDF model gives them high weights correctly.",
                "But for terms of class E, TF-IDF model gives high weights to them which are not conformable with the requirement of low weights.",
                "To sum up, terms of class B, C, E cannot be properly weighted in TF-IDF model.",
                "So, we propose a modified model to resolve this problem.",
                "When θ init andθ new are set closely, we assume that most of the stories in a first-level cluster (a direct child node of root node) are on the same topic.",
                "Therefore, we make use of a first-level cluster to capture term distribution (df for all the terms within the cluster) within the topic dynamically.",
                "KL divergence of term distribution in a first-level cluster and the whole story set is used to adjust term weights:  ( , , ) * (1 * ( || )) ( , , ) ( , , ) * (1 * ( || )) cw tw cw tw w d D weight d t w KL P P weight d t w weight d t w KL P P γ γ ∈ + = +∑ (5) where ( ) ( ) ( ) ( ) 1,cw cw c c c c df w df w p y p y N N = = − (6) ( ) ( ) ( ) ( ) 1,t t tw tw t t df w df w p y p y N N = = − (7) where dfc(w) is the number of documents containing term w within cluster C, and Nc is the number of documents in cluster C, and Nt is the total number of documents that arrive before time step t. γ is a const parameter, now is manually set 3.",
                "KL divergence is defined as follows [17]: ( ) ( || ) ( ) log ( )x p x KL P Q p x q x = ∑ (8) The basic idea is: for a story in a topic, the more a term occurs within the topic, and the less it occurs in other topics, it should be assigned higher weights.",
                "Obviously, modified model can meet all the requirements of the five term classes listed above. 5.2 Term Reweighting Based on Term Type and Story Class Previous work found that some classes of news stories could achieve good improvements by giving extra weight to named entities.",
                "But we find that terms of different types should be given different amount of extra weight for different classes of news stories.",
                "We use open-NLP1 to recognize named entity types and part-ofspeech tags for terms that appear in news stories.",
                "Named entity types include person name, organization name, location name, date, time, money and percentage, and five POSs are selected: none (NN), verb (VB), adjective (JJ), adverb (RB) and cardinal number (CD).",
                "Statistical analysis shows topic-level discriminative terms types for different classes of stories.",
                "For the sake of convenience, named entity type and part-of-speech tags are uniformly called term type in subsequent sections.",
                "Determining whether two stories are about the same topic is a basic component for NED task.",
                "So at first we use 2 χ statistic to compute correlations between terms and topics.",
                "For a term t and a topic T, a contingence table is derived: Table 1.",
                "A 2×2 Contingence Table Doc Number belong to topic T not belong to topic T include t A B not include t C D The 2 χ statistic for a specific term t with respect to topic T is defined to be [16]: 2 2 ( , ) ( ) * ( ) ( ) * ( ) * ( ) * ( ) w T A B C D AD CB A C B D A B C D χ = + + + − + + + + (9) News topics for the TDT task are further classified into 11 rules of interpretations (ROIs) 2 .",
                "The ROI can be seen as a higher level class of stories.",
                "The average correlation between a term type and a topic ROI is computed as: 2 avg 2 ( , )( ( , ) )k m m km kT R w P w TP R p w T R P χ χ ∈ ∈ ∑ ∑（ , ）= 1 1 k=1…K, m=1…M (10) where K is the number of term types (set 12 constantly in the paper).",
                "M is the number news classes (ROIs, set 11 in the paper).",
                "Pk represents the set of all terms of type k, and Rm represents the set of all topics of class m, p(t,T) means the probability that t occurs in topic T. Because of limitation of space, only parts of the term types (9 term types) and parts of news classes (8 classes) are listed in table 2 with the average correlation values between them.",
                "The statistics is derived from labeled data in TDT2 corpus. (Results in table 2 are already normalized for convenience in comparison.)",
                "The statistics in table 2 indicates the usefulness of different term types in topic discrimination with respect to different news classes.",
                "We can see that, location name is the most useful term type for three news classes: Natural Disasters, Violence or War, Finances.",
                "And for three other categories Elections, Legal/Criminal Cases, Science and Discovery, person name is the most discriminative term type.",
                "For Scandals/Hearings, date is the most important information for topic discrimination.",
                "In addition, Legal/Criminal Cases and Finance topics have higher correlation with money terms, while Science and Discovery have higher correlation with percentage terms.",
                "Non-name terms are more stable for different classes. 1 . http://opennlp.sourceforge.net/ 2 . http://projects.ldc.upenn.edu/TDT3/Guide/label.html From the analysis of table 2, it is reasonable to adjust term weight according to their term type and the news class the story belongs to.",
                "New term weights are reweighted as follows: ( ) ( ) ( ) ( ) ( , , ) * ( , , ) ( , , ) * class d D type w T class d D type w w d weight d t w weight d t w weight d t w α α ∈ = ∑ (11) where type(w) represents the type of term w, and class(d) represents the class of story d, c kα is reweighting parameter for news class c and term type k. In the work, we just simply use statistics in table 2 as the reweighting parameters.",
                "Even thought using the statistics directly may not the best choice, we do not discuss how to automatically obtain the best parameters.",
                "We will try to use machine learning techniques to obtain the best parameters in the future work.",
                "In the work, we use BoosTexter [20] to classify all stories into one of the 11 ROIs.",
                "BoosTexter is a boosting based machine learning program, which creates a series of simple rules for building a classifier for text or attribute-value data.",
                "We use term weight generated using TF-IDF model as feature for story classification.",
                "We trained the model on the 12000 judged English stories in TDT2, and classify the rest of the stories in TDT2 and all stories in TDT3.",
                "Classification results are used for term reweighting in formula (11).",
                "Since the class labels of topic-off stories are not given in TDT datasets, we cannot give the classification accuracy here.",
                "Thus we do not discuss the effects of classification accuracy to NED performance in the paper. 6.",
                "EXPERIMENTAL SETUP 6.1 Datasets We used two LDC [18] datasets TDT2 and TDT3 for our experiments.",
                "TDT2 contains news stories from January to June 1998.",
                "It contains around 54,000 stories from sources like ABC, Associated Press, CNN, New York Times, Public Radio International, Voice of America etc.",
                "Only English stories in the collection were considered.",
                "TDT3 contains approximately 31,000 English stories collected from October to December 1998.",
                "In addition to the sources used in TDT2, it also contains stories from NBC and MSNBC TV broadcasts.",
                "We used transcribed versions of the TV and radio broadcasts besides textual news.",
                "TDT2 dataset is labeled with about 100 topics, and approximately 12,000 English stories belong to at least one of these topics.",
                "TDT3 dataset is labeled with about 120 topics, and approximately 8000 English stories belong to at least one of these topics.",
                "All the topics are classified into 11 Rules of Interpretation: (1)Elections, (2)Scandals/Hearings, (3)Legal/Criminal Cases, (4)Natural Disasters, (5)Accidents, (6)Ongoing Violence or War, (7)Science and Discovery News, (8)Finance, (9)New Law, (10)Sports News, (11)MISC.",
                "News. 6.2 Evaluation Metric TDT uses a cost function CDet that combines the probabilities of missing a new story and a false alarm [19]: * * * *Det Miss Miss Target FA FA NontargetC C P P C P P= + (12) Table 2.",
                "Average correlation between term types and news classes where CMiss means the cost of missing a new story, PMiss means the probability of missing a new story, and PTarget means the probability of seeing a new story in the data; CFA means the cost of a false alarm, PFA means the probability of a false alarm, and PNontarget means the probability of seeing an old story.",
                "The cost CDet is normalized such that a perfect system scores 0 and a trivial system, which is the better one of mark all stories as new or old, scores 1: ( ( * , * ) ) Det Det Miss Target FA Nontarget C Norm C min C P C P = (13) New event detection system gives two outputs for each story.",
                "The first part is yes or no indicating whether the story triggers a new event or not.",
                "The second part is a score indicating confidence of the first decision.",
                "Confidence scores can be used to plot DET curve, i.e., curves that plot false alarm vs. miss probabilities.",
                "Minimum normalized cost can be determined if optimal threshold on the score were chosen. 7.",
                "EXPERIMENTAL RESULTS 7.1 Main Results To test the approaches proposed in the model, we implemented and tested five systems: System-1: this system is used as baseline.",
                "It is implemented based on the basic model described in section 3, i.e., using incremental TF-IDF model to generate term weights, and using Hellinger distance to compute document similarity.",
                "Similarity score normalization is also employed [8].",
                "S-S detection procedure is used.",
                "System-2: this system is the same as system-1 except that S-C detection procedure is used.",
                "System-3: this system is the same as system-1 except that it uses the new detection procedure which is based on indexing-tree.",
                "System-4: implemented based on the approach presented in section 5.1, i.e., terms are reweighted according to the distance between term distributions in a cluster and all stories.",
                "The new detection procedure is used.",
                "System-5: implemented based on the approach presented in section 5.2, i.e., terms of different types are reweighted according to news class using trained parameters.",
                "The new detection procedure is used.",
                "The following are some other NED systems: System-6: [21] for each pair of stories, it computes three similarity values for named entity, non-named entity and all terms respectively.",
                "And employ Support Vector Machine to predict new or old using the similarity values as features.",
                "System-7: [8] it extended a basic incremental TF-IDF model to include source-specific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, etc.",
                "System-8: [13] it split document representation into two parts: named entities and non-named entities, and choose one effective part for each news class.",
                "Table 3 and table 4 show topic-weighted normalized costs and comparing times on TDT2 and TDT3 datasets respectively.",
                "Since no heldout data set for fine-tuning the threshold θ new was available for experiments on TDT2, we only report minimum normalized costs for our systems in table 3.",
                "System-5 outperforms all other systems including system-6, and it performs only 2.78e+8 comparing times in detection procedure which is only 13.4% of system-1.",
                "Table 3.",
                "NED results on TDT2 Systems Min Norm(CDet) Cmp times System-1 0.5749 2.08e+9 System-2① 0.6673 3.77e+8 System-3② 0.5765 2.81e+8 System-4② 0.5431 2.99e+8 System-5② 0.5089 2.78e+8 System-6 0.5300 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 When evaluating on the normalized costs on TDT3, we use the optimal thresholds obtained from TDT2 data set for all systems.",
                "System-2 reduces comparing times to 1.29e+9 which is just 18.3% of system-1, but at the same time it also gets a deteriorated minimum normalized cost which is 0.0499 higher than system-1.",
                "System-3 uses the new detection procedure based on news indexing-tree.",
                "It requires even less comparing times than system-2.",
                "This is because story-story comparisons usually yield greater similarities than story-cluster ones, so stories tend to be combined Location Person Date Organization Money Percentage NN JJ CD Elections 0.37 1 0.04 0.58 0.08 0.03 0.32 0.13 0.1 Scandals/Hearings 0.66 0.62 0.28 1 0.11 0.02 0.27 0.13 0.05 Legal/Criminal Cases 0.48 1 0.02 0.62 0.15 0 0.22 0.24 0.09 Natural Disasters 1 0.27 0 0.04 0.04 0 0.25 0.04 0.02 Violence or War 1 0.36 0.02 0.14 0.02 0.04 0.21 0.11 0.02 Science and Discovery 0.11 1 0.01 0.22 0.08 0.12 0.19 0.08 0.03 Finances 1 0.45 0.04 0.98 0.13 0.02 0.29 0.06 0.05 Sports 0.16 0.27 0.01 1 0.02 0 0.11 0.03 0.01 together in system-3.",
                "And system-3 is basically equivalent to system-1 in accuracy results.",
                "System-4 adjusts term weights based on the distance of term distributions between the whole corpus and cluster story set, yielding a good improvement by 0.0468 compared to system-1.",
                "The best system (system-5) has a minimum normalized cost 0.5012, which is 0.0797 better than system-1, and also better than any other results previously reported for this dataset [8, 13].",
                "Further more, system-5 only needs 1.05e+8 comparing times which is 14.9% of system-1.",
                "Table 4.",
                "NED results on TDT3 Systems Norm(CDet) Min Norm(CDet) Cmp times System-1 0.6159 0.5809 7.04e+8 System-2① 0.6493 0.6308 1.29e+8 System-3② 0.6197 0.5868 1.03e+8 System-4② 0.5601 0.5341 1.03e+8 System-5② 0.5413 0.5012 1.05e+8 System-7 -- 0.5783 -System-8 -- 0.5229 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 Figure5 shows the five DET curves for our systems on data set TDT3.",
                "System-5 achieves the minimum cost at a false alarm rate of 0.0157 and a miss rate of 0.4310.",
                "We can observe that System4 and System-5 obtain lower miss probability at regions of low false alarm probabilities.",
                "The hypothesis is that, more weight value is transferred to key terms of topics from non-key terms.",
                "Similarity score between two stories belonging to different topics are lower than before, because their overlapping terms are usually not key terms of their topics. 7.2 Parameter selection for indexing-tree detection Figure 3 shows the minimum normalized costs obtained by system-3 on TDT3 using different parameters.",
                "Theθ init parameter is tested on six values spanning from 0.03 to 0.18.",
                "And the λ parameter is tested on four values 1, 2, 3 and 4.",
                "We can see that, whenθ init is set to 0.12, which is the closest one toθ new, the costs are lower than others.",
                "This is easy to explain, because when stories belonging to the same topic are put in a cluster, it is more reasonable for the cluster to represent the stories in it.",
                "When parameter λ is set to 3 or 4, the costs are better than other cases, but there is no much difference between 3 and 4. 0 0.05 0.1 0.15 0.2 1 2 3 4 0.5 0.6 0.7 0.8 0.9 1 θ-initλ MinCost 0.6 0.65 0.7 0.75 0.8 0.85 0.9 Figure 3.",
                "Min Cost on TDT3 (δ =0.15) 0 0.05 0.1 0.15 0.2 1 2 3 4 0 0.5 1 1.5 2 2.5 x 10 8 θ-init λ Comparingtimes 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 8 Figure 4.",
                "Comparing times on TDT3 (δ =0.15) Figure 4 gives the comparing times used by system-3 on TDT3 with the same parameters as figure 3.",
                "The comparing times are strongly dependent onθ init.",
                "Because the greaterθ init is, the less stories combined together, the more comparing times are needed for new event decision.",
                "So we useθ init =0.13,λ =3,δ =0.15 for system-3, 4, and 5.",
                "In this parameter setting, we can get both low minimum normalized costs and less comparing times. 8.",
                "CONCLUSION We have proposed a news indexing-tree based detection procedure in our model.",
                "It reduces comparing times to about one seventh of traditional method without hurting NED accuracy.",
                "We also have presented two extensions to the basic TF-IDF model.",
                "The first extension is made by adjust term weights based on term distributions between the whole corpus and a cluster story set.",
                "And the second extension to basic TF-IDF model is better use of term types (named entities types and part-of-speed) according to news categories.",
                "Our experimental results on TDT2 and TDT3 datasets show that both of the two extensions contribute significantly to improvement in accuracy.",
                "We did not consider news time information as a clue for NED task, since most of the topics last for a long time and TDT data sets only span for a relative short period (no more than 6 months).",
                "For the future work, we want to collect news set which span for a longer period from internet, and integrate time information in NED task.",
                "Since topic is a relative coarse-grained news cluster, we also want to refine cluster granularity to event-level, and identify different events and their relations within a topic.",
                "Acknowledgments This work is supported by the National Natural Science Foundation of China under Grant No. 90604025.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsor. 9.",
                "REFERENCES [1] http://www.nist.gov/speech/tests/tdt/index.htm [2] In Topic Detection and Tracking.",
                "Event-based Information Organization.",
                "Kluwer Academic Publishers, 2002. .01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90 1 2 5 10 20 40 60 80 90 False Alarm Probability (in %) MissProbability(in%) SYSTEM1 Topic Weighted Curve SYSTEM1 Min Norm(Cost) SYSTEM2 Topic Weighted Curve SYSTEM2 Min Norm(Cost) SYSTEM3 Topic Weighted Curve SYSTEM3 Min Norm(Cost) SYSTEM4 Topic Weighted Curve SYSTEM4 Min Norm(Cost) SYSTEM5 Topic Weighted Curve SYSTEM5 Min Norm(Cost) Random Performance Figure 5.",
                "DET curves on TDT3 [3] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B.T.",
                "Archibald, and X. Liu.",
                "Learning Approaches for Detecting and Tracking News Events.",
                "In IEEE Intelligent Systems Special Issue on Applications of Intelligent Information Retrieval, volume 14 (4), 1999, 32-43. [4] Y. Yang, T. Pierce, and J. Carbonell.",
                "A Study on Retrospective and On-line Event Detection.",
                "In Proceedings of SIGIR-98, Melbourne, Australia, 1998, 28-36. [5] J. Allan, V. Lavrenko, D. Malin, and R. Swan.",
                "Detections, Bounds, and Timelines: Umass and tdt-3.",
                "In Proceedings of Topic Detection and Tracking Workshop (TDT-3), Vienna, VA, 2000, 167-174. [6] R. Papka and J. Allan.",
                "On-line New Event Detection Using Single Pass Clustering TITLE2:.",
                "Technical Report UM-CS1998-021, 1998. [7] W. Lam, H. Meng, K. Wong, and J.",
                "Yen.",
                "Using Contextual Analysis for News Event Detection.",
                "International Journal on Intelligent Systems, 2001, 525-546. [8] B. Thorsten, C. Francine, and F. Ayman.",
                "A System for New Event Detection.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2003, 330-337. [9] S. Nicola and C. Joe.",
                "Combining Semantic and Syntactic Document Classifiers to Improve First Story Detection.",
                "In Proceedings of the 24th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topicconditioned Novelty Detection.",
                "In Proceedings of the 8th ACM SIGKDD International Conference, ACM Press. 2002, 688-693. [11] M. Juha, A.M. Helena, and S. Marko.",
                "Applying Semantic Classes in Event Detection and Tracking.",
                "In Proceedings of International Conference on Natural Language Processing (ICON 2002), 2002, pages 175-183. [12] M. Juha, A.M. Helena, and S. Marko.",
                "Simple Semantics in Topic Detection and Tracking.",
                "Information Retrieval, 7(3-4): 2004, 347-368. [13] K. Giridhar and J. Allan.",
                "Text Classification and Named Entities for New Event Detection.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2004, 297-304. [14] J. P. Callan, W. B. Croft, and S. M. Harding.",
                "The INQUERY Retrieval System.",
                "In Proceedings of DEXA-92, 3rd International Conference on Database and Expert Systems Applications, 1992, 78-83. [15] R. Krovetz.",
                "Viewing Morphology as An Inference Process.",
                "In Proceedings of ACM SIGIR93, 1993, 61-81. [16] Y. Yang and J. Pedersen.",
                "A Comparative Study on Feature Selection in Text Categorization.",
                "In J. D. H. Fisher, editor, The Fourteenth International Conference on Machine Learning (ICML97), Morgan Kaufmann, 1997, 412-420. [17] T. M. Cover, and J.A.",
                "Thomas.",
                "Elements of Information Theory.",
                "Wiley. 1991. [18] The linguistic data consortium, http://www.ldc,upenn.edu/. [19] The 2001 TDT task definition and evaluation plan, http://www.nist.gov/speech/tests/tdt/tdt2001/evalplan.htm. [20] R. E. Schapire and Y.",
                "Singer.",
                "Boostexter: A Boosting-based System for Text Categorization.",
                "In Machine Learning 39(2/3):1, Kluwer Academic Publishers, 2000, 35-168. [21] K. Giridhar and J. Allan. 2005.",
                "Using Names and Topics for New Event Detection.",
                "In Proceedings of Human Technology Conference and Conference on Empirical Methods in Natural Language, Vancouver, 2005, 121-128"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "real-time index": {
            "translated_key": "índice en tiempo real",
            "is_in_text": false,
            "original_annotated_sentences": [
                "New Event Detection Based on Indexing-tree and Named Entity Zhang Kuo Tsinghua University Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Tsinghua University Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Tsinghua University Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn ABSTRACT New Event Detection (NED) aims at detecting from one or multiple streams of news stories that which one is reported on a new event (i.e.",
                "not reported previously).",
                "With the overwhelming volume of news available today, there is an increasing need for a NED system which is able to detect new events more efficiently and accurately.",
                "In this paper we propose a new NED model to speed up the NED task by using news indexing-tree dynamically.",
                "Moreover, based on the observation that terms of different types have different effects for NED task, two term reweighting approaches are proposed to improve NED accuracy.",
                "In the first approach, we propose to adjust term weights dynamically based on previous story clusters and in the second approach, we propose to employ statistics on training data to learn the named entity reweighting model for each class of stories.",
                "Experimental results on two Linguistic Data Consortium (LDC) datasets TDT2 and TDT3 show that the proposed model can improve both efficiency and accuracy of NED task significantly, compared to the baseline system and other existing systems.",
                "Categories and Subject Descriptors H.3.3 [Information Systems]: Information Search and Retrieval; H.4.2 [Information Systems Applications]: Types of Systemsdecision support.",
                "General Terms Algorithms, Performance, Experimentation 1.",
                "INTRODUCTION Topic Detection and Tracking (TDT) program aims to develop techniques which can effectively organize, search and structure news text materials from a variety of newswire and broadcast media [1].",
                "New Event Detection (NED) is one of the five tasks in TDT.",
                "It is the task of online identification of the earliest report for each topic as soon as that report arrives in the sequence of documents.",
                "A Topic is defined as a seminal event or activity, along with directly related events and activities [2].",
                "An Event is defined as something (non-trivial) happening in a certain place at a certain time [3].",
                "For instance, when a bomb explodes in a building, the exploding is the seminal event that triggers the topic, and other stories on the same topic would be those discussing salvaging efforts, the search for perpetrators, arrests and trial and so on.",
                "Useful news information is usually buried in a mass of data generated everyday.",
                "Therefore, NED systems are very useful for people who need to detect novel information from real-time news stream.",
                "These real-life needs often occur in domains like financial markets, news analysis, and intelligence gathering.",
                "In most of state-of-the-art (currently) NED systems, each news story on hand is compared to all the previous received stories.",
                "If all the similarities between them do not exceed a threshold, then the story triggers a new event.",
                "They are usually in the form of cosine similarity or Hellinger similarity metric.",
                "The core problem of NED is to identify whether two stories are on the same topic.",
                "Obviously, these systems cannot take advantage of topic information.",
                "Further more, it is not acceptable in real applications because of the large amount of computation required in the NED process.",
                "Other systems organize previous stories into clusters (each cluster corresponds to a topic), and new story is compared to the previous clusters instead of stories.",
                "This manner can reduce comparing times significantly.",
                "Nevertheless, it has been proved that this manner is less accurate [4, 5].",
                "This is because sometimes stories within a topic drift far away from each other, which could lead low similarity between a story and its topic.",
                "On the other hand, some proposed NED systems tried to improve accuracy by making better use of named entities [10, 11, 12, 13].",
                "However, none of the systems have considered that terms of different types (e.g.",
                "Noun, Verb or Person name) have different effects for different classes of stories in determining whether two stories are on the same topic.",
                "For example, the names of election candidates (Person name) are very important for stories of election class; the locations (Location name) where accidents happened are important for stories of accidents class.",
                "So, in NED, there still exist following three problems to be investigated: (1) How to speed up the detection procedure while do not decrease the detection accuracy? (2) How to make good use of cluster (topic) information to improve accuracy? (3) How to obtain better news story representation by better understanding of named entities.",
                "Driven by these problems, we have proposed three approaches in this paper. (1)To make the detection procedure faster, we propose a new NED procedure based on news indexing-tree created dynamically.",
                "Story indexing-tree is created by assembling similar stories together to form news clusters in different hierarchies according to their values of similarity.",
                "Comparisons between current story and previous clusters could help find the most similar story in less comparing times.",
                "The new procedure can reduce the amount of comparing times without hurting accuracy. (2)We use the clusters of the first floor in the indexing-tree as news topics, in which term weights are adjusted dynamically according to term distribution in the clusters.",
                "In this approach, cluster (topic) information is used properly, so the problem of theme decentralization is avoided. (3)Based on observations on the statistics obtained from training data, we found that terms of different types (e.g.",
                "Noun and Verb) have different effects for different classes of stories in determining whether two stories are on the same topic.",
                "And we propose to use statistics to optimize the weights of the terms of different types in a story according to the news class that the story belongs to.",
                "On TDT3 dataset, the new NED model just uses 14.9% comparing times of the basic model, while its minimum normalized cost is 0.5012, which is 0.0797 better than the basic model, and also better than any other results previously reported for this dataset [8, 13].",
                "The rest of the paper is organized as follows.",
                "We start off this paper by summarizing the previous work in NED in section 2.",
                "Section 3 presents the basic model for NED that most current systems use.",
                "Section 4 describes our new detection procedure based on news indexing-tree.",
                "In section 5, two term reweighting methods are proposed to improve NED accuracy.",
                "Section 6 gives our experimental data and evaluation metrics.",
                "We finally wrap up with the experimental results in Section 7, and the conclusions and future work in Section 8. 2.",
                "RELATED WORK Papka et al. proposed Single-Pass clustering on NED [6].",
                "When a new story was encountered, it was processed immediately to extract term features and a query representation of the storys content is built up.",
                "Then it was compared with all the previous queries.",
                "If the document did not trigger any queries by exceeding a threshold, it was marked as a new event.",
                "Lam et al build up previous query representations of story clusters, each of which corresponds to a topic [7].",
                "In this manner comparisons happen between stories and clusters.",
                "Recent years, most work focus on proposing better methods on comparison of stories and document representation.",
                "Brants et al. [8] extended a basic incremental TF-IDF model to include sourcespecific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, term reweighting based on inverse event frequencies, and segmentation of documents.",
                "Good improvements on TDT bench-marks were shown.",
                "Stokes et al. [9] utilized a combination of evidence from two distinct representations of a documents content.",
                "One of the representations was the usual free text vector, the other made use of lexical chains (created using WordNet) to build another term vector.",
                "Then the two representations are combined in a linear fashion.",
                "A marginal increase in effectiveness was achieved when the combined representation was used.",
                "Some efforts have been done on how to utilize named entities to improve NED.",
                "Yang et al. gave location named entities four times weight than other terms and named entities [10].",
                "DOREMI research group combined semantic similarities of person names, location names and time together with textual similarity [11][12].",
                "UMass [13] research group split document representation into two parts: named entities and non-named entities.",
                "And it was found that some classes of news could achieve better performance using named entity representation, while some other classes of news could achieve better performance using non-named entity representation.",
                "Both [10] and [13] used text categorization technique to classify news stories in advance.",
                "In [13] news stories are classified automatically at first, and then test sensitivities of names and non-name terms for NED for each class.",
                "In [10] frequent terms for each class are removed from document representation.",
                "For example, word election does not help identify different elections.",
                "In their work, effectiveness of different kinds of names (or terms with different POS) for NED in different news classes are not investigated.",
                "We use statistical analysis to reveal the fact and use it to improve NED performance. 3.",
                "BASIC MODEL In this section, we present the basic New Event Detection model which is similar to what most current systems apply.",
                "Then, we propose our new model by extending the basic model.",
                "New Event Detection systems use news story stream as input, in which stories are strictly time-ordered.",
                "Only previously received stories are available when dealing with current story.",
                "The output is a decision for whether the current story is on a new event or not and the confidence of the decision.",
                "Usually, a NED model consists of three parts: story representation, similarity calculation and detection procedure. 3.1 Story Representation Preprocessing is needed before generating story representation.",
                "For preprocessing, we tokenize words, recognize abbreviations, normalize abbreviations, add part-of-speech tags, remove stopwords included in the stop list used in InQuery [14], replace words with their stems using K-stem algorithm[15], and then generate word vector for each news story.",
                "We use incremental TF-IDF model for term weight calculation [4].",
                "In a TF-IDF model, term frequency in a news document is weighted by the inverse document frequency, which is generated from training corpus.",
                "When a new term occurs in testing process, there are two solutions: simply ignore the new term or set df of the term as a small const (e.g. df = 1).",
                "The new term receives too low weight in the first solution (0) and too high weight in the second solution.",
                "In incremental TF-IDF model, document frequencies are updated dynamically in each time step t: 1( ) ( ) ( )t t D tdf w df w df w−= + (1) where Dt represents news story set received in time t, and dfDt(w) means the number of documents that term w occurs in, and dft(w) means the total number of documents that term w occurs in before time t. In this work, each time window includes 50 news stories.",
                "Thus, each story d received in t is represented as follows: 1 2{ ( , , ), ( , , ),..., ( , , )}nd weight d t w weight d t w weight d t w→ where n means the number of distinct terms in story d, and ( , , )weight d t w means the weight of term w in story d at time t: log( ( , ) 1) log(( 1) /( ( ) 0.5)) ( , , ) log( ( , ) 1) log(( 1) /( ( ) 0.5)) t t t t w d tf d w N df w weight d t w tf d w N df w ∈ + + + = + + +∑ (2) where Nt means the total number of news stories before time t, and tf(d,w) means how many times term w occurs in news story d. 3.2 Similarity Calculation We use Hellinger distance for the calculation of similarity between two stories, for two stories d and d at time t, their similarity is defined as follows: , ( , , ) ( , , ) * ( , , ) w d d sim d d t weight d t w weight d t w ∈ = ∑ (3) 3.3 Detection Procedure For each story d received in time step t, the value ( ) ( ) ( ) ( ( , , )) time d time d n d max sim d d t < = (4) is a score used to determine whether d is a story about a new topic and at the same time is an indication of the confidence in our decision [8]. time(d) means the publication time of story d. If the score exceeds the thresholdθ new, then there exists a sufficiently similar document, thus d is a old story, otherwise, there is no sufficiently similar previous document, thus d is an new story. 4.",
                "New NED Procedure Traditional NED systems can be classified into two main types on the aspect of detection procedure: (1) S-S type, in which the story on hand is compared to each story received previously, and use the highest similarity to determine whether current story is about a new event; (2) S-C type, in which the story on hand is compared to all previous clusters each of which representing a topic, and the highest similarity is used for final decision for current story.",
                "If the highest similarity exceeds thresholdθ new, then it is an old story, and put it into the most similar cluster; otherwise it is a new story and create a new cluster.",
                "Previous work show that the first manner is more accurate than the second one [4][5].",
                "Since sometimes stories within a topic drift far away from each other, a story may have very low similarity with its topic.",
                "So using similarities between stories for determining new story is better than using similarities between story and clusters.",
                "Nevertheless, the first manner needs much more comparing times which means the first manner is low efficient.",
                "We propose a new detection procedure which uses comparisons with previous clusters to help find the most similar story in less comparing times, and the final new event decision is made according to the most similar story.",
                "Therefore, we can get both the accuracy of S-S type methods and the efficiency of S-C type methods.",
                "The new procedure creates a news indexing-tree dynamically, in which similar stories are put together to form a hierarchy of clusters.",
                "We index similar stories together by their common ancestor (a cluster node).",
                "Dissimilar stories are indexed in different clusters.",
                "When a story is coming, we use comparisons between the current story and previous hierarchical clusters to help find the most similar story which is useful for new event decision.",
                "After the new event decision is made, the current story is inserted to the indexing-tree for the following detection.",
                "The news indexing-tree is defined formally as follows: S-Tree = {r, NC , NS , E} where r is the root of S-Tree, NC is the set of all cluster nodes, NS is the set of all story nodes, and E is the set of all edges in S-Tree.",
                "We define a set of constraints for a S-Tree: ⅰ . , is an non-terminal node in the treeC i i N i∀ ∈ → ⅱ . , is a terminal node in the treeS i i N i∀ ∈ → ⅲ . , out degree of is at least 2C i i N i∀ ∈ → ⅳ . , is represented as the centroid of its desendantsC i i iN∀ ∈ → For a news story di, the comparison procedure and inserting procedure based on indexing-tree are defined as follows.",
                "An example is shown by Figure 1 and Figure 2.",
                "Figure 1.",
                "Comparison procedure Figure 2.",
                "Inserting procedure Comparison procedure: Step 1: compare di to all the direct child nodes of r and select λ nodes with highest similarities, e.g., C1 2 and C1 3 in Figure 1.",
                "Step 2: for each selected node in the last step, e.g.",
                "C1 2, compare di to all its direct child nodes, and select λ nodes with highest similarities, e.g.",
                "C2 2 and d8.",
                "Repeat step 2 for all non-terminal nodes.",
                "Step 3: record the terminal node with the highest similarty to di, e.g. s5, and the similarity value (0.20).",
                "Inserting di to the S-tree with r as root: Find the node n which is direct child of r in the path from r to the terminal node with highest similarity s, e.g.",
                "C1 2.",
                "If s is smaller than θ init+(h-1)δ , then add di to the tree as a direct child of r. Otherwise, if n is a terminal node, then create a cluster node instead of n, and add both n and di as its direct children; if n is an non-terminal node, then repeat this procedure and insert di to the sub-tree with n as root recursively.",
                "Here h is the length between n and the root of S-tree.",
                "The more the stories in a cluster similar to each other, the better the cluster represents the stories in it.",
                "Hence we add no constraints on the maximum of trees height and degree of a node.",
                "Therefore, we cannot give the complexity of this indexing-tree based procedure.",
                "But we will give the number of comparing times needed by the new procedure in our experiments in section7. 5.",
                "Term Reweighting Methods In this section, two term reweighting methods are proposed to improve NED accuracy.",
                "In the first method, a new way is explored for better using of cluster (topic) information.",
                "The second one finds a better way to make use of named entities based on news classification. 5.1 Term Reweighting Based on Distribution Distance TF-IDF is the most prevalent model used in information retrieval systems.",
                "The basic idea is that the fewer documents a term appears in, the more important the term is in discrimination of documents (relevant or not relevant to a query containing the term).",
                "Nevertheless, in TDT domain, we need to discriminate documents with regard to topics rather than queries.",
                "Intuitively, using cluster (topic) vectors to compare with subsequent news stories should outperform using story vectors.",
                "Unfortunately, the experimental results do not support this intuition [4][5].",
                "Based on observation on data, we find the reason is that a news topic usually contains many directly or indirectly related events, while they all have their own sub-subjects which are usually different with each other.",
                "Take the topic described in section 1 as an example, events like the explosion and salvage have very low similarities with events about criminal trial, therefore stories about trial would have low similarity with the topic vector built on its previous events.",
                "This section focuses on how to effectively make use of topic information and at the same time avoid the problem of content decentralization.",
                "At first, we classify terms into 5 classes to help analysis the needs of the modified model: Term class A: terms that occur frequently in the whole corpus, e.g., year and people.",
                "Terms of this class should be given low weights because they do not help much for topic discrimination.",
                "Term class B: terms that occur frequently within a news category, e.g., election, storm.",
                "They are useful to distinguish two stories in different news categories.",
                "However, they cannot provide information to determine whether two stories are on the same or different topics.",
                "In another words, term election and term storm are not helpful in differentiate two election campaigns and two storm disasters.",
                "Therefore, terms of this class should be assigned lower weights.",
                "Term class C: terms that occur frequently in a topic, and infrequently in other topics, e.g., the name of a crash plane, the name of a specific hurricane.",
                "News stories that belong to different topics rarely have overlap terms in this class.",
                "The more frequently a term appears in a topic, the more important the term is for a story belonging to the topic, therefore the term should be set higher weight.",
                "Term class D: terms that appear in a topic exclusively, but not frequently.",
                "For example, the name of a fireman who did very well in a salvage action, which may appears in only two or three stories but never appeared in other topics.",
                "Terms of this type should receive more weights than in TF-IDF model.",
                "However, since they are not popular in the topic, it is not appropriate to give them too high weights.",
                "Term class E: terms with low document frequency, and appear in different topics.",
                "Terms of this class should receive lower weights.",
                "Now we analyze whether TF-IDF model can give proper weights to the five classes of terms.",
                "Obviously, terms of class A are lowly weighted in TF-IDF model, which is conformable with the requirement described above.",
                "In TF-IDF model, terms of class B are highly dependant with the number of stories in a news class.",
                "TF-IDF model cannot provide low weights if the story containing the term belongs to a relative small news class.",
                "For a term of class C, the more frequently it appears in a topic, the less weight TFIDF model gives to it.",
                "This strongly conflicts with the requirement of terms in class C. For terms of class D, TF-IDF model gives them high weights correctly.",
                "But for terms of class E, TF-IDF model gives high weights to them which are not conformable with the requirement of low weights.",
                "To sum up, terms of class B, C, E cannot be properly weighted in TF-IDF model.",
                "So, we propose a modified model to resolve this problem.",
                "When θ init andθ new are set closely, we assume that most of the stories in a first-level cluster (a direct child node of root node) are on the same topic.",
                "Therefore, we make use of a first-level cluster to capture term distribution (df for all the terms within the cluster) within the topic dynamically.",
                "KL divergence of term distribution in a first-level cluster and the whole story set is used to adjust term weights:  ( , , ) * (1 * ( || )) ( , , ) ( , , ) * (1 * ( || )) cw tw cw tw w d D weight d t w KL P P weight d t w weight d t w KL P P γ γ ∈ + = +∑ (5) where ( ) ( ) ( ) ( ) 1,cw cw c c c c df w df w p y p y N N = = − (6) ( ) ( ) ( ) ( ) 1,t t tw tw t t df w df w p y p y N N = = − (7) where dfc(w) is the number of documents containing term w within cluster C, and Nc is the number of documents in cluster C, and Nt is the total number of documents that arrive before time step t. γ is a const parameter, now is manually set 3.",
                "KL divergence is defined as follows [17]: ( ) ( || ) ( ) log ( )x p x KL P Q p x q x = ∑ (8) The basic idea is: for a story in a topic, the more a term occurs within the topic, and the less it occurs in other topics, it should be assigned higher weights.",
                "Obviously, modified model can meet all the requirements of the five term classes listed above. 5.2 Term Reweighting Based on Term Type and Story Class Previous work found that some classes of news stories could achieve good improvements by giving extra weight to named entities.",
                "But we find that terms of different types should be given different amount of extra weight for different classes of news stories.",
                "We use open-NLP1 to recognize named entity types and part-ofspeech tags for terms that appear in news stories.",
                "Named entity types include person name, organization name, location name, date, time, money and percentage, and five POSs are selected: none (NN), verb (VB), adjective (JJ), adverb (RB) and cardinal number (CD).",
                "Statistical analysis shows topic-level discriminative terms types for different classes of stories.",
                "For the sake of convenience, named entity type and part-of-speech tags are uniformly called term type in subsequent sections.",
                "Determining whether two stories are about the same topic is a basic component for NED task.",
                "So at first we use 2 χ statistic to compute correlations between terms and topics.",
                "For a term t and a topic T, a contingence table is derived: Table 1.",
                "A 2×2 Contingence Table Doc Number belong to topic T not belong to topic T include t A B not include t C D The 2 χ statistic for a specific term t with respect to topic T is defined to be [16]: 2 2 ( , ) ( ) * ( ) ( ) * ( ) * ( ) * ( ) w T A B C D AD CB A C B D A B C D χ = + + + − + + + + (9) News topics for the TDT task are further classified into 11 rules of interpretations (ROIs) 2 .",
                "The ROI can be seen as a higher level class of stories.",
                "The average correlation between a term type and a topic ROI is computed as: 2 avg 2 ( , )( ( , ) )k m m km kT R w P w TP R p w T R P χ χ ∈ ∈ ∑ ∑（ , ）= 1 1 k=1…K, m=1…M (10) where K is the number of term types (set 12 constantly in the paper).",
                "M is the number news classes (ROIs, set 11 in the paper).",
                "Pk represents the set of all terms of type k, and Rm represents the set of all topics of class m, p(t,T) means the probability that t occurs in topic T. Because of limitation of space, only parts of the term types (9 term types) and parts of news classes (8 classes) are listed in table 2 with the average correlation values between them.",
                "The statistics is derived from labeled data in TDT2 corpus. (Results in table 2 are already normalized for convenience in comparison.)",
                "The statistics in table 2 indicates the usefulness of different term types in topic discrimination with respect to different news classes.",
                "We can see that, location name is the most useful term type for three news classes: Natural Disasters, Violence or War, Finances.",
                "And for three other categories Elections, Legal/Criminal Cases, Science and Discovery, person name is the most discriminative term type.",
                "For Scandals/Hearings, date is the most important information for topic discrimination.",
                "In addition, Legal/Criminal Cases and Finance topics have higher correlation with money terms, while Science and Discovery have higher correlation with percentage terms.",
                "Non-name terms are more stable for different classes. 1 . http://opennlp.sourceforge.net/ 2 . http://projects.ldc.upenn.edu/TDT3/Guide/label.html From the analysis of table 2, it is reasonable to adjust term weight according to their term type and the news class the story belongs to.",
                "New term weights are reweighted as follows: ( ) ( ) ( ) ( ) ( , , ) * ( , , ) ( , , ) * class d D type w T class d D type w w d weight d t w weight d t w weight d t w α α ∈ = ∑ (11) where type(w) represents the type of term w, and class(d) represents the class of story d, c kα is reweighting parameter for news class c and term type k. In the work, we just simply use statistics in table 2 as the reweighting parameters.",
                "Even thought using the statistics directly may not the best choice, we do not discuss how to automatically obtain the best parameters.",
                "We will try to use machine learning techniques to obtain the best parameters in the future work.",
                "In the work, we use BoosTexter [20] to classify all stories into one of the 11 ROIs.",
                "BoosTexter is a boosting based machine learning program, which creates a series of simple rules for building a classifier for text or attribute-value data.",
                "We use term weight generated using TF-IDF model as feature for story classification.",
                "We trained the model on the 12000 judged English stories in TDT2, and classify the rest of the stories in TDT2 and all stories in TDT3.",
                "Classification results are used for term reweighting in formula (11).",
                "Since the class labels of topic-off stories are not given in TDT datasets, we cannot give the classification accuracy here.",
                "Thus we do not discuss the effects of classification accuracy to NED performance in the paper. 6.",
                "EXPERIMENTAL SETUP 6.1 Datasets We used two LDC [18] datasets TDT2 and TDT3 for our experiments.",
                "TDT2 contains news stories from January to June 1998.",
                "It contains around 54,000 stories from sources like ABC, Associated Press, CNN, New York Times, Public Radio International, Voice of America etc.",
                "Only English stories in the collection were considered.",
                "TDT3 contains approximately 31,000 English stories collected from October to December 1998.",
                "In addition to the sources used in TDT2, it also contains stories from NBC and MSNBC TV broadcasts.",
                "We used transcribed versions of the TV and radio broadcasts besides textual news.",
                "TDT2 dataset is labeled with about 100 topics, and approximately 12,000 English stories belong to at least one of these topics.",
                "TDT3 dataset is labeled with about 120 topics, and approximately 8000 English stories belong to at least one of these topics.",
                "All the topics are classified into 11 Rules of Interpretation: (1)Elections, (2)Scandals/Hearings, (3)Legal/Criminal Cases, (4)Natural Disasters, (5)Accidents, (6)Ongoing Violence or War, (7)Science and Discovery News, (8)Finance, (9)New Law, (10)Sports News, (11)MISC.",
                "News. 6.2 Evaluation Metric TDT uses a cost function CDet that combines the probabilities of missing a new story and a false alarm [19]: * * * *Det Miss Miss Target FA FA NontargetC C P P C P P= + (12) Table 2.",
                "Average correlation between term types and news classes where CMiss means the cost of missing a new story, PMiss means the probability of missing a new story, and PTarget means the probability of seeing a new story in the data; CFA means the cost of a false alarm, PFA means the probability of a false alarm, and PNontarget means the probability of seeing an old story.",
                "The cost CDet is normalized such that a perfect system scores 0 and a trivial system, which is the better one of mark all stories as new or old, scores 1: ( ( * , * ) ) Det Det Miss Target FA Nontarget C Norm C min C P C P = (13) New event detection system gives two outputs for each story.",
                "The first part is yes or no indicating whether the story triggers a new event or not.",
                "The second part is a score indicating confidence of the first decision.",
                "Confidence scores can be used to plot DET curve, i.e., curves that plot false alarm vs. miss probabilities.",
                "Minimum normalized cost can be determined if optimal threshold on the score were chosen. 7.",
                "EXPERIMENTAL RESULTS 7.1 Main Results To test the approaches proposed in the model, we implemented and tested five systems: System-1: this system is used as baseline.",
                "It is implemented based on the basic model described in section 3, i.e., using incremental TF-IDF model to generate term weights, and using Hellinger distance to compute document similarity.",
                "Similarity score normalization is also employed [8].",
                "S-S detection procedure is used.",
                "System-2: this system is the same as system-1 except that S-C detection procedure is used.",
                "System-3: this system is the same as system-1 except that it uses the new detection procedure which is based on indexing-tree.",
                "System-4: implemented based on the approach presented in section 5.1, i.e., terms are reweighted according to the distance between term distributions in a cluster and all stories.",
                "The new detection procedure is used.",
                "System-5: implemented based on the approach presented in section 5.2, i.e., terms of different types are reweighted according to news class using trained parameters.",
                "The new detection procedure is used.",
                "The following are some other NED systems: System-6: [21] for each pair of stories, it computes three similarity values for named entity, non-named entity and all terms respectively.",
                "And employ Support Vector Machine to predict new or old using the similarity values as features.",
                "System-7: [8] it extended a basic incremental TF-IDF model to include source-specific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, etc.",
                "System-8: [13] it split document representation into two parts: named entities and non-named entities, and choose one effective part for each news class.",
                "Table 3 and table 4 show topic-weighted normalized costs and comparing times on TDT2 and TDT3 datasets respectively.",
                "Since no heldout data set for fine-tuning the threshold θ new was available for experiments on TDT2, we only report minimum normalized costs for our systems in table 3.",
                "System-5 outperforms all other systems including system-6, and it performs only 2.78e+8 comparing times in detection procedure which is only 13.4% of system-1.",
                "Table 3.",
                "NED results on TDT2 Systems Min Norm(CDet) Cmp times System-1 0.5749 2.08e+9 System-2① 0.6673 3.77e+8 System-3② 0.5765 2.81e+8 System-4② 0.5431 2.99e+8 System-5② 0.5089 2.78e+8 System-6 0.5300 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 When evaluating on the normalized costs on TDT3, we use the optimal thresholds obtained from TDT2 data set for all systems.",
                "System-2 reduces comparing times to 1.29e+9 which is just 18.3% of system-1, but at the same time it also gets a deteriorated minimum normalized cost which is 0.0499 higher than system-1.",
                "System-3 uses the new detection procedure based on news indexing-tree.",
                "It requires even less comparing times than system-2.",
                "This is because story-story comparisons usually yield greater similarities than story-cluster ones, so stories tend to be combined Location Person Date Organization Money Percentage NN JJ CD Elections 0.37 1 0.04 0.58 0.08 0.03 0.32 0.13 0.1 Scandals/Hearings 0.66 0.62 0.28 1 0.11 0.02 0.27 0.13 0.05 Legal/Criminal Cases 0.48 1 0.02 0.62 0.15 0 0.22 0.24 0.09 Natural Disasters 1 0.27 0 0.04 0.04 0 0.25 0.04 0.02 Violence or War 1 0.36 0.02 0.14 0.02 0.04 0.21 0.11 0.02 Science and Discovery 0.11 1 0.01 0.22 0.08 0.12 0.19 0.08 0.03 Finances 1 0.45 0.04 0.98 0.13 0.02 0.29 0.06 0.05 Sports 0.16 0.27 0.01 1 0.02 0 0.11 0.03 0.01 together in system-3.",
                "And system-3 is basically equivalent to system-1 in accuracy results.",
                "System-4 adjusts term weights based on the distance of term distributions between the whole corpus and cluster story set, yielding a good improvement by 0.0468 compared to system-1.",
                "The best system (system-5) has a minimum normalized cost 0.5012, which is 0.0797 better than system-1, and also better than any other results previously reported for this dataset [8, 13].",
                "Further more, system-5 only needs 1.05e+8 comparing times which is 14.9% of system-1.",
                "Table 4.",
                "NED results on TDT3 Systems Norm(CDet) Min Norm(CDet) Cmp times System-1 0.6159 0.5809 7.04e+8 System-2① 0.6493 0.6308 1.29e+8 System-3② 0.6197 0.5868 1.03e+8 System-4② 0.5601 0.5341 1.03e+8 System-5② 0.5413 0.5012 1.05e+8 System-7 -- 0.5783 -System-8 -- 0.5229 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 Figure5 shows the five DET curves for our systems on data set TDT3.",
                "System-5 achieves the minimum cost at a false alarm rate of 0.0157 and a miss rate of 0.4310.",
                "We can observe that System4 and System-5 obtain lower miss probability at regions of low false alarm probabilities.",
                "The hypothesis is that, more weight value is transferred to key terms of topics from non-key terms.",
                "Similarity score between two stories belonging to different topics are lower than before, because their overlapping terms are usually not key terms of their topics. 7.2 Parameter selection for indexing-tree detection Figure 3 shows the minimum normalized costs obtained by system-3 on TDT3 using different parameters.",
                "Theθ init parameter is tested on six values spanning from 0.03 to 0.18.",
                "And the λ parameter is tested on four values 1, 2, 3 and 4.",
                "We can see that, whenθ init is set to 0.12, which is the closest one toθ new, the costs are lower than others.",
                "This is easy to explain, because when stories belonging to the same topic are put in a cluster, it is more reasonable for the cluster to represent the stories in it.",
                "When parameter λ is set to 3 or 4, the costs are better than other cases, but there is no much difference between 3 and 4. 0 0.05 0.1 0.15 0.2 1 2 3 4 0.5 0.6 0.7 0.8 0.9 1 θ-initλ MinCost 0.6 0.65 0.7 0.75 0.8 0.85 0.9 Figure 3.",
                "Min Cost on TDT3 (δ =0.15) 0 0.05 0.1 0.15 0.2 1 2 3 4 0 0.5 1 1.5 2 2.5 x 10 8 θ-init λ Comparingtimes 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 8 Figure 4.",
                "Comparing times on TDT3 (δ =0.15) Figure 4 gives the comparing times used by system-3 on TDT3 with the same parameters as figure 3.",
                "The comparing times are strongly dependent onθ init.",
                "Because the greaterθ init is, the less stories combined together, the more comparing times are needed for new event decision.",
                "So we useθ init =0.13,λ =3,δ =0.15 for system-3, 4, and 5.",
                "In this parameter setting, we can get both low minimum normalized costs and less comparing times. 8.",
                "CONCLUSION We have proposed a news indexing-tree based detection procedure in our model.",
                "It reduces comparing times to about one seventh of traditional method without hurting NED accuracy.",
                "We also have presented two extensions to the basic TF-IDF model.",
                "The first extension is made by adjust term weights based on term distributions between the whole corpus and a cluster story set.",
                "And the second extension to basic TF-IDF model is better use of term types (named entities types and part-of-speed) according to news categories.",
                "Our experimental results on TDT2 and TDT3 datasets show that both of the two extensions contribute significantly to improvement in accuracy.",
                "We did not consider news time information as a clue for NED task, since most of the topics last for a long time and TDT data sets only span for a relative short period (no more than 6 months).",
                "For the future work, we want to collect news set which span for a longer period from internet, and integrate time information in NED task.",
                "Since topic is a relative coarse-grained news cluster, we also want to refine cluster granularity to event-level, and identify different events and their relations within a topic.",
                "Acknowledgments This work is supported by the National Natural Science Foundation of China under Grant No. 90604025.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsor. 9.",
                "REFERENCES [1] http://www.nist.gov/speech/tests/tdt/index.htm [2] In Topic Detection and Tracking.",
                "Event-based Information Organization.",
                "Kluwer Academic Publishers, 2002. .01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90 1 2 5 10 20 40 60 80 90 False Alarm Probability (in %) MissProbability(in%) SYSTEM1 Topic Weighted Curve SYSTEM1 Min Norm(Cost) SYSTEM2 Topic Weighted Curve SYSTEM2 Min Norm(Cost) SYSTEM3 Topic Weighted Curve SYSTEM3 Min Norm(Cost) SYSTEM4 Topic Weighted Curve SYSTEM4 Min Norm(Cost) SYSTEM5 Topic Weighted Curve SYSTEM5 Min Norm(Cost) Random Performance Figure 5.",
                "DET curves on TDT3 [3] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B.T.",
                "Archibald, and X. Liu.",
                "Learning Approaches for Detecting and Tracking News Events.",
                "In IEEE Intelligent Systems Special Issue on Applications of Intelligent Information Retrieval, volume 14 (4), 1999, 32-43. [4] Y. Yang, T. Pierce, and J. Carbonell.",
                "A Study on Retrospective and On-line Event Detection.",
                "In Proceedings of SIGIR-98, Melbourne, Australia, 1998, 28-36. [5] J. Allan, V. Lavrenko, D. Malin, and R. Swan.",
                "Detections, Bounds, and Timelines: Umass and tdt-3.",
                "In Proceedings of Topic Detection and Tracking Workshop (TDT-3), Vienna, VA, 2000, 167-174. [6] R. Papka and J. Allan.",
                "On-line New Event Detection Using Single Pass Clustering TITLE2:.",
                "Technical Report UM-CS1998-021, 1998. [7] W. Lam, H. Meng, K. Wong, and J.",
                "Yen.",
                "Using Contextual Analysis for News Event Detection.",
                "International Journal on Intelligent Systems, 2001, 525-546. [8] B. Thorsten, C. Francine, and F. Ayman.",
                "A System for New Event Detection.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2003, 330-337. [9] S. Nicola and C. Joe.",
                "Combining Semantic and Syntactic Document Classifiers to Improve First Story Detection.",
                "In Proceedings of the 24th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topicconditioned Novelty Detection.",
                "In Proceedings of the 8th ACM SIGKDD International Conference, ACM Press. 2002, 688-693. [11] M. Juha, A.M. Helena, and S. Marko.",
                "Applying Semantic Classes in Event Detection and Tracking.",
                "In Proceedings of International Conference on Natural Language Processing (ICON 2002), 2002, pages 175-183. [12] M. Juha, A.M. Helena, and S. Marko.",
                "Simple Semantics in Topic Detection and Tracking.",
                "Information Retrieval, 7(3-4): 2004, 347-368. [13] K. Giridhar and J. Allan.",
                "Text Classification and Named Entities for New Event Detection.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference, New York, NY, USA.",
                "ACM Press. 2004, 297-304. [14] J. P. Callan, W. B. Croft, and S. M. Harding.",
                "The INQUERY Retrieval System.",
                "In Proceedings of DEXA-92, 3rd International Conference on Database and Expert Systems Applications, 1992, 78-83. [15] R. Krovetz.",
                "Viewing Morphology as An Inference Process.",
                "In Proceedings of ACM SIGIR93, 1993, 61-81. [16] Y. Yang and J. Pedersen.",
                "A Comparative Study on Feature Selection in Text Categorization.",
                "In J. D. H. Fisher, editor, The Fourteenth International Conference on Machine Learning (ICML97), Morgan Kaufmann, 1997, 412-420. [17] T. M. Cover, and J.A.",
                "Thomas.",
                "Elements of Information Theory.",
                "Wiley. 1991. [18] The linguistic data consortium, http://www.ldc,upenn.edu/. [19] The 2001 TDT task definition and evaluation plan, http://www.nist.gov/speech/tests/tdt/tdt2001/evalplan.htm. [20] R. E. Schapire and Y.",
                "Singer.",
                "Boostexter: A Boosting-based System for Text Categorization.",
                "In Machine Learning 39(2/3):1, Kluwer Academic Publishers, 2000, 35-168. [21] K. Giridhar and J. Allan. 2005.",
                "Using Names and Topics for New Event Detection.",
                "In Proceedings of Human Technology Conference and Conference on Empirical Methods in Natural Language, Vancouver, 2005, 121-128"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        }
    }
}