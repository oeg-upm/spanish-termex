{
    "id": "H-29",
    "original_text": "Estimation and Use of Uncertainty in Pseudo-relevance Feedback Kevyn Collins-Thompson and Jamie Callan Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213-8213 U.S.A. {kct | callan}@cs.cmu.edu ABSTRACT Existing pseudo-relevance feedback methods typically perform averaging over the top-retrieved documents, but ignore an important statistical dimension: the risk or variance associated with either the individual document models, or their combination. Treating the baseline feedback method as a black box, and the output feedback model as a random variable, we estimate a posterior distribution for the feedback model by resampling a given querys top-retrieved documents, using the posterior mean or mode as the enhanced feedback model. We then perform model combination over several enhanced models, each based on a slightly modified query sampled from the original query. We find that resampling documents helps increase individual feedback model precision by removing noise terms, while sampling from the query improves robustness (worst-case performance) by emphasizing terms related to multiple query aspects. The result is a meta-feedback algorithm that is both more robust and more precise than the original strong baseline method. Categories and Subject Descriptors: H.3.3 [Information Retrieval]: Retrieval Models General Terms: Algorithms, Experimentation 1. INTRODUCTION Uncertainty is an inherent feature of information retrieval. Not only do we not know the queries that will be presented to our retrieval algorithm ahead of time, but the users information need may be vague or incompletely specified by these queries. Even if the query were perfectly specified, language in the collection documents is inherently complex and ambiguous and matching such language effectively is a formidable problem by itself. With this in mind, we wish to treat many important quantities calculated by the retrieval system, whether a relevance score for a document, or a weight for a query expansion term, as random variables whose true value is uncertain but where the uncertainty about the true value may be quantified by replacing the fixed value with a probability distribution over possible values. In this way, retrieval algorithms may attempt to quantify the risk or uncertainty associated with their output rankings, or improve the stability or precision of their internal calculations. Current algorithms for pseudo-relevance feedback (PRF) tend to follow the same basic method whether we use vector space-based algorithms such as Rocchios formula [16], or more recent language modeling approaches such as Relevance Models [10]. First, a set of top-retrieved documents is obtained from an initial query and assumed to approximate a set of relevant documents. Next, a single feedback model vector is computed according to some sort of average, centroid, or expectation over the set of possibly-relevant document models. For example, the document vectors may be combined with equal weighting, as in Rocchio, or by query likelihood, as may be done using the Relevance Model1 . The use of an expectation is reasonable for practical and theoretical reasons, but by itself ignores potentially valuable information about the risk of the feedback model. Our main hypothesis in this paper is that estimating the uncertainty in feedback is useful and leads to better individual feedback models and more robust combined models. Therefore, we propose a method for estimating uncertainty associated with an individual feedback model in terms of a posterior distribution over language models. To do this, we systematically vary the inputs to the baseline feedback method and fit a Dirichlet distribution to the output. We use the posterior mean or mode as the improved feedback model estimate. This process is shown in Figure 1. As we show later, the mean and mode may vary significantly from the single feedback model proposed by the baseline method. We also perform model combination using several improved feedback language models obtained by a small number of new queries sampled from the original query. A models weight combines two complementary factors: the models probability of generating the query, and the variance of the model, with high-variance models getting lower weight. 1 For example, an expected parameter vector conditioned on the query observation is formed from top-retrieved documents, which are treated as training strings (see [10], p. 62). Figure 1: Estimating the uncertainty of the feedback model for a single query. 2. SAMPLING-BASED FEEDBACK In Sections 2.1-2.5 we describe a general method for estimating a probability distribution over the set of possible language models. In Sections 2.6 and 2.7 we summarize how different query samples are used to generate multiple feedback models, which are then combined. 2.1 Modeling Feedback Uncertainty Given a query Q and a collection C, we assume a probabilistic retrieval system that assigns a real-valued document score f(D, Q) to each document D in C, such that the score is proportional to the estimated probability of relevance. We make no other assumptions about f(D, Q). The nature of f(D, Q) may be complex: for example, if the retrieval system supports structured query languages [12], then f(D, Q) may represent the output of an arbitrarily complex inference network defined by the structured query operators. In theory, the scoring function can vary from query to query, although in this study for simplicity we keep the scoring function the same for all queries. Our specific query method is given in Section 3. We treat the feedback algorithm as a black box and assume that the inputs to the feedback algorithm are the original query and the corresponding top-retrieved documents, with a score being given to each document. We assume that the output of the feedback algorithm is a vector of term weights to be used to add or reweight the terms in the representation of the original query, with the vector normalized to form a probability distribution. We view the the inputs to the feedback black box as random variables, and analyze the feedback model as a random variable that changes in response to changes in the inputs. Like the document scoring function f(D, Q), the feedback algorithm may implement a complex, non-linear scoring formula, and so as its inputs vary, the resulting feedback models may have a complex distribution over the space of feedback models (the sample space). Because of this potential complexity, we do not attempt to derive a posterior distribution in closed form, but instead use simulation. We call this distribution over possible feedback models the feedback model distribution. Our goal in this section is to estimate a useful approximation to the feedback model distribution. For a specific framework for experiments, we use the language modeling (LM) approach for information retrieval [15]. The score of a document D with respect to a query Q and collection C is given by p(Q|D) with respect to language models ˆθQ and ˆθD estimated for the query and document respectively. We denote the set of k top-retrieved documents from collection C in response to Q by DQ(k, C). For simplicity, we assume that queries and documents are generated by multinomial distributions whose parameters are represented by unigram language models. To incorporate feedback in the LM approach, we assume a model-based scheme in which our goal is take the query and resulting ranked documents DQ(k, C) as input, and output an expansion language model ˆθE, which is then interpolated with the original query model ˆθQ: ˆθNew = (1 − α) · ˆθQ + α · ˆθE (1) This includes the possibility of α = 1 where the original query mode is completely replaced by the feedback model. Our sample space is the set of all possible language models LF that may be output as feedback models. Our approach is to take samples from this space and then fit a distribution to the samples using maximum likelihood. For simplicity, we start by assuming the latent feedback distribution has the form of a Dirichlet distribution. Although the Dirichlet is a unimodal distribution, and in general quite limited in its expressiveness in the sample space, it is a natural match for the multinomial language model, can be estimated quickly, and can capture the most salient features of confident and uncertain feedback models, such as the overall spread of the distibution. 2.2 Resampling document models We would like an approximation to the posterior distribution of the feedback model LF . To accomplish this, we apply a widely-used simulation technique called bootstrap sampling ([7], p. 474) on the input parameters, namely, the set of top-retrieved documents. Bootstrap sampling allows us to simulate the approximate effect of perturbing the parameters within the black box feedback algorithm by perturbing the inputs to that algorithm in a systematic way, while making no assumptions about the nature of the feedback algorithm. Specifically, we sample k documents with replacement from DQ(k, C), and calculate an expansion language model θb using the black box feedback method. We repeat this process B times to obtain a set of B feedback language models, to which we then fit a Dirichlet distribution. Typically B is in the range of 20 to 50 samples, with performance being relatively stable in this range. Note that instead of treating each top document as equally likely, we sample according to the estimated probabilities of relevance of each document in DQ(k, C). Thus, a document is more likely to be chosen the higher it is in the ranking. 2.3 Justification for a sampling approach The rationale for our sampling approach has two parts. First, we want to improve the quality of individual feedback models by smoothing out variation when the baseline feedback model is unstable. In this respect, our approach resembles bagging [4], an ensemble approach which generates multiple versions of a predictor by making bootstrap copies of the training set, and then averages the (numerical) predictors. In our application, top-retrieved documents can be seen as a kind of noisy training set for relevance. Second, sampling is an effective way to estimate basic properties of the feedback posterior distribution, which can then be used for improved model combination. For example, a model may be weighted by its prediction confidence, estimated as a function of the variability of the posterior around the model. foo2-401.map-Dim:5434,Size:12*12units,gaussianneighborhood (a) Topic 401 Foreign minorities, Germany foo2-402.map-Dim:5698,Size:12*12units,gaussianneighborhood (b) Topic 402 Behavioral genetics foo2-459.map-Dim:8969,Size:12*12units,gaussianneighborhood (c) Topic 459 When can a lender foreclose on property Figure 2: Visualization of expansion language model variance using self-organizing maps, showing the distribution of language models that results from resampling the inputs to the baseline expansion method. The language model that would have been chosen by the baseline expansion is at the center of each map. The similarity function is JensenShannon divergence. 2.4 Visualizing feedback distributions Before describing how we fit and use the Dirichlet distribution over feedback models, it is instructive to view some examples of actual feedback model distributions that result from bootstrap sampling the top-retrieved documents from different TREC topics. Each point in our sample space is a language model, which typically has several thousand dimensions. To help analyze the behavior of our method we used a Self-Organizing Map (via the SOM-PAK package [9]), to flatten and visualize the high-dimensional density function2 . The density maps for three TREC topics are shown in Figure 2 above. The dark areas represent regions of high similarity between language models. The light areas represent regions of low similarity - the valleys between clusters. Each diagram is centered on the language model that would have been chosen by the baseline expansion. A single peak (mode) is evident in some examples, but more complex structure appears in others. Also, while the distribution is usually close to the baseline feedback model, for some topics they are a significant distance apart (as measured by JensenShannon divergence), as in Subfigure 2c. In such cases, the mode or mean of the feedback distribution often performs significantly better than the baseline (and in a smaller proportion of cases, significantly worse). 2.5 Fitting a posterior feedback distribution After obtaining feedback model samples by resampling the feedback model inputs, we estimate the feedback distribution. We assume that the multinomial feedback models {ˆθ1, . . . , ˆθB} were generated by a latent Dirichlet distribution with parameters {α1, . . . , αN }. To estimate the {α1, . . . , αN }, we fit the Dirichlet parameters to the B language model samples according to maximum likelihood using a generalized Newton procedure, details of which are given in Minka [13]. We assume a simple Dirichlet prior over the {α1, . . . , αN }, setting each to αi = μ · p(wi | C), where μ is a parameter and p(· | C) is the collection language model estimated from a set of documents from collection C. The parameter fitting converges very quickly - typically just 2 or 2 Because our points are language models in the multinomial simplex, we extended SOM-PAK to support JensenShannon divergence, a widely-used similarity measure between probability distributions. 3 iterations are enough - so that it is practical to apply at query-time when computational overhead must be small. In practice, we can restrict the calculation to the vocabulary of the top-retrieved documents, instead of the entire collection. Note that for this step we are re-using the existing retrieved documents and not performing additional queries. Given the parameters of an N-dimensional Dirichlet distribution Dir(α) the mean μ and mode x vectors are easy to calculate and are given respectively by μi = αiP αi (2) and xi = αi−1P αi−N . (3) We can then choose the language model at the mean or the mode of the posterior as the final enhanced feedback model. (We found the mode to give slightly better performance.) For information retrieval, the number of samples we will have available is likely to be quite small for performance reasons - usually less than ten. Moreover, while random sampling is useful in certain cases, it is perfectly acceptable to allow deterministic sampling distributions, but these must be designed carefully in order to approximate an accurate output variance. We leave this for future study. 2.6 Query variants We use the following methods for generating variants of the original query. Each variant corresponds to a different assumption about which aspects of the original query may be important. This is a form of deterministic sampling. We selected three simple methods that cover complimentary assumptions about the query. No-expansion Use only the original query. The assumption is that the given terms are a complete description of the information need. Leave-one-out A single term is left out of the original query. The assumption is that one of the query terms is a noise term. Single-term A single term is chosen from the original query. This assumes that only one aspect of the query, namely, that represented by the term, is most important. After generating a variant of the original query, we combine it with the original query using a weight αSUB so that we do not stray too far. In this study, we set αSUB = 0.5. For example, using the Indri [12] query language, a leave-oneout variant of the initial query that omits the term ireland for TREC topic 404 is: #weight(0.5 #combine(ireland peace talks) 0.5 #combine(peace talks)) 2.7 Combining enhanced feedback models from multiple query variants When using multiple query variants, the resulting enhanced feedback models are combined using Bayesian model combination. To do this, we treat each word as an item to be classified as belonging to a relevant or non-relevant class, and derive a class probability for each word by combining the scores from each query variant. Each score is given by that terms probability in the Dirichlet distribution. The term scores are weighted by the inverse of the variance of the term in the enhanced feedback models Dirichlet distribution. The prior probability of a words membership in the relevant class is given by the probability of the original query in the entire enhanced expansion model. 3. EVALUATION In this section we present results confirming the usefulness of estimating a feedback model distribution from weighted resampling of top-ranked documents, and of combining the feedback models obtained from different small changes in the original query. 3.1 General method We evaluated performance on a total of 350 queries derived from four sets of TREC topics: 51-200 (TREC-1&2), 351-400 (TREC-7), 401-450 (TREC-8), and 451-550 (wt10g, TREC-9&10). We chose these for their varied content and document properties. For example, wt10g documents are Web pages with a wide variety of subjects and styles while TREC-1&2 documents are more homogeneous news articles. Indexing and retrieval was performed using the Indri system in the Lemur toolkit [12] [1]. Our queries were derived from the words in the title field of the TREC topics. Phrases were not used. To generate the baseline queries passed to Indri, we wrapped the query terms with Indris #combine operator. For example, the initial query for topic 404 is: #combine(ireland peace talks) We performed Krovetz stemming for all experiments. Because we found that the baseline (Indri) expansion method performed better using a stopword list with the feedback model, all experiments used a stoplist of 419 common English words. However, an interesting side-effect of our resampling approach is that it tends to remove many stopwords from the feedback model, making a stoplist less critical. This is discussed further in Section 3.6. 3.2 Baseline feedback method For our baseline expansion method, we use an algorithm included in Indri 1.0 as the default expansion method. This method first selects terms using a log-odds calculation described by Ponte [14], but assigns final term weights using Lavrenkos relevance model[10]. We chose the Indri method because it gives a consistently strong baseline, is based on a language modeling approach, and is simple to experiment with. In a TREC evaluation using the GOV2 corpus [6], the method was one of the topperforming runs, achieving a 19.8% gain in MAP compared to using unexpanded queries. In this study, it achieves an average gain in MAP of 17.25% over the four collections. Indris expansion method first calculates a log-odds ratio o(v) for each potential expansion term v given by o(v) = X D log p(v|D) p(v|C) (4) over all documents D containing v, in collection C. Then, the expansion term candidates are sorted by descending o(v), and the top m are chosen. Finally, the term weights r(v) used in the expanded query are calculated based on the relevance model r(v) = X D p(q|D)p(v|D) p(v) p(D) (5) The quantity p(q|D) is the probability score assigned to the document in the initial retrieval. We use Dirichlet smoothing of p(v|D) with μ = 1000. This relevance model is then combined with the original query using linear interpolation, weighted by a parameter α. By default we used the top 50 documents for feedback and the top 20 expansion terms, with the feedback interpolation parameter α = 0.5 unless otherwise stated. For example, the baseline expanded query for topic 404 is: #weight(0.5 #combine(ireland peace talks) 0.5 #weight(0.10 ireland 0.08 peace 0.08 northern ...) 3.3 Expansion performance We measure our feedback algorithms effectiveness by two main criteria: precision, and robustness. Robustness, and the tradeoff between precision and robustness, is analyzed in Section 3.4. In this section, we examine average precision and precision in the top 10 documents (P10). We also include recall at 1,000 documents. For each query, we obtained a set of B feedback models using the Indri baseline. Each feedback model was obtained from a random sample of the top k documents taken with replacement. For these experiments, B = 30 and k = 50. Each feedback model contained 20 terms. On the query side, we used leave-one-out (LOO) sampling to create the query variants. Single-term query sampling had consistently worse performance across all collections and so our results here focus on LOO sampling. We used the methods described in Section 2 to estimate an enhanced feedback model from the Dirichlet posterior distribution for each query variant, and to combine the feedback models from all the query variants. We call our method resampling expansion and denote it as RS-FB here. We denote the Indri baseline feedback method as Base-FB. Results from applying both the baseline expansion method (Base-FB) and resampling expansion (RS-FB) are shown in Table 1. We observe several trends in this table. First, the average precision of RS-FB was comparable to Base-FB, achieving an average gain of 17.6% compared to using no expansion across the four collections. The Indri baseline expansion gain was 17.25%. Also, the RS-FB method achieved consistent improvements in P10 over Base-FB for every topic set, with an average improvement of 6.89% over Base-FB for all 350 topics. The lowest P10 gain over Base-FB was +3.82% for TREC-7 and the highest was +11.95% for wt10g. Finally, both Base-FB and RS-FB also consistently improved recall over using no expansion, with Base-FB achieving better recall than RS-FB for all topic sets. 3.4 Retrieval robustness We use the term robustness to mean the worst-case average precision performance of a feedback algorithm. Ideally, a robust feedback method would never perform worse than using the original query, while often performing better using the expansion. To evaluate robustness in this study, we use a very simple measure called the robustness index (RI)3 . For a set of queries Q, the RI measure is defined as: RI(Q) = n+ − n− |Q| (6) where n+ is the number of queries helped by the feedback method and n− is the number of queries hurt. Here, by helped we mean obtaining a higher average precision as a result of feedback. The value of RI ranges from a minimum 3 This is sometimes also called the reliability of improvement index and was used in Sakai et al. [17]. Collection NoExp Base-FB RS-FB TREC 1&2 AvgP 0.1818 0.2419 (+33.04%) 0.2406 (+32.24%) P10 0.4443 0.4913 (+10.57%) 0.5363 (+17.83%) Recall 15084/37393 19172/37393 15396/37393 TREC 7 AvgP 0.1890 0.2175 (+15.07%) 0.2169 (+14.75%) P10 0.4200 0.4320 (+2.85%) 0.4480 (+6.67%) Recall 2179/4674 2608/4674 2487/4674 TREC 8 AvgP 0.2031 0.2361 (+16.25%) 0.2268 (+11.70%) P10 0.3960 0.4160 (+5.05%) 0.4340 (+9.59%) Recall 2144/4728 2642/4728 2485/4728 wt10g AvgP 0.1741 0.1829 (+5.06%) 0.1946 (+11.78%) P10 0.2760 0.2630 (-4.71%) 0.2960 (+7.24%) Recall 3361/5980 3725/5980 3664/5980 Table 1: Comparison of baseline (Base-FB) feedback and feedback using re-sampling (RS-FB). Improvement shown for BaseFB and RS-FB is relative to using no expansion. (a) TREC 1&2 (upper curve); TREC 8 (lower curve) (b) TREC 7 (upper curve); wt10g (lower curve) Figure 3: The trade-off between robustness and average precision for different corpora. The x-axis gives the change in MAP over using baseline expansion with α = 0.5. The yaxis gives the Robustness Index (RI). Each curve through uncircled points shows the RI/MAP tradeoff using the simple small-α strategy (see text) as α decreases from 0.5 to zero in the direction of the arrow. Circled points represent the tradeoffs obtained by resampling feedback for α = 0.5. Collection N Base-FB RS-FB n− RI n− RI TREC 1&2 103 26 +0.495 15 +0.709 TREC 7 46 14 +0.391 10 +0.565 TREC 8 44 12 +0.455 12 +0.455 wt10g 91 48 -0.055 39 +0.143 Combined 284 100 +0.296 76 +0.465 Table 2: Comparison of robustness index (RI) for baseline feedback (Base-FB) vs. resampling feedback (RS-FB). Also shown are the actual number of queries hurt by feedback (n−) for each method and collection. Queries for which initial average precision was negligible (≤ 0.01) were ignored, giving the remaining query count in column N. of −1.0, when all queries are hurt by the feedback method, to +1.0 when all queries are helped. The RI measure does not take into account the magnitude or distribution of the amount of change across the set Q. However, it is easy to understand as a general indication of robustness. One obvious way to improve the worst-case performance of feedback is simply to use a smaller fixed α interpolation parameter, such as α = 0.3, placing less weight on the (possibly risky) feedback model and more on the original query. We call this the small-α strategy. Since we are also reducing the potential gains when the feedback model is right, however, we would expect some trade-off between average precision and robustness. We therefore compared the precision/robustness trade-off between our resampling feedback algorithm, and the simple small-α method. The results are summarized in Figure 3. In the figure, the curve for each topic set interpolates between trade-off points, beginning at x=0, where α = 0.5, and continuing in the direction of the arrow as α decreases and the original query is given more and more weight. As expected, robustness continuously increases as we move along the curve, but mean average precision generally drops as the gains from feedback are eliminated. For comparison, the performance of resampling feedback at α = 0.5 is shown for each collection as the circled point. Higher and to the right is better. This figure shows that resampling feedback gives a somewhat better trade-off than the small-α approach for 3 of the 4 collections. Figure 4: Histogram showing improved robustness of resampling feedback (RS-FB) over baseline feedback (Base-FB) for all datasets combined. Queries are binned by % change in AP compared to the unexpanded query. Collection DS + QV DS + No QV TREC 1&2 AvgP 0.2406 0.2547 (+5.86%) P10 0.5263 0.5362 (+1.88%) RI 0.7087 0.6515 (-0.0572) TREC 7 AvgP 0.2169 0.2200 (+1.43%) P10 0.4480 0.4300 (-4.02%) RI 0.5652 0.2609 (-0.3043) TREC 8 AvgP 0.2268 0.2257 (-0.49%) P10 0.4340 0.4200 (-3.23%) RI 0.4545 0.4091 (-0.0454) wt10g AvgP 0.1946 0.1865 (-4.16%) P10 0.2960 0.2680 (-9.46%) RI 0.1429 0.0220 (-0.1209) Table 3: Comparison of resampling feedback using document sampling (DS) with (QV) and without (No QV) combining feedback models from multiple query variants. Table 2 gives the Robustness Index scores for Base-FB and RS-FB. The RS-FB feedback method obtained higher robustness than Base-FB on three of the four topic sets, with only slightly worse performance on TREC-8. A more detailed view showing the distribution over relative changes in AP is given by the histogram in Figure 4. Compared to Base-FB, the RS-FB method achieves a noticable reduction in the number of queries significantly hurt by expansion (i.e. where AP is hurt by 25% or more), while preserving positive gains in AP. 3.5 Effect of query and document sampling methods Given our algorithms improved robustness seen in Section 3.4, an important question is what component of our system is responsible. Is it the use of document re-sampling, the use of multiple query variants, or some other factor? The results in Table 3 suggest that the model combination based on query variants may be largely account for the improved robustness. When query variants are turned off and the original query is used by itself with document sampling, there is little net change in average precision, a small decrease in P10 for 3 out of the 4 topic sets, but a significant drop in robustness for all topic sets. In two cases, the RI measure drops by more than 50%. We also examined the effect of the document sampling method on retrieval effectiveness, using two different strategies. The uniform weighting strategy ignored the relevance scores from the initial retrieval and gave each document in the top k the same probability of selection. In contrast, the relevance-score weighting strategy chose documents with probability proportional to their relevance scores. In this way, documents that were more highly ranked were more likely to be selected. Results are shown in Table 4. The relevance-score weighting strategy performs better overall, with significantly higher RI and P10 scores on 3 of the 4 topic sets. The difference in average precision between the methods, however, is less marked. This suggests that uniform weighting acts to increase variance in retrieval results: when initial average precision is high, there are many relevant documents in the top k and uniform sampling may give a more representative relevance model than focusing on the highly-ranked items. On the other hand, when initial precision is low, there are few relevant documents in the bottom ranks and uniform sampling mixes in more of the non-relevant documents. For space reasons we only summarize our findings on sample size here. The number of samples has some effect on precision when less than 10, but performance stabilizes at around 15 to 20 samples. We used 30 samples for our experiments. Much beyond this level, the additional benefits of more samples decrease as the initial score distribution is more closely fit and the processing time increases. 3.6 The effect of resampling on expansion term quality Ideally, a retrieval model should not require a stopword list when estimating a model of relevance: a robust statistical model should down-weight stopwords automatically depending on context. Stopwords can harm feedback if selected as feedback terms, because they are typically poor discriminators and waste valuable term slots. In practice, however, because most term selection methods resemble a tf · idf type of weighting, terms with low idf but very high tf can sometimes be selected as expansion term candidates. This happens, for example, even with the Relevance Model approach that is part of our baseline feedback. To ensure as strong a baseline as possible, we use a stoplist for all experiments reported here. If we turn off the stopword list, however, we obtain results such as those shown in Table 5 where four of the top ten baseline feedback terms for TREC topic 60 (said, but, their, not) are stopwords using the BaseFB method. (The top 100 expansion terms were selected to generate this example.) Indris method attempts to address the stopword problem by applying an initial step based on Ponte [14] to select less-common terms that have high log-odds of being in the top-ranked documents compared to the whole collection. Nevertheless, this does not overcome the stopword problem completely, especially as the number of feedback terms grows. Using resampling feedback, however, appears to mitigate Collection QV + Uniform QV + Relevance-score weighting weighting TREC 1&2 AvgP 0.2545 0.2406 (-5.46%) P10 0.5369 0.5263 (-1.97%) RI 0.6212 0.7087 (+14.09%) TREC 7 AvgP 0.2174 0.2169 (-0.23%) P10 0.4320 0.4480 (+3.70%) RI 0.4783 0.5652 (+18.17%) TREC 8 AvgP 0.2267 0.2268 (+0.04%) P10 0.4120 0.4340 (+5.34%) RI 0.4545 0.4545 (+0.00%) wt10g AvgP 0.1808 0.1946 (+7.63%) P10 0.2680 0.2960 (+10.45%) RI 0.0220 0.1099 (+399.5%) Table 4: Comparison of uniform and relevance-weighted document sampling. The percentage change compared to uniform sampling is shown in parentheses. QV indicates that query variants were used in both runs. Baseline FB p(wi|R) Resampling FB p(wi|R) said 0.055 court 0.026 court 0.055 pay 0.018 pay 0.034 federal 0.012 but 0.026 education 0.011 employees 0.024 teachers 0.010 their 0.024 employees 0.010 not 0.023 case 0.010 federal 0.021 their 0.009 workers 0.020 appeals 0.008 education 0.020 union 0.007 Table 5: Feedback term quality when a stoplist is not used. Feedback terms for TREC topic 60: merit pay vs seniority. the effect of stopwords automatically. In the example of Table 5, resampling feedback leaves only one stopword (their) in the top ten. We observed similar feedback term behavior across many other topics. The reason for this effect appears to be the interaction of the term selection score with the top-m term cutoff. While the presence and even proportion of particular stopwords is fairly stable across different document samples, their relative position in the top-m list is not, as sets of documents with varying numbers of better, lower-frequency term candidates are examined for each sample. As a result, while some number of stopwords may appear in each sampled document set, any given stopword tends to fall below the cutoff for multiple samples, leading to its classification as a high-variance, low-weight feature. 4. RELATED WORK Our approach is related to previous work from several areas of information retrieval and machine learning. Our use of query variation was inspired by the work of YomTov et al. [20], Carpineto et al. [5], and Amati et al. [2], among others. These studies use the idea of creating multiple subqueries and then examining the nature of the overlap in the documents and/or expansion terms that result from each subquery. Model combination is performed using heuristics. In particular, the studies of Amati et al. and Carpineto et al. investigated combining terms from individual distributional methods using a term-reranking combination heuristic. In a set of TREC topics they found wide average variation in the rank-distance of terms from different expansion methods. Their combination method gave modest positive improvements in average precision. The idea of examining the overlap between lists of suggested terms has also been used in early query expansion approaches. Xu and Crofts method of Local Context Analysis (LCA) [19] includes a factor in the empirically-derived weighting formula that causes expansion terms to be preferred that have connections to multiple query terms. On the document side, recent work by Zhou & Croft [21] explored the idea of adding noise to documents, re-scoring them, and using the stability of the resulting rankings as an estimate of query difficulty. This is related to our use of document sampling to estimate the risk of the feedback model built from the different sets of top-retrieved documents. Sakai et al. [17] proposed an approach to improving the robustness of pseudo-relevance feedback using a method they call selective sampling. The essence of their method is that they allow skipping of some top-ranked documents, based on a clustering criterion, in order to select a more varied and novel set of documents later in the ranking for use by a traditional pseudo-feedback method. Their study did not find significant improvements in either robustness (RI) or MAP on their corpora. Greiff, Morgan and Ponte [8] explored the role of variance in term weighting. In a series of simulations that simplified the problem to 2-feature documents, they found that average precision degrades as term frequency variance - high noiseincreases. Downweighting terms with high variance resulted in improved average precision. This seems in accord with our own findings for individual feedback models. Estimates of output variance have recently been used for improved text classification. Lee et al. [11] used queryspecific variance estimates of classifier outputs to perform improved model combination. Instead of using sampling, they were able to derive closed-form expressions for classifier variance by assuming base classifiers using simple types of inference networks. Ando and Zhang proposed a method that they call structural feedback [3] and showed how to apply it to query expansion for the TREC Genomics Track. They used r query variations to obtain R different sets Sr of top-ranked documents that have been intersected with the top-ranked documents obtained from the original query qorig. For each Si, the normalized centroid vector ˆwi of the documents is calculated. Principal component analysis (PCA) is then applied to the ˆwi to obtain the matrix Φ of H left singular vectors φh that are used to obtain the new, expanded query qexp = qorig + ΦT Φqorig. (7) In the case H = 1, we have a single left singular vector φ: qexp = qorig + (φT qorig)φ so that the dot product φT qorig is a type of dynamic weight on the expanded query that is based on the similarity of the original query to the expanded query. The use of variance as a feedback model quality measure occurs indirectly through the application of PCA. It would be interesting to study the connections between this approach and our own modelfitting method. Finally, in language modeling approaches to feedback, Tao and Zhai [18] describe a method for more robust feedback that allows each document to have a different feedback α. The feedback weights are derived automatically using regularized EM. A roughly equal balance of query and expansion model is implied by their EM stopping condition. They propose tailoring the stopping parameter η based on a function of some quality measure of feedback documents. 5. CONCLUSIONS We have presented a new approach to pseudo-relevance feedback based on document and query sampling. The use of sampling is a very flexible and powerful device and is motivated by our general desire to extend current models of retrieval by estimating the risk or variance associated with the parameters or output of retrieval processes. Such variance estimates, for example, may be naturally used in a Bayesian framework for improved model estimation and combination. Applications such as selective expansion may then be implemented in a principled way. While our study uses the language modeling approach as a framework for experiments, we make few assumptions about the actual workings of the feedback algorithm. We believe it is likely that any reasonably effective baseline feedback algorithm would benefit from our approach. Our results on standard TREC collections show that our framework improves the robustness of a strong baseline feedback method across a variety of collections, without sacrificing average precision. It also gives small but consistent gains in top10 precision. In future work, we envision an investigation into how varying the set of sampling methods used and the number of samples controls the trade-off between robustness, accuracy, and efficiency. Acknowledgements We thank Paul Bennett for valuable discussions related to this work, which was supported by NSF grants #IIS-0534345 and #CNS-0454018, and U.S. Dept. of Education grant #R305G03123. Any opinions, findings, and conclusions or recommendations expressed in this material are the authors. and do not necessarily reflect those of the sponsors. 6. REFERENCES [1] The Lemur toolkit for language modeling and retrieval. http://www.lemurproject.org. [2] G. Amati, C. Carpineto, and G. Romano. Query difficulty, robustness, and selective application of query expansion. In Proc. of the 25th European Conf. on Information Retrieval (ECIR 2004), pages 127-137. [3] R. K. Ando and T. Zhang. A high-performance semi-supervised learning method for text chunking. In Proc. of the 43rd Annual Meeting of the ACL, pages 1-9, June 2005. [4] L. Breiman. Bagging predictors. Machine Learning, 24(2):123-140, 1996. [5] C. Carpineto, G. Romano, and V. Giannini. Improving retrieval feedback with multiple term-ranking function combination. ACM Trans. Info. Systems, 20(3):259 - 290. [6] K. Collins-Thompson, P. Ogilvie, and J. Callan. Initial results with structured queries and language models on half a terabyte of text. In Proc. of 2005 Text REtrieval Conference. NIST Special Publication. [7] R. O. Duda, P. E. Hart, and D. G. Stork. Pattern Classification. Wiley and Sons, 2nd edition, 2001. [8] W. R. Greiff, W. T. Morgan, and J. M. Ponte. The role of variance in term weighting for probabilistic information retrieval. In Proc. of the 11th Intl. Conf. on Info. and Knowledge Mgmt. (CIKM 2002), pages 252-259. [9] T. Kohonen, J. Hynninen, J. Kangas, and J. Laaksonen. SOMPAK: The self-organizing map program package. Technical Report A31, Helsinki University of Technology, 1996. http://www.cis.hut.fi/research/papers/som tr96.ps.Z. [10] V. Lavrenko. A Generative Theory of Relevance. PhD thesis, University of Massachusetts, Amherst, 2004. [11] C.-H. Lee, R. Greiner, and S. Wang. Using query-specific variance estimates to combine Bayesian classifiers. In Proc. of the 23rd Intl. Conf. on Machine Learning (ICML 2006), pages 529-536. [12] D. Metzler and W. B. Croft. Combining the language model and inference network approaches to retrieval. Info. Processing and Mgmt., 40(5):735-750, 2004. [13] T. Minka. Estimating a Dirichlet distribution. Technical report, 2000. http://research.microsoft.com/ minka/papers/dirichlet. [14] J. Ponte. Advances in Information Retrieval, chapter Language models for relevance feedback, pages 73-96. 2000. W.B. Croft, ed. [15] J. M. Ponte and W. B. Croft. A language modeling approach to information retrieval. In Proc. of the 1998 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 275-281. [16] J. Rocchio. The SMART Retrieval System, chapter Relevance Feedback in Information Retrieval, pages 313-323. Prentice-Hall, 1971. G. Salton, ed. [17] T. Sakai, T. Manabe, and M. Koyama. Flexible pseudo-relevance feedback via selective sampling. ACM Transactions on Asian Language Information Processing (TALIP), 4(2):111-135, 2005. [18] T. Tao and C. Zhai. Regularized estimation of mixture models for robust pseudo-relevance feedback. In Proc. of the 2006 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 162-169. [19] J. Xu and W. B. Croft. Improving the effectiveness of information retrieval with local context analysis. ACM Trans. Inf. Syst., 18(1):79-112, 2000. [20] E. YomTov, S. Fine, D. Carmel, and A. Darlow. Learning to estimate query difficulty. In Proc. of the 2005 ACM SIGIR Conf. on Research and Development in Information Retrieval, pages 512-519. [21] Y. Zhou and W. B. Croft. Ranking robustness: a novel framework to predict query performance. In Proc. of the 15th ACM Intl. Conf. on Information and Knowledge Mgmt. (CIKM 2006), pages 567-574.",
    "original_translation": "Estimación y Uso de la Incertidumbre en la Retroalimentación de Pseudo-Relevancia Kevyn Collins-Thompson y Jamie Callan Instituto de Tecnologías del Lenguaje Escuela de Ciencias de la Computación Universidad Carnegie Mellon Pittsburgh, PA 15213-8213 EE. UU. {kct | callan}@cs.cmu.edu RESUMEN Los métodos existentes de retroalimentación de pseudo-relevancia suelen realizar un promedio de los documentos mejor clasificados, pero ignoran una dimensión estadística importante: el riesgo o la varianza asociada tanto a los modelos de documentos individuales como a su combinación. Tratando el método de retroalimentación de línea base como una caja negra, y el modelo de retroalimentación de salida como una variable aleatoria, estimamos una distribución posterior para el modelo de retroalimentación mediante el remuestreo de los documentos mejor recuperados de una consulta dada, utilizando la media o la moda posterior como el modelo de retroalimentación mejorado. Luego realizamos la combinación de modelos sobre varios modelos mejorados, cada uno basado en una consulta ligeramente modificada muestreada de la consulta original. Observamos que el remuestreo de documentos ayuda a aumentar la precisión del modelo de retroalimentación individual al eliminar términos de ruido, mientras que el muestreo de la consulta mejora la robustez (rendimiento en el peor de los casos) al enfatizar términos relacionados con múltiples aspectos de la consulta. El resultado es un algoritmo de meta-retroalimentación que es tanto más robusto como más preciso que el método de referencia original. Categorías y Descriptores de Asignaturas: H.3.3 [Recuperación de Información]: Modelos de Recuperación Términos Generales: Algoritmos, Experimentación 1. INTRODUCCIÓN La incertidumbre es una característica inherente de la recuperación de información. No solo no sabemos las consultas que se presentarán a nuestro algoritmo de recuperación con anticipación, sino que la necesidad de información de los usuarios puede ser vaga o estar incompletamente especificada por estas consultas. Incluso si la consulta estuviera perfectamente especificada, el lenguaje en los documentos de la colección es inherentemente complejo y ambiguo, y hacer coincidir dicho lenguaje de manera efectiva es un problema formidable por sí mismo. Con esto en mente, deseamos tratar muchas cantidades importantes calculadas por el sistema de recuperación, ya sea un puntaje de relevancia para un documento, o un peso para un término de expansión de consulta, como variables aleatorias cuyo valor real es incierto pero donde la incertidumbre sobre el valor real puede ser cuantificada reemplazando el valor fijo con una distribución de probabilidad sobre posibles valores. De esta manera, los algoritmos de recuperación pueden intentar cuantificar el riesgo o la incertidumbre asociados con sus clasificaciones de salida, o mejorar la estabilidad o precisión de sus cálculos internos. Los algoritmos actuales para la retroalimentación de pseudo relevancia (PRF) tienden a seguir el mismo método básico ya sea que utilicemos algoritmos basados en espacio vectorial como la fórmula de Rocchio [16], o enfoques más recientes de modelado de lenguaje como Modelos de Relevancia [10]. Primero, se obtiene un conjunto de documentos recuperados en la parte superior a partir de una consulta inicial y se asume que aproxima un conjunto de documentos relevantes. A continuación, se calcula un vector de modelo de retroalimentación único según algún tipo de promedio, centroide o expectativa sobre el conjunto de modelos de documentos posiblemente relevantes. Por ejemplo, los vectores de documentos pueden combinarse con igual ponderación, como en Rocchio, o por probabilidad de consulta, como se puede hacer utilizando el Modelo de Relevancia. El uso de una expectativa es razonable por razones prácticas y teóricas, pero por sí sola ignora información potencialmente valiosa sobre el riesgo del modelo de retroalimentación. Nuestra hipótesis principal en este artículo es que estimar la incertidumbre en la retroalimentación es útil y conduce a modelos de retroalimentación individual más efectivos y a modelos combinados más robustos. Por lo tanto, proponemos un método para estimar la incertidumbre asociada con un modelo de retroalimentación individual en términos de una distribución posterior sobre modelos de lenguaje. Para hacer esto, variamos sistemáticamente las entradas al método de retroalimentación base y ajustamos una distribución de Dirichlet a la salida. Utilizamos la media posterior o el modo como la estimación mejorada del modelo de retroalimentación. Este proceso se muestra en la Figura 1. Como mostraremos más adelante, la media y la moda pueden variar significativamente del modelo de retroalimentación única propuesto por el método de referencia. También realizamos la combinación de modelos utilizando varios modelos de lenguaje de retroalimentación mejorados obtenidos a partir de un pequeño número de nuevas consultas muestreadas de la consulta original. El peso de un modelo combina dos factores complementarios: la probabilidad del modelo de generar la consulta y la varianza del modelo, siendo los modelos con alta varianza los que obtienen un peso menor. Por ejemplo, un vector de parámetros esperado condicionado a la observación de la consulta se forma a partir de los documentos recuperados en la parte superior, que se tratan como cadenas de entrenamiento (ver [10], p. 62). Figura 1: Estimación de la incertidumbre del modelo de retroalimentación para una sola consulta. 2. En las Secciones 2.1-2.5 describimos un método general para estimar una distribución de probabilidad sobre el conjunto de posibles modelos de lenguaje basado en muestreo. En las Secciones 2.6 y 2.7 resumimos cómo se utilizan diferentes muestras de consultas para generar múltiples modelos de retroalimentación, que luego se combinan. 2.1 Modelado de la Incertidumbre de la Retroalimentación Dado una consulta Q y una colección C, asumimos un sistema de recuperación probabilístico que asigna un puntaje de documento de valor real f(D, Q) a cada documento D en C, de manera que el puntaje sea proporcional a la probabilidad estimada de relevancia. No hacemos ninguna otra suposición sobre f(D, Q). La naturaleza de f(D, Q) puede ser compleja: por ejemplo, si el sistema de recuperación admite lenguajes de consulta estructurados [12], entonces f(D, Q) puede representar la salida de una red de inferencia arbitrariamente compleja definida por los operadores de consulta estructurados. En teoría, la función de puntuación puede variar de una consulta a otra, aunque en este estudio por simplicidad mantenemos la misma función de puntuación para todas las consultas. Nuestro método de consulta específico se encuentra en la Sección 3. Tratamos el algoritmo de retroalimentación como una caja negra y asumimos que los insumos del algoritmo de retroalimentación son la consulta original y los documentos recuperados principales correspondientes, a los cuales se les asigna una puntuación a cada documento. Suponemos que la salida del algoritmo de retroalimentación es un vector de pesos de términos que se utilizarán para agregar o reponderar los términos en la representación de la consulta original, con el vector normalizado para formar una distribución de probabilidad. Consideramos las entradas a la caja negra de retroalimentación como variables aleatorias, y analizamos el modelo de retroalimentación como una variable aleatoria que cambia en respuesta a cambios en las entradas. Al igual que la función de puntuación del documento f(D, Q), el algoritmo de retroalimentación puede implementar una fórmula de puntuación compleja y no lineal, por lo que, a medida que sus entradas varían, los modelos de retroalimentación resultantes pueden tener una distribución compleja en el espacio de modelos de retroalimentación (el espacio muestral). Debido a esta complejidad potencial, no intentamos derivar una distribución posterior de forma cerrada, sino que en su lugar utilizamos simulación. Llamamos a esta distribución sobre posibles modelos de retroalimentación la distribución de modelos de retroalimentación. Nuestro objetivo en esta sección es estimar una aproximación útil a la distribución del modelo de retroalimentación. Para un marco específico de experimentos, utilizamos el enfoque de modelado de lenguaje (LM) para la recuperación de información [15]. El puntaje de un documento D con respecto a una consulta Q y una colección C se da por p(Q|D) con respecto a los modelos de lenguaje ˆθQ y ˆθD estimados para la consulta y el documento respectivamente. Denotamos el conjunto de los k documentos mejor recuperados de la colección C en respuesta a Q como DQ(k, C). Para simplificar, asumimos que las consultas y documentos son generados por distribuciones multinomiales cuyos parámetros están representados por modelos de lenguaje unigram. Para incorporar retroalimentación en el enfoque de LM, asumimos un esquema basado en modelos en el que nuestro objetivo es tomar la consulta y los documentos clasificados resultantes DQ(k, C) como entrada, y producir un modelo de lenguaje expandido ˆθE, que luego se interpola con el modelo de consulta original ˆθQ: ˆθNew = (1 − α) · ˆθQ + α · ˆθE (1). Esto incluye la posibilidad de α = 1 donde el modelo de consulta original es completamente reemplazado por el modelo de retroalimentación. Nuestro espacio muestral es el conjunto de todos los posibles modelos de lenguaje LF que pueden ser generados como modelos de retroalimentación. Nuestro enfoque es tomar muestras de este espacio y luego ajustar una distribución a las muestras utilizando máxima verosimilitud. Para simplificar, comenzamos asumiendo que la distribución de retroalimentación latente tiene la forma de una distribución de Dirichlet. Aunque la distribución de Dirichlet es unimodal y en general bastante limitada en su expresividad en el espacio muestral, es una coincidencia natural para el modelo de lenguaje multinomial, puede ser estimada rápidamente y capturar las características más relevantes de los modelos de retroalimentación confiada e incierta, como la dispersión general de la distribución. 2.2 Re-muestreo de modelos de documentos. Nos gustaría una aproximación a la distribución posterior del modelo de retroalimentación LF. Para lograr esto, aplicamos una técnica de simulación ampliamente utilizada llamada muestreo bootstrap ([7], p. 474) en los parámetros de entrada, es decir, el conjunto de documentos recuperados en la parte superior. El muestreo de arranque nos permite simular el efecto aproximado de perturbar los parámetros dentro del algoritmo de retroalimentación de la caja negra al perturbar los insumos de ese algoritmo de manera sistemática, sin hacer suposiciones sobre la naturaleza del algoritmo de retroalimentación. Específicamente, muestreamos k documentos con reemplazo de DQ(k, C), y calculamos un modelo de lenguaje de expansión θb utilizando el método de retroalimentación de caja negra. Repetimos este proceso B veces para obtener un conjunto de B modelos de lenguaje de retroalimentación, a los cuales luego ajustamos una distribución de Dirichlet. Normalmente, B se encuentra en el rango de 20 a 50 muestras, con un rendimiento relativamente estable en este rango. Ten en cuenta que en lugar de tratar cada documento superior como igualmente probable, muestreamos de acuerdo con las probabilidades estimadas de relevancia de cada documento en DQ(k, C). Por lo tanto, es más probable que un documento sea elegido cuanto más alto esté en la clasificación. 2.3 Justificación de un enfoque de muestreo La justificación de nuestro enfoque de muestreo tiene dos partes. Primero, queremos mejorar la calidad de los modelos de retroalimentación individual al suavizar la variación cuando el modelo de retroalimentación base es inestable. En este sentido, nuestro enfoque se asemeja al bagging [4], un enfoque de conjunto que genera múltiples versiones de un predictor haciendo copias de arranque del conjunto de entrenamiento, y luego promedia los predictores (numéricos). En nuestra aplicación, los documentos recuperados en la parte superior pueden ser vistos como una especie de conjunto de entrenamiento ruidoso para la relevancia. Segundo, el muestreo es una forma efectiva de estimar propiedades básicas de la distribución posterior de retroalimentación, las cuales pueden ser utilizadas para mejorar la combinación de modelos. Por ejemplo, un modelo puede ser ponderado por su confianza de predicción, estimada como una función de la variabilidad del posterior alrededor del modelo. foo2-401.map-Dim:5434,Size:12*12units, vecindario gaussiano (a) Tema 401 Minorías extranjeras, Alemania foo2-402.map-Dim:5698,Size:12*12units, vecindario gaussiano (b) Tema 402 Genética del comportamiento foo2-459.map-Dim:8969,Size:12*12units, vecindario gaussiano (c) Tema 459 ¿Cuándo puede un prestamista ejecutar una hipoteca sobre una propiedad? Figura 2: Visualización de la varianza del modelo de lenguaje de expansión utilizando mapas autoorganizados, mostrando la distribución de modelos de lenguaje que resulta de remuestrear las entradas al método de expansión de referencia. El modelo de lenguaje que habría sido elegido por la expansión de la línea base se encuentra en el centro de cada mapa. La función de similitud es la divergencia JensenShannon. 2.4 Visualización de distribuciones de retroalimentación Antes de describir cómo ajustamos y utilizamos la distribución de Dirichlet sobre modelos de retroalimentación, es instructivo ver algunos ejemplos de distribuciones de modelos de retroalimentación reales que resultan de muestrear por bootstrap los documentos más recuperados de diferentes temas de TREC. Cada punto en nuestro espacio muestral es un modelo de lenguaje, que típicamente tiene varias miles de dimensiones. Para ayudar a analizar el comportamiento de nuestro método, utilizamos un Mapa Auto-organizado (a través del paquete SOM-PAK [9]), para aplanar y visualizar la función de densidad de alta dimensión. Los mapas de densidad para tres temas de TREC se muestran en la Figura 2 arriba. Las áreas oscuras representan regiones de alta similitud entre los modelos de lenguaje. Las áreas claras representan regiones de baja similitud, los valles entre los grupos. Cada diagrama está centrado en el modelo de lenguaje que habría sido elegido por la expansión de referencia. Un solo pico (modo) es evidente en algunos ejemplos, pero en otros aparece una estructura más compleja. Además, aunque la distribución suele estar cerca del modelo de retroalimentación de referencia, para algunos temas están a una distancia significativa (según la divergencia de JensenShannon), como se muestra en la Subfigura 2c. En tales casos, la moda o la media de la distribución de retroalimentación a menudo funcionan significativamente mejor que el valor base (y en una proporción menor de casos, significativamente peor). 2.5 Ajuste de una distribución de retroalimentación posterior Después de obtener muestras del modelo de retroalimentación mediante el remuestreo de las entradas del modelo de retroalimentación, estimamos la distribución de retroalimentación. Suponemos que los modelos de retroalimentación multinomial {ˆθ1, . . . , ˆθB} fueron generados por una distribución de Dirichlet latente con parámetros {α1, . . . , αN}. Para estimar los {α1, . . . , αN }, ajustamos los parámetros de Dirichlet a las muestras del modelo de lenguaje B según la máxima verosimilitud utilizando un procedimiento de Newton generalizado, cuyos detalles se encuentran en Minka [13]. Suponemos una distribución de Dirichlet simple sobre los {α1, . . . , αN}, estableciendo cada uno como αi = μ · p(wi | C), donde μ es un parámetro y p(· | C) es el modelo de lenguaje de colección estimado a partir de un conjunto de documentos de la colección C. El ajuste de parámetros converge muy rápidamente, generalmente en solo 2 o 3 iteraciones. Dado que nuestros puntos son modelos de lenguaje en el simplex multinomial, extendimos SOM-PAK para admitir la divergencia de Jensen-Shannon, una medida de similitud ampliamente utilizada entre distribuciones de probabilidad. 3 iteraciones son suficientes, por lo que es práctico aplicarlo en tiempo de consulta cuando la sobrecarga computacional debe ser pequeña. En la práctica, podemos restringir el cálculo al vocabulario de los documentos recuperados en la parte superior, en lugar de toda la colección. Ten en cuenta que para este paso estamos reutilizando los documentos recuperados existentes y no realizando consultas adicionales. Dado los parámetros de una distribución de Dirichlet N-dimensional Dir(α), los vectores de media μ y moda x son fáciles de calcular y se dan respectivamente por μi = αiP αi (2) y xi = αi−1P αi−N . (3) Luego podemos elegir el modelo de lenguaje en la media o en la moda del posterior como el modelo de retroalimentación mejorado final. (Encontramos que la moda proporciona un rendimiento ligeramente mejor). Para la recuperación de información, es probable que el número de muestras disponibles sea bastante pequeño por razones de rendimiento, generalmente menos de diez. Además, si bien el muestreo aleatorio es útil en ciertos casos, es perfectamente aceptable permitir distribuciones de muestreo determinísticas, pero estas deben diseñarse cuidadosamente para aproximar una varianza de salida precisa. Dejamos esto para estudios futuros. 2.6 Variantes de consulta Utilizamos los siguientes métodos para generar variantes de la consulta original. Cada variante corresponde a una suposición diferente sobre qué aspectos de la consulta original pueden ser importantes. Esta es una forma de muestreo determinístico. Seleccionamos tres métodos simples que cubren suposiciones complementarias sobre la consulta. Uso sin expansión. La suposición es que los términos dados son una descripción completa de la necesidad de información. Deja uno fuera. Se deja fuera un término del query original. La suposición es que uno de los términos de la consulta es un término de ruido. Se elige un término único de la consulta original. Esto asume que solo un aspecto de la consulta, es decir, el representado por el término, es el más importante. Después de generar una variante de la consulta original, la combinamos con la consulta original utilizando un peso αSUB para no alejarnos demasiado. En este estudio, establecimos αSUB = 0.5. Por ejemplo, utilizando el lenguaje de consulta Indri [12], una variante de dejar uno fuera de la consulta inicial que omite el término \"ireland\" para el tema TREC 404 es: #weight(0.5 #combine(ireland peace talks) 0.5 #combine(peace talks)) 2.7 Combinando modelos de retroalimentación mejorados de múltiples variantes de consulta. Cuando se utilizan múltiples variantes de consulta, los modelos de retroalimentación mejorados resultantes se combinan utilizando la combinación de modelos bayesianos. Para hacer esto, tratamos cada palabra como un elemento a clasificar como perteneciente a una clase relevante o no relevante, y derivamos una probabilidad de clase para cada palabra combinando las puntuaciones de cada variante de consulta. Cada puntuación es dada por la probabilidad de ese término en la distribución de Dirichlet. Las puntuaciones de los términos se ponderan por el inverso de la varianza del término en los modelos de retroalimentación mejorada de la distribución de Dirichlet. La probabilidad previa de pertenencia de una palabra a la clase relevante se da por la probabilidad de la consulta original en todo el modelo de expansión mejorado. 3. EVALUACIÓN En esta sección presentamos resultados que confirman la utilidad de estimar una distribución del modelo de retroalimentación a partir del remuestreo ponderado de documentos mejor clasificados, y de combinar los modelos de retroalimentación obtenidos a partir de diferentes pequeños cambios en la consulta original. 3.1 Método general Evaluamos el rendimiento en un total de 350 consultas derivadas de cuatro conjuntos de temas de TREC: 51-200 (TREC-1&2), 351-400 (TREC-7), 401-450 (TREC-8) y 451-550 (wt10g, TREC-9&10). Elegimos estos por su contenido variado y propiedades del documento. Por ejemplo, los documentos wt10g son páginas web con una amplia variedad de temas y estilos, mientras que los documentos TREC-1&2 son artículos de noticias más homogéneos. La indexación y recuperación se realizó utilizando el sistema Indri en el kit de herramientas Lemur [12] [1]. Nuestras consultas se derivaron de las palabras en el campo del título de los temas de TREC. Las frases no fueron utilizadas. Para generar las consultas base que se pasaron a Indri, envolvimos los términos de la consulta con el operador #combine de Indri. Por ejemplo, la consulta inicial para el tema 404 es: #combine(ireland peace talks) Realizamos el stemming de Krovetz para todos los experimentos. Debido a que encontramos que el método de expansión de la base (Indri) funcionó mejor utilizando una lista de palabras vacías con el modelo de retroalimentación, todos los experimentos utilizaron una lista de 419 palabras comunes en inglés. Sin embargo, un efecto secundario interesante de nuestro enfoque de remuestreo es que tiende a eliminar muchas palabras vacías del modelo de retroalimentación, lo que hace que una lista de paradas sea menos crítica. Esto se discute más a fondo en la Sección 3.6. Método de retroalimentación de línea base 3.2 Para nuestro método de expansión de línea base, utilizamos un algoritmo incluido en Indri 1.0 como el método de expansión predeterminado. Este método primero selecciona términos utilizando un cálculo de logaritmo de probabilidades descrito por Ponte [14], pero asigna pesos finales a los términos utilizando el modelo de relevancia de Lavrenko [10]. Elegimos el método Indri porque proporciona una línea base consistentemente sólida, se basa en un enfoque de modelado del lenguaje y es fácil de experimentar con él. En una evaluación de TREC utilizando el corpus GOV2, el método fue uno de los mejores resultados, logrando un aumento del 19.8% en el MAP en comparación con el uso de consultas no expandidas. En este estudio, se logra una ganancia promedio en MAP del 17.25% en las cuatro colecciones. El método de expansión de Indri primero calcula una razón de logaritmos de probabilidades o(v) para cada término de expansión potencial v dado por o(v) = X D log p(v|D) p(v|C) (4) sobre todos los documentos D que contienen v, en la colección C. Luego, los candidatos a términos de expansión se ordenan por o(v) descendente, y se eligen los primeros m. Finalmente, los pesos de término r(v) utilizados en la consulta ampliada se calculan en base al modelo de relevancia r(v) = X D p(q|D)p(v|D) p(v) p(D) (5). La cantidad p(q|D) es la puntuación de probabilidad asignada al documento en la recuperación inicial. Utilizamos suavizado de Dirichlet de p(v|D) con μ = 1000. Este modelo de relevancia se combina luego con la consulta original utilizando interpolación lineal, ponderada por un parámetro α. Por defecto, utilizamos los 50 documentos principales para la retroalimentación y los 20 términos de expansión, con el parámetro de interpolación de retroalimentación α = 0.5 a menos que se indique lo contrario. Por ejemplo, la consulta expandida base para el tema 404 es: #weight(0.5 #combine(ireland peace talks) 0.5 #weight(0.10 ireland 0.08 peace 0.08 northern ...) 3.3 Rendimiento de la expansión. Medimos la efectividad de nuestros algoritmos de retroalimentación mediante dos criterios principales: precisión y robustez. La robustez, y el equilibrio entre precisión y robustez, se analiza en la Sección 3.4. En esta sección, examinamos la precisión promedio y la precisión en los 10 documentos principales (P10). También incluimos la recuperación de 1,000 documentos. Para cada consulta, obtuvimos un conjunto de B modelos de retroalimentación utilizando la línea base de Indri. Cada modelo de retroalimentación se obtuvo a partir de una muestra aleatoria de los k documentos principales tomados con reemplazo. Para estos experimentos, B = 30 y k = 50. Cada modelo de retroalimentación contenía 20 términos. En el lado de la consulta, utilizamos muestreo de dejar uno fuera (LOO) para crear las variantes de consulta. El muestreo de consulta de un solo término tuvo un rendimiento consistentemente peor en todas las colecciones, por lo que nuestros resultados se centran en el muestreo de LOO aquí. Utilizamos los métodos descritos en la Sección 2 para estimar un modelo de retroalimentación mejorado a partir de la distribución posterior de Dirichlet para cada variante de consulta, y para combinar los modelos de retroalimentación de todas las variantes de consulta. Llamamos a nuestro método expansión de remuestreo y lo denotamos como RS-FB aquí. Denominamos al método de retroalimentación de línea base Indri como Base-FB. Los resultados de aplicar tanto el método de expansión de línea base (Base-FB) como la expansión de remuestreo (RS-FB) se muestran en la Tabla 1. Observamos varias tendencias en esta tabla. Primero, la precisión promedio de RS-FB fue comparable a Base-FB, logrando una ganancia promedio del 17.6% en comparación con no utilizar expansión en las cuatro colecciones. La ganancia de expansión de la línea base de Indri fue del 17.25%. Además, el método RS-FB logró mejoras consistentes en P10 sobre Base-FB para cada conjunto de temas, con una mejora promedio del 6.89% sobre Base-FB para los 350 temas. La ganancia P10 más baja sobre Base-FB fue del +3.82% para TREC-7 y la más alta fue del +11.95% para wt10g. Finalmente, tanto Base-FB como RS-FB también mejoraron consistentemente la recuperación en comparación con no utilizar expansión, siendo Base-FB el que logró una mejor recuperación que RS-FB para todos los conjuntos de temas. 3.4 Robustez de recuperación. Utilizamos el término robustez para referirnos al rendimiento de precisión promedio en el peor de los casos de un algoritmo de retroalimentación. Idealmente, un método de retroalimentación sólido nunca debería funcionar peor que usar la consulta original, mientras que a menudo funciona mejor utilizando la expansión. Para evaluar la robustez en este estudio, utilizamos una medida muy simple llamada índice de robustez (RI). Para un conjunto de consultas Q, la medida de RI se define como: RI(Q) = n+ − n− |Q| (6) donde n+ es el número de consultas ayudadas por el método de retroalimentación y n− es el número de consultas perjudicadas. Aquí, por \"ayudar\" nos referimos a obtener una precisión promedio más alta como resultado de la retroalimentación. El valor de RI varía desde un mínimo de 3. A veces también se le llama índice de confiabilidad de mejora y fue utilizado en Sakai et al. [17]. Colección NoExp Base-FB RS-FB TREC 1&2 AvgP 0.1818 0.2419 (+33.04%) 0.2406 (+32.24%) P10 0.4443 0.4913 (+10.57%) 0.5363 (+17.83%) Recall 15084/37393 19172/37393 15396/37393 TREC 7 AvgP 0.1890 0.2175 (+15.07%) 0.2169 (+14.75%) P10 0.4200 0.4320 (+2.85%) 0.4480 (+6.67%) Recall 2179/4674 2608/4674 2487/4674 TREC 8 AvgP 0.2031 0.2361 (+16.25%) 0.2268 (+11.70%) P10 0.3960 0.4160 (+5.05%) 0.4340 (+9.59%) Recall 2144/4728 2642/4728 2485/4728 wt10g AvgP 0.1741 0.1829 (+5.06%) 0.1946 (+11.78%) P10 0.2760 0.2630 (-4.71%) 0.2960 (+7.24%) Recall 3361/5980 3725/5980 3664/5980 Tabla 1: Comparación de la retroalimentación base (Base-FB) y la retroalimentación utilizando re-muestreo (RS-FB). La mejora mostrada para BaseFB y RS-FB es relativa al uso de ninguna expansión. (a) TREC 1&2 (curva superior); TREC 8 (curva inferior) (b) TREC 7 (curva superior); wt10g (curva inferior) Figura 3: La compensación entre robustez y precisión promedio para diferentes corpora. El eje x muestra el cambio en la MAP al utilizar la expansión de la línea base con α = 0.5. El eje y muestra el Índice de Robustez (RI). Cada curva a través de los puntos no circulados muestra el compromiso entre RI/MAP utilizando la estrategia simple de pequeño α (ver texto) a medida que α disminuye de 0.5 a cero en la dirección de la flecha. Los puntos circulados representan los compromisos obtenidos al volver a muestrear la retroalimentación para α = 0.5. Colección N Base-FB RS-FB n− RI n− RI TREC 1&2 103 26 +0.495 15 +0.709 TREC 7 46 14 +0.391 10 +0.565 TREC 8 44 12 +0.455 12 +0.455 wt10g 91 48 -0.055 39 +0.143 Combinado 284 100 +0.296 76 +0.465 Tabla 2: Comparación del índice de robustez (RI) para retroalimentación de línea base (Base-FB) vs. retroalimentación de remuestreo (RS-FB). También se muestran el número real de consultas perjudicadas por la retroalimentación (n−) para cada método y colección. Las consultas para las cuales la precisión promedio inicial era insignificante (≤ 0.01) fueron ignoradas, dando como resultado el recuento de consultas restantes en la columna N, de -1.0, cuando todas las consultas son perjudicadas por el método de retroalimentación, a +1.0 cuando todas las consultas son ayudadas. La medida de RI no tiene en cuenta la magnitud o distribución de la cantidad de cambio en el conjunto Q. Sin embargo, es fácil de entender como una indicación general de robustez. Una forma obvia de mejorar el rendimiento en el peor de los casos de la retroalimentación es simplemente utilizar un parámetro de interpolación α fijo más pequeño, como α = 0.3, otorgando menos peso al modelo de retroalimentación (posiblemente arriesgado) y más al cuestionario original. Llamamos a esto la estrategia de pequeña α. Dado que también estamos reduciendo las ganancias potenciales cuando el modelo de retroalimentación es correcto, sin embargo, esperaríamos algún tipo de compensación entre la precisión promedio y la robustez. Por lo tanto, comparamos el equilibrio entre precisión y robustez entre nuestro algoritmo de retroalimentación de remuestreo y el método simple de pequeño α. Los resultados se resumen en la Figura 3. En la figura, la curva para cada conjunto de temas interpola entre los puntos de compensación, comenzando en x=0, donde α = 0.5, y continuando en la dirección de la flecha a medida que α disminuye y la consulta original recibe cada vez más peso. Como era de esperar, la robustez aumenta continuamente a medida que avanzamos a lo largo de la curva, pero la precisión media promedio generalmente disminuye a medida que se eliminan las ganancias del feedback. Para la comparación, se muestra el rendimiento del re-muestreo con retroalimentación en α = 0.5 para cada colección como el punto marcado con un círculo. Más alto y a la derecha es mejor. Esta figura muestra que el re-muestreo de retroalimentación ofrece un compromiso algo mejor que el enfoque de pequeño α para 3 de las 4 colecciones. Figura 4: Histograma que muestra la mayor robustez del re-muestreo de retroalimentación (RS-FB) sobre la retroalimentación base (Base-FB) para todos los conjuntos de datos combinados. Las consultas se agrupan por el % de cambio en AP en comparación con la consulta no expandida. Colección DS + QV DS + No QV TREC 1&2 AvgP 0.2406 0.2547 (+5.86%) P10 0.5263 0.5362 (+1.88%) RI 0.7087 0.6515 (-0.0572) TREC 7 AvgP 0.2169 0.2200 (+1.43%) P10 0.4480 0.4300 (-4.02%) RI 0.5652 0.2609 (-0.3043) TREC 8 AvgP 0.2268 0.2257 (-0.49%) P10 0.4340 0.4200 (-3.23%) RI 0.4545 0.4091 (-0.0454) wt10g AvgP 0.1946 0.1865 (-4.16%) P10 0.2960 0.2680 (-9.46%) RI 0.1429 0.0220 (-0.1209) Tabla 3: Comparación de la retroalimentación de remuestreo utilizando muestreo de documentos (DS) con (QV) y sin (No QV) combinar modelos de retroalimentación de múltiples variantes de consulta. La Tabla 2 muestra los puntajes del Índice de Robustez para Base-FB y RS-FB. El método de retroalimentación RS-FB obtuvo una mayor robustez que Base-FB en tres de los cuatro conjuntos de temas, con solo un rendimiento ligeramente peor en TREC-8. Una vista más detallada que muestra la distribución de los cambios relativos en AP se presenta en el histograma de la Figura 4. En comparación con Base-FB, el método RS-FB logra una reducción notable en el número de consultas significativamente afectadas por la expansión (es decir, donde AP se ve afectado en un 25% o más), al tiempo que conserva ganancias positivas en AP. 3.5 Efecto de los métodos de muestreo de consultas y documentos Dada la mayor robustez de nuestros algoritmos observada en la Sección 3.4, una pregunta importante es qué componente de nuestro sistema es responsable. ¿Es el uso de la re-muestreo de documentos, el uso de múltiples variantes de consulta, o algún otro factor? Los resultados en la Tabla 3 sugieren que la combinación de modelos basada en variantes de consulta puede ser en gran medida responsable de la mayor robustez. Cuando se desactivan las variantes de consulta y se utiliza la consulta original por sí sola con muestreo de documentos, hay poco cambio neto en la precisión promedio, una pequeña disminución en P10 para 3 de los 4 conjuntos de temas, pero una caída significativa en la robustez para todos los conjuntos de temas. En dos casos, la medida de RI disminuye en más del 50%. También examinamos el efecto del método de muestreo de documentos en la efectividad de la recuperación, utilizando dos estrategias diferentes. La estrategia de ponderación uniforme ignoró las puntuaciones de relevancia de la recuperación inicial y otorgó a cada documento en los primeros k la misma probabilidad de selección. Por el contrario, la estrategia de ponderación de puntajes de relevancia seleccionó documentos con una probabilidad proporcional a sus puntajes de relevancia. De esta manera, los documentos que tenían una clasificación más alta tenían más probabilidades de ser seleccionados. Los resultados se muestran en la Tabla 4. La estrategia de ponderación de la puntuación de relevancia tiene un mejor rendimiento en general, con puntajes de RI y P10 significativamente más altos en 3 de los 4 conjuntos de temas. La diferencia en la precisión promedio entre los métodos, sin embargo, es menos marcada. Esto sugiere que el peso uniforme actúa para aumentar la varianza en los resultados de recuperación: cuando la precisión promedio inicial es alta, hay muchos documentos relevantes en los primeros k y el muestreo uniforme puede proporcionar un modelo de relevancia más representativo que centrarse en los elementos altamente clasificados. Por otro lado, cuando la precisión inicial es baja, hay pocos documentos relevantes en las posiciones inferiores y el muestreo uniforme mezcla más de los documentos no relevantes. Por razones de espacio, solo resumimos nuestros hallazgos sobre el tamaño de la muestra aquí. El número de muestras tiene cierto efecto en la precisión cuando es menor a 10, pero el rendimiento se estabiliza alrededor de 15 a 20 muestras. Utilizamos 30 muestras para nuestros experimentos. Mucho más allá de este nivel, los beneficios adicionales de más muestras disminuyen a medida que la distribución inicial de puntajes se ajusta más estrechamente y aumenta el tiempo de procesamiento. 3.6 El efecto del remuestreo en la calidad del término de expansión Idealmente, un modelo de recuperación no debería requerir una lista de palabras vacías al estimar un modelo de relevancia: un modelo estadístico robusto debería reducir automáticamente el peso de las palabras vacías dependiendo del contexto. Las palabras vacías pueden perjudicar la retroalimentación si se seleccionan como términos de retroalimentación, ya que suelen ser malos discriminadores y desperdician espacios de términos valiosos. En la práctica, sin embargo, debido a que la mayoría de los métodos de selección de términos se asemejan a un tipo de ponderación tf · idf, a veces se pueden seleccionar términos con un idf bajo pero un tf muy alto como candidatos de términos de expansión. Esto sucede, por ejemplo, incluso con el enfoque del Modelo de Relevancia que forma parte de nuestra retroalimentación base. Para garantizar una línea base lo más sólida posible, utilizamos una lista de paradas para todos los experimentos reportados aquí. Si desactivamos la lista de palabras vacías, sin embargo, obtenemos resultados como los mostrados en la Tabla 5 donde cuatro de los diez términos de retroalimentación básica principales para el tema 60 de TREC (dijo, pero, su, no) son palabras vacías utilizando el método BaseFB. (Se seleccionaron los 100 términos de expansión principales para generar este ejemplo). El método Indris intenta abordar el problema de las palabras vacías aplicando un paso inicial basado en Ponte [14] para seleccionar términos menos comunes que tienen altas probabilidades logarítmicas de estar en los documentos mejor clasificados en comparación con toda la colección. Sin embargo, esto no resuelve completamente el problema de las palabras vacías, especialmente a medida que aumenta el número de términos de retroalimentación. Sin embargo, el uso de retroalimentación de remuestreo parece mitigar la ponderación de la colección QV + QV uniforme + ponderación de puntaje de relevancia TREC 1&2 AvgP 0.2545 0.2406 (-5.46%) P10 0.5369 0.5263 (-1.97%) RI 0.6212 0.7087 (+14.09%) TREC 7 AvgP 0.2174 0.2169 (-0.23%) P10 0.4320 0.4480 (+3.70%) RI 0.4783 0.5652 (+18.17%) TREC 8 AvgP 0.2267 0.2268 (+0.04%) P10 0.4120 0.4340 (+5.34%) RI 0.4545 0.4545 (+0.00%) wt10g AvgP 0.1808 0.1946 (+7.63%) P10 0.2680 0.2960 (+10.45%) RI 0.0220 0.1099 (+399.5%) Tabla 4: Comparación de muestreo de documentos uniforme y ponderado por relevancia. El cambio porcentual en comparación con el muestreo uniforme se muestra entre paréntesis. QV indica que se utilizaron variantes de consulta en ambas ejecuciones. FB de línea base p(wi|R) FB de remuestreo p(wi|R) dijo 0.055 corte 0.026 corte 0.055 pagar 0.018 pagar 0.034 federal 0.012 pero 0.026 educación 0.011 empleados 0.024 maestros 0.010 sus 0.024 empleados 0.010 no 0.023 caso 0.010 federal 0.021 sus 0.009 trabajadores 0.020 apelaciones 0.008 educación 0.020 sindicato 0.007 Tabla 5: Calidad del término de retroalimentación cuando no se utiliza una lista de paradas. Términos de retroalimentación para el tema 60 de TREC: pago por mérito vs antigüedad. el efecto de las palabras vacías automáticamente. En el ejemplo de la Tabla 5, la retroalimentación de remuestreo deja solo una palabra vacía (su) en las diez primeras posiciones. Observamos un comportamiento similar del término de retroalimentación en muchos otros temas. La razón de este efecto parece ser la interacción de la puntuación de selección de términos con el límite superior de términos m. Si bien la presencia e incluso la proporción de palabras vacías particulares es bastante estable en diferentes muestras de documentos, su posición relativa en la lista de los primeros m elementos no lo es, ya que se examinan conjuntos de documentos con diferentes números de candidatos de términos mejores y de menor frecuencia para cada muestra. Como resultado, si bien algunos stopwords pueden aparecer en cada conjunto de documentos muestreados, tiende a ocurrir que cualquier stopword dado caiga por debajo del umbral para múltiples muestras, lo que lleva a su clasificación como una característica de alta varianza y bajo peso. 4. TRABAJO RELACIONADO Nuestro enfoque está relacionado con trabajos previos de varias áreas de recuperación de información y aprendizaje automático. Nuestro uso de la variación de consultas fue inspirado por el trabajo de YomTov et al. [20], Carpineto et al. [5] y Amati et al. [2], entre otros. Estos estudios utilizan la idea de crear múltiples subconsultas y luego examinar la naturaleza de la superposición en los documentos y/o términos de expansión que resultan de cada subconsulta. La combinación de modelos se realiza utilizando heurísticas. En particular, los estudios de Amati et al. y Carpineto et al. investigaron la combinación de términos de métodos de distribución individuales utilizando una heurística de combinación de reordenamiento de términos. En un conjunto de temas de TREC encontraron una amplia variación promedio en la distancia de rango de los términos de diferentes métodos de expansión. Su método de combinación proporcionó modestas mejoras positivas en la precisión promedio. La idea de examinar la superposición entre listas de términos sugeridos también ha sido utilizada en enfoques tempranos de expansión de consultas. El método de Análisis de Contexto Local (LCA) de Xu y Crofts [19] incluye un factor en la fórmula de ponderación derivada empíricamente que hace que se prefieran los términos de expansión que tienen conexiones con múltiples términos de consulta. En el lado del documento, el trabajo reciente de Zhou & Croft [21] exploró la idea de agregar ruido a los documentos, volver a puntuarlos y utilizar la estabilidad de las clasificaciones resultantes como una estimación de la dificultad de la consulta. Esto está relacionado con nuestro uso de muestreo de documentos para estimar el riesgo del modelo de retroalimentación construido a partir de los diferentes conjuntos de documentos recuperados en la parte superior. Sakai et al. [17] propusieron un enfoque para mejorar la robustez de la retroalimentación de relevancia pseudo utilizando un método que llaman muestreo selectivo. La esencia de su método es que permiten omitir algunos documentos de alta clasificación, basándose en un criterio de agrupación, para seleccionar un conjunto de documentos más variado e innovador más adelante en la clasificación para su uso por un método tradicional de retroalimentación pseudo. Su estudio no encontró mejoras significativas ni en robustez (RI) ni en MAP en sus corpora. Greiff, Morgan y Ponte [8] exploraron el papel de la varianza en la ponderación de términos. En una serie de simulaciones que simplificaron el problema a documentos de 2 características, encontraron que la precisión promedio disminuye a medida que la varianza de la frecuencia de términos - ruido alto - aumenta. La reducción del peso de los términos con alta varianza resultó en una mejora de la precisión promedio. Esto parece estar en concordancia con nuestros propios hallazgos para los modelos de retroalimentación individual. Las estimaciones de la varianza de la producción han sido utilizadas recientemente para mejorar la clasificación de textos. Lee et al. [11] utilizaron estimaciones de varianza específicas de la consulta de las salidas del clasificador para realizar una combinación de modelos mejorada. En lugar de utilizar muestreo, pudieron derivar expresiones en forma cerrada para la varianza del clasificador asumiendo clasificadores base utilizando tipos simples de redes de inferencia. Ando y Zhang propusieron un método que ellos llaman retroalimentación estructural [3] y mostraron cómo aplicarlo a la expansión de consultas para la pista de Genómica de TREC. Utilizaron variaciones de consultas r para obtener R conjuntos diferentes Sr de documentos mejor clasificados que se han intersectado con los documentos mejor clasificados obtenidos de la consulta original qorig. Para cada Si, se calcula el vector centróide normalizado ˆwi de los documentos. El análisis de componentes principales (PCA) se aplica luego a los ˆwi para obtener la matriz Φ de los vectores singulares izquierdos φh que se utilizan para obtener la nueva consulta expandida qexp = qorig + ΦT Φqorig. (7) En el caso de H = 1, tenemos un único vector singular izquierdo φ: qexp = qorig + (φT qorig)φ, de modo que el producto punto φT qorig es un tipo de peso dinámico en la consulta expandida que se basa en la similitud de la consulta original con la consulta expandida. El uso de la varianza como medida de calidad del modelo de retroalimentación ocurre de forma indirecta a través de la aplicación de PCA. Sería interesante estudiar las conexiones entre este enfoque y nuestro propio método de ajuste de modelos. Finalmente, en los enfoques de modelado del lenguaje para retroalimentación, Tao y Zhai [18] describen un método para una retroalimentación más robusta que permite que cada documento tenga un α de retroalimentación diferente. Los pesos de retroalimentación se derivan automáticamente utilizando EM regularizado. La condición de parada de EM implica un equilibrio aproximadamente igual entre el modelo de consulta y expansión. Proponen adaptar el parámetro de parada η basado en una función de alguna medida de calidad de los documentos de retroalimentación. CONCLUSIONES Hemos presentado un nuevo enfoque para la retroalimentación de relevancia pseudo basado en el muestreo de documentos y consultas. El uso del muestreo es un dispositivo muy flexible y poderoso, motivado por nuestro deseo general de ampliar los modelos actuales de recuperación mediante la estimación del riesgo o la varianza asociada con los parámetros o la salida de los procesos de recuperación. Tales estimaciones de varianza, por ejemplo, pueden ser utilizadas de forma natural en un marco bayesiano para una mejor estimación y combinación de modelos. Aplicaciones como la expansión selectiva pueden ser implementadas de manera fundamentada. Si bien nuestro estudio utiliza el enfoque de modelado del lenguaje como marco para experimentos, hacemos pocas suposiciones sobre el funcionamiento real del algoritmo de retroalimentación. Creemos que es probable que cualquier algoritmo de retroalimentación de línea base razonablemente efectivo se beneficiaría de nuestro enfoque. Nuestros resultados en colecciones estándar de TREC muestran que nuestro marco mejora la robustez de un método de retroalimentación de referencia sólido en una variedad de colecciones, sin sacrificar la precisión promedio. También proporciona pequeñas pero consistentes mejoras en la precisión del top 10. En trabajos futuros, visualizamos una investigación sobre cómo variar el conjunto de métodos de muestreo utilizados y el número de muestras controla el equilibrio entre robustez, precisión y eficiencia. Agradecimientos Agradecemos a Paul Bennett por las valiosas discusiones relacionadas con este trabajo, el cual fue apoyado por las becas de la NSF #IIS-0534345 y #CNS-0454018, y la beca del Departamento de Educación de los EE. UU. #R305G03123. Cualquier opinión, hallazgo, conclusión o recomendación expresada en este material son del autor y no necesariamente reflejan las de los patrocinadores. REFERENCIAS [1] El kit de herramientas Lemur para modelado de lenguaje y recuperación. http://www.lemurproject.org. [2] G. Amati, C. Carpineto y G. Romano. Dificultad de la consulta, robustez y aplicación selectiva de la expansión de consultas. En Actas de la 25ª Conferencia Europea sobre Recuperación de Información (ECIR 2004), páginas 127-137. [3] R. K. Ando y T. Zhang. Un método de aprendizaje semisupervisado de alto rendimiento para segmentación de texto. En Actas de la 43ª Reunión Anual de la ACL, páginas 1-9, junio de 2005. [4] L. Breiman. Empaquetando predictores. Aprendizaje automático, 24(2):123-140, 1996. [5] C. Carpineto, G. Romano y V. Giannini. Mejorando la retroalimentación de recuperación con la combinación de múltiples funciones de clasificación de términos. ACM Trans. Información. Sistemas, 20(3):259 - 290. [6] K. Collins-Thompson, P. Ogilvie y J. Callan. Resultados iniciales con consultas estructuradas y modelos de lenguaje en medio terabyte de texto. En Actas de la Conferencia de Recuperación de Información de Texto de 2005. Publicación Especial del NIST. [7] R. O. Duda, P. E. Hart y D. G. Stork. Clasificación de patrones. Wiley and Sons, 2da edición, 2001. [8] W. R. Greiff, W. T. Morgan y J. M. Ponte. El papel de la varianza en la ponderación de términos para la recuperación de información probabilística. En Actas de la 11ª Conferencia Internacional. Conf. sobre Gestión de Información y Conocimiento (CIKM 2002), páginas 252-259. [9] T. Kohonen, J. Hynninen, J. Kangas y J. Laaksonen. SOMPAK: El paquete de programas de mapas autoorganizados. Informe técnico A31, Universidad de Tecnología de Helsinki, 1996. http://www.cis.hut.fi/research/papers/som tr96.ps.Z. [10] V. Lavrenko. Una teoría generativa de relevancia. Tesis doctoral, Universidad de Massachusetts, Amherst, 2004. [11] C.-H. Lee, R. Greiner y S. Wang. Utilizando estimaciones de varianza específicas de la consulta para combinar clasificadores bayesianos. En Actas del 23º Congreso Internacional. Conf. sobre Aprendizaje Automático (ICML 2006), páginas 529-536. [12] D. Metzler y W. B. Croft. Combinando el modelo de lenguaje y los enfoques de red de inferencia para la recuperación. Información. Procesamiento y Gestión, 40(5):735-750, 2004. [13] T. Minka. Estimando una distribución de Dirichlet. Informe técnico, 2000. http://research.microsoft.com/minka/papers/dirichlet. [14] J. Ponte. Avances en la Recuperación de Información, capítulo Modelos de lenguaje para retroalimentación de relevancia, páginas 73-96. 2000. W.B. Croft, ed. [15] J. M. Ponte y W. B. Croft. Un enfoque de modelado del lenguaje para la recuperación de información. En Actas de la Conferencia ACM SIGIR de 1998 sobre Investigación y Desarrollo en Recuperación de Información, páginas 275-281. [16] J. Rocchio. El Sistema de Recuperación SMART, capítulo Retroalimentación de Relevancia en la Recuperación de Información, páginas 313-323. Prentice-Hall, 1971. G. Salton, ed. [17] T. Sakai, T. Manabe y M. Koyama. Retroalimentación de pseudo-relevancia flexible a través de muestreo selectivo. ACM Transactions on Asian Language Information Processing (TALIP), 4(2):111-135, 2005. [18] T. Tao y C. Zhai. Estimación regularizada de modelos de mezcla para retroalimentación de pseudo relevancia robusta. En Actas de la Conferencia ACM SIGIR 2006 sobre Investigación y Desarrollo en Recuperación de Información, páginas 162-169. [19] J. Xu y W. B. Croft. Mejorando la efectividad de la recuperación de información con análisis de contexto local. ACM Trans. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate to Spanish? Syst., 18(1):79-112, 2000. [20] E. YomTov, S. Fine, D. Carmel, y A. Darlow. Aprendiendo a estimar la dificultad de la consulta. En Actas de la Conferencia ACM SIGIR 2005 sobre Investigación y Desarrollo en Recuperación de Información, páginas 512-519. [21] Y. Zhou y W. B. Croft. Robustez del ranking: un nuevo marco para predecir el rendimiento de la consulta. En Actas de la 15ª Conferencia Internacional de la ACM. Conferencia sobre Gestión de Información y Conocimiento (CIKM 2006), páginas 567-574.",
    "original_sentences": [
        "Estimation and Use of Uncertainty in Pseudo-relevance Feedback Kevyn Collins-Thompson and Jamie Callan Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213-8213 U.S.A. {kct | callan}@cs.cmu.edu ABSTRACT Existing pseudo-relevance feedback methods typically perform averaging over the top-retrieved documents, but ignore an important statistical dimension: the risk or variance associated with either the individual document models, or their combination.",
        "Treating the baseline feedback method as a black box, and the output feedback model as a random variable, we estimate a posterior distribution for the feedback model by resampling a given querys top-retrieved documents, using the posterior mean or mode as the enhanced feedback model.",
        "We then perform model combination over several enhanced models, each based on a slightly modified query sampled from the original query.",
        "We find that resampling documents helps increase individual feedback model precision by removing noise terms, while sampling from the query improves robustness (worst-case performance) by emphasizing terms related to multiple query aspects.",
        "The result is a meta-feedback algorithm that is both more robust and more precise than the original strong baseline method.",
        "Categories and Subject Descriptors: H.3.3 [Information Retrieval]: Retrieval Models General Terms: Algorithms, Experimentation 1.",
        "INTRODUCTION Uncertainty is an inherent feature of information retrieval.",
        "Not only do we not know the queries that will be presented to our retrieval algorithm ahead of time, but the users information need may be vague or incompletely specified by these queries.",
        "Even if the query were perfectly specified, language in the collection documents is inherently complex and ambiguous and matching such language effectively is a formidable problem by itself.",
        "With this in mind, we wish to treat many important quantities calculated by the retrieval system, whether a relevance score for a document, or a weight for a query expansion term, as random variables whose true value is uncertain but where the uncertainty about the true value may be quantified by replacing the fixed value with a probability distribution over possible values.",
        "In this way, retrieval algorithms may attempt to quantify the risk or uncertainty associated with their output rankings, or improve the stability or precision of their internal calculations.",
        "Current algorithms for pseudo-relevance feedback (PRF) tend to follow the same basic method whether we use vector space-based algorithms such as Rocchios formula [16], or more recent language modeling approaches such as Relevance Models [10].",
        "First, a set of top-retrieved documents is obtained from an initial query and assumed to approximate a set of relevant documents.",
        "Next, a single feedback model vector is computed according to some sort of average, centroid, or expectation over the set of possibly-relevant document models.",
        "For example, the document vectors may be combined with equal weighting, as in Rocchio, or by query likelihood, as may be done using the Relevance Model1 .",
        "The use of an expectation is reasonable for practical and theoretical reasons, but by itself ignores potentially valuable information about the risk of the feedback model.",
        "Our main hypothesis in this paper is that estimating the uncertainty in feedback is useful and leads to better individual feedback models and more robust combined models.",
        "Therefore, we propose a method for estimating uncertainty associated with an individual feedback model in terms of a posterior distribution over language models.",
        "To do this, we systematically vary the inputs to the baseline feedback method and fit a Dirichlet distribution to the output.",
        "We use the posterior mean or mode as the improved feedback model estimate.",
        "This process is shown in Figure 1.",
        "As we show later, the mean and mode may vary significantly from the single feedback model proposed by the baseline method.",
        "We also perform model combination using several improved feedback language models obtained by a small number of new queries sampled from the original query.",
        "A models weight combines two complementary factors: the models probability of generating the query, and the variance of the model, with high-variance models getting lower weight. 1 For example, an expected parameter vector conditioned on the query observation is formed from top-retrieved documents, which are treated as training strings (see [10], p. 62).",
        "Figure 1: Estimating the uncertainty of the feedback model for a single query. 2.",
        "SAMPLING-BASED FEEDBACK In Sections 2.1-2.5 we describe a general method for estimating a probability distribution over the set of possible language models.",
        "In Sections 2.6 and 2.7 we summarize how different query samples are used to generate multiple feedback models, which are then combined. 2.1 Modeling Feedback Uncertainty Given a query Q and a collection C, we assume a probabilistic retrieval system that assigns a real-valued document score f(D, Q) to each document D in C, such that the score is proportional to the estimated probability of relevance.",
        "We make no other assumptions about f(D, Q).",
        "The nature of f(D, Q) may be complex: for example, if the retrieval system supports structured query languages [12], then f(D, Q) may represent the output of an arbitrarily complex inference network defined by the structured query operators.",
        "In theory, the scoring function can vary from query to query, although in this study for simplicity we keep the scoring function the same for all queries.",
        "Our specific query method is given in Section 3.",
        "We treat the feedback algorithm as a black box and assume that the inputs to the feedback algorithm are the original query and the corresponding top-retrieved documents, with a score being given to each document.",
        "We assume that the output of the feedback algorithm is a vector of term weights to be used to add or reweight the terms in the representation of the original query, with the vector normalized to form a probability distribution.",
        "We view the the inputs to the feedback black box as random variables, and analyze the feedback model as a random variable that changes in response to changes in the inputs.",
        "Like the document scoring function f(D, Q), the feedback algorithm may implement a complex, non-linear scoring formula, and so as its inputs vary, the resulting feedback models may have a complex distribution over the space of feedback models (the sample space).",
        "Because of this potential complexity, we do not attempt to derive a posterior distribution in closed form, but instead use simulation.",
        "We call this distribution over possible feedback models the feedback model distribution.",
        "Our goal in this section is to estimate a useful approximation to the feedback model distribution.",
        "For a specific framework for experiments, we use the language modeling (LM) approach for information retrieval [15].",
        "The score of a document D with respect to a query Q and collection C is given by p(Q|D) with respect to language models ˆθQ and ˆθD estimated for the query and document respectively.",
        "We denote the set of k top-retrieved documents from collection C in response to Q by DQ(k, C).",
        "For simplicity, we assume that queries and documents are generated by multinomial distributions whose parameters are represented by unigram language models.",
        "To incorporate feedback in the LM approach, we assume a model-based scheme in which our goal is take the query and resulting ranked documents DQ(k, C) as input, and output an expansion language model ˆθE, which is then interpolated with the original query model ˆθQ: ˆθNew = (1 − α) · ˆθQ + α · ˆθE (1) This includes the possibility of α = 1 where the original query mode is completely replaced by the feedback model.",
        "Our sample space is the set of all possible language models LF that may be output as feedback models.",
        "Our approach is to take samples from this space and then fit a distribution to the samples using maximum likelihood.",
        "For simplicity, we start by assuming the latent feedback distribution has the form of a Dirichlet distribution.",
        "Although the Dirichlet is a unimodal distribution, and in general quite limited in its expressiveness in the sample space, it is a natural match for the multinomial language model, can be estimated quickly, and can capture the most salient features of confident and uncertain feedback models, such as the overall spread of the distibution. 2.2 Resampling document models We would like an approximation to the posterior distribution of the feedback model LF .",
        "To accomplish this, we apply a widely-used simulation technique called bootstrap sampling ([7], p. 474) on the input parameters, namely, the set of top-retrieved documents.",
        "Bootstrap sampling allows us to simulate the approximate effect of perturbing the parameters within the black box feedback algorithm by perturbing the inputs to that algorithm in a systematic way, while making no assumptions about the nature of the feedback algorithm.",
        "Specifically, we sample k documents with replacement from DQ(k, C), and calculate an expansion language model θb using the black box feedback method.",
        "We repeat this process B times to obtain a set of B feedback language models, to which we then fit a Dirichlet distribution.",
        "Typically B is in the range of 20 to 50 samples, with performance being relatively stable in this range.",
        "Note that instead of treating each top document as equally likely, we sample according to the estimated probabilities of relevance of each document in DQ(k, C).",
        "Thus, a document is more likely to be chosen the higher it is in the ranking. 2.3 Justification for a sampling approach The rationale for our sampling approach has two parts.",
        "First, we want to improve the quality of individual feedback models by smoothing out variation when the baseline feedback model is unstable.",
        "In this respect, our approach resembles bagging [4], an ensemble approach which generates multiple versions of a predictor by making bootstrap copies of the training set, and then averages the (numerical) predictors.",
        "In our application, top-retrieved documents can be seen as a kind of noisy training set for relevance.",
        "Second, sampling is an effective way to estimate basic properties of the feedback posterior distribution, which can then be used for improved model combination.",
        "For example, a model may be weighted by its prediction confidence, estimated as a function of the variability of the posterior around the model. foo2-401.map-Dim:5434,Size:12*12units,gaussianneighborhood (a) Topic 401 Foreign minorities, Germany foo2-402.map-Dim:5698,Size:12*12units,gaussianneighborhood (b) Topic 402 Behavioral genetics foo2-459.map-Dim:8969,Size:12*12units,gaussianneighborhood (c) Topic 459 When can a lender foreclose on property Figure 2: Visualization of expansion language model variance using self-organizing maps, showing the distribution of language models that results from resampling the inputs to the baseline expansion method.",
        "The language model that would have been chosen by the baseline expansion is at the center of each map.",
        "The similarity function is JensenShannon divergence. 2.4 Visualizing feedback distributions Before describing how we fit and use the Dirichlet distribution over feedback models, it is instructive to view some examples of actual feedback model distributions that result from bootstrap sampling the top-retrieved documents from different TREC topics.",
        "Each point in our sample space is a language model, which typically has several thousand dimensions.",
        "To help analyze the behavior of our method we used a Self-Organizing Map (via the SOM-PAK package [9]), to flatten and visualize the high-dimensional density function2 .",
        "The density maps for three TREC topics are shown in Figure 2 above.",
        "The dark areas represent regions of high similarity between language models.",
        "The light areas represent regions of low similarity - the valleys between clusters.",
        "Each diagram is centered on the language model that would have been chosen by the baseline expansion.",
        "A single peak (mode) is evident in some examples, but more complex structure appears in others.",
        "Also, while the distribution is usually close to the baseline feedback model, for some topics they are a significant distance apart (as measured by JensenShannon divergence), as in Subfigure 2c.",
        "In such cases, the mode or mean of the feedback distribution often performs significantly better than the baseline (and in a smaller proportion of cases, significantly worse). 2.5 Fitting a posterior feedback distribution After obtaining feedback model samples by resampling the feedback model inputs, we estimate the feedback distribution.",
        "We assume that the multinomial feedback models {ˆθ1, . . . , ˆθB} were generated by a latent Dirichlet distribution with parameters {α1, . . . , αN }.",
        "To estimate the {α1, . . . , αN }, we fit the Dirichlet parameters to the B language model samples according to maximum likelihood using a generalized Newton procedure, details of which are given in Minka [13].",
        "We assume a simple Dirichlet prior over the {α1, . . . , αN }, setting each to αi = μ · p(wi | C), where μ is a parameter and p(· | C) is the collection language model estimated from a set of documents from collection C. The parameter fitting converges very quickly - typically just 2 or 2 Because our points are language models in the multinomial simplex, we extended SOM-PAK to support JensenShannon divergence, a widely-used similarity measure between probability distributions. 3 iterations are enough - so that it is practical to apply at query-time when computational overhead must be small.",
        "In practice, we can restrict the calculation to the vocabulary of the top-retrieved documents, instead of the entire collection.",
        "Note that for this step we are re-using the existing retrieved documents and not performing additional queries.",
        "Given the parameters of an N-dimensional Dirichlet distribution Dir(α) the mean μ and mode x vectors are easy to calculate and are given respectively by μi = αiP αi (2) and xi = αi−1P αi−N . (3) We can then choose the language model at the mean or the mode of the posterior as the final enhanced feedback model. (We found the mode to give slightly better performance.)",
        "For information retrieval, the number of samples we will have available is likely to be quite small for performance reasons - usually less than ten.",
        "Moreover, while random sampling is useful in certain cases, it is perfectly acceptable to allow deterministic sampling distributions, but these must be designed carefully in order to approximate an accurate output variance.",
        "We leave this for future study. 2.6 Query variants We use the following methods for generating variants of the original query.",
        "Each variant corresponds to a different assumption about which aspects of the original query may be important.",
        "This is a form of deterministic sampling.",
        "We selected three simple methods that cover complimentary assumptions about the query.",
        "No-expansion Use only the original query.",
        "The assumption is that the given terms are a complete description of the information need.",
        "Leave-one-out A single term is left out of the original query.",
        "The assumption is that one of the query terms is a noise term.",
        "Single-term A single term is chosen from the original query.",
        "This assumes that only one aspect of the query, namely, that represented by the term, is most important.",
        "After generating a variant of the original query, we combine it with the original query using a weight αSUB so that we do not stray too far.",
        "In this study, we set αSUB = 0.5.",
        "For example, using the Indri [12] query language, a leave-oneout variant of the initial query that omits the term ireland for TREC topic 404 is: #weight(0.5 #combine(ireland peace talks) 0.5 #combine(peace talks)) 2.7 Combining enhanced feedback models from multiple query variants When using multiple query variants, the resulting enhanced feedback models are combined using Bayesian model combination.",
        "To do this, we treat each word as an item to be classified as belonging to a relevant or non-relevant class, and derive a class probability for each word by combining the scores from each query variant.",
        "Each score is given by that terms probability in the Dirichlet distribution.",
        "The term scores are weighted by the inverse of the variance of the term in the enhanced feedback models Dirichlet distribution.",
        "The prior probability of a words membership in the relevant class is given by the probability of the original query in the entire enhanced expansion model. 3.",
        "EVALUATION In this section we present results confirming the usefulness of estimating a feedback model distribution from weighted resampling of top-ranked documents, and of combining the feedback models obtained from different small changes in the original query. 3.1 General method We evaluated performance on a total of 350 queries derived from four sets of TREC topics: 51-200 (TREC-1&2), 351-400 (TREC-7), 401-450 (TREC-8), and 451-550 (wt10g, TREC-9&10).",
        "We chose these for their varied content and document properties.",
        "For example, wt10g documents are Web pages with a wide variety of subjects and styles while TREC-1&2 documents are more homogeneous news articles.",
        "Indexing and retrieval was performed using the Indri system in the Lemur toolkit [12] [1].",
        "Our queries were derived from the words in the title field of the TREC topics.",
        "Phrases were not used.",
        "To generate the baseline queries passed to Indri, we wrapped the query terms with Indris #combine operator.",
        "For example, the initial query for topic 404 is: #combine(ireland peace talks) We performed Krovetz stemming for all experiments.",
        "Because we found that the baseline (Indri) expansion method performed better using a stopword list with the feedback model, all experiments used a stoplist of 419 common English words.",
        "However, an interesting side-effect of our resampling approach is that it tends to remove many stopwords from the feedback model, making a stoplist less critical.",
        "This is discussed further in Section 3.6. 3.2 Baseline feedback method For our baseline expansion method, we use an algorithm included in Indri 1.0 as the default expansion method.",
        "This method first selects terms using a log-odds calculation described by Ponte [14], but assigns final term weights using Lavrenkos relevance model[10].",
        "We chose the Indri method because it gives a consistently strong baseline, is based on a language modeling approach, and is simple to experiment with.",
        "In a TREC evaluation using the GOV2 corpus [6], the method was one of the topperforming runs, achieving a 19.8% gain in MAP compared to using unexpanded queries.",
        "In this study, it achieves an average gain in MAP of 17.25% over the four collections.",
        "Indris expansion method first calculates a log-odds ratio o(v) for each potential expansion term v given by o(v) = X D log p(v|D) p(v|C) (4) over all documents D containing v, in collection C. Then, the expansion term candidates are sorted by descending o(v), and the top m are chosen.",
        "Finally, the term weights r(v) used in the expanded query are calculated based on the relevance model r(v) = X D p(q|D)p(v|D) p(v) p(D) (5) The quantity p(q|D) is the probability score assigned to the document in the initial retrieval.",
        "We use Dirichlet smoothing of p(v|D) with μ = 1000.",
        "This relevance model is then combined with the original query using linear interpolation, weighted by a parameter α.",
        "By default we used the top 50 documents for feedback and the top 20 expansion terms, with the feedback interpolation parameter α = 0.5 unless otherwise stated.",
        "For example, the baseline expanded query for topic 404 is: #weight(0.5 #combine(ireland peace talks) 0.5 #weight(0.10 ireland 0.08 peace 0.08 northern ...) 3.3 Expansion performance We measure our feedback algorithms effectiveness by two main criteria: precision, and robustness.",
        "Robustness, and the tradeoff between precision and robustness, is analyzed in Section 3.4.",
        "In this section, we examine average precision and precision in the top 10 documents (P10).",
        "We also include recall at 1,000 documents.",
        "For each query, we obtained a set of B feedback models using the Indri baseline.",
        "Each feedback model was obtained from a random sample of the top k documents taken with replacement.",
        "For these experiments, B = 30 and k = 50.",
        "Each feedback model contained 20 terms.",
        "On the query side, we used leave-one-out (LOO) sampling to create the query variants.",
        "Single-term query sampling had consistently worse performance across all collections and so our results here focus on LOO sampling.",
        "We used the methods described in Section 2 to estimate an enhanced feedback model from the Dirichlet posterior distribution for each query variant, and to combine the feedback models from all the query variants.",
        "We call our method resampling expansion and denote it as RS-FB here.",
        "We denote the Indri baseline feedback method as Base-FB.",
        "Results from applying both the baseline expansion method (Base-FB) and resampling expansion (RS-FB) are shown in Table 1.",
        "We observe several trends in this table.",
        "First, the average precision of RS-FB was comparable to Base-FB, achieving an average gain of 17.6% compared to using no expansion across the four collections.",
        "The Indri baseline expansion gain was 17.25%.",
        "Also, the RS-FB method achieved consistent improvements in P10 over Base-FB for every topic set, with an average improvement of 6.89% over Base-FB for all 350 topics.",
        "The lowest P10 gain over Base-FB was +3.82% for TREC-7 and the highest was +11.95% for wt10g.",
        "Finally, both Base-FB and RS-FB also consistently improved recall over using no expansion, with Base-FB achieving better recall than RS-FB for all topic sets. 3.4 Retrieval robustness We use the term robustness to mean the worst-case average precision performance of a feedback algorithm.",
        "Ideally, a robust feedback method would never perform worse than using the original query, while often performing better using the expansion.",
        "To evaluate robustness in this study, we use a very simple measure called the robustness index (RI)3 .",
        "For a set of queries Q, the RI measure is defined as: RI(Q) = n+ − n− |Q| (6) where n+ is the number of queries helped by the feedback method and n− is the number of queries hurt.",
        "Here, by helped we mean obtaining a higher average precision as a result of feedback.",
        "The value of RI ranges from a minimum 3 This is sometimes also called the reliability of improvement index and was used in Sakai et al. [17].",
        "Collection NoExp Base-FB RS-FB TREC 1&2 AvgP 0.1818 0.2419 (+33.04%) 0.2406 (+32.24%) P10 0.4443 0.4913 (+10.57%) 0.5363 (+17.83%) Recall 15084/37393 19172/37393 15396/37393 TREC 7 AvgP 0.1890 0.2175 (+15.07%) 0.2169 (+14.75%) P10 0.4200 0.4320 (+2.85%) 0.4480 (+6.67%) Recall 2179/4674 2608/4674 2487/4674 TREC 8 AvgP 0.2031 0.2361 (+16.25%) 0.2268 (+11.70%) P10 0.3960 0.4160 (+5.05%) 0.4340 (+9.59%) Recall 2144/4728 2642/4728 2485/4728 wt10g AvgP 0.1741 0.1829 (+5.06%) 0.1946 (+11.78%) P10 0.2760 0.2630 (-4.71%) 0.2960 (+7.24%) Recall 3361/5980 3725/5980 3664/5980 Table 1: Comparison of baseline (Base-FB) feedback and feedback using re-sampling (RS-FB).",
        "Improvement shown for BaseFB and RS-FB is relative to using no expansion. (a) TREC 1&2 (upper curve); TREC 8 (lower curve) (b) TREC 7 (upper curve); wt10g (lower curve) Figure 3: The trade-off between robustness and average precision for different corpora.",
        "The x-axis gives the change in MAP over using baseline expansion with α = 0.5.",
        "The yaxis gives the Robustness Index (RI).",
        "Each curve through uncircled points shows the RI/MAP tradeoff using the simple small-α strategy (see text) as α decreases from 0.5 to zero in the direction of the arrow.",
        "Circled points represent the tradeoffs obtained by resampling feedback for α = 0.5.",
        "Collection N Base-FB RS-FB n− RI n− RI TREC 1&2 103 26 +0.495 15 +0.709 TREC 7 46 14 +0.391 10 +0.565 TREC 8 44 12 +0.455 12 +0.455 wt10g 91 48 -0.055 39 +0.143 Combined 284 100 +0.296 76 +0.465 Table 2: Comparison of robustness index (RI) for baseline feedback (Base-FB) vs. resampling feedback (RS-FB).",
        "Also shown are the actual number of queries hurt by feedback (n−) for each method and collection.",
        "Queries for which initial average precision was negligible (≤ 0.01) were ignored, giving the remaining query count in column N. of −1.0, when all queries are hurt by the feedback method, to +1.0 when all queries are helped.",
        "The RI measure does not take into account the magnitude or distribution of the amount of change across the set Q.",
        "However, it is easy to understand as a general indication of robustness.",
        "One obvious way to improve the worst-case performance of feedback is simply to use a smaller fixed α interpolation parameter, such as α = 0.3, placing less weight on the (possibly risky) feedback model and more on the original query.",
        "We call this the small-α strategy.",
        "Since we are also reducing the potential gains when the feedback model is right, however, we would expect some trade-off between average precision and robustness.",
        "We therefore compared the precision/robustness trade-off between our resampling feedback algorithm, and the simple small-α method.",
        "The results are summarized in Figure 3.",
        "In the figure, the curve for each topic set interpolates between trade-off points, beginning at x=0, where α = 0.5, and continuing in the direction of the arrow as α decreases and the original query is given more and more weight.",
        "As expected, robustness continuously increases as we move along the curve, but mean average precision generally drops as the gains from feedback are eliminated.",
        "For comparison, the performance of resampling feedback at α = 0.5 is shown for each collection as the circled point.",
        "Higher and to the right is better.",
        "This figure shows that resampling feedback gives a somewhat better trade-off than the small-α approach for 3 of the 4 collections.",
        "Figure 4: Histogram showing improved robustness of resampling feedback (RS-FB) over baseline feedback (Base-FB) for all datasets combined.",
        "Queries are binned by % change in AP compared to the unexpanded query.",
        "Collection DS + QV DS + No QV TREC 1&2 AvgP 0.2406 0.2547 (+5.86%) P10 0.5263 0.5362 (+1.88%) RI 0.7087 0.6515 (-0.0572) TREC 7 AvgP 0.2169 0.2200 (+1.43%) P10 0.4480 0.4300 (-4.02%) RI 0.5652 0.2609 (-0.3043) TREC 8 AvgP 0.2268 0.2257 (-0.49%) P10 0.4340 0.4200 (-3.23%) RI 0.4545 0.4091 (-0.0454) wt10g AvgP 0.1946 0.1865 (-4.16%) P10 0.2960 0.2680 (-9.46%) RI 0.1429 0.0220 (-0.1209) Table 3: Comparison of resampling feedback using document sampling (DS) with (QV) and without (No QV) combining feedback models from multiple query variants.",
        "Table 2 gives the Robustness Index scores for Base-FB and RS-FB.",
        "The RS-FB feedback method obtained higher robustness than Base-FB on three of the four topic sets, with only slightly worse performance on TREC-8.",
        "A more detailed view showing the distribution over relative changes in AP is given by the histogram in Figure 4.",
        "Compared to Base-FB, the RS-FB method achieves a noticable reduction in the number of queries significantly hurt by expansion (i.e. where AP is hurt by 25% or more), while preserving positive gains in AP. 3.5 Effect of query and document sampling methods Given our algorithms improved robustness seen in Section 3.4, an important question is what component of our system is responsible.",
        "Is it the use of document re-sampling, the use of multiple query variants, or some other factor?",
        "The results in Table 3 suggest that the model combination based on query variants may be largely account for the improved robustness.",
        "When query variants are turned off and the original query is used by itself with document sampling, there is little net change in average precision, a small decrease in P10 for 3 out of the 4 topic sets, but a significant drop in robustness for all topic sets.",
        "In two cases, the RI measure drops by more than 50%.",
        "We also examined the effect of the document sampling method on retrieval effectiveness, using two different strategies.",
        "The uniform weighting strategy ignored the relevance scores from the initial retrieval and gave each document in the top k the same probability of selection.",
        "In contrast, the relevance-score weighting strategy chose documents with probability proportional to their relevance scores.",
        "In this way, documents that were more highly ranked were more likely to be selected.",
        "Results are shown in Table 4.",
        "The relevance-score weighting strategy performs better overall, with significantly higher RI and P10 scores on 3 of the 4 topic sets.",
        "The difference in average precision between the methods, however, is less marked.",
        "This suggests that uniform weighting acts to increase variance in retrieval results: when initial average precision is high, there are many relevant documents in the top k and uniform sampling may give a more representative relevance model than focusing on the highly-ranked items.",
        "On the other hand, when initial precision is low, there are few relevant documents in the bottom ranks and uniform sampling mixes in more of the non-relevant documents.",
        "For space reasons we only summarize our findings on sample size here.",
        "The number of samples has some effect on precision when less than 10, but performance stabilizes at around 15 to 20 samples.",
        "We used 30 samples for our experiments.",
        "Much beyond this level, the additional benefits of more samples decrease as the initial score distribution is more closely fit and the processing time increases. 3.6 The effect of resampling on expansion term quality Ideally, a retrieval model should not require a stopword list when estimating a model of relevance: a robust statistical model should down-weight stopwords automatically depending on context.",
        "Stopwords can harm feedback if selected as feedback terms, because they are typically poor discriminators and waste valuable term slots.",
        "In practice, however, because most term selection methods resemble a tf · idf type of weighting, terms with low idf but very high tf can sometimes be selected as expansion term candidates.",
        "This happens, for example, even with the Relevance Model approach that is part of our baseline feedback.",
        "To ensure as strong a baseline as possible, we use a stoplist for all experiments reported here.",
        "If we turn off the stopword list, however, we obtain results such as those shown in Table 5 where four of the top ten baseline feedback terms for TREC topic 60 (said, but, their, not) are stopwords using the BaseFB method. (The top 100 expansion terms were selected to generate this example.)",
        "Indris method attempts to address the stopword problem by applying an initial step based on Ponte [14] to select less-common terms that have high log-odds of being in the top-ranked documents compared to the whole collection.",
        "Nevertheless, this does not overcome the stopword problem completely, especially as the number of feedback terms grows.",
        "Using resampling feedback, however, appears to mitigate Collection QV + Uniform QV + Relevance-score weighting weighting TREC 1&2 AvgP 0.2545 0.2406 (-5.46%) P10 0.5369 0.5263 (-1.97%) RI 0.6212 0.7087 (+14.09%) TREC 7 AvgP 0.2174 0.2169 (-0.23%) P10 0.4320 0.4480 (+3.70%) RI 0.4783 0.5652 (+18.17%) TREC 8 AvgP 0.2267 0.2268 (+0.04%) P10 0.4120 0.4340 (+5.34%) RI 0.4545 0.4545 (+0.00%) wt10g AvgP 0.1808 0.1946 (+7.63%) P10 0.2680 0.2960 (+10.45%) RI 0.0220 0.1099 (+399.5%) Table 4: Comparison of uniform and relevance-weighted document sampling.",
        "The percentage change compared to uniform sampling is shown in parentheses.",
        "QV indicates that query variants were used in both runs.",
        "Baseline FB p(wi|R) Resampling FB p(wi|R) said 0.055 court 0.026 court 0.055 pay 0.018 pay 0.034 federal 0.012 but 0.026 education 0.011 employees 0.024 teachers 0.010 their 0.024 employees 0.010 not 0.023 case 0.010 federal 0.021 their 0.009 workers 0.020 appeals 0.008 education 0.020 union 0.007 Table 5: Feedback term quality when a stoplist is not used.",
        "Feedback terms for TREC topic 60: merit pay vs seniority. the effect of stopwords automatically.",
        "In the example of Table 5, resampling feedback leaves only one stopword (their) in the top ten.",
        "We observed similar feedback term behavior across many other topics.",
        "The reason for this effect appears to be the interaction of the term selection score with the top-m term cutoff.",
        "While the presence and even proportion of particular stopwords is fairly stable across different document samples, their relative position in the top-m list is not, as sets of documents with varying numbers of better, lower-frequency term candidates are examined for each sample.",
        "As a result, while some number of stopwords may appear in each sampled document set, any given stopword tends to fall below the cutoff for multiple samples, leading to its classification as a high-variance, low-weight feature. 4.",
        "RELATED WORK Our approach is related to previous work from several areas of information retrieval and machine learning.",
        "Our use of query variation was inspired by the work of YomTov et al. [20], Carpineto et al. [5], and Amati et al. [2], among others.",
        "These studies use the idea of creating multiple subqueries and then examining the nature of the overlap in the documents and/or expansion terms that result from each subquery.",
        "Model combination is performed using heuristics.",
        "In particular, the studies of Amati et al. and Carpineto et al. investigated combining terms from individual distributional methods using a term-reranking combination heuristic.",
        "In a set of TREC topics they found wide average variation in the rank-distance of terms from different expansion methods.",
        "Their combination method gave modest positive improvements in average precision.",
        "The idea of examining the overlap between lists of suggested terms has also been used in early query expansion approaches.",
        "Xu and Crofts method of Local Context Analysis (LCA) [19] includes a factor in the empirically-derived weighting formula that causes expansion terms to be preferred that have connections to multiple query terms.",
        "On the document side, recent work by Zhou & Croft [21] explored the idea of adding noise to documents, re-scoring them, and using the stability of the resulting rankings as an estimate of query difficulty.",
        "This is related to our use of document sampling to estimate the risk of the feedback model built from the different sets of top-retrieved documents.",
        "Sakai et al. [17] proposed an approach to improving the robustness of pseudo-relevance feedback using a method they call selective sampling.",
        "The essence of their method is that they allow skipping of some top-ranked documents, based on a clustering criterion, in order to select a more varied and novel set of documents later in the ranking for use by a traditional pseudo-feedback method.",
        "Their study did not find significant improvements in either robustness (RI) or MAP on their corpora.",
        "Greiff, Morgan and Ponte [8] explored the role of variance in term weighting.",
        "In a series of simulations that simplified the problem to 2-feature documents, they found that average precision degrades as term frequency variance - high noiseincreases.",
        "Downweighting terms with high variance resulted in improved average precision.",
        "This seems in accord with our own findings for individual feedback models.",
        "Estimates of output variance have recently been used for improved text classification.",
        "Lee et al. [11] used queryspecific variance estimates of classifier outputs to perform improved model combination.",
        "Instead of using sampling, they were able to derive closed-form expressions for classifier variance by assuming base classifiers using simple types of inference networks.",
        "Ando and Zhang proposed a method that they call structural feedback [3] and showed how to apply it to query expansion for the TREC Genomics Track.",
        "They used r query variations to obtain R different sets Sr of top-ranked documents that have been intersected with the top-ranked documents obtained from the original query qorig.",
        "For each Si, the normalized centroid vector ˆwi of the documents is calculated.",
        "Principal component analysis (PCA) is then applied to the ˆwi to obtain the matrix Φ of H left singular vectors φh that are used to obtain the new, expanded query qexp = qorig + ΦT Φqorig. (7) In the case H = 1, we have a single left singular vector φ: qexp = qorig + (φT qorig)φ so that the dot product φT qorig is a type of dynamic weight on the expanded query that is based on the similarity of the original query to the expanded query.",
        "The use of variance as a feedback model quality measure occurs indirectly through the application of PCA.",
        "It would be interesting to study the connections between this approach and our own modelfitting method.",
        "Finally, in language modeling approaches to feedback, Tao and Zhai [18] describe a method for more robust feedback that allows each document to have a different feedback α.",
        "The feedback weights are derived automatically using regularized EM.",
        "A roughly equal balance of query and expansion model is implied by their EM stopping condition.",
        "They propose tailoring the stopping parameter η based on a function of some quality measure of feedback documents. 5.",
        "CONCLUSIONS We have presented a new approach to pseudo-relevance feedback based on document and query sampling.",
        "The use of sampling is a very flexible and powerful device and is motivated by our general desire to extend current models of retrieval by estimating the risk or variance associated with the parameters or output of retrieval processes.",
        "Such variance estimates, for example, may be naturally used in a Bayesian framework for improved model estimation and combination.",
        "Applications such as selective expansion may then be implemented in a principled way.",
        "While our study uses the language modeling approach as a framework for experiments, we make few assumptions about the actual workings of the feedback algorithm.",
        "We believe it is likely that any reasonably effective baseline feedback algorithm would benefit from our approach.",
        "Our results on standard TREC collections show that our framework improves the robustness of a strong baseline feedback method across a variety of collections, without sacrificing average precision.",
        "It also gives small but consistent gains in top10 precision.",
        "In future work, we envision an investigation into how varying the set of sampling methods used and the number of samples controls the trade-off between robustness, accuracy, and efficiency.",
        "Acknowledgements We thank Paul Bennett for valuable discussions related to this work, which was supported by NSF grants #IIS-0534345 and #CNS-0454018, and U.S. Dept. of Education grant #R305G03123.",
        "Any opinions, findings, and conclusions or recommendations expressed in this material are the authors. and do not necessarily reflect those of the sponsors. 6.",
        "REFERENCES [1] The Lemur toolkit for language modeling and retrieval. http://www.lemurproject.org. [2] G. Amati, C. Carpineto, and G. Romano.",
        "Query difficulty, robustness, and selective application of query expansion.",
        "In Proc. of the 25th European Conf. on Information Retrieval (ECIR 2004), pages 127-137. [3] R. K. Ando and T. Zhang.",
        "A high-performance semi-supervised learning method for text chunking.",
        "In Proc. of the 43rd Annual Meeting of the ACL, pages 1-9, June 2005. [4] L. Breiman.",
        "Bagging predictors.",
        "Machine Learning, 24(2):123-140, 1996. [5] C. Carpineto, G. Romano, and V. Giannini.",
        "Improving retrieval feedback with multiple term-ranking function combination.",
        "ACM Trans.",
        "Info.",
        "Systems, 20(3):259 - 290. [6] K. Collins-Thompson, P. Ogilvie, and J. Callan.",
        "Initial results with structured queries and language models on half a terabyte of text.",
        "In Proc. of 2005 Text REtrieval Conference.",
        "NIST Special Publication. [7] R. O. Duda, P. E. Hart, and D. G. Stork.",
        "Pattern Classification.",
        "Wiley and Sons, 2nd edition, 2001. [8] W. R. Greiff, W. T. Morgan, and J. M. Ponte.",
        "The role of variance in term weighting for probabilistic information retrieval.",
        "In Proc. of the 11th Intl.",
        "Conf. on Info. and Knowledge Mgmt. (CIKM 2002), pages 252-259. [9] T. Kohonen, J. Hynninen, J. Kangas, and J. Laaksonen.",
        "SOMPAK: The self-organizing map program package.",
        "Technical Report A31, Helsinki University of Technology, 1996. http://www.cis.hut.fi/research/papers/som tr96.ps.Z. [10] V. Lavrenko.",
        "A Generative Theory of Relevance.",
        "PhD thesis, University of Massachusetts, Amherst, 2004. [11] C.-H. Lee, R. Greiner, and S. Wang.",
        "Using query-specific variance estimates to combine Bayesian classifiers.",
        "In Proc. of the 23rd Intl.",
        "Conf. on Machine Learning (ICML 2006), pages 529-536. [12] D. Metzler and W. B. Croft.",
        "Combining the language model and inference network approaches to retrieval.",
        "Info.",
        "Processing and Mgmt., 40(5):735-750, 2004. [13] T. Minka.",
        "Estimating a Dirichlet distribution.",
        "Technical report, 2000. http://research.microsoft.com/ minka/papers/dirichlet. [14] J. Ponte.",
        "Advances in Information Retrieval, chapter Language models for relevance feedback, pages 73-96. 2000.",
        "W.B.",
        "Croft, ed. [15] J. M. Ponte and W. B. Croft.",
        "A language modeling approach to information retrieval.",
        "In Proc. of the 1998 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 275-281. [16] J. Rocchio.",
        "The SMART Retrieval System, chapter Relevance Feedback in Information Retrieval, pages 313-323.",
        "Prentice-Hall, 1971.",
        "G. Salton, ed. [17] T. Sakai, T. Manabe, and M. Koyama.",
        "Flexible pseudo-relevance feedback via selective sampling.",
        "ACM Transactions on Asian Language Information Processing (TALIP), 4(2):111-135, 2005. [18] T. Tao and C. Zhai.",
        "Regularized estimation of mixture models for robust pseudo-relevance feedback.",
        "In Proc. of the 2006 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 162-169. [19] J. Xu and W. B. Croft.",
        "Improving the effectiveness of information retrieval with local context analysis.",
        "ACM Trans.",
        "Inf.",
        "Syst., 18(1):79-112, 2000. [20] E. YomTov, S. Fine, D. Carmel, and A. Darlow.",
        "Learning to estimate query difficulty.",
        "In Proc. of the 2005 ACM SIGIR Conf. on Research and Development in Information Retrieval, pages 512-519. [21] Y. Zhou and W. B. Croft.",
        "Ranking robustness: a novel framework to predict query performance.",
        "In Proc. of the 15th ACM Intl.",
        "Conf. on Information and Knowledge Mgmt. (CIKM 2006), pages 567-574."
    ],
    "translated_text_sentences": [
        "Estimación y Uso de la Incertidumbre en la Retroalimentación de Pseudo-Relevancia Kevyn Collins-Thompson y Jamie Callan Instituto de Tecnologías del Lenguaje Escuela de Ciencias de la Computación Universidad Carnegie Mellon Pittsburgh, PA 15213-8213 EE. UU. {kct | callan}@cs.cmu.edu RESUMEN Los métodos existentes de retroalimentación de pseudo-relevancia suelen realizar un promedio de los documentos mejor clasificados, pero ignoran una dimensión estadística importante: el riesgo o la varianza asociada tanto a los modelos de documentos individuales como a su combinación.",
        "Tratando el método de retroalimentación de línea base como una caja negra, y el modelo de retroalimentación de salida como una variable aleatoria, estimamos una distribución posterior para el modelo de retroalimentación mediante el remuestreo de los documentos mejor recuperados de una consulta dada, utilizando la media o la moda posterior como el modelo de retroalimentación mejorado.",
        "Luego realizamos la combinación de modelos sobre varios modelos mejorados, cada uno basado en una consulta ligeramente modificada muestreada de la consulta original.",
        "Observamos que el remuestreo de documentos ayuda a aumentar la precisión del modelo de retroalimentación individual al eliminar términos de ruido, mientras que el muestreo de la consulta mejora la robustez (rendimiento en el peor de los casos) al enfatizar términos relacionados con múltiples aspectos de la consulta.",
        "El resultado es un algoritmo de meta-retroalimentación que es tanto más robusto como más preciso que el método de referencia original.",
        "Categorías y Descriptores de Asignaturas: H.3.3 [Recuperación de Información]: Modelos de Recuperación Términos Generales: Algoritmos, Experimentación 1.",
        "INTRODUCCIÓN La incertidumbre es una característica inherente de la recuperación de información.",
        "No solo no sabemos las consultas que se presentarán a nuestro algoritmo de recuperación con anticipación, sino que la necesidad de información de los usuarios puede ser vaga o estar incompletamente especificada por estas consultas.",
        "Incluso si la consulta estuviera perfectamente especificada, el lenguaje en los documentos de la colección es inherentemente complejo y ambiguo, y hacer coincidir dicho lenguaje de manera efectiva es un problema formidable por sí mismo.",
        "Con esto en mente, deseamos tratar muchas cantidades importantes calculadas por el sistema de recuperación, ya sea un puntaje de relevancia para un documento, o un peso para un término de expansión de consulta, como variables aleatorias cuyo valor real es incierto pero donde la incertidumbre sobre el valor real puede ser cuantificada reemplazando el valor fijo con una distribución de probabilidad sobre posibles valores.",
        "De esta manera, los algoritmos de recuperación pueden intentar cuantificar el riesgo o la incertidumbre asociados con sus clasificaciones de salida, o mejorar la estabilidad o precisión de sus cálculos internos.",
        "Los algoritmos actuales para la retroalimentación de pseudo relevancia (PRF) tienden a seguir el mismo método básico ya sea que utilicemos algoritmos basados en espacio vectorial como la fórmula de Rocchio [16], o enfoques más recientes de modelado de lenguaje como Modelos de Relevancia [10].",
        "Primero, se obtiene un conjunto de documentos recuperados en la parte superior a partir de una consulta inicial y se asume que aproxima un conjunto de documentos relevantes.",
        "A continuación, se calcula un vector de modelo de retroalimentación único según algún tipo de promedio, centroide o expectativa sobre el conjunto de modelos de documentos posiblemente relevantes.",
        "Por ejemplo, los vectores de documentos pueden combinarse con igual ponderación, como en Rocchio, o por probabilidad de consulta, como se puede hacer utilizando el Modelo de Relevancia.",
        "El uso de una expectativa es razonable por razones prácticas y teóricas, pero por sí sola ignora información potencialmente valiosa sobre el riesgo del modelo de retroalimentación.",
        "Nuestra hipótesis principal en este artículo es que estimar la incertidumbre en la retroalimentación es útil y conduce a modelos de retroalimentación individual más efectivos y a modelos combinados más robustos.",
        "Por lo tanto, proponemos un método para estimar la incertidumbre asociada con un modelo de retroalimentación individual en términos de una distribución posterior sobre modelos de lenguaje.",
        "Para hacer esto, variamos sistemáticamente las entradas al método de retroalimentación base y ajustamos una distribución de Dirichlet a la salida.",
        "Utilizamos la media posterior o el modo como la estimación mejorada del modelo de retroalimentación.",
        "Este proceso se muestra en la Figura 1.",
        "Como mostraremos más adelante, la media y la moda pueden variar significativamente del modelo de retroalimentación única propuesto por el método de referencia.",
        "También realizamos la combinación de modelos utilizando varios modelos de lenguaje de retroalimentación mejorados obtenidos a partir de un pequeño número de nuevas consultas muestreadas de la consulta original.",
        "El peso de un modelo combina dos factores complementarios: la probabilidad del modelo de generar la consulta y la varianza del modelo, siendo los modelos con alta varianza los que obtienen un peso menor. Por ejemplo, un vector de parámetros esperado condicionado a la observación de la consulta se forma a partir de los documentos recuperados en la parte superior, que se tratan como cadenas de entrenamiento (ver [10], p. 62).",
        "Figura 1: Estimación de la incertidumbre del modelo de retroalimentación para una sola consulta. 2.",
        "En las Secciones 2.1-2.5 describimos un método general para estimar una distribución de probabilidad sobre el conjunto de posibles modelos de lenguaje basado en muestreo.",
        "En las Secciones 2.6 y 2.7 resumimos cómo se utilizan diferentes muestras de consultas para generar múltiples modelos de retroalimentación, que luego se combinan. 2.1 Modelado de la Incertidumbre de la Retroalimentación Dado una consulta Q y una colección C, asumimos un sistema de recuperación probabilístico que asigna un puntaje de documento de valor real f(D, Q) a cada documento D en C, de manera que el puntaje sea proporcional a la probabilidad estimada de relevancia.",
        "No hacemos ninguna otra suposición sobre f(D, Q).",
        "La naturaleza de f(D, Q) puede ser compleja: por ejemplo, si el sistema de recuperación admite lenguajes de consulta estructurados [12], entonces f(D, Q) puede representar la salida de una red de inferencia arbitrariamente compleja definida por los operadores de consulta estructurados.",
        "En teoría, la función de puntuación puede variar de una consulta a otra, aunque en este estudio por simplicidad mantenemos la misma función de puntuación para todas las consultas.",
        "Nuestro método de consulta específico se encuentra en la Sección 3.",
        "Tratamos el algoritmo de retroalimentación como una caja negra y asumimos que los insumos del algoritmo de retroalimentación son la consulta original y los documentos recuperados principales correspondientes, a los cuales se les asigna una puntuación a cada documento.",
        "Suponemos que la salida del algoritmo de retroalimentación es un vector de pesos de términos que se utilizarán para agregar o reponderar los términos en la representación de la consulta original, con el vector normalizado para formar una distribución de probabilidad.",
        "Consideramos las entradas a la caja negra de retroalimentación como variables aleatorias, y analizamos el modelo de retroalimentación como una variable aleatoria que cambia en respuesta a cambios en las entradas.",
        "Al igual que la función de puntuación del documento f(D, Q), el algoritmo de retroalimentación puede implementar una fórmula de puntuación compleja y no lineal, por lo que, a medida que sus entradas varían, los modelos de retroalimentación resultantes pueden tener una distribución compleja en el espacio de modelos de retroalimentación (el espacio muestral).",
        "Debido a esta complejidad potencial, no intentamos derivar una distribución posterior de forma cerrada, sino que en su lugar utilizamos simulación.",
        "Llamamos a esta distribución sobre posibles modelos de retroalimentación la distribución de modelos de retroalimentación.",
        "Nuestro objetivo en esta sección es estimar una aproximación útil a la distribución del modelo de retroalimentación.",
        "Para un marco específico de experimentos, utilizamos el enfoque de modelado de lenguaje (LM) para la recuperación de información [15].",
        "El puntaje de un documento D con respecto a una consulta Q y una colección C se da por p(Q|D) con respecto a los modelos de lenguaje ˆθQ y ˆθD estimados para la consulta y el documento respectivamente.",
        "Denotamos el conjunto de los k documentos mejor recuperados de la colección C en respuesta a Q como DQ(k, C).",
        "Para simplificar, asumimos que las consultas y documentos son generados por distribuciones multinomiales cuyos parámetros están representados por modelos de lenguaje unigram.",
        "Para incorporar retroalimentación en el enfoque de LM, asumimos un esquema basado en modelos en el que nuestro objetivo es tomar la consulta y los documentos clasificados resultantes DQ(k, C) como entrada, y producir un modelo de lenguaje expandido ˆθE, que luego se interpola con el modelo de consulta original ˆθQ: ˆθNew = (1 − α) · ˆθQ + α · ˆθE (1). Esto incluye la posibilidad de α = 1 donde el modelo de consulta original es completamente reemplazado por el modelo de retroalimentación.",
        "Nuestro espacio muestral es el conjunto de todos los posibles modelos de lenguaje LF que pueden ser generados como modelos de retroalimentación.",
        "Nuestro enfoque es tomar muestras de este espacio y luego ajustar una distribución a las muestras utilizando máxima verosimilitud.",
        "Para simplificar, comenzamos asumiendo que la distribución de retroalimentación latente tiene la forma de una distribución de Dirichlet.",
        "Aunque la distribución de Dirichlet es unimodal y en general bastante limitada en su expresividad en el espacio muestral, es una coincidencia natural para el modelo de lenguaje multinomial, puede ser estimada rápidamente y capturar las características más relevantes de los modelos de retroalimentación confiada e incierta, como la dispersión general de la distribución. 2.2 Re-muestreo de modelos de documentos. Nos gustaría una aproximación a la distribución posterior del modelo de retroalimentación LF.",
        "Para lograr esto, aplicamos una técnica de simulación ampliamente utilizada llamada muestreo bootstrap ([7], p. 474) en los parámetros de entrada, es decir, el conjunto de documentos recuperados en la parte superior.",
        "El muestreo de arranque nos permite simular el efecto aproximado de perturbar los parámetros dentro del algoritmo de retroalimentación de la caja negra al perturbar los insumos de ese algoritmo de manera sistemática, sin hacer suposiciones sobre la naturaleza del algoritmo de retroalimentación.",
        "Específicamente, muestreamos k documentos con reemplazo de DQ(k, C), y calculamos un modelo de lenguaje de expansión θb utilizando el método de retroalimentación de caja negra.",
        "Repetimos este proceso B veces para obtener un conjunto de B modelos de lenguaje de retroalimentación, a los cuales luego ajustamos una distribución de Dirichlet.",
        "Normalmente, B se encuentra en el rango de 20 a 50 muestras, con un rendimiento relativamente estable en este rango.",
        "Ten en cuenta que en lugar de tratar cada documento superior como igualmente probable, muestreamos de acuerdo con las probabilidades estimadas de relevancia de cada documento en DQ(k, C).",
        "Por lo tanto, es más probable que un documento sea elegido cuanto más alto esté en la clasificación. 2.3 Justificación de un enfoque de muestreo La justificación de nuestro enfoque de muestreo tiene dos partes.",
        "Primero, queremos mejorar la calidad de los modelos de retroalimentación individual al suavizar la variación cuando el modelo de retroalimentación base es inestable.",
        "En este sentido, nuestro enfoque se asemeja al bagging [4], un enfoque de conjunto que genera múltiples versiones de un predictor haciendo copias de arranque del conjunto de entrenamiento, y luego promedia los predictores (numéricos).",
        "En nuestra aplicación, los documentos recuperados en la parte superior pueden ser vistos como una especie de conjunto de entrenamiento ruidoso para la relevancia.",
        "Segundo, el muestreo es una forma efectiva de estimar propiedades básicas de la distribución posterior de retroalimentación, las cuales pueden ser utilizadas para mejorar la combinación de modelos.",
        "Por ejemplo, un modelo puede ser ponderado por su confianza de predicción, estimada como una función de la variabilidad del posterior alrededor del modelo. foo2-401.map-Dim:5434,Size:12*12units, vecindario gaussiano (a) Tema 401 Minorías extranjeras, Alemania foo2-402.map-Dim:5698,Size:12*12units, vecindario gaussiano (b) Tema 402 Genética del comportamiento foo2-459.map-Dim:8969,Size:12*12units, vecindario gaussiano (c) Tema 459 ¿Cuándo puede un prestamista ejecutar una hipoteca sobre una propiedad? Figura 2: Visualización de la varianza del modelo de lenguaje de expansión utilizando mapas autoorganizados, mostrando la distribución de modelos de lenguaje que resulta de remuestrear las entradas al método de expansión de referencia.",
        "El modelo de lenguaje que habría sido elegido por la expansión de la línea base se encuentra en el centro de cada mapa.",
        "La función de similitud es la divergencia JensenShannon. 2.4 Visualización de distribuciones de retroalimentación Antes de describir cómo ajustamos y utilizamos la distribución de Dirichlet sobre modelos de retroalimentación, es instructivo ver algunos ejemplos de distribuciones de modelos de retroalimentación reales que resultan de muestrear por bootstrap los documentos más recuperados de diferentes temas de TREC.",
        "Cada punto en nuestro espacio muestral es un modelo de lenguaje, que típicamente tiene varias miles de dimensiones.",
        "Para ayudar a analizar el comportamiento de nuestro método, utilizamos un Mapa Auto-organizado (a través del paquete SOM-PAK [9]), para aplanar y visualizar la función de densidad de alta dimensión.",
        "Los mapas de densidad para tres temas de TREC se muestran en la Figura 2 arriba.",
        "Las áreas oscuras representan regiones de alta similitud entre los modelos de lenguaje.",
        "Las áreas claras representan regiones de baja similitud, los valles entre los grupos.",
        "Cada diagrama está centrado en el modelo de lenguaje que habría sido elegido por la expansión de referencia.",
        "Un solo pico (modo) es evidente en algunos ejemplos, pero en otros aparece una estructura más compleja.",
        "Además, aunque la distribución suele estar cerca del modelo de retroalimentación de referencia, para algunos temas están a una distancia significativa (según la divergencia de JensenShannon), como se muestra en la Subfigura 2c.",
        "En tales casos, la moda o la media de la distribución de retroalimentación a menudo funcionan significativamente mejor que el valor base (y en una proporción menor de casos, significativamente peor). 2.5 Ajuste de una distribución de retroalimentación posterior Después de obtener muestras del modelo de retroalimentación mediante el remuestreo de las entradas del modelo de retroalimentación, estimamos la distribución de retroalimentación.",
        "Suponemos que los modelos de retroalimentación multinomial {ˆθ1, . . . , ˆθB} fueron generados por una distribución de Dirichlet latente con parámetros {α1, . . . , αN}.",
        "Para estimar los {α1, . . . , αN }, ajustamos los parámetros de Dirichlet a las muestras del modelo de lenguaje B según la máxima verosimilitud utilizando un procedimiento de Newton generalizado, cuyos detalles se encuentran en Minka [13].",
        "Suponemos una distribución de Dirichlet simple sobre los {α1, . . . , αN}, estableciendo cada uno como αi = μ · p(wi | C), donde μ es un parámetro y p(· | C) es el modelo de lenguaje de colección estimado a partir de un conjunto de documentos de la colección C. El ajuste de parámetros converge muy rápidamente, generalmente en solo 2 o 3 iteraciones. Dado que nuestros puntos son modelos de lenguaje en el simplex multinomial, extendimos SOM-PAK para admitir la divergencia de Jensen-Shannon, una medida de similitud ampliamente utilizada entre distribuciones de probabilidad. 3 iteraciones son suficientes, por lo que es práctico aplicarlo en tiempo de consulta cuando la sobrecarga computacional debe ser pequeña.",
        "En la práctica, podemos restringir el cálculo al vocabulario de los documentos recuperados en la parte superior, en lugar de toda la colección.",
        "Ten en cuenta que para este paso estamos reutilizando los documentos recuperados existentes y no realizando consultas adicionales.",
        "Dado los parámetros de una distribución de Dirichlet N-dimensional Dir(α), los vectores de media μ y moda x son fáciles de calcular y se dan respectivamente por μi = αiP αi (2) y xi = αi−1P αi−N . (3) Luego podemos elegir el modelo de lenguaje en la media o en la moda del posterior como el modelo de retroalimentación mejorado final. (Encontramos que la moda proporciona un rendimiento ligeramente mejor).",
        "Para la recuperación de información, es probable que el número de muestras disponibles sea bastante pequeño por razones de rendimiento, generalmente menos de diez.",
        "Además, si bien el muestreo aleatorio es útil en ciertos casos, es perfectamente aceptable permitir distribuciones de muestreo determinísticas, pero estas deben diseñarse cuidadosamente para aproximar una varianza de salida precisa.",
        "Dejamos esto para estudios futuros. 2.6 Variantes de consulta Utilizamos los siguientes métodos para generar variantes de la consulta original.",
        "Cada variante corresponde a una suposición diferente sobre qué aspectos de la consulta original pueden ser importantes.",
        "Esta es una forma de muestreo determinístico.",
        "Seleccionamos tres métodos simples que cubren suposiciones complementarias sobre la consulta.",
        "Uso sin expansión.",
        "La suposición es que los términos dados son una descripción completa de la necesidad de información.",
        "Deja uno fuera. Se deja fuera un término del query original.",
        "La suposición es que uno de los términos de la consulta es un término de ruido.",
        "Se elige un término único de la consulta original.",
        "Esto asume que solo un aspecto de la consulta, es decir, el representado por el término, es el más importante.",
        "Después de generar una variante de la consulta original, la combinamos con la consulta original utilizando un peso αSUB para no alejarnos demasiado.",
        "En este estudio, establecimos αSUB = 0.5.",
        "Por ejemplo, utilizando el lenguaje de consulta Indri [12], una variante de dejar uno fuera de la consulta inicial que omite el término \"ireland\" para el tema TREC 404 es: #weight(0.5 #combine(ireland peace talks) 0.5 #combine(peace talks)) 2.7 Combinando modelos de retroalimentación mejorados de múltiples variantes de consulta. Cuando se utilizan múltiples variantes de consulta, los modelos de retroalimentación mejorados resultantes se combinan utilizando la combinación de modelos bayesianos.",
        "Para hacer esto, tratamos cada palabra como un elemento a clasificar como perteneciente a una clase relevante o no relevante, y derivamos una probabilidad de clase para cada palabra combinando las puntuaciones de cada variante de consulta.",
        "Cada puntuación es dada por la probabilidad de ese término en la distribución de Dirichlet.",
        "Las puntuaciones de los términos se ponderan por el inverso de la varianza del término en los modelos de retroalimentación mejorada de la distribución de Dirichlet.",
        "La probabilidad previa de pertenencia de una palabra a la clase relevante se da por la probabilidad de la consulta original en todo el modelo de expansión mejorado. 3.",
        "EVALUACIÓN En esta sección presentamos resultados que confirman la utilidad de estimar una distribución del modelo de retroalimentación a partir del remuestreo ponderado de documentos mejor clasificados, y de combinar los modelos de retroalimentación obtenidos a partir de diferentes pequeños cambios en la consulta original. 3.1 Método general Evaluamos el rendimiento en un total de 350 consultas derivadas de cuatro conjuntos de temas de TREC: 51-200 (TREC-1&2), 351-400 (TREC-7), 401-450 (TREC-8) y 451-550 (wt10g, TREC-9&10).",
        "Elegimos estos por su contenido variado y propiedades del documento.",
        "Por ejemplo, los documentos wt10g son páginas web con una amplia variedad de temas y estilos, mientras que los documentos TREC-1&2 son artículos de noticias más homogéneos.",
        "La indexación y recuperación se realizó utilizando el sistema Indri en el kit de herramientas Lemur [12] [1].",
        "Nuestras consultas se derivaron de las palabras en el campo del título de los temas de TREC.",
        "Las frases no fueron utilizadas.",
        "Para generar las consultas base que se pasaron a Indri, envolvimos los términos de la consulta con el operador #combine de Indri.",
        "Por ejemplo, la consulta inicial para el tema 404 es: #combine(ireland peace talks) Realizamos el stemming de Krovetz para todos los experimentos.",
        "Debido a que encontramos que el método de expansión de la base (Indri) funcionó mejor utilizando una lista de palabras vacías con el modelo de retroalimentación, todos los experimentos utilizaron una lista de 419 palabras comunes en inglés.",
        "Sin embargo, un efecto secundario interesante de nuestro enfoque de remuestreo es que tiende a eliminar muchas palabras vacías del modelo de retroalimentación, lo que hace que una lista de paradas sea menos crítica.",
        "Esto se discute más a fondo en la Sección 3.6. Método de retroalimentación de línea base 3.2 Para nuestro método de expansión de línea base, utilizamos un algoritmo incluido en Indri 1.0 como el método de expansión predeterminado.",
        "Este método primero selecciona términos utilizando un cálculo de logaritmo de probabilidades descrito por Ponte [14], pero asigna pesos finales a los términos utilizando el modelo de relevancia de Lavrenko [10].",
        "Elegimos el método Indri porque proporciona una línea base consistentemente sólida, se basa en un enfoque de modelado del lenguaje y es fácil de experimentar con él.",
        "En una evaluación de TREC utilizando el corpus GOV2, el método fue uno de los mejores resultados, logrando un aumento del 19.8% en el MAP en comparación con el uso de consultas no expandidas.",
        "En este estudio, se logra una ganancia promedio en MAP del 17.25% en las cuatro colecciones.",
        "El método de expansión de Indri primero calcula una razón de logaritmos de probabilidades o(v) para cada término de expansión potencial v dado por o(v) = X D log p(v|D) p(v|C) (4) sobre todos los documentos D que contienen v, en la colección C. Luego, los candidatos a términos de expansión se ordenan por o(v) descendente, y se eligen los primeros m.",
        "Finalmente, los pesos de término r(v) utilizados en la consulta ampliada se calculan en base al modelo de relevancia r(v) = X D p(q|D)p(v|D) p(v) p(D) (5). La cantidad p(q|D) es la puntuación de probabilidad asignada al documento en la recuperación inicial.",
        "Utilizamos suavizado de Dirichlet de p(v|D) con μ = 1000.",
        "Este modelo de relevancia se combina luego con la consulta original utilizando interpolación lineal, ponderada por un parámetro α.",
        "Por defecto, utilizamos los 50 documentos principales para la retroalimentación y los 20 términos de expansión, con el parámetro de interpolación de retroalimentación α = 0.5 a menos que se indique lo contrario.",
        "Por ejemplo, la consulta expandida base para el tema 404 es: #weight(0.5 #combine(ireland peace talks) 0.5 #weight(0.10 ireland 0.08 peace 0.08 northern ...) 3.3 Rendimiento de la expansión. Medimos la efectividad de nuestros algoritmos de retroalimentación mediante dos criterios principales: precisión y robustez.",
        "La robustez, y el equilibrio entre precisión y robustez, se analiza en la Sección 3.4.",
        "En esta sección, examinamos la precisión promedio y la precisión en los 10 documentos principales (P10).",
        "También incluimos la recuperación de 1,000 documentos.",
        "Para cada consulta, obtuvimos un conjunto de B modelos de retroalimentación utilizando la línea base de Indri.",
        "Cada modelo de retroalimentación se obtuvo a partir de una muestra aleatoria de los k documentos principales tomados con reemplazo.",
        "Para estos experimentos, B = 30 y k = 50.",
        "Cada modelo de retroalimentación contenía 20 términos.",
        "En el lado de la consulta, utilizamos muestreo de dejar uno fuera (LOO) para crear las variantes de consulta.",
        "El muestreo de consulta de un solo término tuvo un rendimiento consistentemente peor en todas las colecciones, por lo que nuestros resultados se centran en el muestreo de LOO aquí.",
        "Utilizamos los métodos descritos en la Sección 2 para estimar un modelo de retroalimentación mejorado a partir de la distribución posterior de Dirichlet para cada variante de consulta, y para combinar los modelos de retroalimentación de todas las variantes de consulta.",
        "Llamamos a nuestro método expansión de remuestreo y lo denotamos como RS-FB aquí.",
        "Denominamos al método de retroalimentación de línea base Indri como Base-FB.",
        "Los resultados de aplicar tanto el método de expansión de línea base (Base-FB) como la expansión de remuestreo (RS-FB) se muestran en la Tabla 1.",
        "Observamos varias tendencias en esta tabla.",
        "Primero, la precisión promedio de RS-FB fue comparable a Base-FB, logrando una ganancia promedio del 17.6% en comparación con no utilizar expansión en las cuatro colecciones.",
        "La ganancia de expansión de la línea base de Indri fue del 17.25%.",
        "Además, el método RS-FB logró mejoras consistentes en P10 sobre Base-FB para cada conjunto de temas, con una mejora promedio del 6.89% sobre Base-FB para los 350 temas.",
        "La ganancia P10 más baja sobre Base-FB fue del +3.82% para TREC-7 y la más alta fue del +11.95% para wt10g.",
        "Finalmente, tanto Base-FB como RS-FB también mejoraron consistentemente la recuperación en comparación con no utilizar expansión, siendo Base-FB el que logró una mejor recuperación que RS-FB para todos los conjuntos de temas. 3.4 Robustez de recuperación. Utilizamos el término robustez para referirnos al rendimiento de precisión promedio en el peor de los casos de un algoritmo de retroalimentación.",
        "Idealmente, un método de retroalimentación sólido nunca debería funcionar peor que usar la consulta original, mientras que a menudo funciona mejor utilizando la expansión.",
        "Para evaluar la robustez en este estudio, utilizamos una medida muy simple llamada índice de robustez (RI).",
        "Para un conjunto de consultas Q, la medida de RI se define como: RI(Q) = n+ − n− |Q| (6) donde n+ es el número de consultas ayudadas por el método de retroalimentación y n− es el número de consultas perjudicadas.",
        "Aquí, por \"ayudar\" nos referimos a obtener una precisión promedio más alta como resultado de la retroalimentación.",
        "El valor de RI varía desde un mínimo de 3. A veces también se le llama índice de confiabilidad de mejora y fue utilizado en Sakai et al. [17].",
        "Colección NoExp Base-FB RS-FB TREC 1&2 AvgP 0.1818 0.2419 (+33.04%) 0.2406 (+32.24%) P10 0.4443 0.4913 (+10.57%) 0.5363 (+17.83%) Recall 15084/37393 19172/37393 15396/37393 TREC 7 AvgP 0.1890 0.2175 (+15.07%) 0.2169 (+14.75%) P10 0.4200 0.4320 (+2.85%) 0.4480 (+6.67%) Recall 2179/4674 2608/4674 2487/4674 TREC 8 AvgP 0.2031 0.2361 (+16.25%) 0.2268 (+11.70%) P10 0.3960 0.4160 (+5.05%) 0.4340 (+9.59%) Recall 2144/4728 2642/4728 2485/4728 wt10g AvgP 0.1741 0.1829 (+5.06%) 0.1946 (+11.78%) P10 0.2760 0.2630 (-4.71%) 0.2960 (+7.24%) Recall 3361/5980 3725/5980 3664/5980 Tabla 1: Comparación de la retroalimentación base (Base-FB) y la retroalimentación utilizando re-muestreo (RS-FB).",
        "La mejora mostrada para BaseFB y RS-FB es relativa al uso de ninguna expansión. (a) TREC 1&2 (curva superior); TREC 8 (curva inferior) (b) TREC 7 (curva superior); wt10g (curva inferior) Figura 3: La compensación entre robustez y precisión promedio para diferentes corpora.",
        "El eje x muestra el cambio en la MAP al utilizar la expansión de la línea base con α = 0.5.",
        "El eje y muestra el Índice de Robustez (RI).",
        "Cada curva a través de los puntos no circulados muestra el compromiso entre RI/MAP utilizando la estrategia simple de pequeño α (ver texto) a medida que α disminuye de 0.5 a cero en la dirección de la flecha.",
        "Los puntos circulados representan los compromisos obtenidos al volver a muestrear la retroalimentación para α = 0.5.",
        "Colección N Base-FB RS-FB n− RI n− RI TREC 1&2 103 26 +0.495 15 +0.709 TREC 7 46 14 +0.391 10 +0.565 TREC 8 44 12 +0.455 12 +0.455 wt10g 91 48 -0.055 39 +0.143 Combinado 284 100 +0.296 76 +0.465 Tabla 2: Comparación del índice de robustez (RI) para retroalimentación de línea base (Base-FB) vs. retroalimentación de remuestreo (RS-FB).",
        "También se muestran el número real de consultas perjudicadas por la retroalimentación (n−) para cada método y colección.",
        "Las consultas para las cuales la precisión promedio inicial era insignificante (≤ 0.01) fueron ignoradas, dando como resultado el recuento de consultas restantes en la columna N, de -1.0, cuando todas las consultas son perjudicadas por el método de retroalimentación, a +1.0 cuando todas las consultas son ayudadas.",
        "La medida de RI no tiene en cuenta la magnitud o distribución de la cantidad de cambio en el conjunto Q.",
        "Sin embargo, es fácil de entender como una indicación general de robustez.",
        "Una forma obvia de mejorar el rendimiento en el peor de los casos de la retroalimentación es simplemente utilizar un parámetro de interpolación α fijo más pequeño, como α = 0.3, otorgando menos peso al modelo de retroalimentación (posiblemente arriesgado) y más al cuestionario original.",
        "Llamamos a esto la estrategia de pequeña α.",
        "Dado que también estamos reduciendo las ganancias potenciales cuando el modelo de retroalimentación es correcto, sin embargo, esperaríamos algún tipo de compensación entre la precisión promedio y la robustez.",
        "Por lo tanto, comparamos el equilibrio entre precisión y robustez entre nuestro algoritmo de retroalimentación de remuestreo y el método simple de pequeño α.",
        "Los resultados se resumen en la Figura 3.",
        "En la figura, la curva para cada conjunto de temas interpola entre los puntos de compensación, comenzando en x=0, donde α = 0.5, y continuando en la dirección de la flecha a medida que α disminuye y la consulta original recibe cada vez más peso.",
        "Como era de esperar, la robustez aumenta continuamente a medida que avanzamos a lo largo de la curva, pero la precisión media promedio generalmente disminuye a medida que se eliminan las ganancias del feedback.",
        "Para la comparación, se muestra el rendimiento del re-muestreo con retroalimentación en α = 0.5 para cada colección como el punto marcado con un círculo.",
        "Más alto y a la derecha es mejor.",
        "Esta figura muestra que el re-muestreo de retroalimentación ofrece un compromiso algo mejor que el enfoque de pequeño α para 3 de las 4 colecciones.",
        "Figura 4: Histograma que muestra la mayor robustez del re-muestreo de retroalimentación (RS-FB) sobre la retroalimentación base (Base-FB) para todos los conjuntos de datos combinados.",
        "Las consultas se agrupan por el % de cambio en AP en comparación con la consulta no expandida.",
        "Colección DS + QV DS + No QV TREC 1&2 AvgP 0.2406 0.2547 (+5.86%) P10 0.5263 0.5362 (+1.88%) RI 0.7087 0.6515 (-0.0572) TREC 7 AvgP 0.2169 0.2200 (+1.43%) P10 0.4480 0.4300 (-4.02%) RI 0.5652 0.2609 (-0.3043) TREC 8 AvgP 0.2268 0.2257 (-0.49%) P10 0.4340 0.4200 (-3.23%) RI 0.4545 0.4091 (-0.0454) wt10g AvgP 0.1946 0.1865 (-4.16%) P10 0.2960 0.2680 (-9.46%) RI 0.1429 0.0220 (-0.1209) Tabla 3: Comparación de la retroalimentación de remuestreo utilizando muestreo de documentos (DS) con (QV) y sin (No QV) combinar modelos de retroalimentación de múltiples variantes de consulta.",
        "La Tabla 2 muestra los puntajes del Índice de Robustez para Base-FB y RS-FB.",
        "El método de retroalimentación RS-FB obtuvo una mayor robustez que Base-FB en tres de los cuatro conjuntos de temas, con solo un rendimiento ligeramente peor en TREC-8.",
        "Una vista más detallada que muestra la distribución de los cambios relativos en AP se presenta en el histograma de la Figura 4.",
        "En comparación con Base-FB, el método RS-FB logra una reducción notable en el número de consultas significativamente afectadas por la expansión (es decir, donde AP se ve afectado en un 25% o más), al tiempo que conserva ganancias positivas en AP. 3.5 Efecto de los métodos de muestreo de consultas y documentos Dada la mayor robustez de nuestros algoritmos observada en la Sección 3.4, una pregunta importante es qué componente de nuestro sistema es responsable.",
        "¿Es el uso de la re-muestreo de documentos, el uso de múltiples variantes de consulta, o algún otro factor?",
        "Los resultados en la Tabla 3 sugieren que la combinación de modelos basada en variantes de consulta puede ser en gran medida responsable de la mayor robustez.",
        "Cuando se desactivan las variantes de consulta y se utiliza la consulta original por sí sola con muestreo de documentos, hay poco cambio neto en la precisión promedio, una pequeña disminución en P10 para 3 de los 4 conjuntos de temas, pero una caída significativa en la robustez para todos los conjuntos de temas.",
        "En dos casos, la medida de RI disminuye en más del 50%.",
        "También examinamos el efecto del método de muestreo de documentos en la efectividad de la recuperación, utilizando dos estrategias diferentes.",
        "La estrategia de ponderación uniforme ignoró las puntuaciones de relevancia de la recuperación inicial y otorgó a cada documento en los primeros k la misma probabilidad de selección.",
        "Por el contrario, la estrategia de ponderación de puntajes de relevancia seleccionó documentos con una probabilidad proporcional a sus puntajes de relevancia.",
        "De esta manera, los documentos que tenían una clasificación más alta tenían más probabilidades de ser seleccionados.",
        "Los resultados se muestran en la Tabla 4.",
        "La estrategia de ponderación de la puntuación de relevancia tiene un mejor rendimiento en general, con puntajes de RI y P10 significativamente más altos en 3 de los 4 conjuntos de temas.",
        "La diferencia en la precisión promedio entre los métodos, sin embargo, es menos marcada.",
        "Esto sugiere que el peso uniforme actúa para aumentar la varianza en los resultados de recuperación: cuando la precisión promedio inicial es alta, hay muchos documentos relevantes en los primeros k y el muestreo uniforme puede proporcionar un modelo de relevancia más representativo que centrarse en los elementos altamente clasificados.",
        "Por otro lado, cuando la precisión inicial es baja, hay pocos documentos relevantes en las posiciones inferiores y el muestreo uniforme mezcla más de los documentos no relevantes.",
        "Por razones de espacio, solo resumimos nuestros hallazgos sobre el tamaño de la muestra aquí.",
        "El número de muestras tiene cierto efecto en la precisión cuando es menor a 10, pero el rendimiento se estabiliza alrededor de 15 a 20 muestras.",
        "Utilizamos 30 muestras para nuestros experimentos.",
        "Mucho más allá de este nivel, los beneficios adicionales de más muestras disminuyen a medida que la distribución inicial de puntajes se ajusta más estrechamente y aumenta el tiempo de procesamiento. 3.6 El efecto del remuestreo en la calidad del término de expansión Idealmente, un modelo de recuperación no debería requerir una lista de palabras vacías al estimar un modelo de relevancia: un modelo estadístico robusto debería reducir automáticamente el peso de las palabras vacías dependiendo del contexto.",
        "Las palabras vacías pueden perjudicar la retroalimentación si se seleccionan como términos de retroalimentación, ya que suelen ser malos discriminadores y desperdician espacios de términos valiosos.",
        "En la práctica, sin embargo, debido a que la mayoría de los métodos de selección de términos se asemejan a un tipo de ponderación tf · idf, a veces se pueden seleccionar términos con un idf bajo pero un tf muy alto como candidatos de términos de expansión.",
        "Esto sucede, por ejemplo, incluso con el enfoque del Modelo de Relevancia que forma parte de nuestra retroalimentación base.",
        "Para garantizar una línea base lo más sólida posible, utilizamos una lista de paradas para todos los experimentos reportados aquí.",
        "Si desactivamos la lista de palabras vacías, sin embargo, obtenemos resultados como los mostrados en la Tabla 5 donde cuatro de los diez términos de retroalimentación básica principales para el tema 60 de TREC (dijo, pero, su, no) son palabras vacías utilizando el método BaseFB. (Se seleccionaron los 100 términos de expansión principales para generar este ejemplo).",
        "El método Indris intenta abordar el problema de las palabras vacías aplicando un paso inicial basado en Ponte [14] para seleccionar términos menos comunes que tienen altas probabilidades logarítmicas de estar en los documentos mejor clasificados en comparación con toda la colección.",
        "Sin embargo, esto no resuelve completamente el problema de las palabras vacías, especialmente a medida que aumenta el número de términos de retroalimentación.",
        "Sin embargo, el uso de retroalimentación de remuestreo parece mitigar la ponderación de la colección QV + QV uniforme + ponderación de puntaje de relevancia TREC 1&2 AvgP 0.2545 0.2406 (-5.46%) P10 0.5369 0.5263 (-1.97%) RI 0.6212 0.7087 (+14.09%) TREC 7 AvgP 0.2174 0.2169 (-0.23%) P10 0.4320 0.4480 (+3.70%) RI 0.4783 0.5652 (+18.17%) TREC 8 AvgP 0.2267 0.2268 (+0.04%) P10 0.4120 0.4340 (+5.34%) RI 0.4545 0.4545 (+0.00%) wt10g AvgP 0.1808 0.1946 (+7.63%) P10 0.2680 0.2960 (+10.45%) RI 0.0220 0.1099 (+399.5%) Tabla 4: Comparación de muestreo de documentos uniforme y ponderado por relevancia.",
        "El cambio porcentual en comparación con el muestreo uniforme se muestra entre paréntesis.",
        "QV indica que se utilizaron variantes de consulta en ambas ejecuciones.",
        "FB de línea base p(wi|R) FB de remuestreo p(wi|R) dijo 0.055 corte 0.026 corte 0.055 pagar 0.018 pagar 0.034 federal 0.012 pero 0.026 educación 0.011 empleados 0.024 maestros 0.010 sus 0.024 empleados 0.010 no 0.023 caso 0.010 federal 0.021 sus 0.009 trabajadores 0.020 apelaciones 0.008 educación 0.020 sindicato 0.007 Tabla 5: Calidad del término de retroalimentación cuando no se utiliza una lista de paradas.",
        "Términos de retroalimentación para el tema 60 de TREC: pago por mérito vs antigüedad. el efecto de las palabras vacías automáticamente.",
        "En el ejemplo de la Tabla 5, la retroalimentación de remuestreo deja solo una palabra vacía (su) en las diez primeras posiciones.",
        "Observamos un comportamiento similar del término de retroalimentación en muchos otros temas.",
        "La razón de este efecto parece ser la interacción de la puntuación de selección de términos con el límite superior de términos m.",
        "Si bien la presencia e incluso la proporción de palabras vacías particulares es bastante estable en diferentes muestras de documentos, su posición relativa en la lista de los primeros m elementos no lo es, ya que se examinan conjuntos de documentos con diferentes números de candidatos de términos mejores y de menor frecuencia para cada muestra.",
        "Como resultado, si bien algunos stopwords pueden aparecer en cada conjunto de documentos muestreados, tiende a ocurrir que cualquier stopword dado caiga por debajo del umbral para múltiples muestras, lo que lleva a su clasificación como una característica de alta varianza y bajo peso. 4.",
        "TRABAJO RELACIONADO Nuestro enfoque está relacionado con trabajos previos de varias áreas de recuperación de información y aprendizaje automático.",
        "Nuestro uso de la variación de consultas fue inspirado por el trabajo de YomTov et al. [20], Carpineto et al. [5] y Amati et al. [2], entre otros.",
        "Estos estudios utilizan la idea de crear múltiples subconsultas y luego examinar la naturaleza de la superposición en los documentos y/o términos de expansión que resultan de cada subconsulta.",
        "La combinación de modelos se realiza utilizando heurísticas.",
        "En particular, los estudios de Amati et al. y Carpineto et al. investigaron la combinación de términos de métodos de distribución individuales utilizando una heurística de combinación de reordenamiento de términos.",
        "En un conjunto de temas de TREC encontraron una amplia variación promedio en la distancia de rango de los términos de diferentes métodos de expansión.",
        "Su método de combinación proporcionó modestas mejoras positivas en la precisión promedio.",
        "La idea de examinar la superposición entre listas de términos sugeridos también ha sido utilizada en enfoques tempranos de expansión de consultas.",
        "El método de Análisis de Contexto Local (LCA) de Xu y Crofts [19] incluye un factor en la fórmula de ponderación derivada empíricamente que hace que se prefieran los términos de expansión que tienen conexiones con múltiples términos de consulta.",
        "En el lado del documento, el trabajo reciente de Zhou & Croft [21] exploró la idea de agregar ruido a los documentos, volver a puntuarlos y utilizar la estabilidad de las clasificaciones resultantes como una estimación de la dificultad de la consulta.",
        "Esto está relacionado con nuestro uso de muestreo de documentos para estimar el riesgo del modelo de retroalimentación construido a partir de los diferentes conjuntos de documentos recuperados en la parte superior.",
        "Sakai et al. [17] propusieron un enfoque para mejorar la robustez de la retroalimentación de relevancia pseudo utilizando un método que llaman muestreo selectivo.",
        "La esencia de su método es que permiten omitir algunos documentos de alta clasificación, basándose en un criterio de agrupación, para seleccionar un conjunto de documentos más variado e innovador más adelante en la clasificación para su uso por un método tradicional de retroalimentación pseudo.",
        "Su estudio no encontró mejoras significativas ni en robustez (RI) ni en MAP en sus corpora.",
        "Greiff, Morgan y Ponte [8] exploraron el papel de la varianza en la ponderación de términos.",
        "En una serie de simulaciones que simplificaron el problema a documentos de 2 características, encontraron que la precisión promedio disminuye a medida que la varianza de la frecuencia de términos - ruido alto - aumenta.",
        "La reducción del peso de los términos con alta varianza resultó en una mejora de la precisión promedio.",
        "Esto parece estar en concordancia con nuestros propios hallazgos para los modelos de retroalimentación individual.",
        "Las estimaciones de la varianza de la producción han sido utilizadas recientemente para mejorar la clasificación de textos.",
        "Lee et al. [11] utilizaron estimaciones de varianza específicas de la consulta de las salidas del clasificador para realizar una combinación de modelos mejorada.",
        "En lugar de utilizar muestreo, pudieron derivar expresiones en forma cerrada para la varianza del clasificador asumiendo clasificadores base utilizando tipos simples de redes de inferencia.",
        "Ando y Zhang propusieron un método que ellos llaman retroalimentación estructural [3] y mostraron cómo aplicarlo a la expansión de consultas para la pista de Genómica de TREC.",
        "Utilizaron variaciones de consultas r para obtener R conjuntos diferentes Sr de documentos mejor clasificados que se han intersectado con los documentos mejor clasificados obtenidos de la consulta original qorig.",
        "Para cada Si, se calcula el vector centróide normalizado ˆwi de los documentos.",
        "El análisis de componentes principales (PCA) se aplica luego a los ˆwi para obtener la matriz Φ de los vectores singulares izquierdos φh que se utilizan para obtener la nueva consulta expandida qexp = qorig + ΦT Φqorig. (7) En el caso de H = 1, tenemos un único vector singular izquierdo φ: qexp = qorig + (φT qorig)φ, de modo que el producto punto φT qorig es un tipo de peso dinámico en la consulta expandida que se basa en la similitud de la consulta original con la consulta expandida.",
        "El uso de la varianza como medida de calidad del modelo de retroalimentación ocurre de forma indirecta a través de la aplicación de PCA.",
        "Sería interesante estudiar las conexiones entre este enfoque y nuestro propio método de ajuste de modelos.",
        "Finalmente, en los enfoques de modelado del lenguaje para retroalimentación, Tao y Zhai [18] describen un método para una retroalimentación más robusta que permite que cada documento tenga un α de retroalimentación diferente.",
        "Los pesos de retroalimentación se derivan automáticamente utilizando EM regularizado.",
        "La condición de parada de EM implica un equilibrio aproximadamente igual entre el modelo de consulta y expansión.",
        "Proponen adaptar el parámetro de parada η basado en una función de alguna medida de calidad de los documentos de retroalimentación.",
        "CONCLUSIONES Hemos presentado un nuevo enfoque para la retroalimentación de relevancia pseudo basado en el muestreo de documentos y consultas.",
        "El uso del muestreo es un dispositivo muy flexible y poderoso, motivado por nuestro deseo general de ampliar los modelos actuales de recuperación mediante la estimación del riesgo o la varianza asociada con los parámetros o la salida de los procesos de recuperación.",
        "Tales estimaciones de varianza, por ejemplo, pueden ser utilizadas de forma natural en un marco bayesiano para una mejor estimación y combinación de modelos.",
        "Aplicaciones como la expansión selectiva pueden ser implementadas de manera fundamentada.",
        "Si bien nuestro estudio utiliza el enfoque de modelado del lenguaje como marco para experimentos, hacemos pocas suposiciones sobre el funcionamiento real del algoritmo de retroalimentación.",
        "Creemos que es probable que cualquier algoritmo de retroalimentación de línea base razonablemente efectivo se beneficiaría de nuestro enfoque.",
        "Nuestros resultados en colecciones estándar de TREC muestran que nuestro marco mejora la robustez de un método de retroalimentación de referencia sólido en una variedad de colecciones, sin sacrificar la precisión promedio.",
        "También proporciona pequeñas pero consistentes mejoras en la precisión del top 10.",
        "En trabajos futuros, visualizamos una investigación sobre cómo variar el conjunto de métodos de muestreo utilizados y el número de muestras controla el equilibrio entre robustez, precisión y eficiencia.",
        "Agradecimientos Agradecemos a Paul Bennett por las valiosas discusiones relacionadas con este trabajo, el cual fue apoyado por las becas de la NSF #IIS-0534345 y #CNS-0454018, y la beca del Departamento de Educación de los EE. UU. #R305G03123.",
        "Cualquier opinión, hallazgo, conclusión o recomendación expresada en este material son del autor y no necesariamente reflejan las de los patrocinadores.",
        "REFERENCIAS [1] El kit de herramientas Lemur para modelado de lenguaje y recuperación. http://www.lemurproject.org. [2] G. Amati, C. Carpineto y G. Romano.",
        "Dificultad de la consulta, robustez y aplicación selectiva de la expansión de consultas.",
        "En Actas de la 25ª Conferencia Europea sobre Recuperación de Información (ECIR 2004), páginas 127-137. [3] R. K. Ando y T. Zhang.",
        "Un método de aprendizaje semisupervisado de alto rendimiento para segmentación de texto.",
        "En Actas de la 43ª Reunión Anual de la ACL, páginas 1-9, junio de 2005. [4] L. Breiman.",
        "Empaquetando predictores.",
        "Aprendizaje automático, 24(2):123-140, 1996. [5] C. Carpineto, G. Romano y V. Giannini.",
        "Mejorando la retroalimentación de recuperación con la combinación de múltiples funciones de clasificación de términos.",
        "ACM Trans.",
        "Información.",
        "Sistemas, 20(3):259 - 290. [6] K. Collins-Thompson, P. Ogilvie y J. Callan.",
        "Resultados iniciales con consultas estructuradas y modelos de lenguaje en medio terabyte de texto.",
        "En Actas de la Conferencia de Recuperación de Información de Texto de 2005.",
        "Publicación Especial del NIST. [7] R. O. Duda, P. E. Hart y D. G. Stork.",
        "Clasificación de patrones.",
        "Wiley and Sons, 2da edición, 2001. [8] W. R. Greiff, W. T. Morgan y J. M. Ponte.",
        "El papel de la varianza en la ponderación de términos para la recuperación de información probabilística.",
        "En Actas de la 11ª Conferencia Internacional.",
        "Conf. sobre Gestión de Información y Conocimiento (CIKM 2002), páginas 252-259. [9] T. Kohonen, J. Hynninen, J. Kangas y J. Laaksonen.",
        "SOMPAK: El paquete de programas de mapas autoorganizados.",
        "Informe técnico A31, Universidad de Tecnología de Helsinki, 1996. http://www.cis.hut.fi/research/papers/som tr96.ps.Z. [10] V. Lavrenko.",
        "Una teoría generativa de relevancia.",
        "Tesis doctoral, Universidad de Massachusetts, Amherst, 2004. [11] C.-H. Lee, R. Greiner y S. Wang.",
        "Utilizando estimaciones de varianza específicas de la consulta para combinar clasificadores bayesianos.",
        "En Actas del 23º Congreso Internacional.",
        "Conf. sobre Aprendizaje Automático (ICML 2006), páginas 529-536. [12] D. Metzler y W. B. Croft.",
        "Combinando el modelo de lenguaje y los enfoques de red de inferencia para la recuperación.",
        "Información.",
        "Procesamiento y Gestión, 40(5):735-750, 2004. [13] T. Minka.",
        "Estimando una distribución de Dirichlet.",
        "Informe técnico, 2000. http://research.microsoft.com/minka/papers/dirichlet. [14] J. Ponte.",
        "Avances en la Recuperación de Información, capítulo Modelos de lenguaje para retroalimentación de relevancia, páginas 73-96. 2000.",
        "W.B.",
        "Croft, ed. [15] J. M. Ponte y W. B. Croft.",
        "Un enfoque de modelado del lenguaje para la recuperación de información.",
        "En Actas de la Conferencia ACM SIGIR de 1998 sobre Investigación y Desarrollo en Recuperación de Información, páginas 275-281. [16] J. Rocchio.",
        "El Sistema de Recuperación SMART, capítulo Retroalimentación de Relevancia en la Recuperación de Información, páginas 313-323.",
        "Prentice-Hall, 1971.",
        "G. Salton, ed. [17] T. Sakai, T. Manabe y M. Koyama.",
        "Retroalimentación de pseudo-relevancia flexible a través de muestreo selectivo.",
        "ACM Transactions on Asian Language Information Processing (TALIP), 4(2):111-135, 2005. [18] T. Tao y C. Zhai.",
        "Estimación regularizada de modelos de mezcla para retroalimentación de pseudo relevancia robusta.",
        "En Actas de la Conferencia ACM SIGIR 2006 sobre Investigación y Desarrollo en Recuperación de Información, páginas 162-169. [19] J. Xu y W. B. Croft.",
        "Mejorando la efectividad de la recuperación de información con análisis de contexto local.",
        "ACM Trans.",
        "I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate to Spanish?",
        "Syst., 18(1):79-112, 2000. [20] E. YomTov, S. Fine, D. Carmel, y A. Darlow.",
        "Aprendiendo a estimar la dificultad de la consulta.",
        "En Actas de la Conferencia ACM SIGIR 2005 sobre Investigación y Desarrollo en Recuperación de Información, páginas 512-519. [21] Y. Zhou y W. B. Croft.",
        "Robustez del ranking: un nuevo marco para predecir el rendimiento de la consulta.",
        "En Actas de la 15ª Conferencia Internacional de la ACM.",
        "Conferencia sobre Gestión de Información y Conocimiento (CIKM 2006), páginas 567-574."
    ],
    "error_count": 3,
    "keys": {
        "feedback method": {
            "translated_key": "método de retroalimentación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Estimation and Use of Uncertainty in Pseudo-relevance Feedback Kevyn Collins-Thompson and Jamie Callan Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213-8213 U.S.A. {kct | callan}@cs.cmu.edu ABSTRACT Existing pseudo-relevance feedback methods typically perform averaging over the top-retrieved documents, but ignore an important statistical dimension: the risk or variance associated with either the individual document models, or their combination.",
                "Treating the baseline <br>feedback method</br> as a black box, and the output feedback model as a random variable, we estimate a posterior distribution for the feedback model by resampling a given querys top-retrieved documents, using the posterior mean or mode as the enhanced feedback model.",
                "We then perform model combination over several enhanced models, each based on a slightly modified query sampled from the original query.",
                "We find that resampling documents helps increase individual feedback model precision by removing noise terms, while sampling from the query improves robustness (worst-case performance) by emphasizing terms related to multiple query aspects.",
                "The result is a meta-feedback algorithm that is both more robust and more precise than the original strong baseline method.",
                "Categories and Subject Descriptors: H.3.3 [Information Retrieval]: Retrieval Models General Terms: Algorithms, Experimentation 1.",
                "INTRODUCTION Uncertainty is an inherent feature of information retrieval.",
                "Not only do we not know the queries that will be presented to our retrieval algorithm ahead of time, but the users information need may be vague or incompletely specified by these queries.",
                "Even if the query were perfectly specified, language in the collection documents is inherently complex and ambiguous and matching such language effectively is a formidable problem by itself.",
                "With this in mind, we wish to treat many important quantities calculated by the retrieval system, whether a relevance score for a document, or a weight for a query expansion term, as random variables whose true value is uncertain but where the uncertainty about the true value may be quantified by replacing the fixed value with a probability distribution over possible values.",
                "In this way, retrieval algorithms may attempt to quantify the risk or uncertainty associated with their output rankings, or improve the stability or precision of their internal calculations.",
                "Current algorithms for pseudo-relevance feedback (PRF) tend to follow the same basic method whether we use vector space-based algorithms such as Rocchios formula [16], or more recent language modeling approaches such as Relevance Models [10].",
                "First, a set of top-retrieved documents is obtained from an initial query and assumed to approximate a set of relevant documents.",
                "Next, a single feedback model vector is computed according to some sort of average, centroid, or expectation over the set of possibly-relevant document models.",
                "For example, the document vectors may be combined with equal weighting, as in Rocchio, or by query likelihood, as may be done using the Relevance Model1 .",
                "The use of an expectation is reasonable for practical and theoretical reasons, but by itself ignores potentially valuable information about the risk of the feedback model.",
                "Our main hypothesis in this paper is that estimating the uncertainty in feedback is useful and leads to better individual feedback models and more robust combined models.",
                "Therefore, we propose a method for estimating uncertainty associated with an individual feedback model in terms of a posterior distribution over language models.",
                "To do this, we systematically vary the inputs to the baseline <br>feedback method</br> and fit a Dirichlet distribution to the output.",
                "We use the posterior mean or mode as the improved feedback model estimate.",
                "This process is shown in Figure 1.",
                "As we show later, the mean and mode may vary significantly from the single feedback model proposed by the baseline method.",
                "We also perform model combination using several improved feedback language models obtained by a small number of new queries sampled from the original query.",
                "A models weight combines two complementary factors: the models probability of generating the query, and the variance of the model, with high-variance models getting lower weight. 1 For example, an expected parameter vector conditioned on the query observation is formed from top-retrieved documents, which are treated as training strings (see [10], p. 62).",
                "Figure 1: Estimating the uncertainty of the feedback model for a single query. 2.",
                "SAMPLING-BASED FEEDBACK In Sections 2.1-2.5 we describe a general method for estimating a probability distribution over the set of possible language models.",
                "In Sections 2.6 and 2.7 we summarize how different query samples are used to generate multiple feedback models, which are then combined. 2.1 Modeling Feedback Uncertainty Given a query Q and a collection C, we assume a probabilistic retrieval system that assigns a real-valued document score f(D, Q) to each document D in C, such that the score is proportional to the estimated probability of relevance.",
                "We make no other assumptions about f(D, Q).",
                "The nature of f(D, Q) may be complex: for example, if the retrieval system supports structured query languages [12], then f(D, Q) may represent the output of an arbitrarily complex inference network defined by the structured query operators.",
                "In theory, the scoring function can vary from query to query, although in this study for simplicity we keep the scoring function the same for all queries.",
                "Our specific query method is given in Section 3.",
                "We treat the feedback algorithm as a black box and assume that the inputs to the feedback algorithm are the original query and the corresponding top-retrieved documents, with a score being given to each document.",
                "We assume that the output of the feedback algorithm is a vector of term weights to be used to add or reweight the terms in the representation of the original query, with the vector normalized to form a probability distribution.",
                "We view the the inputs to the feedback black box as random variables, and analyze the feedback model as a random variable that changes in response to changes in the inputs.",
                "Like the document scoring function f(D, Q), the feedback algorithm may implement a complex, non-linear scoring formula, and so as its inputs vary, the resulting feedback models may have a complex distribution over the space of feedback models (the sample space).",
                "Because of this potential complexity, we do not attempt to derive a posterior distribution in closed form, but instead use simulation.",
                "We call this distribution over possible feedback models the feedback model distribution.",
                "Our goal in this section is to estimate a useful approximation to the feedback model distribution.",
                "For a specific framework for experiments, we use the language modeling (LM) approach for information retrieval [15].",
                "The score of a document D with respect to a query Q and collection C is given by p(Q|D) with respect to language models ˆθQ and ˆθD estimated for the query and document respectively.",
                "We denote the set of k top-retrieved documents from collection C in response to Q by DQ(k, C).",
                "For simplicity, we assume that queries and documents are generated by multinomial distributions whose parameters are represented by unigram language models.",
                "To incorporate feedback in the LM approach, we assume a model-based scheme in which our goal is take the query and resulting ranked documents DQ(k, C) as input, and output an expansion language model ˆθE, which is then interpolated with the original query model ˆθQ: ˆθNew = (1 − α) · ˆθQ + α · ˆθE (1) This includes the possibility of α = 1 where the original query mode is completely replaced by the feedback model.",
                "Our sample space is the set of all possible language models LF that may be output as feedback models.",
                "Our approach is to take samples from this space and then fit a distribution to the samples using maximum likelihood.",
                "For simplicity, we start by assuming the latent feedback distribution has the form of a Dirichlet distribution.",
                "Although the Dirichlet is a unimodal distribution, and in general quite limited in its expressiveness in the sample space, it is a natural match for the multinomial language model, can be estimated quickly, and can capture the most salient features of confident and uncertain feedback models, such as the overall spread of the distibution. 2.2 Resampling document models We would like an approximation to the posterior distribution of the feedback model LF .",
                "To accomplish this, we apply a widely-used simulation technique called bootstrap sampling ([7], p. 474) on the input parameters, namely, the set of top-retrieved documents.",
                "Bootstrap sampling allows us to simulate the approximate effect of perturbing the parameters within the black box feedback algorithm by perturbing the inputs to that algorithm in a systematic way, while making no assumptions about the nature of the feedback algorithm.",
                "Specifically, we sample k documents with replacement from DQ(k, C), and calculate an expansion language model θb using the black box <br>feedback method</br>.",
                "We repeat this process B times to obtain a set of B feedback language models, to which we then fit a Dirichlet distribution.",
                "Typically B is in the range of 20 to 50 samples, with performance being relatively stable in this range.",
                "Note that instead of treating each top document as equally likely, we sample according to the estimated probabilities of relevance of each document in DQ(k, C).",
                "Thus, a document is more likely to be chosen the higher it is in the ranking. 2.3 Justification for a sampling approach The rationale for our sampling approach has two parts.",
                "First, we want to improve the quality of individual feedback models by smoothing out variation when the baseline feedback model is unstable.",
                "In this respect, our approach resembles bagging [4], an ensemble approach which generates multiple versions of a predictor by making bootstrap copies of the training set, and then averages the (numerical) predictors.",
                "In our application, top-retrieved documents can be seen as a kind of noisy training set for relevance.",
                "Second, sampling is an effective way to estimate basic properties of the feedback posterior distribution, which can then be used for improved model combination.",
                "For example, a model may be weighted by its prediction confidence, estimated as a function of the variability of the posterior around the model. foo2-401.map-Dim:5434,Size:12*12units,gaussianneighborhood (a) Topic 401 Foreign minorities, Germany foo2-402.map-Dim:5698,Size:12*12units,gaussianneighborhood (b) Topic 402 Behavioral genetics foo2-459.map-Dim:8969,Size:12*12units,gaussianneighborhood (c) Topic 459 When can a lender foreclose on property Figure 2: Visualization of expansion language model variance using self-organizing maps, showing the distribution of language models that results from resampling the inputs to the baseline expansion method.",
                "The language model that would have been chosen by the baseline expansion is at the center of each map.",
                "The similarity function is JensenShannon divergence. 2.4 Visualizing feedback distributions Before describing how we fit and use the Dirichlet distribution over feedback models, it is instructive to view some examples of actual feedback model distributions that result from bootstrap sampling the top-retrieved documents from different TREC topics.",
                "Each point in our sample space is a language model, which typically has several thousand dimensions.",
                "To help analyze the behavior of our method we used a Self-Organizing Map (via the SOM-PAK package [9]), to flatten and visualize the high-dimensional density function2 .",
                "The density maps for three TREC topics are shown in Figure 2 above.",
                "The dark areas represent regions of high similarity between language models.",
                "The light areas represent regions of low similarity - the valleys between clusters.",
                "Each diagram is centered on the language model that would have been chosen by the baseline expansion.",
                "A single peak (mode) is evident in some examples, but more complex structure appears in others.",
                "Also, while the distribution is usually close to the baseline feedback model, for some topics they are a significant distance apart (as measured by JensenShannon divergence), as in Subfigure 2c.",
                "In such cases, the mode or mean of the feedback distribution often performs significantly better than the baseline (and in a smaller proportion of cases, significantly worse). 2.5 Fitting a posterior feedback distribution After obtaining feedback model samples by resampling the feedback model inputs, we estimate the feedback distribution.",
                "We assume that the multinomial feedback models {ˆθ1, . . . , ˆθB} were generated by a latent Dirichlet distribution with parameters {α1, . . . , αN }.",
                "To estimate the {α1, . . . , αN }, we fit the Dirichlet parameters to the B language model samples according to maximum likelihood using a generalized Newton procedure, details of which are given in Minka [13].",
                "We assume a simple Dirichlet prior over the {α1, . . . , αN }, setting each to αi = μ · p(wi | C), where μ is a parameter and p(· | C) is the collection language model estimated from a set of documents from collection C. The parameter fitting converges very quickly - typically just 2 or 2 Because our points are language models in the multinomial simplex, we extended SOM-PAK to support JensenShannon divergence, a widely-used similarity measure between probability distributions. 3 iterations are enough - so that it is practical to apply at query-time when computational overhead must be small.",
                "In practice, we can restrict the calculation to the vocabulary of the top-retrieved documents, instead of the entire collection.",
                "Note that for this step we are re-using the existing retrieved documents and not performing additional queries.",
                "Given the parameters of an N-dimensional Dirichlet distribution Dir(α) the mean μ and mode x vectors are easy to calculate and are given respectively by μi = αiP αi (2) and xi = αi−1P αi−N . (3) We can then choose the language model at the mean or the mode of the posterior as the final enhanced feedback model. (We found the mode to give slightly better performance.)",
                "For information retrieval, the number of samples we will have available is likely to be quite small for performance reasons - usually less than ten.",
                "Moreover, while random sampling is useful in certain cases, it is perfectly acceptable to allow deterministic sampling distributions, but these must be designed carefully in order to approximate an accurate output variance.",
                "We leave this for future study. 2.6 Query variants We use the following methods for generating variants of the original query.",
                "Each variant corresponds to a different assumption about which aspects of the original query may be important.",
                "This is a form of deterministic sampling.",
                "We selected three simple methods that cover complimentary assumptions about the query.",
                "No-expansion Use only the original query.",
                "The assumption is that the given terms are a complete description of the information need.",
                "Leave-one-out A single term is left out of the original query.",
                "The assumption is that one of the query terms is a noise term.",
                "Single-term A single term is chosen from the original query.",
                "This assumes that only one aspect of the query, namely, that represented by the term, is most important.",
                "After generating a variant of the original query, we combine it with the original query using a weight αSUB so that we do not stray too far.",
                "In this study, we set αSUB = 0.5.",
                "For example, using the Indri [12] query language, a leave-oneout variant of the initial query that omits the term ireland for TREC topic 404 is: #weight(0.5 #combine(ireland peace talks) 0.5 #combine(peace talks)) 2.7 Combining enhanced feedback models from multiple query variants When using multiple query variants, the resulting enhanced feedback models are combined using Bayesian model combination.",
                "To do this, we treat each word as an item to be classified as belonging to a relevant or non-relevant class, and derive a class probability for each word by combining the scores from each query variant.",
                "Each score is given by that terms probability in the Dirichlet distribution.",
                "The term scores are weighted by the inverse of the variance of the term in the enhanced feedback models Dirichlet distribution.",
                "The prior probability of a words membership in the relevant class is given by the probability of the original query in the entire enhanced expansion model. 3.",
                "EVALUATION In this section we present results confirming the usefulness of estimating a feedback model distribution from weighted resampling of top-ranked documents, and of combining the feedback models obtained from different small changes in the original query. 3.1 General method We evaluated performance on a total of 350 queries derived from four sets of TREC topics: 51-200 (TREC-1&2), 351-400 (TREC-7), 401-450 (TREC-8), and 451-550 (wt10g, TREC-9&10).",
                "We chose these for their varied content and document properties.",
                "For example, wt10g documents are Web pages with a wide variety of subjects and styles while TREC-1&2 documents are more homogeneous news articles.",
                "Indexing and retrieval was performed using the Indri system in the Lemur toolkit [12] [1].",
                "Our queries were derived from the words in the title field of the TREC topics.",
                "Phrases were not used.",
                "To generate the baseline queries passed to Indri, we wrapped the query terms with Indris #combine operator.",
                "For example, the initial query for topic 404 is: #combine(ireland peace talks) We performed Krovetz stemming for all experiments.",
                "Because we found that the baseline (Indri) expansion method performed better using a stopword list with the feedback model, all experiments used a stoplist of 419 common English words.",
                "However, an interesting side-effect of our resampling approach is that it tends to remove many stopwords from the feedback model, making a stoplist less critical.",
                "This is discussed further in Section 3.6. 3.2 Baseline <br>feedback method</br> For our baseline expansion method, we use an algorithm included in Indri 1.0 as the default expansion method.",
                "This method first selects terms using a log-odds calculation described by Ponte [14], but assigns final term weights using Lavrenkos relevance model[10].",
                "We chose the Indri method because it gives a consistently strong baseline, is based on a language modeling approach, and is simple to experiment with.",
                "In a TREC evaluation using the GOV2 corpus [6], the method was one of the topperforming runs, achieving a 19.8% gain in MAP compared to using unexpanded queries.",
                "In this study, it achieves an average gain in MAP of 17.25% over the four collections.",
                "Indris expansion method first calculates a log-odds ratio o(v) for each potential expansion term v given by o(v) = X D log p(v|D) p(v|C) (4) over all documents D containing v, in collection C. Then, the expansion term candidates are sorted by descending o(v), and the top m are chosen.",
                "Finally, the term weights r(v) used in the expanded query are calculated based on the relevance model r(v) = X D p(q|D)p(v|D) p(v) p(D) (5) The quantity p(q|D) is the probability score assigned to the document in the initial retrieval.",
                "We use Dirichlet smoothing of p(v|D) with μ = 1000.",
                "This relevance model is then combined with the original query using linear interpolation, weighted by a parameter α.",
                "By default we used the top 50 documents for feedback and the top 20 expansion terms, with the feedback interpolation parameter α = 0.5 unless otherwise stated.",
                "For example, the baseline expanded query for topic 404 is: #weight(0.5 #combine(ireland peace talks) 0.5 #weight(0.10 ireland 0.08 peace 0.08 northern ...) 3.3 Expansion performance We measure our feedback algorithms effectiveness by two main criteria: precision, and robustness.",
                "Robustness, and the tradeoff between precision and robustness, is analyzed in Section 3.4.",
                "In this section, we examine average precision and precision in the top 10 documents (P10).",
                "We also include recall at 1,000 documents.",
                "For each query, we obtained a set of B feedback models using the Indri baseline.",
                "Each feedback model was obtained from a random sample of the top k documents taken with replacement.",
                "For these experiments, B = 30 and k = 50.",
                "Each feedback model contained 20 terms.",
                "On the query side, we used leave-one-out (LOO) sampling to create the query variants.",
                "Single-term query sampling had consistently worse performance across all collections and so our results here focus on LOO sampling.",
                "We used the methods described in Section 2 to estimate an enhanced feedback model from the Dirichlet posterior distribution for each query variant, and to combine the feedback models from all the query variants.",
                "We call our method resampling expansion and denote it as RS-FB here.",
                "We denote the Indri baseline <br>feedback method</br> as Base-FB.",
                "Results from applying both the baseline expansion method (Base-FB) and resampling expansion (RS-FB) are shown in Table 1.",
                "We observe several trends in this table.",
                "First, the average precision of RS-FB was comparable to Base-FB, achieving an average gain of 17.6% compared to using no expansion across the four collections.",
                "The Indri baseline expansion gain was 17.25%.",
                "Also, the RS-FB method achieved consistent improvements in P10 over Base-FB for every topic set, with an average improvement of 6.89% over Base-FB for all 350 topics.",
                "The lowest P10 gain over Base-FB was +3.82% for TREC-7 and the highest was +11.95% for wt10g.",
                "Finally, both Base-FB and RS-FB also consistently improved recall over using no expansion, with Base-FB achieving better recall than RS-FB for all topic sets. 3.4 Retrieval robustness We use the term robustness to mean the worst-case average precision performance of a feedback algorithm.",
                "Ideally, a robust <br>feedback method</br> would never perform worse than using the original query, while often performing better using the expansion.",
                "To evaluate robustness in this study, we use a very simple measure called the robustness index (RI)3 .",
                "For a set of queries Q, the RI measure is defined as: RI(Q) = n+ − n− |Q| (6) where n+ is the number of queries helped by the <br>feedback method</br> and n− is the number of queries hurt.",
                "Here, by helped we mean obtaining a higher average precision as a result of feedback.",
                "The value of RI ranges from a minimum 3 This is sometimes also called the reliability of improvement index and was used in Sakai et al. [17].",
                "Collection NoExp Base-FB RS-FB TREC 1&2 AvgP 0.1818 0.2419 (+33.04%) 0.2406 (+32.24%) P10 0.4443 0.4913 (+10.57%) 0.5363 (+17.83%) Recall 15084/37393 19172/37393 15396/37393 TREC 7 AvgP 0.1890 0.2175 (+15.07%) 0.2169 (+14.75%) P10 0.4200 0.4320 (+2.85%) 0.4480 (+6.67%) Recall 2179/4674 2608/4674 2487/4674 TREC 8 AvgP 0.2031 0.2361 (+16.25%) 0.2268 (+11.70%) P10 0.3960 0.4160 (+5.05%) 0.4340 (+9.59%) Recall 2144/4728 2642/4728 2485/4728 wt10g AvgP 0.1741 0.1829 (+5.06%) 0.1946 (+11.78%) P10 0.2760 0.2630 (-4.71%) 0.2960 (+7.24%) Recall 3361/5980 3725/5980 3664/5980 Table 1: Comparison of baseline (Base-FB) feedback and feedback using re-sampling (RS-FB).",
                "Improvement shown for BaseFB and RS-FB is relative to using no expansion. (a) TREC 1&2 (upper curve); TREC 8 (lower curve) (b) TREC 7 (upper curve); wt10g (lower curve) Figure 3: The trade-off between robustness and average precision for different corpora.",
                "The x-axis gives the change in MAP over using baseline expansion with α = 0.5.",
                "The yaxis gives the Robustness Index (RI).",
                "Each curve through uncircled points shows the RI/MAP tradeoff using the simple small-α strategy (see text) as α decreases from 0.5 to zero in the direction of the arrow.",
                "Circled points represent the tradeoffs obtained by resampling feedback for α = 0.5.",
                "Collection N Base-FB RS-FB n− RI n− RI TREC 1&2 103 26 +0.495 15 +0.709 TREC 7 46 14 +0.391 10 +0.565 TREC 8 44 12 +0.455 12 +0.455 wt10g 91 48 -0.055 39 +0.143 Combined 284 100 +0.296 76 +0.465 Table 2: Comparison of robustness index (RI) for baseline feedback (Base-FB) vs. resampling feedback (RS-FB).",
                "Also shown are the actual number of queries hurt by feedback (n−) for each method and collection.",
                "Queries for which initial average precision was negligible (≤ 0.01) were ignored, giving the remaining query count in column N. of −1.0, when all queries are hurt by the <br>feedback method</br>, to +1.0 when all queries are helped.",
                "The RI measure does not take into account the magnitude or distribution of the amount of change across the set Q.",
                "However, it is easy to understand as a general indication of robustness.",
                "One obvious way to improve the worst-case performance of feedback is simply to use a smaller fixed α interpolation parameter, such as α = 0.3, placing less weight on the (possibly risky) feedback model and more on the original query.",
                "We call this the small-α strategy.",
                "Since we are also reducing the potential gains when the feedback model is right, however, we would expect some trade-off between average precision and robustness.",
                "We therefore compared the precision/robustness trade-off between our resampling feedback algorithm, and the simple small-α method.",
                "The results are summarized in Figure 3.",
                "In the figure, the curve for each topic set interpolates between trade-off points, beginning at x=0, where α = 0.5, and continuing in the direction of the arrow as α decreases and the original query is given more and more weight.",
                "As expected, robustness continuously increases as we move along the curve, but mean average precision generally drops as the gains from feedback are eliminated.",
                "For comparison, the performance of resampling feedback at α = 0.5 is shown for each collection as the circled point.",
                "Higher and to the right is better.",
                "This figure shows that resampling feedback gives a somewhat better trade-off than the small-α approach for 3 of the 4 collections.",
                "Figure 4: Histogram showing improved robustness of resampling feedback (RS-FB) over baseline feedback (Base-FB) for all datasets combined.",
                "Queries are binned by % change in AP compared to the unexpanded query.",
                "Collection DS + QV DS + No QV TREC 1&2 AvgP 0.2406 0.2547 (+5.86%) P10 0.5263 0.5362 (+1.88%) RI 0.7087 0.6515 (-0.0572) TREC 7 AvgP 0.2169 0.2200 (+1.43%) P10 0.4480 0.4300 (-4.02%) RI 0.5652 0.2609 (-0.3043) TREC 8 AvgP 0.2268 0.2257 (-0.49%) P10 0.4340 0.4200 (-3.23%) RI 0.4545 0.4091 (-0.0454) wt10g AvgP 0.1946 0.1865 (-4.16%) P10 0.2960 0.2680 (-9.46%) RI 0.1429 0.0220 (-0.1209) Table 3: Comparison of resampling feedback using document sampling (DS) with (QV) and without (No QV) combining feedback models from multiple query variants.",
                "Table 2 gives the Robustness Index scores for Base-FB and RS-FB.",
                "The RS-FB <br>feedback method</br> obtained higher robustness than Base-FB on three of the four topic sets, with only slightly worse performance on TREC-8.",
                "A more detailed view showing the distribution over relative changes in AP is given by the histogram in Figure 4.",
                "Compared to Base-FB, the RS-FB method achieves a noticable reduction in the number of queries significantly hurt by expansion (i.e. where AP is hurt by 25% or more), while preserving positive gains in AP. 3.5 Effect of query and document sampling methods Given our algorithms improved robustness seen in Section 3.4, an important question is what component of our system is responsible.",
                "Is it the use of document re-sampling, the use of multiple query variants, or some other factor?",
                "The results in Table 3 suggest that the model combination based on query variants may be largely account for the improved robustness.",
                "When query variants are turned off and the original query is used by itself with document sampling, there is little net change in average precision, a small decrease in P10 for 3 out of the 4 topic sets, but a significant drop in robustness for all topic sets.",
                "In two cases, the RI measure drops by more than 50%.",
                "We also examined the effect of the document sampling method on retrieval effectiveness, using two different strategies.",
                "The uniform weighting strategy ignored the relevance scores from the initial retrieval and gave each document in the top k the same probability of selection.",
                "In contrast, the relevance-score weighting strategy chose documents with probability proportional to their relevance scores.",
                "In this way, documents that were more highly ranked were more likely to be selected.",
                "Results are shown in Table 4.",
                "The relevance-score weighting strategy performs better overall, with significantly higher RI and P10 scores on 3 of the 4 topic sets.",
                "The difference in average precision between the methods, however, is less marked.",
                "This suggests that uniform weighting acts to increase variance in retrieval results: when initial average precision is high, there are many relevant documents in the top k and uniform sampling may give a more representative relevance model than focusing on the highly-ranked items.",
                "On the other hand, when initial precision is low, there are few relevant documents in the bottom ranks and uniform sampling mixes in more of the non-relevant documents.",
                "For space reasons we only summarize our findings on sample size here.",
                "The number of samples has some effect on precision when less than 10, but performance stabilizes at around 15 to 20 samples.",
                "We used 30 samples for our experiments.",
                "Much beyond this level, the additional benefits of more samples decrease as the initial score distribution is more closely fit and the processing time increases. 3.6 The effect of resampling on expansion term quality Ideally, a retrieval model should not require a stopword list when estimating a model of relevance: a robust statistical model should down-weight stopwords automatically depending on context.",
                "Stopwords can harm feedback if selected as feedback terms, because they are typically poor discriminators and waste valuable term slots.",
                "In practice, however, because most term selection methods resemble a tf · idf type of weighting, terms with low idf but very high tf can sometimes be selected as expansion term candidates.",
                "This happens, for example, even with the Relevance Model approach that is part of our baseline feedback.",
                "To ensure as strong a baseline as possible, we use a stoplist for all experiments reported here.",
                "If we turn off the stopword list, however, we obtain results such as those shown in Table 5 where four of the top ten baseline feedback terms for TREC topic 60 (said, but, their, not) are stopwords using the BaseFB method. (The top 100 expansion terms were selected to generate this example.)",
                "Indris method attempts to address the stopword problem by applying an initial step based on Ponte [14] to select less-common terms that have high log-odds of being in the top-ranked documents compared to the whole collection.",
                "Nevertheless, this does not overcome the stopword problem completely, especially as the number of feedback terms grows.",
                "Using resampling feedback, however, appears to mitigate Collection QV + Uniform QV + Relevance-score weighting weighting TREC 1&2 AvgP 0.2545 0.2406 (-5.46%) P10 0.5369 0.5263 (-1.97%) RI 0.6212 0.7087 (+14.09%) TREC 7 AvgP 0.2174 0.2169 (-0.23%) P10 0.4320 0.4480 (+3.70%) RI 0.4783 0.5652 (+18.17%) TREC 8 AvgP 0.2267 0.2268 (+0.04%) P10 0.4120 0.4340 (+5.34%) RI 0.4545 0.4545 (+0.00%) wt10g AvgP 0.1808 0.1946 (+7.63%) P10 0.2680 0.2960 (+10.45%) RI 0.0220 0.1099 (+399.5%) Table 4: Comparison of uniform and relevance-weighted document sampling.",
                "The percentage change compared to uniform sampling is shown in parentheses.",
                "QV indicates that query variants were used in both runs.",
                "Baseline FB p(wi|R) Resampling FB p(wi|R) said 0.055 court 0.026 court 0.055 pay 0.018 pay 0.034 federal 0.012 but 0.026 education 0.011 employees 0.024 teachers 0.010 their 0.024 employees 0.010 not 0.023 case 0.010 federal 0.021 their 0.009 workers 0.020 appeals 0.008 education 0.020 union 0.007 Table 5: Feedback term quality when a stoplist is not used.",
                "Feedback terms for TREC topic 60: merit pay vs seniority. the effect of stopwords automatically.",
                "In the example of Table 5, resampling feedback leaves only one stopword (their) in the top ten.",
                "We observed similar feedback term behavior across many other topics.",
                "The reason for this effect appears to be the interaction of the term selection score with the top-m term cutoff.",
                "While the presence and even proportion of particular stopwords is fairly stable across different document samples, their relative position in the top-m list is not, as sets of documents with varying numbers of better, lower-frequency term candidates are examined for each sample.",
                "As a result, while some number of stopwords may appear in each sampled document set, any given stopword tends to fall below the cutoff for multiple samples, leading to its classification as a high-variance, low-weight feature. 4.",
                "RELATED WORK Our approach is related to previous work from several areas of information retrieval and machine learning.",
                "Our use of query variation was inspired by the work of YomTov et al. [20], Carpineto et al. [5], and Amati et al. [2], among others.",
                "These studies use the idea of creating multiple subqueries and then examining the nature of the overlap in the documents and/or expansion terms that result from each subquery.",
                "Model combination is performed using heuristics.",
                "In particular, the studies of Amati et al. and Carpineto et al. investigated combining terms from individual distributional methods using a term-reranking combination heuristic.",
                "In a set of TREC topics they found wide average variation in the rank-distance of terms from different expansion methods.",
                "Their combination method gave modest positive improvements in average precision.",
                "The idea of examining the overlap between lists of suggested terms has also been used in early query expansion approaches.",
                "Xu and Crofts method of Local Context Analysis (LCA) [19] includes a factor in the empirically-derived weighting formula that causes expansion terms to be preferred that have connections to multiple query terms.",
                "On the document side, recent work by Zhou & Croft [21] explored the idea of adding noise to documents, re-scoring them, and using the stability of the resulting rankings as an estimate of query difficulty.",
                "This is related to our use of document sampling to estimate the risk of the feedback model built from the different sets of top-retrieved documents.",
                "Sakai et al. [17] proposed an approach to improving the robustness of pseudo-relevance feedback using a method they call selective sampling.",
                "The essence of their method is that they allow skipping of some top-ranked documents, based on a clustering criterion, in order to select a more varied and novel set of documents later in the ranking for use by a traditional pseudo-<br>feedback method</br>.",
                "Their study did not find significant improvements in either robustness (RI) or MAP on their corpora.",
                "Greiff, Morgan and Ponte [8] explored the role of variance in term weighting.",
                "In a series of simulations that simplified the problem to 2-feature documents, they found that average precision degrades as term frequency variance - high noiseincreases.",
                "Downweighting terms with high variance resulted in improved average precision.",
                "This seems in accord with our own findings for individual feedback models.",
                "Estimates of output variance have recently been used for improved text classification.",
                "Lee et al. [11] used queryspecific variance estimates of classifier outputs to perform improved model combination.",
                "Instead of using sampling, they were able to derive closed-form expressions for classifier variance by assuming base classifiers using simple types of inference networks.",
                "Ando and Zhang proposed a method that they call structural feedback [3] and showed how to apply it to query expansion for the TREC Genomics Track.",
                "They used r query variations to obtain R different sets Sr of top-ranked documents that have been intersected with the top-ranked documents obtained from the original query qorig.",
                "For each Si, the normalized centroid vector ˆwi of the documents is calculated.",
                "Principal component analysis (PCA) is then applied to the ˆwi to obtain the matrix Φ of H left singular vectors φh that are used to obtain the new, expanded query qexp = qorig + ΦT Φqorig. (7) In the case H = 1, we have a single left singular vector φ: qexp = qorig + (φT qorig)φ so that the dot product φT qorig is a type of dynamic weight on the expanded query that is based on the similarity of the original query to the expanded query.",
                "The use of variance as a feedback model quality measure occurs indirectly through the application of PCA.",
                "It would be interesting to study the connections between this approach and our own modelfitting method.",
                "Finally, in language modeling approaches to feedback, Tao and Zhai [18] describe a method for more robust feedback that allows each document to have a different feedback α.",
                "The feedback weights are derived automatically using regularized EM.",
                "A roughly equal balance of query and expansion model is implied by their EM stopping condition.",
                "They propose tailoring the stopping parameter η based on a function of some quality measure of feedback documents. 5.",
                "CONCLUSIONS We have presented a new approach to pseudo-relevance feedback based on document and query sampling.",
                "The use of sampling is a very flexible and powerful device and is motivated by our general desire to extend current models of retrieval by estimating the risk or variance associated with the parameters or output of retrieval processes.",
                "Such variance estimates, for example, may be naturally used in a Bayesian framework for improved model estimation and combination.",
                "Applications such as selective expansion may then be implemented in a principled way.",
                "While our study uses the language modeling approach as a framework for experiments, we make few assumptions about the actual workings of the feedback algorithm.",
                "We believe it is likely that any reasonably effective baseline feedback algorithm would benefit from our approach.",
                "Our results on standard TREC collections show that our framework improves the robustness of a strong baseline <br>feedback method</br> across a variety of collections, without sacrificing average precision.",
                "It also gives small but consistent gains in top10 precision.",
                "In future work, we envision an investigation into how varying the set of sampling methods used and the number of samples controls the trade-off between robustness, accuracy, and efficiency.",
                "Acknowledgements We thank Paul Bennett for valuable discussions related to this work, which was supported by NSF grants #IIS-0534345 and #CNS-0454018, and U.S. Dept. of Education grant #R305G03123.",
                "Any opinions, findings, and conclusions or recommendations expressed in this material are the authors. and do not necessarily reflect those of the sponsors. 6.",
                "REFERENCES [1] The Lemur toolkit for language modeling and retrieval. http://www.lemurproject.org. [2] G. Amati, C. Carpineto, and G. Romano.",
                "Query difficulty, robustness, and selective application of query expansion.",
                "In Proc. of the 25th European Conf. on Information Retrieval (ECIR 2004), pages 127-137. [3] R. K. Ando and T. Zhang.",
                "A high-performance semi-supervised learning method for text chunking.",
                "In Proc. of the 43rd Annual Meeting of the ACL, pages 1-9, June 2005. [4] L. Breiman.",
                "Bagging predictors.",
                "Machine Learning, 24(2):123-140, 1996. [5] C. Carpineto, G. Romano, and V. Giannini.",
                "Improving retrieval feedback with multiple term-ranking function combination.",
                "ACM Trans.",
                "Info.",
                "Systems, 20(3):259 - 290. [6] K. Collins-Thompson, P. Ogilvie, and J. Callan.",
                "Initial results with structured queries and language models on half a terabyte of text.",
                "In Proc. of 2005 Text REtrieval Conference.",
                "NIST Special Publication. [7] R. O. Duda, P. E. Hart, and D. G. Stork.",
                "Pattern Classification.",
                "Wiley and Sons, 2nd edition, 2001. [8] W. R. Greiff, W. T. Morgan, and J. M. Ponte.",
                "The role of variance in term weighting for probabilistic information retrieval.",
                "In Proc. of the 11th Intl.",
                "Conf. on Info. and Knowledge Mgmt. (CIKM 2002), pages 252-259. [9] T. Kohonen, J. Hynninen, J. Kangas, and J. Laaksonen.",
                "SOMPAK: The self-organizing map program package.",
                "Technical Report A31, Helsinki University of Technology, 1996. http://www.cis.hut.fi/research/papers/som tr96.ps.Z. [10] V. Lavrenko.",
                "A Generative Theory of Relevance.",
                "PhD thesis, University of Massachusetts, Amherst, 2004. [11] C.-H. Lee, R. Greiner, and S. Wang.",
                "Using query-specific variance estimates to combine Bayesian classifiers.",
                "In Proc. of the 23rd Intl.",
                "Conf. on Machine Learning (ICML 2006), pages 529-536. [12] D. Metzler and W. B. Croft.",
                "Combining the language model and inference network approaches to retrieval.",
                "Info.",
                "Processing and Mgmt., 40(5):735-750, 2004. [13] T. Minka.",
                "Estimating a Dirichlet distribution.",
                "Technical report, 2000. http://research.microsoft.com/ minka/papers/dirichlet. [14] J. Ponte.",
                "Advances in Information Retrieval, chapter Language models for relevance feedback, pages 73-96. 2000.",
                "W.B.",
                "Croft, ed. [15] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In Proc. of the 1998 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 275-281. [16] J. Rocchio.",
                "The SMART Retrieval System, chapter Relevance Feedback in Information Retrieval, pages 313-323.",
                "Prentice-Hall, 1971.",
                "G. Salton, ed. [17] T. Sakai, T. Manabe, and M. Koyama.",
                "Flexible pseudo-relevance feedback via selective sampling.",
                "ACM Transactions on Asian Language Information Processing (TALIP), 4(2):111-135, 2005. [18] T. Tao and C. Zhai.",
                "Regularized estimation of mixture models for robust pseudo-relevance feedback.",
                "In Proc. of the 2006 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 162-169. [19] J. Xu and W. B. Croft.",
                "Improving the effectiveness of information retrieval with local context analysis.",
                "ACM Trans.",
                "Inf.",
                "Syst., 18(1):79-112, 2000. [20] E. YomTov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty.",
                "In Proc. of the 2005 ACM SIGIR Conf. on Research and Development in Information Retrieval, pages 512-519. [21] Y. Zhou and W. B. Croft.",
                "Ranking robustness: a novel framework to predict query performance.",
                "In Proc. of the 15th ACM Intl.",
                "Conf. on Information and Knowledge Mgmt. (CIKM 2006), pages 567-574."
            ],
            "original_annotated_samples": [
                "Treating the baseline <br>feedback method</br> as a black box, and the output feedback model as a random variable, we estimate a posterior distribution for the feedback model by resampling a given querys top-retrieved documents, using the posterior mean or mode as the enhanced feedback model.",
                "To do this, we systematically vary the inputs to the baseline <br>feedback method</br> and fit a Dirichlet distribution to the output.",
                "Specifically, we sample k documents with replacement from DQ(k, C), and calculate an expansion language model θb using the black box <br>feedback method</br>.",
                "This is discussed further in Section 3.6. 3.2 Baseline <br>feedback method</br> For our baseline expansion method, we use an algorithm included in Indri 1.0 as the default expansion method.",
                "We denote the Indri baseline <br>feedback method</br> as Base-FB."
            ],
            "translated_annotated_samples": [
                "Tratando el <br>método de retroalimentación</br> de línea base como una caja negra, y el modelo de retroalimentación de salida como una variable aleatoria, estimamos una distribución posterior para el modelo de retroalimentación mediante el remuestreo de los documentos mejor recuperados de una consulta dada, utilizando la media o la moda posterior como el modelo de retroalimentación mejorado.",
                "Para hacer esto, variamos sistemáticamente las entradas al <br>método de retroalimentación</br> base y ajustamos una distribución de Dirichlet a la salida.",
                "Específicamente, muestreamos k documentos con reemplazo de DQ(k, C), y calculamos un modelo de lenguaje de expansión θb utilizando el <br>método de retroalimentación</br> de caja negra.",
                "Esto se discute más a fondo en la Sección 3.6. Método de retroalimentación de línea base 3.2 Para nuestro método de expansión de línea base, utilizamos un algoritmo incluido en Indri 1.0 como el método de expansión predeterminado.",
                "Denominamos al <br>método de retroalimentación</br> de línea base Indri como Base-FB."
            ],
            "translated_text": "Estimación y Uso de la Incertidumbre en la Retroalimentación de Pseudo-Relevancia Kevyn Collins-Thompson y Jamie Callan Instituto de Tecnologías del Lenguaje Escuela de Ciencias de la Computación Universidad Carnegie Mellon Pittsburgh, PA 15213-8213 EE. UU. {kct | callan}@cs.cmu.edu RESUMEN Los métodos existentes de retroalimentación de pseudo-relevancia suelen realizar un promedio de los documentos mejor clasificados, pero ignoran una dimensión estadística importante: el riesgo o la varianza asociada tanto a los modelos de documentos individuales como a su combinación. Tratando el <br>método de retroalimentación</br> de línea base como una caja negra, y el modelo de retroalimentación de salida como una variable aleatoria, estimamos una distribución posterior para el modelo de retroalimentación mediante el remuestreo de los documentos mejor recuperados de una consulta dada, utilizando la media o la moda posterior como el modelo de retroalimentación mejorado. Luego realizamos la combinación de modelos sobre varios modelos mejorados, cada uno basado en una consulta ligeramente modificada muestreada de la consulta original. Observamos que el remuestreo de documentos ayuda a aumentar la precisión del modelo de retroalimentación individual al eliminar términos de ruido, mientras que el muestreo de la consulta mejora la robustez (rendimiento en el peor de los casos) al enfatizar términos relacionados con múltiples aspectos de la consulta. El resultado es un algoritmo de meta-retroalimentación que es tanto más robusto como más preciso que el método de referencia original. Categorías y Descriptores de Asignaturas: H.3.3 [Recuperación de Información]: Modelos de Recuperación Términos Generales: Algoritmos, Experimentación 1. INTRODUCCIÓN La incertidumbre es una característica inherente de la recuperación de información. No solo no sabemos las consultas que se presentarán a nuestro algoritmo de recuperación con anticipación, sino que la necesidad de información de los usuarios puede ser vaga o estar incompletamente especificada por estas consultas. Incluso si la consulta estuviera perfectamente especificada, el lenguaje en los documentos de la colección es inherentemente complejo y ambiguo, y hacer coincidir dicho lenguaje de manera efectiva es un problema formidable por sí mismo. Con esto en mente, deseamos tratar muchas cantidades importantes calculadas por el sistema de recuperación, ya sea un puntaje de relevancia para un documento, o un peso para un término de expansión de consulta, como variables aleatorias cuyo valor real es incierto pero donde la incertidumbre sobre el valor real puede ser cuantificada reemplazando el valor fijo con una distribución de probabilidad sobre posibles valores. De esta manera, los algoritmos de recuperación pueden intentar cuantificar el riesgo o la incertidumbre asociados con sus clasificaciones de salida, o mejorar la estabilidad o precisión de sus cálculos internos. Los algoritmos actuales para la retroalimentación de pseudo relevancia (PRF) tienden a seguir el mismo método básico ya sea que utilicemos algoritmos basados en espacio vectorial como la fórmula de Rocchio [16], o enfoques más recientes de modelado de lenguaje como Modelos de Relevancia [10]. Primero, se obtiene un conjunto de documentos recuperados en la parte superior a partir de una consulta inicial y se asume que aproxima un conjunto de documentos relevantes. A continuación, se calcula un vector de modelo de retroalimentación único según algún tipo de promedio, centroide o expectativa sobre el conjunto de modelos de documentos posiblemente relevantes. Por ejemplo, los vectores de documentos pueden combinarse con igual ponderación, como en Rocchio, o por probabilidad de consulta, como se puede hacer utilizando el Modelo de Relevancia. El uso de una expectativa es razonable por razones prácticas y teóricas, pero por sí sola ignora información potencialmente valiosa sobre el riesgo del modelo de retroalimentación. Nuestra hipótesis principal en este artículo es que estimar la incertidumbre en la retroalimentación es útil y conduce a modelos de retroalimentación individual más efectivos y a modelos combinados más robustos. Por lo tanto, proponemos un método para estimar la incertidumbre asociada con un modelo de retroalimentación individual en términos de una distribución posterior sobre modelos de lenguaje. Para hacer esto, variamos sistemáticamente las entradas al <br>método de retroalimentación</br> base y ajustamos una distribución de Dirichlet a la salida. Utilizamos la media posterior o el modo como la estimación mejorada del modelo de retroalimentación. Este proceso se muestra en la Figura 1. Como mostraremos más adelante, la media y la moda pueden variar significativamente del modelo de retroalimentación única propuesto por el método de referencia. También realizamos la combinación de modelos utilizando varios modelos de lenguaje de retroalimentación mejorados obtenidos a partir de un pequeño número de nuevas consultas muestreadas de la consulta original. El peso de un modelo combina dos factores complementarios: la probabilidad del modelo de generar la consulta y la varianza del modelo, siendo los modelos con alta varianza los que obtienen un peso menor. Por ejemplo, un vector de parámetros esperado condicionado a la observación de la consulta se forma a partir de los documentos recuperados en la parte superior, que se tratan como cadenas de entrenamiento (ver [10], p. 62). Figura 1: Estimación de la incertidumbre del modelo de retroalimentación para una sola consulta. 2. En las Secciones 2.1-2.5 describimos un método general para estimar una distribución de probabilidad sobre el conjunto de posibles modelos de lenguaje basado en muestreo. En las Secciones 2.6 y 2.7 resumimos cómo se utilizan diferentes muestras de consultas para generar múltiples modelos de retroalimentación, que luego se combinan. 2.1 Modelado de la Incertidumbre de la Retroalimentación Dado una consulta Q y una colección C, asumimos un sistema de recuperación probabilístico que asigna un puntaje de documento de valor real f(D, Q) a cada documento D en C, de manera que el puntaje sea proporcional a la probabilidad estimada de relevancia. No hacemos ninguna otra suposición sobre f(D, Q). La naturaleza de f(D, Q) puede ser compleja: por ejemplo, si el sistema de recuperación admite lenguajes de consulta estructurados [12], entonces f(D, Q) puede representar la salida de una red de inferencia arbitrariamente compleja definida por los operadores de consulta estructurados. En teoría, la función de puntuación puede variar de una consulta a otra, aunque en este estudio por simplicidad mantenemos la misma función de puntuación para todas las consultas. Nuestro método de consulta específico se encuentra en la Sección 3. Tratamos el algoritmo de retroalimentación como una caja negra y asumimos que los insumos del algoritmo de retroalimentación son la consulta original y los documentos recuperados principales correspondientes, a los cuales se les asigna una puntuación a cada documento. Suponemos que la salida del algoritmo de retroalimentación es un vector de pesos de términos que se utilizarán para agregar o reponderar los términos en la representación de la consulta original, con el vector normalizado para formar una distribución de probabilidad. Consideramos las entradas a la caja negra de retroalimentación como variables aleatorias, y analizamos el modelo de retroalimentación como una variable aleatoria que cambia en respuesta a cambios en las entradas. Al igual que la función de puntuación del documento f(D, Q), el algoritmo de retroalimentación puede implementar una fórmula de puntuación compleja y no lineal, por lo que, a medida que sus entradas varían, los modelos de retroalimentación resultantes pueden tener una distribución compleja en el espacio de modelos de retroalimentación (el espacio muestral). Debido a esta complejidad potencial, no intentamos derivar una distribución posterior de forma cerrada, sino que en su lugar utilizamos simulación. Llamamos a esta distribución sobre posibles modelos de retroalimentación la distribución de modelos de retroalimentación. Nuestro objetivo en esta sección es estimar una aproximación útil a la distribución del modelo de retroalimentación. Para un marco específico de experimentos, utilizamos el enfoque de modelado de lenguaje (LM) para la recuperación de información [15]. El puntaje de un documento D con respecto a una consulta Q y una colección C se da por p(Q|D) con respecto a los modelos de lenguaje ˆθQ y ˆθD estimados para la consulta y el documento respectivamente. Denotamos el conjunto de los k documentos mejor recuperados de la colección C en respuesta a Q como DQ(k, C). Para simplificar, asumimos que las consultas y documentos son generados por distribuciones multinomiales cuyos parámetros están representados por modelos de lenguaje unigram. Para incorporar retroalimentación en el enfoque de LM, asumimos un esquema basado en modelos en el que nuestro objetivo es tomar la consulta y los documentos clasificados resultantes DQ(k, C) como entrada, y producir un modelo de lenguaje expandido ˆθE, que luego se interpola con el modelo de consulta original ˆθQ: ˆθNew = (1 − α) · ˆθQ + α · ˆθE (1). Esto incluye la posibilidad de α = 1 donde el modelo de consulta original es completamente reemplazado por el modelo de retroalimentación. Nuestro espacio muestral es el conjunto de todos los posibles modelos de lenguaje LF que pueden ser generados como modelos de retroalimentación. Nuestro enfoque es tomar muestras de este espacio y luego ajustar una distribución a las muestras utilizando máxima verosimilitud. Para simplificar, comenzamos asumiendo que la distribución de retroalimentación latente tiene la forma de una distribución de Dirichlet. Aunque la distribución de Dirichlet es unimodal y en general bastante limitada en su expresividad en el espacio muestral, es una coincidencia natural para el modelo de lenguaje multinomial, puede ser estimada rápidamente y capturar las características más relevantes de los modelos de retroalimentación confiada e incierta, como la dispersión general de la distribución. 2.2 Re-muestreo de modelos de documentos. Nos gustaría una aproximación a la distribución posterior del modelo de retroalimentación LF. Para lograr esto, aplicamos una técnica de simulación ampliamente utilizada llamada muestreo bootstrap ([7], p. 474) en los parámetros de entrada, es decir, el conjunto de documentos recuperados en la parte superior. El muestreo de arranque nos permite simular el efecto aproximado de perturbar los parámetros dentro del algoritmo de retroalimentación de la caja negra al perturbar los insumos de ese algoritmo de manera sistemática, sin hacer suposiciones sobre la naturaleza del algoritmo de retroalimentación. Específicamente, muestreamos k documentos con reemplazo de DQ(k, C), y calculamos un modelo de lenguaje de expansión θb utilizando el <br>método de retroalimentación</br> de caja negra. Repetimos este proceso B veces para obtener un conjunto de B modelos de lenguaje de retroalimentación, a los cuales luego ajustamos una distribución de Dirichlet. Normalmente, B se encuentra en el rango de 20 a 50 muestras, con un rendimiento relativamente estable en este rango. Ten en cuenta que en lugar de tratar cada documento superior como igualmente probable, muestreamos de acuerdo con las probabilidades estimadas de relevancia de cada documento en DQ(k, C). Por lo tanto, es más probable que un documento sea elegido cuanto más alto esté en la clasificación. 2.3 Justificación de un enfoque de muestreo La justificación de nuestro enfoque de muestreo tiene dos partes. Primero, queremos mejorar la calidad de los modelos de retroalimentación individual al suavizar la variación cuando el modelo de retroalimentación base es inestable. En este sentido, nuestro enfoque se asemeja al bagging [4], un enfoque de conjunto que genera múltiples versiones de un predictor haciendo copias de arranque del conjunto de entrenamiento, y luego promedia los predictores (numéricos). En nuestra aplicación, los documentos recuperados en la parte superior pueden ser vistos como una especie de conjunto de entrenamiento ruidoso para la relevancia. Segundo, el muestreo es una forma efectiva de estimar propiedades básicas de la distribución posterior de retroalimentación, las cuales pueden ser utilizadas para mejorar la combinación de modelos. Por ejemplo, un modelo puede ser ponderado por su confianza de predicción, estimada como una función de la variabilidad del posterior alrededor del modelo. foo2-401.map-Dim:5434,Size:12*12units, vecindario gaussiano (a) Tema 401 Minorías extranjeras, Alemania foo2-402.map-Dim:5698,Size:12*12units, vecindario gaussiano (b) Tema 402 Genética del comportamiento foo2-459.map-Dim:8969,Size:12*12units, vecindario gaussiano (c) Tema 459 ¿Cuándo puede un prestamista ejecutar una hipoteca sobre una propiedad? Figura 2: Visualización de la varianza del modelo de lenguaje de expansión utilizando mapas autoorganizados, mostrando la distribución de modelos de lenguaje que resulta de remuestrear las entradas al método de expansión de referencia. El modelo de lenguaje que habría sido elegido por la expansión de la línea base se encuentra en el centro de cada mapa. La función de similitud es la divergencia JensenShannon. 2.4 Visualización de distribuciones de retroalimentación Antes de describir cómo ajustamos y utilizamos la distribución de Dirichlet sobre modelos de retroalimentación, es instructivo ver algunos ejemplos de distribuciones de modelos de retroalimentación reales que resultan de muestrear por bootstrap los documentos más recuperados de diferentes temas de TREC. Cada punto en nuestro espacio muestral es un modelo de lenguaje, que típicamente tiene varias miles de dimensiones. Para ayudar a analizar el comportamiento de nuestro método, utilizamos un Mapa Auto-organizado (a través del paquete SOM-PAK [9]), para aplanar y visualizar la función de densidad de alta dimensión. Los mapas de densidad para tres temas de TREC se muestran en la Figura 2 arriba. Las áreas oscuras representan regiones de alta similitud entre los modelos de lenguaje. Las áreas claras representan regiones de baja similitud, los valles entre los grupos. Cada diagrama está centrado en el modelo de lenguaje que habría sido elegido por la expansión de referencia. Un solo pico (modo) es evidente en algunos ejemplos, pero en otros aparece una estructura más compleja. Además, aunque la distribución suele estar cerca del modelo de retroalimentación de referencia, para algunos temas están a una distancia significativa (según la divergencia de JensenShannon), como se muestra en la Subfigura 2c. En tales casos, la moda o la media de la distribución de retroalimentación a menudo funcionan significativamente mejor que el valor base (y en una proporción menor de casos, significativamente peor). 2.5 Ajuste de una distribución de retroalimentación posterior Después de obtener muestras del modelo de retroalimentación mediante el remuestreo de las entradas del modelo de retroalimentación, estimamos la distribución de retroalimentación. Suponemos que los modelos de retroalimentación multinomial {ˆθ1, . . . , ˆθB} fueron generados por una distribución de Dirichlet latente con parámetros {α1, . . . , αN}. Para estimar los {α1, . . . , αN }, ajustamos los parámetros de Dirichlet a las muestras del modelo de lenguaje B según la máxima verosimilitud utilizando un procedimiento de Newton generalizado, cuyos detalles se encuentran en Minka [13]. Suponemos una distribución de Dirichlet simple sobre los {α1, . . . , αN}, estableciendo cada uno como αi = μ · p(wi | C), donde μ es un parámetro y p(· | C) es el modelo de lenguaje de colección estimado a partir de un conjunto de documentos de la colección C. El ajuste de parámetros converge muy rápidamente, generalmente en solo 2 o 3 iteraciones. Dado que nuestros puntos son modelos de lenguaje en el simplex multinomial, extendimos SOM-PAK para admitir la divergencia de Jensen-Shannon, una medida de similitud ampliamente utilizada entre distribuciones de probabilidad. 3 iteraciones son suficientes, por lo que es práctico aplicarlo en tiempo de consulta cuando la sobrecarga computacional debe ser pequeña. En la práctica, podemos restringir el cálculo al vocabulario de los documentos recuperados en la parte superior, en lugar de toda la colección. Ten en cuenta que para este paso estamos reutilizando los documentos recuperados existentes y no realizando consultas adicionales. Dado los parámetros de una distribución de Dirichlet N-dimensional Dir(α), los vectores de media μ y moda x son fáciles de calcular y se dan respectivamente por μi = αiP αi (2) y xi = αi−1P αi−N . (3) Luego podemos elegir el modelo de lenguaje en la media o en la moda del posterior como el modelo de retroalimentación mejorado final. (Encontramos que la moda proporciona un rendimiento ligeramente mejor). Para la recuperación de información, es probable que el número de muestras disponibles sea bastante pequeño por razones de rendimiento, generalmente menos de diez. Además, si bien el muestreo aleatorio es útil en ciertos casos, es perfectamente aceptable permitir distribuciones de muestreo determinísticas, pero estas deben diseñarse cuidadosamente para aproximar una varianza de salida precisa. Dejamos esto para estudios futuros. 2.6 Variantes de consulta Utilizamos los siguientes métodos para generar variantes de la consulta original. Cada variante corresponde a una suposición diferente sobre qué aspectos de la consulta original pueden ser importantes. Esta es una forma de muestreo determinístico. Seleccionamos tres métodos simples que cubren suposiciones complementarias sobre la consulta. Uso sin expansión. La suposición es que los términos dados son una descripción completa de la necesidad de información. Deja uno fuera. Se deja fuera un término del query original. La suposición es que uno de los términos de la consulta es un término de ruido. Se elige un término único de la consulta original. Esto asume que solo un aspecto de la consulta, es decir, el representado por el término, es el más importante. Después de generar una variante de la consulta original, la combinamos con la consulta original utilizando un peso αSUB para no alejarnos demasiado. En este estudio, establecimos αSUB = 0.5. Por ejemplo, utilizando el lenguaje de consulta Indri [12], una variante de dejar uno fuera de la consulta inicial que omite el término \"ireland\" para el tema TREC 404 es: #weight(0.5 #combine(ireland peace talks) 0.5 #combine(peace talks)) 2.7 Combinando modelos de retroalimentación mejorados de múltiples variantes de consulta. Cuando se utilizan múltiples variantes de consulta, los modelos de retroalimentación mejorados resultantes se combinan utilizando la combinación de modelos bayesianos. Para hacer esto, tratamos cada palabra como un elemento a clasificar como perteneciente a una clase relevante o no relevante, y derivamos una probabilidad de clase para cada palabra combinando las puntuaciones de cada variante de consulta. Cada puntuación es dada por la probabilidad de ese término en la distribución de Dirichlet. Las puntuaciones de los términos se ponderan por el inverso de la varianza del término en los modelos de retroalimentación mejorada de la distribución de Dirichlet. La probabilidad previa de pertenencia de una palabra a la clase relevante se da por la probabilidad de la consulta original en todo el modelo de expansión mejorado. 3. EVALUACIÓN En esta sección presentamos resultados que confirman la utilidad de estimar una distribución del modelo de retroalimentación a partir del remuestreo ponderado de documentos mejor clasificados, y de combinar los modelos de retroalimentación obtenidos a partir de diferentes pequeños cambios en la consulta original. 3.1 Método general Evaluamos el rendimiento en un total de 350 consultas derivadas de cuatro conjuntos de temas de TREC: 51-200 (TREC-1&2), 351-400 (TREC-7), 401-450 (TREC-8) y 451-550 (wt10g, TREC-9&10). Elegimos estos por su contenido variado y propiedades del documento. Por ejemplo, los documentos wt10g son páginas web con una amplia variedad de temas y estilos, mientras que los documentos TREC-1&2 son artículos de noticias más homogéneos. La indexación y recuperación se realizó utilizando el sistema Indri en el kit de herramientas Lemur [12] [1]. Nuestras consultas se derivaron de las palabras en el campo del título de los temas de TREC. Las frases no fueron utilizadas. Para generar las consultas base que se pasaron a Indri, envolvimos los términos de la consulta con el operador #combine de Indri. Por ejemplo, la consulta inicial para el tema 404 es: #combine(ireland peace talks) Realizamos el stemming de Krovetz para todos los experimentos. Debido a que encontramos que el método de expansión de la base (Indri) funcionó mejor utilizando una lista de palabras vacías con el modelo de retroalimentación, todos los experimentos utilizaron una lista de 419 palabras comunes en inglés. Sin embargo, un efecto secundario interesante de nuestro enfoque de remuestreo es que tiende a eliminar muchas palabras vacías del modelo de retroalimentación, lo que hace que una lista de paradas sea menos crítica. Esto se discute más a fondo en la Sección 3.6. Método de retroalimentación de línea base 3.2 Para nuestro método de expansión de línea base, utilizamos un algoritmo incluido en Indri 1.0 como el método de expansión predeterminado. Este método primero selecciona términos utilizando un cálculo de logaritmo de probabilidades descrito por Ponte [14], pero asigna pesos finales a los términos utilizando el modelo de relevancia de Lavrenko [10]. Elegimos el método Indri porque proporciona una línea base consistentemente sólida, se basa en un enfoque de modelado del lenguaje y es fácil de experimentar con él. En una evaluación de TREC utilizando el corpus GOV2, el método fue uno de los mejores resultados, logrando un aumento del 19.8% en el MAP en comparación con el uso de consultas no expandidas. En este estudio, se logra una ganancia promedio en MAP del 17.25% en las cuatro colecciones. El método de expansión de Indri primero calcula una razón de logaritmos de probabilidades o(v) para cada término de expansión potencial v dado por o(v) = X D log p(v|D) p(v|C) (4) sobre todos los documentos D que contienen v, en la colección C. Luego, los candidatos a términos de expansión se ordenan por o(v) descendente, y se eligen los primeros m. Finalmente, los pesos de término r(v) utilizados en la consulta ampliada se calculan en base al modelo de relevancia r(v) = X D p(q|D)p(v|D) p(v) p(D) (5). La cantidad p(q|D) es la puntuación de probabilidad asignada al documento en la recuperación inicial. Utilizamos suavizado de Dirichlet de p(v|D) con μ = 1000. Este modelo de relevancia se combina luego con la consulta original utilizando interpolación lineal, ponderada por un parámetro α. Por defecto, utilizamos los 50 documentos principales para la retroalimentación y los 20 términos de expansión, con el parámetro de interpolación de retroalimentación α = 0.5 a menos que se indique lo contrario. Por ejemplo, la consulta expandida base para el tema 404 es: #weight(0.5 #combine(ireland peace talks) 0.5 #weight(0.10 ireland 0.08 peace 0.08 northern ...) 3.3 Rendimiento de la expansión. Medimos la efectividad de nuestros algoritmos de retroalimentación mediante dos criterios principales: precisión y robustez. La robustez, y el equilibrio entre precisión y robustez, se analiza en la Sección 3.4. En esta sección, examinamos la precisión promedio y la precisión en los 10 documentos principales (P10). También incluimos la recuperación de 1,000 documentos. Para cada consulta, obtuvimos un conjunto de B modelos de retroalimentación utilizando la línea base de Indri. Cada modelo de retroalimentación se obtuvo a partir de una muestra aleatoria de los k documentos principales tomados con reemplazo. Para estos experimentos, B = 30 y k = 50. Cada modelo de retroalimentación contenía 20 términos. En el lado de la consulta, utilizamos muestreo de dejar uno fuera (LOO) para crear las variantes de consulta. El muestreo de consulta de un solo término tuvo un rendimiento consistentemente peor en todas las colecciones, por lo que nuestros resultados se centran en el muestreo de LOO aquí. Utilizamos los métodos descritos en la Sección 2 para estimar un modelo de retroalimentación mejorado a partir de la distribución posterior de Dirichlet para cada variante de consulta, y para combinar los modelos de retroalimentación de todas las variantes de consulta. Llamamos a nuestro método expansión de remuestreo y lo denotamos como RS-FB aquí. Denominamos al <br>método de retroalimentación</br> de línea base Indri como Base-FB. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "posterior distribution": {
            "translated_key": "distribución posterior",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Estimation and Use of Uncertainty in Pseudo-relevance Feedback Kevyn Collins-Thompson and Jamie Callan Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213-8213 U.S.A. {kct | callan}@cs.cmu.edu ABSTRACT Existing pseudo-relevance feedback methods typically perform averaging over the top-retrieved documents, but ignore an important statistical dimension: the risk or variance associated with either the individual document models, or their combination.",
                "Treating the baseline feedback method as a black box, and the output feedback model as a random variable, we estimate a <br>posterior distribution</br> for the feedback model by resampling a given querys top-retrieved documents, using the posterior mean or mode as the enhanced feedback model.",
                "We then perform model combination over several enhanced models, each based on a slightly modified query sampled from the original query.",
                "We find that resampling documents helps increase individual feedback model precision by removing noise terms, while sampling from the query improves robustness (worst-case performance) by emphasizing terms related to multiple query aspects.",
                "The result is a meta-feedback algorithm that is both more robust and more precise than the original strong baseline method.",
                "Categories and Subject Descriptors: H.3.3 [Information Retrieval]: Retrieval Models General Terms: Algorithms, Experimentation 1.",
                "INTRODUCTION Uncertainty is an inherent feature of information retrieval.",
                "Not only do we not know the queries that will be presented to our retrieval algorithm ahead of time, but the users information need may be vague or incompletely specified by these queries.",
                "Even if the query were perfectly specified, language in the collection documents is inherently complex and ambiguous and matching such language effectively is a formidable problem by itself.",
                "With this in mind, we wish to treat many important quantities calculated by the retrieval system, whether a relevance score for a document, or a weight for a query expansion term, as random variables whose true value is uncertain but where the uncertainty about the true value may be quantified by replacing the fixed value with a probability distribution over possible values.",
                "In this way, retrieval algorithms may attempt to quantify the risk or uncertainty associated with their output rankings, or improve the stability or precision of their internal calculations.",
                "Current algorithms for pseudo-relevance feedback (PRF) tend to follow the same basic method whether we use vector space-based algorithms such as Rocchios formula [16], or more recent language modeling approaches such as Relevance Models [10].",
                "First, a set of top-retrieved documents is obtained from an initial query and assumed to approximate a set of relevant documents.",
                "Next, a single feedback model vector is computed according to some sort of average, centroid, or expectation over the set of possibly-relevant document models.",
                "For example, the document vectors may be combined with equal weighting, as in Rocchio, or by query likelihood, as may be done using the Relevance Model1 .",
                "The use of an expectation is reasonable for practical and theoretical reasons, but by itself ignores potentially valuable information about the risk of the feedback model.",
                "Our main hypothesis in this paper is that estimating the uncertainty in feedback is useful and leads to better individual feedback models and more robust combined models.",
                "Therefore, we propose a method for estimating uncertainty associated with an individual feedback model in terms of a <br>posterior distribution</br> over language models.",
                "To do this, we systematically vary the inputs to the baseline feedback method and fit a Dirichlet distribution to the output.",
                "We use the posterior mean or mode as the improved feedback model estimate.",
                "This process is shown in Figure 1.",
                "As we show later, the mean and mode may vary significantly from the single feedback model proposed by the baseline method.",
                "We also perform model combination using several improved feedback language models obtained by a small number of new queries sampled from the original query.",
                "A models weight combines two complementary factors: the models probability of generating the query, and the variance of the model, with high-variance models getting lower weight. 1 For example, an expected parameter vector conditioned on the query observation is formed from top-retrieved documents, which are treated as training strings (see [10], p. 62).",
                "Figure 1: Estimating the uncertainty of the feedback model for a single query. 2.",
                "SAMPLING-BASED FEEDBACK In Sections 2.1-2.5 we describe a general method for estimating a probability distribution over the set of possible language models.",
                "In Sections 2.6 and 2.7 we summarize how different query samples are used to generate multiple feedback models, which are then combined. 2.1 Modeling Feedback Uncertainty Given a query Q and a collection C, we assume a probabilistic retrieval system that assigns a real-valued document score f(D, Q) to each document D in C, such that the score is proportional to the estimated probability of relevance.",
                "We make no other assumptions about f(D, Q).",
                "The nature of f(D, Q) may be complex: for example, if the retrieval system supports structured query languages [12], then f(D, Q) may represent the output of an arbitrarily complex inference network defined by the structured query operators.",
                "In theory, the scoring function can vary from query to query, although in this study for simplicity we keep the scoring function the same for all queries.",
                "Our specific query method is given in Section 3.",
                "We treat the feedback algorithm as a black box and assume that the inputs to the feedback algorithm are the original query and the corresponding top-retrieved documents, with a score being given to each document.",
                "We assume that the output of the feedback algorithm is a vector of term weights to be used to add or reweight the terms in the representation of the original query, with the vector normalized to form a probability distribution.",
                "We view the the inputs to the feedback black box as random variables, and analyze the feedback model as a random variable that changes in response to changes in the inputs.",
                "Like the document scoring function f(D, Q), the feedback algorithm may implement a complex, non-linear scoring formula, and so as its inputs vary, the resulting feedback models may have a complex distribution over the space of feedback models (the sample space).",
                "Because of this potential complexity, we do not attempt to derive a <br>posterior distribution</br> in closed form, but instead use simulation.",
                "We call this distribution over possible feedback models the feedback model distribution.",
                "Our goal in this section is to estimate a useful approximation to the feedback model distribution.",
                "For a specific framework for experiments, we use the language modeling (LM) approach for information retrieval [15].",
                "The score of a document D with respect to a query Q and collection C is given by p(Q|D) with respect to language models ˆθQ and ˆθD estimated for the query and document respectively.",
                "We denote the set of k top-retrieved documents from collection C in response to Q by DQ(k, C).",
                "For simplicity, we assume that queries and documents are generated by multinomial distributions whose parameters are represented by unigram language models.",
                "To incorporate feedback in the LM approach, we assume a model-based scheme in which our goal is take the query and resulting ranked documents DQ(k, C) as input, and output an expansion language model ˆθE, which is then interpolated with the original query model ˆθQ: ˆθNew = (1 − α) · ˆθQ + α · ˆθE (1) This includes the possibility of α = 1 where the original query mode is completely replaced by the feedback model.",
                "Our sample space is the set of all possible language models LF that may be output as feedback models.",
                "Our approach is to take samples from this space and then fit a distribution to the samples using maximum likelihood.",
                "For simplicity, we start by assuming the latent feedback distribution has the form of a Dirichlet distribution.",
                "Although the Dirichlet is a unimodal distribution, and in general quite limited in its expressiveness in the sample space, it is a natural match for the multinomial language model, can be estimated quickly, and can capture the most salient features of confident and uncertain feedback models, such as the overall spread of the distibution. 2.2 Resampling document models We would like an approximation to the <br>posterior distribution</br> of the feedback model LF .",
                "To accomplish this, we apply a widely-used simulation technique called bootstrap sampling ([7], p. 474) on the input parameters, namely, the set of top-retrieved documents.",
                "Bootstrap sampling allows us to simulate the approximate effect of perturbing the parameters within the black box feedback algorithm by perturbing the inputs to that algorithm in a systematic way, while making no assumptions about the nature of the feedback algorithm.",
                "Specifically, we sample k documents with replacement from DQ(k, C), and calculate an expansion language model θb using the black box feedback method.",
                "We repeat this process B times to obtain a set of B feedback language models, to which we then fit a Dirichlet distribution.",
                "Typically B is in the range of 20 to 50 samples, with performance being relatively stable in this range.",
                "Note that instead of treating each top document as equally likely, we sample according to the estimated probabilities of relevance of each document in DQ(k, C).",
                "Thus, a document is more likely to be chosen the higher it is in the ranking. 2.3 Justification for a sampling approach The rationale for our sampling approach has two parts.",
                "First, we want to improve the quality of individual feedback models by smoothing out variation when the baseline feedback model is unstable.",
                "In this respect, our approach resembles bagging [4], an ensemble approach which generates multiple versions of a predictor by making bootstrap copies of the training set, and then averages the (numerical) predictors.",
                "In our application, top-retrieved documents can be seen as a kind of noisy training set for relevance.",
                "Second, sampling is an effective way to estimate basic properties of the feedback <br>posterior distribution</br>, which can then be used for improved model combination.",
                "For example, a model may be weighted by its prediction confidence, estimated as a function of the variability of the posterior around the model. foo2-401.map-Dim:5434,Size:12*12units,gaussianneighborhood (a) Topic 401 Foreign minorities, Germany foo2-402.map-Dim:5698,Size:12*12units,gaussianneighborhood (b) Topic 402 Behavioral genetics foo2-459.map-Dim:8969,Size:12*12units,gaussianneighborhood (c) Topic 459 When can a lender foreclose on property Figure 2: Visualization of expansion language model variance using self-organizing maps, showing the distribution of language models that results from resampling the inputs to the baseline expansion method.",
                "The language model that would have been chosen by the baseline expansion is at the center of each map.",
                "The similarity function is JensenShannon divergence. 2.4 Visualizing feedback distributions Before describing how we fit and use the Dirichlet distribution over feedback models, it is instructive to view some examples of actual feedback model distributions that result from bootstrap sampling the top-retrieved documents from different TREC topics.",
                "Each point in our sample space is a language model, which typically has several thousand dimensions.",
                "To help analyze the behavior of our method we used a Self-Organizing Map (via the SOM-PAK package [9]), to flatten and visualize the high-dimensional density function2 .",
                "The density maps for three TREC topics are shown in Figure 2 above.",
                "The dark areas represent regions of high similarity between language models.",
                "The light areas represent regions of low similarity - the valleys between clusters.",
                "Each diagram is centered on the language model that would have been chosen by the baseline expansion.",
                "A single peak (mode) is evident in some examples, but more complex structure appears in others.",
                "Also, while the distribution is usually close to the baseline feedback model, for some topics they are a significant distance apart (as measured by JensenShannon divergence), as in Subfigure 2c.",
                "In such cases, the mode or mean of the feedback distribution often performs significantly better than the baseline (and in a smaller proportion of cases, significantly worse). 2.5 Fitting a posterior feedback distribution After obtaining feedback model samples by resampling the feedback model inputs, we estimate the feedback distribution.",
                "We assume that the multinomial feedback models {ˆθ1, . . . , ˆθB} were generated by a latent Dirichlet distribution with parameters {α1, . . . , αN }.",
                "To estimate the {α1, . . . , αN }, we fit the Dirichlet parameters to the B language model samples according to maximum likelihood using a generalized Newton procedure, details of which are given in Minka [13].",
                "We assume a simple Dirichlet prior over the {α1, . . . , αN }, setting each to αi = μ · p(wi | C), where μ is a parameter and p(· | C) is the collection language model estimated from a set of documents from collection C. The parameter fitting converges very quickly - typically just 2 or 2 Because our points are language models in the multinomial simplex, we extended SOM-PAK to support JensenShannon divergence, a widely-used similarity measure between probability distributions. 3 iterations are enough - so that it is practical to apply at query-time when computational overhead must be small.",
                "In practice, we can restrict the calculation to the vocabulary of the top-retrieved documents, instead of the entire collection.",
                "Note that for this step we are re-using the existing retrieved documents and not performing additional queries.",
                "Given the parameters of an N-dimensional Dirichlet distribution Dir(α) the mean μ and mode x vectors are easy to calculate and are given respectively by μi = αiP αi (2) and xi = αi−1P αi−N . (3) We can then choose the language model at the mean or the mode of the posterior as the final enhanced feedback model. (We found the mode to give slightly better performance.)",
                "For information retrieval, the number of samples we will have available is likely to be quite small for performance reasons - usually less than ten.",
                "Moreover, while random sampling is useful in certain cases, it is perfectly acceptable to allow deterministic sampling distributions, but these must be designed carefully in order to approximate an accurate output variance.",
                "We leave this for future study. 2.6 Query variants We use the following methods for generating variants of the original query.",
                "Each variant corresponds to a different assumption about which aspects of the original query may be important.",
                "This is a form of deterministic sampling.",
                "We selected three simple methods that cover complimentary assumptions about the query.",
                "No-expansion Use only the original query.",
                "The assumption is that the given terms are a complete description of the information need.",
                "Leave-one-out A single term is left out of the original query.",
                "The assumption is that one of the query terms is a noise term.",
                "Single-term A single term is chosen from the original query.",
                "This assumes that only one aspect of the query, namely, that represented by the term, is most important.",
                "After generating a variant of the original query, we combine it with the original query using a weight αSUB so that we do not stray too far.",
                "In this study, we set αSUB = 0.5.",
                "For example, using the Indri [12] query language, a leave-oneout variant of the initial query that omits the term ireland for TREC topic 404 is: #weight(0.5 #combine(ireland peace talks) 0.5 #combine(peace talks)) 2.7 Combining enhanced feedback models from multiple query variants When using multiple query variants, the resulting enhanced feedback models are combined using Bayesian model combination.",
                "To do this, we treat each word as an item to be classified as belonging to a relevant or non-relevant class, and derive a class probability for each word by combining the scores from each query variant.",
                "Each score is given by that terms probability in the Dirichlet distribution.",
                "The term scores are weighted by the inverse of the variance of the term in the enhanced feedback models Dirichlet distribution.",
                "The prior probability of a words membership in the relevant class is given by the probability of the original query in the entire enhanced expansion model. 3.",
                "EVALUATION In this section we present results confirming the usefulness of estimating a feedback model distribution from weighted resampling of top-ranked documents, and of combining the feedback models obtained from different small changes in the original query. 3.1 General method We evaluated performance on a total of 350 queries derived from four sets of TREC topics: 51-200 (TREC-1&2), 351-400 (TREC-7), 401-450 (TREC-8), and 451-550 (wt10g, TREC-9&10).",
                "We chose these for their varied content and document properties.",
                "For example, wt10g documents are Web pages with a wide variety of subjects and styles while TREC-1&2 documents are more homogeneous news articles.",
                "Indexing and retrieval was performed using the Indri system in the Lemur toolkit [12] [1].",
                "Our queries were derived from the words in the title field of the TREC topics.",
                "Phrases were not used.",
                "To generate the baseline queries passed to Indri, we wrapped the query terms with Indris #combine operator.",
                "For example, the initial query for topic 404 is: #combine(ireland peace talks) We performed Krovetz stemming for all experiments.",
                "Because we found that the baseline (Indri) expansion method performed better using a stopword list with the feedback model, all experiments used a stoplist of 419 common English words.",
                "However, an interesting side-effect of our resampling approach is that it tends to remove many stopwords from the feedback model, making a stoplist less critical.",
                "This is discussed further in Section 3.6. 3.2 Baseline feedback method For our baseline expansion method, we use an algorithm included in Indri 1.0 as the default expansion method.",
                "This method first selects terms using a log-odds calculation described by Ponte [14], but assigns final term weights using Lavrenkos relevance model[10].",
                "We chose the Indri method because it gives a consistently strong baseline, is based on a language modeling approach, and is simple to experiment with.",
                "In a TREC evaluation using the GOV2 corpus [6], the method was one of the topperforming runs, achieving a 19.8% gain in MAP compared to using unexpanded queries.",
                "In this study, it achieves an average gain in MAP of 17.25% over the four collections.",
                "Indris expansion method first calculates a log-odds ratio o(v) for each potential expansion term v given by o(v) = X D log p(v|D) p(v|C) (4) over all documents D containing v, in collection C. Then, the expansion term candidates are sorted by descending o(v), and the top m are chosen.",
                "Finally, the term weights r(v) used in the expanded query are calculated based on the relevance model r(v) = X D p(q|D)p(v|D) p(v) p(D) (5) The quantity p(q|D) is the probability score assigned to the document in the initial retrieval.",
                "We use Dirichlet smoothing of p(v|D) with μ = 1000.",
                "This relevance model is then combined with the original query using linear interpolation, weighted by a parameter α.",
                "By default we used the top 50 documents for feedback and the top 20 expansion terms, with the feedback interpolation parameter α = 0.5 unless otherwise stated.",
                "For example, the baseline expanded query for topic 404 is: #weight(0.5 #combine(ireland peace talks) 0.5 #weight(0.10 ireland 0.08 peace 0.08 northern ...) 3.3 Expansion performance We measure our feedback algorithms effectiveness by two main criteria: precision, and robustness.",
                "Robustness, and the tradeoff between precision and robustness, is analyzed in Section 3.4.",
                "In this section, we examine average precision and precision in the top 10 documents (P10).",
                "We also include recall at 1,000 documents.",
                "For each query, we obtained a set of B feedback models using the Indri baseline.",
                "Each feedback model was obtained from a random sample of the top k documents taken with replacement.",
                "For these experiments, B = 30 and k = 50.",
                "Each feedback model contained 20 terms.",
                "On the query side, we used leave-one-out (LOO) sampling to create the query variants.",
                "Single-term query sampling had consistently worse performance across all collections and so our results here focus on LOO sampling.",
                "We used the methods described in Section 2 to estimate an enhanced feedback model from the Dirichlet <br>posterior distribution</br> for each query variant, and to combine the feedback models from all the query variants.",
                "We call our method resampling expansion and denote it as RS-FB here.",
                "We denote the Indri baseline feedback method as Base-FB.",
                "Results from applying both the baseline expansion method (Base-FB) and resampling expansion (RS-FB) are shown in Table 1.",
                "We observe several trends in this table.",
                "First, the average precision of RS-FB was comparable to Base-FB, achieving an average gain of 17.6% compared to using no expansion across the four collections.",
                "The Indri baseline expansion gain was 17.25%.",
                "Also, the RS-FB method achieved consistent improvements in P10 over Base-FB for every topic set, with an average improvement of 6.89% over Base-FB for all 350 topics.",
                "The lowest P10 gain over Base-FB was +3.82% for TREC-7 and the highest was +11.95% for wt10g.",
                "Finally, both Base-FB and RS-FB also consistently improved recall over using no expansion, with Base-FB achieving better recall than RS-FB for all topic sets. 3.4 Retrieval robustness We use the term robustness to mean the worst-case average precision performance of a feedback algorithm.",
                "Ideally, a robust feedback method would never perform worse than using the original query, while often performing better using the expansion.",
                "To evaluate robustness in this study, we use a very simple measure called the robustness index (RI)3 .",
                "For a set of queries Q, the RI measure is defined as: RI(Q) = n+ − n− |Q| (6) where n+ is the number of queries helped by the feedback method and n− is the number of queries hurt.",
                "Here, by helped we mean obtaining a higher average precision as a result of feedback.",
                "The value of RI ranges from a minimum 3 This is sometimes also called the reliability of improvement index and was used in Sakai et al. [17].",
                "Collection NoExp Base-FB RS-FB TREC 1&2 AvgP 0.1818 0.2419 (+33.04%) 0.2406 (+32.24%) P10 0.4443 0.4913 (+10.57%) 0.5363 (+17.83%) Recall 15084/37393 19172/37393 15396/37393 TREC 7 AvgP 0.1890 0.2175 (+15.07%) 0.2169 (+14.75%) P10 0.4200 0.4320 (+2.85%) 0.4480 (+6.67%) Recall 2179/4674 2608/4674 2487/4674 TREC 8 AvgP 0.2031 0.2361 (+16.25%) 0.2268 (+11.70%) P10 0.3960 0.4160 (+5.05%) 0.4340 (+9.59%) Recall 2144/4728 2642/4728 2485/4728 wt10g AvgP 0.1741 0.1829 (+5.06%) 0.1946 (+11.78%) P10 0.2760 0.2630 (-4.71%) 0.2960 (+7.24%) Recall 3361/5980 3725/5980 3664/5980 Table 1: Comparison of baseline (Base-FB) feedback and feedback using re-sampling (RS-FB).",
                "Improvement shown for BaseFB and RS-FB is relative to using no expansion. (a) TREC 1&2 (upper curve); TREC 8 (lower curve) (b) TREC 7 (upper curve); wt10g (lower curve) Figure 3: The trade-off between robustness and average precision for different corpora.",
                "The x-axis gives the change in MAP over using baseline expansion with α = 0.5.",
                "The yaxis gives the Robustness Index (RI).",
                "Each curve through uncircled points shows the RI/MAP tradeoff using the simple small-α strategy (see text) as α decreases from 0.5 to zero in the direction of the arrow.",
                "Circled points represent the tradeoffs obtained by resampling feedback for α = 0.5.",
                "Collection N Base-FB RS-FB n− RI n− RI TREC 1&2 103 26 +0.495 15 +0.709 TREC 7 46 14 +0.391 10 +0.565 TREC 8 44 12 +0.455 12 +0.455 wt10g 91 48 -0.055 39 +0.143 Combined 284 100 +0.296 76 +0.465 Table 2: Comparison of robustness index (RI) for baseline feedback (Base-FB) vs. resampling feedback (RS-FB).",
                "Also shown are the actual number of queries hurt by feedback (n−) for each method and collection.",
                "Queries for which initial average precision was negligible (≤ 0.01) were ignored, giving the remaining query count in column N. of −1.0, when all queries are hurt by the feedback method, to +1.0 when all queries are helped.",
                "The RI measure does not take into account the magnitude or distribution of the amount of change across the set Q.",
                "However, it is easy to understand as a general indication of robustness.",
                "One obvious way to improve the worst-case performance of feedback is simply to use a smaller fixed α interpolation parameter, such as α = 0.3, placing less weight on the (possibly risky) feedback model and more on the original query.",
                "We call this the small-α strategy.",
                "Since we are also reducing the potential gains when the feedback model is right, however, we would expect some trade-off between average precision and robustness.",
                "We therefore compared the precision/robustness trade-off between our resampling feedback algorithm, and the simple small-α method.",
                "The results are summarized in Figure 3.",
                "In the figure, the curve for each topic set interpolates between trade-off points, beginning at x=0, where α = 0.5, and continuing in the direction of the arrow as α decreases and the original query is given more and more weight.",
                "As expected, robustness continuously increases as we move along the curve, but mean average precision generally drops as the gains from feedback are eliminated.",
                "For comparison, the performance of resampling feedback at α = 0.5 is shown for each collection as the circled point.",
                "Higher and to the right is better.",
                "This figure shows that resampling feedback gives a somewhat better trade-off than the small-α approach for 3 of the 4 collections.",
                "Figure 4: Histogram showing improved robustness of resampling feedback (RS-FB) over baseline feedback (Base-FB) for all datasets combined.",
                "Queries are binned by % change in AP compared to the unexpanded query.",
                "Collection DS + QV DS + No QV TREC 1&2 AvgP 0.2406 0.2547 (+5.86%) P10 0.5263 0.5362 (+1.88%) RI 0.7087 0.6515 (-0.0572) TREC 7 AvgP 0.2169 0.2200 (+1.43%) P10 0.4480 0.4300 (-4.02%) RI 0.5652 0.2609 (-0.3043) TREC 8 AvgP 0.2268 0.2257 (-0.49%) P10 0.4340 0.4200 (-3.23%) RI 0.4545 0.4091 (-0.0454) wt10g AvgP 0.1946 0.1865 (-4.16%) P10 0.2960 0.2680 (-9.46%) RI 0.1429 0.0220 (-0.1209) Table 3: Comparison of resampling feedback using document sampling (DS) with (QV) and without (No QV) combining feedback models from multiple query variants.",
                "Table 2 gives the Robustness Index scores for Base-FB and RS-FB.",
                "The RS-FB feedback method obtained higher robustness than Base-FB on three of the four topic sets, with only slightly worse performance on TREC-8.",
                "A more detailed view showing the distribution over relative changes in AP is given by the histogram in Figure 4.",
                "Compared to Base-FB, the RS-FB method achieves a noticable reduction in the number of queries significantly hurt by expansion (i.e. where AP is hurt by 25% or more), while preserving positive gains in AP. 3.5 Effect of query and document sampling methods Given our algorithms improved robustness seen in Section 3.4, an important question is what component of our system is responsible.",
                "Is it the use of document re-sampling, the use of multiple query variants, or some other factor?",
                "The results in Table 3 suggest that the model combination based on query variants may be largely account for the improved robustness.",
                "When query variants are turned off and the original query is used by itself with document sampling, there is little net change in average precision, a small decrease in P10 for 3 out of the 4 topic sets, but a significant drop in robustness for all topic sets.",
                "In two cases, the RI measure drops by more than 50%.",
                "We also examined the effect of the document sampling method on retrieval effectiveness, using two different strategies.",
                "The uniform weighting strategy ignored the relevance scores from the initial retrieval and gave each document in the top k the same probability of selection.",
                "In contrast, the relevance-score weighting strategy chose documents with probability proportional to their relevance scores.",
                "In this way, documents that were more highly ranked were more likely to be selected.",
                "Results are shown in Table 4.",
                "The relevance-score weighting strategy performs better overall, with significantly higher RI and P10 scores on 3 of the 4 topic sets.",
                "The difference in average precision between the methods, however, is less marked.",
                "This suggests that uniform weighting acts to increase variance in retrieval results: when initial average precision is high, there are many relevant documents in the top k and uniform sampling may give a more representative relevance model than focusing on the highly-ranked items.",
                "On the other hand, when initial precision is low, there are few relevant documents in the bottom ranks and uniform sampling mixes in more of the non-relevant documents.",
                "For space reasons we only summarize our findings on sample size here.",
                "The number of samples has some effect on precision when less than 10, but performance stabilizes at around 15 to 20 samples.",
                "We used 30 samples for our experiments.",
                "Much beyond this level, the additional benefits of more samples decrease as the initial score distribution is more closely fit and the processing time increases. 3.6 The effect of resampling on expansion term quality Ideally, a retrieval model should not require a stopword list when estimating a model of relevance: a robust statistical model should down-weight stopwords automatically depending on context.",
                "Stopwords can harm feedback if selected as feedback terms, because they are typically poor discriminators and waste valuable term slots.",
                "In practice, however, because most term selection methods resemble a tf · idf type of weighting, terms with low idf but very high tf can sometimes be selected as expansion term candidates.",
                "This happens, for example, even with the Relevance Model approach that is part of our baseline feedback.",
                "To ensure as strong a baseline as possible, we use a stoplist for all experiments reported here.",
                "If we turn off the stopword list, however, we obtain results such as those shown in Table 5 where four of the top ten baseline feedback terms for TREC topic 60 (said, but, their, not) are stopwords using the BaseFB method. (The top 100 expansion terms were selected to generate this example.)",
                "Indris method attempts to address the stopword problem by applying an initial step based on Ponte [14] to select less-common terms that have high log-odds of being in the top-ranked documents compared to the whole collection.",
                "Nevertheless, this does not overcome the stopword problem completely, especially as the number of feedback terms grows.",
                "Using resampling feedback, however, appears to mitigate Collection QV + Uniform QV + Relevance-score weighting weighting TREC 1&2 AvgP 0.2545 0.2406 (-5.46%) P10 0.5369 0.5263 (-1.97%) RI 0.6212 0.7087 (+14.09%) TREC 7 AvgP 0.2174 0.2169 (-0.23%) P10 0.4320 0.4480 (+3.70%) RI 0.4783 0.5652 (+18.17%) TREC 8 AvgP 0.2267 0.2268 (+0.04%) P10 0.4120 0.4340 (+5.34%) RI 0.4545 0.4545 (+0.00%) wt10g AvgP 0.1808 0.1946 (+7.63%) P10 0.2680 0.2960 (+10.45%) RI 0.0220 0.1099 (+399.5%) Table 4: Comparison of uniform and relevance-weighted document sampling.",
                "The percentage change compared to uniform sampling is shown in parentheses.",
                "QV indicates that query variants were used in both runs.",
                "Baseline FB p(wi|R) Resampling FB p(wi|R) said 0.055 court 0.026 court 0.055 pay 0.018 pay 0.034 federal 0.012 but 0.026 education 0.011 employees 0.024 teachers 0.010 their 0.024 employees 0.010 not 0.023 case 0.010 federal 0.021 their 0.009 workers 0.020 appeals 0.008 education 0.020 union 0.007 Table 5: Feedback term quality when a stoplist is not used.",
                "Feedback terms for TREC topic 60: merit pay vs seniority. the effect of stopwords automatically.",
                "In the example of Table 5, resampling feedback leaves only one stopword (their) in the top ten.",
                "We observed similar feedback term behavior across many other topics.",
                "The reason for this effect appears to be the interaction of the term selection score with the top-m term cutoff.",
                "While the presence and even proportion of particular stopwords is fairly stable across different document samples, their relative position in the top-m list is not, as sets of documents with varying numbers of better, lower-frequency term candidates are examined for each sample.",
                "As a result, while some number of stopwords may appear in each sampled document set, any given stopword tends to fall below the cutoff for multiple samples, leading to its classification as a high-variance, low-weight feature. 4.",
                "RELATED WORK Our approach is related to previous work from several areas of information retrieval and machine learning.",
                "Our use of query variation was inspired by the work of YomTov et al. [20], Carpineto et al. [5], and Amati et al. [2], among others.",
                "These studies use the idea of creating multiple subqueries and then examining the nature of the overlap in the documents and/or expansion terms that result from each subquery.",
                "Model combination is performed using heuristics.",
                "In particular, the studies of Amati et al. and Carpineto et al. investigated combining terms from individual distributional methods using a term-reranking combination heuristic.",
                "In a set of TREC topics they found wide average variation in the rank-distance of terms from different expansion methods.",
                "Their combination method gave modest positive improvements in average precision.",
                "The idea of examining the overlap between lists of suggested terms has also been used in early query expansion approaches.",
                "Xu and Crofts method of Local Context Analysis (LCA) [19] includes a factor in the empirically-derived weighting formula that causes expansion terms to be preferred that have connections to multiple query terms.",
                "On the document side, recent work by Zhou & Croft [21] explored the idea of adding noise to documents, re-scoring them, and using the stability of the resulting rankings as an estimate of query difficulty.",
                "This is related to our use of document sampling to estimate the risk of the feedback model built from the different sets of top-retrieved documents.",
                "Sakai et al. [17] proposed an approach to improving the robustness of pseudo-relevance feedback using a method they call selective sampling.",
                "The essence of their method is that they allow skipping of some top-ranked documents, based on a clustering criterion, in order to select a more varied and novel set of documents later in the ranking for use by a traditional pseudo-feedback method.",
                "Their study did not find significant improvements in either robustness (RI) or MAP on their corpora.",
                "Greiff, Morgan and Ponte [8] explored the role of variance in term weighting.",
                "In a series of simulations that simplified the problem to 2-feature documents, they found that average precision degrades as term frequency variance - high noiseincreases.",
                "Downweighting terms with high variance resulted in improved average precision.",
                "This seems in accord with our own findings for individual feedback models.",
                "Estimates of output variance have recently been used for improved text classification.",
                "Lee et al. [11] used queryspecific variance estimates of classifier outputs to perform improved model combination.",
                "Instead of using sampling, they were able to derive closed-form expressions for classifier variance by assuming base classifiers using simple types of inference networks.",
                "Ando and Zhang proposed a method that they call structural feedback [3] and showed how to apply it to query expansion for the TREC Genomics Track.",
                "They used r query variations to obtain R different sets Sr of top-ranked documents that have been intersected with the top-ranked documents obtained from the original query qorig.",
                "For each Si, the normalized centroid vector ˆwi of the documents is calculated.",
                "Principal component analysis (PCA) is then applied to the ˆwi to obtain the matrix Φ of H left singular vectors φh that are used to obtain the new, expanded query qexp = qorig + ΦT Φqorig. (7) In the case H = 1, we have a single left singular vector φ: qexp = qorig + (φT qorig)φ so that the dot product φT qorig is a type of dynamic weight on the expanded query that is based on the similarity of the original query to the expanded query.",
                "The use of variance as a feedback model quality measure occurs indirectly through the application of PCA.",
                "It would be interesting to study the connections between this approach and our own modelfitting method.",
                "Finally, in language modeling approaches to feedback, Tao and Zhai [18] describe a method for more robust feedback that allows each document to have a different feedback α.",
                "The feedback weights are derived automatically using regularized EM.",
                "A roughly equal balance of query and expansion model is implied by their EM stopping condition.",
                "They propose tailoring the stopping parameter η based on a function of some quality measure of feedback documents. 5.",
                "CONCLUSIONS We have presented a new approach to pseudo-relevance feedback based on document and query sampling.",
                "The use of sampling is a very flexible and powerful device and is motivated by our general desire to extend current models of retrieval by estimating the risk or variance associated with the parameters or output of retrieval processes.",
                "Such variance estimates, for example, may be naturally used in a Bayesian framework for improved model estimation and combination.",
                "Applications such as selective expansion may then be implemented in a principled way.",
                "While our study uses the language modeling approach as a framework for experiments, we make few assumptions about the actual workings of the feedback algorithm.",
                "We believe it is likely that any reasonably effective baseline feedback algorithm would benefit from our approach.",
                "Our results on standard TREC collections show that our framework improves the robustness of a strong baseline feedback method across a variety of collections, without sacrificing average precision.",
                "It also gives small but consistent gains in top10 precision.",
                "In future work, we envision an investigation into how varying the set of sampling methods used and the number of samples controls the trade-off between robustness, accuracy, and efficiency.",
                "Acknowledgements We thank Paul Bennett for valuable discussions related to this work, which was supported by NSF grants #IIS-0534345 and #CNS-0454018, and U.S. Dept. of Education grant #R305G03123.",
                "Any opinions, findings, and conclusions or recommendations expressed in this material are the authors. and do not necessarily reflect those of the sponsors. 6.",
                "REFERENCES [1] The Lemur toolkit for language modeling and retrieval. http://www.lemurproject.org. [2] G. Amati, C. Carpineto, and G. Romano.",
                "Query difficulty, robustness, and selective application of query expansion.",
                "In Proc. of the 25th European Conf. on Information Retrieval (ECIR 2004), pages 127-137. [3] R. K. Ando and T. Zhang.",
                "A high-performance semi-supervised learning method for text chunking.",
                "In Proc. of the 43rd Annual Meeting of the ACL, pages 1-9, June 2005. [4] L. Breiman.",
                "Bagging predictors.",
                "Machine Learning, 24(2):123-140, 1996. [5] C. Carpineto, G. Romano, and V. Giannini.",
                "Improving retrieval feedback with multiple term-ranking function combination.",
                "ACM Trans.",
                "Info.",
                "Systems, 20(3):259 - 290. [6] K. Collins-Thompson, P. Ogilvie, and J. Callan.",
                "Initial results with structured queries and language models on half a terabyte of text.",
                "In Proc. of 2005 Text REtrieval Conference.",
                "NIST Special Publication. [7] R. O. Duda, P. E. Hart, and D. G. Stork.",
                "Pattern Classification.",
                "Wiley and Sons, 2nd edition, 2001. [8] W. R. Greiff, W. T. Morgan, and J. M. Ponte.",
                "The role of variance in term weighting for probabilistic information retrieval.",
                "In Proc. of the 11th Intl.",
                "Conf. on Info. and Knowledge Mgmt. (CIKM 2002), pages 252-259. [9] T. Kohonen, J. Hynninen, J. Kangas, and J. Laaksonen.",
                "SOMPAK: The self-organizing map program package.",
                "Technical Report A31, Helsinki University of Technology, 1996. http://www.cis.hut.fi/research/papers/som tr96.ps.Z. [10] V. Lavrenko.",
                "A Generative Theory of Relevance.",
                "PhD thesis, University of Massachusetts, Amherst, 2004. [11] C.-H. Lee, R. Greiner, and S. Wang.",
                "Using query-specific variance estimates to combine Bayesian classifiers.",
                "In Proc. of the 23rd Intl.",
                "Conf. on Machine Learning (ICML 2006), pages 529-536. [12] D. Metzler and W. B. Croft.",
                "Combining the language model and inference network approaches to retrieval.",
                "Info.",
                "Processing and Mgmt., 40(5):735-750, 2004. [13] T. Minka.",
                "Estimating a Dirichlet distribution.",
                "Technical report, 2000. http://research.microsoft.com/ minka/papers/dirichlet. [14] J. Ponte.",
                "Advances in Information Retrieval, chapter Language models for relevance feedback, pages 73-96. 2000.",
                "W.B.",
                "Croft, ed. [15] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In Proc. of the 1998 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 275-281. [16] J. Rocchio.",
                "The SMART Retrieval System, chapter Relevance Feedback in Information Retrieval, pages 313-323.",
                "Prentice-Hall, 1971.",
                "G. Salton, ed. [17] T. Sakai, T. Manabe, and M. Koyama.",
                "Flexible pseudo-relevance feedback via selective sampling.",
                "ACM Transactions on Asian Language Information Processing (TALIP), 4(2):111-135, 2005. [18] T. Tao and C. Zhai.",
                "Regularized estimation of mixture models for robust pseudo-relevance feedback.",
                "In Proc. of the 2006 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 162-169. [19] J. Xu and W. B. Croft.",
                "Improving the effectiveness of information retrieval with local context analysis.",
                "ACM Trans.",
                "Inf.",
                "Syst., 18(1):79-112, 2000. [20] E. YomTov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty.",
                "In Proc. of the 2005 ACM SIGIR Conf. on Research and Development in Information Retrieval, pages 512-519. [21] Y. Zhou and W. B. Croft.",
                "Ranking robustness: a novel framework to predict query performance.",
                "In Proc. of the 15th ACM Intl.",
                "Conf. on Information and Knowledge Mgmt. (CIKM 2006), pages 567-574."
            ],
            "original_annotated_samples": [
                "Treating the baseline feedback method as a black box, and the output feedback model as a random variable, we estimate a <br>posterior distribution</br> for the feedback model by resampling a given querys top-retrieved documents, using the posterior mean or mode as the enhanced feedback model.",
                "Therefore, we propose a method for estimating uncertainty associated with an individual feedback model in terms of a <br>posterior distribution</br> over language models.",
                "Because of this potential complexity, we do not attempt to derive a <br>posterior distribution</br> in closed form, but instead use simulation.",
                "Although the Dirichlet is a unimodal distribution, and in general quite limited in its expressiveness in the sample space, it is a natural match for the multinomial language model, can be estimated quickly, and can capture the most salient features of confident and uncertain feedback models, such as the overall spread of the distibution. 2.2 Resampling document models We would like an approximation to the <br>posterior distribution</br> of the feedback model LF .",
                "Second, sampling is an effective way to estimate basic properties of the feedback <br>posterior distribution</br>, which can then be used for improved model combination."
            ],
            "translated_annotated_samples": [
                "Tratando el método de retroalimentación de línea base como una caja negra, y el modelo de retroalimentación de salida como una variable aleatoria, estimamos una <br>distribución posterior</br> para el modelo de retroalimentación mediante el remuestreo de los documentos mejor recuperados de una consulta dada, utilizando la media o la moda posterior como el modelo de retroalimentación mejorado.",
                "Por lo tanto, proponemos un método para estimar la incertidumbre asociada con un modelo de retroalimentación individual en términos de una <br>distribución posterior</br> sobre modelos de lenguaje.",
                "Debido a esta complejidad potencial, no intentamos derivar una <br>distribución posterior</br> de forma cerrada, sino que en su lugar utilizamos simulación.",
                "Aunque la distribución de Dirichlet es unimodal y en general bastante limitada en su expresividad en el espacio muestral, es una coincidencia natural para el modelo de lenguaje multinomial, puede ser estimada rápidamente y capturar las características más relevantes de los modelos de retroalimentación confiada e incierta, como la dispersión general de la distribución. 2.2 Re-muestreo de modelos de documentos. Nos gustaría una aproximación a la <br>distribución posterior</br> del modelo de retroalimentación LF.",
                "Segundo, el muestreo es una forma efectiva de estimar propiedades básicas de la <br>distribución posterior</br> de retroalimentación, las cuales pueden ser utilizadas para mejorar la combinación de modelos."
            ],
            "translated_text": "Estimación y Uso de la Incertidumbre en la Retroalimentación de Pseudo-Relevancia Kevyn Collins-Thompson y Jamie Callan Instituto de Tecnologías del Lenguaje Escuela de Ciencias de la Computación Universidad Carnegie Mellon Pittsburgh, PA 15213-8213 EE. UU. {kct | callan}@cs.cmu.edu RESUMEN Los métodos existentes de retroalimentación de pseudo-relevancia suelen realizar un promedio de los documentos mejor clasificados, pero ignoran una dimensión estadística importante: el riesgo o la varianza asociada tanto a los modelos de documentos individuales como a su combinación. Tratando el método de retroalimentación de línea base como una caja negra, y el modelo de retroalimentación de salida como una variable aleatoria, estimamos una <br>distribución posterior</br> para el modelo de retroalimentación mediante el remuestreo de los documentos mejor recuperados de una consulta dada, utilizando la media o la moda posterior como el modelo de retroalimentación mejorado. Luego realizamos la combinación de modelos sobre varios modelos mejorados, cada uno basado en una consulta ligeramente modificada muestreada de la consulta original. Observamos que el remuestreo de documentos ayuda a aumentar la precisión del modelo de retroalimentación individual al eliminar términos de ruido, mientras que el muestreo de la consulta mejora la robustez (rendimiento en el peor de los casos) al enfatizar términos relacionados con múltiples aspectos de la consulta. El resultado es un algoritmo de meta-retroalimentación que es tanto más robusto como más preciso que el método de referencia original. Categorías y Descriptores de Asignaturas: H.3.3 [Recuperación de Información]: Modelos de Recuperación Términos Generales: Algoritmos, Experimentación 1. INTRODUCCIÓN La incertidumbre es una característica inherente de la recuperación de información. No solo no sabemos las consultas que se presentarán a nuestro algoritmo de recuperación con anticipación, sino que la necesidad de información de los usuarios puede ser vaga o estar incompletamente especificada por estas consultas. Incluso si la consulta estuviera perfectamente especificada, el lenguaje en los documentos de la colección es inherentemente complejo y ambiguo, y hacer coincidir dicho lenguaje de manera efectiva es un problema formidable por sí mismo. Con esto en mente, deseamos tratar muchas cantidades importantes calculadas por el sistema de recuperación, ya sea un puntaje de relevancia para un documento, o un peso para un término de expansión de consulta, como variables aleatorias cuyo valor real es incierto pero donde la incertidumbre sobre el valor real puede ser cuantificada reemplazando el valor fijo con una distribución de probabilidad sobre posibles valores. De esta manera, los algoritmos de recuperación pueden intentar cuantificar el riesgo o la incertidumbre asociados con sus clasificaciones de salida, o mejorar la estabilidad o precisión de sus cálculos internos. Los algoritmos actuales para la retroalimentación de pseudo relevancia (PRF) tienden a seguir el mismo método básico ya sea que utilicemos algoritmos basados en espacio vectorial como la fórmula de Rocchio [16], o enfoques más recientes de modelado de lenguaje como Modelos de Relevancia [10]. Primero, se obtiene un conjunto de documentos recuperados en la parte superior a partir de una consulta inicial y se asume que aproxima un conjunto de documentos relevantes. A continuación, se calcula un vector de modelo de retroalimentación único según algún tipo de promedio, centroide o expectativa sobre el conjunto de modelos de documentos posiblemente relevantes. Por ejemplo, los vectores de documentos pueden combinarse con igual ponderación, como en Rocchio, o por probabilidad de consulta, como se puede hacer utilizando el Modelo de Relevancia. El uso de una expectativa es razonable por razones prácticas y teóricas, pero por sí sola ignora información potencialmente valiosa sobre el riesgo del modelo de retroalimentación. Nuestra hipótesis principal en este artículo es que estimar la incertidumbre en la retroalimentación es útil y conduce a modelos de retroalimentación individual más efectivos y a modelos combinados más robustos. Por lo tanto, proponemos un método para estimar la incertidumbre asociada con un modelo de retroalimentación individual en términos de una <br>distribución posterior</br> sobre modelos de lenguaje. Para hacer esto, variamos sistemáticamente las entradas al método de retroalimentación base y ajustamos una distribución de Dirichlet a la salida. Utilizamos la media posterior o el modo como la estimación mejorada del modelo de retroalimentación. Este proceso se muestra en la Figura 1. Como mostraremos más adelante, la media y la moda pueden variar significativamente del modelo de retroalimentación única propuesto por el método de referencia. También realizamos la combinación de modelos utilizando varios modelos de lenguaje de retroalimentación mejorados obtenidos a partir de un pequeño número de nuevas consultas muestreadas de la consulta original. El peso de un modelo combina dos factores complementarios: la probabilidad del modelo de generar la consulta y la varianza del modelo, siendo los modelos con alta varianza los que obtienen un peso menor. Por ejemplo, un vector de parámetros esperado condicionado a la observación de la consulta se forma a partir de los documentos recuperados en la parte superior, que se tratan como cadenas de entrenamiento (ver [10], p. 62). Figura 1: Estimación de la incertidumbre del modelo de retroalimentación para una sola consulta. 2. En las Secciones 2.1-2.5 describimos un método general para estimar una distribución de probabilidad sobre el conjunto de posibles modelos de lenguaje basado en muestreo. En las Secciones 2.6 y 2.7 resumimos cómo se utilizan diferentes muestras de consultas para generar múltiples modelos de retroalimentación, que luego se combinan. 2.1 Modelado de la Incertidumbre de la Retroalimentación Dado una consulta Q y una colección C, asumimos un sistema de recuperación probabilístico que asigna un puntaje de documento de valor real f(D, Q) a cada documento D en C, de manera que el puntaje sea proporcional a la probabilidad estimada de relevancia. No hacemos ninguna otra suposición sobre f(D, Q). La naturaleza de f(D, Q) puede ser compleja: por ejemplo, si el sistema de recuperación admite lenguajes de consulta estructurados [12], entonces f(D, Q) puede representar la salida de una red de inferencia arbitrariamente compleja definida por los operadores de consulta estructurados. En teoría, la función de puntuación puede variar de una consulta a otra, aunque en este estudio por simplicidad mantenemos la misma función de puntuación para todas las consultas. Nuestro método de consulta específico se encuentra en la Sección 3. Tratamos el algoritmo de retroalimentación como una caja negra y asumimos que los insumos del algoritmo de retroalimentación son la consulta original y los documentos recuperados principales correspondientes, a los cuales se les asigna una puntuación a cada documento. Suponemos que la salida del algoritmo de retroalimentación es un vector de pesos de términos que se utilizarán para agregar o reponderar los términos en la representación de la consulta original, con el vector normalizado para formar una distribución de probabilidad. Consideramos las entradas a la caja negra de retroalimentación como variables aleatorias, y analizamos el modelo de retroalimentación como una variable aleatoria que cambia en respuesta a cambios en las entradas. Al igual que la función de puntuación del documento f(D, Q), el algoritmo de retroalimentación puede implementar una fórmula de puntuación compleja y no lineal, por lo que, a medida que sus entradas varían, los modelos de retroalimentación resultantes pueden tener una distribución compleja en el espacio de modelos de retroalimentación (el espacio muestral). Debido a esta complejidad potencial, no intentamos derivar una <br>distribución posterior</br> de forma cerrada, sino que en su lugar utilizamos simulación. Llamamos a esta distribución sobre posibles modelos de retroalimentación la distribución de modelos de retroalimentación. Nuestro objetivo en esta sección es estimar una aproximación útil a la distribución del modelo de retroalimentación. Para un marco específico de experimentos, utilizamos el enfoque de modelado de lenguaje (LM) para la recuperación de información [15]. El puntaje de un documento D con respecto a una consulta Q y una colección C se da por p(Q|D) con respecto a los modelos de lenguaje ˆθQ y ˆθD estimados para la consulta y el documento respectivamente. Denotamos el conjunto de los k documentos mejor recuperados de la colección C en respuesta a Q como DQ(k, C). Para simplificar, asumimos que las consultas y documentos son generados por distribuciones multinomiales cuyos parámetros están representados por modelos de lenguaje unigram. Para incorporar retroalimentación en el enfoque de LM, asumimos un esquema basado en modelos en el que nuestro objetivo es tomar la consulta y los documentos clasificados resultantes DQ(k, C) como entrada, y producir un modelo de lenguaje expandido ˆθE, que luego se interpola con el modelo de consulta original ˆθQ: ˆθNew = (1 − α) · ˆθQ + α · ˆθE (1). Esto incluye la posibilidad de α = 1 donde el modelo de consulta original es completamente reemplazado por el modelo de retroalimentación. Nuestro espacio muestral es el conjunto de todos los posibles modelos de lenguaje LF que pueden ser generados como modelos de retroalimentación. Nuestro enfoque es tomar muestras de este espacio y luego ajustar una distribución a las muestras utilizando máxima verosimilitud. Para simplificar, comenzamos asumiendo que la distribución de retroalimentación latente tiene la forma de una distribución de Dirichlet. Aunque la distribución de Dirichlet es unimodal y en general bastante limitada en su expresividad en el espacio muestral, es una coincidencia natural para el modelo de lenguaje multinomial, puede ser estimada rápidamente y capturar las características más relevantes de los modelos de retroalimentación confiada e incierta, como la dispersión general de la distribución. 2.2 Re-muestreo de modelos de documentos. Nos gustaría una aproximación a la <br>distribución posterior</br> del modelo de retroalimentación LF. Para lograr esto, aplicamos una técnica de simulación ampliamente utilizada llamada muestreo bootstrap ([7], p. 474) en los parámetros de entrada, es decir, el conjunto de documentos recuperados en la parte superior. El muestreo de arranque nos permite simular el efecto aproximado de perturbar los parámetros dentro del algoritmo de retroalimentación de la caja negra al perturbar los insumos de ese algoritmo de manera sistemática, sin hacer suposiciones sobre la naturaleza del algoritmo de retroalimentación. Específicamente, muestreamos k documentos con reemplazo de DQ(k, C), y calculamos un modelo de lenguaje de expansión θb utilizando el método de retroalimentación de caja negra. Repetimos este proceso B veces para obtener un conjunto de B modelos de lenguaje de retroalimentación, a los cuales luego ajustamos una distribución de Dirichlet. Normalmente, B se encuentra en el rango de 20 a 50 muestras, con un rendimiento relativamente estable en este rango. Ten en cuenta que en lugar de tratar cada documento superior como igualmente probable, muestreamos de acuerdo con las probabilidades estimadas de relevancia de cada documento en DQ(k, C). Por lo tanto, es más probable que un documento sea elegido cuanto más alto esté en la clasificación. 2.3 Justificación de un enfoque de muestreo La justificación de nuestro enfoque de muestreo tiene dos partes. Primero, queremos mejorar la calidad de los modelos de retroalimentación individual al suavizar la variación cuando el modelo de retroalimentación base es inestable. En este sentido, nuestro enfoque se asemeja al bagging [4], un enfoque de conjunto que genera múltiples versiones de un predictor haciendo copias de arranque del conjunto de entrenamiento, y luego promedia los predictores (numéricos). En nuestra aplicación, los documentos recuperados en la parte superior pueden ser vistos como una especie de conjunto de entrenamiento ruidoso para la relevancia. Segundo, el muestreo es una forma efectiva de estimar propiedades básicas de la <br>distribución posterior</br> de retroalimentación, las cuales pueden ser utilizadas para mejorar la combinación de modelos. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "enhanced feedback model": {
            "translated_key": "modelo de retroalimentación mejorado",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Estimation and Use of Uncertainty in Pseudo-relevance Feedback Kevyn Collins-Thompson and Jamie Callan Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213-8213 U.S.A. {kct | callan}@cs.cmu.edu ABSTRACT Existing pseudo-relevance feedback methods typically perform averaging over the top-retrieved documents, but ignore an important statistical dimension: the risk or variance associated with either the individual document models, or their combination.",
                "Treating the baseline feedback method as a black box, and the output feedback model as a random variable, we estimate a posterior distribution for the feedback model by resampling a given querys top-retrieved documents, using the posterior mean or mode as the <br>enhanced feedback model</br>.",
                "We then perform model combination over several enhanced models, each based on a slightly modified query sampled from the original query.",
                "We find that resampling documents helps increase individual feedback model precision by removing noise terms, while sampling from the query improves robustness (worst-case performance) by emphasizing terms related to multiple query aspects.",
                "The result is a meta-feedback algorithm that is both more robust and more precise than the original strong baseline method.",
                "Categories and Subject Descriptors: H.3.3 [Information Retrieval]: Retrieval Models General Terms: Algorithms, Experimentation 1.",
                "INTRODUCTION Uncertainty is an inherent feature of information retrieval.",
                "Not only do we not know the queries that will be presented to our retrieval algorithm ahead of time, but the users information need may be vague or incompletely specified by these queries.",
                "Even if the query were perfectly specified, language in the collection documents is inherently complex and ambiguous and matching such language effectively is a formidable problem by itself.",
                "With this in mind, we wish to treat many important quantities calculated by the retrieval system, whether a relevance score for a document, or a weight for a query expansion term, as random variables whose true value is uncertain but where the uncertainty about the true value may be quantified by replacing the fixed value with a probability distribution over possible values.",
                "In this way, retrieval algorithms may attempt to quantify the risk or uncertainty associated with their output rankings, or improve the stability or precision of their internal calculations.",
                "Current algorithms for pseudo-relevance feedback (PRF) tend to follow the same basic method whether we use vector space-based algorithms such as Rocchios formula [16], or more recent language modeling approaches such as Relevance Models [10].",
                "First, a set of top-retrieved documents is obtained from an initial query and assumed to approximate a set of relevant documents.",
                "Next, a single feedback model vector is computed according to some sort of average, centroid, or expectation over the set of possibly-relevant document models.",
                "For example, the document vectors may be combined with equal weighting, as in Rocchio, or by query likelihood, as may be done using the Relevance Model1 .",
                "The use of an expectation is reasonable for practical and theoretical reasons, but by itself ignores potentially valuable information about the risk of the feedback model.",
                "Our main hypothesis in this paper is that estimating the uncertainty in feedback is useful and leads to better individual feedback models and more robust combined models.",
                "Therefore, we propose a method for estimating uncertainty associated with an individual feedback model in terms of a posterior distribution over language models.",
                "To do this, we systematically vary the inputs to the baseline feedback method and fit a Dirichlet distribution to the output.",
                "We use the posterior mean or mode as the improved feedback model estimate.",
                "This process is shown in Figure 1.",
                "As we show later, the mean and mode may vary significantly from the single feedback model proposed by the baseline method.",
                "We also perform model combination using several improved feedback language models obtained by a small number of new queries sampled from the original query.",
                "A models weight combines two complementary factors: the models probability of generating the query, and the variance of the model, with high-variance models getting lower weight. 1 For example, an expected parameter vector conditioned on the query observation is formed from top-retrieved documents, which are treated as training strings (see [10], p. 62).",
                "Figure 1: Estimating the uncertainty of the feedback model for a single query. 2.",
                "SAMPLING-BASED FEEDBACK In Sections 2.1-2.5 we describe a general method for estimating a probability distribution over the set of possible language models.",
                "In Sections 2.6 and 2.7 we summarize how different query samples are used to generate multiple feedback models, which are then combined. 2.1 Modeling Feedback Uncertainty Given a query Q and a collection C, we assume a probabilistic retrieval system that assigns a real-valued document score f(D, Q) to each document D in C, such that the score is proportional to the estimated probability of relevance.",
                "We make no other assumptions about f(D, Q).",
                "The nature of f(D, Q) may be complex: for example, if the retrieval system supports structured query languages [12], then f(D, Q) may represent the output of an arbitrarily complex inference network defined by the structured query operators.",
                "In theory, the scoring function can vary from query to query, although in this study for simplicity we keep the scoring function the same for all queries.",
                "Our specific query method is given in Section 3.",
                "We treat the feedback algorithm as a black box and assume that the inputs to the feedback algorithm are the original query and the corresponding top-retrieved documents, with a score being given to each document.",
                "We assume that the output of the feedback algorithm is a vector of term weights to be used to add or reweight the terms in the representation of the original query, with the vector normalized to form a probability distribution.",
                "We view the the inputs to the feedback black box as random variables, and analyze the feedback model as a random variable that changes in response to changes in the inputs.",
                "Like the document scoring function f(D, Q), the feedback algorithm may implement a complex, non-linear scoring formula, and so as its inputs vary, the resulting feedback models may have a complex distribution over the space of feedback models (the sample space).",
                "Because of this potential complexity, we do not attempt to derive a posterior distribution in closed form, but instead use simulation.",
                "We call this distribution over possible feedback models the feedback model distribution.",
                "Our goal in this section is to estimate a useful approximation to the feedback model distribution.",
                "For a specific framework for experiments, we use the language modeling (LM) approach for information retrieval [15].",
                "The score of a document D with respect to a query Q and collection C is given by p(Q|D) with respect to language models ˆθQ and ˆθD estimated for the query and document respectively.",
                "We denote the set of k top-retrieved documents from collection C in response to Q by DQ(k, C).",
                "For simplicity, we assume that queries and documents are generated by multinomial distributions whose parameters are represented by unigram language models.",
                "To incorporate feedback in the LM approach, we assume a model-based scheme in which our goal is take the query and resulting ranked documents DQ(k, C) as input, and output an expansion language model ˆθE, which is then interpolated with the original query model ˆθQ: ˆθNew = (1 − α) · ˆθQ + α · ˆθE (1) This includes the possibility of α = 1 where the original query mode is completely replaced by the feedback model.",
                "Our sample space is the set of all possible language models LF that may be output as feedback models.",
                "Our approach is to take samples from this space and then fit a distribution to the samples using maximum likelihood.",
                "For simplicity, we start by assuming the latent feedback distribution has the form of a Dirichlet distribution.",
                "Although the Dirichlet is a unimodal distribution, and in general quite limited in its expressiveness in the sample space, it is a natural match for the multinomial language model, can be estimated quickly, and can capture the most salient features of confident and uncertain feedback models, such as the overall spread of the distibution. 2.2 Resampling document models We would like an approximation to the posterior distribution of the feedback model LF .",
                "To accomplish this, we apply a widely-used simulation technique called bootstrap sampling ([7], p. 474) on the input parameters, namely, the set of top-retrieved documents.",
                "Bootstrap sampling allows us to simulate the approximate effect of perturbing the parameters within the black box feedback algorithm by perturbing the inputs to that algorithm in a systematic way, while making no assumptions about the nature of the feedback algorithm.",
                "Specifically, we sample k documents with replacement from DQ(k, C), and calculate an expansion language model θb using the black box feedback method.",
                "We repeat this process B times to obtain a set of B feedback language models, to which we then fit a Dirichlet distribution.",
                "Typically B is in the range of 20 to 50 samples, with performance being relatively stable in this range.",
                "Note that instead of treating each top document as equally likely, we sample according to the estimated probabilities of relevance of each document in DQ(k, C).",
                "Thus, a document is more likely to be chosen the higher it is in the ranking. 2.3 Justification for a sampling approach The rationale for our sampling approach has two parts.",
                "First, we want to improve the quality of individual feedback models by smoothing out variation when the baseline feedback model is unstable.",
                "In this respect, our approach resembles bagging [4], an ensemble approach which generates multiple versions of a predictor by making bootstrap copies of the training set, and then averages the (numerical) predictors.",
                "In our application, top-retrieved documents can be seen as a kind of noisy training set for relevance.",
                "Second, sampling is an effective way to estimate basic properties of the feedback posterior distribution, which can then be used for improved model combination.",
                "For example, a model may be weighted by its prediction confidence, estimated as a function of the variability of the posterior around the model. foo2-401.map-Dim:5434,Size:12*12units,gaussianneighborhood (a) Topic 401 Foreign minorities, Germany foo2-402.map-Dim:5698,Size:12*12units,gaussianneighborhood (b) Topic 402 Behavioral genetics foo2-459.map-Dim:8969,Size:12*12units,gaussianneighborhood (c) Topic 459 When can a lender foreclose on property Figure 2: Visualization of expansion language model variance using self-organizing maps, showing the distribution of language models that results from resampling the inputs to the baseline expansion method.",
                "The language model that would have been chosen by the baseline expansion is at the center of each map.",
                "The similarity function is JensenShannon divergence. 2.4 Visualizing feedback distributions Before describing how we fit and use the Dirichlet distribution over feedback models, it is instructive to view some examples of actual feedback model distributions that result from bootstrap sampling the top-retrieved documents from different TREC topics.",
                "Each point in our sample space is a language model, which typically has several thousand dimensions.",
                "To help analyze the behavior of our method we used a Self-Organizing Map (via the SOM-PAK package [9]), to flatten and visualize the high-dimensional density function2 .",
                "The density maps for three TREC topics are shown in Figure 2 above.",
                "The dark areas represent regions of high similarity between language models.",
                "The light areas represent regions of low similarity - the valleys between clusters.",
                "Each diagram is centered on the language model that would have been chosen by the baseline expansion.",
                "A single peak (mode) is evident in some examples, but more complex structure appears in others.",
                "Also, while the distribution is usually close to the baseline feedback model, for some topics they are a significant distance apart (as measured by JensenShannon divergence), as in Subfigure 2c.",
                "In such cases, the mode or mean of the feedback distribution often performs significantly better than the baseline (and in a smaller proportion of cases, significantly worse). 2.5 Fitting a posterior feedback distribution After obtaining feedback model samples by resampling the feedback model inputs, we estimate the feedback distribution.",
                "We assume that the multinomial feedback models {ˆθ1, . . . , ˆθB} were generated by a latent Dirichlet distribution with parameters {α1, . . . , αN }.",
                "To estimate the {α1, . . . , αN }, we fit the Dirichlet parameters to the B language model samples according to maximum likelihood using a generalized Newton procedure, details of which are given in Minka [13].",
                "We assume a simple Dirichlet prior over the {α1, . . . , αN }, setting each to αi = μ · p(wi | C), where μ is a parameter and p(· | C) is the collection language model estimated from a set of documents from collection C. The parameter fitting converges very quickly - typically just 2 or 2 Because our points are language models in the multinomial simplex, we extended SOM-PAK to support JensenShannon divergence, a widely-used similarity measure between probability distributions. 3 iterations are enough - so that it is practical to apply at query-time when computational overhead must be small.",
                "In practice, we can restrict the calculation to the vocabulary of the top-retrieved documents, instead of the entire collection.",
                "Note that for this step we are re-using the existing retrieved documents and not performing additional queries.",
                "Given the parameters of an N-dimensional Dirichlet distribution Dir(α) the mean μ and mode x vectors are easy to calculate and are given respectively by μi = αiP αi (2) and xi = αi−1P αi−N . (3) We can then choose the language model at the mean or the mode of the posterior as the final <br>enhanced feedback model</br>. (We found the mode to give slightly better performance.)",
                "For information retrieval, the number of samples we will have available is likely to be quite small for performance reasons - usually less than ten.",
                "Moreover, while random sampling is useful in certain cases, it is perfectly acceptable to allow deterministic sampling distributions, but these must be designed carefully in order to approximate an accurate output variance.",
                "We leave this for future study. 2.6 Query variants We use the following methods for generating variants of the original query.",
                "Each variant corresponds to a different assumption about which aspects of the original query may be important.",
                "This is a form of deterministic sampling.",
                "We selected three simple methods that cover complimentary assumptions about the query.",
                "No-expansion Use only the original query.",
                "The assumption is that the given terms are a complete description of the information need.",
                "Leave-one-out A single term is left out of the original query.",
                "The assumption is that one of the query terms is a noise term.",
                "Single-term A single term is chosen from the original query.",
                "This assumes that only one aspect of the query, namely, that represented by the term, is most important.",
                "After generating a variant of the original query, we combine it with the original query using a weight αSUB so that we do not stray too far.",
                "In this study, we set αSUB = 0.5.",
                "For example, using the Indri [12] query language, a leave-oneout variant of the initial query that omits the term ireland for TREC topic 404 is: #weight(0.5 #combine(ireland peace talks) 0.5 #combine(peace talks)) 2.7 Combining enhanced feedback models from multiple query variants When using multiple query variants, the resulting enhanced feedback models are combined using Bayesian model combination.",
                "To do this, we treat each word as an item to be classified as belonging to a relevant or non-relevant class, and derive a class probability for each word by combining the scores from each query variant.",
                "Each score is given by that terms probability in the Dirichlet distribution.",
                "The term scores are weighted by the inverse of the variance of the term in the enhanced feedback models Dirichlet distribution.",
                "The prior probability of a words membership in the relevant class is given by the probability of the original query in the entire enhanced expansion model. 3.",
                "EVALUATION In this section we present results confirming the usefulness of estimating a feedback model distribution from weighted resampling of top-ranked documents, and of combining the feedback models obtained from different small changes in the original query. 3.1 General method We evaluated performance on a total of 350 queries derived from four sets of TREC topics: 51-200 (TREC-1&2), 351-400 (TREC-7), 401-450 (TREC-8), and 451-550 (wt10g, TREC-9&10).",
                "We chose these for their varied content and document properties.",
                "For example, wt10g documents are Web pages with a wide variety of subjects and styles while TREC-1&2 documents are more homogeneous news articles.",
                "Indexing and retrieval was performed using the Indri system in the Lemur toolkit [12] [1].",
                "Our queries were derived from the words in the title field of the TREC topics.",
                "Phrases were not used.",
                "To generate the baseline queries passed to Indri, we wrapped the query terms with Indris #combine operator.",
                "For example, the initial query for topic 404 is: #combine(ireland peace talks) We performed Krovetz stemming for all experiments.",
                "Because we found that the baseline (Indri) expansion method performed better using a stopword list with the feedback model, all experiments used a stoplist of 419 common English words.",
                "However, an interesting side-effect of our resampling approach is that it tends to remove many stopwords from the feedback model, making a stoplist less critical.",
                "This is discussed further in Section 3.6. 3.2 Baseline feedback method For our baseline expansion method, we use an algorithm included in Indri 1.0 as the default expansion method.",
                "This method first selects terms using a log-odds calculation described by Ponte [14], but assigns final term weights using Lavrenkos relevance model[10].",
                "We chose the Indri method because it gives a consistently strong baseline, is based on a language modeling approach, and is simple to experiment with.",
                "In a TREC evaluation using the GOV2 corpus [6], the method was one of the topperforming runs, achieving a 19.8% gain in MAP compared to using unexpanded queries.",
                "In this study, it achieves an average gain in MAP of 17.25% over the four collections.",
                "Indris expansion method first calculates a log-odds ratio o(v) for each potential expansion term v given by o(v) = X D log p(v|D) p(v|C) (4) over all documents D containing v, in collection C. Then, the expansion term candidates are sorted by descending o(v), and the top m are chosen.",
                "Finally, the term weights r(v) used in the expanded query are calculated based on the relevance model r(v) = X D p(q|D)p(v|D) p(v) p(D) (5) The quantity p(q|D) is the probability score assigned to the document in the initial retrieval.",
                "We use Dirichlet smoothing of p(v|D) with μ = 1000.",
                "This relevance model is then combined with the original query using linear interpolation, weighted by a parameter α.",
                "By default we used the top 50 documents for feedback and the top 20 expansion terms, with the feedback interpolation parameter α = 0.5 unless otherwise stated.",
                "For example, the baseline expanded query for topic 404 is: #weight(0.5 #combine(ireland peace talks) 0.5 #weight(0.10 ireland 0.08 peace 0.08 northern ...) 3.3 Expansion performance We measure our feedback algorithms effectiveness by two main criteria: precision, and robustness.",
                "Robustness, and the tradeoff between precision and robustness, is analyzed in Section 3.4.",
                "In this section, we examine average precision and precision in the top 10 documents (P10).",
                "We also include recall at 1,000 documents.",
                "For each query, we obtained a set of B feedback models using the Indri baseline.",
                "Each feedback model was obtained from a random sample of the top k documents taken with replacement.",
                "For these experiments, B = 30 and k = 50.",
                "Each feedback model contained 20 terms.",
                "On the query side, we used leave-one-out (LOO) sampling to create the query variants.",
                "Single-term query sampling had consistently worse performance across all collections and so our results here focus on LOO sampling.",
                "We used the methods described in Section 2 to estimate an <br>enhanced feedback model</br> from the Dirichlet posterior distribution for each query variant, and to combine the feedback models from all the query variants.",
                "We call our method resampling expansion and denote it as RS-FB here.",
                "We denote the Indri baseline feedback method as Base-FB.",
                "Results from applying both the baseline expansion method (Base-FB) and resampling expansion (RS-FB) are shown in Table 1.",
                "We observe several trends in this table.",
                "First, the average precision of RS-FB was comparable to Base-FB, achieving an average gain of 17.6% compared to using no expansion across the four collections.",
                "The Indri baseline expansion gain was 17.25%.",
                "Also, the RS-FB method achieved consistent improvements in P10 over Base-FB for every topic set, with an average improvement of 6.89% over Base-FB for all 350 topics.",
                "The lowest P10 gain over Base-FB was +3.82% for TREC-7 and the highest was +11.95% for wt10g.",
                "Finally, both Base-FB and RS-FB also consistently improved recall over using no expansion, with Base-FB achieving better recall than RS-FB for all topic sets. 3.4 Retrieval robustness We use the term robustness to mean the worst-case average precision performance of a feedback algorithm.",
                "Ideally, a robust feedback method would never perform worse than using the original query, while often performing better using the expansion.",
                "To evaluate robustness in this study, we use a very simple measure called the robustness index (RI)3 .",
                "For a set of queries Q, the RI measure is defined as: RI(Q) = n+ − n− |Q| (6) where n+ is the number of queries helped by the feedback method and n− is the number of queries hurt.",
                "Here, by helped we mean obtaining a higher average precision as a result of feedback.",
                "The value of RI ranges from a minimum 3 This is sometimes also called the reliability of improvement index and was used in Sakai et al. [17].",
                "Collection NoExp Base-FB RS-FB TREC 1&2 AvgP 0.1818 0.2419 (+33.04%) 0.2406 (+32.24%) P10 0.4443 0.4913 (+10.57%) 0.5363 (+17.83%) Recall 15084/37393 19172/37393 15396/37393 TREC 7 AvgP 0.1890 0.2175 (+15.07%) 0.2169 (+14.75%) P10 0.4200 0.4320 (+2.85%) 0.4480 (+6.67%) Recall 2179/4674 2608/4674 2487/4674 TREC 8 AvgP 0.2031 0.2361 (+16.25%) 0.2268 (+11.70%) P10 0.3960 0.4160 (+5.05%) 0.4340 (+9.59%) Recall 2144/4728 2642/4728 2485/4728 wt10g AvgP 0.1741 0.1829 (+5.06%) 0.1946 (+11.78%) P10 0.2760 0.2630 (-4.71%) 0.2960 (+7.24%) Recall 3361/5980 3725/5980 3664/5980 Table 1: Comparison of baseline (Base-FB) feedback and feedback using re-sampling (RS-FB).",
                "Improvement shown for BaseFB and RS-FB is relative to using no expansion. (a) TREC 1&2 (upper curve); TREC 8 (lower curve) (b) TREC 7 (upper curve); wt10g (lower curve) Figure 3: The trade-off between robustness and average precision for different corpora.",
                "The x-axis gives the change in MAP over using baseline expansion with α = 0.5.",
                "The yaxis gives the Robustness Index (RI).",
                "Each curve through uncircled points shows the RI/MAP tradeoff using the simple small-α strategy (see text) as α decreases from 0.5 to zero in the direction of the arrow.",
                "Circled points represent the tradeoffs obtained by resampling feedback for α = 0.5.",
                "Collection N Base-FB RS-FB n− RI n− RI TREC 1&2 103 26 +0.495 15 +0.709 TREC 7 46 14 +0.391 10 +0.565 TREC 8 44 12 +0.455 12 +0.455 wt10g 91 48 -0.055 39 +0.143 Combined 284 100 +0.296 76 +0.465 Table 2: Comparison of robustness index (RI) for baseline feedback (Base-FB) vs. resampling feedback (RS-FB).",
                "Also shown are the actual number of queries hurt by feedback (n−) for each method and collection.",
                "Queries for which initial average precision was negligible (≤ 0.01) were ignored, giving the remaining query count in column N. of −1.0, when all queries are hurt by the feedback method, to +1.0 when all queries are helped.",
                "The RI measure does not take into account the magnitude or distribution of the amount of change across the set Q.",
                "However, it is easy to understand as a general indication of robustness.",
                "One obvious way to improve the worst-case performance of feedback is simply to use a smaller fixed α interpolation parameter, such as α = 0.3, placing less weight on the (possibly risky) feedback model and more on the original query.",
                "We call this the small-α strategy.",
                "Since we are also reducing the potential gains when the feedback model is right, however, we would expect some trade-off between average precision and robustness.",
                "We therefore compared the precision/robustness trade-off between our resampling feedback algorithm, and the simple small-α method.",
                "The results are summarized in Figure 3.",
                "In the figure, the curve for each topic set interpolates between trade-off points, beginning at x=0, where α = 0.5, and continuing in the direction of the arrow as α decreases and the original query is given more and more weight.",
                "As expected, robustness continuously increases as we move along the curve, but mean average precision generally drops as the gains from feedback are eliminated.",
                "For comparison, the performance of resampling feedback at α = 0.5 is shown for each collection as the circled point.",
                "Higher and to the right is better.",
                "This figure shows that resampling feedback gives a somewhat better trade-off than the small-α approach for 3 of the 4 collections.",
                "Figure 4: Histogram showing improved robustness of resampling feedback (RS-FB) over baseline feedback (Base-FB) for all datasets combined.",
                "Queries are binned by % change in AP compared to the unexpanded query.",
                "Collection DS + QV DS + No QV TREC 1&2 AvgP 0.2406 0.2547 (+5.86%) P10 0.5263 0.5362 (+1.88%) RI 0.7087 0.6515 (-0.0572) TREC 7 AvgP 0.2169 0.2200 (+1.43%) P10 0.4480 0.4300 (-4.02%) RI 0.5652 0.2609 (-0.3043) TREC 8 AvgP 0.2268 0.2257 (-0.49%) P10 0.4340 0.4200 (-3.23%) RI 0.4545 0.4091 (-0.0454) wt10g AvgP 0.1946 0.1865 (-4.16%) P10 0.2960 0.2680 (-9.46%) RI 0.1429 0.0220 (-0.1209) Table 3: Comparison of resampling feedback using document sampling (DS) with (QV) and without (No QV) combining feedback models from multiple query variants.",
                "Table 2 gives the Robustness Index scores for Base-FB and RS-FB.",
                "The RS-FB feedback method obtained higher robustness than Base-FB on three of the four topic sets, with only slightly worse performance on TREC-8.",
                "A more detailed view showing the distribution over relative changes in AP is given by the histogram in Figure 4.",
                "Compared to Base-FB, the RS-FB method achieves a noticable reduction in the number of queries significantly hurt by expansion (i.e. where AP is hurt by 25% or more), while preserving positive gains in AP. 3.5 Effect of query and document sampling methods Given our algorithms improved robustness seen in Section 3.4, an important question is what component of our system is responsible.",
                "Is it the use of document re-sampling, the use of multiple query variants, or some other factor?",
                "The results in Table 3 suggest that the model combination based on query variants may be largely account for the improved robustness.",
                "When query variants are turned off and the original query is used by itself with document sampling, there is little net change in average precision, a small decrease in P10 for 3 out of the 4 topic sets, but a significant drop in robustness for all topic sets.",
                "In two cases, the RI measure drops by more than 50%.",
                "We also examined the effect of the document sampling method on retrieval effectiveness, using two different strategies.",
                "The uniform weighting strategy ignored the relevance scores from the initial retrieval and gave each document in the top k the same probability of selection.",
                "In contrast, the relevance-score weighting strategy chose documents with probability proportional to their relevance scores.",
                "In this way, documents that were more highly ranked were more likely to be selected.",
                "Results are shown in Table 4.",
                "The relevance-score weighting strategy performs better overall, with significantly higher RI and P10 scores on 3 of the 4 topic sets.",
                "The difference in average precision between the methods, however, is less marked.",
                "This suggests that uniform weighting acts to increase variance in retrieval results: when initial average precision is high, there are many relevant documents in the top k and uniform sampling may give a more representative relevance model than focusing on the highly-ranked items.",
                "On the other hand, when initial precision is low, there are few relevant documents in the bottom ranks and uniform sampling mixes in more of the non-relevant documents.",
                "For space reasons we only summarize our findings on sample size here.",
                "The number of samples has some effect on precision when less than 10, but performance stabilizes at around 15 to 20 samples.",
                "We used 30 samples for our experiments.",
                "Much beyond this level, the additional benefits of more samples decrease as the initial score distribution is more closely fit and the processing time increases. 3.6 The effect of resampling on expansion term quality Ideally, a retrieval model should not require a stopword list when estimating a model of relevance: a robust statistical model should down-weight stopwords automatically depending on context.",
                "Stopwords can harm feedback if selected as feedback terms, because they are typically poor discriminators and waste valuable term slots.",
                "In practice, however, because most term selection methods resemble a tf · idf type of weighting, terms with low idf but very high tf can sometimes be selected as expansion term candidates.",
                "This happens, for example, even with the Relevance Model approach that is part of our baseline feedback.",
                "To ensure as strong a baseline as possible, we use a stoplist for all experiments reported here.",
                "If we turn off the stopword list, however, we obtain results such as those shown in Table 5 where four of the top ten baseline feedback terms for TREC topic 60 (said, but, their, not) are stopwords using the BaseFB method. (The top 100 expansion terms were selected to generate this example.)",
                "Indris method attempts to address the stopword problem by applying an initial step based on Ponte [14] to select less-common terms that have high log-odds of being in the top-ranked documents compared to the whole collection.",
                "Nevertheless, this does not overcome the stopword problem completely, especially as the number of feedback terms grows.",
                "Using resampling feedback, however, appears to mitigate Collection QV + Uniform QV + Relevance-score weighting weighting TREC 1&2 AvgP 0.2545 0.2406 (-5.46%) P10 0.5369 0.5263 (-1.97%) RI 0.6212 0.7087 (+14.09%) TREC 7 AvgP 0.2174 0.2169 (-0.23%) P10 0.4320 0.4480 (+3.70%) RI 0.4783 0.5652 (+18.17%) TREC 8 AvgP 0.2267 0.2268 (+0.04%) P10 0.4120 0.4340 (+5.34%) RI 0.4545 0.4545 (+0.00%) wt10g AvgP 0.1808 0.1946 (+7.63%) P10 0.2680 0.2960 (+10.45%) RI 0.0220 0.1099 (+399.5%) Table 4: Comparison of uniform and relevance-weighted document sampling.",
                "The percentage change compared to uniform sampling is shown in parentheses.",
                "QV indicates that query variants were used in both runs.",
                "Baseline FB p(wi|R) Resampling FB p(wi|R) said 0.055 court 0.026 court 0.055 pay 0.018 pay 0.034 federal 0.012 but 0.026 education 0.011 employees 0.024 teachers 0.010 their 0.024 employees 0.010 not 0.023 case 0.010 federal 0.021 their 0.009 workers 0.020 appeals 0.008 education 0.020 union 0.007 Table 5: Feedback term quality when a stoplist is not used.",
                "Feedback terms for TREC topic 60: merit pay vs seniority. the effect of stopwords automatically.",
                "In the example of Table 5, resampling feedback leaves only one stopword (their) in the top ten.",
                "We observed similar feedback term behavior across many other topics.",
                "The reason for this effect appears to be the interaction of the term selection score with the top-m term cutoff.",
                "While the presence and even proportion of particular stopwords is fairly stable across different document samples, their relative position in the top-m list is not, as sets of documents with varying numbers of better, lower-frequency term candidates are examined for each sample.",
                "As a result, while some number of stopwords may appear in each sampled document set, any given stopword tends to fall below the cutoff for multiple samples, leading to its classification as a high-variance, low-weight feature. 4.",
                "RELATED WORK Our approach is related to previous work from several areas of information retrieval and machine learning.",
                "Our use of query variation was inspired by the work of YomTov et al. [20], Carpineto et al. [5], and Amati et al. [2], among others.",
                "These studies use the idea of creating multiple subqueries and then examining the nature of the overlap in the documents and/or expansion terms that result from each subquery.",
                "Model combination is performed using heuristics.",
                "In particular, the studies of Amati et al. and Carpineto et al. investigated combining terms from individual distributional methods using a term-reranking combination heuristic.",
                "In a set of TREC topics they found wide average variation in the rank-distance of terms from different expansion methods.",
                "Their combination method gave modest positive improvements in average precision.",
                "The idea of examining the overlap between lists of suggested terms has also been used in early query expansion approaches.",
                "Xu and Crofts method of Local Context Analysis (LCA) [19] includes a factor in the empirically-derived weighting formula that causes expansion terms to be preferred that have connections to multiple query terms.",
                "On the document side, recent work by Zhou & Croft [21] explored the idea of adding noise to documents, re-scoring them, and using the stability of the resulting rankings as an estimate of query difficulty.",
                "This is related to our use of document sampling to estimate the risk of the feedback model built from the different sets of top-retrieved documents.",
                "Sakai et al. [17] proposed an approach to improving the robustness of pseudo-relevance feedback using a method they call selective sampling.",
                "The essence of their method is that they allow skipping of some top-ranked documents, based on a clustering criterion, in order to select a more varied and novel set of documents later in the ranking for use by a traditional pseudo-feedback method.",
                "Their study did not find significant improvements in either robustness (RI) or MAP on their corpora.",
                "Greiff, Morgan and Ponte [8] explored the role of variance in term weighting.",
                "In a series of simulations that simplified the problem to 2-feature documents, they found that average precision degrades as term frequency variance - high noiseincreases.",
                "Downweighting terms with high variance resulted in improved average precision.",
                "This seems in accord with our own findings for individual feedback models.",
                "Estimates of output variance have recently been used for improved text classification.",
                "Lee et al. [11] used queryspecific variance estimates of classifier outputs to perform improved model combination.",
                "Instead of using sampling, they were able to derive closed-form expressions for classifier variance by assuming base classifiers using simple types of inference networks.",
                "Ando and Zhang proposed a method that they call structural feedback [3] and showed how to apply it to query expansion for the TREC Genomics Track.",
                "They used r query variations to obtain R different sets Sr of top-ranked documents that have been intersected with the top-ranked documents obtained from the original query qorig.",
                "For each Si, the normalized centroid vector ˆwi of the documents is calculated.",
                "Principal component analysis (PCA) is then applied to the ˆwi to obtain the matrix Φ of H left singular vectors φh that are used to obtain the new, expanded query qexp = qorig + ΦT Φqorig. (7) In the case H = 1, we have a single left singular vector φ: qexp = qorig + (φT qorig)φ so that the dot product φT qorig is a type of dynamic weight on the expanded query that is based on the similarity of the original query to the expanded query.",
                "The use of variance as a feedback model quality measure occurs indirectly through the application of PCA.",
                "It would be interesting to study the connections between this approach and our own modelfitting method.",
                "Finally, in language modeling approaches to feedback, Tao and Zhai [18] describe a method for more robust feedback that allows each document to have a different feedback α.",
                "The feedback weights are derived automatically using regularized EM.",
                "A roughly equal balance of query and expansion model is implied by their EM stopping condition.",
                "They propose tailoring the stopping parameter η based on a function of some quality measure of feedback documents. 5.",
                "CONCLUSIONS We have presented a new approach to pseudo-relevance feedback based on document and query sampling.",
                "The use of sampling is a very flexible and powerful device and is motivated by our general desire to extend current models of retrieval by estimating the risk or variance associated with the parameters or output of retrieval processes.",
                "Such variance estimates, for example, may be naturally used in a Bayesian framework for improved model estimation and combination.",
                "Applications such as selective expansion may then be implemented in a principled way.",
                "While our study uses the language modeling approach as a framework for experiments, we make few assumptions about the actual workings of the feedback algorithm.",
                "We believe it is likely that any reasonably effective baseline feedback algorithm would benefit from our approach.",
                "Our results on standard TREC collections show that our framework improves the robustness of a strong baseline feedback method across a variety of collections, without sacrificing average precision.",
                "It also gives small but consistent gains in top10 precision.",
                "In future work, we envision an investigation into how varying the set of sampling methods used and the number of samples controls the trade-off between robustness, accuracy, and efficiency.",
                "Acknowledgements We thank Paul Bennett for valuable discussions related to this work, which was supported by NSF grants #IIS-0534345 and #CNS-0454018, and U.S. Dept. of Education grant #R305G03123.",
                "Any opinions, findings, and conclusions or recommendations expressed in this material are the authors. and do not necessarily reflect those of the sponsors. 6.",
                "REFERENCES [1] The Lemur toolkit for language modeling and retrieval. http://www.lemurproject.org. [2] G. Amati, C. Carpineto, and G. Romano.",
                "Query difficulty, robustness, and selective application of query expansion.",
                "In Proc. of the 25th European Conf. on Information Retrieval (ECIR 2004), pages 127-137. [3] R. K. Ando and T. Zhang.",
                "A high-performance semi-supervised learning method for text chunking.",
                "In Proc. of the 43rd Annual Meeting of the ACL, pages 1-9, June 2005. [4] L. Breiman.",
                "Bagging predictors.",
                "Machine Learning, 24(2):123-140, 1996. [5] C. Carpineto, G. Romano, and V. Giannini.",
                "Improving retrieval feedback with multiple term-ranking function combination.",
                "ACM Trans.",
                "Info.",
                "Systems, 20(3):259 - 290. [6] K. Collins-Thompson, P. Ogilvie, and J. Callan.",
                "Initial results with structured queries and language models on half a terabyte of text.",
                "In Proc. of 2005 Text REtrieval Conference.",
                "NIST Special Publication. [7] R. O. Duda, P. E. Hart, and D. G. Stork.",
                "Pattern Classification.",
                "Wiley and Sons, 2nd edition, 2001. [8] W. R. Greiff, W. T. Morgan, and J. M. Ponte.",
                "The role of variance in term weighting for probabilistic information retrieval.",
                "In Proc. of the 11th Intl.",
                "Conf. on Info. and Knowledge Mgmt. (CIKM 2002), pages 252-259. [9] T. Kohonen, J. Hynninen, J. Kangas, and J. Laaksonen.",
                "SOMPAK: The self-organizing map program package.",
                "Technical Report A31, Helsinki University of Technology, 1996. http://www.cis.hut.fi/research/papers/som tr96.ps.Z. [10] V. Lavrenko.",
                "A Generative Theory of Relevance.",
                "PhD thesis, University of Massachusetts, Amherst, 2004. [11] C.-H. Lee, R. Greiner, and S. Wang.",
                "Using query-specific variance estimates to combine Bayesian classifiers.",
                "In Proc. of the 23rd Intl.",
                "Conf. on Machine Learning (ICML 2006), pages 529-536. [12] D. Metzler and W. B. Croft.",
                "Combining the language model and inference network approaches to retrieval.",
                "Info.",
                "Processing and Mgmt., 40(5):735-750, 2004. [13] T. Minka.",
                "Estimating a Dirichlet distribution.",
                "Technical report, 2000. http://research.microsoft.com/ minka/papers/dirichlet. [14] J. Ponte.",
                "Advances in Information Retrieval, chapter Language models for relevance feedback, pages 73-96. 2000.",
                "W.B.",
                "Croft, ed. [15] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In Proc. of the 1998 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 275-281. [16] J. Rocchio.",
                "The SMART Retrieval System, chapter Relevance Feedback in Information Retrieval, pages 313-323.",
                "Prentice-Hall, 1971.",
                "G. Salton, ed. [17] T. Sakai, T. Manabe, and M. Koyama.",
                "Flexible pseudo-relevance feedback via selective sampling.",
                "ACM Transactions on Asian Language Information Processing (TALIP), 4(2):111-135, 2005. [18] T. Tao and C. Zhai.",
                "Regularized estimation of mixture models for robust pseudo-relevance feedback.",
                "In Proc. of the 2006 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 162-169. [19] J. Xu and W. B. Croft.",
                "Improving the effectiveness of information retrieval with local context analysis.",
                "ACM Trans.",
                "Inf.",
                "Syst., 18(1):79-112, 2000. [20] E. YomTov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty.",
                "In Proc. of the 2005 ACM SIGIR Conf. on Research and Development in Information Retrieval, pages 512-519. [21] Y. Zhou and W. B. Croft.",
                "Ranking robustness: a novel framework to predict query performance.",
                "In Proc. of the 15th ACM Intl.",
                "Conf. on Information and Knowledge Mgmt. (CIKM 2006), pages 567-574."
            ],
            "original_annotated_samples": [
                "Treating the baseline feedback method as a black box, and the output feedback model as a random variable, we estimate a posterior distribution for the feedback model by resampling a given querys top-retrieved documents, using the posterior mean or mode as the <br>enhanced feedback model</br>.",
                "Given the parameters of an N-dimensional Dirichlet distribution Dir(α) the mean μ and mode x vectors are easy to calculate and are given respectively by μi = αiP αi (2) and xi = αi−1P αi−N . (3) We can then choose the language model at the mean or the mode of the posterior as the final <br>enhanced feedback model</br>. (We found the mode to give slightly better performance.)",
                "We used the methods described in Section 2 to estimate an <br>enhanced feedback model</br> from the Dirichlet posterior distribution for each query variant, and to combine the feedback models from all the query variants."
            ],
            "translated_annotated_samples": [
                "Tratando el método de retroalimentación de línea base como una caja negra, y el modelo de retroalimentación de salida como una variable aleatoria, estimamos una distribución posterior para el modelo de retroalimentación mediante el remuestreo de los documentos mejor recuperados de una consulta dada, utilizando la media o la moda posterior como el <br>modelo de retroalimentación mejorado</br>.",
                "Dado los parámetros de una distribución de Dirichlet N-dimensional Dir(α), los vectores de media μ y moda x son fáciles de calcular y se dan respectivamente por μi = αiP αi (2) y xi = αi−1P αi−N . (3) Luego podemos elegir el modelo de lenguaje en la media o en la moda del posterior como el <br>modelo de retroalimentación mejorado</br> final. (Encontramos que la moda proporciona un rendimiento ligeramente mejor).",
                "Utilizamos los métodos descritos en la Sección 2 para estimar un <br>modelo de retroalimentación mejorado</br> a partir de la distribución posterior de Dirichlet para cada variante de consulta, y para combinar los modelos de retroalimentación de todas las variantes de consulta."
            ],
            "translated_text": "Estimación y Uso de la Incertidumbre en la Retroalimentación de Pseudo-Relevancia Kevyn Collins-Thompson y Jamie Callan Instituto de Tecnologías del Lenguaje Escuela de Ciencias de la Computación Universidad Carnegie Mellon Pittsburgh, PA 15213-8213 EE. UU. {kct | callan}@cs.cmu.edu RESUMEN Los métodos existentes de retroalimentación de pseudo-relevancia suelen realizar un promedio de los documentos mejor clasificados, pero ignoran una dimensión estadística importante: el riesgo o la varianza asociada tanto a los modelos de documentos individuales como a su combinación. Tratando el método de retroalimentación de línea base como una caja negra, y el modelo de retroalimentación de salida como una variable aleatoria, estimamos una distribución posterior para el modelo de retroalimentación mediante el remuestreo de los documentos mejor recuperados de una consulta dada, utilizando la media o la moda posterior como el <br>modelo de retroalimentación mejorado</br>. Luego realizamos la combinación de modelos sobre varios modelos mejorados, cada uno basado en una consulta ligeramente modificada muestreada de la consulta original. Observamos que el remuestreo de documentos ayuda a aumentar la precisión del modelo de retroalimentación individual al eliminar términos de ruido, mientras que el muestreo de la consulta mejora la robustez (rendimiento en el peor de los casos) al enfatizar términos relacionados con múltiples aspectos de la consulta. El resultado es un algoritmo de meta-retroalimentación que es tanto más robusto como más preciso que el método de referencia original. Categorías y Descriptores de Asignaturas: H.3.3 [Recuperación de Información]: Modelos de Recuperación Términos Generales: Algoritmos, Experimentación 1. INTRODUCCIÓN La incertidumbre es una característica inherente de la recuperación de información. No solo no sabemos las consultas que se presentarán a nuestro algoritmo de recuperación con anticipación, sino que la necesidad de información de los usuarios puede ser vaga o estar incompletamente especificada por estas consultas. Incluso si la consulta estuviera perfectamente especificada, el lenguaje en los documentos de la colección es inherentemente complejo y ambiguo, y hacer coincidir dicho lenguaje de manera efectiva es un problema formidable por sí mismo. Con esto en mente, deseamos tratar muchas cantidades importantes calculadas por el sistema de recuperación, ya sea un puntaje de relevancia para un documento, o un peso para un término de expansión de consulta, como variables aleatorias cuyo valor real es incierto pero donde la incertidumbre sobre el valor real puede ser cuantificada reemplazando el valor fijo con una distribución de probabilidad sobre posibles valores. De esta manera, los algoritmos de recuperación pueden intentar cuantificar el riesgo o la incertidumbre asociados con sus clasificaciones de salida, o mejorar la estabilidad o precisión de sus cálculos internos. Los algoritmos actuales para la retroalimentación de pseudo relevancia (PRF) tienden a seguir el mismo método básico ya sea que utilicemos algoritmos basados en espacio vectorial como la fórmula de Rocchio [16], o enfoques más recientes de modelado de lenguaje como Modelos de Relevancia [10]. Primero, se obtiene un conjunto de documentos recuperados en la parte superior a partir de una consulta inicial y se asume que aproxima un conjunto de documentos relevantes. A continuación, se calcula un vector de modelo de retroalimentación único según algún tipo de promedio, centroide o expectativa sobre el conjunto de modelos de documentos posiblemente relevantes. Por ejemplo, los vectores de documentos pueden combinarse con igual ponderación, como en Rocchio, o por probabilidad de consulta, como se puede hacer utilizando el Modelo de Relevancia. El uso de una expectativa es razonable por razones prácticas y teóricas, pero por sí sola ignora información potencialmente valiosa sobre el riesgo del modelo de retroalimentación. Nuestra hipótesis principal en este artículo es que estimar la incertidumbre en la retroalimentación es útil y conduce a modelos de retroalimentación individual más efectivos y a modelos combinados más robustos. Por lo tanto, proponemos un método para estimar la incertidumbre asociada con un modelo de retroalimentación individual en términos de una distribución posterior sobre modelos de lenguaje. Para hacer esto, variamos sistemáticamente las entradas al método de retroalimentación base y ajustamos una distribución de Dirichlet a la salida. Utilizamos la media posterior o el modo como la estimación mejorada del modelo de retroalimentación. Este proceso se muestra en la Figura 1. Como mostraremos más adelante, la media y la moda pueden variar significativamente del modelo de retroalimentación única propuesto por el método de referencia. También realizamos la combinación de modelos utilizando varios modelos de lenguaje de retroalimentación mejorados obtenidos a partir de un pequeño número de nuevas consultas muestreadas de la consulta original. El peso de un modelo combina dos factores complementarios: la probabilidad del modelo de generar la consulta y la varianza del modelo, siendo los modelos con alta varianza los que obtienen un peso menor. Por ejemplo, un vector de parámetros esperado condicionado a la observación de la consulta se forma a partir de los documentos recuperados en la parte superior, que se tratan como cadenas de entrenamiento (ver [10], p. 62). Figura 1: Estimación de la incertidumbre del modelo de retroalimentación para una sola consulta. 2. En las Secciones 2.1-2.5 describimos un método general para estimar una distribución de probabilidad sobre el conjunto de posibles modelos de lenguaje basado en muestreo. En las Secciones 2.6 y 2.7 resumimos cómo se utilizan diferentes muestras de consultas para generar múltiples modelos de retroalimentación, que luego se combinan. 2.1 Modelado de la Incertidumbre de la Retroalimentación Dado una consulta Q y una colección C, asumimos un sistema de recuperación probabilístico que asigna un puntaje de documento de valor real f(D, Q) a cada documento D en C, de manera que el puntaje sea proporcional a la probabilidad estimada de relevancia. No hacemos ninguna otra suposición sobre f(D, Q). La naturaleza de f(D, Q) puede ser compleja: por ejemplo, si el sistema de recuperación admite lenguajes de consulta estructurados [12], entonces f(D, Q) puede representar la salida de una red de inferencia arbitrariamente compleja definida por los operadores de consulta estructurados. En teoría, la función de puntuación puede variar de una consulta a otra, aunque en este estudio por simplicidad mantenemos la misma función de puntuación para todas las consultas. Nuestro método de consulta específico se encuentra en la Sección 3. Tratamos el algoritmo de retroalimentación como una caja negra y asumimos que los insumos del algoritmo de retroalimentación son la consulta original y los documentos recuperados principales correspondientes, a los cuales se les asigna una puntuación a cada documento. Suponemos que la salida del algoritmo de retroalimentación es un vector de pesos de términos que se utilizarán para agregar o reponderar los términos en la representación de la consulta original, con el vector normalizado para formar una distribución de probabilidad. Consideramos las entradas a la caja negra de retroalimentación como variables aleatorias, y analizamos el modelo de retroalimentación como una variable aleatoria que cambia en respuesta a cambios en las entradas. Al igual que la función de puntuación del documento f(D, Q), el algoritmo de retroalimentación puede implementar una fórmula de puntuación compleja y no lineal, por lo que, a medida que sus entradas varían, los modelos de retroalimentación resultantes pueden tener una distribución compleja en el espacio de modelos de retroalimentación (el espacio muestral). Debido a esta complejidad potencial, no intentamos derivar una distribución posterior de forma cerrada, sino que en su lugar utilizamos simulación. Llamamos a esta distribución sobre posibles modelos de retroalimentación la distribución de modelos de retroalimentación. Nuestro objetivo en esta sección es estimar una aproximación útil a la distribución del modelo de retroalimentación. Para un marco específico de experimentos, utilizamos el enfoque de modelado de lenguaje (LM) para la recuperación de información [15]. El puntaje de un documento D con respecto a una consulta Q y una colección C se da por p(Q|D) con respecto a los modelos de lenguaje ˆθQ y ˆθD estimados para la consulta y el documento respectivamente. Denotamos el conjunto de los k documentos mejor recuperados de la colección C en respuesta a Q como DQ(k, C). Para simplificar, asumimos que las consultas y documentos son generados por distribuciones multinomiales cuyos parámetros están representados por modelos de lenguaje unigram. Para incorporar retroalimentación en el enfoque de LM, asumimos un esquema basado en modelos en el que nuestro objetivo es tomar la consulta y los documentos clasificados resultantes DQ(k, C) como entrada, y producir un modelo de lenguaje expandido ˆθE, que luego se interpola con el modelo de consulta original ˆθQ: ˆθNew = (1 − α) · ˆθQ + α · ˆθE (1). Esto incluye la posibilidad de α = 1 donde el modelo de consulta original es completamente reemplazado por el modelo de retroalimentación. Nuestro espacio muestral es el conjunto de todos los posibles modelos de lenguaje LF que pueden ser generados como modelos de retroalimentación. Nuestro enfoque es tomar muestras de este espacio y luego ajustar una distribución a las muestras utilizando máxima verosimilitud. Para simplificar, comenzamos asumiendo que la distribución de retroalimentación latente tiene la forma de una distribución de Dirichlet. Aunque la distribución de Dirichlet es unimodal y en general bastante limitada en su expresividad en el espacio muestral, es una coincidencia natural para el modelo de lenguaje multinomial, puede ser estimada rápidamente y capturar las características más relevantes de los modelos de retroalimentación confiada e incierta, como la dispersión general de la distribución. 2.2 Re-muestreo de modelos de documentos. Nos gustaría una aproximación a la distribución posterior del modelo de retroalimentación LF. Para lograr esto, aplicamos una técnica de simulación ampliamente utilizada llamada muestreo bootstrap ([7], p. 474) en los parámetros de entrada, es decir, el conjunto de documentos recuperados en la parte superior. El muestreo de arranque nos permite simular el efecto aproximado de perturbar los parámetros dentro del algoritmo de retroalimentación de la caja negra al perturbar los insumos de ese algoritmo de manera sistemática, sin hacer suposiciones sobre la naturaleza del algoritmo de retroalimentación. Específicamente, muestreamos k documentos con reemplazo de DQ(k, C), y calculamos un modelo de lenguaje de expansión θb utilizando el método de retroalimentación de caja negra. Repetimos este proceso B veces para obtener un conjunto de B modelos de lenguaje de retroalimentación, a los cuales luego ajustamos una distribución de Dirichlet. Normalmente, B se encuentra en el rango de 20 a 50 muestras, con un rendimiento relativamente estable en este rango. Ten en cuenta que en lugar de tratar cada documento superior como igualmente probable, muestreamos de acuerdo con las probabilidades estimadas de relevancia de cada documento en DQ(k, C). Por lo tanto, es más probable que un documento sea elegido cuanto más alto esté en la clasificación. 2.3 Justificación de un enfoque de muestreo La justificación de nuestro enfoque de muestreo tiene dos partes. Primero, queremos mejorar la calidad de los modelos de retroalimentación individual al suavizar la variación cuando el modelo de retroalimentación base es inestable. En este sentido, nuestro enfoque se asemeja al bagging [4], un enfoque de conjunto que genera múltiples versiones de un predictor haciendo copias de arranque del conjunto de entrenamiento, y luego promedia los predictores (numéricos). En nuestra aplicación, los documentos recuperados en la parte superior pueden ser vistos como una especie de conjunto de entrenamiento ruidoso para la relevancia. Segundo, el muestreo es una forma efectiva de estimar propiedades básicas de la distribución posterior de retroalimentación, las cuales pueden ser utilizadas para mejorar la combinación de modelos. Por ejemplo, un modelo puede ser ponderado por su confianza de predicción, estimada como una función de la variabilidad del posterior alrededor del modelo. foo2-401.map-Dim:5434,Size:12*12units, vecindario gaussiano (a) Tema 401 Minorías extranjeras, Alemania foo2-402.map-Dim:5698,Size:12*12units, vecindario gaussiano (b) Tema 402 Genética del comportamiento foo2-459.map-Dim:8969,Size:12*12units, vecindario gaussiano (c) Tema 459 ¿Cuándo puede un prestamista ejecutar una hipoteca sobre una propiedad? Figura 2: Visualización de la varianza del modelo de lenguaje de expansión utilizando mapas autoorganizados, mostrando la distribución de modelos de lenguaje que resulta de remuestrear las entradas al método de expansión de referencia. El modelo de lenguaje que habría sido elegido por la expansión de la línea base se encuentra en el centro de cada mapa. La función de similitud es la divergencia JensenShannon. 2.4 Visualización de distribuciones de retroalimentación Antes de describir cómo ajustamos y utilizamos la distribución de Dirichlet sobre modelos de retroalimentación, es instructivo ver algunos ejemplos de distribuciones de modelos de retroalimentación reales que resultan de muestrear por bootstrap los documentos más recuperados de diferentes temas de TREC. Cada punto en nuestro espacio muestral es un modelo de lenguaje, que típicamente tiene varias miles de dimensiones. Para ayudar a analizar el comportamiento de nuestro método, utilizamos un Mapa Auto-organizado (a través del paquete SOM-PAK [9]), para aplanar y visualizar la función de densidad de alta dimensión. Los mapas de densidad para tres temas de TREC se muestran en la Figura 2 arriba. Las áreas oscuras representan regiones de alta similitud entre los modelos de lenguaje. Las áreas claras representan regiones de baja similitud, los valles entre los grupos. Cada diagrama está centrado en el modelo de lenguaje que habría sido elegido por la expansión de referencia. Un solo pico (modo) es evidente en algunos ejemplos, pero en otros aparece una estructura más compleja. Además, aunque la distribución suele estar cerca del modelo de retroalimentación de referencia, para algunos temas están a una distancia significativa (según la divergencia de JensenShannon), como se muestra en la Subfigura 2c. En tales casos, la moda o la media de la distribución de retroalimentación a menudo funcionan significativamente mejor que el valor base (y en una proporción menor de casos, significativamente peor). 2.5 Ajuste de una distribución de retroalimentación posterior Después de obtener muestras del modelo de retroalimentación mediante el remuestreo de las entradas del modelo de retroalimentación, estimamos la distribución de retroalimentación. Suponemos que los modelos de retroalimentación multinomial {ˆθ1, . . . , ˆθB} fueron generados por una distribución de Dirichlet latente con parámetros {α1, . . . , αN}. Para estimar los {α1, . . . , αN }, ajustamos los parámetros de Dirichlet a las muestras del modelo de lenguaje B según la máxima verosimilitud utilizando un procedimiento de Newton generalizado, cuyos detalles se encuentran en Minka [13]. Suponemos una distribución de Dirichlet simple sobre los {α1, . . . , αN}, estableciendo cada uno como αi = μ · p(wi | C), donde μ es un parámetro y p(· | C) es el modelo de lenguaje de colección estimado a partir de un conjunto de documentos de la colección C. El ajuste de parámetros converge muy rápidamente, generalmente en solo 2 o 3 iteraciones. Dado que nuestros puntos son modelos de lenguaje en el simplex multinomial, extendimos SOM-PAK para admitir la divergencia de Jensen-Shannon, una medida de similitud ampliamente utilizada entre distribuciones de probabilidad. 3 iteraciones son suficientes, por lo que es práctico aplicarlo en tiempo de consulta cuando la sobrecarga computacional debe ser pequeña. En la práctica, podemos restringir el cálculo al vocabulario de los documentos recuperados en la parte superior, en lugar de toda la colección. Ten en cuenta que para este paso estamos reutilizando los documentos recuperados existentes y no realizando consultas adicionales. Dado los parámetros de una distribución de Dirichlet N-dimensional Dir(α), los vectores de media μ y moda x son fáciles de calcular y se dan respectivamente por μi = αiP αi (2) y xi = αi−1P αi−N . (3) Luego podemos elegir el modelo de lenguaje en la media o en la moda del posterior como el <br>modelo de retroalimentación mejorado</br> final. (Encontramos que la moda proporciona un rendimiento ligeramente mejor). Para la recuperación de información, es probable que el número de muestras disponibles sea bastante pequeño por razones de rendimiento, generalmente menos de diez. Además, si bien el muestreo aleatorio es útil en ciertos casos, es perfectamente aceptable permitir distribuciones de muestreo determinísticas, pero estas deben diseñarse cuidadosamente para aproximar una varianza de salida precisa. Dejamos esto para estudios futuros. 2.6 Variantes de consulta Utilizamos los siguientes métodos para generar variantes de la consulta original. Cada variante corresponde a una suposición diferente sobre qué aspectos de la consulta original pueden ser importantes. Esta es una forma de muestreo determinístico. Seleccionamos tres métodos simples que cubren suposiciones complementarias sobre la consulta. Uso sin expansión. La suposición es que los términos dados son una descripción completa de la necesidad de información. Deja uno fuera. Se deja fuera un término del query original. La suposición es que uno de los términos de la consulta es un término de ruido. Se elige un término único de la consulta original. Esto asume que solo un aspecto de la consulta, es decir, el representado por el término, es el más importante. Después de generar una variante de la consulta original, la combinamos con la consulta original utilizando un peso αSUB para no alejarnos demasiado. En este estudio, establecimos αSUB = 0.5. Por ejemplo, utilizando el lenguaje de consulta Indri [12], una variante de dejar uno fuera de la consulta inicial que omite el término \"ireland\" para el tema TREC 404 es: #weight(0.5 #combine(ireland peace talks) 0.5 #combine(peace talks)) 2.7 Combinando modelos de retroalimentación mejorados de múltiples variantes de consulta. Cuando se utilizan múltiples variantes de consulta, los modelos de retroalimentación mejorados resultantes se combinan utilizando la combinación de modelos bayesianos. Para hacer esto, tratamos cada palabra como un elemento a clasificar como perteneciente a una clase relevante o no relevante, y derivamos una probabilidad de clase para cada palabra combinando las puntuaciones de cada variante de consulta. Cada puntuación es dada por la probabilidad de ese término en la distribución de Dirichlet. Las puntuaciones de los términos se ponderan por el inverso de la varianza del término en los modelos de retroalimentación mejorada de la distribución de Dirichlet. La probabilidad previa de pertenencia de una palabra a la clase relevante se da por la probabilidad de la consulta original en todo el modelo de expansión mejorado. 3. EVALUACIÓN En esta sección presentamos resultados que confirman la utilidad de estimar una distribución del modelo de retroalimentación a partir del remuestreo ponderado de documentos mejor clasificados, y de combinar los modelos de retroalimentación obtenidos a partir de diferentes pequeños cambios en la consulta original. 3.1 Método general Evaluamos el rendimiento en un total de 350 consultas derivadas de cuatro conjuntos de temas de TREC: 51-200 (TREC-1&2), 351-400 (TREC-7), 401-450 (TREC-8) y 451-550 (wt10g, TREC-9&10). Elegimos estos por su contenido variado y propiedades del documento. Por ejemplo, los documentos wt10g son páginas web con una amplia variedad de temas y estilos, mientras que los documentos TREC-1&2 son artículos de noticias más homogéneos. La indexación y recuperación se realizó utilizando el sistema Indri en el kit de herramientas Lemur [12] [1]. Nuestras consultas se derivaron de las palabras en el campo del título de los temas de TREC. Las frases no fueron utilizadas. Para generar las consultas base que se pasaron a Indri, envolvimos los términos de la consulta con el operador #combine de Indri. Por ejemplo, la consulta inicial para el tema 404 es: #combine(ireland peace talks) Realizamos el stemming de Krovetz para todos los experimentos. Debido a que encontramos que el método de expansión de la base (Indri) funcionó mejor utilizando una lista de palabras vacías con el modelo de retroalimentación, todos los experimentos utilizaron una lista de 419 palabras comunes en inglés. Sin embargo, un efecto secundario interesante de nuestro enfoque de remuestreo es que tiende a eliminar muchas palabras vacías del modelo de retroalimentación, lo que hace que una lista de paradas sea menos crítica. Esto se discute más a fondo en la Sección 3.6. Método de retroalimentación de línea base 3.2 Para nuestro método de expansión de línea base, utilizamos un algoritmo incluido en Indri 1.0 como el método de expansión predeterminado. Este método primero selecciona términos utilizando un cálculo de logaritmo de probabilidades descrito por Ponte [14], pero asigna pesos finales a los términos utilizando el modelo de relevancia de Lavrenko [10]. Elegimos el método Indri porque proporciona una línea base consistentemente sólida, se basa en un enfoque de modelado del lenguaje y es fácil de experimentar con él. En una evaluación de TREC utilizando el corpus GOV2, el método fue uno de los mejores resultados, logrando un aumento del 19.8% en el MAP en comparación con el uso de consultas no expandidas. En este estudio, se logra una ganancia promedio en MAP del 17.25% en las cuatro colecciones. El método de expansión de Indri primero calcula una razón de logaritmos de probabilidades o(v) para cada término de expansión potencial v dado por o(v) = X D log p(v|D) p(v|C) (4) sobre todos los documentos D que contienen v, en la colección C. Luego, los candidatos a términos de expansión se ordenan por o(v) descendente, y se eligen los primeros m. Finalmente, los pesos de término r(v) utilizados en la consulta ampliada se calculan en base al modelo de relevancia r(v) = X D p(q|D)p(v|D) p(v) p(D) (5). La cantidad p(q|D) es la puntuación de probabilidad asignada al documento en la recuperación inicial. Utilizamos suavizado de Dirichlet de p(v|D) con μ = 1000. Este modelo de relevancia se combina luego con la consulta original utilizando interpolación lineal, ponderada por un parámetro α. Por defecto, utilizamos los 50 documentos principales para la retroalimentación y los 20 términos de expansión, con el parámetro de interpolación de retroalimentación α = 0.5 a menos que se indique lo contrario. Por ejemplo, la consulta expandida base para el tema 404 es: #weight(0.5 #combine(ireland peace talks) 0.5 #weight(0.10 ireland 0.08 peace 0.08 northern ...) 3.3 Rendimiento de la expansión. Medimos la efectividad de nuestros algoritmos de retroalimentación mediante dos criterios principales: precisión y robustez. La robustez, y el equilibrio entre precisión y robustez, se analiza en la Sección 3.4. En esta sección, examinamos la precisión promedio y la precisión en los 10 documentos principales (P10). También incluimos la recuperación de 1,000 documentos. Para cada consulta, obtuvimos un conjunto de B modelos de retroalimentación utilizando la línea base de Indri. Cada modelo de retroalimentación se obtuvo a partir de una muestra aleatoria de los k documentos principales tomados con reemplazo. Para estos experimentos, B = 30 y k = 50. Cada modelo de retroalimentación contenía 20 términos. En el lado de la consulta, utilizamos muestreo de dejar uno fuera (LOO) para crear las variantes de consulta. El muestreo de consulta de un solo término tuvo un rendimiento consistentemente peor en todas las colecciones, por lo que nuestros resultados se centran en el muestreo de LOO aquí. Utilizamos los métodos descritos en la Sección 2 para estimar un <br>modelo de retroalimentación mejorado</br> a partir de la distribución posterior de Dirichlet para cada variante de consulta, y para combinar los modelos de retroalimentación de todas las variantes de consulta. Llamamos a nuestro método expansión de remuestreo y lo denotamos como RS-FB aquí. Denominamos al método de retroalimentación de línea base Indri como Base-FB. Los resultados de aplicar tanto el método de expansión de línea base (Base-FB) como la expansión de remuestreo (RS-FB) se muestran en la Tabla 1. Observamos varias tendencias en esta tabla. Primero, la precisión promedio de RS-FB fue comparable a Base-FB, logrando una ganancia promedio del 17.6% en comparación con no utilizar expansión en las cuatro colecciones. La ganancia de expansión de la línea base de Indri fue del 17.25%. Además, el método RS-FB logró mejoras consistentes en P10 sobre Base-FB para cada conjunto de temas, con una mejora promedio del 6.89% sobre Base-FB para los 350 temas. La ganancia P10 más baja sobre Base-FB fue del +3.82% para TREC-7 y la más alta fue del +11.95% para wt10g. Finalmente, tanto Base-FB como RS-FB también mejoraron consistentemente la recuperación en comparación con no utilizar expansión, siendo Base-FB el que logró una mejor recuperación que RS-FB para todos los conjuntos de temas. 3.4 Robustez de recuperación. Utilizamos el término robustez para referirnos al rendimiento de precisión promedio en el peor de los casos de un algoritmo de retroalimentación. Idealmente, un método de retroalimentación sólido nunca debería funcionar peor que usar la consulta original, mientras que a menudo funciona mejor utilizando la expansión. Para evaluar la robustez en este estudio, utilizamos una medida muy simple llamada índice de robustez (RI). Para un conjunto de consultas Q, la medida de RI se define como: RI(Q) = n+ − n− |Q| (6) donde n+ es el número de consultas ayudadas por el método de retroalimentación y n− es el número de consultas perjudicadas. Aquí, por \"ayudar\" nos referimos a obtener una precisión promedio más alta como resultado de la retroalimentación. El valor de RI varía desde un mínimo de 3. A veces también se le llama índice de confiabilidad de mejora y fue utilizado en Sakai et al. [17]. Colección NoExp Base-FB RS-FB TREC 1&2 AvgP 0.1818 0.2419 (+33.04%) 0.2406 (+32.24%) P10 0.4443 0.4913 (+10.57%) 0.5363 (+17.83%) Recall 15084/37393 19172/37393 15396/37393 TREC 7 AvgP 0.1890 0.2175 (+15.07%) 0.2169 (+14.75%) P10 0.4200 0.4320 (+2.85%) 0.4480 (+6.67%) Recall 2179/4674 2608/4674 2487/4674 TREC 8 AvgP 0.2031 0.2361 (+16.25%) 0.2268 (+11.70%) P10 0.3960 0.4160 (+5.05%) 0.4340 (+9.59%) Recall 2144/4728 2642/4728 2485/4728 wt10g AvgP 0.1741 0.1829 (+5.06%) 0.1946 (+11.78%) P10 0.2760 0.2630 (-4.71%) 0.2960 (+7.24%) Recall 3361/5980 3725/5980 3664/5980 Tabla 1: Comparación de la retroalimentación base (Base-FB) y la retroalimentación utilizando re-muestreo (RS-FB). La mejora mostrada para BaseFB y RS-FB es relativa al uso de ninguna expansión. (a) TREC 1&2 (curva superior); TREC 8 (curva inferior) (b) TREC 7 (curva superior); wt10g (curva inferior) Figura 3: La compensación entre robustez y precisión promedio para diferentes corpora. El eje x muestra el cambio en la MAP al utilizar la expansión de la línea base con α = 0.5. El eje y muestra el Índice de Robustez (RI). Cada curva a través de los puntos no circulados muestra el compromiso entre RI/MAP utilizando la estrategia simple de pequeño α (ver texto) a medida que α disminuye de 0.5 a cero en la dirección de la flecha. Los puntos circulados representan los compromisos obtenidos al volver a muestrear la retroalimentación para α = 0.5. Colección N Base-FB RS-FB n− RI n− RI TREC 1&2 103 26 +0.495 15 +0.709 TREC 7 46 14 +0.391 10 +0.565 TREC 8 44 12 +0.455 12 +0.455 wt10g 91 48 -0.055 39 +0.143 Combinado 284 100 +0.296 76 +0.465 Tabla 2: Comparación del índice de robustez (RI) para retroalimentación de línea base (Base-FB) vs. retroalimentación de remuestreo (RS-FB). También se muestran el número real de consultas perjudicadas por la retroalimentación (n−) para cada método y colección. Las consultas para las cuales la precisión promedio inicial era insignificante (≤ 0.01) fueron ignoradas, dando como resultado el recuento de consultas restantes en la columna N, de -1.0, cuando todas las consultas son perjudicadas por el método de retroalimentación, a +1.0 cuando todas las consultas son ayudadas. La medida de RI no tiene en cuenta la magnitud o distribución de la cantidad de cambio en el conjunto Q. Sin embargo, es fácil de entender como una indicación general de robustez. Una forma obvia de mejorar el rendimiento en el peor de los casos de la retroalimentación es simplemente utilizar un parámetro de interpolación α fijo más pequeño, como α = 0.3, otorgando menos peso al modelo de retroalimentación (posiblemente arriesgado) y más al cuestionario original. Llamamos a esto la estrategia de pequeña α. Dado que también estamos reduciendo las ganancias potenciales cuando el modelo de retroalimentación es correcto, sin embargo, esperaríamos algún tipo de compensación entre la precisión promedio y la robustez. Por lo tanto, comparamos el equilibrio entre precisión y robustez entre nuestro algoritmo de retroalimentación de remuestreo y el método simple de pequeño α. Los resultados se resumen en la Figura 3. En la figura, la curva para cada conjunto de temas interpola entre los puntos de compensación, comenzando en x=0, donde α = 0.5, y continuando en la dirección de la flecha a medida que α disminuye y la consulta original recibe cada vez más peso. Como era de esperar, la robustez aumenta continuamente a medida que avanzamos a lo largo de la curva, pero la precisión media promedio generalmente disminuye a medida que se eliminan las ganancias del feedback. Para la comparación, se muestra el rendimiento del re-muestreo con retroalimentación en α = 0.5 para cada colección como el punto marcado con un círculo. Más alto y a la derecha es mejor. Esta figura muestra que el re-muestreo de retroalimentación ofrece un compromiso algo mejor que el enfoque de pequeño α para 3 de las 4 colecciones. Figura 4: Histograma que muestra la mayor robustez del re-muestreo de retroalimentación (RS-FB) sobre la retroalimentación base (Base-FB) para todos los conjuntos de datos combinados. Las consultas se agrupan por el % de cambio en AP en comparación con la consulta no expandida. Colección DS + QV DS + No QV TREC 1&2 AvgP 0.2406 0.2547 (+5.86%) P10 0.5263 0.5362 (+1.88%) RI 0.7087 0.6515 (-0.0572) TREC 7 AvgP 0.2169 0.2200 (+1.43%) P10 0.4480 0.4300 (-4.02%) RI 0.5652 0.2609 (-0.3043) TREC 8 AvgP 0.2268 0.2257 (-0.49%) P10 0.4340 0.4200 (-3.23%) RI 0.4545 0.4091 (-0.0454) wt10g AvgP 0.1946 0.1865 (-4.16%) P10 0.2960 0.2680 (-9.46%) RI 0.1429 0.0220 (-0.1209) Tabla 3: Comparación de la retroalimentación de remuestreo utilizando muestreo de documentos (DS) con (QV) y sin (No QV) combinar modelos de retroalimentación de múltiples variantes de consulta. La Tabla 2 muestra los puntajes del Índice de Robustez para Base-FB y RS-FB. El método de retroalimentación RS-FB obtuvo una mayor robustez que Base-FB en tres de los cuatro conjuntos de temas, con solo un rendimiento ligeramente peor en TREC-8. Una vista más detallada que muestra la distribución de los cambios relativos en AP se presenta en el histograma de la Figura 4. En comparación con Base-FB, el método RS-FB logra una reducción notable en el número de consultas significativamente afectadas por la expansión (es decir, donde AP se ve afectado en un 25% o más), al tiempo que conserva ganancias positivas en AP. 3.5 Efecto de los métodos de muestreo de consultas y documentos Dada la mayor robustez de nuestros algoritmos observada en la Sección 3.4, una pregunta importante es qué componente de nuestro sistema es responsable. ¿Es el uso de la re-muestreo de documentos, el uso de múltiples variantes de consulta, o algún otro factor? Los resultados en la Tabla 3 sugieren que la combinación de modelos basada en variantes de consulta puede ser en gran medida responsable de la mayor robustez. Cuando se desactivan las variantes de consulta y se utiliza la consulta original por sí sola con muestreo de documentos, hay poco cambio neto en la precisión promedio, una pequeña disminución en P10 para 3 de los 4 conjuntos de temas, pero una caída significativa en la robustez para todos los conjuntos de temas. En dos casos, la medida de RI disminuye en más del 50%. También examinamos el efecto del método de muestreo de documentos en la efectividad de la recuperación, utilizando dos estrategias diferentes. La estrategia de ponderación uniforme ignoró las puntuaciones de relevancia de la recuperación inicial y otorgó a cada documento en los primeros k la misma probabilidad de selección. Por el contrario, la estrategia de ponderación de puntajes de relevancia seleccionó documentos con una probabilidad proporcional a sus puntajes de relevancia. De esta manera, los documentos que tenían una clasificación más alta tenían más probabilidades de ser seleccionados. Los resultados se muestran en la Tabla 4. La estrategia de ponderación de la puntuación de relevancia tiene un mejor rendimiento en general, con puntajes de RI y P10 significativamente más altos en 3 de los 4 conjuntos de temas. La diferencia en la precisión promedio entre los métodos, sin embargo, es menos marcada. Esto sugiere que el peso uniforme actúa para aumentar la varianza en los resultados de recuperación: cuando la precisión promedio inicial es alta, hay muchos documentos relevantes en los primeros k y el muestreo uniforme puede proporcionar un modelo de relevancia más representativo que centrarse en los elementos altamente clasificados. Por otro lado, cuando la precisión inicial es baja, hay pocos documentos relevantes en las posiciones inferiores y el muestreo uniforme mezcla más de los documentos no relevantes. Por razones de espacio, solo resumimos nuestros hallazgos sobre el tamaño de la muestra aquí. El número de muestras tiene cierto efecto en la precisión cuando es menor a 10, pero el rendimiento se estabiliza alrededor de 15 a 20 muestras. Utilizamos 30 muestras para nuestros experimentos. Mucho más allá de este nivel, los beneficios adicionales de más muestras disminuyen a medida que la distribución inicial de puntajes se ajusta más estrechamente y aumenta el tiempo de procesamiento. 3.6 El efecto del remuestreo en la calidad del término de expansión Idealmente, un modelo de recuperación no debería requerir una lista de palabras vacías al estimar un modelo de relevancia: un modelo estadístico robusto debería reducir automáticamente el peso de las palabras vacías dependiendo del contexto. Las palabras vacías pueden perjudicar la retroalimentación si se seleccionan como términos de retroalimentación, ya que suelen ser malos discriminadores y desperdician espacios de términos valiosos. En la práctica, sin embargo, debido a que la mayoría de los métodos de selección de términos se asemejan a un tipo de ponderación tf · idf, a veces se pueden seleccionar términos con un idf bajo pero un tf muy alto como candidatos de términos de expansión. Esto sucede, por ejemplo, incluso con el enfoque del Modelo de Relevancia que forma parte de nuestra retroalimentación base. Para garantizar una línea base lo más sólida posible, utilizamos una lista de paradas para todos los experimentos reportados aquí. Si desactivamos la lista de palabras vacías, sin embargo, obtenemos resultados como los mostrados en la Tabla 5 donde cuatro de los diez términos de retroalimentación básica principales para el tema 60 de TREC (dijo, pero, su, no) son palabras vacías utilizando el método BaseFB. (Se seleccionaron los 100 términos de expansión principales para generar este ejemplo). El método Indris intenta abordar el problema de las palabras vacías aplicando un paso inicial basado en Ponte [14] para seleccionar términos menos comunes que tienen altas probabilidades logarítmicas de estar en los documentos mejor clasificados en comparación con toda la colección. Sin embargo, esto no resuelve completamente el problema de las palabras vacías, especialmente a medida que aumenta el número de términos de retroalimentación. Sin embargo, el uso de retroalimentación de remuestreo parece mitigar la ponderación de la colección QV + QV uniforme + ponderación de puntaje de relevancia TREC 1&2 AvgP 0.2545 0.2406 (-5.46%) P10 0.5369 0.5263 (-1.97%) RI 0.6212 0.7087 (+14.09%) TREC 7 AvgP 0.2174 0.2169 (-0.23%) P10 0.4320 0.4480 (+3.70%) RI 0.4783 0.5652 (+18.17%) TREC 8 AvgP 0.2267 0.2268 (+0.04%) P10 0.4120 0.4340 (+5.34%) RI 0.4545 0.4545 (+0.00%) wt10g AvgP 0.1808 0.1946 (+7.63%) P10 0.2680 0.2960 (+10.45%) RI 0.0220 0.1099 (+399.5%) Tabla 4: Comparación de muestreo de documentos uniforme y ponderado por relevancia. El cambio porcentual en comparación con el muestreo uniforme se muestra entre paréntesis. QV indica que se utilizaron variantes de consulta en ambas ejecuciones. FB de línea base p(wi|R) FB de remuestreo p(wi|R) dijo 0.055 corte 0.026 corte 0.055 pagar 0.018 pagar 0.034 federal 0.012 pero 0.026 educación 0.011 empleados 0.024 maestros 0.010 sus 0.024 empleados 0.010 no 0.023 caso 0.010 federal 0.021 sus 0.009 trabajadores 0.020 apelaciones 0.008 educación 0.020 sindicato 0.007 Tabla 5: Calidad del término de retroalimentación cuando no se utiliza una lista de paradas. Términos de retroalimentación para el tema 60 de TREC: pago por mérito vs antigüedad. el efecto de las palabras vacías automáticamente. En el ejemplo de la Tabla 5, la retroalimentación de remuestreo deja solo una palabra vacía (su) en las diez primeras posiciones. Observamos un comportamiento similar del término de retroalimentación en muchos otros temas. La razón de este efecto parece ser la interacción de la puntuación de selección de términos con el límite superior de términos m. Si bien la presencia e incluso la proporción de palabras vacías particulares es bastante estable en diferentes muestras de documentos, su posición relativa en la lista de los primeros m elementos no lo es, ya que se examinan conjuntos de documentos con diferentes números de candidatos de términos mejores y de menor frecuencia para cada muestra. Como resultado, si bien algunos stopwords pueden aparecer en cada conjunto de documentos muestreados, tiende a ocurrir que cualquier stopword dado caiga por debajo del umbral para múltiples muestras, lo que lleva a su clasificación como una característica de alta varianza y bajo peso. 4. TRABAJO RELACIONADO Nuestro enfoque está relacionado con trabajos previos de varias áreas de recuperación de información y aprendizaje automático. Nuestro uso de la variación de consultas fue inspirado por el trabajo de YomTov et al. [20], Carpineto et al. [5] y Amati et al. [2], entre otros. Estos estudios utilizan la idea de crear múltiples subconsultas y luego examinar la naturaleza de la superposición en los documentos y/o términos de expansión que resultan de cada subconsulta. La combinación de modelos se realiza utilizando heurísticas. En particular, los estudios de Amati et al. y Carpineto et al. investigaron la combinación de términos de métodos de distribución individuales utilizando una heurística de combinación de reordenamiento de términos. En un conjunto de temas de TREC encontraron una amplia variación promedio en la distancia de rango de los términos de diferentes métodos de expansión. Su método de combinación proporcionó modestas mejoras positivas en la precisión promedio. La idea de examinar la superposición entre listas de términos sugeridos también ha sido utilizada en enfoques tempranos de expansión de consultas. El método de Análisis de Contexto Local (LCA) de Xu y Crofts [19] incluye un factor en la fórmula de ponderación derivada empíricamente que hace que se prefieran los términos de expansión que tienen conexiones con múltiples términos de consulta. En el lado del documento, el trabajo reciente de Zhou & Croft [21] exploró la idea de agregar ruido a los documentos, volver a puntuarlos y utilizar la estabilidad de las clasificaciones resultantes como una estimación de la dificultad de la consulta. Esto está relacionado con nuestro uso de muestreo de documentos para estimar el riesgo del modelo de retroalimentación construido a partir de los diferentes conjuntos de documentos recuperados en la parte superior. Sakai et al. [17] propusieron un enfoque para mejorar la robustez de la retroalimentación de relevancia pseudo utilizando un método que llaman muestreo selectivo. La esencia de su método es que permiten omitir algunos documentos de alta clasificación, basándose en un criterio de agrupación, para seleccionar un conjunto de documentos más variado e innovador más adelante en la clasificación para su uso por un método tradicional de retroalimentación pseudo. Su estudio no encontró mejoras significativas ni en robustez (RI) ni en MAP en sus corpora. Greiff, Morgan y Ponte [8] exploraron el papel de la varianza en la ponderación de términos. En una serie de simulaciones que simplificaron el problema a documentos de 2 características, encontraron que la precisión promedio disminuye a medida que la varianza de la frecuencia de términos - ruido alto - aumenta. La reducción del peso de los términos con alta varianza resultó en una mejora de la precisión promedio. Esto parece estar en concordancia con nuestros propios hallazgos para los modelos de retroalimentación individual. Las estimaciones de la varianza de la producción han sido utilizadas recientemente para mejorar la clasificación de textos. Lee et al. [11] utilizaron estimaciones de varianza específicas de la consulta de las salidas del clasificador para realizar una combinación de modelos mejorada. En lugar de utilizar muestreo, pudieron derivar expresiones en forma cerrada para la varianza del clasificador asumiendo clasificadores base utilizando tipos simples de redes de inferencia. Ando y Zhang propusieron un método que ellos llaman retroalimentación estructural [3] y mostraron cómo aplicarlo a la expansión de consultas para la pista de Genómica de TREC. Utilizaron variaciones de consultas r para obtener R conjuntos diferentes Sr de documentos mejor clasificados que se han intersectado con los documentos mejor clasificados obtenidos de la consulta original qorig. Para cada Si, se calcula el vector centróide normalizado ˆwi de los documentos. El análisis de componentes principales (PCA) se aplica luego a los ˆwi para obtener la matriz Φ de los vectores singulares izquierdos φh que se utilizan para obtener la nueva consulta expandida qexp = qorig + ΦT Φqorig. (7) En el caso de H = 1, tenemos un único vector singular izquierdo φ: qexp = qorig + (φT qorig)φ, de modo que el producto punto φT qorig es un tipo de peso dinámico en la consulta expandida que se basa en la similitud de la consulta original con la consulta expandida. El uso de la varianza como medida de calidad del modelo de retroalimentación ocurre de forma indirecta a través de la aplicación de PCA. Sería interesante estudiar las conexiones entre este enfoque y nuestro propio método de ajuste de modelos. Finalmente, en los enfoques de modelado del lenguaje para retroalimentación, Tao y Zhai [18] describen un método para una retroalimentación más robusta que permite que cada documento tenga un α de retroalimentación diferente. Los pesos de retroalimentación se derivan automáticamente utilizando EM regularizado. La condición de parada de EM implica un equilibrio aproximadamente igual entre el modelo de consulta y expansión. Proponen adaptar el parámetro de parada η basado en una función de alguna medida de calidad de los documentos de retroalimentación. CONCLUSIONES Hemos presentado un nuevo enfoque para la retroalimentación de relevancia pseudo basado en el muestreo de documentos y consultas. El uso del muestreo es un dispositivo muy flexible y poderoso, motivado por nuestro deseo general de ampliar los modelos actuales de recuperación mediante la estimación del riesgo o la varianza asociada con los parámetros o la salida de los procesos de recuperación. Tales estimaciones de varianza, por ejemplo, pueden ser utilizadas de forma natural en un marco bayesiano para una mejor estimación y combinación de modelos. Aplicaciones como la expansión selectiva pueden ser implementadas de manera fundamentada. Si bien nuestro estudio utiliza el enfoque de modelado del lenguaje como marco para experimentos, hacemos pocas suposiciones sobre el funcionamiento real del algoritmo de retroalimentación. Creemos que es probable que cualquier algoritmo de retroalimentación de línea base razonablemente efectivo se beneficiaría de nuestro enfoque. Nuestros resultados en colecciones estándar de TREC muestran que nuestro marco mejora la robustez de un método de retroalimentación de referencia sólido en una variedad de colecciones, sin sacrificar la precisión promedio. También proporciona pequeñas pero consistentes mejoras en la precisión del top 10. En trabajos futuros, visualizamos una investigación sobre cómo variar el conjunto de métodos de muestreo utilizados y el número de muestras controla el equilibrio entre robustez, precisión y eficiencia. Agradecimientos Agradecemos a Paul Bennett por las valiosas discusiones relacionadas con este trabajo, el cual fue apoyado por las becas de la NSF #IIS-0534345 y #CNS-0454018, y la beca del Departamento de Educación de los EE. UU. #R305G03123. Cualquier opinión, hallazgo, conclusión o recomendación expresada en este material son del autor y no necesariamente reflejan las de los patrocinadores. REFERENCIAS [1] El kit de herramientas Lemur para modelado de lenguaje y recuperación. http://www.lemurproject.org. [2] G. Amati, C. Carpineto y G. Romano. Dificultad de la consulta, robustez y aplicación selectiva de la expansión de consultas. En Actas de la 25ª Conferencia Europea sobre Recuperación de Información (ECIR 2004), páginas 127-137. [3] R. K. Ando y T. Zhang. Un método de aprendizaje semisupervisado de alto rendimiento para segmentación de texto. En Actas de la 43ª Reunión Anual de la ACL, páginas 1-9, junio de 2005. [4] L. Breiman. Empaquetando predictores. Aprendizaje automático, 24(2):123-140, 1996. [5] C. Carpineto, G. Romano y V. Giannini. Mejorando la retroalimentación de recuperación con la combinación de múltiples funciones de clasificación de términos. ACM Trans. Información. Sistemas, 20(3):259 - 290. [6] K. Collins-Thompson, P. Ogilvie y J. Callan. Resultados iniciales con consultas estructuradas y modelos de lenguaje en medio terabyte de texto. En Actas de la Conferencia de Recuperación de Información de Texto de 2005. Publicación Especial del NIST. [7] R. O. Duda, P. E. Hart y D. G. Stork. Clasificación de patrones. Wiley and Sons, 2da edición, 2001. [8] W. R. Greiff, W. T. Morgan y J. M. Ponte. El papel de la varianza en la ponderación de términos para la recuperación de información probabilística. En Actas de la 11ª Conferencia Internacional. Conf. sobre Gestión de Información y Conocimiento (CIKM 2002), páginas 252-259. [9] T. Kohonen, J. Hynninen, J. Kangas y J. Laaksonen. SOMPAK: El paquete de programas de mapas autoorganizados. Informe técnico A31, Universidad de Tecnología de Helsinki, 1996. http://www.cis.hut.fi/research/papers/som tr96.ps.Z. [10] V. Lavrenko. Una teoría generativa de relevancia. Tesis doctoral, Universidad de Massachusetts, Amherst, 2004. [11] C.-H. Lee, R. Greiner y S. Wang. Utilizando estimaciones de varianza específicas de la consulta para combinar clasificadores bayesianos. En Actas del 23º Congreso Internacional. Conf. sobre Aprendizaje Automático (ICML 2006), páginas 529-536. [12] D. Metzler y W. B. Croft. Combinando el modelo de lenguaje y los enfoques de red de inferencia para la recuperación. Información. Procesamiento y Gestión, 40(5):735-750, 2004. [13] T. Minka. Estimando una distribución de Dirichlet. Informe técnico, 2000. http://research.microsoft.com/minka/papers/dirichlet. [14] J. Ponte. Avances en la Recuperación de Información, capítulo Modelos de lenguaje para retroalimentación de relevancia, páginas 73-96. 2000. W.B. Croft, ed. [15] J. M. Ponte y W. B. Croft. Un enfoque de modelado del lenguaje para la recuperación de información. En Actas de la Conferencia ACM SIGIR de 1998 sobre Investigación y Desarrollo en Recuperación de Información, páginas 275-281. [16] J. Rocchio. El Sistema de Recuperación SMART, capítulo Retroalimentación de Relevancia en la Recuperación de Información, páginas 313-323. Prentice-Hall, 1971. G. Salton, ed. [17] T. Sakai, T. Manabe y M. Koyama. Retroalimentación de pseudo-relevancia flexible a través de muestreo selectivo. ACM Transactions on Asian Language Information Processing (TALIP), 4(2):111-135, 2005. [18] T. Tao y C. Zhai. Estimación regularizada de modelos de mezcla para retroalimentación de pseudo relevancia robusta. En Actas de la Conferencia ACM SIGIR 2006 sobre Investigación y Desarrollo en Recuperación de Información, páginas 162-169. [19] J. Xu y W. B. Croft. Mejorando la efectividad de la recuperación de información con análisis de contexto local. ACM Trans. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate to Spanish? Syst., 18(1):79-112, 2000. [20] E. YomTov, S. Fine, D. Carmel, y A. Darlow. Aprendiendo a estimar la dificultad de la consulta. En Actas de la Conferencia ACM SIGIR 2005 sobre Investigación y Desarrollo en Recuperación de Información, páginas 512-519. [21] Y. Zhou y W. B. Croft. Robustez del ranking: un nuevo marco para predecir el rendimiento de la consulta. En Actas de la 15ª Conferencia Internacional de la ACM. Conferencia sobre Gestión de Información y Conocimiento (CIKM 2006), páginas 567-574. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "information retrieval": {
            "translated_key": "recuperación de información",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Estimation and Use of Uncertainty in Pseudo-relevance Feedback Kevyn Collins-Thompson and Jamie Callan Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213-8213 U.S.A. {kct | callan}@cs.cmu.edu ABSTRACT Existing pseudo-relevance feedback methods typically perform averaging over the top-retrieved documents, but ignore an important statistical dimension: the risk or variance associated with either the individual document models, or their combination.",
                "Treating the baseline feedback method as a black box, and the output feedback model as a random variable, we estimate a posterior distribution for the feedback model by resampling a given querys top-retrieved documents, using the posterior mean or mode as the enhanced feedback model.",
                "We then perform model combination over several enhanced models, each based on a slightly modified query sampled from the original query.",
                "We find that resampling documents helps increase individual feedback model precision by removing noise terms, while sampling from the query improves robustness (worst-case performance) by emphasizing terms related to multiple query aspects.",
                "The result is a meta-feedback algorithm that is both more robust and more precise than the original strong baseline method.",
                "Categories and Subject Descriptors: H.3.3 [<br>information retrieval</br>]: Retrieval Models General Terms: Algorithms, Experimentation 1.",
                "INTRODUCTION Uncertainty is an inherent feature of <br>information retrieval</br>.",
                "Not only do we not know the queries that will be presented to our retrieval algorithm ahead of time, but the users information need may be vague or incompletely specified by these queries.",
                "Even if the query were perfectly specified, language in the collection documents is inherently complex and ambiguous and matching such language effectively is a formidable problem by itself.",
                "With this in mind, we wish to treat many important quantities calculated by the retrieval system, whether a relevance score for a document, or a weight for a query expansion term, as random variables whose true value is uncertain but where the uncertainty about the true value may be quantified by replacing the fixed value with a probability distribution over possible values.",
                "In this way, retrieval algorithms may attempt to quantify the risk or uncertainty associated with their output rankings, or improve the stability or precision of their internal calculations.",
                "Current algorithms for pseudo-relevance feedback (PRF) tend to follow the same basic method whether we use vector space-based algorithms such as Rocchios formula [16], or more recent language modeling approaches such as Relevance Models [10].",
                "First, a set of top-retrieved documents is obtained from an initial query and assumed to approximate a set of relevant documents.",
                "Next, a single feedback model vector is computed according to some sort of average, centroid, or expectation over the set of possibly-relevant document models.",
                "For example, the document vectors may be combined with equal weighting, as in Rocchio, or by query likelihood, as may be done using the Relevance Model1 .",
                "The use of an expectation is reasonable for practical and theoretical reasons, but by itself ignores potentially valuable information about the risk of the feedback model.",
                "Our main hypothesis in this paper is that estimating the uncertainty in feedback is useful and leads to better individual feedback models and more robust combined models.",
                "Therefore, we propose a method for estimating uncertainty associated with an individual feedback model in terms of a posterior distribution over language models.",
                "To do this, we systematically vary the inputs to the baseline feedback method and fit a Dirichlet distribution to the output.",
                "We use the posterior mean or mode as the improved feedback model estimate.",
                "This process is shown in Figure 1.",
                "As we show later, the mean and mode may vary significantly from the single feedback model proposed by the baseline method.",
                "We also perform model combination using several improved feedback language models obtained by a small number of new queries sampled from the original query.",
                "A models weight combines two complementary factors: the models probability of generating the query, and the variance of the model, with high-variance models getting lower weight. 1 For example, an expected parameter vector conditioned on the query observation is formed from top-retrieved documents, which are treated as training strings (see [10], p. 62).",
                "Figure 1: Estimating the uncertainty of the feedback model for a single query. 2.",
                "SAMPLING-BASED FEEDBACK In Sections 2.1-2.5 we describe a general method for estimating a probability distribution over the set of possible language models.",
                "In Sections 2.6 and 2.7 we summarize how different query samples are used to generate multiple feedback models, which are then combined. 2.1 Modeling Feedback Uncertainty Given a query Q and a collection C, we assume a probabilistic retrieval system that assigns a real-valued document score f(D, Q) to each document D in C, such that the score is proportional to the estimated probability of relevance.",
                "We make no other assumptions about f(D, Q).",
                "The nature of f(D, Q) may be complex: for example, if the retrieval system supports structured query languages [12], then f(D, Q) may represent the output of an arbitrarily complex inference network defined by the structured query operators.",
                "In theory, the scoring function can vary from query to query, although in this study for simplicity we keep the scoring function the same for all queries.",
                "Our specific query method is given in Section 3.",
                "We treat the feedback algorithm as a black box and assume that the inputs to the feedback algorithm are the original query and the corresponding top-retrieved documents, with a score being given to each document.",
                "We assume that the output of the feedback algorithm is a vector of term weights to be used to add or reweight the terms in the representation of the original query, with the vector normalized to form a probability distribution.",
                "We view the the inputs to the feedback black box as random variables, and analyze the feedback model as a random variable that changes in response to changes in the inputs.",
                "Like the document scoring function f(D, Q), the feedback algorithm may implement a complex, non-linear scoring formula, and so as its inputs vary, the resulting feedback models may have a complex distribution over the space of feedback models (the sample space).",
                "Because of this potential complexity, we do not attempt to derive a posterior distribution in closed form, but instead use simulation.",
                "We call this distribution over possible feedback models the feedback model distribution.",
                "Our goal in this section is to estimate a useful approximation to the feedback model distribution.",
                "For a specific framework for experiments, we use the language modeling (LM) approach for <br>information retrieval</br> [15].",
                "The score of a document D with respect to a query Q and collection C is given by p(Q|D) with respect to language models ˆθQ and ˆθD estimated for the query and document respectively.",
                "We denote the set of k top-retrieved documents from collection C in response to Q by DQ(k, C).",
                "For simplicity, we assume that queries and documents are generated by multinomial distributions whose parameters are represented by unigram language models.",
                "To incorporate feedback in the LM approach, we assume a model-based scheme in which our goal is take the query and resulting ranked documents DQ(k, C) as input, and output an expansion language model ˆθE, which is then interpolated with the original query model ˆθQ: ˆθNew = (1 − α) · ˆθQ + α · ˆθE (1) This includes the possibility of α = 1 where the original query mode is completely replaced by the feedback model.",
                "Our sample space is the set of all possible language models LF that may be output as feedback models.",
                "Our approach is to take samples from this space and then fit a distribution to the samples using maximum likelihood.",
                "For simplicity, we start by assuming the latent feedback distribution has the form of a Dirichlet distribution.",
                "Although the Dirichlet is a unimodal distribution, and in general quite limited in its expressiveness in the sample space, it is a natural match for the multinomial language model, can be estimated quickly, and can capture the most salient features of confident and uncertain feedback models, such as the overall spread of the distibution. 2.2 Resampling document models We would like an approximation to the posterior distribution of the feedback model LF .",
                "To accomplish this, we apply a widely-used simulation technique called bootstrap sampling ([7], p. 474) on the input parameters, namely, the set of top-retrieved documents.",
                "Bootstrap sampling allows us to simulate the approximate effect of perturbing the parameters within the black box feedback algorithm by perturbing the inputs to that algorithm in a systematic way, while making no assumptions about the nature of the feedback algorithm.",
                "Specifically, we sample k documents with replacement from DQ(k, C), and calculate an expansion language model θb using the black box feedback method.",
                "We repeat this process B times to obtain a set of B feedback language models, to which we then fit a Dirichlet distribution.",
                "Typically B is in the range of 20 to 50 samples, with performance being relatively stable in this range.",
                "Note that instead of treating each top document as equally likely, we sample according to the estimated probabilities of relevance of each document in DQ(k, C).",
                "Thus, a document is more likely to be chosen the higher it is in the ranking. 2.3 Justification for a sampling approach The rationale for our sampling approach has two parts.",
                "First, we want to improve the quality of individual feedback models by smoothing out variation when the baseline feedback model is unstable.",
                "In this respect, our approach resembles bagging [4], an ensemble approach which generates multiple versions of a predictor by making bootstrap copies of the training set, and then averages the (numerical) predictors.",
                "In our application, top-retrieved documents can be seen as a kind of noisy training set for relevance.",
                "Second, sampling is an effective way to estimate basic properties of the feedback posterior distribution, which can then be used for improved model combination.",
                "For example, a model may be weighted by its prediction confidence, estimated as a function of the variability of the posterior around the model. foo2-401.map-Dim:5434,Size:12*12units,gaussianneighborhood (a) Topic 401 Foreign minorities, Germany foo2-402.map-Dim:5698,Size:12*12units,gaussianneighborhood (b) Topic 402 Behavioral genetics foo2-459.map-Dim:8969,Size:12*12units,gaussianneighborhood (c) Topic 459 When can a lender foreclose on property Figure 2: Visualization of expansion language model variance using self-organizing maps, showing the distribution of language models that results from resampling the inputs to the baseline expansion method.",
                "The language model that would have been chosen by the baseline expansion is at the center of each map.",
                "The similarity function is JensenShannon divergence. 2.4 Visualizing feedback distributions Before describing how we fit and use the Dirichlet distribution over feedback models, it is instructive to view some examples of actual feedback model distributions that result from bootstrap sampling the top-retrieved documents from different TREC topics.",
                "Each point in our sample space is a language model, which typically has several thousand dimensions.",
                "To help analyze the behavior of our method we used a Self-Organizing Map (via the SOM-PAK package [9]), to flatten and visualize the high-dimensional density function2 .",
                "The density maps for three TREC topics are shown in Figure 2 above.",
                "The dark areas represent regions of high similarity between language models.",
                "The light areas represent regions of low similarity - the valleys between clusters.",
                "Each diagram is centered on the language model that would have been chosen by the baseline expansion.",
                "A single peak (mode) is evident in some examples, but more complex structure appears in others.",
                "Also, while the distribution is usually close to the baseline feedback model, for some topics they are a significant distance apart (as measured by JensenShannon divergence), as in Subfigure 2c.",
                "In such cases, the mode or mean of the feedback distribution often performs significantly better than the baseline (and in a smaller proportion of cases, significantly worse). 2.5 Fitting a posterior feedback distribution After obtaining feedback model samples by resampling the feedback model inputs, we estimate the feedback distribution.",
                "We assume that the multinomial feedback models {ˆθ1, . . . , ˆθB} were generated by a latent Dirichlet distribution with parameters {α1, . . . , αN }.",
                "To estimate the {α1, . . . , αN }, we fit the Dirichlet parameters to the B language model samples according to maximum likelihood using a generalized Newton procedure, details of which are given in Minka [13].",
                "We assume a simple Dirichlet prior over the {α1, . . . , αN }, setting each to αi = μ · p(wi | C), where μ is a parameter and p(· | C) is the collection language model estimated from a set of documents from collection C. The parameter fitting converges very quickly - typically just 2 or 2 Because our points are language models in the multinomial simplex, we extended SOM-PAK to support JensenShannon divergence, a widely-used similarity measure between probability distributions. 3 iterations are enough - so that it is practical to apply at query-time when computational overhead must be small.",
                "In practice, we can restrict the calculation to the vocabulary of the top-retrieved documents, instead of the entire collection.",
                "Note that for this step we are re-using the existing retrieved documents and not performing additional queries.",
                "Given the parameters of an N-dimensional Dirichlet distribution Dir(α) the mean μ and mode x vectors are easy to calculate and are given respectively by μi = αiP αi (2) and xi = αi−1P αi−N . (3) We can then choose the language model at the mean or the mode of the posterior as the final enhanced feedback model. (We found the mode to give slightly better performance.)",
                "For <br>information retrieval</br>, the number of samples we will have available is likely to be quite small for performance reasons - usually less than ten.",
                "Moreover, while random sampling is useful in certain cases, it is perfectly acceptable to allow deterministic sampling distributions, but these must be designed carefully in order to approximate an accurate output variance.",
                "We leave this for future study. 2.6 Query variants We use the following methods for generating variants of the original query.",
                "Each variant corresponds to a different assumption about which aspects of the original query may be important.",
                "This is a form of deterministic sampling.",
                "We selected three simple methods that cover complimentary assumptions about the query.",
                "No-expansion Use only the original query.",
                "The assumption is that the given terms are a complete description of the information need.",
                "Leave-one-out A single term is left out of the original query.",
                "The assumption is that one of the query terms is a noise term.",
                "Single-term A single term is chosen from the original query.",
                "This assumes that only one aspect of the query, namely, that represented by the term, is most important.",
                "After generating a variant of the original query, we combine it with the original query using a weight αSUB so that we do not stray too far.",
                "In this study, we set αSUB = 0.5.",
                "For example, using the Indri [12] query language, a leave-oneout variant of the initial query that omits the term ireland for TREC topic 404 is: #weight(0.5 #combine(ireland peace talks) 0.5 #combine(peace talks)) 2.7 Combining enhanced feedback models from multiple query variants When using multiple query variants, the resulting enhanced feedback models are combined using Bayesian model combination.",
                "To do this, we treat each word as an item to be classified as belonging to a relevant or non-relevant class, and derive a class probability for each word by combining the scores from each query variant.",
                "Each score is given by that terms probability in the Dirichlet distribution.",
                "The term scores are weighted by the inverse of the variance of the term in the enhanced feedback models Dirichlet distribution.",
                "The prior probability of a words membership in the relevant class is given by the probability of the original query in the entire enhanced expansion model. 3.",
                "EVALUATION In this section we present results confirming the usefulness of estimating a feedback model distribution from weighted resampling of top-ranked documents, and of combining the feedback models obtained from different small changes in the original query. 3.1 General method We evaluated performance on a total of 350 queries derived from four sets of TREC topics: 51-200 (TREC-1&2), 351-400 (TREC-7), 401-450 (TREC-8), and 451-550 (wt10g, TREC-9&10).",
                "We chose these for their varied content and document properties.",
                "For example, wt10g documents are Web pages with a wide variety of subjects and styles while TREC-1&2 documents are more homogeneous news articles.",
                "Indexing and retrieval was performed using the Indri system in the Lemur toolkit [12] [1].",
                "Our queries were derived from the words in the title field of the TREC topics.",
                "Phrases were not used.",
                "To generate the baseline queries passed to Indri, we wrapped the query terms with Indris #combine operator.",
                "For example, the initial query for topic 404 is: #combine(ireland peace talks) We performed Krovetz stemming for all experiments.",
                "Because we found that the baseline (Indri) expansion method performed better using a stopword list with the feedback model, all experiments used a stoplist of 419 common English words.",
                "However, an interesting side-effect of our resampling approach is that it tends to remove many stopwords from the feedback model, making a stoplist less critical.",
                "This is discussed further in Section 3.6. 3.2 Baseline feedback method For our baseline expansion method, we use an algorithm included in Indri 1.0 as the default expansion method.",
                "This method first selects terms using a log-odds calculation described by Ponte [14], but assigns final term weights using Lavrenkos relevance model[10].",
                "We chose the Indri method because it gives a consistently strong baseline, is based on a language modeling approach, and is simple to experiment with.",
                "In a TREC evaluation using the GOV2 corpus [6], the method was one of the topperforming runs, achieving a 19.8% gain in MAP compared to using unexpanded queries.",
                "In this study, it achieves an average gain in MAP of 17.25% over the four collections.",
                "Indris expansion method first calculates a log-odds ratio o(v) for each potential expansion term v given by o(v) = X D log p(v|D) p(v|C) (4) over all documents D containing v, in collection C. Then, the expansion term candidates are sorted by descending o(v), and the top m are chosen.",
                "Finally, the term weights r(v) used in the expanded query are calculated based on the relevance model r(v) = X D p(q|D)p(v|D) p(v) p(D) (5) The quantity p(q|D) is the probability score assigned to the document in the initial retrieval.",
                "We use Dirichlet smoothing of p(v|D) with μ = 1000.",
                "This relevance model is then combined with the original query using linear interpolation, weighted by a parameter α.",
                "By default we used the top 50 documents for feedback and the top 20 expansion terms, with the feedback interpolation parameter α = 0.5 unless otherwise stated.",
                "For example, the baseline expanded query for topic 404 is: #weight(0.5 #combine(ireland peace talks) 0.5 #weight(0.10 ireland 0.08 peace 0.08 northern ...) 3.3 Expansion performance We measure our feedback algorithms effectiveness by two main criteria: precision, and robustness.",
                "Robustness, and the tradeoff between precision and robustness, is analyzed in Section 3.4.",
                "In this section, we examine average precision and precision in the top 10 documents (P10).",
                "We also include recall at 1,000 documents.",
                "For each query, we obtained a set of B feedback models using the Indri baseline.",
                "Each feedback model was obtained from a random sample of the top k documents taken with replacement.",
                "For these experiments, B = 30 and k = 50.",
                "Each feedback model contained 20 terms.",
                "On the query side, we used leave-one-out (LOO) sampling to create the query variants.",
                "Single-term query sampling had consistently worse performance across all collections and so our results here focus on LOO sampling.",
                "We used the methods described in Section 2 to estimate an enhanced feedback model from the Dirichlet posterior distribution for each query variant, and to combine the feedback models from all the query variants.",
                "We call our method resampling expansion and denote it as RS-FB here.",
                "We denote the Indri baseline feedback method as Base-FB.",
                "Results from applying both the baseline expansion method (Base-FB) and resampling expansion (RS-FB) are shown in Table 1.",
                "We observe several trends in this table.",
                "First, the average precision of RS-FB was comparable to Base-FB, achieving an average gain of 17.6% compared to using no expansion across the four collections.",
                "The Indri baseline expansion gain was 17.25%.",
                "Also, the RS-FB method achieved consistent improvements in P10 over Base-FB for every topic set, with an average improvement of 6.89% over Base-FB for all 350 topics.",
                "The lowest P10 gain over Base-FB was +3.82% for TREC-7 and the highest was +11.95% for wt10g.",
                "Finally, both Base-FB and RS-FB also consistently improved recall over using no expansion, with Base-FB achieving better recall than RS-FB for all topic sets. 3.4 Retrieval robustness We use the term robustness to mean the worst-case average precision performance of a feedback algorithm.",
                "Ideally, a robust feedback method would never perform worse than using the original query, while often performing better using the expansion.",
                "To evaluate robustness in this study, we use a very simple measure called the robustness index (RI)3 .",
                "For a set of queries Q, the RI measure is defined as: RI(Q) = n+ − n− |Q| (6) where n+ is the number of queries helped by the feedback method and n− is the number of queries hurt.",
                "Here, by helped we mean obtaining a higher average precision as a result of feedback.",
                "The value of RI ranges from a minimum 3 This is sometimes also called the reliability of improvement index and was used in Sakai et al. [17].",
                "Collection NoExp Base-FB RS-FB TREC 1&2 AvgP 0.1818 0.2419 (+33.04%) 0.2406 (+32.24%) P10 0.4443 0.4913 (+10.57%) 0.5363 (+17.83%) Recall 15084/37393 19172/37393 15396/37393 TREC 7 AvgP 0.1890 0.2175 (+15.07%) 0.2169 (+14.75%) P10 0.4200 0.4320 (+2.85%) 0.4480 (+6.67%) Recall 2179/4674 2608/4674 2487/4674 TREC 8 AvgP 0.2031 0.2361 (+16.25%) 0.2268 (+11.70%) P10 0.3960 0.4160 (+5.05%) 0.4340 (+9.59%) Recall 2144/4728 2642/4728 2485/4728 wt10g AvgP 0.1741 0.1829 (+5.06%) 0.1946 (+11.78%) P10 0.2760 0.2630 (-4.71%) 0.2960 (+7.24%) Recall 3361/5980 3725/5980 3664/5980 Table 1: Comparison of baseline (Base-FB) feedback and feedback using re-sampling (RS-FB).",
                "Improvement shown for BaseFB and RS-FB is relative to using no expansion. (a) TREC 1&2 (upper curve); TREC 8 (lower curve) (b) TREC 7 (upper curve); wt10g (lower curve) Figure 3: The trade-off between robustness and average precision for different corpora.",
                "The x-axis gives the change in MAP over using baseline expansion with α = 0.5.",
                "The yaxis gives the Robustness Index (RI).",
                "Each curve through uncircled points shows the RI/MAP tradeoff using the simple small-α strategy (see text) as α decreases from 0.5 to zero in the direction of the arrow.",
                "Circled points represent the tradeoffs obtained by resampling feedback for α = 0.5.",
                "Collection N Base-FB RS-FB n− RI n− RI TREC 1&2 103 26 +0.495 15 +0.709 TREC 7 46 14 +0.391 10 +0.565 TREC 8 44 12 +0.455 12 +0.455 wt10g 91 48 -0.055 39 +0.143 Combined 284 100 +0.296 76 +0.465 Table 2: Comparison of robustness index (RI) for baseline feedback (Base-FB) vs. resampling feedback (RS-FB).",
                "Also shown are the actual number of queries hurt by feedback (n−) for each method and collection.",
                "Queries for which initial average precision was negligible (≤ 0.01) were ignored, giving the remaining query count in column N. of −1.0, when all queries are hurt by the feedback method, to +1.0 when all queries are helped.",
                "The RI measure does not take into account the magnitude or distribution of the amount of change across the set Q.",
                "However, it is easy to understand as a general indication of robustness.",
                "One obvious way to improve the worst-case performance of feedback is simply to use a smaller fixed α interpolation parameter, such as α = 0.3, placing less weight on the (possibly risky) feedback model and more on the original query.",
                "We call this the small-α strategy.",
                "Since we are also reducing the potential gains when the feedback model is right, however, we would expect some trade-off between average precision and robustness.",
                "We therefore compared the precision/robustness trade-off between our resampling feedback algorithm, and the simple small-α method.",
                "The results are summarized in Figure 3.",
                "In the figure, the curve for each topic set interpolates between trade-off points, beginning at x=0, where α = 0.5, and continuing in the direction of the arrow as α decreases and the original query is given more and more weight.",
                "As expected, robustness continuously increases as we move along the curve, but mean average precision generally drops as the gains from feedback are eliminated.",
                "For comparison, the performance of resampling feedback at α = 0.5 is shown for each collection as the circled point.",
                "Higher and to the right is better.",
                "This figure shows that resampling feedback gives a somewhat better trade-off than the small-α approach for 3 of the 4 collections.",
                "Figure 4: Histogram showing improved robustness of resampling feedback (RS-FB) over baseline feedback (Base-FB) for all datasets combined.",
                "Queries are binned by % change in AP compared to the unexpanded query.",
                "Collection DS + QV DS + No QV TREC 1&2 AvgP 0.2406 0.2547 (+5.86%) P10 0.5263 0.5362 (+1.88%) RI 0.7087 0.6515 (-0.0572) TREC 7 AvgP 0.2169 0.2200 (+1.43%) P10 0.4480 0.4300 (-4.02%) RI 0.5652 0.2609 (-0.3043) TREC 8 AvgP 0.2268 0.2257 (-0.49%) P10 0.4340 0.4200 (-3.23%) RI 0.4545 0.4091 (-0.0454) wt10g AvgP 0.1946 0.1865 (-4.16%) P10 0.2960 0.2680 (-9.46%) RI 0.1429 0.0220 (-0.1209) Table 3: Comparison of resampling feedback using document sampling (DS) with (QV) and without (No QV) combining feedback models from multiple query variants.",
                "Table 2 gives the Robustness Index scores for Base-FB and RS-FB.",
                "The RS-FB feedback method obtained higher robustness than Base-FB on three of the four topic sets, with only slightly worse performance on TREC-8.",
                "A more detailed view showing the distribution over relative changes in AP is given by the histogram in Figure 4.",
                "Compared to Base-FB, the RS-FB method achieves a noticable reduction in the number of queries significantly hurt by expansion (i.e. where AP is hurt by 25% or more), while preserving positive gains in AP. 3.5 Effect of query and document sampling methods Given our algorithms improved robustness seen in Section 3.4, an important question is what component of our system is responsible.",
                "Is it the use of document re-sampling, the use of multiple query variants, or some other factor?",
                "The results in Table 3 suggest that the model combination based on query variants may be largely account for the improved robustness.",
                "When query variants are turned off and the original query is used by itself with document sampling, there is little net change in average precision, a small decrease in P10 for 3 out of the 4 topic sets, but a significant drop in robustness for all topic sets.",
                "In two cases, the RI measure drops by more than 50%.",
                "We also examined the effect of the document sampling method on retrieval effectiveness, using two different strategies.",
                "The uniform weighting strategy ignored the relevance scores from the initial retrieval and gave each document in the top k the same probability of selection.",
                "In contrast, the relevance-score weighting strategy chose documents with probability proportional to their relevance scores.",
                "In this way, documents that were more highly ranked were more likely to be selected.",
                "Results are shown in Table 4.",
                "The relevance-score weighting strategy performs better overall, with significantly higher RI and P10 scores on 3 of the 4 topic sets.",
                "The difference in average precision between the methods, however, is less marked.",
                "This suggests that uniform weighting acts to increase variance in retrieval results: when initial average precision is high, there are many relevant documents in the top k and uniform sampling may give a more representative relevance model than focusing on the highly-ranked items.",
                "On the other hand, when initial precision is low, there are few relevant documents in the bottom ranks and uniform sampling mixes in more of the non-relevant documents.",
                "For space reasons we only summarize our findings on sample size here.",
                "The number of samples has some effect on precision when less than 10, but performance stabilizes at around 15 to 20 samples.",
                "We used 30 samples for our experiments.",
                "Much beyond this level, the additional benefits of more samples decrease as the initial score distribution is more closely fit and the processing time increases. 3.6 The effect of resampling on expansion term quality Ideally, a retrieval model should not require a stopword list when estimating a model of relevance: a robust statistical model should down-weight stopwords automatically depending on context.",
                "Stopwords can harm feedback if selected as feedback terms, because they are typically poor discriminators and waste valuable term slots.",
                "In practice, however, because most term selection methods resemble a tf · idf type of weighting, terms with low idf but very high tf can sometimes be selected as expansion term candidates.",
                "This happens, for example, even with the Relevance Model approach that is part of our baseline feedback.",
                "To ensure as strong a baseline as possible, we use a stoplist for all experiments reported here.",
                "If we turn off the stopword list, however, we obtain results such as those shown in Table 5 where four of the top ten baseline feedback terms for TREC topic 60 (said, but, their, not) are stopwords using the BaseFB method. (The top 100 expansion terms were selected to generate this example.)",
                "Indris method attempts to address the stopword problem by applying an initial step based on Ponte [14] to select less-common terms that have high log-odds of being in the top-ranked documents compared to the whole collection.",
                "Nevertheless, this does not overcome the stopword problem completely, especially as the number of feedback terms grows.",
                "Using resampling feedback, however, appears to mitigate Collection QV + Uniform QV + Relevance-score weighting weighting TREC 1&2 AvgP 0.2545 0.2406 (-5.46%) P10 0.5369 0.5263 (-1.97%) RI 0.6212 0.7087 (+14.09%) TREC 7 AvgP 0.2174 0.2169 (-0.23%) P10 0.4320 0.4480 (+3.70%) RI 0.4783 0.5652 (+18.17%) TREC 8 AvgP 0.2267 0.2268 (+0.04%) P10 0.4120 0.4340 (+5.34%) RI 0.4545 0.4545 (+0.00%) wt10g AvgP 0.1808 0.1946 (+7.63%) P10 0.2680 0.2960 (+10.45%) RI 0.0220 0.1099 (+399.5%) Table 4: Comparison of uniform and relevance-weighted document sampling.",
                "The percentage change compared to uniform sampling is shown in parentheses.",
                "QV indicates that query variants were used in both runs.",
                "Baseline FB p(wi|R) Resampling FB p(wi|R) said 0.055 court 0.026 court 0.055 pay 0.018 pay 0.034 federal 0.012 but 0.026 education 0.011 employees 0.024 teachers 0.010 their 0.024 employees 0.010 not 0.023 case 0.010 federal 0.021 their 0.009 workers 0.020 appeals 0.008 education 0.020 union 0.007 Table 5: Feedback term quality when a stoplist is not used.",
                "Feedback terms for TREC topic 60: merit pay vs seniority. the effect of stopwords automatically.",
                "In the example of Table 5, resampling feedback leaves only one stopword (their) in the top ten.",
                "We observed similar feedback term behavior across many other topics.",
                "The reason for this effect appears to be the interaction of the term selection score with the top-m term cutoff.",
                "While the presence and even proportion of particular stopwords is fairly stable across different document samples, their relative position in the top-m list is not, as sets of documents with varying numbers of better, lower-frequency term candidates are examined for each sample.",
                "As a result, while some number of stopwords may appear in each sampled document set, any given stopword tends to fall below the cutoff for multiple samples, leading to its classification as a high-variance, low-weight feature. 4.",
                "RELATED WORK Our approach is related to previous work from several areas of <br>information retrieval</br> and machine learning.",
                "Our use of query variation was inspired by the work of YomTov et al. [20], Carpineto et al. [5], and Amati et al. [2], among others.",
                "These studies use the idea of creating multiple subqueries and then examining the nature of the overlap in the documents and/or expansion terms that result from each subquery.",
                "Model combination is performed using heuristics.",
                "In particular, the studies of Amati et al. and Carpineto et al. investigated combining terms from individual distributional methods using a term-reranking combination heuristic.",
                "In a set of TREC topics they found wide average variation in the rank-distance of terms from different expansion methods.",
                "Their combination method gave modest positive improvements in average precision.",
                "The idea of examining the overlap between lists of suggested terms has also been used in early query expansion approaches.",
                "Xu and Crofts method of Local Context Analysis (LCA) [19] includes a factor in the empirically-derived weighting formula that causes expansion terms to be preferred that have connections to multiple query terms.",
                "On the document side, recent work by Zhou & Croft [21] explored the idea of adding noise to documents, re-scoring them, and using the stability of the resulting rankings as an estimate of query difficulty.",
                "This is related to our use of document sampling to estimate the risk of the feedback model built from the different sets of top-retrieved documents.",
                "Sakai et al. [17] proposed an approach to improving the robustness of pseudo-relevance feedback using a method they call selective sampling.",
                "The essence of their method is that they allow skipping of some top-ranked documents, based on a clustering criterion, in order to select a more varied and novel set of documents later in the ranking for use by a traditional pseudo-feedback method.",
                "Their study did not find significant improvements in either robustness (RI) or MAP on their corpora.",
                "Greiff, Morgan and Ponte [8] explored the role of variance in term weighting.",
                "In a series of simulations that simplified the problem to 2-feature documents, they found that average precision degrades as term frequency variance - high noiseincreases.",
                "Downweighting terms with high variance resulted in improved average precision.",
                "This seems in accord with our own findings for individual feedback models.",
                "Estimates of output variance have recently been used for improved text classification.",
                "Lee et al. [11] used queryspecific variance estimates of classifier outputs to perform improved model combination.",
                "Instead of using sampling, they were able to derive closed-form expressions for classifier variance by assuming base classifiers using simple types of inference networks.",
                "Ando and Zhang proposed a method that they call structural feedback [3] and showed how to apply it to query expansion for the TREC Genomics Track.",
                "They used r query variations to obtain R different sets Sr of top-ranked documents that have been intersected with the top-ranked documents obtained from the original query qorig.",
                "For each Si, the normalized centroid vector ˆwi of the documents is calculated.",
                "Principal component analysis (PCA) is then applied to the ˆwi to obtain the matrix Φ of H left singular vectors φh that are used to obtain the new, expanded query qexp = qorig + ΦT Φqorig. (7) In the case H = 1, we have a single left singular vector φ: qexp = qorig + (φT qorig)φ so that the dot product φT qorig is a type of dynamic weight on the expanded query that is based on the similarity of the original query to the expanded query.",
                "The use of variance as a feedback model quality measure occurs indirectly through the application of PCA.",
                "It would be interesting to study the connections between this approach and our own modelfitting method.",
                "Finally, in language modeling approaches to feedback, Tao and Zhai [18] describe a method for more robust feedback that allows each document to have a different feedback α.",
                "The feedback weights are derived automatically using regularized EM.",
                "A roughly equal balance of query and expansion model is implied by their EM stopping condition.",
                "They propose tailoring the stopping parameter η based on a function of some quality measure of feedback documents. 5.",
                "CONCLUSIONS We have presented a new approach to pseudo-relevance feedback based on document and query sampling.",
                "The use of sampling is a very flexible and powerful device and is motivated by our general desire to extend current models of retrieval by estimating the risk or variance associated with the parameters or output of retrieval processes.",
                "Such variance estimates, for example, may be naturally used in a Bayesian framework for improved model estimation and combination.",
                "Applications such as selective expansion may then be implemented in a principled way.",
                "While our study uses the language modeling approach as a framework for experiments, we make few assumptions about the actual workings of the feedback algorithm.",
                "We believe it is likely that any reasonably effective baseline feedback algorithm would benefit from our approach.",
                "Our results on standard TREC collections show that our framework improves the robustness of a strong baseline feedback method across a variety of collections, without sacrificing average precision.",
                "It also gives small but consistent gains in top10 precision.",
                "In future work, we envision an investigation into how varying the set of sampling methods used and the number of samples controls the trade-off between robustness, accuracy, and efficiency.",
                "Acknowledgements We thank Paul Bennett for valuable discussions related to this work, which was supported by NSF grants #IIS-0534345 and #CNS-0454018, and U.S. Dept. of Education grant #R305G03123.",
                "Any opinions, findings, and conclusions or recommendations expressed in this material are the authors. and do not necessarily reflect those of the sponsors. 6.",
                "REFERENCES [1] The Lemur toolkit for language modeling and retrieval. http://www.lemurproject.org. [2] G. Amati, C. Carpineto, and G. Romano.",
                "Query difficulty, robustness, and selective application of query expansion.",
                "In Proc. of the 25th European Conf. on <br>information retrieval</br> (ECIR 2004), pages 127-137. [3] R. K. Ando and T. Zhang.",
                "A high-performance semi-supervised learning method for text chunking.",
                "In Proc. of the 43rd Annual Meeting of the ACL, pages 1-9, June 2005. [4] L. Breiman.",
                "Bagging predictors.",
                "Machine Learning, 24(2):123-140, 1996. [5] C. Carpineto, G. Romano, and V. Giannini.",
                "Improving retrieval feedback with multiple term-ranking function combination.",
                "ACM Trans.",
                "Info.",
                "Systems, 20(3):259 - 290. [6] K. Collins-Thompson, P. Ogilvie, and J. Callan.",
                "Initial results with structured queries and language models on half a terabyte of text.",
                "In Proc. of 2005 Text REtrieval Conference.",
                "NIST Special Publication. [7] R. O. Duda, P. E. Hart, and D. G. Stork.",
                "Pattern Classification.",
                "Wiley and Sons, 2nd edition, 2001. [8] W. R. Greiff, W. T. Morgan, and J. M. Ponte.",
                "The role of variance in term weighting for probabilistic <br>information retrieval</br>.",
                "In Proc. of the 11th Intl.",
                "Conf. on Info. and Knowledge Mgmt. (CIKM 2002), pages 252-259. [9] T. Kohonen, J. Hynninen, J. Kangas, and J. Laaksonen.",
                "SOMPAK: The self-organizing map program package.",
                "Technical Report A31, Helsinki University of Technology, 1996. http://www.cis.hut.fi/research/papers/som tr96.ps.Z. [10] V. Lavrenko.",
                "A Generative Theory of Relevance.",
                "PhD thesis, University of Massachusetts, Amherst, 2004. [11] C.-H. Lee, R. Greiner, and S. Wang.",
                "Using query-specific variance estimates to combine Bayesian classifiers.",
                "In Proc. of the 23rd Intl.",
                "Conf. on Machine Learning (ICML 2006), pages 529-536. [12] D. Metzler and W. B. Croft.",
                "Combining the language model and inference network approaches to retrieval.",
                "Info.",
                "Processing and Mgmt., 40(5):735-750, 2004. [13] T. Minka.",
                "Estimating a Dirichlet distribution.",
                "Technical report, 2000. http://research.microsoft.com/ minka/papers/dirichlet. [14] J. Ponte.",
                "Advances in <br>information retrieval</br>, chapter Language models for relevance feedback, pages 73-96. 2000.",
                "W.B.",
                "Croft, ed. [15] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to <br>information retrieval</br>.",
                "In Proc. of the 1998 ACM SIGIR Conference on Research and Development in <br>information retrieval</br>, pages 275-281. [16] J. Rocchio.",
                "The SMART Retrieval System, chapter Relevance Feedback in <br>information retrieval</br>, pages 313-323.",
                "Prentice-Hall, 1971.",
                "G. Salton, ed. [17] T. Sakai, T. Manabe, and M. Koyama.",
                "Flexible pseudo-relevance feedback via selective sampling.",
                "ACM Transactions on Asian Language Information Processing (TALIP), 4(2):111-135, 2005. [18] T. Tao and C. Zhai.",
                "Regularized estimation of mixture models for robust pseudo-relevance feedback.",
                "In Proc. of the 2006 ACM SIGIR Conference on Research and Development in <br>information retrieval</br>, pages 162-169. [19] J. Xu and W. B. Croft.",
                "Improving the effectiveness of <br>information retrieval</br> with local context analysis.",
                "ACM Trans.",
                "Inf.",
                "Syst., 18(1):79-112, 2000. [20] E. YomTov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty.",
                "In Proc. of the 2005 ACM SIGIR Conf. on Research and Development in <br>information retrieval</br>, pages 512-519. [21] Y. Zhou and W. B. Croft.",
                "Ranking robustness: a novel framework to predict query performance.",
                "In Proc. of the 15th ACM Intl.",
                "Conf. on Information and Knowledge Mgmt. (CIKM 2006), pages 567-574."
            ],
            "original_annotated_samples": [
                "Categories and Subject Descriptors: H.3.3 [<br>information retrieval</br>]: Retrieval Models General Terms: Algorithms, Experimentation 1.",
                "INTRODUCTION Uncertainty is an inherent feature of <br>information retrieval</br>.",
                "For a specific framework for experiments, we use the language modeling (LM) approach for <br>information retrieval</br> [15].",
                "For <br>information retrieval</br>, the number of samples we will have available is likely to be quite small for performance reasons - usually less than ten.",
                "RELATED WORK Our approach is related to previous work from several areas of <br>information retrieval</br> and machine learning."
            ],
            "translated_annotated_samples": [
                "Categorías y Descriptores de Asignaturas: H.3.3 [Recuperación de Información]: Modelos de Recuperación Términos Generales: Algoritmos, Experimentación 1.",
                "INTRODUCCIÓN La incertidumbre es una característica inherente de la <br>recuperación de información</br>.",
                "Para un marco específico de experimentos, utilizamos el enfoque de modelado de lenguaje (LM) para la <br>recuperación de información</br> [15].",
                "Para la <br>recuperación de información</br>, es probable que el número de muestras disponibles sea bastante pequeño por razones de rendimiento, generalmente menos de diez.",
                "TRABAJO RELACIONADO Nuestro enfoque está relacionado con trabajos previos de varias áreas de <br>recuperación de información</br> y aprendizaje automático."
            ],
            "translated_text": "Estimación y Uso de la Incertidumbre en la Retroalimentación de Pseudo-Relevancia Kevyn Collins-Thompson y Jamie Callan Instituto de Tecnologías del Lenguaje Escuela de Ciencias de la Computación Universidad Carnegie Mellon Pittsburgh, PA 15213-8213 EE. UU. {kct | callan}@cs.cmu.edu RESUMEN Los métodos existentes de retroalimentación de pseudo-relevancia suelen realizar un promedio de los documentos mejor clasificados, pero ignoran una dimensión estadística importante: el riesgo o la varianza asociada tanto a los modelos de documentos individuales como a su combinación. Tratando el método de retroalimentación de línea base como una caja negra, y el modelo de retroalimentación de salida como una variable aleatoria, estimamos una distribución posterior para el modelo de retroalimentación mediante el remuestreo de los documentos mejor recuperados de una consulta dada, utilizando la media o la moda posterior como el modelo de retroalimentación mejorado. Luego realizamos la combinación de modelos sobre varios modelos mejorados, cada uno basado en una consulta ligeramente modificada muestreada de la consulta original. Observamos que el remuestreo de documentos ayuda a aumentar la precisión del modelo de retroalimentación individual al eliminar términos de ruido, mientras que el muestreo de la consulta mejora la robustez (rendimiento en el peor de los casos) al enfatizar términos relacionados con múltiples aspectos de la consulta. El resultado es un algoritmo de meta-retroalimentación que es tanto más robusto como más preciso que el método de referencia original. Categorías y Descriptores de Asignaturas: H.3.3 [Recuperación de Información]: Modelos de Recuperación Términos Generales: Algoritmos, Experimentación 1. INTRODUCCIÓN La incertidumbre es una característica inherente de la <br>recuperación de información</br>. No solo no sabemos las consultas que se presentarán a nuestro algoritmo de recuperación con anticipación, sino que la necesidad de información de los usuarios puede ser vaga o estar incompletamente especificada por estas consultas. Incluso si la consulta estuviera perfectamente especificada, el lenguaje en los documentos de la colección es inherentemente complejo y ambiguo, y hacer coincidir dicho lenguaje de manera efectiva es un problema formidable por sí mismo. Con esto en mente, deseamos tratar muchas cantidades importantes calculadas por el sistema de recuperación, ya sea un puntaje de relevancia para un documento, o un peso para un término de expansión de consulta, como variables aleatorias cuyo valor real es incierto pero donde la incertidumbre sobre el valor real puede ser cuantificada reemplazando el valor fijo con una distribución de probabilidad sobre posibles valores. De esta manera, los algoritmos de recuperación pueden intentar cuantificar el riesgo o la incertidumbre asociados con sus clasificaciones de salida, o mejorar la estabilidad o precisión de sus cálculos internos. Los algoritmos actuales para la retroalimentación de pseudo relevancia (PRF) tienden a seguir el mismo método básico ya sea que utilicemos algoritmos basados en espacio vectorial como la fórmula de Rocchio [16], o enfoques más recientes de modelado de lenguaje como Modelos de Relevancia [10]. Primero, se obtiene un conjunto de documentos recuperados en la parte superior a partir de una consulta inicial y se asume que aproxima un conjunto de documentos relevantes. A continuación, se calcula un vector de modelo de retroalimentación único según algún tipo de promedio, centroide o expectativa sobre el conjunto de modelos de documentos posiblemente relevantes. Por ejemplo, los vectores de documentos pueden combinarse con igual ponderación, como en Rocchio, o por probabilidad de consulta, como se puede hacer utilizando el Modelo de Relevancia. El uso de una expectativa es razonable por razones prácticas y teóricas, pero por sí sola ignora información potencialmente valiosa sobre el riesgo del modelo de retroalimentación. Nuestra hipótesis principal en este artículo es que estimar la incertidumbre en la retroalimentación es útil y conduce a modelos de retroalimentación individual más efectivos y a modelos combinados más robustos. Por lo tanto, proponemos un método para estimar la incertidumbre asociada con un modelo de retroalimentación individual en términos de una distribución posterior sobre modelos de lenguaje. Para hacer esto, variamos sistemáticamente las entradas al método de retroalimentación base y ajustamos una distribución de Dirichlet a la salida. Utilizamos la media posterior o el modo como la estimación mejorada del modelo de retroalimentación. Este proceso se muestra en la Figura 1. Como mostraremos más adelante, la media y la moda pueden variar significativamente del modelo de retroalimentación única propuesto por el método de referencia. También realizamos la combinación de modelos utilizando varios modelos de lenguaje de retroalimentación mejorados obtenidos a partir de un pequeño número de nuevas consultas muestreadas de la consulta original. El peso de un modelo combina dos factores complementarios: la probabilidad del modelo de generar la consulta y la varianza del modelo, siendo los modelos con alta varianza los que obtienen un peso menor. Por ejemplo, un vector de parámetros esperado condicionado a la observación de la consulta se forma a partir de los documentos recuperados en la parte superior, que se tratan como cadenas de entrenamiento (ver [10], p. 62). Figura 1: Estimación de la incertidumbre del modelo de retroalimentación para una sola consulta. 2. En las Secciones 2.1-2.5 describimos un método general para estimar una distribución de probabilidad sobre el conjunto de posibles modelos de lenguaje basado en muestreo. En las Secciones 2.6 y 2.7 resumimos cómo se utilizan diferentes muestras de consultas para generar múltiples modelos de retroalimentación, que luego se combinan. 2.1 Modelado de la Incertidumbre de la Retroalimentación Dado una consulta Q y una colección C, asumimos un sistema de recuperación probabilístico que asigna un puntaje de documento de valor real f(D, Q) a cada documento D en C, de manera que el puntaje sea proporcional a la probabilidad estimada de relevancia. No hacemos ninguna otra suposición sobre f(D, Q). La naturaleza de f(D, Q) puede ser compleja: por ejemplo, si el sistema de recuperación admite lenguajes de consulta estructurados [12], entonces f(D, Q) puede representar la salida de una red de inferencia arbitrariamente compleja definida por los operadores de consulta estructurados. En teoría, la función de puntuación puede variar de una consulta a otra, aunque en este estudio por simplicidad mantenemos la misma función de puntuación para todas las consultas. Nuestro método de consulta específico se encuentra en la Sección 3. Tratamos el algoritmo de retroalimentación como una caja negra y asumimos que los insumos del algoritmo de retroalimentación son la consulta original y los documentos recuperados principales correspondientes, a los cuales se les asigna una puntuación a cada documento. Suponemos que la salida del algoritmo de retroalimentación es un vector de pesos de términos que se utilizarán para agregar o reponderar los términos en la representación de la consulta original, con el vector normalizado para formar una distribución de probabilidad. Consideramos las entradas a la caja negra de retroalimentación como variables aleatorias, y analizamos el modelo de retroalimentación como una variable aleatoria que cambia en respuesta a cambios en las entradas. Al igual que la función de puntuación del documento f(D, Q), el algoritmo de retroalimentación puede implementar una fórmula de puntuación compleja y no lineal, por lo que, a medida que sus entradas varían, los modelos de retroalimentación resultantes pueden tener una distribución compleja en el espacio de modelos de retroalimentación (el espacio muestral). Debido a esta complejidad potencial, no intentamos derivar una distribución posterior de forma cerrada, sino que en su lugar utilizamos simulación. Llamamos a esta distribución sobre posibles modelos de retroalimentación la distribución de modelos de retroalimentación. Nuestro objetivo en esta sección es estimar una aproximación útil a la distribución del modelo de retroalimentación. Para un marco específico de experimentos, utilizamos el enfoque de modelado de lenguaje (LM) para la <br>recuperación de información</br> [15]. El puntaje de un documento D con respecto a una consulta Q y una colección C se da por p(Q|D) con respecto a los modelos de lenguaje ˆθQ y ˆθD estimados para la consulta y el documento respectivamente. Denotamos el conjunto de los k documentos mejor recuperados de la colección C en respuesta a Q como DQ(k, C). Para simplificar, asumimos que las consultas y documentos son generados por distribuciones multinomiales cuyos parámetros están representados por modelos de lenguaje unigram. Para incorporar retroalimentación en el enfoque de LM, asumimos un esquema basado en modelos en el que nuestro objetivo es tomar la consulta y los documentos clasificados resultantes DQ(k, C) como entrada, y producir un modelo de lenguaje expandido ˆθE, que luego se interpola con el modelo de consulta original ˆθQ: ˆθNew = (1 − α) · ˆθQ + α · ˆθE (1). Esto incluye la posibilidad de α = 1 donde el modelo de consulta original es completamente reemplazado por el modelo de retroalimentación. Nuestro espacio muestral es el conjunto de todos los posibles modelos de lenguaje LF que pueden ser generados como modelos de retroalimentación. Nuestro enfoque es tomar muestras de este espacio y luego ajustar una distribución a las muestras utilizando máxima verosimilitud. Para simplificar, comenzamos asumiendo que la distribución de retroalimentación latente tiene la forma de una distribución de Dirichlet. Aunque la distribución de Dirichlet es unimodal y en general bastante limitada en su expresividad en el espacio muestral, es una coincidencia natural para el modelo de lenguaje multinomial, puede ser estimada rápidamente y capturar las características más relevantes de los modelos de retroalimentación confiada e incierta, como la dispersión general de la distribución. 2.2 Re-muestreo de modelos de documentos. Nos gustaría una aproximación a la distribución posterior del modelo de retroalimentación LF. Para lograr esto, aplicamos una técnica de simulación ampliamente utilizada llamada muestreo bootstrap ([7], p. 474) en los parámetros de entrada, es decir, el conjunto de documentos recuperados en la parte superior. El muestreo de arranque nos permite simular el efecto aproximado de perturbar los parámetros dentro del algoritmo de retroalimentación de la caja negra al perturbar los insumos de ese algoritmo de manera sistemática, sin hacer suposiciones sobre la naturaleza del algoritmo de retroalimentación. Específicamente, muestreamos k documentos con reemplazo de DQ(k, C), y calculamos un modelo de lenguaje de expansión θb utilizando el método de retroalimentación de caja negra. Repetimos este proceso B veces para obtener un conjunto de B modelos de lenguaje de retroalimentación, a los cuales luego ajustamos una distribución de Dirichlet. Normalmente, B se encuentra en el rango de 20 a 50 muestras, con un rendimiento relativamente estable en este rango. Ten en cuenta que en lugar de tratar cada documento superior como igualmente probable, muestreamos de acuerdo con las probabilidades estimadas de relevancia de cada documento en DQ(k, C). Por lo tanto, es más probable que un documento sea elegido cuanto más alto esté en la clasificación. 2.3 Justificación de un enfoque de muestreo La justificación de nuestro enfoque de muestreo tiene dos partes. Primero, queremos mejorar la calidad de los modelos de retroalimentación individual al suavizar la variación cuando el modelo de retroalimentación base es inestable. En este sentido, nuestro enfoque se asemeja al bagging [4], un enfoque de conjunto que genera múltiples versiones de un predictor haciendo copias de arranque del conjunto de entrenamiento, y luego promedia los predictores (numéricos). En nuestra aplicación, los documentos recuperados en la parte superior pueden ser vistos como una especie de conjunto de entrenamiento ruidoso para la relevancia. Segundo, el muestreo es una forma efectiva de estimar propiedades básicas de la distribución posterior de retroalimentación, las cuales pueden ser utilizadas para mejorar la combinación de modelos. Por ejemplo, un modelo puede ser ponderado por su confianza de predicción, estimada como una función de la variabilidad del posterior alrededor del modelo. foo2-401.map-Dim:5434,Size:12*12units, vecindario gaussiano (a) Tema 401 Minorías extranjeras, Alemania foo2-402.map-Dim:5698,Size:12*12units, vecindario gaussiano (b) Tema 402 Genética del comportamiento foo2-459.map-Dim:8969,Size:12*12units, vecindario gaussiano (c) Tema 459 ¿Cuándo puede un prestamista ejecutar una hipoteca sobre una propiedad? Figura 2: Visualización de la varianza del modelo de lenguaje de expansión utilizando mapas autoorganizados, mostrando la distribución de modelos de lenguaje que resulta de remuestrear las entradas al método de expansión de referencia. El modelo de lenguaje que habría sido elegido por la expansión de la línea base se encuentra en el centro de cada mapa. La función de similitud es la divergencia JensenShannon. 2.4 Visualización de distribuciones de retroalimentación Antes de describir cómo ajustamos y utilizamos la distribución de Dirichlet sobre modelos de retroalimentación, es instructivo ver algunos ejemplos de distribuciones de modelos de retroalimentación reales que resultan de muestrear por bootstrap los documentos más recuperados de diferentes temas de TREC. Cada punto en nuestro espacio muestral es un modelo de lenguaje, que típicamente tiene varias miles de dimensiones. Para ayudar a analizar el comportamiento de nuestro método, utilizamos un Mapa Auto-organizado (a través del paquete SOM-PAK [9]), para aplanar y visualizar la función de densidad de alta dimensión. Los mapas de densidad para tres temas de TREC se muestran en la Figura 2 arriba. Las áreas oscuras representan regiones de alta similitud entre los modelos de lenguaje. Las áreas claras representan regiones de baja similitud, los valles entre los grupos. Cada diagrama está centrado en el modelo de lenguaje que habría sido elegido por la expansión de referencia. Un solo pico (modo) es evidente en algunos ejemplos, pero en otros aparece una estructura más compleja. Además, aunque la distribución suele estar cerca del modelo de retroalimentación de referencia, para algunos temas están a una distancia significativa (según la divergencia de JensenShannon), como se muestra en la Subfigura 2c. En tales casos, la moda o la media de la distribución de retroalimentación a menudo funcionan significativamente mejor que el valor base (y en una proporción menor de casos, significativamente peor). 2.5 Ajuste de una distribución de retroalimentación posterior Después de obtener muestras del modelo de retroalimentación mediante el remuestreo de las entradas del modelo de retroalimentación, estimamos la distribución de retroalimentación. Suponemos que los modelos de retroalimentación multinomial {ˆθ1, . . . , ˆθB} fueron generados por una distribución de Dirichlet latente con parámetros {α1, . . . , αN}. Para estimar los {α1, . . . , αN }, ajustamos los parámetros de Dirichlet a las muestras del modelo de lenguaje B según la máxima verosimilitud utilizando un procedimiento de Newton generalizado, cuyos detalles se encuentran en Minka [13]. Suponemos una distribución de Dirichlet simple sobre los {α1, . . . , αN}, estableciendo cada uno como αi = μ · p(wi | C), donde μ es un parámetro y p(· | C) es el modelo de lenguaje de colección estimado a partir de un conjunto de documentos de la colección C. El ajuste de parámetros converge muy rápidamente, generalmente en solo 2 o 3 iteraciones. Dado que nuestros puntos son modelos de lenguaje en el simplex multinomial, extendimos SOM-PAK para admitir la divergencia de Jensen-Shannon, una medida de similitud ampliamente utilizada entre distribuciones de probabilidad. 3 iteraciones son suficientes, por lo que es práctico aplicarlo en tiempo de consulta cuando la sobrecarga computacional debe ser pequeña. En la práctica, podemos restringir el cálculo al vocabulario de los documentos recuperados en la parte superior, en lugar de toda la colección. Ten en cuenta que para este paso estamos reutilizando los documentos recuperados existentes y no realizando consultas adicionales. Dado los parámetros de una distribución de Dirichlet N-dimensional Dir(α), los vectores de media μ y moda x son fáciles de calcular y se dan respectivamente por μi = αiP αi (2) y xi = αi−1P αi−N . (3) Luego podemos elegir el modelo de lenguaje en la media o en la moda del posterior como el modelo de retroalimentación mejorado final. (Encontramos que la moda proporciona un rendimiento ligeramente mejor). Para la <br>recuperación de información</br>, es probable que el número de muestras disponibles sea bastante pequeño por razones de rendimiento, generalmente menos de diez. Además, si bien el muestreo aleatorio es útil en ciertos casos, es perfectamente aceptable permitir distribuciones de muestreo determinísticas, pero estas deben diseñarse cuidadosamente para aproximar una varianza de salida precisa. Dejamos esto para estudios futuros. 2.6 Variantes de consulta Utilizamos los siguientes métodos para generar variantes de la consulta original. Cada variante corresponde a una suposición diferente sobre qué aspectos de la consulta original pueden ser importantes. Esta es una forma de muestreo determinístico. Seleccionamos tres métodos simples que cubren suposiciones complementarias sobre la consulta. Uso sin expansión. La suposición es que los términos dados son una descripción completa de la necesidad de información. Deja uno fuera. Se deja fuera un término del query original. La suposición es que uno de los términos de la consulta es un término de ruido. Se elige un término único de la consulta original. Esto asume que solo un aspecto de la consulta, es decir, el representado por el término, es el más importante. Después de generar una variante de la consulta original, la combinamos con la consulta original utilizando un peso αSUB para no alejarnos demasiado. En este estudio, establecimos αSUB = 0.5. Por ejemplo, utilizando el lenguaje de consulta Indri [12], una variante de dejar uno fuera de la consulta inicial que omite el término \"ireland\" para el tema TREC 404 es: #weight(0.5 #combine(ireland peace talks) 0.5 #combine(peace talks)) 2.7 Combinando modelos de retroalimentación mejorados de múltiples variantes de consulta. Cuando se utilizan múltiples variantes de consulta, los modelos de retroalimentación mejorados resultantes se combinan utilizando la combinación de modelos bayesianos. Para hacer esto, tratamos cada palabra como un elemento a clasificar como perteneciente a una clase relevante o no relevante, y derivamos una probabilidad de clase para cada palabra combinando las puntuaciones de cada variante de consulta. Cada puntuación es dada por la probabilidad de ese término en la distribución de Dirichlet. Las puntuaciones de los términos se ponderan por el inverso de la varianza del término en los modelos de retroalimentación mejorada de la distribución de Dirichlet. La probabilidad previa de pertenencia de una palabra a la clase relevante se da por la probabilidad de la consulta original en todo el modelo de expansión mejorado. 3. EVALUACIÓN En esta sección presentamos resultados que confirman la utilidad de estimar una distribución del modelo de retroalimentación a partir del remuestreo ponderado de documentos mejor clasificados, y de combinar los modelos de retroalimentación obtenidos a partir de diferentes pequeños cambios en la consulta original. 3.1 Método general Evaluamos el rendimiento en un total de 350 consultas derivadas de cuatro conjuntos de temas de TREC: 51-200 (TREC-1&2), 351-400 (TREC-7), 401-450 (TREC-8) y 451-550 (wt10g, TREC-9&10). Elegimos estos por su contenido variado y propiedades del documento. Por ejemplo, los documentos wt10g son páginas web con una amplia variedad de temas y estilos, mientras que los documentos TREC-1&2 son artículos de noticias más homogéneos. La indexación y recuperación se realizó utilizando el sistema Indri en el kit de herramientas Lemur [12] [1]. Nuestras consultas se derivaron de las palabras en el campo del título de los temas de TREC. Las frases no fueron utilizadas. Para generar las consultas base que se pasaron a Indri, envolvimos los términos de la consulta con el operador #combine de Indri. Por ejemplo, la consulta inicial para el tema 404 es: #combine(ireland peace talks) Realizamos el stemming de Krovetz para todos los experimentos. Debido a que encontramos que el método de expansión de la base (Indri) funcionó mejor utilizando una lista de palabras vacías con el modelo de retroalimentación, todos los experimentos utilizaron una lista de 419 palabras comunes en inglés. Sin embargo, un efecto secundario interesante de nuestro enfoque de remuestreo es que tiende a eliminar muchas palabras vacías del modelo de retroalimentación, lo que hace que una lista de paradas sea menos crítica. Esto se discute más a fondo en la Sección 3.6. Método de retroalimentación de línea base 3.2 Para nuestro método de expansión de línea base, utilizamos un algoritmo incluido en Indri 1.0 como el método de expansión predeterminado. Este método primero selecciona términos utilizando un cálculo de logaritmo de probabilidades descrito por Ponte [14], pero asigna pesos finales a los términos utilizando el modelo de relevancia de Lavrenko [10]. Elegimos el método Indri porque proporciona una línea base consistentemente sólida, se basa en un enfoque de modelado del lenguaje y es fácil de experimentar con él. En una evaluación de TREC utilizando el corpus GOV2, el método fue uno de los mejores resultados, logrando un aumento del 19.8% en el MAP en comparación con el uso de consultas no expandidas. En este estudio, se logra una ganancia promedio en MAP del 17.25% en las cuatro colecciones. El método de expansión de Indri primero calcula una razón de logaritmos de probabilidades o(v) para cada término de expansión potencial v dado por o(v) = X D log p(v|D) p(v|C) (4) sobre todos los documentos D que contienen v, en la colección C. Luego, los candidatos a términos de expansión se ordenan por o(v) descendente, y se eligen los primeros m. Finalmente, los pesos de término r(v) utilizados en la consulta ampliada se calculan en base al modelo de relevancia r(v) = X D p(q|D)p(v|D) p(v) p(D) (5). La cantidad p(q|D) es la puntuación de probabilidad asignada al documento en la recuperación inicial. Utilizamos suavizado de Dirichlet de p(v|D) con μ = 1000. Este modelo de relevancia se combina luego con la consulta original utilizando interpolación lineal, ponderada por un parámetro α. Por defecto, utilizamos los 50 documentos principales para la retroalimentación y los 20 términos de expansión, con el parámetro de interpolación de retroalimentación α = 0.5 a menos que se indique lo contrario. Por ejemplo, la consulta expandida base para el tema 404 es: #weight(0.5 #combine(ireland peace talks) 0.5 #weight(0.10 ireland 0.08 peace 0.08 northern ...) 3.3 Rendimiento de la expansión. Medimos la efectividad de nuestros algoritmos de retroalimentación mediante dos criterios principales: precisión y robustez. La robustez, y el equilibrio entre precisión y robustez, se analiza en la Sección 3.4. En esta sección, examinamos la precisión promedio y la precisión en los 10 documentos principales (P10). También incluimos la recuperación de 1,000 documentos. Para cada consulta, obtuvimos un conjunto de B modelos de retroalimentación utilizando la línea base de Indri. Cada modelo de retroalimentación se obtuvo a partir de una muestra aleatoria de los k documentos principales tomados con reemplazo. Para estos experimentos, B = 30 y k = 50. Cada modelo de retroalimentación contenía 20 términos. En el lado de la consulta, utilizamos muestreo de dejar uno fuera (LOO) para crear las variantes de consulta. El muestreo de consulta de un solo término tuvo un rendimiento consistentemente peor en todas las colecciones, por lo que nuestros resultados se centran en el muestreo de LOO aquí. Utilizamos los métodos descritos en la Sección 2 para estimar un modelo de retroalimentación mejorado a partir de la distribución posterior de Dirichlet para cada variante de consulta, y para combinar los modelos de retroalimentación de todas las variantes de consulta. Llamamos a nuestro método expansión de remuestreo y lo denotamos como RS-FB aquí. Denominamos al método de retroalimentación de línea base Indri como Base-FB. Los resultados de aplicar tanto el método de expansión de línea base (Base-FB) como la expansión de remuestreo (RS-FB) se muestran en la Tabla 1. Observamos varias tendencias en esta tabla. Primero, la precisión promedio de RS-FB fue comparable a Base-FB, logrando una ganancia promedio del 17.6% en comparación con no utilizar expansión en las cuatro colecciones. La ganancia de expansión de la línea base de Indri fue del 17.25%. Además, el método RS-FB logró mejoras consistentes en P10 sobre Base-FB para cada conjunto de temas, con una mejora promedio del 6.89% sobre Base-FB para los 350 temas. La ganancia P10 más baja sobre Base-FB fue del +3.82% para TREC-7 y la más alta fue del +11.95% para wt10g. Finalmente, tanto Base-FB como RS-FB también mejoraron consistentemente la recuperación en comparación con no utilizar expansión, siendo Base-FB el que logró una mejor recuperación que RS-FB para todos los conjuntos de temas. 3.4 Robustez de recuperación. Utilizamos el término robustez para referirnos al rendimiento de precisión promedio en el peor de los casos de un algoritmo de retroalimentación. Idealmente, un método de retroalimentación sólido nunca debería funcionar peor que usar la consulta original, mientras que a menudo funciona mejor utilizando la expansión. Para evaluar la robustez en este estudio, utilizamos una medida muy simple llamada índice de robustez (RI). Para un conjunto de consultas Q, la medida de RI se define como: RI(Q) = n+ − n− |Q| (6) donde n+ es el número de consultas ayudadas por el método de retroalimentación y n− es el número de consultas perjudicadas. Aquí, por \"ayudar\" nos referimos a obtener una precisión promedio más alta como resultado de la retroalimentación. El valor de RI varía desde un mínimo de 3. A veces también se le llama índice de confiabilidad de mejora y fue utilizado en Sakai et al. [17]. Colección NoExp Base-FB RS-FB TREC 1&2 AvgP 0.1818 0.2419 (+33.04%) 0.2406 (+32.24%) P10 0.4443 0.4913 (+10.57%) 0.5363 (+17.83%) Recall 15084/37393 19172/37393 15396/37393 TREC 7 AvgP 0.1890 0.2175 (+15.07%) 0.2169 (+14.75%) P10 0.4200 0.4320 (+2.85%) 0.4480 (+6.67%) Recall 2179/4674 2608/4674 2487/4674 TREC 8 AvgP 0.2031 0.2361 (+16.25%) 0.2268 (+11.70%) P10 0.3960 0.4160 (+5.05%) 0.4340 (+9.59%) Recall 2144/4728 2642/4728 2485/4728 wt10g AvgP 0.1741 0.1829 (+5.06%) 0.1946 (+11.78%) P10 0.2760 0.2630 (-4.71%) 0.2960 (+7.24%) Recall 3361/5980 3725/5980 3664/5980 Tabla 1: Comparación de la retroalimentación base (Base-FB) y la retroalimentación utilizando re-muestreo (RS-FB). La mejora mostrada para BaseFB y RS-FB es relativa al uso de ninguna expansión. (a) TREC 1&2 (curva superior); TREC 8 (curva inferior) (b) TREC 7 (curva superior); wt10g (curva inferior) Figura 3: La compensación entre robustez y precisión promedio para diferentes corpora. El eje x muestra el cambio en la MAP al utilizar la expansión de la línea base con α = 0.5. El eje y muestra el Índice de Robustez (RI). Cada curva a través de los puntos no circulados muestra el compromiso entre RI/MAP utilizando la estrategia simple de pequeño α (ver texto) a medida que α disminuye de 0.5 a cero en la dirección de la flecha. Los puntos circulados representan los compromisos obtenidos al volver a muestrear la retroalimentación para α = 0.5. Colección N Base-FB RS-FB n− RI n− RI TREC 1&2 103 26 +0.495 15 +0.709 TREC 7 46 14 +0.391 10 +0.565 TREC 8 44 12 +0.455 12 +0.455 wt10g 91 48 -0.055 39 +0.143 Combinado 284 100 +0.296 76 +0.465 Tabla 2: Comparación del índice de robustez (RI) para retroalimentación de línea base (Base-FB) vs. retroalimentación de remuestreo (RS-FB). También se muestran el número real de consultas perjudicadas por la retroalimentación (n−) para cada método y colección. Las consultas para las cuales la precisión promedio inicial era insignificante (≤ 0.01) fueron ignoradas, dando como resultado el recuento de consultas restantes en la columna N, de -1.0, cuando todas las consultas son perjudicadas por el método de retroalimentación, a +1.0 cuando todas las consultas son ayudadas. La medida de RI no tiene en cuenta la magnitud o distribución de la cantidad de cambio en el conjunto Q. Sin embargo, es fácil de entender como una indicación general de robustez. Una forma obvia de mejorar el rendimiento en el peor de los casos de la retroalimentación es simplemente utilizar un parámetro de interpolación α fijo más pequeño, como α = 0.3, otorgando menos peso al modelo de retroalimentación (posiblemente arriesgado) y más al cuestionario original. Llamamos a esto la estrategia de pequeña α. Dado que también estamos reduciendo las ganancias potenciales cuando el modelo de retroalimentación es correcto, sin embargo, esperaríamos algún tipo de compensación entre la precisión promedio y la robustez. Por lo tanto, comparamos el equilibrio entre precisión y robustez entre nuestro algoritmo de retroalimentación de remuestreo y el método simple de pequeño α. Los resultados se resumen en la Figura 3. En la figura, la curva para cada conjunto de temas interpola entre los puntos de compensación, comenzando en x=0, donde α = 0.5, y continuando en la dirección de la flecha a medida que α disminuye y la consulta original recibe cada vez más peso. Como era de esperar, la robustez aumenta continuamente a medida que avanzamos a lo largo de la curva, pero la precisión media promedio generalmente disminuye a medida que se eliminan las ganancias del feedback. Para la comparación, se muestra el rendimiento del re-muestreo con retroalimentación en α = 0.5 para cada colección como el punto marcado con un círculo. Más alto y a la derecha es mejor. Esta figura muestra que el re-muestreo de retroalimentación ofrece un compromiso algo mejor que el enfoque de pequeño α para 3 de las 4 colecciones. Figura 4: Histograma que muestra la mayor robustez del re-muestreo de retroalimentación (RS-FB) sobre la retroalimentación base (Base-FB) para todos los conjuntos de datos combinados. Las consultas se agrupan por el % de cambio en AP en comparación con la consulta no expandida. Colección DS + QV DS + No QV TREC 1&2 AvgP 0.2406 0.2547 (+5.86%) P10 0.5263 0.5362 (+1.88%) RI 0.7087 0.6515 (-0.0572) TREC 7 AvgP 0.2169 0.2200 (+1.43%) P10 0.4480 0.4300 (-4.02%) RI 0.5652 0.2609 (-0.3043) TREC 8 AvgP 0.2268 0.2257 (-0.49%) P10 0.4340 0.4200 (-3.23%) RI 0.4545 0.4091 (-0.0454) wt10g AvgP 0.1946 0.1865 (-4.16%) P10 0.2960 0.2680 (-9.46%) RI 0.1429 0.0220 (-0.1209) Tabla 3: Comparación de la retroalimentación de remuestreo utilizando muestreo de documentos (DS) con (QV) y sin (No QV) combinar modelos de retroalimentación de múltiples variantes de consulta. La Tabla 2 muestra los puntajes del Índice de Robustez para Base-FB y RS-FB. El método de retroalimentación RS-FB obtuvo una mayor robustez que Base-FB en tres de los cuatro conjuntos de temas, con solo un rendimiento ligeramente peor en TREC-8. Una vista más detallada que muestra la distribución de los cambios relativos en AP se presenta en el histograma de la Figura 4. En comparación con Base-FB, el método RS-FB logra una reducción notable en el número de consultas significativamente afectadas por la expansión (es decir, donde AP se ve afectado en un 25% o más), al tiempo que conserva ganancias positivas en AP. 3.5 Efecto de los métodos de muestreo de consultas y documentos Dada la mayor robustez de nuestros algoritmos observada en la Sección 3.4, una pregunta importante es qué componente de nuestro sistema es responsable. ¿Es el uso de la re-muestreo de documentos, el uso de múltiples variantes de consulta, o algún otro factor? Los resultados en la Tabla 3 sugieren que la combinación de modelos basada en variantes de consulta puede ser en gran medida responsable de la mayor robustez. Cuando se desactivan las variantes de consulta y se utiliza la consulta original por sí sola con muestreo de documentos, hay poco cambio neto en la precisión promedio, una pequeña disminución en P10 para 3 de los 4 conjuntos de temas, pero una caída significativa en la robustez para todos los conjuntos de temas. En dos casos, la medida de RI disminuye en más del 50%. También examinamos el efecto del método de muestreo de documentos en la efectividad de la recuperación, utilizando dos estrategias diferentes. La estrategia de ponderación uniforme ignoró las puntuaciones de relevancia de la recuperación inicial y otorgó a cada documento en los primeros k la misma probabilidad de selección. Por el contrario, la estrategia de ponderación de puntajes de relevancia seleccionó documentos con una probabilidad proporcional a sus puntajes de relevancia. De esta manera, los documentos que tenían una clasificación más alta tenían más probabilidades de ser seleccionados. Los resultados se muestran en la Tabla 4. La estrategia de ponderación de la puntuación de relevancia tiene un mejor rendimiento en general, con puntajes de RI y P10 significativamente más altos en 3 de los 4 conjuntos de temas. La diferencia en la precisión promedio entre los métodos, sin embargo, es menos marcada. Esto sugiere que el peso uniforme actúa para aumentar la varianza en los resultados de recuperación: cuando la precisión promedio inicial es alta, hay muchos documentos relevantes en los primeros k y el muestreo uniforme puede proporcionar un modelo de relevancia más representativo que centrarse en los elementos altamente clasificados. Por otro lado, cuando la precisión inicial es baja, hay pocos documentos relevantes en las posiciones inferiores y el muestreo uniforme mezcla más de los documentos no relevantes. Por razones de espacio, solo resumimos nuestros hallazgos sobre el tamaño de la muestra aquí. El número de muestras tiene cierto efecto en la precisión cuando es menor a 10, pero el rendimiento se estabiliza alrededor de 15 a 20 muestras. Utilizamos 30 muestras para nuestros experimentos. Mucho más allá de este nivel, los beneficios adicionales de más muestras disminuyen a medida que la distribución inicial de puntajes se ajusta más estrechamente y aumenta el tiempo de procesamiento. 3.6 El efecto del remuestreo en la calidad del término de expansión Idealmente, un modelo de recuperación no debería requerir una lista de palabras vacías al estimar un modelo de relevancia: un modelo estadístico robusto debería reducir automáticamente el peso de las palabras vacías dependiendo del contexto. Las palabras vacías pueden perjudicar la retroalimentación si se seleccionan como términos de retroalimentación, ya que suelen ser malos discriminadores y desperdician espacios de términos valiosos. En la práctica, sin embargo, debido a que la mayoría de los métodos de selección de términos se asemejan a un tipo de ponderación tf · idf, a veces se pueden seleccionar términos con un idf bajo pero un tf muy alto como candidatos de términos de expansión. Esto sucede, por ejemplo, incluso con el enfoque del Modelo de Relevancia que forma parte de nuestra retroalimentación base. Para garantizar una línea base lo más sólida posible, utilizamos una lista de paradas para todos los experimentos reportados aquí. Si desactivamos la lista de palabras vacías, sin embargo, obtenemos resultados como los mostrados en la Tabla 5 donde cuatro de los diez términos de retroalimentación básica principales para el tema 60 de TREC (dijo, pero, su, no) son palabras vacías utilizando el método BaseFB. (Se seleccionaron los 100 términos de expansión principales para generar este ejemplo). El método Indris intenta abordar el problema de las palabras vacías aplicando un paso inicial basado en Ponte [14] para seleccionar términos menos comunes que tienen altas probabilidades logarítmicas de estar en los documentos mejor clasificados en comparación con toda la colección. Sin embargo, esto no resuelve completamente el problema de las palabras vacías, especialmente a medida que aumenta el número de términos de retroalimentación. Sin embargo, el uso de retroalimentación de remuestreo parece mitigar la ponderación de la colección QV + QV uniforme + ponderación de puntaje de relevancia TREC 1&2 AvgP 0.2545 0.2406 (-5.46%) P10 0.5369 0.5263 (-1.97%) RI 0.6212 0.7087 (+14.09%) TREC 7 AvgP 0.2174 0.2169 (-0.23%) P10 0.4320 0.4480 (+3.70%) RI 0.4783 0.5652 (+18.17%) TREC 8 AvgP 0.2267 0.2268 (+0.04%) P10 0.4120 0.4340 (+5.34%) RI 0.4545 0.4545 (+0.00%) wt10g AvgP 0.1808 0.1946 (+7.63%) P10 0.2680 0.2960 (+10.45%) RI 0.0220 0.1099 (+399.5%) Tabla 4: Comparación de muestreo de documentos uniforme y ponderado por relevancia. El cambio porcentual en comparación con el muestreo uniforme se muestra entre paréntesis. QV indica que se utilizaron variantes de consulta en ambas ejecuciones. FB de línea base p(wi|R) FB de remuestreo p(wi|R) dijo 0.055 corte 0.026 corte 0.055 pagar 0.018 pagar 0.034 federal 0.012 pero 0.026 educación 0.011 empleados 0.024 maestros 0.010 sus 0.024 empleados 0.010 no 0.023 caso 0.010 federal 0.021 sus 0.009 trabajadores 0.020 apelaciones 0.008 educación 0.020 sindicato 0.007 Tabla 5: Calidad del término de retroalimentación cuando no se utiliza una lista de paradas. Términos de retroalimentación para el tema 60 de TREC: pago por mérito vs antigüedad. el efecto de las palabras vacías automáticamente. En el ejemplo de la Tabla 5, la retroalimentación de remuestreo deja solo una palabra vacía (su) en las diez primeras posiciones. Observamos un comportamiento similar del término de retroalimentación en muchos otros temas. La razón de este efecto parece ser la interacción de la puntuación de selección de términos con el límite superior de términos m. Si bien la presencia e incluso la proporción de palabras vacías particulares es bastante estable en diferentes muestras de documentos, su posición relativa en la lista de los primeros m elementos no lo es, ya que se examinan conjuntos de documentos con diferentes números de candidatos de términos mejores y de menor frecuencia para cada muestra. Como resultado, si bien algunos stopwords pueden aparecer en cada conjunto de documentos muestreados, tiende a ocurrir que cualquier stopword dado caiga por debajo del umbral para múltiples muestras, lo que lleva a su clasificación como una característica de alta varianza y bajo peso. 4. TRABAJO RELACIONADO Nuestro enfoque está relacionado con trabajos previos de varias áreas de <br>recuperación de información</br> y aprendizaje automático. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "query expansion": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Estimation and Use of Uncertainty in Pseudo-relevance Feedback Kevyn Collins-Thompson and Jamie Callan Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213-8213 U.S.A. {kct | callan}@cs.cmu.edu ABSTRACT Existing pseudo-relevance feedback methods typically perform averaging over the top-retrieved documents, but ignore an important statistical dimension: the risk or variance associated with either the individual document models, or their combination.",
                "Treating the baseline feedback method as a black box, and the output feedback model as a random variable, we estimate a posterior distribution for the feedback model by resampling a given querys top-retrieved documents, using the posterior mean or mode as the enhanced feedback model.",
                "We then perform model combination over several enhanced models, each based on a slightly modified query sampled from the original query.",
                "We find that resampling documents helps increase individual feedback model precision by removing noise terms, while sampling from the query improves robustness (worst-case performance) by emphasizing terms related to multiple query aspects.",
                "The result is a meta-feedback algorithm that is both more robust and more precise than the original strong baseline method.",
                "Categories and Subject Descriptors: H.3.3 [Information Retrieval]: Retrieval Models General Terms: Algorithms, Experimentation 1.",
                "INTRODUCTION Uncertainty is an inherent feature of information retrieval.",
                "Not only do we not know the queries that will be presented to our retrieval algorithm ahead of time, but the users information need may be vague or incompletely specified by these queries.",
                "Even if the query were perfectly specified, language in the collection documents is inherently complex and ambiguous and matching such language effectively is a formidable problem by itself.",
                "With this in mind, we wish to treat many important quantities calculated by the retrieval system, whether a relevance score for a document, or a weight for a <br>query expansion</br> term, as random variables whose true value is uncertain but where the uncertainty about the true value may be quantified by replacing the fixed value with a probability distribution over possible values.",
                "In this way, retrieval algorithms may attempt to quantify the risk or uncertainty associated with their output rankings, or improve the stability or precision of their internal calculations.",
                "Current algorithms for pseudo-relevance feedback (PRF) tend to follow the same basic method whether we use vector space-based algorithms such as Rocchios formula [16], or more recent language modeling approaches such as Relevance Models [10].",
                "First, a set of top-retrieved documents is obtained from an initial query and assumed to approximate a set of relevant documents.",
                "Next, a single feedback model vector is computed according to some sort of average, centroid, or expectation over the set of possibly-relevant document models.",
                "For example, the document vectors may be combined with equal weighting, as in Rocchio, or by query likelihood, as may be done using the Relevance Model1 .",
                "The use of an expectation is reasonable for practical and theoretical reasons, but by itself ignores potentially valuable information about the risk of the feedback model.",
                "Our main hypothesis in this paper is that estimating the uncertainty in feedback is useful and leads to better individual feedback models and more robust combined models.",
                "Therefore, we propose a method for estimating uncertainty associated with an individual feedback model in terms of a posterior distribution over language models.",
                "To do this, we systematically vary the inputs to the baseline feedback method and fit a Dirichlet distribution to the output.",
                "We use the posterior mean or mode as the improved feedback model estimate.",
                "This process is shown in Figure 1.",
                "As we show later, the mean and mode may vary significantly from the single feedback model proposed by the baseline method.",
                "We also perform model combination using several improved feedback language models obtained by a small number of new queries sampled from the original query.",
                "A models weight combines two complementary factors: the models probability of generating the query, and the variance of the model, with high-variance models getting lower weight. 1 For example, an expected parameter vector conditioned on the query observation is formed from top-retrieved documents, which are treated as training strings (see [10], p. 62).",
                "Figure 1: Estimating the uncertainty of the feedback model for a single query. 2.",
                "SAMPLING-BASED FEEDBACK In Sections 2.1-2.5 we describe a general method for estimating a probability distribution over the set of possible language models.",
                "In Sections 2.6 and 2.7 we summarize how different query samples are used to generate multiple feedback models, which are then combined. 2.1 Modeling Feedback Uncertainty Given a query Q and a collection C, we assume a probabilistic retrieval system that assigns a real-valued document score f(D, Q) to each document D in C, such that the score is proportional to the estimated probability of relevance.",
                "We make no other assumptions about f(D, Q).",
                "The nature of f(D, Q) may be complex: for example, if the retrieval system supports structured query languages [12], then f(D, Q) may represent the output of an arbitrarily complex inference network defined by the structured query operators.",
                "In theory, the scoring function can vary from query to query, although in this study for simplicity we keep the scoring function the same for all queries.",
                "Our specific query method is given in Section 3.",
                "We treat the feedback algorithm as a black box and assume that the inputs to the feedback algorithm are the original query and the corresponding top-retrieved documents, with a score being given to each document.",
                "We assume that the output of the feedback algorithm is a vector of term weights to be used to add or reweight the terms in the representation of the original query, with the vector normalized to form a probability distribution.",
                "We view the the inputs to the feedback black box as random variables, and analyze the feedback model as a random variable that changes in response to changes in the inputs.",
                "Like the document scoring function f(D, Q), the feedback algorithm may implement a complex, non-linear scoring formula, and so as its inputs vary, the resulting feedback models may have a complex distribution over the space of feedback models (the sample space).",
                "Because of this potential complexity, we do not attempt to derive a posterior distribution in closed form, but instead use simulation.",
                "We call this distribution over possible feedback models the feedback model distribution.",
                "Our goal in this section is to estimate a useful approximation to the feedback model distribution.",
                "For a specific framework for experiments, we use the language modeling (LM) approach for information retrieval [15].",
                "The score of a document D with respect to a query Q and collection C is given by p(Q|D) with respect to language models ˆθQ and ˆθD estimated for the query and document respectively.",
                "We denote the set of k top-retrieved documents from collection C in response to Q by DQ(k, C).",
                "For simplicity, we assume that queries and documents are generated by multinomial distributions whose parameters are represented by unigram language models.",
                "To incorporate feedback in the LM approach, we assume a model-based scheme in which our goal is take the query and resulting ranked documents DQ(k, C) as input, and output an expansion language model ˆθE, which is then interpolated with the original query model ˆθQ: ˆθNew = (1 − α) · ˆθQ + α · ˆθE (1) This includes the possibility of α = 1 where the original query mode is completely replaced by the feedback model.",
                "Our sample space is the set of all possible language models LF that may be output as feedback models.",
                "Our approach is to take samples from this space and then fit a distribution to the samples using maximum likelihood.",
                "For simplicity, we start by assuming the latent feedback distribution has the form of a Dirichlet distribution.",
                "Although the Dirichlet is a unimodal distribution, and in general quite limited in its expressiveness in the sample space, it is a natural match for the multinomial language model, can be estimated quickly, and can capture the most salient features of confident and uncertain feedback models, such as the overall spread of the distibution. 2.2 Resampling document models We would like an approximation to the posterior distribution of the feedback model LF .",
                "To accomplish this, we apply a widely-used simulation technique called bootstrap sampling ([7], p. 474) on the input parameters, namely, the set of top-retrieved documents.",
                "Bootstrap sampling allows us to simulate the approximate effect of perturbing the parameters within the black box feedback algorithm by perturbing the inputs to that algorithm in a systematic way, while making no assumptions about the nature of the feedback algorithm.",
                "Specifically, we sample k documents with replacement from DQ(k, C), and calculate an expansion language model θb using the black box feedback method.",
                "We repeat this process B times to obtain a set of B feedback language models, to which we then fit a Dirichlet distribution.",
                "Typically B is in the range of 20 to 50 samples, with performance being relatively stable in this range.",
                "Note that instead of treating each top document as equally likely, we sample according to the estimated probabilities of relevance of each document in DQ(k, C).",
                "Thus, a document is more likely to be chosen the higher it is in the ranking. 2.3 Justification for a sampling approach The rationale for our sampling approach has two parts.",
                "First, we want to improve the quality of individual feedback models by smoothing out variation when the baseline feedback model is unstable.",
                "In this respect, our approach resembles bagging [4], an ensemble approach which generates multiple versions of a predictor by making bootstrap copies of the training set, and then averages the (numerical) predictors.",
                "In our application, top-retrieved documents can be seen as a kind of noisy training set for relevance.",
                "Second, sampling is an effective way to estimate basic properties of the feedback posterior distribution, which can then be used for improved model combination.",
                "For example, a model may be weighted by its prediction confidence, estimated as a function of the variability of the posterior around the model. foo2-401.map-Dim:5434,Size:12*12units,gaussianneighborhood (a) Topic 401 Foreign minorities, Germany foo2-402.map-Dim:5698,Size:12*12units,gaussianneighborhood (b) Topic 402 Behavioral genetics foo2-459.map-Dim:8969,Size:12*12units,gaussianneighborhood (c) Topic 459 When can a lender foreclose on property Figure 2: Visualization of expansion language model variance using self-organizing maps, showing the distribution of language models that results from resampling the inputs to the baseline expansion method.",
                "The language model that would have been chosen by the baseline expansion is at the center of each map.",
                "The similarity function is JensenShannon divergence. 2.4 Visualizing feedback distributions Before describing how we fit and use the Dirichlet distribution over feedback models, it is instructive to view some examples of actual feedback model distributions that result from bootstrap sampling the top-retrieved documents from different TREC topics.",
                "Each point in our sample space is a language model, which typically has several thousand dimensions.",
                "To help analyze the behavior of our method we used a Self-Organizing Map (via the SOM-PAK package [9]), to flatten and visualize the high-dimensional density function2 .",
                "The density maps for three TREC topics are shown in Figure 2 above.",
                "The dark areas represent regions of high similarity between language models.",
                "The light areas represent regions of low similarity - the valleys between clusters.",
                "Each diagram is centered on the language model that would have been chosen by the baseline expansion.",
                "A single peak (mode) is evident in some examples, but more complex structure appears in others.",
                "Also, while the distribution is usually close to the baseline feedback model, for some topics they are a significant distance apart (as measured by JensenShannon divergence), as in Subfigure 2c.",
                "In such cases, the mode or mean of the feedback distribution often performs significantly better than the baseline (and in a smaller proportion of cases, significantly worse). 2.5 Fitting a posterior feedback distribution After obtaining feedback model samples by resampling the feedback model inputs, we estimate the feedback distribution.",
                "We assume that the multinomial feedback models {ˆθ1, . . . , ˆθB} were generated by a latent Dirichlet distribution with parameters {α1, . . . , αN }.",
                "To estimate the {α1, . . . , αN }, we fit the Dirichlet parameters to the B language model samples according to maximum likelihood using a generalized Newton procedure, details of which are given in Minka [13].",
                "We assume a simple Dirichlet prior over the {α1, . . . , αN }, setting each to αi = μ · p(wi | C), where μ is a parameter and p(· | C) is the collection language model estimated from a set of documents from collection C. The parameter fitting converges very quickly - typically just 2 or 2 Because our points are language models in the multinomial simplex, we extended SOM-PAK to support JensenShannon divergence, a widely-used similarity measure between probability distributions. 3 iterations are enough - so that it is practical to apply at query-time when computational overhead must be small.",
                "In practice, we can restrict the calculation to the vocabulary of the top-retrieved documents, instead of the entire collection.",
                "Note that for this step we are re-using the existing retrieved documents and not performing additional queries.",
                "Given the parameters of an N-dimensional Dirichlet distribution Dir(α) the mean μ and mode x vectors are easy to calculate and are given respectively by μi = αiP αi (2) and xi = αi−1P αi−N . (3) We can then choose the language model at the mean or the mode of the posterior as the final enhanced feedback model. (We found the mode to give slightly better performance.)",
                "For information retrieval, the number of samples we will have available is likely to be quite small for performance reasons - usually less than ten.",
                "Moreover, while random sampling is useful in certain cases, it is perfectly acceptable to allow deterministic sampling distributions, but these must be designed carefully in order to approximate an accurate output variance.",
                "We leave this for future study. 2.6 Query variants We use the following methods for generating variants of the original query.",
                "Each variant corresponds to a different assumption about which aspects of the original query may be important.",
                "This is a form of deterministic sampling.",
                "We selected three simple methods that cover complimentary assumptions about the query.",
                "No-expansion Use only the original query.",
                "The assumption is that the given terms are a complete description of the information need.",
                "Leave-one-out A single term is left out of the original query.",
                "The assumption is that one of the query terms is a noise term.",
                "Single-term A single term is chosen from the original query.",
                "This assumes that only one aspect of the query, namely, that represented by the term, is most important.",
                "After generating a variant of the original query, we combine it with the original query using a weight αSUB so that we do not stray too far.",
                "In this study, we set αSUB = 0.5.",
                "For example, using the Indri [12] query language, a leave-oneout variant of the initial query that omits the term ireland for TREC topic 404 is: #weight(0.5 #combine(ireland peace talks) 0.5 #combine(peace talks)) 2.7 Combining enhanced feedback models from multiple query variants When using multiple query variants, the resulting enhanced feedback models are combined using Bayesian model combination.",
                "To do this, we treat each word as an item to be classified as belonging to a relevant or non-relevant class, and derive a class probability for each word by combining the scores from each query variant.",
                "Each score is given by that terms probability in the Dirichlet distribution.",
                "The term scores are weighted by the inverse of the variance of the term in the enhanced feedback models Dirichlet distribution.",
                "The prior probability of a words membership in the relevant class is given by the probability of the original query in the entire enhanced expansion model. 3.",
                "EVALUATION In this section we present results confirming the usefulness of estimating a feedback model distribution from weighted resampling of top-ranked documents, and of combining the feedback models obtained from different small changes in the original query. 3.1 General method We evaluated performance on a total of 350 queries derived from four sets of TREC topics: 51-200 (TREC-1&2), 351-400 (TREC-7), 401-450 (TREC-8), and 451-550 (wt10g, TREC-9&10).",
                "We chose these for their varied content and document properties.",
                "For example, wt10g documents are Web pages with a wide variety of subjects and styles while TREC-1&2 documents are more homogeneous news articles.",
                "Indexing and retrieval was performed using the Indri system in the Lemur toolkit [12] [1].",
                "Our queries were derived from the words in the title field of the TREC topics.",
                "Phrases were not used.",
                "To generate the baseline queries passed to Indri, we wrapped the query terms with Indris #combine operator.",
                "For example, the initial query for topic 404 is: #combine(ireland peace talks) We performed Krovetz stemming for all experiments.",
                "Because we found that the baseline (Indri) expansion method performed better using a stopword list with the feedback model, all experiments used a stoplist of 419 common English words.",
                "However, an interesting side-effect of our resampling approach is that it tends to remove many stopwords from the feedback model, making a stoplist less critical.",
                "This is discussed further in Section 3.6. 3.2 Baseline feedback method For our baseline expansion method, we use an algorithm included in Indri 1.0 as the default expansion method.",
                "This method first selects terms using a log-odds calculation described by Ponte [14], but assigns final term weights using Lavrenkos relevance model[10].",
                "We chose the Indri method because it gives a consistently strong baseline, is based on a language modeling approach, and is simple to experiment with.",
                "In a TREC evaluation using the GOV2 corpus [6], the method was one of the topperforming runs, achieving a 19.8% gain in MAP compared to using unexpanded queries.",
                "In this study, it achieves an average gain in MAP of 17.25% over the four collections.",
                "Indris expansion method first calculates a log-odds ratio o(v) for each potential expansion term v given by o(v) = X D log p(v|D) p(v|C) (4) over all documents D containing v, in collection C. Then, the expansion term candidates are sorted by descending o(v), and the top m are chosen.",
                "Finally, the term weights r(v) used in the expanded query are calculated based on the relevance model r(v) = X D p(q|D)p(v|D) p(v) p(D) (5) The quantity p(q|D) is the probability score assigned to the document in the initial retrieval.",
                "We use Dirichlet smoothing of p(v|D) with μ = 1000.",
                "This relevance model is then combined with the original query using linear interpolation, weighted by a parameter α.",
                "By default we used the top 50 documents for feedback and the top 20 expansion terms, with the feedback interpolation parameter α = 0.5 unless otherwise stated.",
                "For example, the baseline expanded query for topic 404 is: #weight(0.5 #combine(ireland peace talks) 0.5 #weight(0.10 ireland 0.08 peace 0.08 northern ...) 3.3 Expansion performance We measure our feedback algorithms effectiveness by two main criteria: precision, and robustness.",
                "Robustness, and the tradeoff between precision and robustness, is analyzed in Section 3.4.",
                "In this section, we examine average precision and precision in the top 10 documents (P10).",
                "We also include recall at 1,000 documents.",
                "For each query, we obtained a set of B feedback models using the Indri baseline.",
                "Each feedback model was obtained from a random sample of the top k documents taken with replacement.",
                "For these experiments, B = 30 and k = 50.",
                "Each feedback model contained 20 terms.",
                "On the query side, we used leave-one-out (LOO) sampling to create the query variants.",
                "Single-term query sampling had consistently worse performance across all collections and so our results here focus on LOO sampling.",
                "We used the methods described in Section 2 to estimate an enhanced feedback model from the Dirichlet posterior distribution for each query variant, and to combine the feedback models from all the query variants.",
                "We call our method resampling expansion and denote it as RS-FB here.",
                "We denote the Indri baseline feedback method as Base-FB.",
                "Results from applying both the baseline expansion method (Base-FB) and resampling expansion (RS-FB) are shown in Table 1.",
                "We observe several trends in this table.",
                "First, the average precision of RS-FB was comparable to Base-FB, achieving an average gain of 17.6% compared to using no expansion across the four collections.",
                "The Indri baseline expansion gain was 17.25%.",
                "Also, the RS-FB method achieved consistent improvements in P10 over Base-FB for every topic set, with an average improvement of 6.89% over Base-FB for all 350 topics.",
                "The lowest P10 gain over Base-FB was +3.82% for TREC-7 and the highest was +11.95% for wt10g.",
                "Finally, both Base-FB and RS-FB also consistently improved recall over using no expansion, with Base-FB achieving better recall than RS-FB for all topic sets. 3.4 Retrieval robustness We use the term robustness to mean the worst-case average precision performance of a feedback algorithm.",
                "Ideally, a robust feedback method would never perform worse than using the original query, while often performing better using the expansion.",
                "To evaluate robustness in this study, we use a very simple measure called the robustness index (RI)3 .",
                "For a set of queries Q, the RI measure is defined as: RI(Q) = n+ − n− |Q| (6) where n+ is the number of queries helped by the feedback method and n− is the number of queries hurt.",
                "Here, by helped we mean obtaining a higher average precision as a result of feedback.",
                "The value of RI ranges from a minimum 3 This is sometimes also called the reliability of improvement index and was used in Sakai et al. [17].",
                "Collection NoExp Base-FB RS-FB TREC 1&2 AvgP 0.1818 0.2419 (+33.04%) 0.2406 (+32.24%) P10 0.4443 0.4913 (+10.57%) 0.5363 (+17.83%) Recall 15084/37393 19172/37393 15396/37393 TREC 7 AvgP 0.1890 0.2175 (+15.07%) 0.2169 (+14.75%) P10 0.4200 0.4320 (+2.85%) 0.4480 (+6.67%) Recall 2179/4674 2608/4674 2487/4674 TREC 8 AvgP 0.2031 0.2361 (+16.25%) 0.2268 (+11.70%) P10 0.3960 0.4160 (+5.05%) 0.4340 (+9.59%) Recall 2144/4728 2642/4728 2485/4728 wt10g AvgP 0.1741 0.1829 (+5.06%) 0.1946 (+11.78%) P10 0.2760 0.2630 (-4.71%) 0.2960 (+7.24%) Recall 3361/5980 3725/5980 3664/5980 Table 1: Comparison of baseline (Base-FB) feedback and feedback using re-sampling (RS-FB).",
                "Improvement shown for BaseFB and RS-FB is relative to using no expansion. (a) TREC 1&2 (upper curve); TREC 8 (lower curve) (b) TREC 7 (upper curve); wt10g (lower curve) Figure 3: The trade-off between robustness and average precision for different corpora.",
                "The x-axis gives the change in MAP over using baseline expansion with α = 0.5.",
                "The yaxis gives the Robustness Index (RI).",
                "Each curve through uncircled points shows the RI/MAP tradeoff using the simple small-α strategy (see text) as α decreases from 0.5 to zero in the direction of the arrow.",
                "Circled points represent the tradeoffs obtained by resampling feedback for α = 0.5.",
                "Collection N Base-FB RS-FB n− RI n− RI TREC 1&2 103 26 +0.495 15 +0.709 TREC 7 46 14 +0.391 10 +0.565 TREC 8 44 12 +0.455 12 +0.455 wt10g 91 48 -0.055 39 +0.143 Combined 284 100 +0.296 76 +0.465 Table 2: Comparison of robustness index (RI) for baseline feedback (Base-FB) vs. resampling feedback (RS-FB).",
                "Also shown are the actual number of queries hurt by feedback (n−) for each method and collection.",
                "Queries for which initial average precision was negligible (≤ 0.01) were ignored, giving the remaining query count in column N. of −1.0, when all queries are hurt by the feedback method, to +1.0 when all queries are helped.",
                "The RI measure does not take into account the magnitude or distribution of the amount of change across the set Q.",
                "However, it is easy to understand as a general indication of robustness.",
                "One obvious way to improve the worst-case performance of feedback is simply to use a smaller fixed α interpolation parameter, such as α = 0.3, placing less weight on the (possibly risky) feedback model and more on the original query.",
                "We call this the small-α strategy.",
                "Since we are also reducing the potential gains when the feedback model is right, however, we would expect some trade-off between average precision and robustness.",
                "We therefore compared the precision/robustness trade-off between our resampling feedback algorithm, and the simple small-α method.",
                "The results are summarized in Figure 3.",
                "In the figure, the curve for each topic set interpolates between trade-off points, beginning at x=0, where α = 0.5, and continuing in the direction of the arrow as α decreases and the original query is given more and more weight.",
                "As expected, robustness continuously increases as we move along the curve, but mean average precision generally drops as the gains from feedback are eliminated.",
                "For comparison, the performance of resampling feedback at α = 0.5 is shown for each collection as the circled point.",
                "Higher and to the right is better.",
                "This figure shows that resampling feedback gives a somewhat better trade-off than the small-α approach for 3 of the 4 collections.",
                "Figure 4: Histogram showing improved robustness of resampling feedback (RS-FB) over baseline feedback (Base-FB) for all datasets combined.",
                "Queries are binned by % change in AP compared to the unexpanded query.",
                "Collection DS + QV DS + No QV TREC 1&2 AvgP 0.2406 0.2547 (+5.86%) P10 0.5263 0.5362 (+1.88%) RI 0.7087 0.6515 (-0.0572) TREC 7 AvgP 0.2169 0.2200 (+1.43%) P10 0.4480 0.4300 (-4.02%) RI 0.5652 0.2609 (-0.3043) TREC 8 AvgP 0.2268 0.2257 (-0.49%) P10 0.4340 0.4200 (-3.23%) RI 0.4545 0.4091 (-0.0454) wt10g AvgP 0.1946 0.1865 (-4.16%) P10 0.2960 0.2680 (-9.46%) RI 0.1429 0.0220 (-0.1209) Table 3: Comparison of resampling feedback using document sampling (DS) with (QV) and without (No QV) combining feedback models from multiple query variants.",
                "Table 2 gives the Robustness Index scores for Base-FB and RS-FB.",
                "The RS-FB feedback method obtained higher robustness than Base-FB on three of the four topic sets, with only slightly worse performance on TREC-8.",
                "A more detailed view showing the distribution over relative changes in AP is given by the histogram in Figure 4.",
                "Compared to Base-FB, the RS-FB method achieves a noticable reduction in the number of queries significantly hurt by expansion (i.e. where AP is hurt by 25% or more), while preserving positive gains in AP. 3.5 Effect of query and document sampling methods Given our algorithms improved robustness seen in Section 3.4, an important question is what component of our system is responsible.",
                "Is it the use of document re-sampling, the use of multiple query variants, or some other factor?",
                "The results in Table 3 suggest that the model combination based on query variants may be largely account for the improved robustness.",
                "When query variants are turned off and the original query is used by itself with document sampling, there is little net change in average precision, a small decrease in P10 for 3 out of the 4 topic sets, but a significant drop in robustness for all topic sets.",
                "In two cases, the RI measure drops by more than 50%.",
                "We also examined the effect of the document sampling method on retrieval effectiveness, using two different strategies.",
                "The uniform weighting strategy ignored the relevance scores from the initial retrieval and gave each document in the top k the same probability of selection.",
                "In contrast, the relevance-score weighting strategy chose documents with probability proportional to their relevance scores.",
                "In this way, documents that were more highly ranked were more likely to be selected.",
                "Results are shown in Table 4.",
                "The relevance-score weighting strategy performs better overall, with significantly higher RI and P10 scores on 3 of the 4 topic sets.",
                "The difference in average precision between the methods, however, is less marked.",
                "This suggests that uniform weighting acts to increase variance in retrieval results: when initial average precision is high, there are many relevant documents in the top k and uniform sampling may give a more representative relevance model than focusing on the highly-ranked items.",
                "On the other hand, when initial precision is low, there are few relevant documents in the bottom ranks and uniform sampling mixes in more of the non-relevant documents.",
                "For space reasons we only summarize our findings on sample size here.",
                "The number of samples has some effect on precision when less than 10, but performance stabilizes at around 15 to 20 samples.",
                "We used 30 samples for our experiments.",
                "Much beyond this level, the additional benefits of more samples decrease as the initial score distribution is more closely fit and the processing time increases. 3.6 The effect of resampling on expansion term quality Ideally, a retrieval model should not require a stopword list when estimating a model of relevance: a robust statistical model should down-weight stopwords automatically depending on context.",
                "Stopwords can harm feedback if selected as feedback terms, because they are typically poor discriminators and waste valuable term slots.",
                "In practice, however, because most term selection methods resemble a tf · idf type of weighting, terms with low idf but very high tf can sometimes be selected as expansion term candidates.",
                "This happens, for example, even with the Relevance Model approach that is part of our baseline feedback.",
                "To ensure as strong a baseline as possible, we use a stoplist for all experiments reported here.",
                "If we turn off the stopword list, however, we obtain results such as those shown in Table 5 where four of the top ten baseline feedback terms for TREC topic 60 (said, but, their, not) are stopwords using the BaseFB method. (The top 100 expansion terms were selected to generate this example.)",
                "Indris method attempts to address the stopword problem by applying an initial step based on Ponte [14] to select less-common terms that have high log-odds of being in the top-ranked documents compared to the whole collection.",
                "Nevertheless, this does not overcome the stopword problem completely, especially as the number of feedback terms grows.",
                "Using resampling feedback, however, appears to mitigate Collection QV + Uniform QV + Relevance-score weighting weighting TREC 1&2 AvgP 0.2545 0.2406 (-5.46%) P10 0.5369 0.5263 (-1.97%) RI 0.6212 0.7087 (+14.09%) TREC 7 AvgP 0.2174 0.2169 (-0.23%) P10 0.4320 0.4480 (+3.70%) RI 0.4783 0.5652 (+18.17%) TREC 8 AvgP 0.2267 0.2268 (+0.04%) P10 0.4120 0.4340 (+5.34%) RI 0.4545 0.4545 (+0.00%) wt10g AvgP 0.1808 0.1946 (+7.63%) P10 0.2680 0.2960 (+10.45%) RI 0.0220 0.1099 (+399.5%) Table 4: Comparison of uniform and relevance-weighted document sampling.",
                "The percentage change compared to uniform sampling is shown in parentheses.",
                "QV indicates that query variants were used in both runs.",
                "Baseline FB p(wi|R) Resampling FB p(wi|R) said 0.055 court 0.026 court 0.055 pay 0.018 pay 0.034 federal 0.012 but 0.026 education 0.011 employees 0.024 teachers 0.010 their 0.024 employees 0.010 not 0.023 case 0.010 federal 0.021 their 0.009 workers 0.020 appeals 0.008 education 0.020 union 0.007 Table 5: Feedback term quality when a stoplist is not used.",
                "Feedback terms for TREC topic 60: merit pay vs seniority. the effect of stopwords automatically.",
                "In the example of Table 5, resampling feedback leaves only one stopword (their) in the top ten.",
                "We observed similar feedback term behavior across many other topics.",
                "The reason for this effect appears to be the interaction of the term selection score with the top-m term cutoff.",
                "While the presence and even proportion of particular stopwords is fairly stable across different document samples, their relative position in the top-m list is not, as sets of documents with varying numbers of better, lower-frequency term candidates are examined for each sample.",
                "As a result, while some number of stopwords may appear in each sampled document set, any given stopword tends to fall below the cutoff for multiple samples, leading to its classification as a high-variance, low-weight feature. 4.",
                "RELATED WORK Our approach is related to previous work from several areas of information retrieval and machine learning.",
                "Our use of query variation was inspired by the work of YomTov et al. [20], Carpineto et al. [5], and Amati et al. [2], among others.",
                "These studies use the idea of creating multiple subqueries and then examining the nature of the overlap in the documents and/or expansion terms that result from each subquery.",
                "Model combination is performed using heuristics.",
                "In particular, the studies of Amati et al. and Carpineto et al. investigated combining terms from individual distributional methods using a term-reranking combination heuristic.",
                "In a set of TREC topics they found wide average variation in the rank-distance of terms from different expansion methods.",
                "Their combination method gave modest positive improvements in average precision.",
                "The idea of examining the overlap between lists of suggested terms has also been used in early <br>query expansion</br> approaches.",
                "Xu and Crofts method of Local Context Analysis (LCA) [19] includes a factor in the empirically-derived weighting formula that causes expansion terms to be preferred that have connections to multiple query terms.",
                "On the document side, recent work by Zhou & Croft [21] explored the idea of adding noise to documents, re-scoring them, and using the stability of the resulting rankings as an estimate of query difficulty.",
                "This is related to our use of document sampling to estimate the risk of the feedback model built from the different sets of top-retrieved documents.",
                "Sakai et al. [17] proposed an approach to improving the robustness of pseudo-relevance feedback using a method they call selective sampling.",
                "The essence of their method is that they allow skipping of some top-ranked documents, based on a clustering criterion, in order to select a more varied and novel set of documents later in the ranking for use by a traditional pseudo-feedback method.",
                "Their study did not find significant improvements in either robustness (RI) or MAP on their corpora.",
                "Greiff, Morgan and Ponte [8] explored the role of variance in term weighting.",
                "In a series of simulations that simplified the problem to 2-feature documents, they found that average precision degrades as term frequency variance - high noiseincreases.",
                "Downweighting terms with high variance resulted in improved average precision.",
                "This seems in accord with our own findings for individual feedback models.",
                "Estimates of output variance have recently been used for improved text classification.",
                "Lee et al. [11] used queryspecific variance estimates of classifier outputs to perform improved model combination.",
                "Instead of using sampling, they were able to derive closed-form expressions for classifier variance by assuming base classifiers using simple types of inference networks.",
                "Ando and Zhang proposed a method that they call structural feedback [3] and showed how to apply it to <br>query expansion</br> for the TREC Genomics Track.",
                "They used r query variations to obtain R different sets Sr of top-ranked documents that have been intersected with the top-ranked documents obtained from the original query qorig.",
                "For each Si, the normalized centroid vector ˆwi of the documents is calculated.",
                "Principal component analysis (PCA) is then applied to the ˆwi to obtain the matrix Φ of H left singular vectors φh that are used to obtain the new, expanded query qexp = qorig + ΦT Φqorig. (7) In the case H = 1, we have a single left singular vector φ: qexp = qorig + (φT qorig)φ so that the dot product φT qorig is a type of dynamic weight on the expanded query that is based on the similarity of the original query to the expanded query.",
                "The use of variance as a feedback model quality measure occurs indirectly through the application of PCA.",
                "It would be interesting to study the connections between this approach and our own modelfitting method.",
                "Finally, in language modeling approaches to feedback, Tao and Zhai [18] describe a method for more robust feedback that allows each document to have a different feedback α.",
                "The feedback weights are derived automatically using regularized EM.",
                "A roughly equal balance of query and expansion model is implied by their EM stopping condition.",
                "They propose tailoring the stopping parameter η based on a function of some quality measure of feedback documents. 5.",
                "CONCLUSIONS We have presented a new approach to pseudo-relevance feedback based on document and query sampling.",
                "The use of sampling is a very flexible and powerful device and is motivated by our general desire to extend current models of retrieval by estimating the risk or variance associated with the parameters or output of retrieval processes.",
                "Such variance estimates, for example, may be naturally used in a Bayesian framework for improved model estimation and combination.",
                "Applications such as selective expansion may then be implemented in a principled way.",
                "While our study uses the language modeling approach as a framework for experiments, we make few assumptions about the actual workings of the feedback algorithm.",
                "We believe it is likely that any reasonably effective baseline feedback algorithm would benefit from our approach.",
                "Our results on standard TREC collections show that our framework improves the robustness of a strong baseline feedback method across a variety of collections, without sacrificing average precision.",
                "It also gives small but consistent gains in top10 precision.",
                "In future work, we envision an investigation into how varying the set of sampling methods used and the number of samples controls the trade-off between robustness, accuracy, and efficiency.",
                "Acknowledgements We thank Paul Bennett for valuable discussions related to this work, which was supported by NSF grants #IIS-0534345 and #CNS-0454018, and U.S. Dept. of Education grant #R305G03123.",
                "Any opinions, findings, and conclusions or recommendations expressed in this material are the authors. and do not necessarily reflect those of the sponsors. 6.",
                "REFERENCES [1] The Lemur toolkit for language modeling and retrieval. http://www.lemurproject.org. [2] G. Amati, C. Carpineto, and G. Romano.",
                "Query difficulty, robustness, and selective application of <br>query expansion</br>.",
                "In Proc. of the 25th European Conf. on Information Retrieval (ECIR 2004), pages 127-137. [3] R. K. Ando and T. Zhang.",
                "A high-performance semi-supervised learning method for text chunking.",
                "In Proc. of the 43rd Annual Meeting of the ACL, pages 1-9, June 2005. [4] L. Breiman.",
                "Bagging predictors.",
                "Machine Learning, 24(2):123-140, 1996. [5] C. Carpineto, G. Romano, and V. Giannini.",
                "Improving retrieval feedback with multiple term-ranking function combination.",
                "ACM Trans.",
                "Info.",
                "Systems, 20(3):259 - 290. [6] K. Collins-Thompson, P. Ogilvie, and J. Callan.",
                "Initial results with structured queries and language models on half a terabyte of text.",
                "In Proc. of 2005 Text REtrieval Conference.",
                "NIST Special Publication. [7] R. O. Duda, P. E. Hart, and D. G. Stork.",
                "Pattern Classification.",
                "Wiley and Sons, 2nd edition, 2001. [8] W. R. Greiff, W. T. Morgan, and J. M. Ponte.",
                "The role of variance in term weighting for probabilistic information retrieval.",
                "In Proc. of the 11th Intl.",
                "Conf. on Info. and Knowledge Mgmt. (CIKM 2002), pages 252-259. [9] T. Kohonen, J. Hynninen, J. Kangas, and J. Laaksonen.",
                "SOMPAK: The self-organizing map program package.",
                "Technical Report A31, Helsinki University of Technology, 1996. http://www.cis.hut.fi/research/papers/som tr96.ps.Z. [10] V. Lavrenko.",
                "A Generative Theory of Relevance.",
                "PhD thesis, University of Massachusetts, Amherst, 2004. [11] C.-H. Lee, R. Greiner, and S. Wang.",
                "Using query-specific variance estimates to combine Bayesian classifiers.",
                "In Proc. of the 23rd Intl.",
                "Conf. on Machine Learning (ICML 2006), pages 529-536. [12] D. Metzler and W. B. Croft.",
                "Combining the language model and inference network approaches to retrieval.",
                "Info.",
                "Processing and Mgmt., 40(5):735-750, 2004. [13] T. Minka.",
                "Estimating a Dirichlet distribution.",
                "Technical report, 2000. http://research.microsoft.com/ minka/papers/dirichlet. [14] J. Ponte.",
                "Advances in Information Retrieval, chapter Language models for relevance feedback, pages 73-96. 2000.",
                "W.B.",
                "Croft, ed. [15] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In Proc. of the 1998 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 275-281. [16] J. Rocchio.",
                "The SMART Retrieval System, chapter Relevance Feedback in Information Retrieval, pages 313-323.",
                "Prentice-Hall, 1971.",
                "G. Salton, ed. [17] T. Sakai, T. Manabe, and M. Koyama.",
                "Flexible pseudo-relevance feedback via selective sampling.",
                "ACM Transactions on Asian Language Information Processing (TALIP), 4(2):111-135, 2005. [18] T. Tao and C. Zhai.",
                "Regularized estimation of mixture models for robust pseudo-relevance feedback.",
                "In Proc. of the 2006 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 162-169. [19] J. Xu and W. B. Croft.",
                "Improving the effectiveness of information retrieval with local context analysis.",
                "ACM Trans.",
                "Inf.",
                "Syst., 18(1):79-112, 2000. [20] E. YomTov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty.",
                "In Proc. of the 2005 ACM SIGIR Conf. on Research and Development in Information Retrieval, pages 512-519. [21] Y. Zhou and W. B. Croft.",
                "Ranking robustness: a novel framework to predict query performance.",
                "In Proc. of the 15th ACM Intl.",
                "Conf. on Information and Knowledge Mgmt. (CIKM 2006), pages 567-574."
            ],
            "original_annotated_samples": [
                "With this in mind, we wish to treat many important quantities calculated by the retrieval system, whether a relevance score for a document, or a weight for a <br>query expansion</br> term, as random variables whose true value is uncertain but where the uncertainty about the true value may be quantified by replacing the fixed value with a probability distribution over possible values.",
                "The idea of examining the overlap between lists of suggested terms has also been used in early <br>query expansion</br> approaches.",
                "Ando and Zhang proposed a method that they call structural feedback [3] and showed how to apply it to <br>query expansion</br> for the TREC Genomics Track.",
                "Query difficulty, robustness, and selective application of <br>query expansion</br>."
            ],
            "translated_annotated_samples": [
                "Con esto en mente, deseamos tratar muchas cantidades importantes calculadas por el sistema de recuperación, ya sea un puntaje de relevancia para un documento, o un peso para un <br>término de expansión de consulta</br>, como variables aleatorias cuyo valor real es incierto pero donde la incertidumbre sobre el valor real puede ser cuantificada reemplazando el valor fijo con una distribución de probabilidad sobre posibles valores.",
                "La idea de examinar la superposición entre listas de términos sugeridos también ha sido utilizada en enfoques tempranos de <br>expansión de consultas</br>.",
                "Ando y Zhang propusieron un método que ellos llaman retroalimentación estructural [3] y mostraron cómo aplicarlo a la <br>expansión de consultas</br> para la pista de Genómica de TREC.",
                "Dificultad de la consulta, robustez y aplicación selectiva de la <br>expansión de consultas</br>."
            ],
            "translated_text": "Estimación y Uso de la Incertidumbre en la Retroalimentación de Pseudo-Relevancia Kevyn Collins-Thompson y Jamie Callan Instituto de Tecnologías del Lenguaje Escuela de Ciencias de la Computación Universidad Carnegie Mellon Pittsburgh, PA 15213-8213 EE. UU. {kct | callan}@cs.cmu.edu RESUMEN Los métodos existentes de retroalimentación de pseudo-relevancia suelen realizar un promedio de los documentos mejor clasificados, pero ignoran una dimensión estadística importante: el riesgo o la varianza asociada tanto a los modelos de documentos individuales como a su combinación. Tratando el método de retroalimentación de línea base como una caja negra, y el modelo de retroalimentación de salida como una variable aleatoria, estimamos una distribución posterior para el modelo de retroalimentación mediante el remuestreo de los documentos mejor recuperados de una consulta dada, utilizando la media o la moda posterior como el modelo de retroalimentación mejorado. Luego realizamos la combinación de modelos sobre varios modelos mejorados, cada uno basado en una consulta ligeramente modificada muestreada de la consulta original. Observamos que el remuestreo de documentos ayuda a aumentar la precisión del modelo de retroalimentación individual al eliminar términos de ruido, mientras que el muestreo de la consulta mejora la robustez (rendimiento en el peor de los casos) al enfatizar términos relacionados con múltiples aspectos de la consulta. El resultado es un algoritmo de meta-retroalimentación que es tanto más robusto como más preciso que el método de referencia original. Categorías y Descriptores de Asignaturas: H.3.3 [Recuperación de Información]: Modelos de Recuperación Términos Generales: Algoritmos, Experimentación 1. INTRODUCCIÓN La incertidumbre es una característica inherente de la recuperación de información. No solo no sabemos las consultas que se presentarán a nuestro algoritmo de recuperación con anticipación, sino que la necesidad de información de los usuarios puede ser vaga o estar incompletamente especificada por estas consultas. Incluso si la consulta estuviera perfectamente especificada, el lenguaje en los documentos de la colección es inherentemente complejo y ambiguo, y hacer coincidir dicho lenguaje de manera efectiva es un problema formidable por sí mismo. Con esto en mente, deseamos tratar muchas cantidades importantes calculadas por el sistema de recuperación, ya sea un puntaje de relevancia para un documento, o un peso para un <br>término de expansión de consulta</br>, como variables aleatorias cuyo valor real es incierto pero donde la incertidumbre sobre el valor real puede ser cuantificada reemplazando el valor fijo con una distribución de probabilidad sobre posibles valores. De esta manera, los algoritmos de recuperación pueden intentar cuantificar el riesgo o la incertidumbre asociados con sus clasificaciones de salida, o mejorar la estabilidad o precisión de sus cálculos internos. Los algoritmos actuales para la retroalimentación de pseudo relevancia (PRF) tienden a seguir el mismo método básico ya sea que utilicemos algoritmos basados en espacio vectorial como la fórmula de Rocchio [16], o enfoques más recientes de modelado de lenguaje como Modelos de Relevancia [10]. Primero, se obtiene un conjunto de documentos recuperados en la parte superior a partir de una consulta inicial y se asume que aproxima un conjunto de documentos relevantes. A continuación, se calcula un vector de modelo de retroalimentación único según algún tipo de promedio, centroide o expectativa sobre el conjunto de modelos de documentos posiblemente relevantes. Por ejemplo, los vectores de documentos pueden combinarse con igual ponderación, como en Rocchio, o por probabilidad de consulta, como se puede hacer utilizando el Modelo de Relevancia. El uso de una expectativa es razonable por razones prácticas y teóricas, pero por sí sola ignora información potencialmente valiosa sobre el riesgo del modelo de retroalimentación. Nuestra hipótesis principal en este artículo es que estimar la incertidumbre en la retroalimentación es útil y conduce a modelos de retroalimentación individual más efectivos y a modelos combinados más robustos. Por lo tanto, proponemos un método para estimar la incertidumbre asociada con un modelo de retroalimentación individual en términos de una distribución posterior sobre modelos de lenguaje. Para hacer esto, variamos sistemáticamente las entradas al método de retroalimentación base y ajustamos una distribución de Dirichlet a la salida. Utilizamos la media posterior o el modo como la estimación mejorada del modelo de retroalimentación. Este proceso se muestra en la Figura 1. Como mostraremos más adelante, la media y la moda pueden variar significativamente del modelo de retroalimentación única propuesto por el método de referencia. También realizamos la combinación de modelos utilizando varios modelos de lenguaje de retroalimentación mejorados obtenidos a partir de un pequeño número de nuevas consultas muestreadas de la consulta original. El peso de un modelo combina dos factores complementarios: la probabilidad del modelo de generar la consulta y la varianza del modelo, siendo los modelos con alta varianza los que obtienen un peso menor. Por ejemplo, un vector de parámetros esperado condicionado a la observación de la consulta se forma a partir de los documentos recuperados en la parte superior, que se tratan como cadenas de entrenamiento (ver [10], p. 62). Figura 1: Estimación de la incertidumbre del modelo de retroalimentación para una sola consulta. 2. En las Secciones 2.1-2.5 describimos un método general para estimar una distribución de probabilidad sobre el conjunto de posibles modelos de lenguaje basado en muestreo. En las Secciones 2.6 y 2.7 resumimos cómo se utilizan diferentes muestras de consultas para generar múltiples modelos de retroalimentación, que luego se combinan. 2.1 Modelado de la Incertidumbre de la Retroalimentación Dado una consulta Q y una colección C, asumimos un sistema de recuperación probabilístico que asigna un puntaje de documento de valor real f(D, Q) a cada documento D en C, de manera que el puntaje sea proporcional a la probabilidad estimada de relevancia. No hacemos ninguna otra suposición sobre f(D, Q). La naturaleza de f(D, Q) puede ser compleja: por ejemplo, si el sistema de recuperación admite lenguajes de consulta estructurados [12], entonces f(D, Q) puede representar la salida de una red de inferencia arbitrariamente compleja definida por los operadores de consulta estructurados. En teoría, la función de puntuación puede variar de una consulta a otra, aunque en este estudio por simplicidad mantenemos la misma función de puntuación para todas las consultas. Nuestro método de consulta específico se encuentra en la Sección 3. Tratamos el algoritmo de retroalimentación como una caja negra y asumimos que los insumos del algoritmo de retroalimentación son la consulta original y los documentos recuperados principales correspondientes, a los cuales se les asigna una puntuación a cada documento. Suponemos que la salida del algoritmo de retroalimentación es un vector de pesos de términos que se utilizarán para agregar o reponderar los términos en la representación de la consulta original, con el vector normalizado para formar una distribución de probabilidad. Consideramos las entradas a la caja negra de retroalimentación como variables aleatorias, y analizamos el modelo de retroalimentación como una variable aleatoria que cambia en respuesta a cambios en las entradas. Al igual que la función de puntuación del documento f(D, Q), el algoritmo de retroalimentación puede implementar una fórmula de puntuación compleja y no lineal, por lo que, a medida que sus entradas varían, los modelos de retroalimentación resultantes pueden tener una distribución compleja en el espacio de modelos de retroalimentación (el espacio muestral). Debido a esta complejidad potencial, no intentamos derivar una distribución posterior de forma cerrada, sino que en su lugar utilizamos simulación. Llamamos a esta distribución sobre posibles modelos de retroalimentación la distribución de modelos de retroalimentación. Nuestro objetivo en esta sección es estimar una aproximación útil a la distribución del modelo de retroalimentación. Para un marco específico de experimentos, utilizamos el enfoque de modelado de lenguaje (LM) para la recuperación de información [15]. El puntaje de un documento D con respecto a una consulta Q y una colección C se da por p(Q|D) con respecto a los modelos de lenguaje ˆθQ y ˆθD estimados para la consulta y el documento respectivamente. Denotamos el conjunto de los k documentos mejor recuperados de la colección C en respuesta a Q como DQ(k, C). Para simplificar, asumimos que las consultas y documentos son generados por distribuciones multinomiales cuyos parámetros están representados por modelos de lenguaje unigram. Para incorporar retroalimentación en el enfoque de LM, asumimos un esquema basado en modelos en el que nuestro objetivo es tomar la consulta y los documentos clasificados resultantes DQ(k, C) como entrada, y producir un modelo de lenguaje expandido ˆθE, que luego se interpola con el modelo de consulta original ˆθQ: ˆθNew = (1 − α) · ˆθQ + α · ˆθE (1). Esto incluye la posibilidad de α = 1 donde el modelo de consulta original es completamente reemplazado por el modelo de retroalimentación. Nuestro espacio muestral es el conjunto de todos los posibles modelos de lenguaje LF que pueden ser generados como modelos de retroalimentación. Nuestro enfoque es tomar muestras de este espacio y luego ajustar una distribución a las muestras utilizando máxima verosimilitud. Para simplificar, comenzamos asumiendo que la distribución de retroalimentación latente tiene la forma de una distribución de Dirichlet. Aunque la distribución de Dirichlet es unimodal y en general bastante limitada en su expresividad en el espacio muestral, es una coincidencia natural para el modelo de lenguaje multinomial, puede ser estimada rápidamente y capturar las características más relevantes de los modelos de retroalimentación confiada e incierta, como la dispersión general de la distribución. 2.2 Re-muestreo de modelos de documentos. Nos gustaría una aproximación a la distribución posterior del modelo de retroalimentación LF. Para lograr esto, aplicamos una técnica de simulación ampliamente utilizada llamada muestreo bootstrap ([7], p. 474) en los parámetros de entrada, es decir, el conjunto de documentos recuperados en la parte superior. El muestreo de arranque nos permite simular el efecto aproximado de perturbar los parámetros dentro del algoritmo de retroalimentación de la caja negra al perturbar los insumos de ese algoritmo de manera sistemática, sin hacer suposiciones sobre la naturaleza del algoritmo de retroalimentación. Específicamente, muestreamos k documentos con reemplazo de DQ(k, C), y calculamos un modelo de lenguaje de expansión θb utilizando el método de retroalimentación de caja negra. Repetimos este proceso B veces para obtener un conjunto de B modelos de lenguaje de retroalimentación, a los cuales luego ajustamos una distribución de Dirichlet. Normalmente, B se encuentra en el rango de 20 a 50 muestras, con un rendimiento relativamente estable en este rango. Ten en cuenta que en lugar de tratar cada documento superior como igualmente probable, muestreamos de acuerdo con las probabilidades estimadas de relevancia de cada documento en DQ(k, C). Por lo tanto, es más probable que un documento sea elegido cuanto más alto esté en la clasificación. 2.3 Justificación de un enfoque de muestreo La justificación de nuestro enfoque de muestreo tiene dos partes. Primero, queremos mejorar la calidad de los modelos de retroalimentación individual al suavizar la variación cuando el modelo de retroalimentación base es inestable. En este sentido, nuestro enfoque se asemeja al bagging [4], un enfoque de conjunto que genera múltiples versiones de un predictor haciendo copias de arranque del conjunto de entrenamiento, y luego promedia los predictores (numéricos). En nuestra aplicación, los documentos recuperados en la parte superior pueden ser vistos como una especie de conjunto de entrenamiento ruidoso para la relevancia. Segundo, el muestreo es una forma efectiva de estimar propiedades básicas de la distribución posterior de retroalimentación, las cuales pueden ser utilizadas para mejorar la combinación de modelos. Por ejemplo, un modelo puede ser ponderado por su confianza de predicción, estimada como una función de la variabilidad del posterior alrededor del modelo. foo2-401.map-Dim:5434,Size:12*12units, vecindario gaussiano (a) Tema 401 Minorías extranjeras, Alemania foo2-402.map-Dim:5698,Size:12*12units, vecindario gaussiano (b) Tema 402 Genética del comportamiento foo2-459.map-Dim:8969,Size:12*12units, vecindario gaussiano (c) Tema 459 ¿Cuándo puede un prestamista ejecutar una hipoteca sobre una propiedad? Figura 2: Visualización de la varianza del modelo de lenguaje de expansión utilizando mapas autoorganizados, mostrando la distribución de modelos de lenguaje que resulta de remuestrear las entradas al método de expansión de referencia. El modelo de lenguaje que habría sido elegido por la expansión de la línea base se encuentra en el centro de cada mapa. La función de similitud es la divergencia JensenShannon. 2.4 Visualización de distribuciones de retroalimentación Antes de describir cómo ajustamos y utilizamos la distribución de Dirichlet sobre modelos de retroalimentación, es instructivo ver algunos ejemplos de distribuciones de modelos de retroalimentación reales que resultan de muestrear por bootstrap los documentos más recuperados de diferentes temas de TREC. Cada punto en nuestro espacio muestral es un modelo de lenguaje, que típicamente tiene varias miles de dimensiones. Para ayudar a analizar el comportamiento de nuestro método, utilizamos un Mapa Auto-organizado (a través del paquete SOM-PAK [9]), para aplanar y visualizar la función de densidad de alta dimensión. Los mapas de densidad para tres temas de TREC se muestran en la Figura 2 arriba. Las áreas oscuras representan regiones de alta similitud entre los modelos de lenguaje. Las áreas claras representan regiones de baja similitud, los valles entre los grupos. Cada diagrama está centrado en el modelo de lenguaje que habría sido elegido por la expansión de referencia. Un solo pico (modo) es evidente en algunos ejemplos, pero en otros aparece una estructura más compleja. Además, aunque la distribución suele estar cerca del modelo de retroalimentación de referencia, para algunos temas están a una distancia significativa (según la divergencia de JensenShannon), como se muestra en la Subfigura 2c. En tales casos, la moda o la media de la distribución de retroalimentación a menudo funcionan significativamente mejor que el valor base (y en una proporción menor de casos, significativamente peor). 2.5 Ajuste de una distribución de retroalimentación posterior Después de obtener muestras del modelo de retroalimentación mediante el remuestreo de las entradas del modelo de retroalimentación, estimamos la distribución de retroalimentación. Suponemos que los modelos de retroalimentación multinomial {ˆθ1, . . . , ˆθB} fueron generados por una distribución de Dirichlet latente con parámetros {α1, . . . , αN}. Para estimar los {α1, . . . , αN }, ajustamos los parámetros de Dirichlet a las muestras del modelo de lenguaje B según la máxima verosimilitud utilizando un procedimiento de Newton generalizado, cuyos detalles se encuentran en Minka [13]. Suponemos una distribución de Dirichlet simple sobre los {α1, . . . , αN}, estableciendo cada uno como αi = μ · p(wi | C), donde μ es un parámetro y p(· | C) es el modelo de lenguaje de colección estimado a partir de un conjunto de documentos de la colección C. El ajuste de parámetros converge muy rápidamente, generalmente en solo 2 o 3 iteraciones. Dado que nuestros puntos son modelos de lenguaje en el simplex multinomial, extendimos SOM-PAK para admitir la divergencia de Jensen-Shannon, una medida de similitud ampliamente utilizada entre distribuciones de probabilidad. 3 iteraciones son suficientes, por lo que es práctico aplicarlo en tiempo de consulta cuando la sobrecarga computacional debe ser pequeña. En la práctica, podemos restringir el cálculo al vocabulario de los documentos recuperados en la parte superior, en lugar de toda la colección. Ten en cuenta que para este paso estamos reutilizando los documentos recuperados existentes y no realizando consultas adicionales. Dado los parámetros de una distribución de Dirichlet N-dimensional Dir(α), los vectores de media μ y moda x son fáciles de calcular y se dan respectivamente por μi = αiP αi (2) y xi = αi−1P αi−N . (3) Luego podemos elegir el modelo de lenguaje en la media o en la moda del posterior como el modelo de retroalimentación mejorado final. (Encontramos que la moda proporciona un rendimiento ligeramente mejor). Para la recuperación de información, es probable que el número de muestras disponibles sea bastante pequeño por razones de rendimiento, generalmente menos de diez. Además, si bien el muestreo aleatorio es útil en ciertos casos, es perfectamente aceptable permitir distribuciones de muestreo determinísticas, pero estas deben diseñarse cuidadosamente para aproximar una varianza de salida precisa. Dejamos esto para estudios futuros. 2.6 Variantes de consulta Utilizamos los siguientes métodos para generar variantes de la consulta original. Cada variante corresponde a una suposición diferente sobre qué aspectos de la consulta original pueden ser importantes. Esta es una forma de muestreo determinístico. Seleccionamos tres métodos simples que cubren suposiciones complementarias sobre la consulta. Uso sin expansión. La suposición es que los términos dados son una descripción completa de la necesidad de información. Deja uno fuera. Se deja fuera un término del query original. La suposición es que uno de los términos de la consulta es un término de ruido. Se elige un término único de la consulta original. Esto asume que solo un aspecto de la consulta, es decir, el representado por el término, es el más importante. Después de generar una variante de la consulta original, la combinamos con la consulta original utilizando un peso αSUB para no alejarnos demasiado. En este estudio, establecimos αSUB = 0.5. Por ejemplo, utilizando el lenguaje de consulta Indri [12], una variante de dejar uno fuera de la consulta inicial que omite el término \"ireland\" para el tema TREC 404 es: #weight(0.5 #combine(ireland peace talks) 0.5 #combine(peace talks)) 2.7 Combinando modelos de retroalimentación mejorados de múltiples variantes de consulta. Cuando se utilizan múltiples variantes de consulta, los modelos de retroalimentación mejorados resultantes se combinan utilizando la combinación de modelos bayesianos. Para hacer esto, tratamos cada palabra como un elemento a clasificar como perteneciente a una clase relevante o no relevante, y derivamos una probabilidad de clase para cada palabra combinando las puntuaciones de cada variante de consulta. Cada puntuación es dada por la probabilidad de ese término en la distribución de Dirichlet. Las puntuaciones de los términos se ponderan por el inverso de la varianza del término en los modelos de retroalimentación mejorada de la distribución de Dirichlet. La probabilidad previa de pertenencia de una palabra a la clase relevante se da por la probabilidad de la consulta original en todo el modelo de expansión mejorado. 3. EVALUACIÓN En esta sección presentamos resultados que confirman la utilidad de estimar una distribución del modelo de retroalimentación a partir del remuestreo ponderado de documentos mejor clasificados, y de combinar los modelos de retroalimentación obtenidos a partir de diferentes pequeños cambios en la consulta original. 3.1 Método general Evaluamos el rendimiento en un total de 350 consultas derivadas de cuatro conjuntos de temas de TREC: 51-200 (TREC-1&2), 351-400 (TREC-7), 401-450 (TREC-8) y 451-550 (wt10g, TREC-9&10). Elegimos estos por su contenido variado y propiedades del documento. Por ejemplo, los documentos wt10g son páginas web con una amplia variedad de temas y estilos, mientras que los documentos TREC-1&2 son artículos de noticias más homogéneos. La indexación y recuperación se realizó utilizando el sistema Indri en el kit de herramientas Lemur [12] [1]. Nuestras consultas se derivaron de las palabras en el campo del título de los temas de TREC. Las frases no fueron utilizadas. Para generar las consultas base que se pasaron a Indri, envolvimos los términos de la consulta con el operador #combine de Indri. Por ejemplo, la consulta inicial para el tema 404 es: #combine(ireland peace talks) Realizamos el stemming de Krovetz para todos los experimentos. Debido a que encontramos que el método de expansión de la base (Indri) funcionó mejor utilizando una lista de palabras vacías con el modelo de retroalimentación, todos los experimentos utilizaron una lista de 419 palabras comunes en inglés. Sin embargo, un efecto secundario interesante de nuestro enfoque de remuestreo es que tiende a eliminar muchas palabras vacías del modelo de retroalimentación, lo que hace que una lista de paradas sea menos crítica. Esto se discute más a fondo en la Sección 3.6. Método de retroalimentación de línea base 3.2 Para nuestro método de expansión de línea base, utilizamos un algoritmo incluido en Indri 1.0 como el método de expansión predeterminado. Este método primero selecciona términos utilizando un cálculo de logaritmo de probabilidades descrito por Ponte [14], pero asigna pesos finales a los términos utilizando el modelo de relevancia de Lavrenko [10]. Elegimos el método Indri porque proporciona una línea base consistentemente sólida, se basa en un enfoque de modelado del lenguaje y es fácil de experimentar con él. En una evaluación de TREC utilizando el corpus GOV2, el método fue uno de los mejores resultados, logrando un aumento del 19.8% en el MAP en comparación con el uso de consultas no expandidas. En este estudio, se logra una ganancia promedio en MAP del 17.25% en las cuatro colecciones. El método de expansión de Indri primero calcula una razón de logaritmos de probabilidades o(v) para cada término de expansión potencial v dado por o(v) = X D log p(v|D) p(v|C) (4) sobre todos los documentos D que contienen v, en la colección C. Luego, los candidatos a términos de expansión se ordenan por o(v) descendente, y se eligen los primeros m. Finalmente, los pesos de término r(v) utilizados en la consulta ampliada se calculan en base al modelo de relevancia r(v) = X D p(q|D)p(v|D) p(v) p(D) (5). La cantidad p(q|D) es la puntuación de probabilidad asignada al documento en la recuperación inicial. Utilizamos suavizado de Dirichlet de p(v|D) con μ = 1000. Este modelo de relevancia se combina luego con la consulta original utilizando interpolación lineal, ponderada por un parámetro α. Por defecto, utilizamos los 50 documentos principales para la retroalimentación y los 20 términos de expansión, con el parámetro de interpolación de retroalimentación α = 0.5 a menos que se indique lo contrario. Por ejemplo, la consulta expandida base para el tema 404 es: #weight(0.5 #combine(ireland peace talks) 0.5 #weight(0.10 ireland 0.08 peace 0.08 northern ...) 3.3 Rendimiento de la expansión. Medimos la efectividad de nuestros algoritmos de retroalimentación mediante dos criterios principales: precisión y robustez. La robustez, y el equilibrio entre precisión y robustez, se analiza en la Sección 3.4. En esta sección, examinamos la precisión promedio y la precisión en los 10 documentos principales (P10). También incluimos la recuperación de 1,000 documentos. Para cada consulta, obtuvimos un conjunto de B modelos de retroalimentación utilizando la línea base de Indri. Cada modelo de retroalimentación se obtuvo a partir de una muestra aleatoria de los k documentos principales tomados con reemplazo. Para estos experimentos, B = 30 y k = 50. Cada modelo de retroalimentación contenía 20 términos. En el lado de la consulta, utilizamos muestreo de dejar uno fuera (LOO) para crear las variantes de consulta. El muestreo de consulta de un solo término tuvo un rendimiento consistentemente peor en todas las colecciones, por lo que nuestros resultados se centran en el muestreo de LOO aquí. Utilizamos los métodos descritos en la Sección 2 para estimar un modelo de retroalimentación mejorado a partir de la distribución posterior de Dirichlet para cada variante de consulta, y para combinar los modelos de retroalimentación de todas las variantes de consulta. Llamamos a nuestro método expansión de remuestreo y lo denotamos como RS-FB aquí. Denominamos al método de retroalimentación de línea base Indri como Base-FB. Los resultados de aplicar tanto el método de expansión de línea base (Base-FB) como la expansión de remuestreo (RS-FB) se muestran en la Tabla 1. Observamos varias tendencias en esta tabla. Primero, la precisión promedio de RS-FB fue comparable a Base-FB, logrando una ganancia promedio del 17.6% en comparación con no utilizar expansión en las cuatro colecciones. La ganancia de expansión de la línea base de Indri fue del 17.25%. Además, el método RS-FB logró mejoras consistentes en P10 sobre Base-FB para cada conjunto de temas, con una mejora promedio del 6.89% sobre Base-FB para los 350 temas. La ganancia P10 más baja sobre Base-FB fue del +3.82% para TREC-7 y la más alta fue del +11.95% para wt10g. Finalmente, tanto Base-FB como RS-FB también mejoraron consistentemente la recuperación en comparación con no utilizar expansión, siendo Base-FB el que logró una mejor recuperación que RS-FB para todos los conjuntos de temas. 3.4 Robustez de recuperación. Utilizamos el término robustez para referirnos al rendimiento de precisión promedio en el peor de los casos de un algoritmo de retroalimentación. Idealmente, un método de retroalimentación sólido nunca debería funcionar peor que usar la consulta original, mientras que a menudo funciona mejor utilizando la expansión. Para evaluar la robustez en este estudio, utilizamos una medida muy simple llamada índice de robustez (RI). Para un conjunto de consultas Q, la medida de RI se define como: RI(Q) = n+ − n− |Q| (6) donde n+ es el número de consultas ayudadas por el método de retroalimentación y n− es el número de consultas perjudicadas. Aquí, por \"ayudar\" nos referimos a obtener una precisión promedio más alta como resultado de la retroalimentación. El valor de RI varía desde un mínimo de 3. A veces también se le llama índice de confiabilidad de mejora y fue utilizado en Sakai et al. [17]. Colección NoExp Base-FB RS-FB TREC 1&2 AvgP 0.1818 0.2419 (+33.04%) 0.2406 (+32.24%) P10 0.4443 0.4913 (+10.57%) 0.5363 (+17.83%) Recall 15084/37393 19172/37393 15396/37393 TREC 7 AvgP 0.1890 0.2175 (+15.07%) 0.2169 (+14.75%) P10 0.4200 0.4320 (+2.85%) 0.4480 (+6.67%) Recall 2179/4674 2608/4674 2487/4674 TREC 8 AvgP 0.2031 0.2361 (+16.25%) 0.2268 (+11.70%) P10 0.3960 0.4160 (+5.05%) 0.4340 (+9.59%) Recall 2144/4728 2642/4728 2485/4728 wt10g AvgP 0.1741 0.1829 (+5.06%) 0.1946 (+11.78%) P10 0.2760 0.2630 (-4.71%) 0.2960 (+7.24%) Recall 3361/5980 3725/5980 3664/5980 Tabla 1: Comparación de la retroalimentación base (Base-FB) y la retroalimentación utilizando re-muestreo (RS-FB). La mejora mostrada para BaseFB y RS-FB es relativa al uso de ninguna expansión. (a) TREC 1&2 (curva superior); TREC 8 (curva inferior) (b) TREC 7 (curva superior); wt10g (curva inferior) Figura 3: La compensación entre robustez y precisión promedio para diferentes corpora. El eje x muestra el cambio en la MAP al utilizar la expansión de la línea base con α = 0.5. El eje y muestra el Índice de Robustez (RI). Cada curva a través de los puntos no circulados muestra el compromiso entre RI/MAP utilizando la estrategia simple de pequeño α (ver texto) a medida que α disminuye de 0.5 a cero en la dirección de la flecha. Los puntos circulados representan los compromisos obtenidos al volver a muestrear la retroalimentación para α = 0.5. Colección N Base-FB RS-FB n− RI n− RI TREC 1&2 103 26 +0.495 15 +0.709 TREC 7 46 14 +0.391 10 +0.565 TREC 8 44 12 +0.455 12 +0.455 wt10g 91 48 -0.055 39 +0.143 Combinado 284 100 +0.296 76 +0.465 Tabla 2: Comparación del índice de robustez (RI) para retroalimentación de línea base (Base-FB) vs. retroalimentación de remuestreo (RS-FB). También se muestran el número real de consultas perjudicadas por la retroalimentación (n−) para cada método y colección. Las consultas para las cuales la precisión promedio inicial era insignificante (≤ 0.01) fueron ignoradas, dando como resultado el recuento de consultas restantes en la columna N, de -1.0, cuando todas las consultas son perjudicadas por el método de retroalimentación, a +1.0 cuando todas las consultas son ayudadas. La medida de RI no tiene en cuenta la magnitud o distribución de la cantidad de cambio en el conjunto Q. Sin embargo, es fácil de entender como una indicación general de robustez. Una forma obvia de mejorar el rendimiento en el peor de los casos de la retroalimentación es simplemente utilizar un parámetro de interpolación α fijo más pequeño, como α = 0.3, otorgando menos peso al modelo de retroalimentación (posiblemente arriesgado) y más al cuestionario original. Llamamos a esto la estrategia de pequeña α. Dado que también estamos reduciendo las ganancias potenciales cuando el modelo de retroalimentación es correcto, sin embargo, esperaríamos algún tipo de compensación entre la precisión promedio y la robustez. Por lo tanto, comparamos el equilibrio entre precisión y robustez entre nuestro algoritmo de retroalimentación de remuestreo y el método simple de pequeño α. Los resultados se resumen en la Figura 3. En la figura, la curva para cada conjunto de temas interpola entre los puntos de compensación, comenzando en x=0, donde α = 0.5, y continuando en la dirección de la flecha a medida que α disminuye y la consulta original recibe cada vez más peso. Como era de esperar, la robustez aumenta continuamente a medida que avanzamos a lo largo de la curva, pero la precisión media promedio generalmente disminuye a medida que se eliminan las ganancias del feedback. Para la comparación, se muestra el rendimiento del re-muestreo con retroalimentación en α = 0.5 para cada colección como el punto marcado con un círculo. Más alto y a la derecha es mejor. Esta figura muestra que el re-muestreo de retroalimentación ofrece un compromiso algo mejor que el enfoque de pequeño α para 3 de las 4 colecciones. Figura 4: Histograma que muestra la mayor robustez del re-muestreo de retroalimentación (RS-FB) sobre la retroalimentación base (Base-FB) para todos los conjuntos de datos combinados. Las consultas se agrupan por el % de cambio en AP en comparación con la consulta no expandida. Colección DS + QV DS + No QV TREC 1&2 AvgP 0.2406 0.2547 (+5.86%) P10 0.5263 0.5362 (+1.88%) RI 0.7087 0.6515 (-0.0572) TREC 7 AvgP 0.2169 0.2200 (+1.43%) P10 0.4480 0.4300 (-4.02%) RI 0.5652 0.2609 (-0.3043) TREC 8 AvgP 0.2268 0.2257 (-0.49%) P10 0.4340 0.4200 (-3.23%) RI 0.4545 0.4091 (-0.0454) wt10g AvgP 0.1946 0.1865 (-4.16%) P10 0.2960 0.2680 (-9.46%) RI 0.1429 0.0220 (-0.1209) Tabla 3: Comparación de la retroalimentación de remuestreo utilizando muestreo de documentos (DS) con (QV) y sin (No QV) combinar modelos de retroalimentación de múltiples variantes de consulta. La Tabla 2 muestra los puntajes del Índice de Robustez para Base-FB y RS-FB. El método de retroalimentación RS-FB obtuvo una mayor robustez que Base-FB en tres de los cuatro conjuntos de temas, con solo un rendimiento ligeramente peor en TREC-8. Una vista más detallada que muestra la distribución de los cambios relativos en AP se presenta en el histograma de la Figura 4. En comparación con Base-FB, el método RS-FB logra una reducción notable en el número de consultas significativamente afectadas por la expansión (es decir, donde AP se ve afectado en un 25% o más), al tiempo que conserva ganancias positivas en AP. 3.5 Efecto de los métodos de muestreo de consultas y documentos Dada la mayor robustez de nuestros algoritmos observada en la Sección 3.4, una pregunta importante es qué componente de nuestro sistema es responsable. ¿Es el uso de la re-muestreo de documentos, el uso de múltiples variantes de consulta, o algún otro factor? Los resultados en la Tabla 3 sugieren que la combinación de modelos basada en variantes de consulta puede ser en gran medida responsable de la mayor robustez. Cuando se desactivan las variantes de consulta y se utiliza la consulta original por sí sola con muestreo de documentos, hay poco cambio neto en la precisión promedio, una pequeña disminución en P10 para 3 de los 4 conjuntos de temas, pero una caída significativa en la robustez para todos los conjuntos de temas. En dos casos, la medida de RI disminuye en más del 50%. También examinamos el efecto del método de muestreo de documentos en la efectividad de la recuperación, utilizando dos estrategias diferentes. La estrategia de ponderación uniforme ignoró las puntuaciones de relevancia de la recuperación inicial y otorgó a cada documento en los primeros k la misma probabilidad de selección. Por el contrario, la estrategia de ponderación de puntajes de relevancia seleccionó documentos con una probabilidad proporcional a sus puntajes de relevancia. De esta manera, los documentos que tenían una clasificación más alta tenían más probabilidades de ser seleccionados. Los resultados se muestran en la Tabla 4. La estrategia de ponderación de la puntuación de relevancia tiene un mejor rendimiento en general, con puntajes de RI y P10 significativamente más altos en 3 de los 4 conjuntos de temas. La diferencia en la precisión promedio entre los métodos, sin embargo, es menos marcada. Esto sugiere que el peso uniforme actúa para aumentar la varianza en los resultados de recuperación: cuando la precisión promedio inicial es alta, hay muchos documentos relevantes en los primeros k y el muestreo uniforme puede proporcionar un modelo de relevancia más representativo que centrarse en los elementos altamente clasificados. Por otro lado, cuando la precisión inicial es baja, hay pocos documentos relevantes en las posiciones inferiores y el muestreo uniforme mezcla más de los documentos no relevantes. Por razones de espacio, solo resumimos nuestros hallazgos sobre el tamaño de la muestra aquí. El número de muestras tiene cierto efecto en la precisión cuando es menor a 10, pero el rendimiento se estabiliza alrededor de 15 a 20 muestras. Utilizamos 30 muestras para nuestros experimentos. Mucho más allá de este nivel, los beneficios adicionales de más muestras disminuyen a medida que la distribución inicial de puntajes se ajusta más estrechamente y aumenta el tiempo de procesamiento. 3.6 El efecto del remuestreo en la calidad del término de expansión Idealmente, un modelo de recuperación no debería requerir una lista de palabras vacías al estimar un modelo de relevancia: un modelo estadístico robusto debería reducir automáticamente el peso de las palabras vacías dependiendo del contexto. Las palabras vacías pueden perjudicar la retroalimentación si se seleccionan como términos de retroalimentación, ya que suelen ser malos discriminadores y desperdician espacios de términos valiosos. En la práctica, sin embargo, debido a que la mayoría de los métodos de selección de términos se asemejan a un tipo de ponderación tf · idf, a veces se pueden seleccionar términos con un idf bajo pero un tf muy alto como candidatos de términos de expansión. Esto sucede, por ejemplo, incluso con el enfoque del Modelo de Relevancia que forma parte de nuestra retroalimentación base. Para garantizar una línea base lo más sólida posible, utilizamos una lista de paradas para todos los experimentos reportados aquí. Si desactivamos la lista de palabras vacías, sin embargo, obtenemos resultados como los mostrados en la Tabla 5 donde cuatro de los diez términos de retroalimentación básica principales para el tema 60 de TREC (dijo, pero, su, no) son palabras vacías utilizando el método BaseFB. (Se seleccionaron los 100 términos de expansión principales para generar este ejemplo). El método Indris intenta abordar el problema de las palabras vacías aplicando un paso inicial basado en Ponte [14] para seleccionar términos menos comunes que tienen altas probabilidades logarítmicas de estar en los documentos mejor clasificados en comparación con toda la colección. Sin embargo, esto no resuelve completamente el problema de las palabras vacías, especialmente a medida que aumenta el número de términos de retroalimentación. Sin embargo, el uso de retroalimentación de remuestreo parece mitigar la ponderación de la colección QV + QV uniforme + ponderación de puntaje de relevancia TREC 1&2 AvgP 0.2545 0.2406 (-5.46%) P10 0.5369 0.5263 (-1.97%) RI 0.6212 0.7087 (+14.09%) TREC 7 AvgP 0.2174 0.2169 (-0.23%) P10 0.4320 0.4480 (+3.70%) RI 0.4783 0.5652 (+18.17%) TREC 8 AvgP 0.2267 0.2268 (+0.04%) P10 0.4120 0.4340 (+5.34%) RI 0.4545 0.4545 (+0.00%) wt10g AvgP 0.1808 0.1946 (+7.63%) P10 0.2680 0.2960 (+10.45%) RI 0.0220 0.1099 (+399.5%) Tabla 4: Comparación de muestreo de documentos uniforme y ponderado por relevancia. El cambio porcentual en comparación con el muestreo uniforme se muestra entre paréntesis. QV indica que se utilizaron variantes de consulta en ambas ejecuciones. FB de línea base p(wi|R) FB de remuestreo p(wi|R) dijo 0.055 corte 0.026 corte 0.055 pagar 0.018 pagar 0.034 federal 0.012 pero 0.026 educación 0.011 empleados 0.024 maestros 0.010 sus 0.024 empleados 0.010 no 0.023 caso 0.010 federal 0.021 sus 0.009 trabajadores 0.020 apelaciones 0.008 educación 0.020 sindicato 0.007 Tabla 5: Calidad del término de retroalimentación cuando no se utiliza una lista de paradas. Términos de retroalimentación para el tema 60 de TREC: pago por mérito vs antigüedad. el efecto de las palabras vacías automáticamente. En el ejemplo de la Tabla 5, la retroalimentación de remuestreo deja solo una palabra vacía (su) en las diez primeras posiciones. Observamos un comportamiento similar del término de retroalimentación en muchos otros temas. La razón de este efecto parece ser la interacción de la puntuación de selección de términos con el límite superior de términos m. Si bien la presencia e incluso la proporción de palabras vacías particulares es bastante estable en diferentes muestras de documentos, su posición relativa en la lista de los primeros m elementos no lo es, ya que se examinan conjuntos de documentos con diferentes números de candidatos de términos mejores y de menor frecuencia para cada muestra. Como resultado, si bien algunos stopwords pueden aparecer en cada conjunto de documentos muestreados, tiende a ocurrir que cualquier stopword dado caiga por debajo del umbral para múltiples muestras, lo que lleva a su clasificación como una característica de alta varianza y bajo peso. 4. TRABAJO RELACIONADO Nuestro enfoque está relacionado con trabajos previos de varias áreas de recuperación de información y aprendizaje automático. Nuestro uso de la variación de consultas fue inspirado por el trabajo de YomTov et al. [20], Carpineto et al. [5] y Amati et al. [2], entre otros. Estos estudios utilizan la idea de crear múltiples subconsultas y luego examinar la naturaleza de la superposición en los documentos y/o términos de expansión que resultan de cada subconsulta. La combinación de modelos se realiza utilizando heurísticas. En particular, los estudios de Amati et al. y Carpineto et al. investigaron la combinación de términos de métodos de distribución individuales utilizando una heurística de combinación de reordenamiento de términos. En un conjunto de temas de TREC encontraron una amplia variación promedio en la distancia de rango de los términos de diferentes métodos de expansión. Su método de combinación proporcionó modestas mejoras positivas en la precisión promedio. La idea de examinar la superposición entre listas de términos sugeridos también ha sido utilizada en enfoques tempranos de <br>expansión de consultas</br>. El método de Análisis de Contexto Local (LCA) de Xu y Crofts [19] incluye un factor en la fórmula de ponderación derivada empíricamente que hace que se prefieran los términos de expansión que tienen conexiones con múltiples términos de consulta. En el lado del documento, el trabajo reciente de Zhou & Croft [21] exploró la idea de agregar ruido a los documentos, volver a puntuarlos y utilizar la estabilidad de las clasificaciones resultantes como una estimación de la dificultad de la consulta. Esto está relacionado con nuestro uso de muestreo de documentos para estimar el riesgo del modelo de retroalimentación construido a partir de los diferentes conjuntos de documentos recuperados en la parte superior. Sakai et al. [17] propusieron un enfoque para mejorar la robustez de la retroalimentación de relevancia pseudo utilizando un método que llaman muestreo selectivo. La esencia de su método es que permiten omitir algunos documentos de alta clasificación, basándose en un criterio de agrupación, para seleccionar un conjunto de documentos más variado e innovador más adelante en la clasificación para su uso por un método tradicional de retroalimentación pseudo. Su estudio no encontró mejoras significativas ni en robustez (RI) ni en MAP en sus corpora. Greiff, Morgan y Ponte [8] exploraron el papel de la varianza en la ponderación de términos. En una serie de simulaciones que simplificaron el problema a documentos de 2 características, encontraron que la precisión promedio disminuye a medida que la varianza de la frecuencia de términos - ruido alto - aumenta. La reducción del peso de los términos con alta varianza resultó en una mejora de la precisión promedio. Esto parece estar en concordancia con nuestros propios hallazgos para los modelos de retroalimentación individual. Las estimaciones de la varianza de la producción han sido utilizadas recientemente para mejorar la clasificación de textos. Lee et al. [11] utilizaron estimaciones de varianza específicas de la consulta de las salidas del clasificador para realizar una combinación de modelos mejorada. En lugar de utilizar muestreo, pudieron derivar expresiones en forma cerrada para la varianza del clasificador asumiendo clasificadores base utilizando tipos simples de redes de inferencia. Ando y Zhang propusieron un método que ellos llaman retroalimentación estructural [3] y mostraron cómo aplicarlo a la <br>expansión de consultas</br> para la pista de Genómica de TREC. Utilizaron variaciones de consultas r para obtener R conjuntos diferentes Sr de documentos mejor clasificados que se han intersectado con los documentos mejor clasificados obtenidos de la consulta original qorig. Para cada Si, se calcula el vector centróide normalizado ˆwi de los documentos. El análisis de componentes principales (PCA) se aplica luego a los ˆwi para obtener la matriz Φ de los vectores singulares izquierdos φh que se utilizan para obtener la nueva consulta expandida qexp = qorig + ΦT Φqorig. (7) En el caso de H = 1, tenemos un único vector singular izquierdo φ: qexp = qorig + (φT qorig)φ, de modo que el producto punto φT qorig es un tipo de peso dinámico en la consulta expandida que se basa en la similitud de la consulta original con la consulta expandida. El uso de la varianza como medida de calidad del modelo de retroalimentación ocurre de forma indirecta a través de la aplicación de PCA. Sería interesante estudiar las conexiones entre este enfoque y nuestro propio método de ajuste de modelos. Finalmente, en los enfoques de modelado del lenguaje para retroalimentación, Tao y Zhai [18] describen un método para una retroalimentación más robusta que permite que cada documento tenga un α de retroalimentación diferente. Los pesos de retroalimentación se derivan automáticamente utilizando EM regularizado. La condición de parada de EM implica un equilibrio aproximadamente igual entre el modelo de consulta y expansión. Proponen adaptar el parámetro de parada η basado en una función de alguna medida de calidad de los documentos de retroalimentación. CONCLUSIONES Hemos presentado un nuevo enfoque para la retroalimentación de relevancia pseudo basado en el muestreo de documentos y consultas. El uso del muestreo es un dispositivo muy flexible y poderoso, motivado por nuestro deseo general de ampliar los modelos actuales de recuperación mediante la estimación del riesgo o la varianza asociada con los parámetros o la salida de los procesos de recuperación. Tales estimaciones de varianza, por ejemplo, pueden ser utilizadas de forma natural en un marco bayesiano para una mejor estimación y combinación de modelos. Aplicaciones como la expansión selectiva pueden ser implementadas de manera fundamentada. Si bien nuestro estudio utiliza el enfoque de modelado del lenguaje como marco para experimentos, hacemos pocas suposiciones sobre el funcionamiento real del algoritmo de retroalimentación. Creemos que es probable que cualquier algoritmo de retroalimentación de línea base razonablemente efectivo se beneficiaría de nuestro enfoque. Nuestros resultados en colecciones estándar de TREC muestran que nuestro marco mejora la robustez de un método de retroalimentación de referencia sólido en una variedad de colecciones, sin sacrificar la precisión promedio. También proporciona pequeñas pero consistentes mejoras en la precisión del top 10. En trabajos futuros, visualizamos una investigación sobre cómo variar el conjunto de métodos de muestreo utilizados y el número de muestras controla el equilibrio entre robustez, precisión y eficiencia. Agradecimientos Agradecemos a Paul Bennett por las valiosas discusiones relacionadas con este trabajo, el cual fue apoyado por las becas de la NSF #IIS-0534345 y #CNS-0454018, y la beca del Departamento de Educación de los EE. UU. #R305G03123. Cualquier opinión, hallazgo, conclusión o recomendación expresada en este material son del autor y no necesariamente reflejan las de los patrocinadores. REFERENCIAS [1] El kit de herramientas Lemur para modelado de lenguaje y recuperación. http://www.lemurproject.org. [2] G. Amati, C. Carpineto y G. Romano. Dificultad de la consulta, robustez y aplicación selectiva de la <br>expansión de consultas</br>. En Actas de la 25ª Conferencia Europea sobre Recuperación de Información (ECIR 2004), páginas 127-137. [3] R. K. Ando y T. Zhang. Un método de aprendizaje semisupervisado de alto rendimiento para segmentación de texto. En Actas de la 43ª Reunión Anual de la ACL, páginas 1-9, junio de 2005. [4] L. Breiman. Empaquetando predictores. Aprendizaje automático, 24(2):123-140, 1996. [5] C. Carpineto, G. Romano y V. Giannini. Mejorando la retroalimentación de recuperación con la combinación de múltiples funciones de clasificación de términos. ACM Trans. Información. Sistemas, 20(3):259 - 290. [6] K. Collins-Thompson, P. Ogilvie y J. Callan. Resultados iniciales con consultas estructuradas y modelos de lenguaje en medio terabyte de texto. En Actas de la Conferencia de Recuperación de Información de Texto de 2005. Publicación Especial del NIST. [7] R. O. Duda, P. E. Hart y D. G. Stork. Clasificación de patrones. Wiley and Sons, 2da edición, 2001. [8] W. R. Greiff, W. T. Morgan y J. M. Ponte. El papel de la varianza en la ponderación de términos para la recuperación de información probabilística. En Actas de la 11ª Conferencia Internacional. Conf. sobre Gestión de Información y Conocimiento (CIKM 2002), páginas 252-259. [9] T. Kohonen, J. Hynninen, J. Kangas y J. Laaksonen. SOMPAK: El paquete de programas de mapas autoorganizados. Informe técnico A31, Universidad de Tecnología de Helsinki, 1996. http://www.cis.hut.fi/research/papers/som tr96.ps.Z. [10] V. Lavrenko. Una teoría generativa de relevancia. Tesis doctoral, Universidad de Massachusetts, Amherst, 2004. [11] C.-H. Lee, R. Greiner y S. Wang. Utilizando estimaciones de varianza específicas de la consulta para combinar clasificadores bayesianos. En Actas del 23º Congreso Internacional. Conf. sobre Aprendizaje Automático (ICML 2006), páginas 529-536. [12] D. Metzler y W. B. Croft. Combinando el modelo de lenguaje y los enfoques de red de inferencia para la recuperación. Información. Procesamiento y Gestión, 40(5):735-750, 2004. [13] T. Minka. Estimando una distribución de Dirichlet. Informe técnico, 2000. http://research.microsoft.com/minka/papers/dirichlet. [14] J. Ponte. Avances en la Recuperación de Información, capítulo Modelos de lenguaje para retroalimentación de relevancia, páginas 73-96. 2000. W.B. Croft, ed. [15] J. M. Ponte y W. B. Croft. Un enfoque de modelado del lenguaje para la recuperación de información. En Actas de la Conferencia ACM SIGIR de 1998 sobre Investigación y Desarrollo en Recuperación de Información, páginas 275-281. [16] J. Rocchio. El Sistema de Recuperación SMART, capítulo Retroalimentación de Relevancia en la Recuperación de Información, páginas 313-323. Prentice-Hall, 1971. G. Salton, ed. [17] T. Sakai, T. Manabe y M. Koyama. Retroalimentación de pseudo-relevancia flexible a través de muestreo selectivo. ACM Transactions on Asian Language Information Processing (TALIP), 4(2):111-135, 2005. [18] T. Tao y C. Zhai. Estimación regularizada de modelos de mezcla para retroalimentación de pseudo relevancia robusta. En Actas de la Conferencia ACM SIGIR 2006 sobre Investigación y Desarrollo en Recuperación de Información, páginas 162-169. [19] J. Xu y W. B. Croft. Mejorando la efectividad de la recuperación de información con análisis de contexto local. ACM Trans. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate to Spanish? Syst., 18(1):79-112, 2000. [20] E. YomTov, S. Fine, D. Carmel, y A. Darlow. Aprendiendo a estimar la dificultad de la consulta. En Actas de la Conferencia ACM SIGIR 2005 sobre Investigación y Desarrollo en Recuperación de Información, páginas 512-519. [21] Y. Zhou y W. B. Croft. Robustez del ranking: un nuevo marco para predecir el rendimiento de la consulta. En Actas de la 15ª Conferencia Internacional de la ACM. Conferencia sobre Gestión de Información y Conocimiento (CIKM 2006), páginas 567-574. ",
            "candidates": [],
            "error": [
                [
                    "término de expansión de consulta",
                    "expansión de consultas",
                    "expansión de consultas",
                    "expansión de consultas"
                ]
            ]
        },
        "probability distribution": {
            "translated_key": "distribución de probabilidad",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Estimation and Use of Uncertainty in Pseudo-relevance Feedback Kevyn Collins-Thompson and Jamie Callan Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213-8213 U.S.A. {kct | callan}@cs.cmu.edu ABSTRACT Existing pseudo-relevance feedback methods typically perform averaging over the top-retrieved documents, but ignore an important statistical dimension: the risk or variance associated with either the individual document models, or their combination.",
                "Treating the baseline feedback method as a black box, and the output feedback model as a random variable, we estimate a posterior distribution for the feedback model by resampling a given querys top-retrieved documents, using the posterior mean or mode as the enhanced feedback model.",
                "We then perform model combination over several enhanced models, each based on a slightly modified query sampled from the original query.",
                "We find that resampling documents helps increase individual feedback model precision by removing noise terms, while sampling from the query improves robustness (worst-case performance) by emphasizing terms related to multiple query aspects.",
                "The result is a meta-feedback algorithm that is both more robust and more precise than the original strong baseline method.",
                "Categories and Subject Descriptors: H.3.3 [Information Retrieval]: Retrieval Models General Terms: Algorithms, Experimentation 1.",
                "INTRODUCTION Uncertainty is an inherent feature of information retrieval.",
                "Not only do we not know the queries that will be presented to our retrieval algorithm ahead of time, but the users information need may be vague or incompletely specified by these queries.",
                "Even if the query were perfectly specified, language in the collection documents is inherently complex and ambiguous and matching such language effectively is a formidable problem by itself.",
                "With this in mind, we wish to treat many important quantities calculated by the retrieval system, whether a relevance score for a document, or a weight for a query expansion term, as random variables whose true value is uncertain but where the uncertainty about the true value may be quantified by replacing the fixed value with a <br>probability distribution</br> over possible values.",
                "In this way, retrieval algorithms may attempt to quantify the risk or uncertainty associated with their output rankings, or improve the stability or precision of their internal calculations.",
                "Current algorithms for pseudo-relevance feedback (PRF) tend to follow the same basic method whether we use vector space-based algorithms such as Rocchios formula [16], or more recent language modeling approaches such as Relevance Models [10].",
                "First, a set of top-retrieved documents is obtained from an initial query and assumed to approximate a set of relevant documents.",
                "Next, a single feedback model vector is computed according to some sort of average, centroid, or expectation over the set of possibly-relevant document models.",
                "For example, the document vectors may be combined with equal weighting, as in Rocchio, or by query likelihood, as may be done using the Relevance Model1 .",
                "The use of an expectation is reasonable for practical and theoretical reasons, but by itself ignores potentially valuable information about the risk of the feedback model.",
                "Our main hypothesis in this paper is that estimating the uncertainty in feedback is useful and leads to better individual feedback models and more robust combined models.",
                "Therefore, we propose a method for estimating uncertainty associated with an individual feedback model in terms of a posterior distribution over language models.",
                "To do this, we systematically vary the inputs to the baseline feedback method and fit a Dirichlet distribution to the output.",
                "We use the posterior mean or mode as the improved feedback model estimate.",
                "This process is shown in Figure 1.",
                "As we show later, the mean and mode may vary significantly from the single feedback model proposed by the baseline method.",
                "We also perform model combination using several improved feedback language models obtained by a small number of new queries sampled from the original query.",
                "A models weight combines two complementary factors: the models probability of generating the query, and the variance of the model, with high-variance models getting lower weight. 1 For example, an expected parameter vector conditioned on the query observation is formed from top-retrieved documents, which are treated as training strings (see [10], p. 62).",
                "Figure 1: Estimating the uncertainty of the feedback model for a single query. 2.",
                "SAMPLING-BASED FEEDBACK In Sections 2.1-2.5 we describe a general method for estimating a <br>probability distribution</br> over the set of possible language models.",
                "In Sections 2.6 and 2.7 we summarize how different query samples are used to generate multiple feedback models, which are then combined. 2.1 Modeling Feedback Uncertainty Given a query Q and a collection C, we assume a probabilistic retrieval system that assigns a real-valued document score f(D, Q) to each document D in C, such that the score is proportional to the estimated probability of relevance.",
                "We make no other assumptions about f(D, Q).",
                "The nature of f(D, Q) may be complex: for example, if the retrieval system supports structured query languages [12], then f(D, Q) may represent the output of an arbitrarily complex inference network defined by the structured query operators.",
                "In theory, the scoring function can vary from query to query, although in this study for simplicity we keep the scoring function the same for all queries.",
                "Our specific query method is given in Section 3.",
                "We treat the feedback algorithm as a black box and assume that the inputs to the feedback algorithm are the original query and the corresponding top-retrieved documents, with a score being given to each document.",
                "We assume that the output of the feedback algorithm is a vector of term weights to be used to add or reweight the terms in the representation of the original query, with the vector normalized to form a <br>probability distribution</br>.",
                "We view the the inputs to the feedback black box as random variables, and analyze the feedback model as a random variable that changes in response to changes in the inputs.",
                "Like the document scoring function f(D, Q), the feedback algorithm may implement a complex, non-linear scoring formula, and so as its inputs vary, the resulting feedback models may have a complex distribution over the space of feedback models (the sample space).",
                "Because of this potential complexity, we do not attempt to derive a posterior distribution in closed form, but instead use simulation.",
                "We call this distribution over possible feedback models the feedback model distribution.",
                "Our goal in this section is to estimate a useful approximation to the feedback model distribution.",
                "For a specific framework for experiments, we use the language modeling (LM) approach for information retrieval [15].",
                "The score of a document D with respect to a query Q and collection C is given by p(Q|D) with respect to language models ˆθQ and ˆθD estimated for the query and document respectively.",
                "We denote the set of k top-retrieved documents from collection C in response to Q by DQ(k, C).",
                "For simplicity, we assume that queries and documents are generated by multinomial distributions whose parameters are represented by unigram language models.",
                "To incorporate feedback in the LM approach, we assume a model-based scheme in which our goal is take the query and resulting ranked documents DQ(k, C) as input, and output an expansion language model ˆθE, which is then interpolated with the original query model ˆθQ: ˆθNew = (1 − α) · ˆθQ + α · ˆθE (1) This includes the possibility of α = 1 where the original query mode is completely replaced by the feedback model.",
                "Our sample space is the set of all possible language models LF that may be output as feedback models.",
                "Our approach is to take samples from this space and then fit a distribution to the samples using maximum likelihood.",
                "For simplicity, we start by assuming the latent feedback distribution has the form of a Dirichlet distribution.",
                "Although the Dirichlet is a unimodal distribution, and in general quite limited in its expressiveness in the sample space, it is a natural match for the multinomial language model, can be estimated quickly, and can capture the most salient features of confident and uncertain feedback models, such as the overall spread of the distibution. 2.2 Resampling document models We would like an approximation to the posterior distribution of the feedback model LF .",
                "To accomplish this, we apply a widely-used simulation technique called bootstrap sampling ([7], p. 474) on the input parameters, namely, the set of top-retrieved documents.",
                "Bootstrap sampling allows us to simulate the approximate effect of perturbing the parameters within the black box feedback algorithm by perturbing the inputs to that algorithm in a systematic way, while making no assumptions about the nature of the feedback algorithm.",
                "Specifically, we sample k documents with replacement from DQ(k, C), and calculate an expansion language model θb using the black box feedback method.",
                "We repeat this process B times to obtain a set of B feedback language models, to which we then fit a Dirichlet distribution.",
                "Typically B is in the range of 20 to 50 samples, with performance being relatively stable in this range.",
                "Note that instead of treating each top document as equally likely, we sample according to the estimated probabilities of relevance of each document in DQ(k, C).",
                "Thus, a document is more likely to be chosen the higher it is in the ranking. 2.3 Justification for a sampling approach The rationale for our sampling approach has two parts.",
                "First, we want to improve the quality of individual feedback models by smoothing out variation when the baseline feedback model is unstable.",
                "In this respect, our approach resembles bagging [4], an ensemble approach which generates multiple versions of a predictor by making bootstrap copies of the training set, and then averages the (numerical) predictors.",
                "In our application, top-retrieved documents can be seen as a kind of noisy training set for relevance.",
                "Second, sampling is an effective way to estimate basic properties of the feedback posterior distribution, which can then be used for improved model combination.",
                "For example, a model may be weighted by its prediction confidence, estimated as a function of the variability of the posterior around the model. foo2-401.map-Dim:5434,Size:12*12units,gaussianneighborhood (a) Topic 401 Foreign minorities, Germany foo2-402.map-Dim:5698,Size:12*12units,gaussianneighborhood (b) Topic 402 Behavioral genetics foo2-459.map-Dim:8969,Size:12*12units,gaussianneighborhood (c) Topic 459 When can a lender foreclose on property Figure 2: Visualization of expansion language model variance using self-organizing maps, showing the distribution of language models that results from resampling the inputs to the baseline expansion method.",
                "The language model that would have been chosen by the baseline expansion is at the center of each map.",
                "The similarity function is JensenShannon divergence. 2.4 Visualizing feedback distributions Before describing how we fit and use the Dirichlet distribution over feedback models, it is instructive to view some examples of actual feedback model distributions that result from bootstrap sampling the top-retrieved documents from different TREC topics.",
                "Each point in our sample space is a language model, which typically has several thousand dimensions.",
                "To help analyze the behavior of our method we used a Self-Organizing Map (via the SOM-PAK package [9]), to flatten and visualize the high-dimensional density function2 .",
                "The density maps for three TREC topics are shown in Figure 2 above.",
                "The dark areas represent regions of high similarity between language models.",
                "The light areas represent regions of low similarity - the valleys between clusters.",
                "Each diagram is centered on the language model that would have been chosen by the baseline expansion.",
                "A single peak (mode) is evident in some examples, but more complex structure appears in others.",
                "Also, while the distribution is usually close to the baseline feedback model, for some topics they are a significant distance apart (as measured by JensenShannon divergence), as in Subfigure 2c.",
                "In such cases, the mode or mean of the feedback distribution often performs significantly better than the baseline (and in a smaller proportion of cases, significantly worse). 2.5 Fitting a posterior feedback distribution After obtaining feedback model samples by resampling the feedback model inputs, we estimate the feedback distribution.",
                "We assume that the multinomial feedback models {ˆθ1, . . . , ˆθB} were generated by a latent Dirichlet distribution with parameters {α1, . . . , αN }.",
                "To estimate the {α1, . . . , αN }, we fit the Dirichlet parameters to the B language model samples according to maximum likelihood using a generalized Newton procedure, details of which are given in Minka [13].",
                "We assume a simple Dirichlet prior over the {α1, . . . , αN }, setting each to αi = μ · p(wi | C), where μ is a parameter and p(· | C) is the collection language model estimated from a set of documents from collection C. The parameter fitting converges very quickly - typically just 2 or 2 Because our points are language models in the multinomial simplex, we extended SOM-PAK to support JensenShannon divergence, a widely-used similarity measure between probability distributions. 3 iterations are enough - so that it is practical to apply at query-time when computational overhead must be small.",
                "In practice, we can restrict the calculation to the vocabulary of the top-retrieved documents, instead of the entire collection.",
                "Note that for this step we are re-using the existing retrieved documents and not performing additional queries.",
                "Given the parameters of an N-dimensional Dirichlet distribution Dir(α) the mean μ and mode x vectors are easy to calculate and are given respectively by μi = αiP αi (2) and xi = αi−1P αi−N . (3) We can then choose the language model at the mean or the mode of the posterior as the final enhanced feedback model. (We found the mode to give slightly better performance.)",
                "For information retrieval, the number of samples we will have available is likely to be quite small for performance reasons - usually less than ten.",
                "Moreover, while random sampling is useful in certain cases, it is perfectly acceptable to allow deterministic sampling distributions, but these must be designed carefully in order to approximate an accurate output variance.",
                "We leave this for future study. 2.6 Query variants We use the following methods for generating variants of the original query.",
                "Each variant corresponds to a different assumption about which aspects of the original query may be important.",
                "This is a form of deterministic sampling.",
                "We selected three simple methods that cover complimentary assumptions about the query.",
                "No-expansion Use only the original query.",
                "The assumption is that the given terms are a complete description of the information need.",
                "Leave-one-out A single term is left out of the original query.",
                "The assumption is that one of the query terms is a noise term.",
                "Single-term A single term is chosen from the original query.",
                "This assumes that only one aspect of the query, namely, that represented by the term, is most important.",
                "After generating a variant of the original query, we combine it with the original query using a weight αSUB so that we do not stray too far.",
                "In this study, we set αSUB = 0.5.",
                "For example, using the Indri [12] query language, a leave-oneout variant of the initial query that omits the term ireland for TREC topic 404 is: #weight(0.5 #combine(ireland peace talks) 0.5 #combine(peace talks)) 2.7 Combining enhanced feedback models from multiple query variants When using multiple query variants, the resulting enhanced feedback models are combined using Bayesian model combination.",
                "To do this, we treat each word as an item to be classified as belonging to a relevant or non-relevant class, and derive a class probability for each word by combining the scores from each query variant.",
                "Each score is given by that terms probability in the Dirichlet distribution.",
                "The term scores are weighted by the inverse of the variance of the term in the enhanced feedback models Dirichlet distribution.",
                "The prior probability of a words membership in the relevant class is given by the probability of the original query in the entire enhanced expansion model. 3.",
                "EVALUATION In this section we present results confirming the usefulness of estimating a feedback model distribution from weighted resampling of top-ranked documents, and of combining the feedback models obtained from different small changes in the original query. 3.1 General method We evaluated performance on a total of 350 queries derived from four sets of TREC topics: 51-200 (TREC-1&2), 351-400 (TREC-7), 401-450 (TREC-8), and 451-550 (wt10g, TREC-9&10).",
                "We chose these for their varied content and document properties.",
                "For example, wt10g documents are Web pages with a wide variety of subjects and styles while TREC-1&2 documents are more homogeneous news articles.",
                "Indexing and retrieval was performed using the Indri system in the Lemur toolkit [12] [1].",
                "Our queries were derived from the words in the title field of the TREC topics.",
                "Phrases were not used.",
                "To generate the baseline queries passed to Indri, we wrapped the query terms with Indris #combine operator.",
                "For example, the initial query for topic 404 is: #combine(ireland peace talks) We performed Krovetz stemming for all experiments.",
                "Because we found that the baseline (Indri) expansion method performed better using a stopword list with the feedback model, all experiments used a stoplist of 419 common English words.",
                "However, an interesting side-effect of our resampling approach is that it tends to remove many stopwords from the feedback model, making a stoplist less critical.",
                "This is discussed further in Section 3.6. 3.2 Baseline feedback method For our baseline expansion method, we use an algorithm included in Indri 1.0 as the default expansion method.",
                "This method first selects terms using a log-odds calculation described by Ponte [14], but assigns final term weights using Lavrenkos relevance model[10].",
                "We chose the Indri method because it gives a consistently strong baseline, is based on a language modeling approach, and is simple to experiment with.",
                "In a TREC evaluation using the GOV2 corpus [6], the method was one of the topperforming runs, achieving a 19.8% gain in MAP compared to using unexpanded queries.",
                "In this study, it achieves an average gain in MAP of 17.25% over the four collections.",
                "Indris expansion method first calculates a log-odds ratio o(v) for each potential expansion term v given by o(v) = X D log p(v|D) p(v|C) (4) over all documents D containing v, in collection C. Then, the expansion term candidates are sorted by descending o(v), and the top m are chosen.",
                "Finally, the term weights r(v) used in the expanded query are calculated based on the relevance model r(v) = X D p(q|D)p(v|D) p(v) p(D) (5) The quantity p(q|D) is the probability score assigned to the document in the initial retrieval.",
                "We use Dirichlet smoothing of p(v|D) with μ = 1000.",
                "This relevance model is then combined with the original query using linear interpolation, weighted by a parameter α.",
                "By default we used the top 50 documents for feedback and the top 20 expansion terms, with the feedback interpolation parameter α = 0.5 unless otherwise stated.",
                "For example, the baseline expanded query for topic 404 is: #weight(0.5 #combine(ireland peace talks) 0.5 #weight(0.10 ireland 0.08 peace 0.08 northern ...) 3.3 Expansion performance We measure our feedback algorithms effectiveness by two main criteria: precision, and robustness.",
                "Robustness, and the tradeoff between precision and robustness, is analyzed in Section 3.4.",
                "In this section, we examine average precision and precision in the top 10 documents (P10).",
                "We also include recall at 1,000 documents.",
                "For each query, we obtained a set of B feedback models using the Indri baseline.",
                "Each feedback model was obtained from a random sample of the top k documents taken with replacement.",
                "For these experiments, B = 30 and k = 50.",
                "Each feedback model contained 20 terms.",
                "On the query side, we used leave-one-out (LOO) sampling to create the query variants.",
                "Single-term query sampling had consistently worse performance across all collections and so our results here focus on LOO sampling.",
                "We used the methods described in Section 2 to estimate an enhanced feedback model from the Dirichlet posterior distribution for each query variant, and to combine the feedback models from all the query variants.",
                "We call our method resampling expansion and denote it as RS-FB here.",
                "We denote the Indri baseline feedback method as Base-FB.",
                "Results from applying both the baseline expansion method (Base-FB) and resampling expansion (RS-FB) are shown in Table 1.",
                "We observe several trends in this table.",
                "First, the average precision of RS-FB was comparable to Base-FB, achieving an average gain of 17.6% compared to using no expansion across the four collections.",
                "The Indri baseline expansion gain was 17.25%.",
                "Also, the RS-FB method achieved consistent improvements in P10 over Base-FB for every topic set, with an average improvement of 6.89% over Base-FB for all 350 topics.",
                "The lowest P10 gain over Base-FB was +3.82% for TREC-7 and the highest was +11.95% for wt10g.",
                "Finally, both Base-FB and RS-FB also consistently improved recall over using no expansion, with Base-FB achieving better recall than RS-FB for all topic sets. 3.4 Retrieval robustness We use the term robustness to mean the worst-case average precision performance of a feedback algorithm.",
                "Ideally, a robust feedback method would never perform worse than using the original query, while often performing better using the expansion.",
                "To evaluate robustness in this study, we use a very simple measure called the robustness index (RI)3 .",
                "For a set of queries Q, the RI measure is defined as: RI(Q) = n+ − n− |Q| (6) where n+ is the number of queries helped by the feedback method and n− is the number of queries hurt.",
                "Here, by helped we mean obtaining a higher average precision as a result of feedback.",
                "The value of RI ranges from a minimum 3 This is sometimes also called the reliability of improvement index and was used in Sakai et al. [17].",
                "Collection NoExp Base-FB RS-FB TREC 1&2 AvgP 0.1818 0.2419 (+33.04%) 0.2406 (+32.24%) P10 0.4443 0.4913 (+10.57%) 0.5363 (+17.83%) Recall 15084/37393 19172/37393 15396/37393 TREC 7 AvgP 0.1890 0.2175 (+15.07%) 0.2169 (+14.75%) P10 0.4200 0.4320 (+2.85%) 0.4480 (+6.67%) Recall 2179/4674 2608/4674 2487/4674 TREC 8 AvgP 0.2031 0.2361 (+16.25%) 0.2268 (+11.70%) P10 0.3960 0.4160 (+5.05%) 0.4340 (+9.59%) Recall 2144/4728 2642/4728 2485/4728 wt10g AvgP 0.1741 0.1829 (+5.06%) 0.1946 (+11.78%) P10 0.2760 0.2630 (-4.71%) 0.2960 (+7.24%) Recall 3361/5980 3725/5980 3664/5980 Table 1: Comparison of baseline (Base-FB) feedback and feedback using re-sampling (RS-FB).",
                "Improvement shown for BaseFB and RS-FB is relative to using no expansion. (a) TREC 1&2 (upper curve); TREC 8 (lower curve) (b) TREC 7 (upper curve); wt10g (lower curve) Figure 3: The trade-off between robustness and average precision for different corpora.",
                "The x-axis gives the change in MAP over using baseline expansion with α = 0.5.",
                "The yaxis gives the Robustness Index (RI).",
                "Each curve through uncircled points shows the RI/MAP tradeoff using the simple small-α strategy (see text) as α decreases from 0.5 to zero in the direction of the arrow.",
                "Circled points represent the tradeoffs obtained by resampling feedback for α = 0.5.",
                "Collection N Base-FB RS-FB n− RI n− RI TREC 1&2 103 26 +0.495 15 +0.709 TREC 7 46 14 +0.391 10 +0.565 TREC 8 44 12 +0.455 12 +0.455 wt10g 91 48 -0.055 39 +0.143 Combined 284 100 +0.296 76 +0.465 Table 2: Comparison of robustness index (RI) for baseline feedback (Base-FB) vs. resampling feedback (RS-FB).",
                "Also shown are the actual number of queries hurt by feedback (n−) for each method and collection.",
                "Queries for which initial average precision was negligible (≤ 0.01) were ignored, giving the remaining query count in column N. of −1.0, when all queries are hurt by the feedback method, to +1.0 when all queries are helped.",
                "The RI measure does not take into account the magnitude or distribution of the amount of change across the set Q.",
                "However, it is easy to understand as a general indication of robustness.",
                "One obvious way to improve the worst-case performance of feedback is simply to use a smaller fixed α interpolation parameter, such as α = 0.3, placing less weight on the (possibly risky) feedback model and more on the original query.",
                "We call this the small-α strategy.",
                "Since we are also reducing the potential gains when the feedback model is right, however, we would expect some trade-off between average precision and robustness.",
                "We therefore compared the precision/robustness trade-off between our resampling feedback algorithm, and the simple small-α method.",
                "The results are summarized in Figure 3.",
                "In the figure, the curve for each topic set interpolates between trade-off points, beginning at x=0, where α = 0.5, and continuing in the direction of the arrow as α decreases and the original query is given more and more weight.",
                "As expected, robustness continuously increases as we move along the curve, but mean average precision generally drops as the gains from feedback are eliminated.",
                "For comparison, the performance of resampling feedback at α = 0.5 is shown for each collection as the circled point.",
                "Higher and to the right is better.",
                "This figure shows that resampling feedback gives a somewhat better trade-off than the small-α approach for 3 of the 4 collections.",
                "Figure 4: Histogram showing improved robustness of resampling feedback (RS-FB) over baseline feedback (Base-FB) for all datasets combined.",
                "Queries are binned by % change in AP compared to the unexpanded query.",
                "Collection DS + QV DS + No QV TREC 1&2 AvgP 0.2406 0.2547 (+5.86%) P10 0.5263 0.5362 (+1.88%) RI 0.7087 0.6515 (-0.0572) TREC 7 AvgP 0.2169 0.2200 (+1.43%) P10 0.4480 0.4300 (-4.02%) RI 0.5652 0.2609 (-0.3043) TREC 8 AvgP 0.2268 0.2257 (-0.49%) P10 0.4340 0.4200 (-3.23%) RI 0.4545 0.4091 (-0.0454) wt10g AvgP 0.1946 0.1865 (-4.16%) P10 0.2960 0.2680 (-9.46%) RI 0.1429 0.0220 (-0.1209) Table 3: Comparison of resampling feedback using document sampling (DS) with (QV) and without (No QV) combining feedback models from multiple query variants.",
                "Table 2 gives the Robustness Index scores for Base-FB and RS-FB.",
                "The RS-FB feedback method obtained higher robustness than Base-FB on three of the four topic sets, with only slightly worse performance on TREC-8.",
                "A more detailed view showing the distribution over relative changes in AP is given by the histogram in Figure 4.",
                "Compared to Base-FB, the RS-FB method achieves a noticable reduction in the number of queries significantly hurt by expansion (i.e. where AP is hurt by 25% or more), while preserving positive gains in AP. 3.5 Effect of query and document sampling methods Given our algorithms improved robustness seen in Section 3.4, an important question is what component of our system is responsible.",
                "Is it the use of document re-sampling, the use of multiple query variants, or some other factor?",
                "The results in Table 3 suggest that the model combination based on query variants may be largely account for the improved robustness.",
                "When query variants are turned off and the original query is used by itself with document sampling, there is little net change in average precision, a small decrease in P10 for 3 out of the 4 topic sets, but a significant drop in robustness for all topic sets.",
                "In two cases, the RI measure drops by more than 50%.",
                "We also examined the effect of the document sampling method on retrieval effectiveness, using two different strategies.",
                "The uniform weighting strategy ignored the relevance scores from the initial retrieval and gave each document in the top k the same probability of selection.",
                "In contrast, the relevance-score weighting strategy chose documents with probability proportional to their relevance scores.",
                "In this way, documents that were more highly ranked were more likely to be selected.",
                "Results are shown in Table 4.",
                "The relevance-score weighting strategy performs better overall, with significantly higher RI and P10 scores on 3 of the 4 topic sets.",
                "The difference in average precision between the methods, however, is less marked.",
                "This suggests that uniform weighting acts to increase variance in retrieval results: when initial average precision is high, there are many relevant documents in the top k and uniform sampling may give a more representative relevance model than focusing on the highly-ranked items.",
                "On the other hand, when initial precision is low, there are few relevant documents in the bottom ranks and uniform sampling mixes in more of the non-relevant documents.",
                "For space reasons we only summarize our findings on sample size here.",
                "The number of samples has some effect on precision when less than 10, but performance stabilizes at around 15 to 20 samples.",
                "We used 30 samples for our experiments.",
                "Much beyond this level, the additional benefits of more samples decrease as the initial score distribution is more closely fit and the processing time increases. 3.6 The effect of resampling on expansion term quality Ideally, a retrieval model should not require a stopword list when estimating a model of relevance: a robust statistical model should down-weight stopwords automatically depending on context.",
                "Stopwords can harm feedback if selected as feedback terms, because they are typically poor discriminators and waste valuable term slots.",
                "In practice, however, because most term selection methods resemble a tf · idf type of weighting, terms with low idf but very high tf can sometimes be selected as expansion term candidates.",
                "This happens, for example, even with the Relevance Model approach that is part of our baseline feedback.",
                "To ensure as strong a baseline as possible, we use a stoplist for all experiments reported here.",
                "If we turn off the stopword list, however, we obtain results such as those shown in Table 5 where four of the top ten baseline feedback terms for TREC topic 60 (said, but, their, not) are stopwords using the BaseFB method. (The top 100 expansion terms were selected to generate this example.)",
                "Indris method attempts to address the stopword problem by applying an initial step based on Ponte [14] to select less-common terms that have high log-odds of being in the top-ranked documents compared to the whole collection.",
                "Nevertheless, this does not overcome the stopword problem completely, especially as the number of feedback terms grows.",
                "Using resampling feedback, however, appears to mitigate Collection QV + Uniform QV + Relevance-score weighting weighting TREC 1&2 AvgP 0.2545 0.2406 (-5.46%) P10 0.5369 0.5263 (-1.97%) RI 0.6212 0.7087 (+14.09%) TREC 7 AvgP 0.2174 0.2169 (-0.23%) P10 0.4320 0.4480 (+3.70%) RI 0.4783 0.5652 (+18.17%) TREC 8 AvgP 0.2267 0.2268 (+0.04%) P10 0.4120 0.4340 (+5.34%) RI 0.4545 0.4545 (+0.00%) wt10g AvgP 0.1808 0.1946 (+7.63%) P10 0.2680 0.2960 (+10.45%) RI 0.0220 0.1099 (+399.5%) Table 4: Comparison of uniform and relevance-weighted document sampling.",
                "The percentage change compared to uniform sampling is shown in parentheses.",
                "QV indicates that query variants were used in both runs.",
                "Baseline FB p(wi|R) Resampling FB p(wi|R) said 0.055 court 0.026 court 0.055 pay 0.018 pay 0.034 federal 0.012 but 0.026 education 0.011 employees 0.024 teachers 0.010 their 0.024 employees 0.010 not 0.023 case 0.010 federal 0.021 their 0.009 workers 0.020 appeals 0.008 education 0.020 union 0.007 Table 5: Feedback term quality when a stoplist is not used.",
                "Feedback terms for TREC topic 60: merit pay vs seniority. the effect of stopwords automatically.",
                "In the example of Table 5, resampling feedback leaves only one stopword (their) in the top ten.",
                "We observed similar feedback term behavior across many other topics.",
                "The reason for this effect appears to be the interaction of the term selection score with the top-m term cutoff.",
                "While the presence and even proportion of particular stopwords is fairly stable across different document samples, their relative position in the top-m list is not, as sets of documents with varying numbers of better, lower-frequency term candidates are examined for each sample.",
                "As a result, while some number of stopwords may appear in each sampled document set, any given stopword tends to fall below the cutoff for multiple samples, leading to its classification as a high-variance, low-weight feature. 4.",
                "RELATED WORK Our approach is related to previous work from several areas of information retrieval and machine learning.",
                "Our use of query variation was inspired by the work of YomTov et al. [20], Carpineto et al. [5], and Amati et al. [2], among others.",
                "These studies use the idea of creating multiple subqueries and then examining the nature of the overlap in the documents and/or expansion terms that result from each subquery.",
                "Model combination is performed using heuristics.",
                "In particular, the studies of Amati et al. and Carpineto et al. investigated combining terms from individual distributional methods using a term-reranking combination heuristic.",
                "In a set of TREC topics they found wide average variation in the rank-distance of terms from different expansion methods.",
                "Their combination method gave modest positive improvements in average precision.",
                "The idea of examining the overlap between lists of suggested terms has also been used in early query expansion approaches.",
                "Xu and Crofts method of Local Context Analysis (LCA) [19] includes a factor in the empirically-derived weighting formula that causes expansion terms to be preferred that have connections to multiple query terms.",
                "On the document side, recent work by Zhou & Croft [21] explored the idea of adding noise to documents, re-scoring them, and using the stability of the resulting rankings as an estimate of query difficulty.",
                "This is related to our use of document sampling to estimate the risk of the feedback model built from the different sets of top-retrieved documents.",
                "Sakai et al. [17] proposed an approach to improving the robustness of pseudo-relevance feedback using a method they call selective sampling.",
                "The essence of their method is that they allow skipping of some top-ranked documents, based on a clustering criterion, in order to select a more varied and novel set of documents later in the ranking for use by a traditional pseudo-feedback method.",
                "Their study did not find significant improvements in either robustness (RI) or MAP on their corpora.",
                "Greiff, Morgan and Ponte [8] explored the role of variance in term weighting.",
                "In a series of simulations that simplified the problem to 2-feature documents, they found that average precision degrades as term frequency variance - high noiseincreases.",
                "Downweighting terms with high variance resulted in improved average precision.",
                "This seems in accord with our own findings for individual feedback models.",
                "Estimates of output variance have recently been used for improved text classification.",
                "Lee et al. [11] used queryspecific variance estimates of classifier outputs to perform improved model combination.",
                "Instead of using sampling, they were able to derive closed-form expressions for classifier variance by assuming base classifiers using simple types of inference networks.",
                "Ando and Zhang proposed a method that they call structural feedback [3] and showed how to apply it to query expansion for the TREC Genomics Track.",
                "They used r query variations to obtain R different sets Sr of top-ranked documents that have been intersected with the top-ranked documents obtained from the original query qorig.",
                "For each Si, the normalized centroid vector ˆwi of the documents is calculated.",
                "Principal component analysis (PCA) is then applied to the ˆwi to obtain the matrix Φ of H left singular vectors φh that are used to obtain the new, expanded query qexp = qorig + ΦT Φqorig. (7) In the case H = 1, we have a single left singular vector φ: qexp = qorig + (φT qorig)φ so that the dot product φT qorig is a type of dynamic weight on the expanded query that is based on the similarity of the original query to the expanded query.",
                "The use of variance as a feedback model quality measure occurs indirectly through the application of PCA.",
                "It would be interesting to study the connections between this approach and our own modelfitting method.",
                "Finally, in language modeling approaches to feedback, Tao and Zhai [18] describe a method for more robust feedback that allows each document to have a different feedback α.",
                "The feedback weights are derived automatically using regularized EM.",
                "A roughly equal balance of query and expansion model is implied by their EM stopping condition.",
                "They propose tailoring the stopping parameter η based on a function of some quality measure of feedback documents. 5.",
                "CONCLUSIONS We have presented a new approach to pseudo-relevance feedback based on document and query sampling.",
                "The use of sampling is a very flexible and powerful device and is motivated by our general desire to extend current models of retrieval by estimating the risk or variance associated with the parameters or output of retrieval processes.",
                "Such variance estimates, for example, may be naturally used in a Bayesian framework for improved model estimation and combination.",
                "Applications such as selective expansion may then be implemented in a principled way.",
                "While our study uses the language modeling approach as a framework for experiments, we make few assumptions about the actual workings of the feedback algorithm.",
                "We believe it is likely that any reasonably effective baseline feedback algorithm would benefit from our approach.",
                "Our results on standard TREC collections show that our framework improves the robustness of a strong baseline feedback method across a variety of collections, without sacrificing average precision.",
                "It also gives small but consistent gains in top10 precision.",
                "In future work, we envision an investigation into how varying the set of sampling methods used and the number of samples controls the trade-off between robustness, accuracy, and efficiency.",
                "Acknowledgements We thank Paul Bennett for valuable discussions related to this work, which was supported by NSF grants #IIS-0534345 and #CNS-0454018, and U.S. Dept. of Education grant #R305G03123.",
                "Any opinions, findings, and conclusions or recommendations expressed in this material are the authors. and do not necessarily reflect those of the sponsors. 6.",
                "REFERENCES [1] The Lemur toolkit for language modeling and retrieval. http://www.lemurproject.org. [2] G. Amati, C. Carpineto, and G. Romano.",
                "Query difficulty, robustness, and selective application of query expansion.",
                "In Proc. of the 25th European Conf. on Information Retrieval (ECIR 2004), pages 127-137. [3] R. K. Ando and T. Zhang.",
                "A high-performance semi-supervised learning method for text chunking.",
                "In Proc. of the 43rd Annual Meeting of the ACL, pages 1-9, June 2005. [4] L. Breiman.",
                "Bagging predictors.",
                "Machine Learning, 24(2):123-140, 1996. [5] C. Carpineto, G. Romano, and V. Giannini.",
                "Improving retrieval feedback with multiple term-ranking function combination.",
                "ACM Trans.",
                "Info.",
                "Systems, 20(3):259 - 290. [6] K. Collins-Thompson, P. Ogilvie, and J. Callan.",
                "Initial results with structured queries and language models on half a terabyte of text.",
                "In Proc. of 2005 Text REtrieval Conference.",
                "NIST Special Publication. [7] R. O. Duda, P. E. Hart, and D. G. Stork.",
                "Pattern Classification.",
                "Wiley and Sons, 2nd edition, 2001. [8] W. R. Greiff, W. T. Morgan, and J. M. Ponte.",
                "The role of variance in term weighting for probabilistic information retrieval.",
                "In Proc. of the 11th Intl.",
                "Conf. on Info. and Knowledge Mgmt. (CIKM 2002), pages 252-259. [9] T. Kohonen, J. Hynninen, J. Kangas, and J. Laaksonen.",
                "SOMPAK: The self-organizing map program package.",
                "Technical Report A31, Helsinki University of Technology, 1996. http://www.cis.hut.fi/research/papers/som tr96.ps.Z. [10] V. Lavrenko.",
                "A Generative Theory of Relevance.",
                "PhD thesis, University of Massachusetts, Amherst, 2004. [11] C.-H. Lee, R. Greiner, and S. Wang.",
                "Using query-specific variance estimates to combine Bayesian classifiers.",
                "In Proc. of the 23rd Intl.",
                "Conf. on Machine Learning (ICML 2006), pages 529-536. [12] D. Metzler and W. B. Croft.",
                "Combining the language model and inference network approaches to retrieval.",
                "Info.",
                "Processing and Mgmt., 40(5):735-750, 2004. [13] T. Minka.",
                "Estimating a Dirichlet distribution.",
                "Technical report, 2000. http://research.microsoft.com/ minka/papers/dirichlet. [14] J. Ponte.",
                "Advances in Information Retrieval, chapter Language models for relevance feedback, pages 73-96. 2000.",
                "W.B.",
                "Croft, ed. [15] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In Proc. of the 1998 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 275-281. [16] J. Rocchio.",
                "The SMART Retrieval System, chapter Relevance Feedback in Information Retrieval, pages 313-323.",
                "Prentice-Hall, 1971.",
                "G. Salton, ed. [17] T. Sakai, T. Manabe, and M. Koyama.",
                "Flexible pseudo-relevance feedback via selective sampling.",
                "ACM Transactions on Asian Language Information Processing (TALIP), 4(2):111-135, 2005. [18] T. Tao and C. Zhai.",
                "Regularized estimation of mixture models for robust pseudo-relevance feedback.",
                "In Proc. of the 2006 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 162-169. [19] J. Xu and W. B. Croft.",
                "Improving the effectiveness of information retrieval with local context analysis.",
                "ACM Trans.",
                "Inf.",
                "Syst., 18(1):79-112, 2000. [20] E. YomTov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty.",
                "In Proc. of the 2005 ACM SIGIR Conf. on Research and Development in Information Retrieval, pages 512-519. [21] Y. Zhou and W. B. Croft.",
                "Ranking robustness: a novel framework to predict query performance.",
                "In Proc. of the 15th ACM Intl.",
                "Conf. on Information and Knowledge Mgmt. (CIKM 2006), pages 567-574."
            ],
            "original_annotated_samples": [
                "With this in mind, we wish to treat many important quantities calculated by the retrieval system, whether a relevance score for a document, or a weight for a query expansion term, as random variables whose true value is uncertain but where the uncertainty about the true value may be quantified by replacing the fixed value with a <br>probability distribution</br> over possible values.",
                "SAMPLING-BASED FEEDBACK In Sections 2.1-2.5 we describe a general method for estimating a <br>probability distribution</br> over the set of possible language models.",
                "We assume that the output of the feedback algorithm is a vector of term weights to be used to add or reweight the terms in the representation of the original query, with the vector normalized to form a <br>probability distribution</br>."
            ],
            "translated_annotated_samples": [
                "Con esto en mente, deseamos tratar muchas cantidades importantes calculadas por el sistema de recuperación, ya sea un puntaje de relevancia para un documento, o un peso para un término de expansión de consulta, como variables aleatorias cuyo valor real es incierto pero donde la incertidumbre sobre el valor real puede ser cuantificada reemplazando el valor fijo con una <br>distribución de probabilidad</br> sobre posibles valores.",
                "En las Secciones 2.1-2.5 describimos un método general para estimar una <br>distribución de probabilidad</br> sobre el conjunto de posibles modelos de lenguaje basado en muestreo.",
                "Suponemos que la salida del algoritmo de retroalimentación es un vector de pesos de términos que se utilizarán para agregar o reponderar los términos en la representación de la consulta original, con el vector normalizado para formar una <br>distribución de probabilidad</br>."
            ],
            "translated_text": "Estimación y Uso de la Incertidumbre en la Retroalimentación de Pseudo-Relevancia Kevyn Collins-Thompson y Jamie Callan Instituto de Tecnologías del Lenguaje Escuela de Ciencias de la Computación Universidad Carnegie Mellon Pittsburgh, PA 15213-8213 EE. UU. {kct | callan}@cs.cmu.edu RESUMEN Los métodos existentes de retroalimentación de pseudo-relevancia suelen realizar un promedio de los documentos mejor clasificados, pero ignoran una dimensión estadística importante: el riesgo o la varianza asociada tanto a los modelos de documentos individuales como a su combinación. Tratando el método de retroalimentación de línea base como una caja negra, y el modelo de retroalimentación de salida como una variable aleatoria, estimamos una distribución posterior para el modelo de retroalimentación mediante el remuestreo de los documentos mejor recuperados de una consulta dada, utilizando la media o la moda posterior como el modelo de retroalimentación mejorado. Luego realizamos la combinación de modelos sobre varios modelos mejorados, cada uno basado en una consulta ligeramente modificada muestreada de la consulta original. Observamos que el remuestreo de documentos ayuda a aumentar la precisión del modelo de retroalimentación individual al eliminar términos de ruido, mientras que el muestreo de la consulta mejora la robustez (rendimiento en el peor de los casos) al enfatizar términos relacionados con múltiples aspectos de la consulta. El resultado es un algoritmo de meta-retroalimentación que es tanto más robusto como más preciso que el método de referencia original. Categorías y Descriptores de Asignaturas: H.3.3 [Recuperación de Información]: Modelos de Recuperación Términos Generales: Algoritmos, Experimentación 1. INTRODUCCIÓN La incertidumbre es una característica inherente de la recuperación de información. No solo no sabemos las consultas que se presentarán a nuestro algoritmo de recuperación con anticipación, sino que la necesidad de información de los usuarios puede ser vaga o estar incompletamente especificada por estas consultas. Incluso si la consulta estuviera perfectamente especificada, el lenguaje en los documentos de la colección es inherentemente complejo y ambiguo, y hacer coincidir dicho lenguaje de manera efectiva es un problema formidable por sí mismo. Con esto en mente, deseamos tratar muchas cantidades importantes calculadas por el sistema de recuperación, ya sea un puntaje de relevancia para un documento, o un peso para un término de expansión de consulta, como variables aleatorias cuyo valor real es incierto pero donde la incertidumbre sobre el valor real puede ser cuantificada reemplazando el valor fijo con una <br>distribución de probabilidad</br> sobre posibles valores. De esta manera, los algoritmos de recuperación pueden intentar cuantificar el riesgo o la incertidumbre asociados con sus clasificaciones de salida, o mejorar la estabilidad o precisión de sus cálculos internos. Los algoritmos actuales para la retroalimentación de pseudo relevancia (PRF) tienden a seguir el mismo método básico ya sea que utilicemos algoritmos basados en espacio vectorial como la fórmula de Rocchio [16], o enfoques más recientes de modelado de lenguaje como Modelos de Relevancia [10]. Primero, se obtiene un conjunto de documentos recuperados en la parte superior a partir de una consulta inicial y se asume que aproxima un conjunto de documentos relevantes. A continuación, se calcula un vector de modelo de retroalimentación único según algún tipo de promedio, centroide o expectativa sobre el conjunto de modelos de documentos posiblemente relevantes. Por ejemplo, los vectores de documentos pueden combinarse con igual ponderación, como en Rocchio, o por probabilidad de consulta, como se puede hacer utilizando el Modelo de Relevancia. El uso de una expectativa es razonable por razones prácticas y teóricas, pero por sí sola ignora información potencialmente valiosa sobre el riesgo del modelo de retroalimentación. Nuestra hipótesis principal en este artículo es que estimar la incertidumbre en la retroalimentación es útil y conduce a modelos de retroalimentación individual más efectivos y a modelos combinados más robustos. Por lo tanto, proponemos un método para estimar la incertidumbre asociada con un modelo de retroalimentación individual en términos de una distribución posterior sobre modelos de lenguaje. Para hacer esto, variamos sistemáticamente las entradas al método de retroalimentación base y ajustamos una distribución de Dirichlet a la salida. Utilizamos la media posterior o el modo como la estimación mejorada del modelo de retroalimentación. Este proceso se muestra en la Figura 1. Como mostraremos más adelante, la media y la moda pueden variar significativamente del modelo de retroalimentación única propuesto por el método de referencia. También realizamos la combinación de modelos utilizando varios modelos de lenguaje de retroalimentación mejorados obtenidos a partir de un pequeño número de nuevas consultas muestreadas de la consulta original. El peso de un modelo combina dos factores complementarios: la probabilidad del modelo de generar la consulta y la varianza del modelo, siendo los modelos con alta varianza los que obtienen un peso menor. Por ejemplo, un vector de parámetros esperado condicionado a la observación de la consulta se forma a partir de los documentos recuperados en la parte superior, que se tratan como cadenas de entrenamiento (ver [10], p. 62). Figura 1: Estimación de la incertidumbre del modelo de retroalimentación para una sola consulta. 2. En las Secciones 2.1-2.5 describimos un método general para estimar una <br>distribución de probabilidad</br> sobre el conjunto de posibles modelos de lenguaje basado en muestreo. En las Secciones 2.6 y 2.7 resumimos cómo se utilizan diferentes muestras de consultas para generar múltiples modelos de retroalimentación, que luego se combinan. 2.1 Modelado de la Incertidumbre de la Retroalimentación Dado una consulta Q y una colección C, asumimos un sistema de recuperación probabilístico que asigna un puntaje de documento de valor real f(D, Q) a cada documento D en C, de manera que el puntaje sea proporcional a la probabilidad estimada de relevancia. No hacemos ninguna otra suposición sobre f(D, Q). La naturaleza de f(D, Q) puede ser compleja: por ejemplo, si el sistema de recuperación admite lenguajes de consulta estructurados [12], entonces f(D, Q) puede representar la salida de una red de inferencia arbitrariamente compleja definida por los operadores de consulta estructurados. En teoría, la función de puntuación puede variar de una consulta a otra, aunque en este estudio por simplicidad mantenemos la misma función de puntuación para todas las consultas. Nuestro método de consulta específico se encuentra en la Sección 3. Tratamos el algoritmo de retroalimentación como una caja negra y asumimos que los insumos del algoritmo de retroalimentación son la consulta original y los documentos recuperados principales correspondientes, a los cuales se les asigna una puntuación a cada documento. Suponemos que la salida del algoritmo de retroalimentación es un vector de pesos de términos que se utilizarán para agregar o reponderar los términos en la representación de la consulta original, con el vector normalizado para formar una <br>distribución de probabilidad</br>. Consideramos las entradas a la caja negra de retroalimentación como variables aleatorias, y analizamos el modelo de retroalimentación como una variable aleatoria que cambia en respuesta a cambios en las entradas. Al igual que la función de puntuación del documento f(D, Q), el algoritmo de retroalimentación puede implementar una fórmula de puntuación compleja y no lineal, por lo que, a medida que sus entradas varían, los modelos de retroalimentación resultantes pueden tener una distribución compleja en el espacio de modelos de retroalimentación (el espacio muestral). Debido a esta complejidad potencial, no intentamos derivar una distribución posterior de forma cerrada, sino que en su lugar utilizamos simulación. Llamamos a esta distribución sobre posibles modelos de retroalimentación la distribución de modelos de retroalimentación. Nuestro objetivo en esta sección es estimar una aproximación útil a la distribución del modelo de retroalimentación. Para un marco específico de experimentos, utilizamos el enfoque de modelado de lenguaje (LM) para la recuperación de información [15]. El puntaje de un documento D con respecto a una consulta Q y una colección C se da por p(Q|D) con respecto a los modelos de lenguaje ˆθQ y ˆθD estimados para la consulta y el documento respectivamente. Denotamos el conjunto de los k documentos mejor recuperados de la colección C en respuesta a Q como DQ(k, C). Para simplificar, asumimos que las consultas y documentos son generados por distribuciones multinomiales cuyos parámetros están representados por modelos de lenguaje unigram. Para incorporar retroalimentación en el enfoque de LM, asumimos un esquema basado en modelos en el que nuestro objetivo es tomar la consulta y los documentos clasificados resultantes DQ(k, C) como entrada, y producir un modelo de lenguaje expandido ˆθE, que luego se interpola con el modelo de consulta original ˆθQ: ˆθNew = (1 − α) · ˆθQ + α · ˆθE (1). Esto incluye la posibilidad de α = 1 donde el modelo de consulta original es completamente reemplazado por el modelo de retroalimentación. Nuestro espacio muestral es el conjunto de todos los posibles modelos de lenguaje LF que pueden ser generados como modelos de retroalimentación. Nuestro enfoque es tomar muestras de este espacio y luego ajustar una distribución a las muestras utilizando máxima verosimilitud. Para simplificar, comenzamos asumiendo que la distribución de retroalimentación latente tiene la forma de una distribución de Dirichlet. Aunque la distribución de Dirichlet es unimodal y en general bastante limitada en su expresividad en el espacio muestral, es una coincidencia natural para el modelo de lenguaje multinomial, puede ser estimada rápidamente y capturar las características más relevantes de los modelos de retroalimentación confiada e incierta, como la dispersión general de la distribución. 2.2 Re-muestreo de modelos de documentos. Nos gustaría una aproximación a la distribución posterior del modelo de retroalimentación LF. Para lograr esto, aplicamos una técnica de simulación ampliamente utilizada llamada muestreo bootstrap ([7], p. 474) en los parámetros de entrada, es decir, el conjunto de documentos recuperados en la parte superior. El muestreo de arranque nos permite simular el efecto aproximado de perturbar los parámetros dentro del algoritmo de retroalimentación de la caja negra al perturbar los insumos de ese algoritmo de manera sistemática, sin hacer suposiciones sobre la naturaleza del algoritmo de retroalimentación. Específicamente, muestreamos k documentos con reemplazo de DQ(k, C), y calculamos un modelo de lenguaje de expansión θb utilizando el método de retroalimentación de caja negra. Repetimos este proceso B veces para obtener un conjunto de B modelos de lenguaje de retroalimentación, a los cuales luego ajustamos una distribución de Dirichlet. Normalmente, B se encuentra en el rango de 20 a 50 muestras, con un rendimiento relativamente estable en este rango. Ten en cuenta que en lugar de tratar cada documento superior como igualmente probable, muestreamos de acuerdo con las probabilidades estimadas de relevancia de cada documento en DQ(k, C). Por lo tanto, es más probable que un documento sea elegido cuanto más alto esté en la clasificación. 2.3 Justificación de un enfoque de muestreo La justificación de nuestro enfoque de muestreo tiene dos partes. Primero, queremos mejorar la calidad de los modelos de retroalimentación individual al suavizar la variación cuando el modelo de retroalimentación base es inestable. En este sentido, nuestro enfoque se asemeja al bagging [4], un enfoque de conjunto que genera múltiples versiones de un predictor haciendo copias de arranque del conjunto de entrenamiento, y luego promedia los predictores (numéricos). En nuestra aplicación, los documentos recuperados en la parte superior pueden ser vistos como una especie de conjunto de entrenamiento ruidoso para la relevancia. Segundo, el muestreo es una forma efectiva de estimar propiedades básicas de la distribución posterior de retroalimentación, las cuales pueden ser utilizadas para mejorar la combinación de modelos. Por ejemplo, un modelo puede ser ponderado por su confianza de predicción, estimada como una función de la variabilidad del posterior alrededor del modelo. foo2-401.map-Dim:5434,Size:12*12units, vecindario gaussiano (a) Tema 401 Minorías extranjeras, Alemania foo2-402.map-Dim:5698,Size:12*12units, vecindario gaussiano (b) Tema 402 Genética del comportamiento foo2-459.map-Dim:8969,Size:12*12units, vecindario gaussiano (c) Tema 459 ¿Cuándo puede un prestamista ejecutar una hipoteca sobre una propiedad? Figura 2: Visualización de la varianza del modelo de lenguaje de expansión utilizando mapas autoorganizados, mostrando la distribución de modelos de lenguaje que resulta de remuestrear las entradas al método de expansión de referencia. El modelo de lenguaje que habría sido elegido por la expansión de la línea base se encuentra en el centro de cada mapa. La función de similitud es la divergencia JensenShannon. 2.4 Visualización de distribuciones de retroalimentación Antes de describir cómo ajustamos y utilizamos la distribución de Dirichlet sobre modelos de retroalimentación, es instructivo ver algunos ejemplos de distribuciones de modelos de retroalimentación reales que resultan de muestrear por bootstrap los documentos más recuperados de diferentes temas de TREC. Cada punto en nuestro espacio muestral es un modelo de lenguaje, que típicamente tiene varias miles de dimensiones. Para ayudar a analizar el comportamiento de nuestro método, utilizamos un Mapa Auto-organizado (a través del paquete SOM-PAK [9]), para aplanar y visualizar la función de densidad de alta dimensión. Los mapas de densidad para tres temas de TREC se muestran en la Figura 2 arriba. Las áreas oscuras representan regiones de alta similitud entre los modelos de lenguaje. Las áreas claras representan regiones de baja similitud, los valles entre los grupos. Cada diagrama está centrado en el modelo de lenguaje que habría sido elegido por la expansión de referencia. Un solo pico (modo) es evidente en algunos ejemplos, pero en otros aparece una estructura más compleja. Además, aunque la distribución suele estar cerca del modelo de retroalimentación de referencia, para algunos temas están a una distancia significativa (según la divergencia de JensenShannon), como se muestra en la Subfigura 2c. En tales casos, la moda o la media de la distribución de retroalimentación a menudo funcionan significativamente mejor que el valor base (y en una proporción menor de casos, significativamente peor). 2.5 Ajuste de una distribución de retroalimentación posterior Después de obtener muestras del modelo de retroalimentación mediante el remuestreo de las entradas del modelo de retroalimentación, estimamos la distribución de retroalimentación. Suponemos que los modelos de retroalimentación multinomial {ˆθ1, . . . , ˆθB} fueron generados por una distribución de Dirichlet latente con parámetros {α1, . . . , αN}. Para estimar los {α1, . . . , αN }, ajustamos los parámetros de Dirichlet a las muestras del modelo de lenguaje B según la máxima verosimilitud utilizando un procedimiento de Newton generalizado, cuyos detalles se encuentran en Minka [13]. Suponemos una distribución de Dirichlet simple sobre los {α1, . . . , αN}, estableciendo cada uno como αi = μ · p(wi | C), donde μ es un parámetro y p(· | C) es el modelo de lenguaje de colección estimado a partir de un conjunto de documentos de la colección C. El ajuste de parámetros converge muy rápidamente, generalmente en solo 2 o 3 iteraciones. Dado que nuestros puntos son modelos de lenguaje en el simplex multinomial, extendimos SOM-PAK para admitir la divergencia de Jensen-Shannon, una medida de similitud ampliamente utilizada entre distribuciones de probabilidad. 3 iteraciones son suficientes, por lo que es práctico aplicarlo en tiempo de consulta cuando la sobrecarga computacional debe ser pequeña. En la práctica, podemos restringir el cálculo al vocabulario de los documentos recuperados en la parte superior, en lugar de toda la colección. Ten en cuenta que para este paso estamos reutilizando los documentos recuperados existentes y no realizando consultas adicionales. Dado los parámetros de una distribución de Dirichlet N-dimensional Dir(α), los vectores de media μ y moda x son fáciles de calcular y se dan respectivamente por μi = αiP αi (2) y xi = αi−1P αi−N . (3) Luego podemos elegir el modelo de lenguaje en la media o en la moda del posterior como el modelo de retroalimentación mejorado final. (Encontramos que la moda proporciona un rendimiento ligeramente mejor). Para la recuperación de información, es probable que el número de muestras disponibles sea bastante pequeño por razones de rendimiento, generalmente menos de diez. Además, si bien el muestreo aleatorio es útil en ciertos casos, es perfectamente aceptable permitir distribuciones de muestreo determinísticas, pero estas deben diseñarse cuidadosamente para aproximar una varianza de salida precisa. Dejamos esto para estudios futuros. 2.6 Variantes de consulta Utilizamos los siguientes métodos para generar variantes de la consulta original. Cada variante corresponde a una suposición diferente sobre qué aspectos de la consulta original pueden ser importantes. Esta es una forma de muestreo determinístico. Seleccionamos tres métodos simples que cubren suposiciones complementarias sobre la consulta. Uso sin expansión. La suposición es que los términos dados son una descripción completa de la necesidad de información. Deja uno fuera. Se deja fuera un término del query original. La suposición es que uno de los términos de la consulta es un término de ruido. Se elige un término único de la consulta original. Esto asume que solo un aspecto de la consulta, es decir, el representado por el término, es el más importante. Después de generar una variante de la consulta original, la combinamos con la consulta original utilizando un peso αSUB para no alejarnos demasiado. En este estudio, establecimos αSUB = 0.5. Por ejemplo, utilizando el lenguaje de consulta Indri [12], una variante de dejar uno fuera de la consulta inicial que omite el término \"ireland\" para el tema TREC 404 es: #weight(0.5 #combine(ireland peace talks) 0.5 #combine(peace talks)) 2.7 Combinando modelos de retroalimentación mejorados de múltiples variantes de consulta. Cuando se utilizan múltiples variantes de consulta, los modelos de retroalimentación mejorados resultantes se combinan utilizando la combinación de modelos bayesianos. Para hacer esto, tratamos cada palabra como un elemento a clasificar como perteneciente a una clase relevante o no relevante, y derivamos una probabilidad de clase para cada palabra combinando las puntuaciones de cada variante de consulta. Cada puntuación es dada por la probabilidad de ese término en la distribución de Dirichlet. Las puntuaciones de los términos se ponderan por el inverso de la varianza del término en los modelos de retroalimentación mejorada de la distribución de Dirichlet. La probabilidad previa de pertenencia de una palabra a la clase relevante se da por la probabilidad de la consulta original en todo el modelo de expansión mejorado. 3. EVALUACIÓN En esta sección presentamos resultados que confirman la utilidad de estimar una distribución del modelo de retroalimentación a partir del remuestreo ponderado de documentos mejor clasificados, y de combinar los modelos de retroalimentación obtenidos a partir de diferentes pequeños cambios en la consulta original. 3.1 Método general Evaluamos el rendimiento en un total de 350 consultas derivadas de cuatro conjuntos de temas de TREC: 51-200 (TREC-1&2), 351-400 (TREC-7), 401-450 (TREC-8) y 451-550 (wt10g, TREC-9&10). Elegimos estos por su contenido variado y propiedades del documento. Por ejemplo, los documentos wt10g son páginas web con una amplia variedad de temas y estilos, mientras que los documentos TREC-1&2 son artículos de noticias más homogéneos. La indexación y recuperación se realizó utilizando el sistema Indri en el kit de herramientas Lemur [12] [1]. Nuestras consultas se derivaron de las palabras en el campo del título de los temas de TREC. Las frases no fueron utilizadas. Para generar las consultas base que se pasaron a Indri, envolvimos los términos de la consulta con el operador #combine de Indri. Por ejemplo, la consulta inicial para el tema 404 es: #combine(ireland peace talks) Realizamos el stemming de Krovetz para todos los experimentos. Debido a que encontramos que el método de expansión de la base (Indri) funcionó mejor utilizando una lista de palabras vacías con el modelo de retroalimentación, todos los experimentos utilizaron una lista de 419 palabras comunes en inglés. Sin embargo, un efecto secundario interesante de nuestro enfoque de remuestreo es que tiende a eliminar muchas palabras vacías del modelo de retroalimentación, lo que hace que una lista de paradas sea menos crítica. Esto se discute más a fondo en la Sección 3.6. Método de retroalimentación de línea base 3.2 Para nuestro método de expansión de línea base, utilizamos un algoritmo incluido en Indri 1.0 como el método de expansión predeterminado. Este método primero selecciona términos utilizando un cálculo de logaritmo de probabilidades descrito por Ponte [14], pero asigna pesos finales a los términos utilizando el modelo de relevancia de Lavrenko [10]. Elegimos el método Indri porque proporciona una línea base consistentemente sólida, se basa en un enfoque de modelado del lenguaje y es fácil de experimentar con él. En una evaluación de TREC utilizando el corpus GOV2, el método fue uno de los mejores resultados, logrando un aumento del 19.8% en el MAP en comparación con el uso de consultas no expandidas. En este estudio, se logra una ganancia promedio en MAP del 17.25% en las cuatro colecciones. El método de expansión de Indri primero calcula una razón de logaritmos de probabilidades o(v) para cada término de expansión potencial v dado por o(v) = X D log p(v|D) p(v|C) (4) sobre todos los documentos D que contienen v, en la colección C. Luego, los candidatos a términos de expansión se ordenan por o(v) descendente, y se eligen los primeros m. Finalmente, los pesos de término r(v) utilizados en la consulta ampliada se calculan en base al modelo de relevancia r(v) = X D p(q|D)p(v|D) p(v) p(D) (5). La cantidad p(q|D) es la puntuación de probabilidad asignada al documento en la recuperación inicial. Utilizamos suavizado de Dirichlet de p(v|D) con μ = 1000. Este modelo de relevancia se combina luego con la consulta original utilizando interpolación lineal, ponderada por un parámetro α. Por defecto, utilizamos los 50 documentos principales para la retroalimentación y los 20 términos de expansión, con el parámetro de interpolación de retroalimentación α = 0.5 a menos que se indique lo contrario. Por ejemplo, la consulta expandida base para el tema 404 es: #weight(0.5 #combine(ireland peace talks) 0.5 #weight(0.10 ireland 0.08 peace 0.08 northern ...) 3.3 Rendimiento de la expansión. Medimos la efectividad de nuestros algoritmos de retroalimentación mediante dos criterios principales: precisión y robustez. La robustez, y el equilibrio entre precisión y robustez, se analiza en la Sección 3.4. En esta sección, examinamos la precisión promedio y la precisión en los 10 documentos principales (P10). También incluimos la recuperación de 1,000 documentos. Para cada consulta, obtuvimos un conjunto de B modelos de retroalimentación utilizando la línea base de Indri. Cada modelo de retroalimentación se obtuvo a partir de una muestra aleatoria de los k documentos principales tomados con reemplazo. Para estos experimentos, B = 30 y k = 50. Cada modelo de retroalimentación contenía 20 términos. En el lado de la consulta, utilizamos muestreo de dejar uno fuera (LOO) para crear las variantes de consulta. El muestreo de consulta de un solo término tuvo un rendimiento consistentemente peor en todas las colecciones, por lo que nuestros resultados se centran en el muestreo de LOO aquí. Utilizamos los métodos descritos en la Sección 2 para estimar un modelo de retroalimentación mejorado a partir de la distribución posterior de Dirichlet para cada variante de consulta, y para combinar los modelos de retroalimentación de todas las variantes de consulta. Llamamos a nuestro método expansión de remuestreo y lo denotamos como RS-FB aquí. Denominamos al método de retroalimentación de línea base Indri como Base-FB. Los resultados de aplicar tanto el método de expansión de línea base (Base-FB) como la expansión de remuestreo (RS-FB) se muestran en la Tabla 1. Observamos varias tendencias en esta tabla. Primero, la precisión promedio de RS-FB fue comparable a Base-FB, logrando una ganancia promedio del 17.6% en comparación con no utilizar expansión en las cuatro colecciones. La ganancia de expansión de la línea base de Indri fue del 17.25%. Además, el método RS-FB logró mejoras consistentes en P10 sobre Base-FB para cada conjunto de temas, con una mejora promedio del 6.89% sobre Base-FB para los 350 temas. La ganancia P10 más baja sobre Base-FB fue del +3.82% para TREC-7 y la más alta fue del +11.95% para wt10g. Finalmente, tanto Base-FB como RS-FB también mejoraron consistentemente la recuperación en comparación con no utilizar expansión, siendo Base-FB el que logró una mejor recuperación que RS-FB para todos los conjuntos de temas. 3.4 Robustez de recuperación. Utilizamos el término robustez para referirnos al rendimiento de precisión promedio en el peor de los casos de un algoritmo de retroalimentación. Idealmente, un método de retroalimentación sólido nunca debería funcionar peor que usar la consulta original, mientras que a menudo funciona mejor utilizando la expansión. Para evaluar la robustez en este estudio, utilizamos una medida muy simple llamada índice de robustez (RI). Para un conjunto de consultas Q, la medida de RI se define como: RI(Q) = n+ − n− |Q| (6) donde n+ es el número de consultas ayudadas por el método de retroalimentación y n− es el número de consultas perjudicadas. Aquí, por \"ayudar\" nos referimos a obtener una precisión promedio más alta como resultado de la retroalimentación. El valor de RI varía desde un mínimo de 3. A veces también se le llama índice de confiabilidad de mejora y fue utilizado en Sakai et al. [17]. Colección NoExp Base-FB RS-FB TREC 1&2 AvgP 0.1818 0.2419 (+33.04%) 0.2406 (+32.24%) P10 0.4443 0.4913 (+10.57%) 0.5363 (+17.83%) Recall 15084/37393 19172/37393 15396/37393 TREC 7 AvgP 0.1890 0.2175 (+15.07%) 0.2169 (+14.75%) P10 0.4200 0.4320 (+2.85%) 0.4480 (+6.67%) Recall 2179/4674 2608/4674 2487/4674 TREC 8 AvgP 0.2031 0.2361 (+16.25%) 0.2268 (+11.70%) P10 0.3960 0.4160 (+5.05%) 0.4340 (+9.59%) Recall 2144/4728 2642/4728 2485/4728 wt10g AvgP 0.1741 0.1829 (+5.06%) 0.1946 (+11.78%) P10 0.2760 0.2630 (-4.71%) 0.2960 (+7.24%) Recall 3361/5980 3725/5980 3664/5980 Tabla 1: Comparación de la retroalimentación base (Base-FB) y la retroalimentación utilizando re-muestreo (RS-FB). La mejora mostrada para BaseFB y RS-FB es relativa al uso de ninguna expansión. (a) TREC 1&2 (curva superior); TREC 8 (curva inferior) (b) TREC 7 (curva superior); wt10g (curva inferior) Figura 3: La compensación entre robustez y precisión promedio para diferentes corpora. El eje x muestra el cambio en la MAP al utilizar la expansión de la línea base con α = 0.5. El eje y muestra el Índice de Robustez (RI). Cada curva a través de los puntos no circulados muestra el compromiso entre RI/MAP utilizando la estrategia simple de pequeño α (ver texto) a medida que α disminuye de 0.5 a cero en la dirección de la flecha. Los puntos circulados representan los compromisos obtenidos al volver a muestrear la retroalimentación para α = 0.5. Colección N Base-FB RS-FB n− RI n− RI TREC 1&2 103 26 +0.495 15 +0.709 TREC 7 46 14 +0.391 10 +0.565 TREC 8 44 12 +0.455 12 +0.455 wt10g 91 48 -0.055 39 +0.143 Combinado 284 100 +0.296 76 +0.465 Tabla 2: Comparación del índice de robustez (RI) para retroalimentación de línea base (Base-FB) vs. retroalimentación de remuestreo (RS-FB). También se muestran el número real de consultas perjudicadas por la retroalimentación (n−) para cada método y colección. Las consultas para las cuales la precisión promedio inicial era insignificante (≤ 0.01) fueron ignoradas, dando como resultado el recuento de consultas restantes en la columna N, de -1.0, cuando todas las consultas son perjudicadas por el método de retroalimentación, a +1.0 cuando todas las consultas son ayudadas. La medida de RI no tiene en cuenta la magnitud o distribución de la cantidad de cambio en el conjunto Q. Sin embargo, es fácil de entender como una indicación general de robustez. Una forma obvia de mejorar el rendimiento en el peor de los casos de la retroalimentación es simplemente utilizar un parámetro de interpolación α fijo más pequeño, como α = 0.3, otorgando menos peso al modelo de retroalimentación (posiblemente arriesgado) y más al cuestionario original. Llamamos a esto la estrategia de pequeña α. Dado que también estamos reduciendo las ganancias potenciales cuando el modelo de retroalimentación es correcto, sin embargo, esperaríamos algún tipo de compensación entre la precisión promedio y la robustez. Por lo tanto, comparamos el equilibrio entre precisión y robustez entre nuestro algoritmo de retroalimentación de remuestreo y el método simple de pequeño α. Los resultados se resumen en la Figura 3. En la figura, la curva para cada conjunto de temas interpola entre los puntos de compensación, comenzando en x=0, donde α = 0.5, y continuando en la dirección de la flecha a medida que α disminuye y la consulta original recibe cada vez más peso. Como era de esperar, la robustez aumenta continuamente a medida que avanzamos a lo largo de la curva, pero la precisión media promedio generalmente disminuye a medida que se eliminan las ganancias del feedback. Para la comparación, se muestra el rendimiento del re-muestreo con retroalimentación en α = 0.5 para cada colección como el punto marcado con un círculo. Más alto y a la derecha es mejor. Esta figura muestra que el re-muestreo de retroalimentación ofrece un compromiso algo mejor que el enfoque de pequeño α para 3 de las 4 colecciones. Figura 4: Histograma que muestra la mayor robustez del re-muestreo de retroalimentación (RS-FB) sobre la retroalimentación base (Base-FB) para todos los conjuntos de datos combinados. Las consultas se agrupan por el % de cambio en AP en comparación con la consulta no expandida. Colección DS + QV DS + No QV TREC 1&2 AvgP 0.2406 0.2547 (+5.86%) P10 0.5263 0.5362 (+1.88%) RI 0.7087 0.6515 (-0.0572) TREC 7 AvgP 0.2169 0.2200 (+1.43%) P10 0.4480 0.4300 (-4.02%) RI 0.5652 0.2609 (-0.3043) TREC 8 AvgP 0.2268 0.2257 (-0.49%) P10 0.4340 0.4200 (-3.23%) RI 0.4545 0.4091 (-0.0454) wt10g AvgP 0.1946 0.1865 (-4.16%) P10 0.2960 0.2680 (-9.46%) RI 0.1429 0.0220 (-0.1209) Tabla 3: Comparación de la retroalimentación de remuestreo utilizando muestreo de documentos (DS) con (QV) y sin (No QV) combinar modelos de retroalimentación de múltiples variantes de consulta. La Tabla 2 muestra los puntajes del Índice de Robustez para Base-FB y RS-FB. El método de retroalimentación RS-FB obtuvo una mayor robustez que Base-FB en tres de los cuatro conjuntos de temas, con solo un rendimiento ligeramente peor en TREC-8. Una vista más detallada que muestra la distribución de los cambios relativos en AP se presenta en el histograma de la Figura 4. En comparación con Base-FB, el método RS-FB logra una reducción notable en el número de consultas significativamente afectadas por la expansión (es decir, donde AP se ve afectado en un 25% o más), al tiempo que conserva ganancias positivas en AP. 3.5 Efecto de los métodos de muestreo de consultas y documentos Dada la mayor robustez de nuestros algoritmos observada en la Sección 3.4, una pregunta importante es qué componente de nuestro sistema es responsable. ¿Es el uso de la re-muestreo de documentos, el uso de múltiples variantes de consulta, o algún otro factor? Los resultados en la Tabla 3 sugieren que la combinación de modelos basada en variantes de consulta puede ser en gran medida responsable de la mayor robustez. Cuando se desactivan las variantes de consulta y se utiliza la consulta original por sí sola con muestreo de documentos, hay poco cambio neto en la precisión promedio, una pequeña disminución en P10 para 3 de los 4 conjuntos de temas, pero una caída significativa en la robustez para todos los conjuntos de temas. En dos casos, la medida de RI disminuye en más del 50%. También examinamos el efecto del método de muestreo de documentos en la efectividad de la recuperación, utilizando dos estrategias diferentes. La estrategia de ponderación uniforme ignoró las puntuaciones de relevancia de la recuperación inicial y otorgó a cada documento en los primeros k la misma probabilidad de selección. Por el contrario, la estrategia de ponderación de puntajes de relevancia seleccionó documentos con una probabilidad proporcional a sus puntajes de relevancia. De esta manera, los documentos que tenían una clasificación más alta tenían más probabilidades de ser seleccionados. Los resultados se muestran en la Tabla 4. La estrategia de ponderación de la puntuación de relevancia tiene un mejor rendimiento en general, con puntajes de RI y P10 significativamente más altos en 3 de los 4 conjuntos de temas. La diferencia en la precisión promedio entre los métodos, sin embargo, es menos marcada. Esto sugiere que el peso uniforme actúa para aumentar la varianza en los resultados de recuperación: cuando la precisión promedio inicial es alta, hay muchos documentos relevantes en los primeros k y el muestreo uniforme puede proporcionar un modelo de relevancia más representativo que centrarse en los elementos altamente clasificados. Por otro lado, cuando la precisión inicial es baja, hay pocos documentos relevantes en las posiciones inferiores y el muestreo uniforme mezcla más de los documentos no relevantes. Por razones de espacio, solo resumimos nuestros hallazgos sobre el tamaño de la muestra aquí. El número de muestras tiene cierto efecto en la precisión cuando es menor a 10, pero el rendimiento se estabiliza alrededor de 15 a 20 muestras. Utilizamos 30 muestras para nuestros experimentos. Mucho más allá de este nivel, los beneficios adicionales de más muestras disminuyen a medida que la distribución inicial de puntajes se ajusta más estrechamente y aumenta el tiempo de procesamiento. 3.6 El efecto del remuestreo en la calidad del término de expansión Idealmente, un modelo de recuperación no debería requerir una lista de palabras vacías al estimar un modelo de relevancia: un modelo estadístico robusto debería reducir automáticamente el peso de las palabras vacías dependiendo del contexto. Las palabras vacías pueden perjudicar la retroalimentación si se seleccionan como términos de retroalimentación, ya que suelen ser malos discriminadores y desperdician espacios de términos valiosos. En la práctica, sin embargo, debido a que la mayoría de los métodos de selección de términos se asemejan a un tipo de ponderación tf · idf, a veces se pueden seleccionar términos con un idf bajo pero un tf muy alto como candidatos de términos de expansión. Esto sucede, por ejemplo, incluso con el enfoque del Modelo de Relevancia que forma parte de nuestra retroalimentación base. Para garantizar una línea base lo más sólida posible, utilizamos una lista de paradas para todos los experimentos reportados aquí. Si desactivamos la lista de palabras vacías, sin embargo, obtenemos resultados como los mostrados en la Tabla 5 donde cuatro de los diez términos de retroalimentación básica principales para el tema 60 de TREC (dijo, pero, su, no) son palabras vacías utilizando el método BaseFB. (Se seleccionaron los 100 términos de expansión principales para generar este ejemplo). El método Indris intenta abordar el problema de las palabras vacías aplicando un paso inicial basado en Ponte [14] para seleccionar términos menos comunes que tienen altas probabilidades logarítmicas de estar en los documentos mejor clasificados en comparación con toda la colección. Sin embargo, esto no resuelve completamente el problema de las palabras vacías, especialmente a medida que aumenta el número de términos de retroalimentación. Sin embargo, el uso de retroalimentación de remuestreo parece mitigar la ponderación de la colección QV + QV uniforme + ponderación de puntaje de relevancia TREC 1&2 AvgP 0.2545 0.2406 (-5.46%) P10 0.5369 0.5263 (-1.97%) RI 0.6212 0.7087 (+14.09%) TREC 7 AvgP 0.2174 0.2169 (-0.23%) P10 0.4320 0.4480 (+3.70%) RI 0.4783 0.5652 (+18.17%) TREC 8 AvgP 0.2267 0.2268 (+0.04%) P10 0.4120 0.4340 (+5.34%) RI 0.4545 0.4545 (+0.00%) wt10g AvgP 0.1808 0.1946 (+7.63%) P10 0.2680 0.2960 (+10.45%) RI 0.0220 0.1099 (+399.5%) Tabla 4: Comparación de muestreo de documentos uniforme y ponderado por relevancia. El cambio porcentual en comparación con el muestreo uniforme se muestra entre paréntesis. QV indica que se utilizaron variantes de consulta en ambas ejecuciones. FB de línea base p(wi|R) FB de remuestreo p(wi|R) dijo 0.055 corte 0.026 corte 0.055 pagar 0.018 pagar 0.034 federal 0.012 pero 0.026 educación 0.011 empleados 0.024 maestros 0.010 sus 0.024 empleados 0.010 no 0.023 caso 0.010 federal 0.021 sus 0.009 trabajadores 0.020 apelaciones 0.008 educación 0.020 sindicato 0.007 Tabla 5: Calidad del término de retroalimentación cuando no se utiliza una lista de paradas. Términos de retroalimentación para el tema 60 de TREC: pago por mérito vs antigüedad. el efecto de las palabras vacías automáticamente. En el ejemplo de la Tabla 5, la retroalimentación de remuestreo deja solo una palabra vacía (su) en las diez primeras posiciones. Observamos un comportamiento similar del término de retroalimentación en muchos otros temas. La razón de este efecto parece ser la interacción de la puntuación de selección de términos con el límite superior de términos m. Si bien la presencia e incluso la proporción de palabras vacías particulares es bastante estable en diferentes muestras de documentos, su posición relativa en la lista de los primeros m elementos no lo es, ya que se examinan conjuntos de documentos con diferentes números de candidatos de términos mejores y de menor frecuencia para cada muestra. Como resultado, si bien algunos stopwords pueden aparecer en cada conjunto de documentos muestreados, tiende a ocurrir que cualquier stopword dado caiga por debajo del umbral para múltiples muestras, lo que lleva a su clasificación como una característica de alta varianza y bajo peso. 4. TRABAJO RELACIONADO Nuestro enfoque está relacionado con trabajos previos de varias áreas de recuperación de información y aprendizaje automático. Nuestro uso de la variación de consultas fue inspirado por el trabajo de YomTov et al. [20], Carpineto et al. [5] y Amati et al. [2], entre otros. Estos estudios utilizan la idea de crear múltiples subconsultas y luego examinar la naturaleza de la superposición en los documentos y/o términos de expansión que resultan de cada subconsulta. La combinación de modelos se realiza utilizando heurísticas. En particular, los estudios de Amati et al. y Carpineto et al. investigaron la combinación de términos de métodos de distribución individuales utilizando una heurística de combinación de reordenamiento de términos. En un conjunto de temas de TREC encontraron una amplia variación promedio en la distancia de rango de los términos de diferentes métodos de expansión. Su método de combinación proporcionó modestas mejoras positivas en la precisión promedio. La idea de examinar la superposición entre listas de términos sugeridos también ha sido utilizada en enfoques tempranos de expansión de consultas. El método de Análisis de Contexto Local (LCA) de Xu y Crofts [19] incluye un factor en la fórmula de ponderación derivada empíricamente que hace que se prefieran los términos de expansión que tienen conexiones con múltiples términos de consulta. En el lado del documento, el trabajo reciente de Zhou & Croft [21] exploró la idea de agregar ruido a los documentos, volver a puntuarlos y utilizar la estabilidad de las clasificaciones resultantes como una estimación de la dificultad de la consulta. Esto está relacionado con nuestro uso de muestreo de documentos para estimar el riesgo del modelo de retroalimentación construido a partir de los diferentes conjuntos de documentos recuperados en la parte superior. Sakai et al. [17] propusieron un enfoque para mejorar la robustez de la retroalimentación de relevancia pseudo utilizando un método que llaman muestreo selectivo. La esencia de su método es que permiten omitir algunos documentos de alta clasificación, basándose en un criterio de agrupación, para seleccionar un conjunto de documentos más variado e innovador más adelante en la clasificación para su uso por un método tradicional de retroalimentación pseudo. Su estudio no encontró mejoras significativas ni en robustez (RI) ni en MAP en sus corpora. Greiff, Morgan y Ponte [8] exploraron el papel de la varianza en la ponderación de términos. En una serie de simulaciones que simplificaron el problema a documentos de 2 características, encontraron que la precisión promedio disminuye a medida que la varianza de la frecuencia de términos - ruido alto - aumenta. La reducción del peso de los términos con alta varianza resultó en una mejora de la precisión promedio. Esto parece estar en concordancia con nuestros propios hallazgos para los modelos de retroalimentación individual. Las estimaciones de la varianza de la producción han sido utilizadas recientemente para mejorar la clasificación de textos. Lee et al. [11] utilizaron estimaciones de varianza específicas de la consulta de las salidas del clasificador para realizar una combinación de modelos mejorada. En lugar de utilizar muestreo, pudieron derivar expresiones en forma cerrada para la varianza del clasificador asumiendo clasificadores base utilizando tipos simples de redes de inferencia. Ando y Zhang propusieron un método que ellos llaman retroalimentación estructural [3] y mostraron cómo aplicarlo a la expansión de consultas para la pista de Genómica de TREC. Utilizaron variaciones de consultas r para obtener R conjuntos diferentes Sr de documentos mejor clasificados que se han intersectado con los documentos mejor clasificados obtenidos de la consulta original qorig. Para cada Si, se calcula el vector centróide normalizado ˆwi de los documentos. El análisis de componentes principales (PCA) se aplica luego a los ˆwi para obtener la matriz Φ de los vectores singulares izquierdos φh que se utilizan para obtener la nueva consulta expandida qexp = qorig + ΦT Φqorig. (7) En el caso de H = 1, tenemos un único vector singular izquierdo φ: qexp = qorig + (φT qorig)φ, de modo que el producto punto φT qorig es un tipo de peso dinámico en la consulta expandida que se basa en la similitud de la consulta original con la consulta expandida. El uso de la varianza como medida de calidad del modelo de retroalimentación ocurre de forma indirecta a través de la aplicación de PCA. Sería interesante estudiar las conexiones entre este enfoque y nuestro propio método de ajuste de modelos. Finalmente, en los enfoques de modelado del lenguaje para retroalimentación, Tao y Zhai [18] describen un método para una retroalimentación más robusta que permite que cada documento tenga un α de retroalimentación diferente. Los pesos de retroalimentación se derivan automáticamente utilizando EM regularizado. La condición de parada de EM implica un equilibrio aproximadamente igual entre el modelo de consulta y expansión. Proponen adaptar el parámetro de parada η basado en una función de alguna medida de calidad de los documentos de retroalimentación. CONCLUSIONES Hemos presentado un nuevo enfoque para la retroalimentación de relevancia pseudo basado en el muestreo de documentos y consultas. El uso del muestreo es un dispositivo muy flexible y poderoso, motivado por nuestro deseo general de ampliar los modelos actuales de recuperación mediante la estimación del riesgo o la varianza asociada con los parámetros o la salida de los procesos de recuperación. Tales estimaciones de varianza, por ejemplo, pueden ser utilizadas de forma natural en un marco bayesiano para una mejor estimación y combinación de modelos. Aplicaciones como la expansión selectiva pueden ser implementadas de manera fundamentada. Si bien nuestro estudio utiliza el enfoque de modelado del lenguaje como marco para experimentos, hacemos pocas suposiciones sobre el funcionamiento real del algoritmo de retroalimentación. Creemos que es probable que cualquier algoritmo de retroalimentación de línea base razonablemente efectivo se beneficiaría de nuestro enfoque. Nuestros resultados en colecciones estándar de TREC muestran que nuestro marco mejora la robustez de un método de retroalimentación de referencia sólido en una variedad de colecciones, sin sacrificar la precisión promedio. También proporciona pequeñas pero consistentes mejoras en la precisión del top 10. En trabajos futuros, visualizamos una investigación sobre cómo variar el conjunto de métodos de muestreo utilizados y el número de muestras controla el equilibrio entre robustez, precisión y eficiencia. Agradecimientos Agradecemos a Paul Bennett por las valiosas discusiones relacionadas con este trabajo, el cual fue apoyado por las becas de la NSF #IIS-0534345 y #CNS-0454018, y la beca del Departamento de Educación de los EE. UU. #R305G03123. Cualquier opinión, hallazgo, conclusión o recomendación expresada en este material son del autor y no necesariamente reflejan las de los patrocinadores. REFERENCIAS [1] El kit de herramientas Lemur para modelado de lenguaje y recuperación. http://www.lemurproject.org. [2] G. Amati, C. Carpineto y G. Romano. Dificultad de la consulta, robustez y aplicación selectiva de la expansión de consultas. En Actas de la 25ª Conferencia Europea sobre Recuperación de Información (ECIR 2004), páginas 127-137. [3] R. K. Ando y T. Zhang. Un método de aprendizaje semisupervisado de alto rendimiento para segmentación de texto. En Actas de la 43ª Reunión Anual de la ACL, páginas 1-9, junio de 2005. [4] L. Breiman. Empaquetando predictores. Aprendizaje automático, 24(2):123-140, 1996. [5] C. Carpineto, G. Romano y V. Giannini. Mejorando la retroalimentación de recuperación con la combinación de múltiples funciones de clasificación de términos. ACM Trans. Información. Sistemas, 20(3):259 - 290. [6] K. Collins-Thompson, P. Ogilvie y J. Callan. Resultados iniciales con consultas estructuradas y modelos de lenguaje en medio terabyte de texto. En Actas de la Conferencia de Recuperación de Información de Texto de 2005. Publicación Especial del NIST. [7] R. O. Duda, P. E. Hart y D. G. Stork. Clasificación de patrones. Wiley and Sons, 2da edición, 2001. [8] W. R. Greiff, W. T. Morgan y J. M. Ponte. El papel de la varianza en la ponderación de términos para la recuperación de información probabilística. En Actas de la 11ª Conferencia Internacional. Conf. sobre Gestión de Información y Conocimiento (CIKM 2002), páginas 252-259. [9] T. Kohonen, J. Hynninen, J. Kangas y J. Laaksonen. SOMPAK: El paquete de programas de mapas autoorganizados. Informe técnico A31, Universidad de Tecnología de Helsinki, 1996. http://www.cis.hut.fi/research/papers/som tr96.ps.Z. [10] V. Lavrenko. Una teoría generativa de relevancia. Tesis doctoral, Universidad de Massachusetts, Amherst, 2004. [11] C.-H. Lee, R. Greiner y S. Wang. Utilizando estimaciones de varianza específicas de la consulta para combinar clasificadores bayesianos. En Actas del 23º Congreso Internacional. Conf. sobre Aprendizaje Automático (ICML 2006), páginas 529-536. [12] D. Metzler y W. B. Croft. Combinando el modelo de lenguaje y los enfoques de red de inferencia para la recuperación. Información. Procesamiento y Gestión, 40(5):735-750, 2004. [13] T. Minka. Estimando una distribución de Dirichlet. Informe técnico, 2000. http://research.microsoft.com/minka/papers/dirichlet. [14] J. Ponte. Avances en la Recuperación de Información, capítulo Modelos de lenguaje para retroalimentación de relevancia, páginas 73-96. 2000. W.B. Croft, ed. [15] J. M. Ponte y W. B. Croft. Un enfoque de modelado del lenguaje para la recuperación de información. En Actas de la Conferencia ACM SIGIR de 1998 sobre Investigación y Desarrollo en Recuperación de Información, páginas 275-281. [16] J. Rocchio. El Sistema de Recuperación SMART, capítulo Retroalimentación de Relevancia en la Recuperación de Información, páginas 313-323. Prentice-Hall, 1971. G. Salton, ed. [17] T. Sakai, T. Manabe y M. Koyama. Retroalimentación de pseudo-relevancia flexible a través de muestreo selectivo. ACM Transactions on Asian Language Information Processing (TALIP), 4(2):111-135, 2005. [18] T. Tao y C. Zhai. Estimación regularizada de modelos de mezcla para retroalimentación de pseudo relevancia robusta. En Actas de la Conferencia ACM SIGIR 2006 sobre Investigación y Desarrollo en Recuperación de Información, páginas 162-169. [19] J. Xu y W. B. Croft. Mejorando la efectividad de la recuperación de información con análisis de contexto local. ACM Trans. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate to Spanish? Syst., 18(1):79-112, 2000. [20] E. YomTov, S. Fine, D. Carmel, y A. Darlow. Aprendiendo a estimar la dificultad de la consulta. En Actas de la Conferencia ACM SIGIR 2005 sobre Investigación y Desarrollo en Recuperación de Información, páginas 512-519. [21] Y. Zhou y W. B. Croft. Robustez del ranking: un nuevo marco para predecir el rendimiento de la consulta. En Actas de la 15ª Conferencia Internacional de la ACM. Conferencia sobre Gestión de Información y Conocimiento (CIKM 2006), páginas 567-574. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "pseudo-relevance feedback": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Estimation and Use of Uncertainty in <br>pseudo-relevance feedback</br> Kevyn Collins-Thompson and Jamie Callan Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213-8213 U.S.A. {kct | callan}@cs.cmu.edu ABSTRACT Existing <br>pseudo-relevance feedback</br> methods typically perform averaging over the top-retrieved documents, but ignore an important statistical dimension: the risk or variance associated with either the individual document models, or their combination.",
                "Treating the baseline feedback method as a black box, and the output feedback model as a random variable, we estimate a posterior distribution for the feedback model by resampling a given querys top-retrieved documents, using the posterior mean or mode as the enhanced feedback model.",
                "We then perform model combination over several enhanced models, each based on a slightly modified query sampled from the original query.",
                "We find that resampling documents helps increase individual feedback model precision by removing noise terms, while sampling from the query improves robustness (worst-case performance) by emphasizing terms related to multiple query aspects.",
                "The result is a meta-feedback algorithm that is both more robust and more precise than the original strong baseline method.",
                "Categories and Subject Descriptors: H.3.3 [Information Retrieval]: Retrieval Models General Terms: Algorithms, Experimentation 1.",
                "INTRODUCTION Uncertainty is an inherent feature of information retrieval.",
                "Not only do we not know the queries that will be presented to our retrieval algorithm ahead of time, but the users information need may be vague or incompletely specified by these queries.",
                "Even if the query were perfectly specified, language in the collection documents is inherently complex and ambiguous and matching such language effectively is a formidable problem by itself.",
                "With this in mind, we wish to treat many important quantities calculated by the retrieval system, whether a relevance score for a document, or a weight for a query expansion term, as random variables whose true value is uncertain but where the uncertainty about the true value may be quantified by replacing the fixed value with a probability distribution over possible values.",
                "In this way, retrieval algorithms may attempt to quantify the risk or uncertainty associated with their output rankings, or improve the stability or precision of their internal calculations.",
                "Current algorithms for <br>pseudo-relevance feedback</br> (PRF) tend to follow the same basic method whether we use vector space-based algorithms such as Rocchios formula [16], or more recent language modeling approaches such as Relevance Models [10].",
                "First, a set of top-retrieved documents is obtained from an initial query and assumed to approximate a set of relevant documents.",
                "Next, a single feedback model vector is computed according to some sort of average, centroid, or expectation over the set of possibly-relevant document models.",
                "For example, the document vectors may be combined with equal weighting, as in Rocchio, or by query likelihood, as may be done using the Relevance Model1 .",
                "The use of an expectation is reasonable for practical and theoretical reasons, but by itself ignores potentially valuable information about the risk of the feedback model.",
                "Our main hypothesis in this paper is that estimating the uncertainty in feedback is useful and leads to better individual feedback models and more robust combined models.",
                "Therefore, we propose a method for estimating uncertainty associated with an individual feedback model in terms of a posterior distribution over language models.",
                "To do this, we systematically vary the inputs to the baseline feedback method and fit a Dirichlet distribution to the output.",
                "We use the posterior mean or mode as the improved feedback model estimate.",
                "This process is shown in Figure 1.",
                "As we show later, the mean and mode may vary significantly from the single feedback model proposed by the baseline method.",
                "We also perform model combination using several improved feedback language models obtained by a small number of new queries sampled from the original query.",
                "A models weight combines two complementary factors: the models probability of generating the query, and the variance of the model, with high-variance models getting lower weight. 1 For example, an expected parameter vector conditioned on the query observation is formed from top-retrieved documents, which are treated as training strings (see [10], p. 62).",
                "Figure 1: Estimating the uncertainty of the feedback model for a single query. 2.",
                "SAMPLING-BASED FEEDBACK In Sections 2.1-2.5 we describe a general method for estimating a probability distribution over the set of possible language models.",
                "In Sections 2.6 and 2.7 we summarize how different query samples are used to generate multiple feedback models, which are then combined. 2.1 Modeling Feedback Uncertainty Given a query Q and a collection C, we assume a probabilistic retrieval system that assigns a real-valued document score f(D, Q) to each document D in C, such that the score is proportional to the estimated probability of relevance.",
                "We make no other assumptions about f(D, Q).",
                "The nature of f(D, Q) may be complex: for example, if the retrieval system supports structured query languages [12], then f(D, Q) may represent the output of an arbitrarily complex inference network defined by the structured query operators.",
                "In theory, the scoring function can vary from query to query, although in this study for simplicity we keep the scoring function the same for all queries.",
                "Our specific query method is given in Section 3.",
                "We treat the feedback algorithm as a black box and assume that the inputs to the feedback algorithm are the original query and the corresponding top-retrieved documents, with a score being given to each document.",
                "We assume that the output of the feedback algorithm is a vector of term weights to be used to add or reweight the terms in the representation of the original query, with the vector normalized to form a probability distribution.",
                "We view the the inputs to the feedback black box as random variables, and analyze the feedback model as a random variable that changes in response to changes in the inputs.",
                "Like the document scoring function f(D, Q), the feedback algorithm may implement a complex, non-linear scoring formula, and so as its inputs vary, the resulting feedback models may have a complex distribution over the space of feedback models (the sample space).",
                "Because of this potential complexity, we do not attempt to derive a posterior distribution in closed form, but instead use simulation.",
                "We call this distribution over possible feedback models the feedback model distribution.",
                "Our goal in this section is to estimate a useful approximation to the feedback model distribution.",
                "For a specific framework for experiments, we use the language modeling (LM) approach for information retrieval [15].",
                "The score of a document D with respect to a query Q and collection C is given by p(Q|D) with respect to language models ˆθQ and ˆθD estimated for the query and document respectively.",
                "We denote the set of k top-retrieved documents from collection C in response to Q by DQ(k, C).",
                "For simplicity, we assume that queries and documents are generated by multinomial distributions whose parameters are represented by unigram language models.",
                "To incorporate feedback in the LM approach, we assume a model-based scheme in which our goal is take the query and resulting ranked documents DQ(k, C) as input, and output an expansion language model ˆθE, which is then interpolated with the original query model ˆθQ: ˆθNew = (1 − α) · ˆθQ + α · ˆθE (1) This includes the possibility of α = 1 where the original query mode is completely replaced by the feedback model.",
                "Our sample space is the set of all possible language models LF that may be output as feedback models.",
                "Our approach is to take samples from this space and then fit a distribution to the samples using maximum likelihood.",
                "For simplicity, we start by assuming the latent feedback distribution has the form of a Dirichlet distribution.",
                "Although the Dirichlet is a unimodal distribution, and in general quite limited in its expressiveness in the sample space, it is a natural match for the multinomial language model, can be estimated quickly, and can capture the most salient features of confident and uncertain feedback models, such as the overall spread of the distibution. 2.2 Resampling document models We would like an approximation to the posterior distribution of the feedback model LF .",
                "To accomplish this, we apply a widely-used simulation technique called bootstrap sampling ([7], p. 474) on the input parameters, namely, the set of top-retrieved documents.",
                "Bootstrap sampling allows us to simulate the approximate effect of perturbing the parameters within the black box feedback algorithm by perturbing the inputs to that algorithm in a systematic way, while making no assumptions about the nature of the feedback algorithm.",
                "Specifically, we sample k documents with replacement from DQ(k, C), and calculate an expansion language model θb using the black box feedback method.",
                "We repeat this process B times to obtain a set of B feedback language models, to which we then fit a Dirichlet distribution.",
                "Typically B is in the range of 20 to 50 samples, with performance being relatively stable in this range.",
                "Note that instead of treating each top document as equally likely, we sample according to the estimated probabilities of relevance of each document in DQ(k, C).",
                "Thus, a document is more likely to be chosen the higher it is in the ranking. 2.3 Justification for a sampling approach The rationale for our sampling approach has two parts.",
                "First, we want to improve the quality of individual feedback models by smoothing out variation when the baseline feedback model is unstable.",
                "In this respect, our approach resembles bagging [4], an ensemble approach which generates multiple versions of a predictor by making bootstrap copies of the training set, and then averages the (numerical) predictors.",
                "In our application, top-retrieved documents can be seen as a kind of noisy training set for relevance.",
                "Second, sampling is an effective way to estimate basic properties of the feedback posterior distribution, which can then be used for improved model combination.",
                "For example, a model may be weighted by its prediction confidence, estimated as a function of the variability of the posterior around the model. foo2-401.map-Dim:5434,Size:12*12units,gaussianneighborhood (a) Topic 401 Foreign minorities, Germany foo2-402.map-Dim:5698,Size:12*12units,gaussianneighborhood (b) Topic 402 Behavioral genetics foo2-459.map-Dim:8969,Size:12*12units,gaussianneighborhood (c) Topic 459 When can a lender foreclose on property Figure 2: Visualization of expansion language model variance using self-organizing maps, showing the distribution of language models that results from resampling the inputs to the baseline expansion method.",
                "The language model that would have been chosen by the baseline expansion is at the center of each map.",
                "The similarity function is JensenShannon divergence. 2.4 Visualizing feedback distributions Before describing how we fit and use the Dirichlet distribution over feedback models, it is instructive to view some examples of actual feedback model distributions that result from bootstrap sampling the top-retrieved documents from different TREC topics.",
                "Each point in our sample space is a language model, which typically has several thousand dimensions.",
                "To help analyze the behavior of our method we used a Self-Organizing Map (via the SOM-PAK package [9]), to flatten and visualize the high-dimensional density function2 .",
                "The density maps for three TREC topics are shown in Figure 2 above.",
                "The dark areas represent regions of high similarity between language models.",
                "The light areas represent regions of low similarity - the valleys between clusters.",
                "Each diagram is centered on the language model that would have been chosen by the baseline expansion.",
                "A single peak (mode) is evident in some examples, but more complex structure appears in others.",
                "Also, while the distribution is usually close to the baseline feedback model, for some topics they are a significant distance apart (as measured by JensenShannon divergence), as in Subfigure 2c.",
                "In such cases, the mode or mean of the feedback distribution often performs significantly better than the baseline (and in a smaller proportion of cases, significantly worse). 2.5 Fitting a posterior feedback distribution After obtaining feedback model samples by resampling the feedback model inputs, we estimate the feedback distribution.",
                "We assume that the multinomial feedback models {ˆθ1, . . . , ˆθB} were generated by a latent Dirichlet distribution with parameters {α1, . . . , αN }.",
                "To estimate the {α1, . . . , αN }, we fit the Dirichlet parameters to the B language model samples according to maximum likelihood using a generalized Newton procedure, details of which are given in Minka [13].",
                "We assume a simple Dirichlet prior over the {α1, . . . , αN }, setting each to αi = μ · p(wi | C), where μ is a parameter and p(· | C) is the collection language model estimated from a set of documents from collection C. The parameter fitting converges very quickly - typically just 2 or 2 Because our points are language models in the multinomial simplex, we extended SOM-PAK to support JensenShannon divergence, a widely-used similarity measure between probability distributions. 3 iterations are enough - so that it is practical to apply at query-time when computational overhead must be small.",
                "In practice, we can restrict the calculation to the vocabulary of the top-retrieved documents, instead of the entire collection.",
                "Note that for this step we are re-using the existing retrieved documents and not performing additional queries.",
                "Given the parameters of an N-dimensional Dirichlet distribution Dir(α) the mean μ and mode x vectors are easy to calculate and are given respectively by μi = αiP αi (2) and xi = αi−1P αi−N . (3) We can then choose the language model at the mean or the mode of the posterior as the final enhanced feedback model. (We found the mode to give slightly better performance.)",
                "For information retrieval, the number of samples we will have available is likely to be quite small for performance reasons - usually less than ten.",
                "Moreover, while random sampling is useful in certain cases, it is perfectly acceptable to allow deterministic sampling distributions, but these must be designed carefully in order to approximate an accurate output variance.",
                "We leave this for future study. 2.6 Query variants We use the following methods for generating variants of the original query.",
                "Each variant corresponds to a different assumption about which aspects of the original query may be important.",
                "This is a form of deterministic sampling.",
                "We selected three simple methods that cover complimentary assumptions about the query.",
                "No-expansion Use only the original query.",
                "The assumption is that the given terms are a complete description of the information need.",
                "Leave-one-out A single term is left out of the original query.",
                "The assumption is that one of the query terms is a noise term.",
                "Single-term A single term is chosen from the original query.",
                "This assumes that only one aspect of the query, namely, that represented by the term, is most important.",
                "After generating a variant of the original query, we combine it with the original query using a weight αSUB so that we do not stray too far.",
                "In this study, we set αSUB = 0.5.",
                "For example, using the Indri [12] query language, a leave-oneout variant of the initial query that omits the term ireland for TREC topic 404 is: #weight(0.5 #combine(ireland peace talks) 0.5 #combine(peace talks)) 2.7 Combining enhanced feedback models from multiple query variants When using multiple query variants, the resulting enhanced feedback models are combined using Bayesian model combination.",
                "To do this, we treat each word as an item to be classified as belonging to a relevant or non-relevant class, and derive a class probability for each word by combining the scores from each query variant.",
                "Each score is given by that terms probability in the Dirichlet distribution.",
                "The term scores are weighted by the inverse of the variance of the term in the enhanced feedback models Dirichlet distribution.",
                "The prior probability of a words membership in the relevant class is given by the probability of the original query in the entire enhanced expansion model. 3.",
                "EVALUATION In this section we present results confirming the usefulness of estimating a feedback model distribution from weighted resampling of top-ranked documents, and of combining the feedback models obtained from different small changes in the original query. 3.1 General method We evaluated performance on a total of 350 queries derived from four sets of TREC topics: 51-200 (TREC-1&2), 351-400 (TREC-7), 401-450 (TREC-8), and 451-550 (wt10g, TREC-9&10).",
                "We chose these for their varied content and document properties.",
                "For example, wt10g documents are Web pages with a wide variety of subjects and styles while TREC-1&2 documents are more homogeneous news articles.",
                "Indexing and retrieval was performed using the Indri system in the Lemur toolkit [12] [1].",
                "Our queries were derived from the words in the title field of the TREC topics.",
                "Phrases were not used.",
                "To generate the baseline queries passed to Indri, we wrapped the query terms with Indris #combine operator.",
                "For example, the initial query for topic 404 is: #combine(ireland peace talks) We performed Krovetz stemming for all experiments.",
                "Because we found that the baseline (Indri) expansion method performed better using a stopword list with the feedback model, all experiments used a stoplist of 419 common English words.",
                "However, an interesting side-effect of our resampling approach is that it tends to remove many stopwords from the feedback model, making a stoplist less critical.",
                "This is discussed further in Section 3.6. 3.2 Baseline feedback method For our baseline expansion method, we use an algorithm included in Indri 1.0 as the default expansion method.",
                "This method first selects terms using a log-odds calculation described by Ponte [14], but assigns final term weights using Lavrenkos relevance model[10].",
                "We chose the Indri method because it gives a consistently strong baseline, is based on a language modeling approach, and is simple to experiment with.",
                "In a TREC evaluation using the GOV2 corpus [6], the method was one of the topperforming runs, achieving a 19.8% gain in MAP compared to using unexpanded queries.",
                "In this study, it achieves an average gain in MAP of 17.25% over the four collections.",
                "Indris expansion method first calculates a log-odds ratio o(v) for each potential expansion term v given by o(v) = X D log p(v|D) p(v|C) (4) over all documents D containing v, in collection C. Then, the expansion term candidates are sorted by descending o(v), and the top m are chosen.",
                "Finally, the term weights r(v) used in the expanded query are calculated based on the relevance model r(v) = X D p(q|D)p(v|D) p(v) p(D) (5) The quantity p(q|D) is the probability score assigned to the document in the initial retrieval.",
                "We use Dirichlet smoothing of p(v|D) with μ = 1000.",
                "This relevance model is then combined with the original query using linear interpolation, weighted by a parameter α.",
                "By default we used the top 50 documents for feedback and the top 20 expansion terms, with the feedback interpolation parameter α = 0.5 unless otherwise stated.",
                "For example, the baseline expanded query for topic 404 is: #weight(0.5 #combine(ireland peace talks) 0.5 #weight(0.10 ireland 0.08 peace 0.08 northern ...) 3.3 Expansion performance We measure our feedback algorithms effectiveness by two main criteria: precision, and robustness.",
                "Robustness, and the tradeoff between precision and robustness, is analyzed in Section 3.4.",
                "In this section, we examine average precision and precision in the top 10 documents (P10).",
                "We also include recall at 1,000 documents.",
                "For each query, we obtained a set of B feedback models using the Indri baseline.",
                "Each feedback model was obtained from a random sample of the top k documents taken with replacement.",
                "For these experiments, B = 30 and k = 50.",
                "Each feedback model contained 20 terms.",
                "On the query side, we used leave-one-out (LOO) sampling to create the query variants.",
                "Single-term query sampling had consistently worse performance across all collections and so our results here focus on LOO sampling.",
                "We used the methods described in Section 2 to estimate an enhanced feedback model from the Dirichlet posterior distribution for each query variant, and to combine the feedback models from all the query variants.",
                "We call our method resampling expansion and denote it as RS-FB here.",
                "We denote the Indri baseline feedback method as Base-FB.",
                "Results from applying both the baseline expansion method (Base-FB) and resampling expansion (RS-FB) are shown in Table 1.",
                "We observe several trends in this table.",
                "First, the average precision of RS-FB was comparable to Base-FB, achieving an average gain of 17.6% compared to using no expansion across the four collections.",
                "The Indri baseline expansion gain was 17.25%.",
                "Also, the RS-FB method achieved consistent improvements in P10 over Base-FB for every topic set, with an average improvement of 6.89% over Base-FB for all 350 topics.",
                "The lowest P10 gain over Base-FB was +3.82% for TREC-7 and the highest was +11.95% for wt10g.",
                "Finally, both Base-FB and RS-FB also consistently improved recall over using no expansion, with Base-FB achieving better recall than RS-FB for all topic sets. 3.4 Retrieval robustness We use the term robustness to mean the worst-case average precision performance of a feedback algorithm.",
                "Ideally, a robust feedback method would never perform worse than using the original query, while often performing better using the expansion.",
                "To evaluate robustness in this study, we use a very simple measure called the robustness index (RI)3 .",
                "For a set of queries Q, the RI measure is defined as: RI(Q) = n+ − n− |Q| (6) where n+ is the number of queries helped by the feedback method and n− is the number of queries hurt.",
                "Here, by helped we mean obtaining a higher average precision as a result of feedback.",
                "The value of RI ranges from a minimum 3 This is sometimes also called the reliability of improvement index and was used in Sakai et al. [17].",
                "Collection NoExp Base-FB RS-FB TREC 1&2 AvgP 0.1818 0.2419 (+33.04%) 0.2406 (+32.24%) P10 0.4443 0.4913 (+10.57%) 0.5363 (+17.83%) Recall 15084/37393 19172/37393 15396/37393 TREC 7 AvgP 0.1890 0.2175 (+15.07%) 0.2169 (+14.75%) P10 0.4200 0.4320 (+2.85%) 0.4480 (+6.67%) Recall 2179/4674 2608/4674 2487/4674 TREC 8 AvgP 0.2031 0.2361 (+16.25%) 0.2268 (+11.70%) P10 0.3960 0.4160 (+5.05%) 0.4340 (+9.59%) Recall 2144/4728 2642/4728 2485/4728 wt10g AvgP 0.1741 0.1829 (+5.06%) 0.1946 (+11.78%) P10 0.2760 0.2630 (-4.71%) 0.2960 (+7.24%) Recall 3361/5980 3725/5980 3664/5980 Table 1: Comparison of baseline (Base-FB) feedback and feedback using re-sampling (RS-FB).",
                "Improvement shown for BaseFB and RS-FB is relative to using no expansion. (a) TREC 1&2 (upper curve); TREC 8 (lower curve) (b) TREC 7 (upper curve); wt10g (lower curve) Figure 3: The trade-off between robustness and average precision for different corpora.",
                "The x-axis gives the change in MAP over using baseline expansion with α = 0.5.",
                "The yaxis gives the Robustness Index (RI).",
                "Each curve through uncircled points shows the RI/MAP tradeoff using the simple small-α strategy (see text) as α decreases from 0.5 to zero in the direction of the arrow.",
                "Circled points represent the tradeoffs obtained by resampling feedback for α = 0.5.",
                "Collection N Base-FB RS-FB n− RI n− RI TREC 1&2 103 26 +0.495 15 +0.709 TREC 7 46 14 +0.391 10 +0.565 TREC 8 44 12 +0.455 12 +0.455 wt10g 91 48 -0.055 39 +0.143 Combined 284 100 +0.296 76 +0.465 Table 2: Comparison of robustness index (RI) for baseline feedback (Base-FB) vs. resampling feedback (RS-FB).",
                "Also shown are the actual number of queries hurt by feedback (n−) for each method and collection.",
                "Queries for which initial average precision was negligible (≤ 0.01) were ignored, giving the remaining query count in column N. of −1.0, when all queries are hurt by the feedback method, to +1.0 when all queries are helped.",
                "The RI measure does not take into account the magnitude or distribution of the amount of change across the set Q.",
                "However, it is easy to understand as a general indication of robustness.",
                "One obvious way to improve the worst-case performance of feedback is simply to use a smaller fixed α interpolation parameter, such as α = 0.3, placing less weight on the (possibly risky) feedback model and more on the original query.",
                "We call this the small-α strategy.",
                "Since we are also reducing the potential gains when the feedback model is right, however, we would expect some trade-off between average precision and robustness.",
                "We therefore compared the precision/robustness trade-off between our resampling feedback algorithm, and the simple small-α method.",
                "The results are summarized in Figure 3.",
                "In the figure, the curve for each topic set interpolates between trade-off points, beginning at x=0, where α = 0.5, and continuing in the direction of the arrow as α decreases and the original query is given more and more weight.",
                "As expected, robustness continuously increases as we move along the curve, but mean average precision generally drops as the gains from feedback are eliminated.",
                "For comparison, the performance of resampling feedback at α = 0.5 is shown for each collection as the circled point.",
                "Higher and to the right is better.",
                "This figure shows that resampling feedback gives a somewhat better trade-off than the small-α approach for 3 of the 4 collections.",
                "Figure 4: Histogram showing improved robustness of resampling feedback (RS-FB) over baseline feedback (Base-FB) for all datasets combined.",
                "Queries are binned by % change in AP compared to the unexpanded query.",
                "Collection DS + QV DS + No QV TREC 1&2 AvgP 0.2406 0.2547 (+5.86%) P10 0.5263 0.5362 (+1.88%) RI 0.7087 0.6515 (-0.0572) TREC 7 AvgP 0.2169 0.2200 (+1.43%) P10 0.4480 0.4300 (-4.02%) RI 0.5652 0.2609 (-0.3043) TREC 8 AvgP 0.2268 0.2257 (-0.49%) P10 0.4340 0.4200 (-3.23%) RI 0.4545 0.4091 (-0.0454) wt10g AvgP 0.1946 0.1865 (-4.16%) P10 0.2960 0.2680 (-9.46%) RI 0.1429 0.0220 (-0.1209) Table 3: Comparison of resampling feedback using document sampling (DS) with (QV) and without (No QV) combining feedback models from multiple query variants.",
                "Table 2 gives the Robustness Index scores for Base-FB and RS-FB.",
                "The RS-FB feedback method obtained higher robustness than Base-FB on three of the four topic sets, with only slightly worse performance on TREC-8.",
                "A more detailed view showing the distribution over relative changes in AP is given by the histogram in Figure 4.",
                "Compared to Base-FB, the RS-FB method achieves a noticable reduction in the number of queries significantly hurt by expansion (i.e. where AP is hurt by 25% or more), while preserving positive gains in AP. 3.5 Effect of query and document sampling methods Given our algorithms improved robustness seen in Section 3.4, an important question is what component of our system is responsible.",
                "Is it the use of document re-sampling, the use of multiple query variants, or some other factor?",
                "The results in Table 3 suggest that the model combination based on query variants may be largely account for the improved robustness.",
                "When query variants are turned off and the original query is used by itself with document sampling, there is little net change in average precision, a small decrease in P10 for 3 out of the 4 topic sets, but a significant drop in robustness for all topic sets.",
                "In two cases, the RI measure drops by more than 50%.",
                "We also examined the effect of the document sampling method on retrieval effectiveness, using two different strategies.",
                "The uniform weighting strategy ignored the relevance scores from the initial retrieval and gave each document in the top k the same probability of selection.",
                "In contrast, the relevance-score weighting strategy chose documents with probability proportional to their relevance scores.",
                "In this way, documents that were more highly ranked were more likely to be selected.",
                "Results are shown in Table 4.",
                "The relevance-score weighting strategy performs better overall, with significantly higher RI and P10 scores on 3 of the 4 topic sets.",
                "The difference in average precision between the methods, however, is less marked.",
                "This suggests that uniform weighting acts to increase variance in retrieval results: when initial average precision is high, there are many relevant documents in the top k and uniform sampling may give a more representative relevance model than focusing on the highly-ranked items.",
                "On the other hand, when initial precision is low, there are few relevant documents in the bottom ranks and uniform sampling mixes in more of the non-relevant documents.",
                "For space reasons we only summarize our findings on sample size here.",
                "The number of samples has some effect on precision when less than 10, but performance stabilizes at around 15 to 20 samples.",
                "We used 30 samples for our experiments.",
                "Much beyond this level, the additional benefits of more samples decrease as the initial score distribution is more closely fit and the processing time increases. 3.6 The effect of resampling on expansion term quality Ideally, a retrieval model should not require a stopword list when estimating a model of relevance: a robust statistical model should down-weight stopwords automatically depending on context.",
                "Stopwords can harm feedback if selected as feedback terms, because they are typically poor discriminators and waste valuable term slots.",
                "In practice, however, because most term selection methods resemble a tf · idf type of weighting, terms with low idf but very high tf can sometimes be selected as expansion term candidates.",
                "This happens, for example, even with the Relevance Model approach that is part of our baseline feedback.",
                "To ensure as strong a baseline as possible, we use a stoplist for all experiments reported here.",
                "If we turn off the stopword list, however, we obtain results such as those shown in Table 5 where four of the top ten baseline feedback terms for TREC topic 60 (said, but, their, not) are stopwords using the BaseFB method. (The top 100 expansion terms were selected to generate this example.)",
                "Indris method attempts to address the stopword problem by applying an initial step based on Ponte [14] to select less-common terms that have high log-odds of being in the top-ranked documents compared to the whole collection.",
                "Nevertheless, this does not overcome the stopword problem completely, especially as the number of feedback terms grows.",
                "Using resampling feedback, however, appears to mitigate Collection QV + Uniform QV + Relevance-score weighting weighting TREC 1&2 AvgP 0.2545 0.2406 (-5.46%) P10 0.5369 0.5263 (-1.97%) RI 0.6212 0.7087 (+14.09%) TREC 7 AvgP 0.2174 0.2169 (-0.23%) P10 0.4320 0.4480 (+3.70%) RI 0.4783 0.5652 (+18.17%) TREC 8 AvgP 0.2267 0.2268 (+0.04%) P10 0.4120 0.4340 (+5.34%) RI 0.4545 0.4545 (+0.00%) wt10g AvgP 0.1808 0.1946 (+7.63%) P10 0.2680 0.2960 (+10.45%) RI 0.0220 0.1099 (+399.5%) Table 4: Comparison of uniform and relevance-weighted document sampling.",
                "The percentage change compared to uniform sampling is shown in parentheses.",
                "QV indicates that query variants were used in both runs.",
                "Baseline FB p(wi|R) Resampling FB p(wi|R) said 0.055 court 0.026 court 0.055 pay 0.018 pay 0.034 federal 0.012 but 0.026 education 0.011 employees 0.024 teachers 0.010 their 0.024 employees 0.010 not 0.023 case 0.010 federal 0.021 their 0.009 workers 0.020 appeals 0.008 education 0.020 union 0.007 Table 5: Feedback term quality when a stoplist is not used.",
                "Feedback terms for TREC topic 60: merit pay vs seniority. the effect of stopwords automatically.",
                "In the example of Table 5, resampling feedback leaves only one stopword (their) in the top ten.",
                "We observed similar feedback term behavior across many other topics.",
                "The reason for this effect appears to be the interaction of the term selection score with the top-m term cutoff.",
                "While the presence and even proportion of particular stopwords is fairly stable across different document samples, their relative position in the top-m list is not, as sets of documents with varying numbers of better, lower-frequency term candidates are examined for each sample.",
                "As a result, while some number of stopwords may appear in each sampled document set, any given stopword tends to fall below the cutoff for multiple samples, leading to its classification as a high-variance, low-weight feature. 4.",
                "RELATED WORK Our approach is related to previous work from several areas of information retrieval and machine learning.",
                "Our use of query variation was inspired by the work of YomTov et al. [20], Carpineto et al. [5], and Amati et al. [2], among others.",
                "These studies use the idea of creating multiple subqueries and then examining the nature of the overlap in the documents and/or expansion terms that result from each subquery.",
                "Model combination is performed using heuristics.",
                "In particular, the studies of Amati et al. and Carpineto et al. investigated combining terms from individual distributional methods using a term-reranking combination heuristic.",
                "In a set of TREC topics they found wide average variation in the rank-distance of terms from different expansion methods.",
                "Their combination method gave modest positive improvements in average precision.",
                "The idea of examining the overlap between lists of suggested terms has also been used in early query expansion approaches.",
                "Xu and Crofts method of Local Context Analysis (LCA) [19] includes a factor in the empirically-derived weighting formula that causes expansion terms to be preferred that have connections to multiple query terms.",
                "On the document side, recent work by Zhou & Croft [21] explored the idea of adding noise to documents, re-scoring them, and using the stability of the resulting rankings as an estimate of query difficulty.",
                "This is related to our use of document sampling to estimate the risk of the feedback model built from the different sets of top-retrieved documents.",
                "Sakai et al. [17] proposed an approach to improving the robustness of <br>pseudo-relevance feedback</br> using a method they call selective sampling.",
                "The essence of their method is that they allow skipping of some top-ranked documents, based on a clustering criterion, in order to select a more varied and novel set of documents later in the ranking for use by a traditional pseudo-feedback method.",
                "Their study did not find significant improvements in either robustness (RI) or MAP on their corpora.",
                "Greiff, Morgan and Ponte [8] explored the role of variance in term weighting.",
                "In a series of simulations that simplified the problem to 2-feature documents, they found that average precision degrades as term frequency variance - high noiseincreases.",
                "Downweighting terms with high variance resulted in improved average precision.",
                "This seems in accord with our own findings for individual feedback models.",
                "Estimates of output variance have recently been used for improved text classification.",
                "Lee et al. [11] used queryspecific variance estimates of classifier outputs to perform improved model combination.",
                "Instead of using sampling, they were able to derive closed-form expressions for classifier variance by assuming base classifiers using simple types of inference networks.",
                "Ando and Zhang proposed a method that they call structural feedback [3] and showed how to apply it to query expansion for the TREC Genomics Track.",
                "They used r query variations to obtain R different sets Sr of top-ranked documents that have been intersected with the top-ranked documents obtained from the original query qorig.",
                "For each Si, the normalized centroid vector ˆwi of the documents is calculated.",
                "Principal component analysis (PCA) is then applied to the ˆwi to obtain the matrix Φ of H left singular vectors φh that are used to obtain the new, expanded query qexp = qorig + ΦT Φqorig. (7) In the case H = 1, we have a single left singular vector φ: qexp = qorig + (φT qorig)φ so that the dot product φT qorig is a type of dynamic weight on the expanded query that is based on the similarity of the original query to the expanded query.",
                "The use of variance as a feedback model quality measure occurs indirectly through the application of PCA.",
                "It would be interesting to study the connections between this approach and our own modelfitting method.",
                "Finally, in language modeling approaches to feedback, Tao and Zhai [18] describe a method for more robust feedback that allows each document to have a different feedback α.",
                "The feedback weights are derived automatically using regularized EM.",
                "A roughly equal balance of query and expansion model is implied by their EM stopping condition.",
                "They propose tailoring the stopping parameter η based on a function of some quality measure of feedback documents. 5.",
                "CONCLUSIONS We have presented a new approach to <br>pseudo-relevance feedback</br> based on document and query sampling.",
                "The use of sampling is a very flexible and powerful device and is motivated by our general desire to extend current models of retrieval by estimating the risk or variance associated with the parameters or output of retrieval processes.",
                "Such variance estimates, for example, may be naturally used in a Bayesian framework for improved model estimation and combination.",
                "Applications such as selective expansion may then be implemented in a principled way.",
                "While our study uses the language modeling approach as a framework for experiments, we make few assumptions about the actual workings of the feedback algorithm.",
                "We believe it is likely that any reasonably effective baseline feedback algorithm would benefit from our approach.",
                "Our results on standard TREC collections show that our framework improves the robustness of a strong baseline feedback method across a variety of collections, without sacrificing average precision.",
                "It also gives small but consistent gains in top10 precision.",
                "In future work, we envision an investigation into how varying the set of sampling methods used and the number of samples controls the trade-off between robustness, accuracy, and efficiency.",
                "Acknowledgements We thank Paul Bennett for valuable discussions related to this work, which was supported by NSF grants #IIS-0534345 and #CNS-0454018, and U.S. Dept. of Education grant #R305G03123.",
                "Any opinions, findings, and conclusions or recommendations expressed in this material are the authors. and do not necessarily reflect those of the sponsors. 6.",
                "REFERENCES [1] The Lemur toolkit for language modeling and retrieval. http://www.lemurproject.org. [2] G. Amati, C. Carpineto, and G. Romano.",
                "Query difficulty, robustness, and selective application of query expansion.",
                "In Proc. of the 25th European Conf. on Information Retrieval (ECIR 2004), pages 127-137. [3] R. K. Ando and T. Zhang.",
                "A high-performance semi-supervised learning method for text chunking.",
                "In Proc. of the 43rd Annual Meeting of the ACL, pages 1-9, June 2005. [4] L. Breiman.",
                "Bagging predictors.",
                "Machine Learning, 24(2):123-140, 1996. [5] C. Carpineto, G. Romano, and V. Giannini.",
                "Improving retrieval feedback with multiple term-ranking function combination.",
                "ACM Trans.",
                "Info.",
                "Systems, 20(3):259 - 290. [6] K. Collins-Thompson, P. Ogilvie, and J. Callan.",
                "Initial results with structured queries and language models on half a terabyte of text.",
                "In Proc. of 2005 Text REtrieval Conference.",
                "NIST Special Publication. [7] R. O. Duda, P. E. Hart, and D. G. Stork.",
                "Pattern Classification.",
                "Wiley and Sons, 2nd edition, 2001. [8] W. R. Greiff, W. T. Morgan, and J. M. Ponte.",
                "The role of variance in term weighting for probabilistic information retrieval.",
                "In Proc. of the 11th Intl.",
                "Conf. on Info. and Knowledge Mgmt. (CIKM 2002), pages 252-259. [9] T. Kohonen, J. Hynninen, J. Kangas, and J. Laaksonen.",
                "SOMPAK: The self-organizing map program package.",
                "Technical Report A31, Helsinki University of Technology, 1996. http://www.cis.hut.fi/research/papers/som tr96.ps.Z. [10] V. Lavrenko.",
                "A Generative Theory of Relevance.",
                "PhD thesis, University of Massachusetts, Amherst, 2004. [11] C.-H. Lee, R. Greiner, and S. Wang.",
                "Using query-specific variance estimates to combine Bayesian classifiers.",
                "In Proc. of the 23rd Intl.",
                "Conf. on Machine Learning (ICML 2006), pages 529-536. [12] D. Metzler and W. B. Croft.",
                "Combining the language model and inference network approaches to retrieval.",
                "Info.",
                "Processing and Mgmt., 40(5):735-750, 2004. [13] T. Minka.",
                "Estimating a Dirichlet distribution.",
                "Technical report, 2000. http://research.microsoft.com/ minka/papers/dirichlet. [14] J. Ponte.",
                "Advances in Information Retrieval, chapter Language models for relevance feedback, pages 73-96. 2000.",
                "W.B.",
                "Croft, ed. [15] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In Proc. of the 1998 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 275-281. [16] J. Rocchio.",
                "The SMART Retrieval System, chapter Relevance Feedback in Information Retrieval, pages 313-323.",
                "Prentice-Hall, 1971.",
                "G. Salton, ed. [17] T. Sakai, T. Manabe, and M. Koyama.",
                "Flexible <br>pseudo-relevance feedback</br> via selective sampling.",
                "ACM Transactions on Asian Language Information Processing (TALIP), 4(2):111-135, 2005. [18] T. Tao and C. Zhai.",
                "Regularized estimation of mixture models for robust <br>pseudo-relevance feedback</br>.",
                "In Proc. of the 2006 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 162-169. [19] J. Xu and W. B. Croft.",
                "Improving the effectiveness of information retrieval with local context analysis.",
                "ACM Trans.",
                "Inf.",
                "Syst., 18(1):79-112, 2000. [20] E. YomTov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty.",
                "In Proc. of the 2005 ACM SIGIR Conf. on Research and Development in Information Retrieval, pages 512-519. [21] Y. Zhou and W. B. Croft.",
                "Ranking robustness: a novel framework to predict query performance.",
                "In Proc. of the 15th ACM Intl.",
                "Conf. on Information and Knowledge Mgmt. (CIKM 2006), pages 567-574."
            ],
            "original_annotated_samples": [
                "Estimation and Use of Uncertainty in <br>pseudo-relevance feedback</br> Kevyn Collins-Thompson and Jamie Callan Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213-8213 U.S.A. {kct | callan}@cs.cmu.edu ABSTRACT Existing <br>pseudo-relevance feedback</br> methods typically perform averaging over the top-retrieved documents, but ignore an important statistical dimension: the risk or variance associated with either the individual document models, or their combination.",
                "Current algorithms for <br>pseudo-relevance feedback</br> (PRF) tend to follow the same basic method whether we use vector space-based algorithms such as Rocchios formula [16], or more recent language modeling approaches such as Relevance Models [10].",
                "Sakai et al. [17] proposed an approach to improving the robustness of <br>pseudo-relevance feedback</br> using a method they call selective sampling.",
                "CONCLUSIONS We have presented a new approach to <br>pseudo-relevance feedback</br> based on document and query sampling.",
                "Flexible <br>pseudo-relevance feedback</br> via selective sampling."
            ],
            "translated_annotated_samples": [
                "Estimación y Uso de la Incertidumbre en la Retroalimentación de Pseudo-Relevancia Kevyn Collins-Thompson y Jamie Callan Instituto de Tecnologías del Lenguaje Escuela de Ciencias de la Computación Universidad Carnegie Mellon Pittsburgh, PA 15213-8213 EE. UU. {kct | callan}@cs.cmu.edu RESUMEN Los métodos existentes de retroalimentación de <br>pseudo-relevancia</br> suelen realizar un promedio de los documentos mejor clasificados, pero ignoran una dimensión estadística importante: el riesgo o la varianza asociada tanto a los modelos de documentos individuales como a su combinación.",
                "Los algoritmos actuales para la <br>retroalimentación de pseudo relevancia</br> (PRF) tienden a seguir el mismo método básico ya sea que utilicemos algoritmos basados en espacio vectorial como la fórmula de Rocchio [16], o enfoques más recientes de modelado de lenguaje como Modelos de Relevancia [10].",
                "Sakai et al. [17] propusieron un enfoque para mejorar la robustez de la <br>retroalimentación de relevancia pseudo</br> utilizando un método que llaman muestreo selectivo.",
                "CONCLUSIONES Hemos presentado un nuevo enfoque para la <br>retroalimentación de relevancia pseudo</br> basado en el muestreo de documentos y consultas.",
                "Retroalimentación de <br>pseudo-relevancia</br> flexible a través de muestreo selectivo."
            ],
            "translated_text": "Estimación y Uso de la Incertidumbre en la Retroalimentación de Pseudo-Relevancia Kevyn Collins-Thompson y Jamie Callan Instituto de Tecnologías del Lenguaje Escuela de Ciencias de la Computación Universidad Carnegie Mellon Pittsburgh, PA 15213-8213 EE. UU. {kct | callan}@cs.cmu.edu RESUMEN Los métodos existentes de retroalimentación de <br>pseudo-relevancia</br> suelen realizar un promedio de los documentos mejor clasificados, pero ignoran una dimensión estadística importante: el riesgo o la varianza asociada tanto a los modelos de documentos individuales como a su combinación. Tratando el método de retroalimentación de línea base como una caja negra, y el modelo de retroalimentación de salida como una variable aleatoria, estimamos una distribución posterior para el modelo de retroalimentación mediante el remuestreo de los documentos mejor recuperados de una consulta dada, utilizando la media o la moda posterior como el modelo de retroalimentación mejorado. Luego realizamos la combinación de modelos sobre varios modelos mejorados, cada uno basado en una consulta ligeramente modificada muestreada de la consulta original. Observamos que el remuestreo de documentos ayuda a aumentar la precisión del modelo de retroalimentación individual al eliminar términos de ruido, mientras que el muestreo de la consulta mejora la robustez (rendimiento en el peor de los casos) al enfatizar términos relacionados con múltiples aspectos de la consulta. El resultado es un algoritmo de meta-retroalimentación que es tanto más robusto como más preciso que el método de referencia original. Categorías y Descriptores de Asignaturas: H.3.3 [Recuperación de Información]: Modelos de Recuperación Términos Generales: Algoritmos, Experimentación 1. INTRODUCCIÓN La incertidumbre es una característica inherente de la recuperación de información. No solo no sabemos las consultas que se presentarán a nuestro algoritmo de recuperación con anticipación, sino que la necesidad de información de los usuarios puede ser vaga o estar incompletamente especificada por estas consultas. Incluso si la consulta estuviera perfectamente especificada, el lenguaje en los documentos de la colección es inherentemente complejo y ambiguo, y hacer coincidir dicho lenguaje de manera efectiva es un problema formidable por sí mismo. Con esto en mente, deseamos tratar muchas cantidades importantes calculadas por el sistema de recuperación, ya sea un puntaje de relevancia para un documento, o un peso para un término de expansión de consulta, como variables aleatorias cuyo valor real es incierto pero donde la incertidumbre sobre el valor real puede ser cuantificada reemplazando el valor fijo con una distribución de probabilidad sobre posibles valores. De esta manera, los algoritmos de recuperación pueden intentar cuantificar el riesgo o la incertidumbre asociados con sus clasificaciones de salida, o mejorar la estabilidad o precisión de sus cálculos internos. Los algoritmos actuales para la <br>retroalimentación de pseudo relevancia</br> (PRF) tienden a seguir el mismo método básico ya sea que utilicemos algoritmos basados en espacio vectorial como la fórmula de Rocchio [16], o enfoques más recientes de modelado de lenguaje como Modelos de Relevancia [10]. Primero, se obtiene un conjunto de documentos recuperados en la parte superior a partir de una consulta inicial y se asume que aproxima un conjunto de documentos relevantes. A continuación, se calcula un vector de modelo de retroalimentación único según algún tipo de promedio, centroide o expectativa sobre el conjunto de modelos de documentos posiblemente relevantes. Por ejemplo, los vectores de documentos pueden combinarse con igual ponderación, como en Rocchio, o por probabilidad de consulta, como se puede hacer utilizando el Modelo de Relevancia. El uso de una expectativa es razonable por razones prácticas y teóricas, pero por sí sola ignora información potencialmente valiosa sobre el riesgo del modelo de retroalimentación. Nuestra hipótesis principal en este artículo es que estimar la incertidumbre en la retroalimentación es útil y conduce a modelos de retroalimentación individual más efectivos y a modelos combinados más robustos. Por lo tanto, proponemos un método para estimar la incertidumbre asociada con un modelo de retroalimentación individual en términos de una distribución posterior sobre modelos de lenguaje. Para hacer esto, variamos sistemáticamente las entradas al método de retroalimentación base y ajustamos una distribución de Dirichlet a la salida. Utilizamos la media posterior o el modo como la estimación mejorada del modelo de retroalimentación. Este proceso se muestra en la Figura 1. Como mostraremos más adelante, la media y la moda pueden variar significativamente del modelo de retroalimentación única propuesto por el método de referencia. También realizamos la combinación de modelos utilizando varios modelos de lenguaje de retroalimentación mejorados obtenidos a partir de un pequeño número de nuevas consultas muestreadas de la consulta original. El peso de un modelo combina dos factores complementarios: la probabilidad del modelo de generar la consulta y la varianza del modelo, siendo los modelos con alta varianza los que obtienen un peso menor. Por ejemplo, un vector de parámetros esperado condicionado a la observación de la consulta se forma a partir de los documentos recuperados en la parte superior, que se tratan como cadenas de entrenamiento (ver [10], p. 62). Figura 1: Estimación de la incertidumbre del modelo de retroalimentación para una sola consulta. 2. En las Secciones 2.1-2.5 describimos un método general para estimar una distribución de probabilidad sobre el conjunto de posibles modelos de lenguaje basado en muestreo. En las Secciones 2.6 y 2.7 resumimos cómo se utilizan diferentes muestras de consultas para generar múltiples modelos de retroalimentación, que luego se combinan. 2.1 Modelado de la Incertidumbre de la Retroalimentación Dado una consulta Q y una colección C, asumimos un sistema de recuperación probabilístico que asigna un puntaje de documento de valor real f(D, Q) a cada documento D en C, de manera que el puntaje sea proporcional a la probabilidad estimada de relevancia. No hacemos ninguna otra suposición sobre f(D, Q). La naturaleza de f(D, Q) puede ser compleja: por ejemplo, si el sistema de recuperación admite lenguajes de consulta estructurados [12], entonces f(D, Q) puede representar la salida de una red de inferencia arbitrariamente compleja definida por los operadores de consulta estructurados. En teoría, la función de puntuación puede variar de una consulta a otra, aunque en este estudio por simplicidad mantenemos la misma función de puntuación para todas las consultas. Nuestro método de consulta específico se encuentra en la Sección 3. Tratamos el algoritmo de retroalimentación como una caja negra y asumimos que los insumos del algoritmo de retroalimentación son la consulta original y los documentos recuperados principales correspondientes, a los cuales se les asigna una puntuación a cada documento. Suponemos que la salida del algoritmo de retroalimentación es un vector de pesos de términos que se utilizarán para agregar o reponderar los términos en la representación de la consulta original, con el vector normalizado para formar una distribución de probabilidad. Consideramos las entradas a la caja negra de retroalimentación como variables aleatorias, y analizamos el modelo de retroalimentación como una variable aleatoria que cambia en respuesta a cambios en las entradas. Al igual que la función de puntuación del documento f(D, Q), el algoritmo de retroalimentación puede implementar una fórmula de puntuación compleja y no lineal, por lo que, a medida que sus entradas varían, los modelos de retroalimentación resultantes pueden tener una distribución compleja en el espacio de modelos de retroalimentación (el espacio muestral). Debido a esta complejidad potencial, no intentamos derivar una distribución posterior de forma cerrada, sino que en su lugar utilizamos simulación. Llamamos a esta distribución sobre posibles modelos de retroalimentación la distribución de modelos de retroalimentación. Nuestro objetivo en esta sección es estimar una aproximación útil a la distribución del modelo de retroalimentación. Para un marco específico de experimentos, utilizamos el enfoque de modelado de lenguaje (LM) para la recuperación de información [15]. El puntaje de un documento D con respecto a una consulta Q y una colección C se da por p(Q|D) con respecto a los modelos de lenguaje ˆθQ y ˆθD estimados para la consulta y el documento respectivamente. Denotamos el conjunto de los k documentos mejor recuperados de la colección C en respuesta a Q como DQ(k, C). Para simplificar, asumimos que las consultas y documentos son generados por distribuciones multinomiales cuyos parámetros están representados por modelos de lenguaje unigram. Para incorporar retroalimentación en el enfoque de LM, asumimos un esquema basado en modelos en el que nuestro objetivo es tomar la consulta y los documentos clasificados resultantes DQ(k, C) como entrada, y producir un modelo de lenguaje expandido ˆθE, que luego se interpola con el modelo de consulta original ˆθQ: ˆθNew = (1 − α) · ˆθQ + α · ˆθE (1). Esto incluye la posibilidad de α = 1 donde el modelo de consulta original es completamente reemplazado por el modelo de retroalimentación. Nuestro espacio muestral es el conjunto de todos los posibles modelos de lenguaje LF que pueden ser generados como modelos de retroalimentación. Nuestro enfoque es tomar muestras de este espacio y luego ajustar una distribución a las muestras utilizando máxima verosimilitud. Para simplificar, comenzamos asumiendo que la distribución de retroalimentación latente tiene la forma de una distribución de Dirichlet. Aunque la distribución de Dirichlet es unimodal y en general bastante limitada en su expresividad en el espacio muestral, es una coincidencia natural para el modelo de lenguaje multinomial, puede ser estimada rápidamente y capturar las características más relevantes de los modelos de retroalimentación confiada e incierta, como la dispersión general de la distribución. 2.2 Re-muestreo de modelos de documentos. Nos gustaría una aproximación a la distribución posterior del modelo de retroalimentación LF. Para lograr esto, aplicamos una técnica de simulación ampliamente utilizada llamada muestreo bootstrap ([7], p. 474) en los parámetros de entrada, es decir, el conjunto de documentos recuperados en la parte superior. El muestreo de arranque nos permite simular el efecto aproximado de perturbar los parámetros dentro del algoritmo de retroalimentación de la caja negra al perturbar los insumos de ese algoritmo de manera sistemática, sin hacer suposiciones sobre la naturaleza del algoritmo de retroalimentación. Específicamente, muestreamos k documentos con reemplazo de DQ(k, C), y calculamos un modelo de lenguaje de expansión θb utilizando el método de retroalimentación de caja negra. Repetimos este proceso B veces para obtener un conjunto de B modelos de lenguaje de retroalimentación, a los cuales luego ajustamos una distribución de Dirichlet. Normalmente, B se encuentra en el rango de 20 a 50 muestras, con un rendimiento relativamente estable en este rango. Ten en cuenta que en lugar de tratar cada documento superior como igualmente probable, muestreamos de acuerdo con las probabilidades estimadas de relevancia de cada documento en DQ(k, C). Por lo tanto, es más probable que un documento sea elegido cuanto más alto esté en la clasificación. 2.3 Justificación de un enfoque de muestreo La justificación de nuestro enfoque de muestreo tiene dos partes. Primero, queremos mejorar la calidad de los modelos de retroalimentación individual al suavizar la variación cuando el modelo de retroalimentación base es inestable. En este sentido, nuestro enfoque se asemeja al bagging [4], un enfoque de conjunto que genera múltiples versiones de un predictor haciendo copias de arranque del conjunto de entrenamiento, y luego promedia los predictores (numéricos). En nuestra aplicación, los documentos recuperados en la parte superior pueden ser vistos como una especie de conjunto de entrenamiento ruidoso para la relevancia. Segundo, el muestreo es una forma efectiva de estimar propiedades básicas de la distribución posterior de retroalimentación, las cuales pueden ser utilizadas para mejorar la combinación de modelos. Por ejemplo, un modelo puede ser ponderado por su confianza de predicción, estimada como una función de la variabilidad del posterior alrededor del modelo. foo2-401.map-Dim:5434,Size:12*12units, vecindario gaussiano (a) Tema 401 Minorías extranjeras, Alemania foo2-402.map-Dim:5698,Size:12*12units, vecindario gaussiano (b) Tema 402 Genética del comportamiento foo2-459.map-Dim:8969,Size:12*12units, vecindario gaussiano (c) Tema 459 ¿Cuándo puede un prestamista ejecutar una hipoteca sobre una propiedad? Figura 2: Visualización de la varianza del modelo de lenguaje de expansión utilizando mapas autoorganizados, mostrando la distribución de modelos de lenguaje que resulta de remuestrear las entradas al método de expansión de referencia. El modelo de lenguaje que habría sido elegido por la expansión de la línea base se encuentra en el centro de cada mapa. La función de similitud es la divergencia JensenShannon. 2.4 Visualización de distribuciones de retroalimentación Antes de describir cómo ajustamos y utilizamos la distribución de Dirichlet sobre modelos de retroalimentación, es instructivo ver algunos ejemplos de distribuciones de modelos de retroalimentación reales que resultan de muestrear por bootstrap los documentos más recuperados de diferentes temas de TREC. Cada punto en nuestro espacio muestral es un modelo de lenguaje, que típicamente tiene varias miles de dimensiones. Para ayudar a analizar el comportamiento de nuestro método, utilizamos un Mapa Auto-organizado (a través del paquete SOM-PAK [9]), para aplanar y visualizar la función de densidad de alta dimensión. Los mapas de densidad para tres temas de TREC se muestran en la Figura 2 arriba. Las áreas oscuras representan regiones de alta similitud entre los modelos de lenguaje. Las áreas claras representan regiones de baja similitud, los valles entre los grupos. Cada diagrama está centrado en el modelo de lenguaje que habría sido elegido por la expansión de referencia. Un solo pico (modo) es evidente en algunos ejemplos, pero en otros aparece una estructura más compleja. Además, aunque la distribución suele estar cerca del modelo de retroalimentación de referencia, para algunos temas están a una distancia significativa (según la divergencia de JensenShannon), como se muestra en la Subfigura 2c. En tales casos, la moda o la media de la distribución de retroalimentación a menudo funcionan significativamente mejor que el valor base (y en una proporción menor de casos, significativamente peor). 2.5 Ajuste de una distribución de retroalimentación posterior Después de obtener muestras del modelo de retroalimentación mediante el remuestreo de las entradas del modelo de retroalimentación, estimamos la distribución de retroalimentación. Suponemos que los modelos de retroalimentación multinomial {ˆθ1, . . . , ˆθB} fueron generados por una distribución de Dirichlet latente con parámetros {α1, . . . , αN}. Para estimar los {α1, . . . , αN }, ajustamos los parámetros de Dirichlet a las muestras del modelo de lenguaje B según la máxima verosimilitud utilizando un procedimiento de Newton generalizado, cuyos detalles se encuentran en Minka [13]. Suponemos una distribución de Dirichlet simple sobre los {α1, . . . , αN}, estableciendo cada uno como αi = μ · p(wi | C), donde μ es un parámetro y p(· | C) es el modelo de lenguaje de colección estimado a partir de un conjunto de documentos de la colección C. El ajuste de parámetros converge muy rápidamente, generalmente en solo 2 o 3 iteraciones. Dado que nuestros puntos son modelos de lenguaje en el simplex multinomial, extendimos SOM-PAK para admitir la divergencia de Jensen-Shannon, una medida de similitud ampliamente utilizada entre distribuciones de probabilidad. 3 iteraciones son suficientes, por lo que es práctico aplicarlo en tiempo de consulta cuando la sobrecarga computacional debe ser pequeña. En la práctica, podemos restringir el cálculo al vocabulario de los documentos recuperados en la parte superior, en lugar de toda la colección. Ten en cuenta que para este paso estamos reutilizando los documentos recuperados existentes y no realizando consultas adicionales. Dado los parámetros de una distribución de Dirichlet N-dimensional Dir(α), los vectores de media μ y moda x son fáciles de calcular y se dan respectivamente por μi = αiP αi (2) y xi = αi−1P αi−N . (3) Luego podemos elegir el modelo de lenguaje en la media o en la moda del posterior como el modelo de retroalimentación mejorado final. (Encontramos que la moda proporciona un rendimiento ligeramente mejor). Para la recuperación de información, es probable que el número de muestras disponibles sea bastante pequeño por razones de rendimiento, generalmente menos de diez. Además, si bien el muestreo aleatorio es útil en ciertos casos, es perfectamente aceptable permitir distribuciones de muestreo determinísticas, pero estas deben diseñarse cuidadosamente para aproximar una varianza de salida precisa. Dejamos esto para estudios futuros. 2.6 Variantes de consulta Utilizamos los siguientes métodos para generar variantes de la consulta original. Cada variante corresponde a una suposición diferente sobre qué aspectos de la consulta original pueden ser importantes. Esta es una forma de muestreo determinístico. Seleccionamos tres métodos simples que cubren suposiciones complementarias sobre la consulta. Uso sin expansión. La suposición es que los términos dados son una descripción completa de la necesidad de información. Deja uno fuera. Se deja fuera un término del query original. La suposición es que uno de los términos de la consulta es un término de ruido. Se elige un término único de la consulta original. Esto asume que solo un aspecto de la consulta, es decir, el representado por el término, es el más importante. Después de generar una variante de la consulta original, la combinamos con la consulta original utilizando un peso αSUB para no alejarnos demasiado. En este estudio, establecimos αSUB = 0.5. Por ejemplo, utilizando el lenguaje de consulta Indri [12], una variante de dejar uno fuera de la consulta inicial que omite el término \"ireland\" para el tema TREC 404 es: #weight(0.5 #combine(ireland peace talks) 0.5 #combine(peace talks)) 2.7 Combinando modelos de retroalimentación mejorados de múltiples variantes de consulta. Cuando se utilizan múltiples variantes de consulta, los modelos de retroalimentación mejorados resultantes se combinan utilizando la combinación de modelos bayesianos. Para hacer esto, tratamos cada palabra como un elemento a clasificar como perteneciente a una clase relevante o no relevante, y derivamos una probabilidad de clase para cada palabra combinando las puntuaciones de cada variante de consulta. Cada puntuación es dada por la probabilidad de ese término en la distribución de Dirichlet. Las puntuaciones de los términos se ponderan por el inverso de la varianza del término en los modelos de retroalimentación mejorada de la distribución de Dirichlet. La probabilidad previa de pertenencia de una palabra a la clase relevante se da por la probabilidad de la consulta original en todo el modelo de expansión mejorado. 3. EVALUACIÓN En esta sección presentamos resultados que confirman la utilidad de estimar una distribución del modelo de retroalimentación a partir del remuestreo ponderado de documentos mejor clasificados, y de combinar los modelos de retroalimentación obtenidos a partir de diferentes pequeños cambios en la consulta original. 3.1 Método general Evaluamos el rendimiento en un total de 350 consultas derivadas de cuatro conjuntos de temas de TREC: 51-200 (TREC-1&2), 351-400 (TREC-7), 401-450 (TREC-8) y 451-550 (wt10g, TREC-9&10). Elegimos estos por su contenido variado y propiedades del documento. Por ejemplo, los documentos wt10g son páginas web con una amplia variedad de temas y estilos, mientras que los documentos TREC-1&2 son artículos de noticias más homogéneos. La indexación y recuperación se realizó utilizando el sistema Indri en el kit de herramientas Lemur [12] [1]. Nuestras consultas se derivaron de las palabras en el campo del título de los temas de TREC. Las frases no fueron utilizadas. Para generar las consultas base que se pasaron a Indri, envolvimos los términos de la consulta con el operador #combine de Indri. Por ejemplo, la consulta inicial para el tema 404 es: #combine(ireland peace talks) Realizamos el stemming de Krovetz para todos los experimentos. Debido a que encontramos que el método de expansión de la base (Indri) funcionó mejor utilizando una lista de palabras vacías con el modelo de retroalimentación, todos los experimentos utilizaron una lista de 419 palabras comunes en inglés. Sin embargo, un efecto secundario interesante de nuestro enfoque de remuestreo es que tiende a eliminar muchas palabras vacías del modelo de retroalimentación, lo que hace que una lista de paradas sea menos crítica. Esto se discute más a fondo en la Sección 3.6. Método de retroalimentación de línea base 3.2 Para nuestro método de expansión de línea base, utilizamos un algoritmo incluido en Indri 1.0 como el método de expansión predeterminado. Este método primero selecciona términos utilizando un cálculo de logaritmo de probabilidades descrito por Ponte [14], pero asigna pesos finales a los términos utilizando el modelo de relevancia de Lavrenko [10]. Elegimos el método Indri porque proporciona una línea base consistentemente sólida, se basa en un enfoque de modelado del lenguaje y es fácil de experimentar con él. En una evaluación de TREC utilizando el corpus GOV2, el método fue uno de los mejores resultados, logrando un aumento del 19.8% en el MAP en comparación con el uso de consultas no expandidas. En este estudio, se logra una ganancia promedio en MAP del 17.25% en las cuatro colecciones. El método de expansión de Indri primero calcula una razón de logaritmos de probabilidades o(v) para cada término de expansión potencial v dado por o(v) = X D log p(v|D) p(v|C) (4) sobre todos los documentos D que contienen v, en la colección C. Luego, los candidatos a términos de expansión se ordenan por o(v) descendente, y se eligen los primeros m. Finalmente, los pesos de término r(v) utilizados en la consulta ampliada se calculan en base al modelo de relevancia r(v) = X D p(q|D)p(v|D) p(v) p(D) (5). La cantidad p(q|D) es la puntuación de probabilidad asignada al documento en la recuperación inicial. Utilizamos suavizado de Dirichlet de p(v|D) con μ = 1000. Este modelo de relevancia se combina luego con la consulta original utilizando interpolación lineal, ponderada por un parámetro α. Por defecto, utilizamos los 50 documentos principales para la retroalimentación y los 20 términos de expansión, con el parámetro de interpolación de retroalimentación α = 0.5 a menos que se indique lo contrario. Por ejemplo, la consulta expandida base para el tema 404 es: #weight(0.5 #combine(ireland peace talks) 0.5 #weight(0.10 ireland 0.08 peace 0.08 northern ...) 3.3 Rendimiento de la expansión. Medimos la efectividad de nuestros algoritmos de retroalimentación mediante dos criterios principales: precisión y robustez. La robustez, y el equilibrio entre precisión y robustez, se analiza en la Sección 3.4. En esta sección, examinamos la precisión promedio y la precisión en los 10 documentos principales (P10). También incluimos la recuperación de 1,000 documentos. Para cada consulta, obtuvimos un conjunto de B modelos de retroalimentación utilizando la línea base de Indri. Cada modelo de retroalimentación se obtuvo a partir de una muestra aleatoria de los k documentos principales tomados con reemplazo. Para estos experimentos, B = 30 y k = 50. Cada modelo de retroalimentación contenía 20 términos. En el lado de la consulta, utilizamos muestreo de dejar uno fuera (LOO) para crear las variantes de consulta. El muestreo de consulta de un solo término tuvo un rendimiento consistentemente peor en todas las colecciones, por lo que nuestros resultados se centran en el muestreo de LOO aquí. Utilizamos los métodos descritos en la Sección 2 para estimar un modelo de retroalimentación mejorado a partir de la distribución posterior de Dirichlet para cada variante de consulta, y para combinar los modelos de retroalimentación de todas las variantes de consulta. Llamamos a nuestro método expansión de remuestreo y lo denotamos como RS-FB aquí. Denominamos al método de retroalimentación de línea base Indri como Base-FB. Los resultados de aplicar tanto el método de expansión de línea base (Base-FB) como la expansión de remuestreo (RS-FB) se muestran en la Tabla 1. Observamos varias tendencias en esta tabla. Primero, la precisión promedio de RS-FB fue comparable a Base-FB, logrando una ganancia promedio del 17.6% en comparación con no utilizar expansión en las cuatro colecciones. La ganancia de expansión de la línea base de Indri fue del 17.25%. Además, el método RS-FB logró mejoras consistentes en P10 sobre Base-FB para cada conjunto de temas, con una mejora promedio del 6.89% sobre Base-FB para los 350 temas. La ganancia P10 más baja sobre Base-FB fue del +3.82% para TREC-7 y la más alta fue del +11.95% para wt10g. Finalmente, tanto Base-FB como RS-FB también mejoraron consistentemente la recuperación en comparación con no utilizar expansión, siendo Base-FB el que logró una mejor recuperación que RS-FB para todos los conjuntos de temas. 3.4 Robustez de recuperación. Utilizamos el término robustez para referirnos al rendimiento de precisión promedio en el peor de los casos de un algoritmo de retroalimentación. Idealmente, un método de retroalimentación sólido nunca debería funcionar peor que usar la consulta original, mientras que a menudo funciona mejor utilizando la expansión. Para evaluar la robustez en este estudio, utilizamos una medida muy simple llamada índice de robustez (RI). Para un conjunto de consultas Q, la medida de RI se define como: RI(Q) = n+ − n− |Q| (6) donde n+ es el número de consultas ayudadas por el método de retroalimentación y n− es el número de consultas perjudicadas. Aquí, por \"ayudar\" nos referimos a obtener una precisión promedio más alta como resultado de la retroalimentación. El valor de RI varía desde un mínimo de 3. A veces también se le llama índice de confiabilidad de mejora y fue utilizado en Sakai et al. [17]. Colección NoExp Base-FB RS-FB TREC 1&2 AvgP 0.1818 0.2419 (+33.04%) 0.2406 (+32.24%) P10 0.4443 0.4913 (+10.57%) 0.5363 (+17.83%) Recall 15084/37393 19172/37393 15396/37393 TREC 7 AvgP 0.1890 0.2175 (+15.07%) 0.2169 (+14.75%) P10 0.4200 0.4320 (+2.85%) 0.4480 (+6.67%) Recall 2179/4674 2608/4674 2487/4674 TREC 8 AvgP 0.2031 0.2361 (+16.25%) 0.2268 (+11.70%) P10 0.3960 0.4160 (+5.05%) 0.4340 (+9.59%) Recall 2144/4728 2642/4728 2485/4728 wt10g AvgP 0.1741 0.1829 (+5.06%) 0.1946 (+11.78%) P10 0.2760 0.2630 (-4.71%) 0.2960 (+7.24%) Recall 3361/5980 3725/5980 3664/5980 Tabla 1: Comparación de la retroalimentación base (Base-FB) y la retroalimentación utilizando re-muestreo (RS-FB). La mejora mostrada para BaseFB y RS-FB es relativa al uso de ninguna expansión. (a) TREC 1&2 (curva superior); TREC 8 (curva inferior) (b) TREC 7 (curva superior); wt10g (curva inferior) Figura 3: La compensación entre robustez y precisión promedio para diferentes corpora. El eje x muestra el cambio en la MAP al utilizar la expansión de la línea base con α = 0.5. El eje y muestra el Índice de Robustez (RI). Cada curva a través de los puntos no circulados muestra el compromiso entre RI/MAP utilizando la estrategia simple de pequeño α (ver texto) a medida que α disminuye de 0.5 a cero en la dirección de la flecha. Los puntos circulados representan los compromisos obtenidos al volver a muestrear la retroalimentación para α = 0.5. Colección N Base-FB RS-FB n− RI n− RI TREC 1&2 103 26 +0.495 15 +0.709 TREC 7 46 14 +0.391 10 +0.565 TREC 8 44 12 +0.455 12 +0.455 wt10g 91 48 -0.055 39 +0.143 Combinado 284 100 +0.296 76 +0.465 Tabla 2: Comparación del índice de robustez (RI) para retroalimentación de línea base (Base-FB) vs. retroalimentación de remuestreo (RS-FB). También se muestran el número real de consultas perjudicadas por la retroalimentación (n−) para cada método y colección. Las consultas para las cuales la precisión promedio inicial era insignificante (≤ 0.01) fueron ignoradas, dando como resultado el recuento de consultas restantes en la columna N, de -1.0, cuando todas las consultas son perjudicadas por el método de retroalimentación, a +1.0 cuando todas las consultas son ayudadas. La medida de RI no tiene en cuenta la magnitud o distribución de la cantidad de cambio en el conjunto Q. Sin embargo, es fácil de entender como una indicación general de robustez. Una forma obvia de mejorar el rendimiento en el peor de los casos de la retroalimentación es simplemente utilizar un parámetro de interpolación α fijo más pequeño, como α = 0.3, otorgando menos peso al modelo de retroalimentación (posiblemente arriesgado) y más al cuestionario original. Llamamos a esto la estrategia de pequeña α. Dado que también estamos reduciendo las ganancias potenciales cuando el modelo de retroalimentación es correcto, sin embargo, esperaríamos algún tipo de compensación entre la precisión promedio y la robustez. Por lo tanto, comparamos el equilibrio entre precisión y robustez entre nuestro algoritmo de retroalimentación de remuestreo y el método simple de pequeño α. Los resultados se resumen en la Figura 3. En la figura, la curva para cada conjunto de temas interpola entre los puntos de compensación, comenzando en x=0, donde α = 0.5, y continuando en la dirección de la flecha a medida que α disminuye y la consulta original recibe cada vez más peso. Como era de esperar, la robustez aumenta continuamente a medida que avanzamos a lo largo de la curva, pero la precisión media promedio generalmente disminuye a medida que se eliminan las ganancias del feedback. Para la comparación, se muestra el rendimiento del re-muestreo con retroalimentación en α = 0.5 para cada colección como el punto marcado con un círculo. Más alto y a la derecha es mejor. Esta figura muestra que el re-muestreo de retroalimentación ofrece un compromiso algo mejor que el enfoque de pequeño α para 3 de las 4 colecciones. Figura 4: Histograma que muestra la mayor robustez del re-muestreo de retroalimentación (RS-FB) sobre la retroalimentación base (Base-FB) para todos los conjuntos de datos combinados. Las consultas se agrupan por el % de cambio en AP en comparación con la consulta no expandida. Colección DS + QV DS + No QV TREC 1&2 AvgP 0.2406 0.2547 (+5.86%) P10 0.5263 0.5362 (+1.88%) RI 0.7087 0.6515 (-0.0572) TREC 7 AvgP 0.2169 0.2200 (+1.43%) P10 0.4480 0.4300 (-4.02%) RI 0.5652 0.2609 (-0.3043) TREC 8 AvgP 0.2268 0.2257 (-0.49%) P10 0.4340 0.4200 (-3.23%) RI 0.4545 0.4091 (-0.0454) wt10g AvgP 0.1946 0.1865 (-4.16%) P10 0.2960 0.2680 (-9.46%) RI 0.1429 0.0220 (-0.1209) Tabla 3: Comparación de la retroalimentación de remuestreo utilizando muestreo de documentos (DS) con (QV) y sin (No QV) combinar modelos de retroalimentación de múltiples variantes de consulta. La Tabla 2 muestra los puntajes del Índice de Robustez para Base-FB y RS-FB. El método de retroalimentación RS-FB obtuvo una mayor robustez que Base-FB en tres de los cuatro conjuntos de temas, con solo un rendimiento ligeramente peor en TREC-8. Una vista más detallada que muestra la distribución de los cambios relativos en AP se presenta en el histograma de la Figura 4. En comparación con Base-FB, el método RS-FB logra una reducción notable en el número de consultas significativamente afectadas por la expansión (es decir, donde AP se ve afectado en un 25% o más), al tiempo que conserva ganancias positivas en AP. 3.5 Efecto de los métodos de muestreo de consultas y documentos Dada la mayor robustez de nuestros algoritmos observada en la Sección 3.4, una pregunta importante es qué componente de nuestro sistema es responsable. ¿Es el uso de la re-muestreo de documentos, el uso de múltiples variantes de consulta, o algún otro factor? Los resultados en la Tabla 3 sugieren que la combinación de modelos basada en variantes de consulta puede ser en gran medida responsable de la mayor robustez. Cuando se desactivan las variantes de consulta y se utiliza la consulta original por sí sola con muestreo de documentos, hay poco cambio neto en la precisión promedio, una pequeña disminución en P10 para 3 de los 4 conjuntos de temas, pero una caída significativa en la robustez para todos los conjuntos de temas. En dos casos, la medida de RI disminuye en más del 50%. También examinamos el efecto del método de muestreo de documentos en la efectividad de la recuperación, utilizando dos estrategias diferentes. La estrategia de ponderación uniforme ignoró las puntuaciones de relevancia de la recuperación inicial y otorgó a cada documento en los primeros k la misma probabilidad de selección. Por el contrario, la estrategia de ponderación de puntajes de relevancia seleccionó documentos con una probabilidad proporcional a sus puntajes de relevancia. De esta manera, los documentos que tenían una clasificación más alta tenían más probabilidades de ser seleccionados. Los resultados se muestran en la Tabla 4. La estrategia de ponderación de la puntuación de relevancia tiene un mejor rendimiento en general, con puntajes de RI y P10 significativamente más altos en 3 de los 4 conjuntos de temas. La diferencia en la precisión promedio entre los métodos, sin embargo, es menos marcada. Esto sugiere que el peso uniforme actúa para aumentar la varianza en los resultados de recuperación: cuando la precisión promedio inicial es alta, hay muchos documentos relevantes en los primeros k y el muestreo uniforme puede proporcionar un modelo de relevancia más representativo que centrarse en los elementos altamente clasificados. Por otro lado, cuando la precisión inicial es baja, hay pocos documentos relevantes en las posiciones inferiores y el muestreo uniforme mezcla más de los documentos no relevantes. Por razones de espacio, solo resumimos nuestros hallazgos sobre el tamaño de la muestra aquí. El número de muestras tiene cierto efecto en la precisión cuando es menor a 10, pero el rendimiento se estabiliza alrededor de 15 a 20 muestras. Utilizamos 30 muestras para nuestros experimentos. Mucho más allá de este nivel, los beneficios adicionales de más muestras disminuyen a medida que la distribución inicial de puntajes se ajusta más estrechamente y aumenta el tiempo de procesamiento. 3.6 El efecto del remuestreo en la calidad del término de expansión Idealmente, un modelo de recuperación no debería requerir una lista de palabras vacías al estimar un modelo de relevancia: un modelo estadístico robusto debería reducir automáticamente el peso de las palabras vacías dependiendo del contexto. Las palabras vacías pueden perjudicar la retroalimentación si se seleccionan como términos de retroalimentación, ya que suelen ser malos discriminadores y desperdician espacios de términos valiosos. En la práctica, sin embargo, debido a que la mayoría de los métodos de selección de términos se asemejan a un tipo de ponderación tf · idf, a veces se pueden seleccionar términos con un idf bajo pero un tf muy alto como candidatos de términos de expansión. Esto sucede, por ejemplo, incluso con el enfoque del Modelo de Relevancia que forma parte de nuestra retroalimentación base. Para garantizar una línea base lo más sólida posible, utilizamos una lista de paradas para todos los experimentos reportados aquí. Si desactivamos la lista de palabras vacías, sin embargo, obtenemos resultados como los mostrados en la Tabla 5 donde cuatro de los diez términos de retroalimentación básica principales para el tema 60 de TREC (dijo, pero, su, no) son palabras vacías utilizando el método BaseFB. (Se seleccionaron los 100 términos de expansión principales para generar este ejemplo). El método Indris intenta abordar el problema de las palabras vacías aplicando un paso inicial basado en Ponte [14] para seleccionar términos menos comunes que tienen altas probabilidades logarítmicas de estar en los documentos mejor clasificados en comparación con toda la colección. Sin embargo, esto no resuelve completamente el problema de las palabras vacías, especialmente a medida que aumenta el número de términos de retroalimentación. Sin embargo, el uso de retroalimentación de remuestreo parece mitigar la ponderación de la colección QV + QV uniforme + ponderación de puntaje de relevancia TREC 1&2 AvgP 0.2545 0.2406 (-5.46%) P10 0.5369 0.5263 (-1.97%) RI 0.6212 0.7087 (+14.09%) TREC 7 AvgP 0.2174 0.2169 (-0.23%) P10 0.4320 0.4480 (+3.70%) RI 0.4783 0.5652 (+18.17%) TREC 8 AvgP 0.2267 0.2268 (+0.04%) P10 0.4120 0.4340 (+5.34%) RI 0.4545 0.4545 (+0.00%) wt10g AvgP 0.1808 0.1946 (+7.63%) P10 0.2680 0.2960 (+10.45%) RI 0.0220 0.1099 (+399.5%) Tabla 4: Comparación de muestreo de documentos uniforme y ponderado por relevancia. El cambio porcentual en comparación con el muestreo uniforme se muestra entre paréntesis. QV indica que se utilizaron variantes de consulta en ambas ejecuciones. FB de línea base p(wi|R) FB de remuestreo p(wi|R) dijo 0.055 corte 0.026 corte 0.055 pagar 0.018 pagar 0.034 federal 0.012 pero 0.026 educación 0.011 empleados 0.024 maestros 0.010 sus 0.024 empleados 0.010 no 0.023 caso 0.010 federal 0.021 sus 0.009 trabajadores 0.020 apelaciones 0.008 educación 0.020 sindicato 0.007 Tabla 5: Calidad del término de retroalimentación cuando no se utiliza una lista de paradas. Términos de retroalimentación para el tema 60 de TREC: pago por mérito vs antigüedad. el efecto de las palabras vacías automáticamente. En el ejemplo de la Tabla 5, la retroalimentación de remuestreo deja solo una palabra vacía (su) en las diez primeras posiciones. Observamos un comportamiento similar del término de retroalimentación en muchos otros temas. La razón de este efecto parece ser la interacción de la puntuación de selección de términos con el límite superior de términos m. Si bien la presencia e incluso la proporción de palabras vacías particulares es bastante estable en diferentes muestras de documentos, su posición relativa en la lista de los primeros m elementos no lo es, ya que se examinan conjuntos de documentos con diferentes números de candidatos de términos mejores y de menor frecuencia para cada muestra. Como resultado, si bien algunos stopwords pueden aparecer en cada conjunto de documentos muestreados, tiende a ocurrir que cualquier stopword dado caiga por debajo del umbral para múltiples muestras, lo que lleva a su clasificación como una característica de alta varianza y bajo peso. 4. TRABAJO RELACIONADO Nuestro enfoque está relacionado con trabajos previos de varias áreas de recuperación de información y aprendizaje automático. Nuestro uso de la variación de consultas fue inspirado por el trabajo de YomTov et al. [20], Carpineto et al. [5] y Amati et al. [2], entre otros. Estos estudios utilizan la idea de crear múltiples subconsultas y luego examinar la naturaleza de la superposición en los documentos y/o términos de expansión que resultan de cada subconsulta. La combinación de modelos se realiza utilizando heurísticas. En particular, los estudios de Amati et al. y Carpineto et al. investigaron la combinación de términos de métodos de distribución individuales utilizando una heurística de combinación de reordenamiento de términos. En un conjunto de temas de TREC encontraron una amplia variación promedio en la distancia de rango de los términos de diferentes métodos de expansión. Su método de combinación proporcionó modestas mejoras positivas en la precisión promedio. La idea de examinar la superposición entre listas de términos sugeridos también ha sido utilizada en enfoques tempranos de expansión de consultas. El método de Análisis de Contexto Local (LCA) de Xu y Crofts [19] incluye un factor en la fórmula de ponderación derivada empíricamente que hace que se prefieran los términos de expansión que tienen conexiones con múltiples términos de consulta. En el lado del documento, el trabajo reciente de Zhou & Croft [21] exploró la idea de agregar ruido a los documentos, volver a puntuarlos y utilizar la estabilidad de las clasificaciones resultantes como una estimación de la dificultad de la consulta. Esto está relacionado con nuestro uso de muestreo de documentos para estimar el riesgo del modelo de retroalimentación construido a partir de los diferentes conjuntos de documentos recuperados en la parte superior. Sakai et al. [17] propusieron un enfoque para mejorar la robustez de la <br>retroalimentación de relevancia pseudo</br> utilizando un método que llaman muestreo selectivo. La esencia de su método es que permiten omitir algunos documentos de alta clasificación, basándose en un criterio de agrupación, para seleccionar un conjunto de documentos más variado e innovador más adelante en la clasificación para su uso por un método tradicional de retroalimentación pseudo. Su estudio no encontró mejoras significativas ni en robustez (RI) ni en MAP en sus corpora. Greiff, Morgan y Ponte [8] exploraron el papel de la varianza en la ponderación de términos. En una serie de simulaciones que simplificaron el problema a documentos de 2 características, encontraron que la precisión promedio disminuye a medida que la varianza de la frecuencia de términos - ruido alto - aumenta. La reducción del peso de los términos con alta varianza resultó en una mejora de la precisión promedio. Esto parece estar en concordancia con nuestros propios hallazgos para los modelos de retroalimentación individual. Las estimaciones de la varianza de la producción han sido utilizadas recientemente para mejorar la clasificación de textos. Lee et al. [11] utilizaron estimaciones de varianza específicas de la consulta de las salidas del clasificador para realizar una combinación de modelos mejorada. En lugar de utilizar muestreo, pudieron derivar expresiones en forma cerrada para la varianza del clasificador asumiendo clasificadores base utilizando tipos simples de redes de inferencia. Ando y Zhang propusieron un método que ellos llaman retroalimentación estructural [3] y mostraron cómo aplicarlo a la expansión de consultas para la pista de Genómica de TREC. Utilizaron variaciones de consultas r para obtener R conjuntos diferentes Sr de documentos mejor clasificados que se han intersectado con los documentos mejor clasificados obtenidos de la consulta original qorig. Para cada Si, se calcula el vector centróide normalizado ˆwi de los documentos. El análisis de componentes principales (PCA) se aplica luego a los ˆwi para obtener la matriz Φ de los vectores singulares izquierdos φh que se utilizan para obtener la nueva consulta expandida qexp = qorig + ΦT Φqorig. (7) En el caso de H = 1, tenemos un único vector singular izquierdo φ: qexp = qorig + (φT qorig)φ, de modo que el producto punto φT qorig es un tipo de peso dinámico en la consulta expandida que se basa en la similitud de la consulta original con la consulta expandida. El uso de la varianza como medida de calidad del modelo de retroalimentación ocurre de forma indirecta a través de la aplicación de PCA. Sería interesante estudiar las conexiones entre este enfoque y nuestro propio método de ajuste de modelos. Finalmente, en los enfoques de modelado del lenguaje para retroalimentación, Tao y Zhai [18] describen un método para una retroalimentación más robusta que permite que cada documento tenga un α de retroalimentación diferente. Los pesos de retroalimentación se derivan automáticamente utilizando EM regularizado. La condición de parada de EM implica un equilibrio aproximadamente igual entre el modelo de consulta y expansión. Proponen adaptar el parámetro de parada η basado en una función de alguna medida de calidad de los documentos de retroalimentación. CONCLUSIONES Hemos presentado un nuevo enfoque para la <br>retroalimentación de relevancia pseudo</br> basado en el muestreo de documentos y consultas. El uso del muestreo es un dispositivo muy flexible y poderoso, motivado por nuestro deseo general de ampliar los modelos actuales de recuperación mediante la estimación del riesgo o la varianza asociada con los parámetros o la salida de los procesos de recuperación. Tales estimaciones de varianza, por ejemplo, pueden ser utilizadas de forma natural en un marco bayesiano para una mejor estimación y combinación de modelos. Aplicaciones como la expansión selectiva pueden ser implementadas de manera fundamentada. Si bien nuestro estudio utiliza el enfoque de modelado del lenguaje como marco para experimentos, hacemos pocas suposiciones sobre el funcionamiento real del algoritmo de retroalimentación. Creemos que es probable que cualquier algoritmo de retroalimentación de línea base razonablemente efectivo se beneficiaría de nuestro enfoque. Nuestros resultados en colecciones estándar de TREC muestran que nuestro marco mejora la robustez de un método de retroalimentación de referencia sólido en una variedad de colecciones, sin sacrificar la precisión promedio. También proporciona pequeñas pero consistentes mejoras en la precisión del top 10. En trabajos futuros, visualizamos una investigación sobre cómo variar el conjunto de métodos de muestreo utilizados y el número de muestras controla el equilibrio entre robustez, precisión y eficiencia. Agradecimientos Agradecemos a Paul Bennett por las valiosas discusiones relacionadas con este trabajo, el cual fue apoyado por las becas de la NSF #IIS-0534345 y #CNS-0454018, y la beca del Departamento de Educación de los EE. UU. #R305G03123. Cualquier opinión, hallazgo, conclusión o recomendación expresada en este material son del autor y no necesariamente reflejan las de los patrocinadores. REFERENCIAS [1] El kit de herramientas Lemur para modelado de lenguaje y recuperación. http://www.lemurproject.org. [2] G. Amati, C. Carpineto y G. Romano. Dificultad de la consulta, robustez y aplicación selectiva de la expansión de consultas. En Actas de la 25ª Conferencia Europea sobre Recuperación de Información (ECIR 2004), páginas 127-137. [3] R. K. Ando y T. Zhang. Un método de aprendizaje semisupervisado de alto rendimiento para segmentación de texto. En Actas de la 43ª Reunión Anual de la ACL, páginas 1-9, junio de 2005. [4] L. Breiman. Empaquetando predictores. Aprendizaje automático, 24(2):123-140, 1996. [5] C. Carpineto, G. Romano y V. Giannini. Mejorando la retroalimentación de recuperación con la combinación de múltiples funciones de clasificación de términos. ACM Trans. Información. Sistemas, 20(3):259 - 290. [6] K. Collins-Thompson, P. Ogilvie y J. Callan. Resultados iniciales con consultas estructuradas y modelos de lenguaje en medio terabyte de texto. En Actas de la Conferencia de Recuperación de Información de Texto de 2005. Publicación Especial del NIST. [7] R. O. Duda, P. E. Hart y D. G. Stork. Clasificación de patrones. Wiley and Sons, 2da edición, 2001. [8] W. R. Greiff, W. T. Morgan y J. M. Ponte. El papel de la varianza en la ponderación de términos para la recuperación de información probabilística. En Actas de la 11ª Conferencia Internacional. Conf. sobre Gestión de Información y Conocimiento (CIKM 2002), páginas 252-259. [9] T. Kohonen, J. Hynninen, J. Kangas y J. Laaksonen. SOMPAK: El paquete de programas de mapas autoorganizados. Informe técnico A31, Universidad de Tecnología de Helsinki, 1996. http://www.cis.hut.fi/research/papers/som tr96.ps.Z. [10] V. Lavrenko. Una teoría generativa de relevancia. Tesis doctoral, Universidad de Massachusetts, Amherst, 2004. [11] C.-H. Lee, R. Greiner y S. Wang. Utilizando estimaciones de varianza específicas de la consulta para combinar clasificadores bayesianos. En Actas del 23º Congreso Internacional. Conf. sobre Aprendizaje Automático (ICML 2006), páginas 529-536. [12] D. Metzler y W. B. Croft. Combinando el modelo de lenguaje y los enfoques de red de inferencia para la recuperación. Información. Procesamiento y Gestión, 40(5):735-750, 2004. [13] T. Minka. Estimando una distribución de Dirichlet. Informe técnico, 2000. http://research.microsoft.com/minka/papers/dirichlet. [14] J. Ponte. Avances en la Recuperación de Información, capítulo Modelos de lenguaje para retroalimentación de relevancia, páginas 73-96. 2000. W.B. Croft, ed. [15] J. M. Ponte y W. B. Croft. Un enfoque de modelado del lenguaje para la recuperación de información. En Actas de la Conferencia ACM SIGIR de 1998 sobre Investigación y Desarrollo en Recuperación de Información, páginas 275-281. [16] J. Rocchio. El Sistema de Recuperación SMART, capítulo Retroalimentación de Relevancia en la Recuperación de Información, páginas 313-323. Prentice-Hall, 1971. G. Salton, ed. [17] T. Sakai, T. Manabe y M. Koyama. Retroalimentación de <br>pseudo-relevancia</br> flexible a través de muestreo selectivo. ",
            "candidates": [],
            "error": [
                [
                    "pseudo-relevancia",
                    "retroalimentación de pseudo relevancia",
                    "retroalimentación de relevancia pseudo",
                    "retroalimentación de relevancia pseudo",
                    "pseudo-relevancia"
                ]
            ]
        },
        "vector space-based algorithm": {
            "translated_key": "algoritmos basados en espacio vectorial",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Estimation and Use of Uncertainty in Pseudo-relevance Feedback Kevyn Collins-Thompson and Jamie Callan Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213-8213 U.S.A. {kct | callan}@cs.cmu.edu ABSTRACT Existing pseudo-relevance feedback methods typically perform averaging over the top-retrieved documents, but ignore an important statistical dimension: the risk or variance associated with either the individual document models, or their combination.",
                "Treating the baseline feedback method as a black box, and the output feedback model as a random variable, we estimate a posterior distribution for the feedback model by resampling a given querys top-retrieved documents, using the posterior mean or mode as the enhanced feedback model.",
                "We then perform model combination over several enhanced models, each based on a slightly modified query sampled from the original query.",
                "We find that resampling documents helps increase individual feedback model precision by removing noise terms, while sampling from the query improves robustness (worst-case performance) by emphasizing terms related to multiple query aspects.",
                "The result is a meta-feedback algorithm that is both more robust and more precise than the original strong baseline method.",
                "Categories and Subject Descriptors: H.3.3 [Information Retrieval]: Retrieval Models General Terms: Algorithms, Experimentation 1.",
                "INTRODUCTION Uncertainty is an inherent feature of information retrieval.",
                "Not only do we not know the queries that will be presented to our retrieval algorithm ahead of time, but the users information need may be vague or incompletely specified by these queries.",
                "Even if the query were perfectly specified, language in the collection documents is inherently complex and ambiguous and matching such language effectively is a formidable problem by itself.",
                "With this in mind, we wish to treat many important quantities calculated by the retrieval system, whether a relevance score for a document, or a weight for a query expansion term, as random variables whose true value is uncertain but where the uncertainty about the true value may be quantified by replacing the fixed value with a probability distribution over possible values.",
                "In this way, retrieval algorithms may attempt to quantify the risk or uncertainty associated with their output rankings, or improve the stability or precision of their internal calculations.",
                "Current algorithms for pseudo-relevance feedback (PRF) tend to follow the same basic method whether we use <br>vector space-based algorithm</br>s such as Rocchios formula [16], or more recent language modeling approaches such as Relevance Models [10].",
                "First, a set of top-retrieved documents is obtained from an initial query and assumed to approximate a set of relevant documents.",
                "Next, a single feedback model vector is computed according to some sort of average, centroid, or expectation over the set of possibly-relevant document models.",
                "For example, the document vectors may be combined with equal weighting, as in Rocchio, or by query likelihood, as may be done using the Relevance Model1 .",
                "The use of an expectation is reasonable for practical and theoretical reasons, but by itself ignores potentially valuable information about the risk of the feedback model.",
                "Our main hypothesis in this paper is that estimating the uncertainty in feedback is useful and leads to better individual feedback models and more robust combined models.",
                "Therefore, we propose a method for estimating uncertainty associated with an individual feedback model in terms of a posterior distribution over language models.",
                "To do this, we systematically vary the inputs to the baseline feedback method and fit a Dirichlet distribution to the output.",
                "We use the posterior mean or mode as the improved feedback model estimate.",
                "This process is shown in Figure 1.",
                "As we show later, the mean and mode may vary significantly from the single feedback model proposed by the baseline method.",
                "We also perform model combination using several improved feedback language models obtained by a small number of new queries sampled from the original query.",
                "A models weight combines two complementary factors: the models probability of generating the query, and the variance of the model, with high-variance models getting lower weight. 1 For example, an expected parameter vector conditioned on the query observation is formed from top-retrieved documents, which are treated as training strings (see [10], p. 62).",
                "Figure 1: Estimating the uncertainty of the feedback model for a single query. 2.",
                "SAMPLING-BASED FEEDBACK In Sections 2.1-2.5 we describe a general method for estimating a probability distribution over the set of possible language models.",
                "In Sections 2.6 and 2.7 we summarize how different query samples are used to generate multiple feedback models, which are then combined. 2.1 Modeling Feedback Uncertainty Given a query Q and a collection C, we assume a probabilistic retrieval system that assigns a real-valued document score f(D, Q) to each document D in C, such that the score is proportional to the estimated probability of relevance.",
                "We make no other assumptions about f(D, Q).",
                "The nature of f(D, Q) may be complex: for example, if the retrieval system supports structured query languages [12], then f(D, Q) may represent the output of an arbitrarily complex inference network defined by the structured query operators.",
                "In theory, the scoring function can vary from query to query, although in this study for simplicity we keep the scoring function the same for all queries.",
                "Our specific query method is given in Section 3.",
                "We treat the feedback algorithm as a black box and assume that the inputs to the feedback algorithm are the original query and the corresponding top-retrieved documents, with a score being given to each document.",
                "We assume that the output of the feedback algorithm is a vector of term weights to be used to add or reweight the terms in the representation of the original query, with the vector normalized to form a probability distribution.",
                "We view the the inputs to the feedback black box as random variables, and analyze the feedback model as a random variable that changes in response to changes in the inputs.",
                "Like the document scoring function f(D, Q), the feedback algorithm may implement a complex, non-linear scoring formula, and so as its inputs vary, the resulting feedback models may have a complex distribution over the space of feedback models (the sample space).",
                "Because of this potential complexity, we do not attempt to derive a posterior distribution in closed form, but instead use simulation.",
                "We call this distribution over possible feedback models the feedback model distribution.",
                "Our goal in this section is to estimate a useful approximation to the feedback model distribution.",
                "For a specific framework for experiments, we use the language modeling (LM) approach for information retrieval [15].",
                "The score of a document D with respect to a query Q and collection C is given by p(Q|D) with respect to language models ˆθQ and ˆθD estimated for the query and document respectively.",
                "We denote the set of k top-retrieved documents from collection C in response to Q by DQ(k, C).",
                "For simplicity, we assume that queries and documents are generated by multinomial distributions whose parameters are represented by unigram language models.",
                "To incorporate feedback in the LM approach, we assume a model-based scheme in which our goal is take the query and resulting ranked documents DQ(k, C) as input, and output an expansion language model ˆθE, which is then interpolated with the original query model ˆθQ: ˆθNew = (1 − α) · ˆθQ + α · ˆθE (1) This includes the possibility of α = 1 where the original query mode is completely replaced by the feedback model.",
                "Our sample space is the set of all possible language models LF that may be output as feedback models.",
                "Our approach is to take samples from this space and then fit a distribution to the samples using maximum likelihood.",
                "For simplicity, we start by assuming the latent feedback distribution has the form of a Dirichlet distribution.",
                "Although the Dirichlet is a unimodal distribution, and in general quite limited in its expressiveness in the sample space, it is a natural match for the multinomial language model, can be estimated quickly, and can capture the most salient features of confident and uncertain feedback models, such as the overall spread of the distibution. 2.2 Resampling document models We would like an approximation to the posterior distribution of the feedback model LF .",
                "To accomplish this, we apply a widely-used simulation technique called bootstrap sampling ([7], p. 474) on the input parameters, namely, the set of top-retrieved documents.",
                "Bootstrap sampling allows us to simulate the approximate effect of perturbing the parameters within the black box feedback algorithm by perturbing the inputs to that algorithm in a systematic way, while making no assumptions about the nature of the feedback algorithm.",
                "Specifically, we sample k documents with replacement from DQ(k, C), and calculate an expansion language model θb using the black box feedback method.",
                "We repeat this process B times to obtain a set of B feedback language models, to which we then fit a Dirichlet distribution.",
                "Typically B is in the range of 20 to 50 samples, with performance being relatively stable in this range.",
                "Note that instead of treating each top document as equally likely, we sample according to the estimated probabilities of relevance of each document in DQ(k, C).",
                "Thus, a document is more likely to be chosen the higher it is in the ranking. 2.3 Justification for a sampling approach The rationale for our sampling approach has two parts.",
                "First, we want to improve the quality of individual feedback models by smoothing out variation when the baseline feedback model is unstable.",
                "In this respect, our approach resembles bagging [4], an ensemble approach which generates multiple versions of a predictor by making bootstrap copies of the training set, and then averages the (numerical) predictors.",
                "In our application, top-retrieved documents can be seen as a kind of noisy training set for relevance.",
                "Second, sampling is an effective way to estimate basic properties of the feedback posterior distribution, which can then be used for improved model combination.",
                "For example, a model may be weighted by its prediction confidence, estimated as a function of the variability of the posterior around the model. foo2-401.map-Dim:5434,Size:12*12units,gaussianneighborhood (a) Topic 401 Foreign minorities, Germany foo2-402.map-Dim:5698,Size:12*12units,gaussianneighborhood (b) Topic 402 Behavioral genetics foo2-459.map-Dim:8969,Size:12*12units,gaussianneighborhood (c) Topic 459 When can a lender foreclose on property Figure 2: Visualization of expansion language model variance using self-organizing maps, showing the distribution of language models that results from resampling the inputs to the baseline expansion method.",
                "The language model that would have been chosen by the baseline expansion is at the center of each map.",
                "The similarity function is JensenShannon divergence. 2.4 Visualizing feedback distributions Before describing how we fit and use the Dirichlet distribution over feedback models, it is instructive to view some examples of actual feedback model distributions that result from bootstrap sampling the top-retrieved documents from different TREC topics.",
                "Each point in our sample space is a language model, which typically has several thousand dimensions.",
                "To help analyze the behavior of our method we used a Self-Organizing Map (via the SOM-PAK package [9]), to flatten and visualize the high-dimensional density function2 .",
                "The density maps for three TREC topics are shown in Figure 2 above.",
                "The dark areas represent regions of high similarity between language models.",
                "The light areas represent regions of low similarity - the valleys between clusters.",
                "Each diagram is centered on the language model that would have been chosen by the baseline expansion.",
                "A single peak (mode) is evident in some examples, but more complex structure appears in others.",
                "Also, while the distribution is usually close to the baseline feedback model, for some topics they are a significant distance apart (as measured by JensenShannon divergence), as in Subfigure 2c.",
                "In such cases, the mode or mean of the feedback distribution often performs significantly better than the baseline (and in a smaller proportion of cases, significantly worse). 2.5 Fitting a posterior feedback distribution After obtaining feedback model samples by resampling the feedback model inputs, we estimate the feedback distribution.",
                "We assume that the multinomial feedback models {ˆθ1, . . . , ˆθB} were generated by a latent Dirichlet distribution with parameters {α1, . . . , αN }.",
                "To estimate the {α1, . . . , αN }, we fit the Dirichlet parameters to the B language model samples according to maximum likelihood using a generalized Newton procedure, details of which are given in Minka [13].",
                "We assume a simple Dirichlet prior over the {α1, . . . , αN }, setting each to αi = μ · p(wi | C), where μ is a parameter and p(· | C) is the collection language model estimated from a set of documents from collection C. The parameter fitting converges very quickly - typically just 2 or 2 Because our points are language models in the multinomial simplex, we extended SOM-PAK to support JensenShannon divergence, a widely-used similarity measure between probability distributions. 3 iterations are enough - so that it is practical to apply at query-time when computational overhead must be small.",
                "In practice, we can restrict the calculation to the vocabulary of the top-retrieved documents, instead of the entire collection.",
                "Note that for this step we are re-using the existing retrieved documents and not performing additional queries.",
                "Given the parameters of an N-dimensional Dirichlet distribution Dir(α) the mean μ and mode x vectors are easy to calculate and are given respectively by μi = αiP αi (2) and xi = αi−1P αi−N . (3) We can then choose the language model at the mean or the mode of the posterior as the final enhanced feedback model. (We found the mode to give slightly better performance.)",
                "For information retrieval, the number of samples we will have available is likely to be quite small for performance reasons - usually less than ten.",
                "Moreover, while random sampling is useful in certain cases, it is perfectly acceptable to allow deterministic sampling distributions, but these must be designed carefully in order to approximate an accurate output variance.",
                "We leave this for future study. 2.6 Query variants We use the following methods for generating variants of the original query.",
                "Each variant corresponds to a different assumption about which aspects of the original query may be important.",
                "This is a form of deterministic sampling.",
                "We selected three simple methods that cover complimentary assumptions about the query.",
                "No-expansion Use only the original query.",
                "The assumption is that the given terms are a complete description of the information need.",
                "Leave-one-out A single term is left out of the original query.",
                "The assumption is that one of the query terms is a noise term.",
                "Single-term A single term is chosen from the original query.",
                "This assumes that only one aspect of the query, namely, that represented by the term, is most important.",
                "After generating a variant of the original query, we combine it with the original query using a weight αSUB so that we do not stray too far.",
                "In this study, we set αSUB = 0.5.",
                "For example, using the Indri [12] query language, a leave-oneout variant of the initial query that omits the term ireland for TREC topic 404 is: #weight(0.5 #combine(ireland peace talks) 0.5 #combine(peace talks)) 2.7 Combining enhanced feedback models from multiple query variants When using multiple query variants, the resulting enhanced feedback models are combined using Bayesian model combination.",
                "To do this, we treat each word as an item to be classified as belonging to a relevant or non-relevant class, and derive a class probability for each word by combining the scores from each query variant.",
                "Each score is given by that terms probability in the Dirichlet distribution.",
                "The term scores are weighted by the inverse of the variance of the term in the enhanced feedback models Dirichlet distribution.",
                "The prior probability of a words membership in the relevant class is given by the probability of the original query in the entire enhanced expansion model. 3.",
                "EVALUATION In this section we present results confirming the usefulness of estimating a feedback model distribution from weighted resampling of top-ranked documents, and of combining the feedback models obtained from different small changes in the original query. 3.1 General method We evaluated performance on a total of 350 queries derived from four sets of TREC topics: 51-200 (TREC-1&2), 351-400 (TREC-7), 401-450 (TREC-8), and 451-550 (wt10g, TREC-9&10).",
                "We chose these for their varied content and document properties.",
                "For example, wt10g documents are Web pages with a wide variety of subjects and styles while TREC-1&2 documents are more homogeneous news articles.",
                "Indexing and retrieval was performed using the Indri system in the Lemur toolkit [12] [1].",
                "Our queries were derived from the words in the title field of the TREC topics.",
                "Phrases were not used.",
                "To generate the baseline queries passed to Indri, we wrapped the query terms with Indris #combine operator.",
                "For example, the initial query for topic 404 is: #combine(ireland peace talks) We performed Krovetz stemming for all experiments.",
                "Because we found that the baseline (Indri) expansion method performed better using a stopword list with the feedback model, all experiments used a stoplist of 419 common English words.",
                "However, an interesting side-effect of our resampling approach is that it tends to remove many stopwords from the feedback model, making a stoplist less critical.",
                "This is discussed further in Section 3.6. 3.2 Baseline feedback method For our baseline expansion method, we use an algorithm included in Indri 1.0 as the default expansion method.",
                "This method first selects terms using a log-odds calculation described by Ponte [14], but assigns final term weights using Lavrenkos relevance model[10].",
                "We chose the Indri method because it gives a consistently strong baseline, is based on a language modeling approach, and is simple to experiment with.",
                "In a TREC evaluation using the GOV2 corpus [6], the method was one of the topperforming runs, achieving a 19.8% gain in MAP compared to using unexpanded queries.",
                "In this study, it achieves an average gain in MAP of 17.25% over the four collections.",
                "Indris expansion method first calculates a log-odds ratio o(v) for each potential expansion term v given by o(v) = X D log p(v|D) p(v|C) (4) over all documents D containing v, in collection C. Then, the expansion term candidates are sorted by descending o(v), and the top m are chosen.",
                "Finally, the term weights r(v) used in the expanded query are calculated based on the relevance model r(v) = X D p(q|D)p(v|D) p(v) p(D) (5) The quantity p(q|D) is the probability score assigned to the document in the initial retrieval.",
                "We use Dirichlet smoothing of p(v|D) with μ = 1000.",
                "This relevance model is then combined with the original query using linear interpolation, weighted by a parameter α.",
                "By default we used the top 50 documents for feedback and the top 20 expansion terms, with the feedback interpolation parameter α = 0.5 unless otherwise stated.",
                "For example, the baseline expanded query for topic 404 is: #weight(0.5 #combine(ireland peace talks) 0.5 #weight(0.10 ireland 0.08 peace 0.08 northern ...) 3.3 Expansion performance We measure our feedback algorithms effectiveness by two main criteria: precision, and robustness.",
                "Robustness, and the tradeoff between precision and robustness, is analyzed in Section 3.4.",
                "In this section, we examine average precision and precision in the top 10 documents (P10).",
                "We also include recall at 1,000 documents.",
                "For each query, we obtained a set of B feedback models using the Indri baseline.",
                "Each feedback model was obtained from a random sample of the top k documents taken with replacement.",
                "For these experiments, B = 30 and k = 50.",
                "Each feedback model contained 20 terms.",
                "On the query side, we used leave-one-out (LOO) sampling to create the query variants.",
                "Single-term query sampling had consistently worse performance across all collections and so our results here focus on LOO sampling.",
                "We used the methods described in Section 2 to estimate an enhanced feedback model from the Dirichlet posterior distribution for each query variant, and to combine the feedback models from all the query variants.",
                "We call our method resampling expansion and denote it as RS-FB here.",
                "We denote the Indri baseline feedback method as Base-FB.",
                "Results from applying both the baseline expansion method (Base-FB) and resampling expansion (RS-FB) are shown in Table 1.",
                "We observe several trends in this table.",
                "First, the average precision of RS-FB was comparable to Base-FB, achieving an average gain of 17.6% compared to using no expansion across the four collections.",
                "The Indri baseline expansion gain was 17.25%.",
                "Also, the RS-FB method achieved consistent improvements in P10 over Base-FB for every topic set, with an average improvement of 6.89% over Base-FB for all 350 topics.",
                "The lowest P10 gain over Base-FB was +3.82% for TREC-7 and the highest was +11.95% for wt10g.",
                "Finally, both Base-FB and RS-FB also consistently improved recall over using no expansion, with Base-FB achieving better recall than RS-FB for all topic sets. 3.4 Retrieval robustness We use the term robustness to mean the worst-case average precision performance of a feedback algorithm.",
                "Ideally, a robust feedback method would never perform worse than using the original query, while often performing better using the expansion.",
                "To evaluate robustness in this study, we use a very simple measure called the robustness index (RI)3 .",
                "For a set of queries Q, the RI measure is defined as: RI(Q) = n+ − n− |Q| (6) where n+ is the number of queries helped by the feedback method and n− is the number of queries hurt.",
                "Here, by helped we mean obtaining a higher average precision as a result of feedback.",
                "The value of RI ranges from a minimum 3 This is sometimes also called the reliability of improvement index and was used in Sakai et al. [17].",
                "Collection NoExp Base-FB RS-FB TREC 1&2 AvgP 0.1818 0.2419 (+33.04%) 0.2406 (+32.24%) P10 0.4443 0.4913 (+10.57%) 0.5363 (+17.83%) Recall 15084/37393 19172/37393 15396/37393 TREC 7 AvgP 0.1890 0.2175 (+15.07%) 0.2169 (+14.75%) P10 0.4200 0.4320 (+2.85%) 0.4480 (+6.67%) Recall 2179/4674 2608/4674 2487/4674 TREC 8 AvgP 0.2031 0.2361 (+16.25%) 0.2268 (+11.70%) P10 0.3960 0.4160 (+5.05%) 0.4340 (+9.59%) Recall 2144/4728 2642/4728 2485/4728 wt10g AvgP 0.1741 0.1829 (+5.06%) 0.1946 (+11.78%) P10 0.2760 0.2630 (-4.71%) 0.2960 (+7.24%) Recall 3361/5980 3725/5980 3664/5980 Table 1: Comparison of baseline (Base-FB) feedback and feedback using re-sampling (RS-FB).",
                "Improvement shown for BaseFB and RS-FB is relative to using no expansion. (a) TREC 1&2 (upper curve); TREC 8 (lower curve) (b) TREC 7 (upper curve); wt10g (lower curve) Figure 3: The trade-off between robustness and average precision for different corpora.",
                "The x-axis gives the change in MAP over using baseline expansion with α = 0.5.",
                "The yaxis gives the Robustness Index (RI).",
                "Each curve through uncircled points shows the RI/MAP tradeoff using the simple small-α strategy (see text) as α decreases from 0.5 to zero in the direction of the arrow.",
                "Circled points represent the tradeoffs obtained by resampling feedback for α = 0.5.",
                "Collection N Base-FB RS-FB n− RI n− RI TREC 1&2 103 26 +0.495 15 +0.709 TREC 7 46 14 +0.391 10 +0.565 TREC 8 44 12 +0.455 12 +0.455 wt10g 91 48 -0.055 39 +0.143 Combined 284 100 +0.296 76 +0.465 Table 2: Comparison of robustness index (RI) for baseline feedback (Base-FB) vs. resampling feedback (RS-FB).",
                "Also shown are the actual number of queries hurt by feedback (n−) for each method and collection.",
                "Queries for which initial average precision was negligible (≤ 0.01) were ignored, giving the remaining query count in column N. of −1.0, when all queries are hurt by the feedback method, to +1.0 when all queries are helped.",
                "The RI measure does not take into account the magnitude or distribution of the amount of change across the set Q.",
                "However, it is easy to understand as a general indication of robustness.",
                "One obvious way to improve the worst-case performance of feedback is simply to use a smaller fixed α interpolation parameter, such as α = 0.3, placing less weight on the (possibly risky) feedback model and more on the original query.",
                "We call this the small-α strategy.",
                "Since we are also reducing the potential gains when the feedback model is right, however, we would expect some trade-off between average precision and robustness.",
                "We therefore compared the precision/robustness trade-off between our resampling feedback algorithm, and the simple small-α method.",
                "The results are summarized in Figure 3.",
                "In the figure, the curve for each topic set interpolates between trade-off points, beginning at x=0, where α = 0.5, and continuing in the direction of the arrow as α decreases and the original query is given more and more weight.",
                "As expected, robustness continuously increases as we move along the curve, but mean average precision generally drops as the gains from feedback are eliminated.",
                "For comparison, the performance of resampling feedback at α = 0.5 is shown for each collection as the circled point.",
                "Higher and to the right is better.",
                "This figure shows that resampling feedback gives a somewhat better trade-off than the small-α approach for 3 of the 4 collections.",
                "Figure 4: Histogram showing improved robustness of resampling feedback (RS-FB) over baseline feedback (Base-FB) for all datasets combined.",
                "Queries are binned by % change in AP compared to the unexpanded query.",
                "Collection DS + QV DS + No QV TREC 1&2 AvgP 0.2406 0.2547 (+5.86%) P10 0.5263 0.5362 (+1.88%) RI 0.7087 0.6515 (-0.0572) TREC 7 AvgP 0.2169 0.2200 (+1.43%) P10 0.4480 0.4300 (-4.02%) RI 0.5652 0.2609 (-0.3043) TREC 8 AvgP 0.2268 0.2257 (-0.49%) P10 0.4340 0.4200 (-3.23%) RI 0.4545 0.4091 (-0.0454) wt10g AvgP 0.1946 0.1865 (-4.16%) P10 0.2960 0.2680 (-9.46%) RI 0.1429 0.0220 (-0.1209) Table 3: Comparison of resampling feedback using document sampling (DS) with (QV) and without (No QV) combining feedback models from multiple query variants.",
                "Table 2 gives the Robustness Index scores for Base-FB and RS-FB.",
                "The RS-FB feedback method obtained higher robustness than Base-FB on three of the four topic sets, with only slightly worse performance on TREC-8.",
                "A more detailed view showing the distribution over relative changes in AP is given by the histogram in Figure 4.",
                "Compared to Base-FB, the RS-FB method achieves a noticable reduction in the number of queries significantly hurt by expansion (i.e. where AP is hurt by 25% or more), while preserving positive gains in AP. 3.5 Effect of query and document sampling methods Given our algorithms improved robustness seen in Section 3.4, an important question is what component of our system is responsible.",
                "Is it the use of document re-sampling, the use of multiple query variants, or some other factor?",
                "The results in Table 3 suggest that the model combination based on query variants may be largely account for the improved robustness.",
                "When query variants are turned off and the original query is used by itself with document sampling, there is little net change in average precision, a small decrease in P10 for 3 out of the 4 topic sets, but a significant drop in robustness for all topic sets.",
                "In two cases, the RI measure drops by more than 50%.",
                "We also examined the effect of the document sampling method on retrieval effectiveness, using two different strategies.",
                "The uniform weighting strategy ignored the relevance scores from the initial retrieval and gave each document in the top k the same probability of selection.",
                "In contrast, the relevance-score weighting strategy chose documents with probability proportional to their relevance scores.",
                "In this way, documents that were more highly ranked were more likely to be selected.",
                "Results are shown in Table 4.",
                "The relevance-score weighting strategy performs better overall, with significantly higher RI and P10 scores on 3 of the 4 topic sets.",
                "The difference in average precision between the methods, however, is less marked.",
                "This suggests that uniform weighting acts to increase variance in retrieval results: when initial average precision is high, there are many relevant documents in the top k and uniform sampling may give a more representative relevance model than focusing on the highly-ranked items.",
                "On the other hand, when initial precision is low, there are few relevant documents in the bottom ranks and uniform sampling mixes in more of the non-relevant documents.",
                "For space reasons we only summarize our findings on sample size here.",
                "The number of samples has some effect on precision when less than 10, but performance stabilizes at around 15 to 20 samples.",
                "We used 30 samples for our experiments.",
                "Much beyond this level, the additional benefits of more samples decrease as the initial score distribution is more closely fit and the processing time increases. 3.6 The effect of resampling on expansion term quality Ideally, a retrieval model should not require a stopword list when estimating a model of relevance: a robust statistical model should down-weight stopwords automatically depending on context.",
                "Stopwords can harm feedback if selected as feedback terms, because they are typically poor discriminators and waste valuable term slots.",
                "In practice, however, because most term selection methods resemble a tf · idf type of weighting, terms with low idf but very high tf can sometimes be selected as expansion term candidates.",
                "This happens, for example, even with the Relevance Model approach that is part of our baseline feedback.",
                "To ensure as strong a baseline as possible, we use a stoplist for all experiments reported here.",
                "If we turn off the stopword list, however, we obtain results such as those shown in Table 5 where four of the top ten baseline feedback terms for TREC topic 60 (said, but, their, not) are stopwords using the BaseFB method. (The top 100 expansion terms were selected to generate this example.)",
                "Indris method attempts to address the stopword problem by applying an initial step based on Ponte [14] to select less-common terms that have high log-odds of being in the top-ranked documents compared to the whole collection.",
                "Nevertheless, this does not overcome the stopword problem completely, especially as the number of feedback terms grows.",
                "Using resampling feedback, however, appears to mitigate Collection QV + Uniform QV + Relevance-score weighting weighting TREC 1&2 AvgP 0.2545 0.2406 (-5.46%) P10 0.5369 0.5263 (-1.97%) RI 0.6212 0.7087 (+14.09%) TREC 7 AvgP 0.2174 0.2169 (-0.23%) P10 0.4320 0.4480 (+3.70%) RI 0.4783 0.5652 (+18.17%) TREC 8 AvgP 0.2267 0.2268 (+0.04%) P10 0.4120 0.4340 (+5.34%) RI 0.4545 0.4545 (+0.00%) wt10g AvgP 0.1808 0.1946 (+7.63%) P10 0.2680 0.2960 (+10.45%) RI 0.0220 0.1099 (+399.5%) Table 4: Comparison of uniform and relevance-weighted document sampling.",
                "The percentage change compared to uniform sampling is shown in parentheses.",
                "QV indicates that query variants were used in both runs.",
                "Baseline FB p(wi|R) Resampling FB p(wi|R) said 0.055 court 0.026 court 0.055 pay 0.018 pay 0.034 federal 0.012 but 0.026 education 0.011 employees 0.024 teachers 0.010 their 0.024 employees 0.010 not 0.023 case 0.010 federal 0.021 their 0.009 workers 0.020 appeals 0.008 education 0.020 union 0.007 Table 5: Feedback term quality when a stoplist is not used.",
                "Feedback terms for TREC topic 60: merit pay vs seniority. the effect of stopwords automatically.",
                "In the example of Table 5, resampling feedback leaves only one stopword (their) in the top ten.",
                "We observed similar feedback term behavior across many other topics.",
                "The reason for this effect appears to be the interaction of the term selection score with the top-m term cutoff.",
                "While the presence and even proportion of particular stopwords is fairly stable across different document samples, their relative position in the top-m list is not, as sets of documents with varying numbers of better, lower-frequency term candidates are examined for each sample.",
                "As a result, while some number of stopwords may appear in each sampled document set, any given stopword tends to fall below the cutoff for multiple samples, leading to its classification as a high-variance, low-weight feature. 4.",
                "RELATED WORK Our approach is related to previous work from several areas of information retrieval and machine learning.",
                "Our use of query variation was inspired by the work of YomTov et al. [20], Carpineto et al. [5], and Amati et al. [2], among others.",
                "These studies use the idea of creating multiple subqueries and then examining the nature of the overlap in the documents and/or expansion terms that result from each subquery.",
                "Model combination is performed using heuristics.",
                "In particular, the studies of Amati et al. and Carpineto et al. investigated combining terms from individual distributional methods using a term-reranking combination heuristic.",
                "In a set of TREC topics they found wide average variation in the rank-distance of terms from different expansion methods.",
                "Their combination method gave modest positive improvements in average precision.",
                "The idea of examining the overlap between lists of suggested terms has also been used in early query expansion approaches.",
                "Xu and Crofts method of Local Context Analysis (LCA) [19] includes a factor in the empirically-derived weighting formula that causes expansion terms to be preferred that have connections to multiple query terms.",
                "On the document side, recent work by Zhou & Croft [21] explored the idea of adding noise to documents, re-scoring them, and using the stability of the resulting rankings as an estimate of query difficulty.",
                "This is related to our use of document sampling to estimate the risk of the feedback model built from the different sets of top-retrieved documents.",
                "Sakai et al. [17] proposed an approach to improving the robustness of pseudo-relevance feedback using a method they call selective sampling.",
                "The essence of their method is that they allow skipping of some top-ranked documents, based on a clustering criterion, in order to select a more varied and novel set of documents later in the ranking for use by a traditional pseudo-feedback method.",
                "Their study did not find significant improvements in either robustness (RI) or MAP on their corpora.",
                "Greiff, Morgan and Ponte [8] explored the role of variance in term weighting.",
                "In a series of simulations that simplified the problem to 2-feature documents, they found that average precision degrades as term frequency variance - high noiseincreases.",
                "Downweighting terms with high variance resulted in improved average precision.",
                "This seems in accord with our own findings for individual feedback models.",
                "Estimates of output variance have recently been used for improved text classification.",
                "Lee et al. [11] used queryspecific variance estimates of classifier outputs to perform improved model combination.",
                "Instead of using sampling, they were able to derive closed-form expressions for classifier variance by assuming base classifiers using simple types of inference networks.",
                "Ando and Zhang proposed a method that they call structural feedback [3] and showed how to apply it to query expansion for the TREC Genomics Track.",
                "They used r query variations to obtain R different sets Sr of top-ranked documents that have been intersected with the top-ranked documents obtained from the original query qorig.",
                "For each Si, the normalized centroid vector ˆwi of the documents is calculated.",
                "Principal component analysis (PCA) is then applied to the ˆwi to obtain the matrix Φ of H left singular vectors φh that are used to obtain the new, expanded query qexp = qorig + ΦT Φqorig. (7) In the case H = 1, we have a single left singular vector φ: qexp = qorig + (φT qorig)φ so that the dot product φT qorig is a type of dynamic weight on the expanded query that is based on the similarity of the original query to the expanded query.",
                "The use of variance as a feedback model quality measure occurs indirectly through the application of PCA.",
                "It would be interesting to study the connections between this approach and our own modelfitting method.",
                "Finally, in language modeling approaches to feedback, Tao and Zhai [18] describe a method for more robust feedback that allows each document to have a different feedback α.",
                "The feedback weights are derived automatically using regularized EM.",
                "A roughly equal balance of query and expansion model is implied by their EM stopping condition.",
                "They propose tailoring the stopping parameter η based on a function of some quality measure of feedback documents. 5.",
                "CONCLUSIONS We have presented a new approach to pseudo-relevance feedback based on document and query sampling.",
                "The use of sampling is a very flexible and powerful device and is motivated by our general desire to extend current models of retrieval by estimating the risk or variance associated with the parameters or output of retrieval processes.",
                "Such variance estimates, for example, may be naturally used in a Bayesian framework for improved model estimation and combination.",
                "Applications such as selective expansion may then be implemented in a principled way.",
                "While our study uses the language modeling approach as a framework for experiments, we make few assumptions about the actual workings of the feedback algorithm.",
                "We believe it is likely that any reasonably effective baseline feedback algorithm would benefit from our approach.",
                "Our results on standard TREC collections show that our framework improves the robustness of a strong baseline feedback method across a variety of collections, without sacrificing average precision.",
                "It also gives small but consistent gains in top10 precision.",
                "In future work, we envision an investigation into how varying the set of sampling methods used and the number of samples controls the trade-off between robustness, accuracy, and efficiency.",
                "Acknowledgements We thank Paul Bennett for valuable discussions related to this work, which was supported by NSF grants #IIS-0534345 and #CNS-0454018, and U.S. Dept. of Education grant #R305G03123.",
                "Any opinions, findings, and conclusions or recommendations expressed in this material are the authors. and do not necessarily reflect those of the sponsors. 6.",
                "REFERENCES [1] The Lemur toolkit for language modeling and retrieval. http://www.lemurproject.org. [2] G. Amati, C. Carpineto, and G. Romano.",
                "Query difficulty, robustness, and selective application of query expansion.",
                "In Proc. of the 25th European Conf. on Information Retrieval (ECIR 2004), pages 127-137. [3] R. K. Ando and T. Zhang.",
                "A high-performance semi-supervised learning method for text chunking.",
                "In Proc. of the 43rd Annual Meeting of the ACL, pages 1-9, June 2005. [4] L. Breiman.",
                "Bagging predictors.",
                "Machine Learning, 24(2):123-140, 1996. [5] C. Carpineto, G. Romano, and V. Giannini.",
                "Improving retrieval feedback with multiple term-ranking function combination.",
                "ACM Trans.",
                "Info.",
                "Systems, 20(3):259 - 290. [6] K. Collins-Thompson, P. Ogilvie, and J. Callan.",
                "Initial results with structured queries and language models on half a terabyte of text.",
                "In Proc. of 2005 Text REtrieval Conference.",
                "NIST Special Publication. [7] R. O. Duda, P. E. Hart, and D. G. Stork.",
                "Pattern Classification.",
                "Wiley and Sons, 2nd edition, 2001. [8] W. R. Greiff, W. T. Morgan, and J. M. Ponte.",
                "The role of variance in term weighting for probabilistic information retrieval.",
                "In Proc. of the 11th Intl.",
                "Conf. on Info. and Knowledge Mgmt. (CIKM 2002), pages 252-259. [9] T. Kohonen, J. Hynninen, J. Kangas, and J. Laaksonen.",
                "SOMPAK: The self-organizing map program package.",
                "Technical Report A31, Helsinki University of Technology, 1996. http://www.cis.hut.fi/research/papers/som tr96.ps.Z. [10] V. Lavrenko.",
                "A Generative Theory of Relevance.",
                "PhD thesis, University of Massachusetts, Amherst, 2004. [11] C.-H. Lee, R. Greiner, and S. Wang.",
                "Using query-specific variance estimates to combine Bayesian classifiers.",
                "In Proc. of the 23rd Intl.",
                "Conf. on Machine Learning (ICML 2006), pages 529-536. [12] D. Metzler and W. B. Croft.",
                "Combining the language model and inference network approaches to retrieval.",
                "Info.",
                "Processing and Mgmt., 40(5):735-750, 2004. [13] T. Minka.",
                "Estimating a Dirichlet distribution.",
                "Technical report, 2000. http://research.microsoft.com/ minka/papers/dirichlet. [14] J. Ponte.",
                "Advances in Information Retrieval, chapter Language models for relevance feedback, pages 73-96. 2000.",
                "W.B.",
                "Croft, ed. [15] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In Proc. of the 1998 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 275-281. [16] J. Rocchio.",
                "The SMART Retrieval System, chapter Relevance Feedback in Information Retrieval, pages 313-323.",
                "Prentice-Hall, 1971.",
                "G. Salton, ed. [17] T. Sakai, T. Manabe, and M. Koyama.",
                "Flexible pseudo-relevance feedback via selective sampling.",
                "ACM Transactions on Asian Language Information Processing (TALIP), 4(2):111-135, 2005. [18] T. Tao and C. Zhai.",
                "Regularized estimation of mixture models for robust pseudo-relevance feedback.",
                "In Proc. of the 2006 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 162-169. [19] J. Xu and W. B. Croft.",
                "Improving the effectiveness of information retrieval with local context analysis.",
                "ACM Trans.",
                "Inf.",
                "Syst., 18(1):79-112, 2000. [20] E. YomTov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty.",
                "In Proc. of the 2005 ACM SIGIR Conf. on Research and Development in Information Retrieval, pages 512-519. [21] Y. Zhou and W. B. Croft.",
                "Ranking robustness: a novel framework to predict query performance.",
                "In Proc. of the 15th ACM Intl.",
                "Conf. on Information and Knowledge Mgmt. (CIKM 2006), pages 567-574."
            ],
            "original_annotated_samples": [
                "Current algorithms for pseudo-relevance feedback (PRF) tend to follow the same basic method whether we use <br>vector space-based algorithm</br>s such as Rocchios formula [16], or more recent language modeling approaches such as Relevance Models [10]."
            ],
            "translated_annotated_samples": [
                "Los algoritmos actuales para la retroalimentación de pseudo relevancia (PRF) tienden a seguir el mismo método básico ya sea que utilicemos <br>algoritmos basados en espacio vectorial</br> como la fórmula de Rocchio [16], o enfoques más recientes de modelado de lenguaje como Modelos de Relevancia [10]."
            ],
            "translated_text": "Estimación y Uso de la Incertidumbre en la Retroalimentación de Pseudo-Relevancia Kevyn Collins-Thompson y Jamie Callan Instituto de Tecnologías del Lenguaje Escuela de Ciencias de la Computación Universidad Carnegie Mellon Pittsburgh, PA 15213-8213 EE. UU. {kct | callan}@cs.cmu.edu RESUMEN Los métodos existentes de retroalimentación de pseudo-relevancia suelen realizar un promedio de los documentos mejor clasificados, pero ignoran una dimensión estadística importante: el riesgo o la varianza asociada tanto a los modelos de documentos individuales como a su combinación. Tratando el método de retroalimentación de línea base como una caja negra, y el modelo de retroalimentación de salida como una variable aleatoria, estimamos una distribución posterior para el modelo de retroalimentación mediante el remuestreo de los documentos mejor recuperados de una consulta dada, utilizando la media o la moda posterior como el modelo de retroalimentación mejorado. Luego realizamos la combinación de modelos sobre varios modelos mejorados, cada uno basado en una consulta ligeramente modificada muestreada de la consulta original. Observamos que el remuestreo de documentos ayuda a aumentar la precisión del modelo de retroalimentación individual al eliminar términos de ruido, mientras que el muestreo de la consulta mejora la robustez (rendimiento en el peor de los casos) al enfatizar términos relacionados con múltiples aspectos de la consulta. El resultado es un algoritmo de meta-retroalimentación que es tanto más robusto como más preciso que el método de referencia original. Categorías y Descriptores de Asignaturas: H.3.3 [Recuperación de Información]: Modelos de Recuperación Términos Generales: Algoritmos, Experimentación 1. INTRODUCCIÓN La incertidumbre es una característica inherente de la recuperación de información. No solo no sabemos las consultas que se presentarán a nuestro algoritmo de recuperación con anticipación, sino que la necesidad de información de los usuarios puede ser vaga o estar incompletamente especificada por estas consultas. Incluso si la consulta estuviera perfectamente especificada, el lenguaje en los documentos de la colección es inherentemente complejo y ambiguo, y hacer coincidir dicho lenguaje de manera efectiva es un problema formidable por sí mismo. Con esto en mente, deseamos tratar muchas cantidades importantes calculadas por el sistema de recuperación, ya sea un puntaje de relevancia para un documento, o un peso para un término de expansión de consulta, como variables aleatorias cuyo valor real es incierto pero donde la incertidumbre sobre el valor real puede ser cuantificada reemplazando el valor fijo con una distribución de probabilidad sobre posibles valores. De esta manera, los algoritmos de recuperación pueden intentar cuantificar el riesgo o la incertidumbre asociados con sus clasificaciones de salida, o mejorar la estabilidad o precisión de sus cálculos internos. Los algoritmos actuales para la retroalimentación de pseudo relevancia (PRF) tienden a seguir el mismo método básico ya sea que utilicemos <br>algoritmos basados en espacio vectorial</br> como la fórmula de Rocchio [16], o enfoques más recientes de modelado de lenguaje como Modelos de Relevancia [10]. Primero, se obtiene un conjunto de documentos recuperados en la parte superior a partir de una consulta inicial y se asume que aproxima un conjunto de documentos relevantes. A continuación, se calcula un vector de modelo de retroalimentación único según algún tipo de promedio, centroide o expectativa sobre el conjunto de modelos de documentos posiblemente relevantes. Por ejemplo, los vectores de documentos pueden combinarse con igual ponderación, como en Rocchio, o por probabilidad de consulta, como se puede hacer utilizando el Modelo de Relevancia. El uso de una expectativa es razonable por razones prácticas y teóricas, pero por sí sola ignora información potencialmente valiosa sobre el riesgo del modelo de retroalimentación. Nuestra hipótesis principal en este artículo es que estimar la incertidumbre en la retroalimentación es útil y conduce a modelos de retroalimentación individual más efectivos y a modelos combinados más robustos. Por lo tanto, proponemos un método para estimar la incertidumbre asociada con un modelo de retroalimentación individual en términos de una distribución posterior sobre modelos de lenguaje. Para hacer esto, variamos sistemáticamente las entradas al método de retroalimentación base y ajustamos una distribución de Dirichlet a la salida. Utilizamos la media posterior o el modo como la estimación mejorada del modelo de retroalimentación. Este proceso se muestra en la Figura 1. Como mostraremos más adelante, la media y la moda pueden variar significativamente del modelo de retroalimentación única propuesto por el método de referencia. También realizamos la combinación de modelos utilizando varios modelos de lenguaje de retroalimentación mejorados obtenidos a partir de un pequeño número de nuevas consultas muestreadas de la consulta original. El peso de un modelo combina dos factores complementarios: la probabilidad del modelo de generar la consulta y la varianza del modelo, siendo los modelos con alta varianza los que obtienen un peso menor. Por ejemplo, un vector de parámetros esperado condicionado a la observación de la consulta se forma a partir de los documentos recuperados en la parte superior, que se tratan como cadenas de entrenamiento (ver [10], p. 62). Figura 1: Estimación de la incertidumbre del modelo de retroalimentación para una sola consulta. 2. En las Secciones 2.1-2.5 describimos un método general para estimar una distribución de probabilidad sobre el conjunto de posibles modelos de lenguaje basado en muestreo. En las Secciones 2.6 y 2.7 resumimos cómo se utilizan diferentes muestras de consultas para generar múltiples modelos de retroalimentación, que luego se combinan. 2.1 Modelado de la Incertidumbre de la Retroalimentación Dado una consulta Q y una colección C, asumimos un sistema de recuperación probabilístico que asigna un puntaje de documento de valor real f(D, Q) a cada documento D en C, de manera que el puntaje sea proporcional a la probabilidad estimada de relevancia. No hacemos ninguna otra suposición sobre f(D, Q). La naturaleza de f(D, Q) puede ser compleja: por ejemplo, si el sistema de recuperación admite lenguajes de consulta estructurados [12], entonces f(D, Q) puede representar la salida de una red de inferencia arbitrariamente compleja definida por los operadores de consulta estructurados. En teoría, la función de puntuación puede variar de una consulta a otra, aunque en este estudio por simplicidad mantenemos la misma función de puntuación para todas las consultas. Nuestro método de consulta específico se encuentra en la Sección 3. Tratamos el algoritmo de retroalimentación como una caja negra y asumimos que los insumos del algoritmo de retroalimentación son la consulta original y los documentos recuperados principales correspondientes, a los cuales se les asigna una puntuación a cada documento. Suponemos que la salida del algoritmo de retroalimentación es un vector de pesos de términos que se utilizarán para agregar o reponderar los términos en la representación de la consulta original, con el vector normalizado para formar una distribución de probabilidad. Consideramos las entradas a la caja negra de retroalimentación como variables aleatorias, y analizamos el modelo de retroalimentación como una variable aleatoria que cambia en respuesta a cambios en las entradas. Al igual que la función de puntuación del documento f(D, Q), el algoritmo de retroalimentación puede implementar una fórmula de puntuación compleja y no lineal, por lo que, a medida que sus entradas varían, los modelos de retroalimentación resultantes pueden tener una distribución compleja en el espacio de modelos de retroalimentación (el espacio muestral). Debido a esta complejidad potencial, no intentamos derivar una distribución posterior de forma cerrada, sino que en su lugar utilizamos simulación. Llamamos a esta distribución sobre posibles modelos de retroalimentación la distribución de modelos de retroalimentación. Nuestro objetivo en esta sección es estimar una aproximación útil a la distribución del modelo de retroalimentación. Para un marco específico de experimentos, utilizamos el enfoque de modelado de lenguaje (LM) para la recuperación de información [15]. El puntaje de un documento D con respecto a una consulta Q y una colección C se da por p(Q|D) con respecto a los modelos de lenguaje ˆθQ y ˆθD estimados para la consulta y el documento respectivamente. Denotamos el conjunto de los k documentos mejor recuperados de la colección C en respuesta a Q como DQ(k, C). Para simplificar, asumimos que las consultas y documentos son generados por distribuciones multinomiales cuyos parámetros están representados por modelos de lenguaje unigram. Para incorporar retroalimentación en el enfoque de LM, asumimos un esquema basado en modelos en el que nuestro objetivo es tomar la consulta y los documentos clasificados resultantes DQ(k, C) como entrada, y producir un modelo de lenguaje expandido ˆθE, que luego se interpola con el modelo de consulta original ˆθQ: ˆθNew = (1 − α) · ˆθQ + α · ˆθE (1). Esto incluye la posibilidad de α = 1 donde el modelo de consulta original es completamente reemplazado por el modelo de retroalimentación. Nuestro espacio muestral es el conjunto de todos los posibles modelos de lenguaje LF que pueden ser generados como modelos de retroalimentación. Nuestro enfoque es tomar muestras de este espacio y luego ajustar una distribución a las muestras utilizando máxima verosimilitud. Para simplificar, comenzamos asumiendo que la distribución de retroalimentación latente tiene la forma de una distribución de Dirichlet. Aunque la distribución de Dirichlet es unimodal y en general bastante limitada en su expresividad en el espacio muestral, es una coincidencia natural para el modelo de lenguaje multinomial, puede ser estimada rápidamente y capturar las características más relevantes de los modelos de retroalimentación confiada e incierta, como la dispersión general de la distribución. 2.2 Re-muestreo de modelos de documentos. Nos gustaría una aproximación a la distribución posterior del modelo de retroalimentación LF. Para lograr esto, aplicamos una técnica de simulación ampliamente utilizada llamada muestreo bootstrap ([7], p. 474) en los parámetros de entrada, es decir, el conjunto de documentos recuperados en la parte superior. El muestreo de arranque nos permite simular el efecto aproximado de perturbar los parámetros dentro del algoritmo de retroalimentación de la caja negra al perturbar los insumos de ese algoritmo de manera sistemática, sin hacer suposiciones sobre la naturaleza del algoritmo de retroalimentación. Específicamente, muestreamos k documentos con reemplazo de DQ(k, C), y calculamos un modelo de lenguaje de expansión θb utilizando el método de retroalimentación de caja negra. Repetimos este proceso B veces para obtener un conjunto de B modelos de lenguaje de retroalimentación, a los cuales luego ajustamos una distribución de Dirichlet. Normalmente, B se encuentra en el rango de 20 a 50 muestras, con un rendimiento relativamente estable en este rango. Ten en cuenta que en lugar de tratar cada documento superior como igualmente probable, muestreamos de acuerdo con las probabilidades estimadas de relevancia de cada documento en DQ(k, C). Por lo tanto, es más probable que un documento sea elegido cuanto más alto esté en la clasificación. 2.3 Justificación de un enfoque de muestreo La justificación de nuestro enfoque de muestreo tiene dos partes. Primero, queremos mejorar la calidad de los modelos de retroalimentación individual al suavizar la variación cuando el modelo de retroalimentación base es inestable. En este sentido, nuestro enfoque se asemeja al bagging [4], un enfoque de conjunto que genera múltiples versiones de un predictor haciendo copias de arranque del conjunto de entrenamiento, y luego promedia los predictores (numéricos). En nuestra aplicación, los documentos recuperados en la parte superior pueden ser vistos como una especie de conjunto de entrenamiento ruidoso para la relevancia. Segundo, el muestreo es una forma efectiva de estimar propiedades básicas de la distribución posterior de retroalimentación, las cuales pueden ser utilizadas para mejorar la combinación de modelos. Por ejemplo, un modelo puede ser ponderado por su confianza de predicción, estimada como una función de la variabilidad del posterior alrededor del modelo. foo2-401.map-Dim:5434,Size:12*12units, vecindario gaussiano (a) Tema 401 Minorías extranjeras, Alemania foo2-402.map-Dim:5698,Size:12*12units, vecindario gaussiano (b) Tema 402 Genética del comportamiento foo2-459.map-Dim:8969,Size:12*12units, vecindario gaussiano (c) Tema 459 ¿Cuándo puede un prestamista ejecutar una hipoteca sobre una propiedad? Figura 2: Visualización de la varianza del modelo de lenguaje de expansión utilizando mapas autoorganizados, mostrando la distribución de modelos de lenguaje que resulta de remuestrear las entradas al método de expansión de referencia. El modelo de lenguaje que habría sido elegido por la expansión de la línea base se encuentra en el centro de cada mapa. La función de similitud es la divergencia JensenShannon. 2.4 Visualización de distribuciones de retroalimentación Antes de describir cómo ajustamos y utilizamos la distribución de Dirichlet sobre modelos de retroalimentación, es instructivo ver algunos ejemplos de distribuciones de modelos de retroalimentación reales que resultan de muestrear por bootstrap los documentos más recuperados de diferentes temas de TREC. Cada punto en nuestro espacio muestral es un modelo de lenguaje, que típicamente tiene varias miles de dimensiones. Para ayudar a analizar el comportamiento de nuestro método, utilizamos un Mapa Auto-organizado (a través del paquete SOM-PAK [9]), para aplanar y visualizar la función de densidad de alta dimensión. Los mapas de densidad para tres temas de TREC se muestran en la Figura 2 arriba. Las áreas oscuras representan regiones de alta similitud entre los modelos de lenguaje. Las áreas claras representan regiones de baja similitud, los valles entre los grupos. Cada diagrama está centrado en el modelo de lenguaje que habría sido elegido por la expansión de referencia. Un solo pico (modo) es evidente en algunos ejemplos, pero en otros aparece una estructura más compleja. Además, aunque la distribución suele estar cerca del modelo de retroalimentación de referencia, para algunos temas están a una distancia significativa (según la divergencia de JensenShannon), como se muestra en la Subfigura 2c. En tales casos, la moda o la media de la distribución de retroalimentación a menudo funcionan significativamente mejor que el valor base (y en una proporción menor de casos, significativamente peor). 2.5 Ajuste de una distribución de retroalimentación posterior Después de obtener muestras del modelo de retroalimentación mediante el remuestreo de las entradas del modelo de retroalimentación, estimamos la distribución de retroalimentación. Suponemos que los modelos de retroalimentación multinomial {ˆθ1, . . . , ˆθB} fueron generados por una distribución de Dirichlet latente con parámetros {α1, . . . , αN}. Para estimar los {α1, . . . , αN }, ajustamos los parámetros de Dirichlet a las muestras del modelo de lenguaje B según la máxima verosimilitud utilizando un procedimiento de Newton generalizado, cuyos detalles se encuentran en Minka [13]. Suponemos una distribución de Dirichlet simple sobre los {α1, . . . , αN}, estableciendo cada uno como αi = μ · p(wi | C), donde μ es un parámetro y p(· | C) es el modelo de lenguaje de colección estimado a partir de un conjunto de documentos de la colección C. El ajuste de parámetros converge muy rápidamente, generalmente en solo 2 o 3 iteraciones. Dado que nuestros puntos son modelos de lenguaje en el simplex multinomial, extendimos SOM-PAK para admitir la divergencia de Jensen-Shannon, una medida de similitud ampliamente utilizada entre distribuciones de probabilidad. 3 iteraciones son suficientes, por lo que es práctico aplicarlo en tiempo de consulta cuando la sobrecarga computacional debe ser pequeña. En la práctica, podemos restringir el cálculo al vocabulario de los documentos recuperados en la parte superior, en lugar de toda la colección. Ten en cuenta que para este paso estamos reutilizando los documentos recuperados existentes y no realizando consultas adicionales. Dado los parámetros de una distribución de Dirichlet N-dimensional Dir(α), los vectores de media μ y moda x son fáciles de calcular y se dan respectivamente por μi = αiP αi (2) y xi = αi−1P αi−N . (3) Luego podemos elegir el modelo de lenguaje en la media o en la moda del posterior como el modelo de retroalimentación mejorado final. (Encontramos que la moda proporciona un rendimiento ligeramente mejor). Para la recuperación de información, es probable que el número de muestras disponibles sea bastante pequeño por razones de rendimiento, generalmente menos de diez. Además, si bien el muestreo aleatorio es útil en ciertos casos, es perfectamente aceptable permitir distribuciones de muestreo determinísticas, pero estas deben diseñarse cuidadosamente para aproximar una varianza de salida precisa. Dejamos esto para estudios futuros. 2.6 Variantes de consulta Utilizamos los siguientes métodos para generar variantes de la consulta original. Cada variante corresponde a una suposición diferente sobre qué aspectos de la consulta original pueden ser importantes. Esta es una forma de muestreo determinístico. Seleccionamos tres métodos simples que cubren suposiciones complementarias sobre la consulta. Uso sin expansión. La suposición es que los términos dados son una descripción completa de la necesidad de información. Deja uno fuera. Se deja fuera un término del query original. La suposición es que uno de los términos de la consulta es un término de ruido. Se elige un término único de la consulta original. Esto asume que solo un aspecto de la consulta, es decir, el representado por el término, es el más importante. Después de generar una variante de la consulta original, la combinamos con la consulta original utilizando un peso αSUB para no alejarnos demasiado. En este estudio, establecimos αSUB = 0.5. Por ejemplo, utilizando el lenguaje de consulta Indri [12], una variante de dejar uno fuera de la consulta inicial que omite el término \"ireland\" para el tema TREC 404 es: #weight(0.5 #combine(ireland peace talks) 0.5 #combine(peace talks)) 2.7 Combinando modelos de retroalimentación mejorados de múltiples variantes de consulta. Cuando se utilizan múltiples variantes de consulta, los modelos de retroalimentación mejorados resultantes se combinan utilizando la combinación de modelos bayesianos. Para hacer esto, tratamos cada palabra como un elemento a clasificar como perteneciente a una clase relevante o no relevante, y derivamos una probabilidad de clase para cada palabra combinando las puntuaciones de cada variante de consulta. Cada puntuación es dada por la probabilidad de ese término en la distribución de Dirichlet. Las puntuaciones de los términos se ponderan por el inverso de la varianza del término en los modelos de retroalimentación mejorada de la distribución de Dirichlet. La probabilidad previa de pertenencia de una palabra a la clase relevante se da por la probabilidad de la consulta original en todo el modelo de expansión mejorado. 3. EVALUACIÓN En esta sección presentamos resultados que confirman la utilidad de estimar una distribución del modelo de retroalimentación a partir del remuestreo ponderado de documentos mejor clasificados, y de combinar los modelos de retroalimentación obtenidos a partir de diferentes pequeños cambios en la consulta original. 3.1 Método general Evaluamos el rendimiento en un total de 350 consultas derivadas de cuatro conjuntos de temas de TREC: 51-200 (TREC-1&2), 351-400 (TREC-7), 401-450 (TREC-8) y 451-550 (wt10g, TREC-9&10). Elegimos estos por su contenido variado y propiedades del documento. Por ejemplo, los documentos wt10g son páginas web con una amplia variedad de temas y estilos, mientras que los documentos TREC-1&2 son artículos de noticias más homogéneos. La indexación y recuperación se realizó utilizando el sistema Indri en el kit de herramientas Lemur [12] [1]. Nuestras consultas se derivaron de las palabras en el campo del título de los temas de TREC. Las frases no fueron utilizadas. Para generar las consultas base que se pasaron a Indri, envolvimos los términos de la consulta con el operador #combine de Indri. Por ejemplo, la consulta inicial para el tema 404 es: #combine(ireland peace talks) Realizamos el stemming de Krovetz para todos los experimentos. Debido a que encontramos que el método de expansión de la base (Indri) funcionó mejor utilizando una lista de palabras vacías con el modelo de retroalimentación, todos los experimentos utilizaron una lista de 419 palabras comunes en inglés. Sin embargo, un efecto secundario interesante de nuestro enfoque de remuestreo es que tiende a eliminar muchas palabras vacías del modelo de retroalimentación, lo que hace que una lista de paradas sea menos crítica. Esto se discute más a fondo en la Sección 3.6. Método de retroalimentación de línea base 3.2 Para nuestro método de expansión de línea base, utilizamos un algoritmo incluido en Indri 1.0 como el método de expansión predeterminado. Este método primero selecciona términos utilizando un cálculo de logaritmo de probabilidades descrito por Ponte [14], pero asigna pesos finales a los términos utilizando el modelo de relevancia de Lavrenko [10]. Elegimos el método Indri porque proporciona una línea base consistentemente sólida, se basa en un enfoque de modelado del lenguaje y es fácil de experimentar con él. En una evaluación de TREC utilizando el corpus GOV2, el método fue uno de los mejores resultados, logrando un aumento del 19.8% en el MAP en comparación con el uso de consultas no expandidas. En este estudio, se logra una ganancia promedio en MAP del 17.25% en las cuatro colecciones. El método de expansión de Indri primero calcula una razón de logaritmos de probabilidades o(v) para cada término de expansión potencial v dado por o(v) = X D log p(v|D) p(v|C) (4) sobre todos los documentos D que contienen v, en la colección C. Luego, los candidatos a términos de expansión se ordenan por o(v) descendente, y se eligen los primeros m. Finalmente, los pesos de término r(v) utilizados en la consulta ampliada se calculan en base al modelo de relevancia r(v) = X D p(q|D)p(v|D) p(v) p(D) (5). La cantidad p(q|D) es la puntuación de probabilidad asignada al documento en la recuperación inicial. Utilizamos suavizado de Dirichlet de p(v|D) con μ = 1000. Este modelo de relevancia se combina luego con la consulta original utilizando interpolación lineal, ponderada por un parámetro α. Por defecto, utilizamos los 50 documentos principales para la retroalimentación y los 20 términos de expansión, con el parámetro de interpolación de retroalimentación α = 0.5 a menos que se indique lo contrario. Por ejemplo, la consulta expandida base para el tema 404 es: #weight(0.5 #combine(ireland peace talks) 0.5 #weight(0.10 ireland 0.08 peace 0.08 northern ...) 3.3 Rendimiento de la expansión. Medimos la efectividad de nuestros algoritmos de retroalimentación mediante dos criterios principales: precisión y robustez. La robustez, y el equilibrio entre precisión y robustez, se analiza en la Sección 3.4. En esta sección, examinamos la precisión promedio y la precisión en los 10 documentos principales (P10). También incluimos la recuperación de 1,000 documentos. Para cada consulta, obtuvimos un conjunto de B modelos de retroalimentación utilizando la línea base de Indri. Cada modelo de retroalimentación se obtuvo a partir de una muestra aleatoria de los k documentos principales tomados con reemplazo. Para estos experimentos, B = 30 y k = 50. Cada modelo de retroalimentación contenía 20 términos. En el lado de la consulta, utilizamos muestreo de dejar uno fuera (LOO) para crear las variantes de consulta. El muestreo de consulta de un solo término tuvo un rendimiento consistentemente peor en todas las colecciones, por lo que nuestros resultados se centran en el muestreo de LOO aquí. Utilizamos los métodos descritos en la Sección 2 para estimar un modelo de retroalimentación mejorado a partir de la distribución posterior de Dirichlet para cada variante de consulta, y para combinar los modelos de retroalimentación de todas las variantes de consulta. Llamamos a nuestro método expansión de remuestreo y lo denotamos como RS-FB aquí. Denominamos al método de retroalimentación de línea base Indri como Base-FB. Los resultados de aplicar tanto el método de expansión de línea base (Base-FB) como la expansión de remuestreo (RS-FB) se muestran en la Tabla 1. Observamos varias tendencias en esta tabla. Primero, la precisión promedio de RS-FB fue comparable a Base-FB, logrando una ganancia promedio del 17.6% en comparación con no utilizar expansión en las cuatro colecciones. La ganancia de expansión de la línea base de Indri fue del 17.25%. Además, el método RS-FB logró mejoras consistentes en P10 sobre Base-FB para cada conjunto de temas, con una mejora promedio del 6.89% sobre Base-FB para los 350 temas. La ganancia P10 más baja sobre Base-FB fue del +3.82% para TREC-7 y la más alta fue del +11.95% para wt10g. Finalmente, tanto Base-FB como RS-FB también mejoraron consistentemente la recuperación en comparación con no utilizar expansión, siendo Base-FB el que logró una mejor recuperación que RS-FB para todos los conjuntos de temas. 3.4 Robustez de recuperación. Utilizamos el término robustez para referirnos al rendimiento de precisión promedio en el peor de los casos de un algoritmo de retroalimentación. Idealmente, un método de retroalimentación sólido nunca debería funcionar peor que usar la consulta original, mientras que a menudo funciona mejor utilizando la expansión. Para evaluar la robustez en este estudio, utilizamos una medida muy simple llamada índice de robustez (RI). Para un conjunto de consultas Q, la medida de RI se define como: RI(Q) = n+ − n− |Q| (6) donde n+ es el número de consultas ayudadas por el método de retroalimentación y n− es el número de consultas perjudicadas. Aquí, por \"ayudar\" nos referimos a obtener una precisión promedio más alta como resultado de la retroalimentación. El valor de RI varía desde un mínimo de 3. A veces también se le llama índice de confiabilidad de mejora y fue utilizado en Sakai et al. [17]. Colección NoExp Base-FB RS-FB TREC 1&2 AvgP 0.1818 0.2419 (+33.04%) 0.2406 (+32.24%) P10 0.4443 0.4913 (+10.57%) 0.5363 (+17.83%) Recall 15084/37393 19172/37393 15396/37393 TREC 7 AvgP 0.1890 0.2175 (+15.07%) 0.2169 (+14.75%) P10 0.4200 0.4320 (+2.85%) 0.4480 (+6.67%) Recall 2179/4674 2608/4674 2487/4674 TREC 8 AvgP 0.2031 0.2361 (+16.25%) 0.2268 (+11.70%) P10 0.3960 0.4160 (+5.05%) 0.4340 (+9.59%) Recall 2144/4728 2642/4728 2485/4728 wt10g AvgP 0.1741 0.1829 (+5.06%) 0.1946 (+11.78%) P10 0.2760 0.2630 (-4.71%) 0.2960 (+7.24%) Recall 3361/5980 3725/5980 3664/5980 Tabla 1: Comparación de la retroalimentación base (Base-FB) y la retroalimentación utilizando re-muestreo (RS-FB). La mejora mostrada para BaseFB y RS-FB es relativa al uso de ninguna expansión. (a) TREC 1&2 (curva superior); TREC 8 (curva inferior) (b) TREC 7 (curva superior); wt10g (curva inferior) Figura 3: La compensación entre robustez y precisión promedio para diferentes corpora. El eje x muestra el cambio en la MAP al utilizar la expansión de la línea base con α = 0.5. El eje y muestra el Índice de Robustez (RI). Cada curva a través de los puntos no circulados muestra el compromiso entre RI/MAP utilizando la estrategia simple de pequeño α (ver texto) a medida que α disminuye de 0.5 a cero en la dirección de la flecha. Los puntos circulados representan los compromisos obtenidos al volver a muestrear la retroalimentación para α = 0.5. Colección N Base-FB RS-FB n− RI n− RI TREC 1&2 103 26 +0.495 15 +0.709 TREC 7 46 14 +0.391 10 +0.565 TREC 8 44 12 +0.455 12 +0.455 wt10g 91 48 -0.055 39 +0.143 Combinado 284 100 +0.296 76 +0.465 Tabla 2: Comparación del índice de robustez (RI) para retroalimentación de línea base (Base-FB) vs. retroalimentación de remuestreo (RS-FB). También se muestran el número real de consultas perjudicadas por la retroalimentación (n−) para cada método y colección. Las consultas para las cuales la precisión promedio inicial era insignificante (≤ 0.01) fueron ignoradas, dando como resultado el recuento de consultas restantes en la columna N, de -1.0, cuando todas las consultas son perjudicadas por el método de retroalimentación, a +1.0 cuando todas las consultas son ayudadas. La medida de RI no tiene en cuenta la magnitud o distribución de la cantidad de cambio en el conjunto Q. Sin embargo, es fácil de entender como una indicación general de robustez. Una forma obvia de mejorar el rendimiento en el peor de los casos de la retroalimentación es simplemente utilizar un parámetro de interpolación α fijo más pequeño, como α = 0.3, otorgando menos peso al modelo de retroalimentación (posiblemente arriesgado) y más al cuestionario original. Llamamos a esto la estrategia de pequeña α. Dado que también estamos reduciendo las ganancias potenciales cuando el modelo de retroalimentación es correcto, sin embargo, esperaríamos algún tipo de compensación entre la precisión promedio y la robustez. Por lo tanto, comparamos el equilibrio entre precisión y robustez entre nuestro algoritmo de retroalimentación de remuestreo y el método simple de pequeño α. Los resultados se resumen en la Figura 3. En la figura, la curva para cada conjunto de temas interpola entre los puntos de compensación, comenzando en x=0, donde α = 0.5, y continuando en la dirección de la flecha a medida que α disminuye y la consulta original recibe cada vez más peso. Como era de esperar, la robustez aumenta continuamente a medida que avanzamos a lo largo de la curva, pero la precisión media promedio generalmente disminuye a medida que se eliminan las ganancias del feedback. Para la comparación, se muestra el rendimiento del re-muestreo con retroalimentación en α = 0.5 para cada colección como el punto marcado con un círculo. Más alto y a la derecha es mejor. Esta figura muestra que el re-muestreo de retroalimentación ofrece un compromiso algo mejor que el enfoque de pequeño α para 3 de las 4 colecciones. Figura 4: Histograma que muestra la mayor robustez del re-muestreo de retroalimentación (RS-FB) sobre la retroalimentación base (Base-FB) para todos los conjuntos de datos combinados. Las consultas se agrupan por el % de cambio en AP en comparación con la consulta no expandida. Colección DS + QV DS + No QV TREC 1&2 AvgP 0.2406 0.2547 (+5.86%) P10 0.5263 0.5362 (+1.88%) RI 0.7087 0.6515 (-0.0572) TREC 7 AvgP 0.2169 0.2200 (+1.43%) P10 0.4480 0.4300 (-4.02%) RI 0.5652 0.2609 (-0.3043) TREC 8 AvgP 0.2268 0.2257 (-0.49%) P10 0.4340 0.4200 (-3.23%) RI 0.4545 0.4091 (-0.0454) wt10g AvgP 0.1946 0.1865 (-4.16%) P10 0.2960 0.2680 (-9.46%) RI 0.1429 0.0220 (-0.1209) Tabla 3: Comparación de la retroalimentación de remuestreo utilizando muestreo de documentos (DS) con (QV) y sin (No QV) combinar modelos de retroalimentación de múltiples variantes de consulta. La Tabla 2 muestra los puntajes del Índice de Robustez para Base-FB y RS-FB. El método de retroalimentación RS-FB obtuvo una mayor robustez que Base-FB en tres de los cuatro conjuntos de temas, con solo un rendimiento ligeramente peor en TREC-8. Una vista más detallada que muestra la distribución de los cambios relativos en AP se presenta en el histograma de la Figura 4. En comparación con Base-FB, el método RS-FB logra una reducción notable en el número de consultas significativamente afectadas por la expansión (es decir, donde AP se ve afectado en un 25% o más), al tiempo que conserva ganancias positivas en AP. 3.5 Efecto de los métodos de muestreo de consultas y documentos Dada la mayor robustez de nuestros algoritmos observada en la Sección 3.4, una pregunta importante es qué componente de nuestro sistema es responsable. ¿Es el uso de la re-muestreo de documentos, el uso de múltiples variantes de consulta, o algún otro factor? Los resultados en la Tabla 3 sugieren que la combinación de modelos basada en variantes de consulta puede ser en gran medida responsable de la mayor robustez. Cuando se desactivan las variantes de consulta y se utiliza la consulta original por sí sola con muestreo de documentos, hay poco cambio neto en la precisión promedio, una pequeña disminución en P10 para 3 de los 4 conjuntos de temas, pero una caída significativa en la robustez para todos los conjuntos de temas. En dos casos, la medida de RI disminuye en más del 50%. También examinamos el efecto del método de muestreo de documentos en la efectividad de la recuperación, utilizando dos estrategias diferentes. La estrategia de ponderación uniforme ignoró las puntuaciones de relevancia de la recuperación inicial y otorgó a cada documento en los primeros k la misma probabilidad de selección. Por el contrario, la estrategia de ponderación de puntajes de relevancia seleccionó documentos con una probabilidad proporcional a sus puntajes de relevancia. De esta manera, los documentos que tenían una clasificación más alta tenían más probabilidades de ser seleccionados. Los resultados se muestran en la Tabla 4. La estrategia de ponderación de la puntuación de relevancia tiene un mejor rendimiento en general, con puntajes de RI y P10 significativamente más altos en 3 de los 4 conjuntos de temas. La diferencia en la precisión promedio entre los métodos, sin embargo, es menos marcada. Esto sugiere que el peso uniforme actúa para aumentar la varianza en los resultados de recuperación: cuando la precisión promedio inicial es alta, hay muchos documentos relevantes en los primeros k y el muestreo uniforme puede proporcionar un modelo de relevancia más representativo que centrarse en los elementos altamente clasificados. Por otro lado, cuando la precisión inicial es baja, hay pocos documentos relevantes en las posiciones inferiores y el muestreo uniforme mezcla más de los documentos no relevantes. Por razones de espacio, solo resumimos nuestros hallazgos sobre el tamaño de la muestra aquí. El número de muestras tiene cierto efecto en la precisión cuando es menor a 10, pero el rendimiento se estabiliza alrededor de 15 a 20 muestras. Utilizamos 30 muestras para nuestros experimentos. Mucho más allá de este nivel, los beneficios adicionales de más muestras disminuyen a medida que la distribución inicial de puntajes se ajusta más estrechamente y aumenta el tiempo de procesamiento. 3.6 El efecto del remuestreo en la calidad del término de expansión Idealmente, un modelo de recuperación no debería requerir una lista de palabras vacías al estimar un modelo de relevancia: un modelo estadístico robusto debería reducir automáticamente el peso de las palabras vacías dependiendo del contexto. Las palabras vacías pueden perjudicar la retroalimentación si se seleccionan como términos de retroalimentación, ya que suelen ser malos discriminadores y desperdician espacios de términos valiosos. En la práctica, sin embargo, debido a que la mayoría de los métodos de selección de términos se asemejan a un tipo de ponderación tf · idf, a veces se pueden seleccionar términos con un idf bajo pero un tf muy alto como candidatos de términos de expansión. Esto sucede, por ejemplo, incluso con el enfoque del Modelo de Relevancia que forma parte de nuestra retroalimentación base. Para garantizar una línea base lo más sólida posible, utilizamos una lista de paradas para todos los experimentos reportados aquí. Si desactivamos la lista de palabras vacías, sin embargo, obtenemos resultados como los mostrados en la Tabla 5 donde cuatro de los diez términos de retroalimentación básica principales para el tema 60 de TREC (dijo, pero, su, no) son palabras vacías utilizando el método BaseFB. (Se seleccionaron los 100 términos de expansión principales para generar este ejemplo). El método Indris intenta abordar el problema de las palabras vacías aplicando un paso inicial basado en Ponte [14] para seleccionar términos menos comunes que tienen altas probabilidades logarítmicas de estar en los documentos mejor clasificados en comparación con toda la colección. Sin embargo, esto no resuelve completamente el problema de las palabras vacías, especialmente a medida que aumenta el número de términos de retroalimentación. Sin embargo, el uso de retroalimentación de remuestreo parece mitigar la ponderación de la colección QV + QV uniforme + ponderación de puntaje de relevancia TREC 1&2 AvgP 0.2545 0.2406 (-5.46%) P10 0.5369 0.5263 (-1.97%) RI 0.6212 0.7087 (+14.09%) TREC 7 AvgP 0.2174 0.2169 (-0.23%) P10 0.4320 0.4480 (+3.70%) RI 0.4783 0.5652 (+18.17%) TREC 8 AvgP 0.2267 0.2268 (+0.04%) P10 0.4120 0.4340 (+5.34%) RI 0.4545 0.4545 (+0.00%) wt10g AvgP 0.1808 0.1946 (+7.63%) P10 0.2680 0.2960 (+10.45%) RI 0.0220 0.1099 (+399.5%) Tabla 4: Comparación de muestreo de documentos uniforme y ponderado por relevancia. El cambio porcentual en comparación con el muestreo uniforme se muestra entre paréntesis. QV indica que se utilizaron variantes de consulta en ambas ejecuciones. FB de línea base p(wi|R) FB de remuestreo p(wi|R) dijo 0.055 corte 0.026 corte 0.055 pagar 0.018 pagar 0.034 federal 0.012 pero 0.026 educación 0.011 empleados 0.024 maestros 0.010 sus 0.024 empleados 0.010 no 0.023 caso 0.010 federal 0.021 sus 0.009 trabajadores 0.020 apelaciones 0.008 educación 0.020 sindicato 0.007 Tabla 5: Calidad del término de retroalimentación cuando no se utiliza una lista de paradas. Términos de retroalimentación para el tema 60 de TREC: pago por mérito vs antigüedad. el efecto de las palabras vacías automáticamente. En el ejemplo de la Tabla 5, la retroalimentación de remuestreo deja solo una palabra vacía (su) en las diez primeras posiciones. Observamos un comportamiento similar del término de retroalimentación en muchos otros temas. La razón de este efecto parece ser la interacción de la puntuación de selección de términos con el límite superior de términos m. Si bien la presencia e incluso la proporción de palabras vacías particulares es bastante estable en diferentes muestras de documentos, su posición relativa en la lista de los primeros m elementos no lo es, ya que se examinan conjuntos de documentos con diferentes números de candidatos de términos mejores y de menor frecuencia para cada muestra. Como resultado, si bien algunos stopwords pueden aparecer en cada conjunto de documentos muestreados, tiende a ocurrir que cualquier stopword dado caiga por debajo del umbral para múltiples muestras, lo que lleva a su clasificación como una característica de alta varianza y bajo peso. 4. TRABAJO RELACIONADO Nuestro enfoque está relacionado con trabajos previos de varias áreas de recuperación de información y aprendizaje automático. Nuestro uso de la variación de consultas fue inspirado por el trabajo de YomTov et al. [20], Carpineto et al. [5] y Amati et al. [2], entre otros. Estos estudios utilizan la idea de crear múltiples subconsultas y luego examinar la naturaleza de la superposición en los documentos y/o términos de expansión que resultan de cada subconsulta. La combinación de modelos se realiza utilizando heurísticas. En particular, los estudios de Amati et al. y Carpineto et al. investigaron la combinación de términos de métodos de distribución individuales utilizando una heurística de combinación de reordenamiento de términos. En un conjunto de temas de TREC encontraron una amplia variación promedio en la distancia de rango de los términos de diferentes métodos de expansión. Su método de combinación proporcionó modestas mejoras positivas en la precisión promedio. La idea de examinar la superposición entre listas de términos sugeridos también ha sido utilizada en enfoques tempranos de expansión de consultas. El método de Análisis de Contexto Local (LCA) de Xu y Crofts [19] incluye un factor en la fórmula de ponderación derivada empíricamente que hace que se prefieran los términos de expansión que tienen conexiones con múltiples términos de consulta. En el lado del documento, el trabajo reciente de Zhou & Croft [21] exploró la idea de agregar ruido a los documentos, volver a puntuarlos y utilizar la estabilidad de las clasificaciones resultantes como una estimación de la dificultad de la consulta. Esto está relacionado con nuestro uso de muestreo de documentos para estimar el riesgo del modelo de retroalimentación construido a partir de los diferentes conjuntos de documentos recuperados en la parte superior. Sakai et al. [17] propusieron un enfoque para mejorar la robustez de la retroalimentación de relevancia pseudo utilizando un método que llaman muestreo selectivo. La esencia de su método es que permiten omitir algunos documentos de alta clasificación, basándose en un criterio de agrupación, para seleccionar un conjunto de documentos más variado e innovador más adelante en la clasificación para su uso por un método tradicional de retroalimentación pseudo. Su estudio no encontró mejoras significativas ni en robustez (RI) ni en MAP en sus corpora. Greiff, Morgan y Ponte [8] exploraron el papel de la varianza en la ponderación de términos. En una serie de simulaciones que simplificaron el problema a documentos de 2 características, encontraron que la precisión promedio disminuye a medida que la varianza de la frecuencia de términos - ruido alto - aumenta. La reducción del peso de los términos con alta varianza resultó en una mejora de la precisión promedio. Esto parece estar en concordancia con nuestros propios hallazgos para los modelos de retroalimentación individual. Las estimaciones de la varianza de la producción han sido utilizadas recientemente para mejorar la clasificación de textos. Lee et al. [11] utilizaron estimaciones de varianza específicas de la consulta de las salidas del clasificador para realizar una combinación de modelos mejorada. En lugar de utilizar muestreo, pudieron derivar expresiones en forma cerrada para la varianza del clasificador asumiendo clasificadores base utilizando tipos simples de redes de inferencia. Ando y Zhang propusieron un método que ellos llaman retroalimentación estructural [3] y mostraron cómo aplicarlo a la expansión de consultas para la pista de Genómica de TREC. Utilizaron variaciones de consultas r para obtener R conjuntos diferentes Sr de documentos mejor clasificados que se han intersectado con los documentos mejor clasificados obtenidos de la consulta original qorig. Para cada Si, se calcula el vector centróide normalizado ˆwi de los documentos. El análisis de componentes principales (PCA) se aplica luego a los ˆwi para obtener la matriz Φ de los vectores singulares izquierdos φh que se utilizan para obtener la nueva consulta expandida qexp = qorig + ΦT Φqorig. (7) En el caso de H = 1, tenemos un único vector singular izquierdo φ: qexp = qorig + (φT qorig)φ, de modo que el producto punto φT qorig es un tipo de peso dinámico en la consulta expandida que se basa en la similitud de la consulta original con la consulta expandida. El uso de la varianza como medida de calidad del modelo de retroalimentación ocurre de forma indirecta a través de la aplicación de PCA. Sería interesante estudiar las conexiones entre este enfoque y nuestro propio método de ajuste de modelos. Finalmente, en los enfoques de modelado del lenguaje para retroalimentación, Tao y Zhai [18] describen un método para una retroalimentación más robusta que permite que cada documento tenga un α de retroalimentación diferente. Los pesos de retroalimentación se derivan automáticamente utilizando EM regularizado. La condición de parada de EM implica un equilibrio aproximadamente igual entre el modelo de consulta y expansión. Proponen adaptar el parámetro de parada η basado en una función de alguna medida de calidad de los documentos de retroalimentación. CONCLUSIONES Hemos presentado un nuevo enfoque para la retroalimentación de relevancia pseudo basado en el muestreo de documentos y consultas. El uso del muestreo es un dispositivo muy flexible y poderoso, motivado por nuestro deseo general de ampliar los modelos actuales de recuperación mediante la estimación del riesgo o la varianza asociada con los parámetros o la salida de los procesos de recuperación. Tales estimaciones de varianza, por ejemplo, pueden ser utilizadas de forma natural en un marco bayesiano para una mejor estimación y combinación de modelos. Aplicaciones como la expansión selectiva pueden ser implementadas de manera fundamentada. Si bien nuestro estudio utiliza el enfoque de modelado del lenguaje como marco para experimentos, hacemos pocas suposiciones sobre el funcionamiento real del algoritmo de retroalimentación. Creemos que es probable que cualquier algoritmo de retroalimentación de línea base razonablemente efectivo se beneficiaría de nuestro enfoque. Nuestros resultados en colecciones estándar de TREC muestran que nuestro marco mejora la robustez de un método de retroalimentación de referencia sólido en una variedad de colecciones, sin sacrificar la precisión promedio. También proporciona pequeñas pero consistentes mejoras en la precisión del top 10. En trabajos futuros, visualizamos una investigación sobre cómo variar el conjunto de métodos de muestreo utilizados y el número de muestras controla el equilibrio entre robustez, precisión y eficiencia. Agradecimientos Agradecemos a Paul Bennett por las valiosas discusiones relacionadas con este trabajo, el cual fue apoyado por las becas de la NSF #IIS-0534345 y #CNS-0454018, y la beca del Departamento de Educación de los EE. UU. #R305G03123. Cualquier opinión, hallazgo, conclusión o recomendación expresada en este material son del autor y no necesariamente reflejan las de los patrocinadores. REFERENCIAS [1] El kit de herramientas Lemur para modelado de lenguaje y recuperación. http://www.lemurproject.org. [2] G. Amati, C. Carpineto y G. Romano. Dificultad de la consulta, robustez y aplicación selectiva de la expansión de consultas. En Actas de la 25ª Conferencia Europea sobre Recuperación de Información (ECIR 2004), páginas 127-137. [3] R. K. Ando y T. Zhang. Un método de aprendizaje semisupervisado de alto rendimiento para segmentación de texto. En Actas de la 43ª Reunión Anual de la ACL, páginas 1-9, junio de 2005. [4] L. Breiman. Empaquetando predictores. Aprendizaje automático, 24(2):123-140, 1996. [5] C. Carpineto, G. Romano y V. Giannini. Mejorando la retroalimentación de recuperación con la combinación de múltiples funciones de clasificación de términos. ACM Trans. Información. Sistemas, 20(3):259 - 290. [6] K. Collins-Thompson, P. Ogilvie y J. Callan. Resultados iniciales con consultas estructuradas y modelos de lenguaje en medio terabyte de texto. En Actas de la Conferencia de Recuperación de Información de Texto de 2005. Publicación Especial del NIST. [7] R. O. Duda, P. E. Hart y D. G. Stork. Clasificación de patrones. Wiley and Sons, 2da edición, 2001. [8] W. R. Greiff, W. T. Morgan y J. M. Ponte. El papel de la varianza en la ponderación de términos para la recuperación de información probabilística. En Actas de la 11ª Conferencia Internacional. Conf. sobre Gestión de Información y Conocimiento (CIKM 2002), páginas 252-259. [9] T. Kohonen, J. Hynninen, J. Kangas y J. Laaksonen. SOMPAK: El paquete de programas de mapas autoorganizados. Informe técnico A31, Universidad de Tecnología de Helsinki, 1996. http://www.cis.hut.fi/research/papers/som tr96.ps.Z. [10] V. Lavrenko. Una teoría generativa de relevancia. Tesis doctoral, Universidad de Massachusetts, Amherst, 2004. [11] C.-H. Lee, R. Greiner y S. Wang. Utilizando estimaciones de varianza específicas de la consulta para combinar clasificadores bayesianos. En Actas del 23º Congreso Internacional. Conf. sobre Aprendizaje Automático (ICML 2006), páginas 529-536. [12] D. Metzler y W. B. Croft. Combinando el modelo de lenguaje y los enfoques de red de inferencia para la recuperación. Información. Procesamiento y Gestión, 40(5):735-750, 2004. [13] T. Minka. Estimando una distribución de Dirichlet. Informe técnico, 2000. http://research.microsoft.com/minka/papers/dirichlet. [14] J. Ponte. Avances en la Recuperación de Información, capítulo Modelos de lenguaje para retroalimentación de relevancia, páginas 73-96. 2000. W.B. Croft, ed. [15] J. M. Ponte y W. B. Croft. Un enfoque de modelado del lenguaje para la recuperación de información. En Actas de la Conferencia ACM SIGIR de 1998 sobre Investigación y Desarrollo en Recuperación de Información, páginas 275-281. [16] J. Rocchio. El Sistema de Recuperación SMART, capítulo Retroalimentación de Relevancia en la Recuperación de Información, páginas 313-323. Prentice-Hall, 1971. G. Salton, ed. [17] T. Sakai, T. Manabe y M. Koyama. Retroalimentación de pseudo-relevancia flexible a través de muestreo selectivo. ACM Transactions on Asian Language Information Processing (TALIP), 4(2):111-135, 2005. [18] T. Tao y C. Zhai. Estimación regularizada de modelos de mezcla para retroalimentación de pseudo relevancia robusta. En Actas de la Conferencia ACM SIGIR 2006 sobre Investigación y Desarrollo en Recuperación de Información, páginas 162-169. [19] J. Xu y W. B. Croft. Mejorando la efectividad de la recuperación de información con análisis de contexto local. ACM Trans. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate to Spanish? Syst., 18(1):79-112, 2000. [20] E. YomTov, S. Fine, D. Carmel, y A. Darlow. Aprendiendo a estimar la dificultad de la consulta. En Actas de la Conferencia ACM SIGIR 2005 sobre Investigación y Desarrollo en Recuperación de Información, páginas 512-519. [21] Y. Zhou y W. B. Croft. Robustez del ranking: un nuevo marco para predecir el rendimiento de la consulta. En Actas de la 15ª Conferencia Internacional de la ACM. Conferencia sobre Gestión de Información y Conocimiento (CIKM 2006), páginas 567-574. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "risk": {
            "translated_key": "riesgo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Estimation and Use of Uncertainty in Pseudo-relevance Feedback Kevyn Collins-Thompson and Jamie Callan Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213-8213 U.S.A. {kct | callan}@cs.cmu.edu ABSTRACT Existing pseudo-relevance feedback methods typically perform averaging over the top-retrieved documents, but ignore an important statistical dimension: the <br>risk</br> or variance associated with either the individual document models, or their combination.",
                "Treating the baseline feedback method as a black box, and the output feedback model as a random variable, we estimate a posterior distribution for the feedback model by resampling a given querys top-retrieved documents, using the posterior mean or mode as the enhanced feedback model.",
                "We then perform model combination over several enhanced models, each based on a slightly modified query sampled from the original query.",
                "We find that resampling documents helps increase individual feedback model precision by removing noise terms, while sampling from the query improves robustness (worst-case performance) by emphasizing terms related to multiple query aspects.",
                "The result is a meta-feedback algorithm that is both more robust and more precise than the original strong baseline method.",
                "Categories and Subject Descriptors: H.3.3 [Information Retrieval]: Retrieval Models General Terms: Algorithms, Experimentation 1.",
                "INTRODUCTION Uncertainty is an inherent feature of information retrieval.",
                "Not only do we not know the queries that will be presented to our retrieval algorithm ahead of time, but the users information need may be vague or incompletely specified by these queries.",
                "Even if the query were perfectly specified, language in the collection documents is inherently complex and ambiguous and matching such language effectively is a formidable problem by itself.",
                "With this in mind, we wish to treat many important quantities calculated by the retrieval system, whether a relevance score for a document, or a weight for a query expansion term, as random variables whose true value is uncertain but where the uncertainty about the true value may be quantified by replacing the fixed value with a probability distribution over possible values.",
                "In this way, retrieval algorithms may attempt to quantify the <br>risk</br> or uncertainty associated with their output rankings, or improve the stability or precision of their internal calculations.",
                "Current algorithms for pseudo-relevance feedback (PRF) tend to follow the same basic method whether we use vector space-based algorithms such as Rocchios formula [16], or more recent language modeling approaches such as Relevance Models [10].",
                "First, a set of top-retrieved documents is obtained from an initial query and assumed to approximate a set of relevant documents.",
                "Next, a single feedback model vector is computed according to some sort of average, centroid, or expectation over the set of possibly-relevant document models.",
                "For example, the document vectors may be combined with equal weighting, as in Rocchio, or by query likelihood, as may be done using the Relevance Model1 .",
                "The use of an expectation is reasonable for practical and theoretical reasons, but by itself ignores potentially valuable information about the <br>risk</br> of the feedback model.",
                "Our main hypothesis in this paper is that estimating the uncertainty in feedback is useful and leads to better individual feedback models and more robust combined models.",
                "Therefore, we propose a method for estimating uncertainty associated with an individual feedback model in terms of a posterior distribution over language models.",
                "To do this, we systematically vary the inputs to the baseline feedback method and fit a Dirichlet distribution to the output.",
                "We use the posterior mean or mode as the improved feedback model estimate.",
                "This process is shown in Figure 1.",
                "As we show later, the mean and mode may vary significantly from the single feedback model proposed by the baseline method.",
                "We also perform model combination using several improved feedback language models obtained by a small number of new queries sampled from the original query.",
                "A models weight combines two complementary factors: the models probability of generating the query, and the variance of the model, with high-variance models getting lower weight. 1 For example, an expected parameter vector conditioned on the query observation is formed from top-retrieved documents, which are treated as training strings (see [10], p. 62).",
                "Figure 1: Estimating the uncertainty of the feedback model for a single query. 2.",
                "SAMPLING-BASED FEEDBACK In Sections 2.1-2.5 we describe a general method for estimating a probability distribution over the set of possible language models.",
                "In Sections 2.6 and 2.7 we summarize how different query samples are used to generate multiple feedback models, which are then combined. 2.1 Modeling Feedback Uncertainty Given a query Q and a collection C, we assume a probabilistic retrieval system that assigns a real-valued document score f(D, Q) to each document D in C, such that the score is proportional to the estimated probability of relevance.",
                "We make no other assumptions about f(D, Q).",
                "The nature of f(D, Q) may be complex: for example, if the retrieval system supports structured query languages [12], then f(D, Q) may represent the output of an arbitrarily complex inference network defined by the structured query operators.",
                "In theory, the scoring function can vary from query to query, although in this study for simplicity we keep the scoring function the same for all queries.",
                "Our specific query method is given in Section 3.",
                "We treat the feedback algorithm as a black box and assume that the inputs to the feedback algorithm are the original query and the corresponding top-retrieved documents, with a score being given to each document.",
                "We assume that the output of the feedback algorithm is a vector of term weights to be used to add or reweight the terms in the representation of the original query, with the vector normalized to form a probability distribution.",
                "We view the the inputs to the feedback black box as random variables, and analyze the feedback model as a random variable that changes in response to changes in the inputs.",
                "Like the document scoring function f(D, Q), the feedback algorithm may implement a complex, non-linear scoring formula, and so as its inputs vary, the resulting feedback models may have a complex distribution over the space of feedback models (the sample space).",
                "Because of this potential complexity, we do not attempt to derive a posterior distribution in closed form, but instead use simulation.",
                "We call this distribution over possible feedback models the feedback model distribution.",
                "Our goal in this section is to estimate a useful approximation to the feedback model distribution.",
                "For a specific framework for experiments, we use the language modeling (LM) approach for information retrieval [15].",
                "The score of a document D with respect to a query Q and collection C is given by p(Q|D) with respect to language models ˆθQ and ˆθD estimated for the query and document respectively.",
                "We denote the set of k top-retrieved documents from collection C in response to Q by DQ(k, C).",
                "For simplicity, we assume that queries and documents are generated by multinomial distributions whose parameters are represented by unigram language models.",
                "To incorporate feedback in the LM approach, we assume a model-based scheme in which our goal is take the query and resulting ranked documents DQ(k, C) as input, and output an expansion language model ˆθE, which is then interpolated with the original query model ˆθQ: ˆθNew = (1 − α) · ˆθQ + α · ˆθE (1) This includes the possibility of α = 1 where the original query mode is completely replaced by the feedback model.",
                "Our sample space is the set of all possible language models LF that may be output as feedback models.",
                "Our approach is to take samples from this space and then fit a distribution to the samples using maximum likelihood.",
                "For simplicity, we start by assuming the latent feedback distribution has the form of a Dirichlet distribution.",
                "Although the Dirichlet is a unimodal distribution, and in general quite limited in its expressiveness in the sample space, it is a natural match for the multinomial language model, can be estimated quickly, and can capture the most salient features of confident and uncertain feedback models, such as the overall spread of the distibution. 2.2 Resampling document models We would like an approximation to the posterior distribution of the feedback model LF .",
                "To accomplish this, we apply a widely-used simulation technique called bootstrap sampling ([7], p. 474) on the input parameters, namely, the set of top-retrieved documents.",
                "Bootstrap sampling allows us to simulate the approximate effect of perturbing the parameters within the black box feedback algorithm by perturbing the inputs to that algorithm in a systematic way, while making no assumptions about the nature of the feedback algorithm.",
                "Specifically, we sample k documents with replacement from DQ(k, C), and calculate an expansion language model θb using the black box feedback method.",
                "We repeat this process B times to obtain a set of B feedback language models, to which we then fit a Dirichlet distribution.",
                "Typically B is in the range of 20 to 50 samples, with performance being relatively stable in this range.",
                "Note that instead of treating each top document as equally likely, we sample according to the estimated probabilities of relevance of each document in DQ(k, C).",
                "Thus, a document is more likely to be chosen the higher it is in the ranking. 2.3 Justification for a sampling approach The rationale for our sampling approach has two parts.",
                "First, we want to improve the quality of individual feedback models by smoothing out variation when the baseline feedback model is unstable.",
                "In this respect, our approach resembles bagging [4], an ensemble approach which generates multiple versions of a predictor by making bootstrap copies of the training set, and then averages the (numerical) predictors.",
                "In our application, top-retrieved documents can be seen as a kind of noisy training set for relevance.",
                "Second, sampling is an effective way to estimate basic properties of the feedback posterior distribution, which can then be used for improved model combination.",
                "For example, a model may be weighted by its prediction confidence, estimated as a function of the variability of the posterior around the model. foo2-401.map-Dim:5434,Size:12*12units,gaussianneighborhood (a) Topic 401 Foreign minorities, Germany foo2-402.map-Dim:5698,Size:12*12units,gaussianneighborhood (b) Topic 402 Behavioral genetics foo2-459.map-Dim:8969,Size:12*12units,gaussianneighborhood (c) Topic 459 When can a lender foreclose on property Figure 2: Visualization of expansion language model variance using self-organizing maps, showing the distribution of language models that results from resampling the inputs to the baseline expansion method.",
                "The language model that would have been chosen by the baseline expansion is at the center of each map.",
                "The similarity function is JensenShannon divergence. 2.4 Visualizing feedback distributions Before describing how we fit and use the Dirichlet distribution over feedback models, it is instructive to view some examples of actual feedback model distributions that result from bootstrap sampling the top-retrieved documents from different TREC topics.",
                "Each point in our sample space is a language model, which typically has several thousand dimensions.",
                "To help analyze the behavior of our method we used a Self-Organizing Map (via the SOM-PAK package [9]), to flatten and visualize the high-dimensional density function2 .",
                "The density maps for three TREC topics are shown in Figure 2 above.",
                "The dark areas represent regions of high similarity between language models.",
                "The light areas represent regions of low similarity - the valleys between clusters.",
                "Each diagram is centered on the language model that would have been chosen by the baseline expansion.",
                "A single peak (mode) is evident in some examples, but more complex structure appears in others.",
                "Also, while the distribution is usually close to the baseline feedback model, for some topics they are a significant distance apart (as measured by JensenShannon divergence), as in Subfigure 2c.",
                "In such cases, the mode or mean of the feedback distribution often performs significantly better than the baseline (and in a smaller proportion of cases, significantly worse). 2.5 Fitting a posterior feedback distribution After obtaining feedback model samples by resampling the feedback model inputs, we estimate the feedback distribution.",
                "We assume that the multinomial feedback models {ˆθ1, . . . , ˆθB} were generated by a latent Dirichlet distribution with parameters {α1, . . . , αN }.",
                "To estimate the {α1, . . . , αN }, we fit the Dirichlet parameters to the B language model samples according to maximum likelihood using a generalized Newton procedure, details of which are given in Minka [13].",
                "We assume a simple Dirichlet prior over the {α1, . . . , αN }, setting each to αi = μ · p(wi | C), where μ is a parameter and p(· | C) is the collection language model estimated from a set of documents from collection C. The parameter fitting converges very quickly - typically just 2 or 2 Because our points are language models in the multinomial simplex, we extended SOM-PAK to support JensenShannon divergence, a widely-used similarity measure between probability distributions. 3 iterations are enough - so that it is practical to apply at query-time when computational overhead must be small.",
                "In practice, we can restrict the calculation to the vocabulary of the top-retrieved documents, instead of the entire collection.",
                "Note that for this step we are re-using the existing retrieved documents and not performing additional queries.",
                "Given the parameters of an N-dimensional Dirichlet distribution Dir(α) the mean μ and mode x vectors are easy to calculate and are given respectively by μi = αiP αi (2) and xi = αi−1P αi−N . (3) We can then choose the language model at the mean or the mode of the posterior as the final enhanced feedback model. (We found the mode to give slightly better performance.)",
                "For information retrieval, the number of samples we will have available is likely to be quite small for performance reasons - usually less than ten.",
                "Moreover, while random sampling is useful in certain cases, it is perfectly acceptable to allow deterministic sampling distributions, but these must be designed carefully in order to approximate an accurate output variance.",
                "We leave this for future study. 2.6 Query variants We use the following methods for generating variants of the original query.",
                "Each variant corresponds to a different assumption about which aspects of the original query may be important.",
                "This is a form of deterministic sampling.",
                "We selected three simple methods that cover complimentary assumptions about the query.",
                "No-expansion Use only the original query.",
                "The assumption is that the given terms are a complete description of the information need.",
                "Leave-one-out A single term is left out of the original query.",
                "The assumption is that one of the query terms is a noise term.",
                "Single-term A single term is chosen from the original query.",
                "This assumes that only one aspect of the query, namely, that represented by the term, is most important.",
                "After generating a variant of the original query, we combine it with the original query using a weight αSUB so that we do not stray too far.",
                "In this study, we set αSUB = 0.5.",
                "For example, using the Indri [12] query language, a leave-oneout variant of the initial query that omits the term ireland for TREC topic 404 is: #weight(0.5 #combine(ireland peace talks) 0.5 #combine(peace talks)) 2.7 Combining enhanced feedback models from multiple query variants When using multiple query variants, the resulting enhanced feedback models are combined using Bayesian model combination.",
                "To do this, we treat each word as an item to be classified as belonging to a relevant or non-relevant class, and derive a class probability for each word by combining the scores from each query variant.",
                "Each score is given by that terms probability in the Dirichlet distribution.",
                "The term scores are weighted by the inverse of the variance of the term in the enhanced feedback models Dirichlet distribution.",
                "The prior probability of a words membership in the relevant class is given by the probability of the original query in the entire enhanced expansion model. 3.",
                "EVALUATION In this section we present results confirming the usefulness of estimating a feedback model distribution from weighted resampling of top-ranked documents, and of combining the feedback models obtained from different small changes in the original query. 3.1 General method We evaluated performance on a total of 350 queries derived from four sets of TREC topics: 51-200 (TREC-1&2), 351-400 (TREC-7), 401-450 (TREC-8), and 451-550 (wt10g, TREC-9&10).",
                "We chose these for their varied content and document properties.",
                "For example, wt10g documents are Web pages with a wide variety of subjects and styles while TREC-1&2 documents are more homogeneous news articles.",
                "Indexing and retrieval was performed using the Indri system in the Lemur toolkit [12] [1].",
                "Our queries were derived from the words in the title field of the TREC topics.",
                "Phrases were not used.",
                "To generate the baseline queries passed to Indri, we wrapped the query terms with Indris #combine operator.",
                "For example, the initial query for topic 404 is: #combine(ireland peace talks) We performed Krovetz stemming for all experiments.",
                "Because we found that the baseline (Indri) expansion method performed better using a stopword list with the feedback model, all experiments used a stoplist of 419 common English words.",
                "However, an interesting side-effect of our resampling approach is that it tends to remove many stopwords from the feedback model, making a stoplist less critical.",
                "This is discussed further in Section 3.6. 3.2 Baseline feedback method For our baseline expansion method, we use an algorithm included in Indri 1.0 as the default expansion method.",
                "This method first selects terms using a log-odds calculation described by Ponte [14], but assigns final term weights using Lavrenkos relevance model[10].",
                "We chose the Indri method because it gives a consistently strong baseline, is based on a language modeling approach, and is simple to experiment with.",
                "In a TREC evaluation using the GOV2 corpus [6], the method was one of the topperforming runs, achieving a 19.8% gain in MAP compared to using unexpanded queries.",
                "In this study, it achieves an average gain in MAP of 17.25% over the four collections.",
                "Indris expansion method first calculates a log-odds ratio o(v) for each potential expansion term v given by o(v) = X D log p(v|D) p(v|C) (4) over all documents D containing v, in collection C. Then, the expansion term candidates are sorted by descending o(v), and the top m are chosen.",
                "Finally, the term weights r(v) used in the expanded query are calculated based on the relevance model r(v) = X D p(q|D)p(v|D) p(v) p(D) (5) The quantity p(q|D) is the probability score assigned to the document in the initial retrieval.",
                "We use Dirichlet smoothing of p(v|D) with μ = 1000.",
                "This relevance model is then combined with the original query using linear interpolation, weighted by a parameter α.",
                "By default we used the top 50 documents for feedback and the top 20 expansion terms, with the feedback interpolation parameter α = 0.5 unless otherwise stated.",
                "For example, the baseline expanded query for topic 404 is: #weight(0.5 #combine(ireland peace talks) 0.5 #weight(0.10 ireland 0.08 peace 0.08 northern ...) 3.3 Expansion performance We measure our feedback algorithms effectiveness by two main criteria: precision, and robustness.",
                "Robustness, and the tradeoff between precision and robustness, is analyzed in Section 3.4.",
                "In this section, we examine average precision and precision in the top 10 documents (P10).",
                "We also include recall at 1,000 documents.",
                "For each query, we obtained a set of B feedback models using the Indri baseline.",
                "Each feedback model was obtained from a random sample of the top k documents taken with replacement.",
                "For these experiments, B = 30 and k = 50.",
                "Each feedback model contained 20 terms.",
                "On the query side, we used leave-one-out (LOO) sampling to create the query variants.",
                "Single-term query sampling had consistently worse performance across all collections and so our results here focus on LOO sampling.",
                "We used the methods described in Section 2 to estimate an enhanced feedback model from the Dirichlet posterior distribution for each query variant, and to combine the feedback models from all the query variants.",
                "We call our method resampling expansion and denote it as RS-FB here.",
                "We denote the Indri baseline feedback method as Base-FB.",
                "Results from applying both the baseline expansion method (Base-FB) and resampling expansion (RS-FB) are shown in Table 1.",
                "We observe several trends in this table.",
                "First, the average precision of RS-FB was comparable to Base-FB, achieving an average gain of 17.6% compared to using no expansion across the four collections.",
                "The Indri baseline expansion gain was 17.25%.",
                "Also, the RS-FB method achieved consistent improvements in P10 over Base-FB for every topic set, with an average improvement of 6.89% over Base-FB for all 350 topics.",
                "The lowest P10 gain over Base-FB was +3.82% for TREC-7 and the highest was +11.95% for wt10g.",
                "Finally, both Base-FB and RS-FB also consistently improved recall over using no expansion, with Base-FB achieving better recall than RS-FB for all topic sets. 3.4 Retrieval robustness We use the term robustness to mean the worst-case average precision performance of a feedback algorithm.",
                "Ideally, a robust feedback method would never perform worse than using the original query, while often performing better using the expansion.",
                "To evaluate robustness in this study, we use a very simple measure called the robustness index (RI)3 .",
                "For a set of queries Q, the RI measure is defined as: RI(Q) = n+ − n− |Q| (6) where n+ is the number of queries helped by the feedback method and n− is the number of queries hurt.",
                "Here, by helped we mean obtaining a higher average precision as a result of feedback.",
                "The value of RI ranges from a minimum 3 This is sometimes also called the reliability of improvement index and was used in Sakai et al. [17].",
                "Collection NoExp Base-FB RS-FB TREC 1&2 AvgP 0.1818 0.2419 (+33.04%) 0.2406 (+32.24%) P10 0.4443 0.4913 (+10.57%) 0.5363 (+17.83%) Recall 15084/37393 19172/37393 15396/37393 TREC 7 AvgP 0.1890 0.2175 (+15.07%) 0.2169 (+14.75%) P10 0.4200 0.4320 (+2.85%) 0.4480 (+6.67%) Recall 2179/4674 2608/4674 2487/4674 TREC 8 AvgP 0.2031 0.2361 (+16.25%) 0.2268 (+11.70%) P10 0.3960 0.4160 (+5.05%) 0.4340 (+9.59%) Recall 2144/4728 2642/4728 2485/4728 wt10g AvgP 0.1741 0.1829 (+5.06%) 0.1946 (+11.78%) P10 0.2760 0.2630 (-4.71%) 0.2960 (+7.24%) Recall 3361/5980 3725/5980 3664/5980 Table 1: Comparison of baseline (Base-FB) feedback and feedback using re-sampling (RS-FB).",
                "Improvement shown for BaseFB and RS-FB is relative to using no expansion. (a) TREC 1&2 (upper curve); TREC 8 (lower curve) (b) TREC 7 (upper curve); wt10g (lower curve) Figure 3: The trade-off between robustness and average precision for different corpora.",
                "The x-axis gives the change in MAP over using baseline expansion with α = 0.5.",
                "The yaxis gives the Robustness Index (RI).",
                "Each curve through uncircled points shows the RI/MAP tradeoff using the simple small-α strategy (see text) as α decreases from 0.5 to zero in the direction of the arrow.",
                "Circled points represent the tradeoffs obtained by resampling feedback for α = 0.5.",
                "Collection N Base-FB RS-FB n− RI n− RI TREC 1&2 103 26 +0.495 15 +0.709 TREC 7 46 14 +0.391 10 +0.565 TREC 8 44 12 +0.455 12 +0.455 wt10g 91 48 -0.055 39 +0.143 Combined 284 100 +0.296 76 +0.465 Table 2: Comparison of robustness index (RI) for baseline feedback (Base-FB) vs. resampling feedback (RS-FB).",
                "Also shown are the actual number of queries hurt by feedback (n−) for each method and collection.",
                "Queries for which initial average precision was negligible (≤ 0.01) were ignored, giving the remaining query count in column N. of −1.0, when all queries are hurt by the feedback method, to +1.0 when all queries are helped.",
                "The RI measure does not take into account the magnitude or distribution of the amount of change across the set Q.",
                "However, it is easy to understand as a general indication of robustness.",
                "One obvious way to improve the worst-case performance of feedback is simply to use a smaller fixed α interpolation parameter, such as α = 0.3, placing less weight on the (possibly risky) feedback model and more on the original query.",
                "We call this the small-α strategy.",
                "Since we are also reducing the potential gains when the feedback model is right, however, we would expect some trade-off between average precision and robustness.",
                "We therefore compared the precision/robustness trade-off between our resampling feedback algorithm, and the simple small-α method.",
                "The results are summarized in Figure 3.",
                "In the figure, the curve for each topic set interpolates between trade-off points, beginning at x=0, where α = 0.5, and continuing in the direction of the arrow as α decreases and the original query is given more and more weight.",
                "As expected, robustness continuously increases as we move along the curve, but mean average precision generally drops as the gains from feedback are eliminated.",
                "For comparison, the performance of resampling feedback at α = 0.5 is shown for each collection as the circled point.",
                "Higher and to the right is better.",
                "This figure shows that resampling feedback gives a somewhat better trade-off than the small-α approach for 3 of the 4 collections.",
                "Figure 4: Histogram showing improved robustness of resampling feedback (RS-FB) over baseline feedback (Base-FB) for all datasets combined.",
                "Queries are binned by % change in AP compared to the unexpanded query.",
                "Collection DS + QV DS + No QV TREC 1&2 AvgP 0.2406 0.2547 (+5.86%) P10 0.5263 0.5362 (+1.88%) RI 0.7087 0.6515 (-0.0572) TREC 7 AvgP 0.2169 0.2200 (+1.43%) P10 0.4480 0.4300 (-4.02%) RI 0.5652 0.2609 (-0.3043) TREC 8 AvgP 0.2268 0.2257 (-0.49%) P10 0.4340 0.4200 (-3.23%) RI 0.4545 0.4091 (-0.0454) wt10g AvgP 0.1946 0.1865 (-4.16%) P10 0.2960 0.2680 (-9.46%) RI 0.1429 0.0220 (-0.1209) Table 3: Comparison of resampling feedback using document sampling (DS) with (QV) and without (No QV) combining feedback models from multiple query variants.",
                "Table 2 gives the Robustness Index scores for Base-FB and RS-FB.",
                "The RS-FB feedback method obtained higher robustness than Base-FB on three of the four topic sets, with only slightly worse performance on TREC-8.",
                "A more detailed view showing the distribution over relative changes in AP is given by the histogram in Figure 4.",
                "Compared to Base-FB, the RS-FB method achieves a noticable reduction in the number of queries significantly hurt by expansion (i.e. where AP is hurt by 25% or more), while preserving positive gains in AP. 3.5 Effect of query and document sampling methods Given our algorithms improved robustness seen in Section 3.4, an important question is what component of our system is responsible.",
                "Is it the use of document re-sampling, the use of multiple query variants, or some other factor?",
                "The results in Table 3 suggest that the model combination based on query variants may be largely account for the improved robustness.",
                "When query variants are turned off and the original query is used by itself with document sampling, there is little net change in average precision, a small decrease in P10 for 3 out of the 4 topic sets, but a significant drop in robustness for all topic sets.",
                "In two cases, the RI measure drops by more than 50%.",
                "We also examined the effect of the document sampling method on retrieval effectiveness, using two different strategies.",
                "The uniform weighting strategy ignored the relevance scores from the initial retrieval and gave each document in the top k the same probability of selection.",
                "In contrast, the relevance-score weighting strategy chose documents with probability proportional to their relevance scores.",
                "In this way, documents that were more highly ranked were more likely to be selected.",
                "Results are shown in Table 4.",
                "The relevance-score weighting strategy performs better overall, with significantly higher RI and P10 scores on 3 of the 4 topic sets.",
                "The difference in average precision between the methods, however, is less marked.",
                "This suggests that uniform weighting acts to increase variance in retrieval results: when initial average precision is high, there are many relevant documents in the top k and uniform sampling may give a more representative relevance model than focusing on the highly-ranked items.",
                "On the other hand, when initial precision is low, there are few relevant documents in the bottom ranks and uniform sampling mixes in more of the non-relevant documents.",
                "For space reasons we only summarize our findings on sample size here.",
                "The number of samples has some effect on precision when less than 10, but performance stabilizes at around 15 to 20 samples.",
                "We used 30 samples for our experiments.",
                "Much beyond this level, the additional benefits of more samples decrease as the initial score distribution is more closely fit and the processing time increases. 3.6 The effect of resampling on expansion term quality Ideally, a retrieval model should not require a stopword list when estimating a model of relevance: a robust statistical model should down-weight stopwords automatically depending on context.",
                "Stopwords can harm feedback if selected as feedback terms, because they are typically poor discriminators and waste valuable term slots.",
                "In practice, however, because most term selection methods resemble a tf · idf type of weighting, terms with low idf but very high tf can sometimes be selected as expansion term candidates.",
                "This happens, for example, even with the Relevance Model approach that is part of our baseline feedback.",
                "To ensure as strong a baseline as possible, we use a stoplist for all experiments reported here.",
                "If we turn off the stopword list, however, we obtain results such as those shown in Table 5 where four of the top ten baseline feedback terms for TREC topic 60 (said, but, their, not) are stopwords using the BaseFB method. (The top 100 expansion terms were selected to generate this example.)",
                "Indris method attempts to address the stopword problem by applying an initial step based on Ponte [14] to select less-common terms that have high log-odds of being in the top-ranked documents compared to the whole collection.",
                "Nevertheless, this does not overcome the stopword problem completely, especially as the number of feedback terms grows.",
                "Using resampling feedback, however, appears to mitigate Collection QV + Uniform QV + Relevance-score weighting weighting TREC 1&2 AvgP 0.2545 0.2406 (-5.46%) P10 0.5369 0.5263 (-1.97%) RI 0.6212 0.7087 (+14.09%) TREC 7 AvgP 0.2174 0.2169 (-0.23%) P10 0.4320 0.4480 (+3.70%) RI 0.4783 0.5652 (+18.17%) TREC 8 AvgP 0.2267 0.2268 (+0.04%) P10 0.4120 0.4340 (+5.34%) RI 0.4545 0.4545 (+0.00%) wt10g AvgP 0.1808 0.1946 (+7.63%) P10 0.2680 0.2960 (+10.45%) RI 0.0220 0.1099 (+399.5%) Table 4: Comparison of uniform and relevance-weighted document sampling.",
                "The percentage change compared to uniform sampling is shown in parentheses.",
                "QV indicates that query variants were used in both runs.",
                "Baseline FB p(wi|R) Resampling FB p(wi|R) said 0.055 court 0.026 court 0.055 pay 0.018 pay 0.034 federal 0.012 but 0.026 education 0.011 employees 0.024 teachers 0.010 their 0.024 employees 0.010 not 0.023 case 0.010 federal 0.021 their 0.009 workers 0.020 appeals 0.008 education 0.020 union 0.007 Table 5: Feedback term quality when a stoplist is not used.",
                "Feedback terms for TREC topic 60: merit pay vs seniority. the effect of stopwords automatically.",
                "In the example of Table 5, resampling feedback leaves only one stopword (their) in the top ten.",
                "We observed similar feedback term behavior across many other topics.",
                "The reason for this effect appears to be the interaction of the term selection score with the top-m term cutoff.",
                "While the presence and even proportion of particular stopwords is fairly stable across different document samples, their relative position in the top-m list is not, as sets of documents with varying numbers of better, lower-frequency term candidates are examined for each sample.",
                "As a result, while some number of stopwords may appear in each sampled document set, any given stopword tends to fall below the cutoff for multiple samples, leading to its classification as a high-variance, low-weight feature. 4.",
                "RELATED WORK Our approach is related to previous work from several areas of information retrieval and machine learning.",
                "Our use of query variation was inspired by the work of YomTov et al. [20], Carpineto et al. [5], and Amati et al. [2], among others.",
                "These studies use the idea of creating multiple subqueries and then examining the nature of the overlap in the documents and/or expansion terms that result from each subquery.",
                "Model combination is performed using heuristics.",
                "In particular, the studies of Amati et al. and Carpineto et al. investigated combining terms from individual distributional methods using a term-reranking combination heuristic.",
                "In a set of TREC topics they found wide average variation in the rank-distance of terms from different expansion methods.",
                "Their combination method gave modest positive improvements in average precision.",
                "The idea of examining the overlap between lists of suggested terms has also been used in early query expansion approaches.",
                "Xu and Crofts method of Local Context Analysis (LCA) [19] includes a factor in the empirically-derived weighting formula that causes expansion terms to be preferred that have connections to multiple query terms.",
                "On the document side, recent work by Zhou & Croft [21] explored the idea of adding noise to documents, re-scoring them, and using the stability of the resulting rankings as an estimate of query difficulty.",
                "This is related to our use of document sampling to estimate the <br>risk</br> of the feedback model built from the different sets of top-retrieved documents.",
                "Sakai et al. [17] proposed an approach to improving the robustness of pseudo-relevance feedback using a method they call selective sampling.",
                "The essence of their method is that they allow skipping of some top-ranked documents, based on a clustering criterion, in order to select a more varied and novel set of documents later in the ranking for use by a traditional pseudo-feedback method.",
                "Their study did not find significant improvements in either robustness (RI) or MAP on their corpora.",
                "Greiff, Morgan and Ponte [8] explored the role of variance in term weighting.",
                "In a series of simulations that simplified the problem to 2-feature documents, they found that average precision degrades as term frequency variance - high noiseincreases.",
                "Downweighting terms with high variance resulted in improved average precision.",
                "This seems in accord with our own findings for individual feedback models.",
                "Estimates of output variance have recently been used for improved text classification.",
                "Lee et al. [11] used queryspecific variance estimates of classifier outputs to perform improved model combination.",
                "Instead of using sampling, they were able to derive closed-form expressions for classifier variance by assuming base classifiers using simple types of inference networks.",
                "Ando and Zhang proposed a method that they call structural feedback [3] and showed how to apply it to query expansion for the TREC Genomics Track.",
                "They used r query variations to obtain R different sets Sr of top-ranked documents that have been intersected with the top-ranked documents obtained from the original query qorig.",
                "For each Si, the normalized centroid vector ˆwi of the documents is calculated.",
                "Principal component analysis (PCA) is then applied to the ˆwi to obtain the matrix Φ of H left singular vectors φh that are used to obtain the new, expanded query qexp = qorig + ΦT Φqorig. (7) In the case H = 1, we have a single left singular vector φ: qexp = qorig + (φT qorig)φ so that the dot product φT qorig is a type of dynamic weight on the expanded query that is based on the similarity of the original query to the expanded query.",
                "The use of variance as a feedback model quality measure occurs indirectly through the application of PCA.",
                "It would be interesting to study the connections between this approach and our own modelfitting method.",
                "Finally, in language modeling approaches to feedback, Tao and Zhai [18] describe a method for more robust feedback that allows each document to have a different feedback α.",
                "The feedback weights are derived automatically using regularized EM.",
                "A roughly equal balance of query and expansion model is implied by their EM stopping condition.",
                "They propose tailoring the stopping parameter η based on a function of some quality measure of feedback documents. 5.",
                "CONCLUSIONS We have presented a new approach to pseudo-relevance feedback based on document and query sampling.",
                "The use of sampling is a very flexible and powerful device and is motivated by our general desire to extend current models of retrieval by estimating the <br>risk</br> or variance associated with the parameters or output of retrieval processes.",
                "Such variance estimates, for example, may be naturally used in a Bayesian framework for improved model estimation and combination.",
                "Applications such as selective expansion may then be implemented in a principled way.",
                "While our study uses the language modeling approach as a framework for experiments, we make few assumptions about the actual workings of the feedback algorithm.",
                "We believe it is likely that any reasonably effective baseline feedback algorithm would benefit from our approach.",
                "Our results on standard TREC collections show that our framework improves the robustness of a strong baseline feedback method across a variety of collections, without sacrificing average precision.",
                "It also gives small but consistent gains in top10 precision.",
                "In future work, we envision an investigation into how varying the set of sampling methods used and the number of samples controls the trade-off between robustness, accuracy, and efficiency.",
                "Acknowledgements We thank Paul Bennett for valuable discussions related to this work, which was supported by NSF grants #IIS-0534345 and #CNS-0454018, and U.S. Dept. of Education grant #R305G03123.",
                "Any opinions, findings, and conclusions or recommendations expressed in this material are the authors. and do not necessarily reflect those of the sponsors. 6.",
                "REFERENCES [1] The Lemur toolkit for language modeling and retrieval. http://www.lemurproject.org. [2] G. Amati, C. Carpineto, and G. Romano.",
                "Query difficulty, robustness, and selective application of query expansion.",
                "In Proc. of the 25th European Conf. on Information Retrieval (ECIR 2004), pages 127-137. [3] R. K. Ando and T. Zhang.",
                "A high-performance semi-supervised learning method for text chunking.",
                "In Proc. of the 43rd Annual Meeting of the ACL, pages 1-9, June 2005. [4] L. Breiman.",
                "Bagging predictors.",
                "Machine Learning, 24(2):123-140, 1996. [5] C. Carpineto, G. Romano, and V. Giannini.",
                "Improving retrieval feedback with multiple term-ranking function combination.",
                "ACM Trans.",
                "Info.",
                "Systems, 20(3):259 - 290. [6] K. Collins-Thompson, P. Ogilvie, and J. Callan.",
                "Initial results with structured queries and language models on half a terabyte of text.",
                "In Proc. of 2005 Text REtrieval Conference.",
                "NIST Special Publication. [7] R. O. Duda, P. E. Hart, and D. G. Stork.",
                "Pattern Classification.",
                "Wiley and Sons, 2nd edition, 2001. [8] W. R. Greiff, W. T. Morgan, and J. M. Ponte.",
                "The role of variance in term weighting for probabilistic information retrieval.",
                "In Proc. of the 11th Intl.",
                "Conf. on Info. and Knowledge Mgmt. (CIKM 2002), pages 252-259. [9] T. Kohonen, J. Hynninen, J. Kangas, and J. Laaksonen.",
                "SOMPAK: The self-organizing map program package.",
                "Technical Report A31, Helsinki University of Technology, 1996. http://www.cis.hut.fi/research/papers/som tr96.ps.Z. [10] V. Lavrenko.",
                "A Generative Theory of Relevance.",
                "PhD thesis, University of Massachusetts, Amherst, 2004. [11] C.-H. Lee, R. Greiner, and S. Wang.",
                "Using query-specific variance estimates to combine Bayesian classifiers.",
                "In Proc. of the 23rd Intl.",
                "Conf. on Machine Learning (ICML 2006), pages 529-536. [12] D. Metzler and W. B. Croft.",
                "Combining the language model and inference network approaches to retrieval.",
                "Info.",
                "Processing and Mgmt., 40(5):735-750, 2004. [13] T. Minka.",
                "Estimating a Dirichlet distribution.",
                "Technical report, 2000. http://research.microsoft.com/ minka/papers/dirichlet. [14] J. Ponte.",
                "Advances in Information Retrieval, chapter Language models for relevance feedback, pages 73-96. 2000.",
                "W.B.",
                "Croft, ed. [15] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In Proc. of the 1998 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 275-281. [16] J. Rocchio.",
                "The SMART Retrieval System, chapter Relevance Feedback in Information Retrieval, pages 313-323.",
                "Prentice-Hall, 1971.",
                "G. Salton, ed. [17] T. Sakai, T. Manabe, and M. Koyama.",
                "Flexible pseudo-relevance feedback via selective sampling.",
                "ACM Transactions on Asian Language Information Processing (TALIP), 4(2):111-135, 2005. [18] T. Tao and C. Zhai.",
                "Regularized estimation of mixture models for robust pseudo-relevance feedback.",
                "In Proc. of the 2006 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 162-169. [19] J. Xu and W. B. Croft.",
                "Improving the effectiveness of information retrieval with local context analysis.",
                "ACM Trans.",
                "Inf.",
                "Syst., 18(1):79-112, 2000. [20] E. YomTov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty.",
                "In Proc. of the 2005 ACM SIGIR Conf. on Research and Development in Information Retrieval, pages 512-519. [21] Y. Zhou and W. B. Croft.",
                "Ranking robustness: a novel framework to predict query performance.",
                "In Proc. of the 15th ACM Intl.",
                "Conf. on Information and Knowledge Mgmt. (CIKM 2006), pages 567-574."
            ],
            "original_annotated_samples": [
                "Estimation and Use of Uncertainty in Pseudo-relevance Feedback Kevyn Collins-Thompson and Jamie Callan Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213-8213 U.S.A. {kct | callan}@cs.cmu.edu ABSTRACT Existing pseudo-relevance feedback methods typically perform averaging over the top-retrieved documents, but ignore an important statistical dimension: the <br>risk</br> or variance associated with either the individual document models, or their combination.",
                "In this way, retrieval algorithms may attempt to quantify the <br>risk</br> or uncertainty associated with their output rankings, or improve the stability or precision of their internal calculations.",
                "The use of an expectation is reasonable for practical and theoretical reasons, but by itself ignores potentially valuable information about the <br>risk</br> of the feedback model.",
                "This is related to our use of document sampling to estimate the <br>risk</br> of the feedback model built from the different sets of top-retrieved documents.",
                "The use of sampling is a very flexible and powerful device and is motivated by our general desire to extend current models of retrieval by estimating the <br>risk</br> or variance associated with the parameters or output of retrieval processes."
            ],
            "translated_annotated_samples": [
                "Estimación y Uso de la Incertidumbre en la Retroalimentación de Pseudo-Relevancia Kevyn Collins-Thompson y Jamie Callan Instituto de Tecnologías del Lenguaje Escuela de Ciencias de la Computación Universidad Carnegie Mellon Pittsburgh, PA 15213-8213 EE. UU. {kct | callan}@cs.cmu.edu RESUMEN Los métodos existentes de retroalimentación de pseudo-relevancia suelen realizar un promedio de los documentos mejor clasificados, pero ignoran una dimensión estadística importante: el <br>riesgo</br> o la varianza asociada tanto a los modelos de documentos individuales como a su combinación.",
                "De esta manera, los algoritmos de recuperación pueden intentar cuantificar el <br>riesgo</br> o la incertidumbre asociados con sus clasificaciones de salida, o mejorar la estabilidad o precisión de sus cálculos internos.",
                "El uso de una expectativa es razonable por razones prácticas y teóricas, pero por sí sola ignora información potencialmente valiosa sobre el <br>riesgo</br> del modelo de retroalimentación.",
                "Esto está relacionado con nuestro uso de muestreo de documentos para estimar el <br>riesgo</br> del modelo de retroalimentación construido a partir de los diferentes conjuntos de documentos recuperados en la parte superior.",
                "El uso del muestreo es un dispositivo muy flexible y poderoso, motivado por nuestro deseo general de ampliar los modelos actuales de recuperación mediante la estimación del <br>riesgo</br> o la varianza asociada con los parámetros o la salida de los procesos de recuperación."
            ],
            "translated_text": "Estimación y Uso de la Incertidumbre en la Retroalimentación de Pseudo-Relevancia Kevyn Collins-Thompson y Jamie Callan Instituto de Tecnologías del Lenguaje Escuela de Ciencias de la Computación Universidad Carnegie Mellon Pittsburgh, PA 15213-8213 EE. UU. {kct | callan}@cs.cmu.edu RESUMEN Los métodos existentes de retroalimentación de pseudo-relevancia suelen realizar un promedio de los documentos mejor clasificados, pero ignoran una dimensión estadística importante: el <br>riesgo</br> o la varianza asociada tanto a los modelos de documentos individuales como a su combinación. Tratando el método de retroalimentación de línea base como una caja negra, y el modelo de retroalimentación de salida como una variable aleatoria, estimamos una distribución posterior para el modelo de retroalimentación mediante el remuestreo de los documentos mejor recuperados de una consulta dada, utilizando la media o la moda posterior como el modelo de retroalimentación mejorado. Luego realizamos la combinación de modelos sobre varios modelos mejorados, cada uno basado en una consulta ligeramente modificada muestreada de la consulta original. Observamos que el remuestreo de documentos ayuda a aumentar la precisión del modelo de retroalimentación individual al eliminar términos de ruido, mientras que el muestreo de la consulta mejora la robustez (rendimiento en el peor de los casos) al enfatizar términos relacionados con múltiples aspectos de la consulta. El resultado es un algoritmo de meta-retroalimentación que es tanto más robusto como más preciso que el método de referencia original. Categorías y Descriptores de Asignaturas: H.3.3 [Recuperación de Información]: Modelos de Recuperación Términos Generales: Algoritmos, Experimentación 1. INTRODUCCIÓN La incertidumbre es una característica inherente de la recuperación de información. No solo no sabemos las consultas que se presentarán a nuestro algoritmo de recuperación con anticipación, sino que la necesidad de información de los usuarios puede ser vaga o estar incompletamente especificada por estas consultas. Incluso si la consulta estuviera perfectamente especificada, el lenguaje en los documentos de la colección es inherentemente complejo y ambiguo, y hacer coincidir dicho lenguaje de manera efectiva es un problema formidable por sí mismo. Con esto en mente, deseamos tratar muchas cantidades importantes calculadas por el sistema de recuperación, ya sea un puntaje de relevancia para un documento, o un peso para un término de expansión de consulta, como variables aleatorias cuyo valor real es incierto pero donde la incertidumbre sobre el valor real puede ser cuantificada reemplazando el valor fijo con una distribución de probabilidad sobre posibles valores. De esta manera, los algoritmos de recuperación pueden intentar cuantificar el <br>riesgo</br> o la incertidumbre asociados con sus clasificaciones de salida, o mejorar la estabilidad o precisión de sus cálculos internos. Los algoritmos actuales para la retroalimentación de pseudo relevancia (PRF) tienden a seguir el mismo método básico ya sea que utilicemos algoritmos basados en espacio vectorial como la fórmula de Rocchio [16], o enfoques más recientes de modelado de lenguaje como Modelos de Relevancia [10]. Primero, se obtiene un conjunto de documentos recuperados en la parte superior a partir de una consulta inicial y se asume que aproxima un conjunto de documentos relevantes. A continuación, se calcula un vector de modelo de retroalimentación único según algún tipo de promedio, centroide o expectativa sobre el conjunto de modelos de documentos posiblemente relevantes. Por ejemplo, los vectores de documentos pueden combinarse con igual ponderación, como en Rocchio, o por probabilidad de consulta, como se puede hacer utilizando el Modelo de Relevancia. El uso de una expectativa es razonable por razones prácticas y teóricas, pero por sí sola ignora información potencialmente valiosa sobre el <br>riesgo</br> del modelo de retroalimentación. Nuestra hipótesis principal en este artículo es que estimar la incertidumbre en la retroalimentación es útil y conduce a modelos de retroalimentación individual más efectivos y a modelos combinados más robustos. Por lo tanto, proponemos un método para estimar la incertidumbre asociada con un modelo de retroalimentación individual en términos de una distribución posterior sobre modelos de lenguaje. Para hacer esto, variamos sistemáticamente las entradas al método de retroalimentación base y ajustamos una distribución de Dirichlet a la salida. Utilizamos la media posterior o el modo como la estimación mejorada del modelo de retroalimentación. Este proceso se muestra en la Figura 1. Como mostraremos más adelante, la media y la moda pueden variar significativamente del modelo de retroalimentación única propuesto por el método de referencia. También realizamos la combinación de modelos utilizando varios modelos de lenguaje de retroalimentación mejorados obtenidos a partir de un pequeño número de nuevas consultas muestreadas de la consulta original. El peso de un modelo combina dos factores complementarios: la probabilidad del modelo de generar la consulta y la varianza del modelo, siendo los modelos con alta varianza los que obtienen un peso menor. Por ejemplo, un vector de parámetros esperado condicionado a la observación de la consulta se forma a partir de los documentos recuperados en la parte superior, que se tratan como cadenas de entrenamiento (ver [10], p. 62). Figura 1: Estimación de la incertidumbre del modelo de retroalimentación para una sola consulta. 2. En las Secciones 2.1-2.5 describimos un método general para estimar una distribución de probabilidad sobre el conjunto de posibles modelos de lenguaje basado en muestreo. En las Secciones 2.6 y 2.7 resumimos cómo se utilizan diferentes muestras de consultas para generar múltiples modelos de retroalimentación, que luego se combinan. 2.1 Modelado de la Incertidumbre de la Retroalimentación Dado una consulta Q y una colección C, asumimos un sistema de recuperación probabilístico que asigna un puntaje de documento de valor real f(D, Q) a cada documento D en C, de manera que el puntaje sea proporcional a la probabilidad estimada de relevancia. No hacemos ninguna otra suposición sobre f(D, Q). La naturaleza de f(D, Q) puede ser compleja: por ejemplo, si el sistema de recuperación admite lenguajes de consulta estructurados [12], entonces f(D, Q) puede representar la salida de una red de inferencia arbitrariamente compleja definida por los operadores de consulta estructurados. En teoría, la función de puntuación puede variar de una consulta a otra, aunque en este estudio por simplicidad mantenemos la misma función de puntuación para todas las consultas. Nuestro método de consulta específico se encuentra en la Sección 3. Tratamos el algoritmo de retroalimentación como una caja negra y asumimos que los insumos del algoritmo de retroalimentación son la consulta original y los documentos recuperados principales correspondientes, a los cuales se les asigna una puntuación a cada documento. Suponemos que la salida del algoritmo de retroalimentación es un vector de pesos de términos que se utilizarán para agregar o reponderar los términos en la representación de la consulta original, con el vector normalizado para formar una distribución de probabilidad. Consideramos las entradas a la caja negra de retroalimentación como variables aleatorias, y analizamos el modelo de retroalimentación como una variable aleatoria que cambia en respuesta a cambios en las entradas. Al igual que la función de puntuación del documento f(D, Q), el algoritmo de retroalimentación puede implementar una fórmula de puntuación compleja y no lineal, por lo que, a medida que sus entradas varían, los modelos de retroalimentación resultantes pueden tener una distribución compleja en el espacio de modelos de retroalimentación (el espacio muestral). Debido a esta complejidad potencial, no intentamos derivar una distribución posterior de forma cerrada, sino que en su lugar utilizamos simulación. Llamamos a esta distribución sobre posibles modelos de retroalimentación la distribución de modelos de retroalimentación. Nuestro objetivo en esta sección es estimar una aproximación útil a la distribución del modelo de retroalimentación. Para un marco específico de experimentos, utilizamos el enfoque de modelado de lenguaje (LM) para la recuperación de información [15]. El puntaje de un documento D con respecto a una consulta Q y una colección C se da por p(Q|D) con respecto a los modelos de lenguaje ˆθQ y ˆθD estimados para la consulta y el documento respectivamente. Denotamos el conjunto de los k documentos mejor recuperados de la colección C en respuesta a Q como DQ(k, C). Para simplificar, asumimos que las consultas y documentos son generados por distribuciones multinomiales cuyos parámetros están representados por modelos de lenguaje unigram. Para incorporar retroalimentación en el enfoque de LM, asumimos un esquema basado en modelos en el que nuestro objetivo es tomar la consulta y los documentos clasificados resultantes DQ(k, C) como entrada, y producir un modelo de lenguaje expandido ˆθE, que luego se interpola con el modelo de consulta original ˆθQ: ˆθNew = (1 − α) · ˆθQ + α · ˆθE (1). Esto incluye la posibilidad de α = 1 donde el modelo de consulta original es completamente reemplazado por el modelo de retroalimentación. Nuestro espacio muestral es el conjunto de todos los posibles modelos de lenguaje LF que pueden ser generados como modelos de retroalimentación. Nuestro enfoque es tomar muestras de este espacio y luego ajustar una distribución a las muestras utilizando máxima verosimilitud. Para simplificar, comenzamos asumiendo que la distribución de retroalimentación latente tiene la forma de una distribución de Dirichlet. Aunque la distribución de Dirichlet es unimodal y en general bastante limitada en su expresividad en el espacio muestral, es una coincidencia natural para el modelo de lenguaje multinomial, puede ser estimada rápidamente y capturar las características más relevantes de los modelos de retroalimentación confiada e incierta, como la dispersión general de la distribución. 2.2 Re-muestreo de modelos de documentos. Nos gustaría una aproximación a la distribución posterior del modelo de retroalimentación LF. Para lograr esto, aplicamos una técnica de simulación ampliamente utilizada llamada muestreo bootstrap ([7], p. 474) en los parámetros de entrada, es decir, el conjunto de documentos recuperados en la parte superior. El muestreo de arranque nos permite simular el efecto aproximado de perturbar los parámetros dentro del algoritmo de retroalimentación de la caja negra al perturbar los insumos de ese algoritmo de manera sistemática, sin hacer suposiciones sobre la naturaleza del algoritmo de retroalimentación. Específicamente, muestreamos k documentos con reemplazo de DQ(k, C), y calculamos un modelo de lenguaje de expansión θb utilizando el método de retroalimentación de caja negra. Repetimos este proceso B veces para obtener un conjunto de B modelos de lenguaje de retroalimentación, a los cuales luego ajustamos una distribución de Dirichlet. Normalmente, B se encuentra en el rango de 20 a 50 muestras, con un rendimiento relativamente estable en este rango. Ten en cuenta que en lugar de tratar cada documento superior como igualmente probable, muestreamos de acuerdo con las probabilidades estimadas de relevancia de cada documento en DQ(k, C). Por lo tanto, es más probable que un documento sea elegido cuanto más alto esté en la clasificación. 2.3 Justificación de un enfoque de muestreo La justificación de nuestro enfoque de muestreo tiene dos partes. Primero, queremos mejorar la calidad de los modelos de retroalimentación individual al suavizar la variación cuando el modelo de retroalimentación base es inestable. En este sentido, nuestro enfoque se asemeja al bagging [4], un enfoque de conjunto que genera múltiples versiones de un predictor haciendo copias de arranque del conjunto de entrenamiento, y luego promedia los predictores (numéricos). En nuestra aplicación, los documentos recuperados en la parte superior pueden ser vistos como una especie de conjunto de entrenamiento ruidoso para la relevancia. Segundo, el muestreo es una forma efectiva de estimar propiedades básicas de la distribución posterior de retroalimentación, las cuales pueden ser utilizadas para mejorar la combinación de modelos. Por ejemplo, un modelo puede ser ponderado por su confianza de predicción, estimada como una función de la variabilidad del posterior alrededor del modelo. foo2-401.map-Dim:5434,Size:12*12units, vecindario gaussiano (a) Tema 401 Minorías extranjeras, Alemania foo2-402.map-Dim:5698,Size:12*12units, vecindario gaussiano (b) Tema 402 Genética del comportamiento foo2-459.map-Dim:8969,Size:12*12units, vecindario gaussiano (c) Tema 459 ¿Cuándo puede un prestamista ejecutar una hipoteca sobre una propiedad? Figura 2: Visualización de la varianza del modelo de lenguaje de expansión utilizando mapas autoorganizados, mostrando la distribución de modelos de lenguaje que resulta de remuestrear las entradas al método de expansión de referencia. El modelo de lenguaje que habría sido elegido por la expansión de la línea base se encuentra en el centro de cada mapa. La función de similitud es la divergencia JensenShannon. 2.4 Visualización de distribuciones de retroalimentación Antes de describir cómo ajustamos y utilizamos la distribución de Dirichlet sobre modelos de retroalimentación, es instructivo ver algunos ejemplos de distribuciones de modelos de retroalimentación reales que resultan de muestrear por bootstrap los documentos más recuperados de diferentes temas de TREC. Cada punto en nuestro espacio muestral es un modelo de lenguaje, que típicamente tiene varias miles de dimensiones. Para ayudar a analizar el comportamiento de nuestro método, utilizamos un Mapa Auto-organizado (a través del paquete SOM-PAK [9]), para aplanar y visualizar la función de densidad de alta dimensión. Los mapas de densidad para tres temas de TREC se muestran en la Figura 2 arriba. Las áreas oscuras representan regiones de alta similitud entre los modelos de lenguaje. Las áreas claras representan regiones de baja similitud, los valles entre los grupos. Cada diagrama está centrado en el modelo de lenguaje que habría sido elegido por la expansión de referencia. Un solo pico (modo) es evidente en algunos ejemplos, pero en otros aparece una estructura más compleja. Además, aunque la distribución suele estar cerca del modelo de retroalimentación de referencia, para algunos temas están a una distancia significativa (según la divergencia de JensenShannon), como se muestra en la Subfigura 2c. En tales casos, la moda o la media de la distribución de retroalimentación a menudo funcionan significativamente mejor que el valor base (y en una proporción menor de casos, significativamente peor). 2.5 Ajuste de una distribución de retroalimentación posterior Después de obtener muestras del modelo de retroalimentación mediante el remuestreo de las entradas del modelo de retroalimentación, estimamos la distribución de retroalimentación. Suponemos que los modelos de retroalimentación multinomial {ˆθ1, . . . , ˆθB} fueron generados por una distribución de Dirichlet latente con parámetros {α1, . . . , αN}. Para estimar los {α1, . . . , αN }, ajustamos los parámetros de Dirichlet a las muestras del modelo de lenguaje B según la máxima verosimilitud utilizando un procedimiento de Newton generalizado, cuyos detalles se encuentran en Minka [13]. Suponemos una distribución de Dirichlet simple sobre los {α1, . . . , αN}, estableciendo cada uno como αi = μ · p(wi | C), donde μ es un parámetro y p(· | C) es el modelo de lenguaje de colección estimado a partir de un conjunto de documentos de la colección C. El ajuste de parámetros converge muy rápidamente, generalmente en solo 2 o 3 iteraciones. Dado que nuestros puntos son modelos de lenguaje en el simplex multinomial, extendimos SOM-PAK para admitir la divergencia de Jensen-Shannon, una medida de similitud ampliamente utilizada entre distribuciones de probabilidad. 3 iteraciones son suficientes, por lo que es práctico aplicarlo en tiempo de consulta cuando la sobrecarga computacional debe ser pequeña. En la práctica, podemos restringir el cálculo al vocabulario de los documentos recuperados en la parte superior, en lugar de toda la colección. Ten en cuenta que para este paso estamos reutilizando los documentos recuperados existentes y no realizando consultas adicionales. Dado los parámetros de una distribución de Dirichlet N-dimensional Dir(α), los vectores de media μ y moda x son fáciles de calcular y se dan respectivamente por μi = αiP αi (2) y xi = αi−1P αi−N . (3) Luego podemos elegir el modelo de lenguaje en la media o en la moda del posterior como el modelo de retroalimentación mejorado final. (Encontramos que la moda proporciona un rendimiento ligeramente mejor). Para la recuperación de información, es probable que el número de muestras disponibles sea bastante pequeño por razones de rendimiento, generalmente menos de diez. Además, si bien el muestreo aleatorio es útil en ciertos casos, es perfectamente aceptable permitir distribuciones de muestreo determinísticas, pero estas deben diseñarse cuidadosamente para aproximar una varianza de salida precisa. Dejamos esto para estudios futuros. 2.6 Variantes de consulta Utilizamos los siguientes métodos para generar variantes de la consulta original. Cada variante corresponde a una suposición diferente sobre qué aspectos de la consulta original pueden ser importantes. Esta es una forma de muestreo determinístico. Seleccionamos tres métodos simples que cubren suposiciones complementarias sobre la consulta. Uso sin expansión. La suposición es que los términos dados son una descripción completa de la necesidad de información. Deja uno fuera. Se deja fuera un término del query original. La suposición es que uno de los términos de la consulta es un término de ruido. Se elige un término único de la consulta original. Esto asume que solo un aspecto de la consulta, es decir, el representado por el término, es el más importante. Después de generar una variante de la consulta original, la combinamos con la consulta original utilizando un peso αSUB para no alejarnos demasiado. En este estudio, establecimos αSUB = 0.5. Por ejemplo, utilizando el lenguaje de consulta Indri [12], una variante de dejar uno fuera de la consulta inicial que omite el término \"ireland\" para el tema TREC 404 es: #weight(0.5 #combine(ireland peace talks) 0.5 #combine(peace talks)) 2.7 Combinando modelos de retroalimentación mejorados de múltiples variantes de consulta. Cuando se utilizan múltiples variantes de consulta, los modelos de retroalimentación mejorados resultantes se combinan utilizando la combinación de modelos bayesianos. Para hacer esto, tratamos cada palabra como un elemento a clasificar como perteneciente a una clase relevante o no relevante, y derivamos una probabilidad de clase para cada palabra combinando las puntuaciones de cada variante de consulta. Cada puntuación es dada por la probabilidad de ese término en la distribución de Dirichlet. Las puntuaciones de los términos se ponderan por el inverso de la varianza del término en los modelos de retroalimentación mejorada de la distribución de Dirichlet. La probabilidad previa de pertenencia de una palabra a la clase relevante se da por la probabilidad de la consulta original en todo el modelo de expansión mejorado. 3. EVALUACIÓN En esta sección presentamos resultados que confirman la utilidad de estimar una distribución del modelo de retroalimentación a partir del remuestreo ponderado de documentos mejor clasificados, y de combinar los modelos de retroalimentación obtenidos a partir de diferentes pequeños cambios en la consulta original. 3.1 Método general Evaluamos el rendimiento en un total de 350 consultas derivadas de cuatro conjuntos de temas de TREC: 51-200 (TREC-1&2), 351-400 (TREC-7), 401-450 (TREC-8) y 451-550 (wt10g, TREC-9&10). Elegimos estos por su contenido variado y propiedades del documento. Por ejemplo, los documentos wt10g son páginas web con una amplia variedad de temas y estilos, mientras que los documentos TREC-1&2 son artículos de noticias más homogéneos. La indexación y recuperación se realizó utilizando el sistema Indri en el kit de herramientas Lemur [12] [1]. Nuestras consultas se derivaron de las palabras en el campo del título de los temas de TREC. Las frases no fueron utilizadas. Para generar las consultas base que se pasaron a Indri, envolvimos los términos de la consulta con el operador #combine de Indri. Por ejemplo, la consulta inicial para el tema 404 es: #combine(ireland peace talks) Realizamos el stemming de Krovetz para todos los experimentos. Debido a que encontramos que el método de expansión de la base (Indri) funcionó mejor utilizando una lista de palabras vacías con el modelo de retroalimentación, todos los experimentos utilizaron una lista de 419 palabras comunes en inglés. Sin embargo, un efecto secundario interesante de nuestro enfoque de remuestreo es que tiende a eliminar muchas palabras vacías del modelo de retroalimentación, lo que hace que una lista de paradas sea menos crítica. Esto se discute más a fondo en la Sección 3.6. Método de retroalimentación de línea base 3.2 Para nuestro método de expansión de línea base, utilizamos un algoritmo incluido en Indri 1.0 como el método de expansión predeterminado. Este método primero selecciona términos utilizando un cálculo de logaritmo de probabilidades descrito por Ponte [14], pero asigna pesos finales a los términos utilizando el modelo de relevancia de Lavrenko [10]. Elegimos el método Indri porque proporciona una línea base consistentemente sólida, se basa en un enfoque de modelado del lenguaje y es fácil de experimentar con él. En una evaluación de TREC utilizando el corpus GOV2, el método fue uno de los mejores resultados, logrando un aumento del 19.8% en el MAP en comparación con el uso de consultas no expandidas. En este estudio, se logra una ganancia promedio en MAP del 17.25% en las cuatro colecciones. El método de expansión de Indri primero calcula una razón de logaritmos de probabilidades o(v) para cada término de expansión potencial v dado por o(v) = X D log p(v|D) p(v|C) (4) sobre todos los documentos D que contienen v, en la colección C. Luego, los candidatos a términos de expansión se ordenan por o(v) descendente, y se eligen los primeros m. Finalmente, los pesos de término r(v) utilizados en la consulta ampliada se calculan en base al modelo de relevancia r(v) = X D p(q|D)p(v|D) p(v) p(D) (5). La cantidad p(q|D) es la puntuación de probabilidad asignada al documento en la recuperación inicial. Utilizamos suavizado de Dirichlet de p(v|D) con μ = 1000. Este modelo de relevancia se combina luego con la consulta original utilizando interpolación lineal, ponderada por un parámetro α. Por defecto, utilizamos los 50 documentos principales para la retroalimentación y los 20 términos de expansión, con el parámetro de interpolación de retroalimentación α = 0.5 a menos que se indique lo contrario. Por ejemplo, la consulta expandida base para el tema 404 es: #weight(0.5 #combine(ireland peace talks) 0.5 #weight(0.10 ireland 0.08 peace 0.08 northern ...) 3.3 Rendimiento de la expansión. Medimos la efectividad de nuestros algoritmos de retroalimentación mediante dos criterios principales: precisión y robustez. La robustez, y el equilibrio entre precisión y robustez, se analiza en la Sección 3.4. En esta sección, examinamos la precisión promedio y la precisión en los 10 documentos principales (P10). También incluimos la recuperación de 1,000 documentos. Para cada consulta, obtuvimos un conjunto de B modelos de retroalimentación utilizando la línea base de Indri. Cada modelo de retroalimentación se obtuvo a partir de una muestra aleatoria de los k documentos principales tomados con reemplazo. Para estos experimentos, B = 30 y k = 50. Cada modelo de retroalimentación contenía 20 términos. En el lado de la consulta, utilizamos muestreo de dejar uno fuera (LOO) para crear las variantes de consulta. El muestreo de consulta de un solo término tuvo un rendimiento consistentemente peor en todas las colecciones, por lo que nuestros resultados se centran en el muestreo de LOO aquí. Utilizamos los métodos descritos en la Sección 2 para estimar un modelo de retroalimentación mejorado a partir de la distribución posterior de Dirichlet para cada variante de consulta, y para combinar los modelos de retroalimentación de todas las variantes de consulta. Llamamos a nuestro método expansión de remuestreo y lo denotamos como RS-FB aquí. Denominamos al método de retroalimentación de línea base Indri como Base-FB. Los resultados de aplicar tanto el método de expansión de línea base (Base-FB) como la expansión de remuestreo (RS-FB) se muestran en la Tabla 1. Observamos varias tendencias en esta tabla. Primero, la precisión promedio de RS-FB fue comparable a Base-FB, logrando una ganancia promedio del 17.6% en comparación con no utilizar expansión en las cuatro colecciones. La ganancia de expansión de la línea base de Indri fue del 17.25%. Además, el método RS-FB logró mejoras consistentes en P10 sobre Base-FB para cada conjunto de temas, con una mejora promedio del 6.89% sobre Base-FB para los 350 temas. La ganancia P10 más baja sobre Base-FB fue del +3.82% para TREC-7 y la más alta fue del +11.95% para wt10g. Finalmente, tanto Base-FB como RS-FB también mejoraron consistentemente la recuperación en comparación con no utilizar expansión, siendo Base-FB el que logró una mejor recuperación que RS-FB para todos los conjuntos de temas. 3.4 Robustez de recuperación. Utilizamos el término robustez para referirnos al rendimiento de precisión promedio en el peor de los casos de un algoritmo de retroalimentación. Idealmente, un método de retroalimentación sólido nunca debería funcionar peor que usar la consulta original, mientras que a menudo funciona mejor utilizando la expansión. Para evaluar la robustez en este estudio, utilizamos una medida muy simple llamada índice de robustez (RI). Para un conjunto de consultas Q, la medida de RI se define como: RI(Q) = n+ − n− |Q| (6) donde n+ es el número de consultas ayudadas por el método de retroalimentación y n− es el número de consultas perjudicadas. Aquí, por \"ayudar\" nos referimos a obtener una precisión promedio más alta como resultado de la retroalimentación. El valor de RI varía desde un mínimo de 3. A veces también se le llama índice de confiabilidad de mejora y fue utilizado en Sakai et al. [17]. Colección NoExp Base-FB RS-FB TREC 1&2 AvgP 0.1818 0.2419 (+33.04%) 0.2406 (+32.24%) P10 0.4443 0.4913 (+10.57%) 0.5363 (+17.83%) Recall 15084/37393 19172/37393 15396/37393 TREC 7 AvgP 0.1890 0.2175 (+15.07%) 0.2169 (+14.75%) P10 0.4200 0.4320 (+2.85%) 0.4480 (+6.67%) Recall 2179/4674 2608/4674 2487/4674 TREC 8 AvgP 0.2031 0.2361 (+16.25%) 0.2268 (+11.70%) P10 0.3960 0.4160 (+5.05%) 0.4340 (+9.59%) Recall 2144/4728 2642/4728 2485/4728 wt10g AvgP 0.1741 0.1829 (+5.06%) 0.1946 (+11.78%) P10 0.2760 0.2630 (-4.71%) 0.2960 (+7.24%) Recall 3361/5980 3725/5980 3664/5980 Tabla 1: Comparación de la retroalimentación base (Base-FB) y la retroalimentación utilizando re-muestreo (RS-FB). La mejora mostrada para BaseFB y RS-FB es relativa al uso de ninguna expansión. (a) TREC 1&2 (curva superior); TREC 8 (curva inferior) (b) TREC 7 (curva superior); wt10g (curva inferior) Figura 3: La compensación entre robustez y precisión promedio para diferentes corpora. El eje x muestra el cambio en la MAP al utilizar la expansión de la línea base con α = 0.5. El eje y muestra el Índice de Robustez (RI). Cada curva a través de los puntos no circulados muestra el compromiso entre RI/MAP utilizando la estrategia simple de pequeño α (ver texto) a medida que α disminuye de 0.5 a cero en la dirección de la flecha. Los puntos circulados representan los compromisos obtenidos al volver a muestrear la retroalimentación para α = 0.5. Colección N Base-FB RS-FB n− RI n− RI TREC 1&2 103 26 +0.495 15 +0.709 TREC 7 46 14 +0.391 10 +0.565 TREC 8 44 12 +0.455 12 +0.455 wt10g 91 48 -0.055 39 +0.143 Combinado 284 100 +0.296 76 +0.465 Tabla 2: Comparación del índice de robustez (RI) para retroalimentación de línea base (Base-FB) vs. retroalimentación de remuestreo (RS-FB). También se muestran el número real de consultas perjudicadas por la retroalimentación (n−) para cada método y colección. Las consultas para las cuales la precisión promedio inicial era insignificante (≤ 0.01) fueron ignoradas, dando como resultado el recuento de consultas restantes en la columna N, de -1.0, cuando todas las consultas son perjudicadas por el método de retroalimentación, a +1.0 cuando todas las consultas son ayudadas. La medida de RI no tiene en cuenta la magnitud o distribución de la cantidad de cambio en el conjunto Q. Sin embargo, es fácil de entender como una indicación general de robustez. Una forma obvia de mejorar el rendimiento en el peor de los casos de la retroalimentación es simplemente utilizar un parámetro de interpolación α fijo más pequeño, como α = 0.3, otorgando menos peso al modelo de retroalimentación (posiblemente arriesgado) y más al cuestionario original. Llamamos a esto la estrategia de pequeña α. Dado que también estamos reduciendo las ganancias potenciales cuando el modelo de retroalimentación es correcto, sin embargo, esperaríamos algún tipo de compensación entre la precisión promedio y la robustez. Por lo tanto, comparamos el equilibrio entre precisión y robustez entre nuestro algoritmo de retroalimentación de remuestreo y el método simple de pequeño α. Los resultados se resumen en la Figura 3. En la figura, la curva para cada conjunto de temas interpola entre los puntos de compensación, comenzando en x=0, donde α = 0.5, y continuando en la dirección de la flecha a medida que α disminuye y la consulta original recibe cada vez más peso. Como era de esperar, la robustez aumenta continuamente a medida que avanzamos a lo largo de la curva, pero la precisión media promedio generalmente disminuye a medida que se eliminan las ganancias del feedback. Para la comparación, se muestra el rendimiento del re-muestreo con retroalimentación en α = 0.5 para cada colección como el punto marcado con un círculo. Más alto y a la derecha es mejor. Esta figura muestra que el re-muestreo de retroalimentación ofrece un compromiso algo mejor que el enfoque de pequeño α para 3 de las 4 colecciones. Figura 4: Histograma que muestra la mayor robustez del re-muestreo de retroalimentación (RS-FB) sobre la retroalimentación base (Base-FB) para todos los conjuntos de datos combinados. Las consultas se agrupan por el % de cambio en AP en comparación con la consulta no expandida. Colección DS + QV DS + No QV TREC 1&2 AvgP 0.2406 0.2547 (+5.86%) P10 0.5263 0.5362 (+1.88%) RI 0.7087 0.6515 (-0.0572) TREC 7 AvgP 0.2169 0.2200 (+1.43%) P10 0.4480 0.4300 (-4.02%) RI 0.5652 0.2609 (-0.3043) TREC 8 AvgP 0.2268 0.2257 (-0.49%) P10 0.4340 0.4200 (-3.23%) RI 0.4545 0.4091 (-0.0454) wt10g AvgP 0.1946 0.1865 (-4.16%) P10 0.2960 0.2680 (-9.46%) RI 0.1429 0.0220 (-0.1209) Tabla 3: Comparación de la retroalimentación de remuestreo utilizando muestreo de documentos (DS) con (QV) y sin (No QV) combinar modelos de retroalimentación de múltiples variantes de consulta. La Tabla 2 muestra los puntajes del Índice de Robustez para Base-FB y RS-FB. El método de retroalimentación RS-FB obtuvo una mayor robustez que Base-FB en tres de los cuatro conjuntos de temas, con solo un rendimiento ligeramente peor en TREC-8. Una vista más detallada que muestra la distribución de los cambios relativos en AP se presenta en el histograma de la Figura 4. En comparación con Base-FB, el método RS-FB logra una reducción notable en el número de consultas significativamente afectadas por la expansión (es decir, donde AP se ve afectado en un 25% o más), al tiempo que conserva ganancias positivas en AP. 3.5 Efecto de los métodos de muestreo de consultas y documentos Dada la mayor robustez de nuestros algoritmos observada en la Sección 3.4, una pregunta importante es qué componente de nuestro sistema es responsable. ¿Es el uso de la re-muestreo de documentos, el uso de múltiples variantes de consulta, o algún otro factor? Los resultados en la Tabla 3 sugieren que la combinación de modelos basada en variantes de consulta puede ser en gran medida responsable de la mayor robustez. Cuando se desactivan las variantes de consulta y se utiliza la consulta original por sí sola con muestreo de documentos, hay poco cambio neto en la precisión promedio, una pequeña disminución en P10 para 3 de los 4 conjuntos de temas, pero una caída significativa en la robustez para todos los conjuntos de temas. En dos casos, la medida de RI disminuye en más del 50%. También examinamos el efecto del método de muestreo de documentos en la efectividad de la recuperación, utilizando dos estrategias diferentes. La estrategia de ponderación uniforme ignoró las puntuaciones de relevancia de la recuperación inicial y otorgó a cada documento en los primeros k la misma probabilidad de selección. Por el contrario, la estrategia de ponderación de puntajes de relevancia seleccionó documentos con una probabilidad proporcional a sus puntajes de relevancia. De esta manera, los documentos que tenían una clasificación más alta tenían más probabilidades de ser seleccionados. Los resultados se muestran en la Tabla 4. La estrategia de ponderación de la puntuación de relevancia tiene un mejor rendimiento en general, con puntajes de RI y P10 significativamente más altos en 3 de los 4 conjuntos de temas. La diferencia en la precisión promedio entre los métodos, sin embargo, es menos marcada. Esto sugiere que el peso uniforme actúa para aumentar la varianza en los resultados de recuperación: cuando la precisión promedio inicial es alta, hay muchos documentos relevantes en los primeros k y el muestreo uniforme puede proporcionar un modelo de relevancia más representativo que centrarse en los elementos altamente clasificados. Por otro lado, cuando la precisión inicial es baja, hay pocos documentos relevantes en las posiciones inferiores y el muestreo uniforme mezcla más de los documentos no relevantes. Por razones de espacio, solo resumimos nuestros hallazgos sobre el tamaño de la muestra aquí. El número de muestras tiene cierto efecto en la precisión cuando es menor a 10, pero el rendimiento se estabiliza alrededor de 15 a 20 muestras. Utilizamos 30 muestras para nuestros experimentos. Mucho más allá de este nivel, los beneficios adicionales de más muestras disminuyen a medida que la distribución inicial de puntajes se ajusta más estrechamente y aumenta el tiempo de procesamiento. 3.6 El efecto del remuestreo en la calidad del término de expansión Idealmente, un modelo de recuperación no debería requerir una lista de palabras vacías al estimar un modelo de relevancia: un modelo estadístico robusto debería reducir automáticamente el peso de las palabras vacías dependiendo del contexto. Las palabras vacías pueden perjudicar la retroalimentación si se seleccionan como términos de retroalimentación, ya que suelen ser malos discriminadores y desperdician espacios de términos valiosos. En la práctica, sin embargo, debido a que la mayoría de los métodos de selección de términos se asemejan a un tipo de ponderación tf · idf, a veces se pueden seleccionar términos con un idf bajo pero un tf muy alto como candidatos de términos de expansión. Esto sucede, por ejemplo, incluso con el enfoque del Modelo de Relevancia que forma parte de nuestra retroalimentación base. Para garantizar una línea base lo más sólida posible, utilizamos una lista de paradas para todos los experimentos reportados aquí. Si desactivamos la lista de palabras vacías, sin embargo, obtenemos resultados como los mostrados en la Tabla 5 donde cuatro de los diez términos de retroalimentación básica principales para el tema 60 de TREC (dijo, pero, su, no) son palabras vacías utilizando el método BaseFB. (Se seleccionaron los 100 términos de expansión principales para generar este ejemplo). El método Indris intenta abordar el problema de las palabras vacías aplicando un paso inicial basado en Ponte [14] para seleccionar términos menos comunes que tienen altas probabilidades logarítmicas de estar en los documentos mejor clasificados en comparación con toda la colección. Sin embargo, esto no resuelve completamente el problema de las palabras vacías, especialmente a medida que aumenta el número de términos de retroalimentación. Sin embargo, el uso de retroalimentación de remuestreo parece mitigar la ponderación de la colección QV + QV uniforme + ponderación de puntaje de relevancia TREC 1&2 AvgP 0.2545 0.2406 (-5.46%) P10 0.5369 0.5263 (-1.97%) RI 0.6212 0.7087 (+14.09%) TREC 7 AvgP 0.2174 0.2169 (-0.23%) P10 0.4320 0.4480 (+3.70%) RI 0.4783 0.5652 (+18.17%) TREC 8 AvgP 0.2267 0.2268 (+0.04%) P10 0.4120 0.4340 (+5.34%) RI 0.4545 0.4545 (+0.00%) wt10g AvgP 0.1808 0.1946 (+7.63%) P10 0.2680 0.2960 (+10.45%) RI 0.0220 0.1099 (+399.5%) Tabla 4: Comparación de muestreo de documentos uniforme y ponderado por relevancia. El cambio porcentual en comparación con el muestreo uniforme se muestra entre paréntesis. QV indica que se utilizaron variantes de consulta en ambas ejecuciones. FB de línea base p(wi|R) FB de remuestreo p(wi|R) dijo 0.055 corte 0.026 corte 0.055 pagar 0.018 pagar 0.034 federal 0.012 pero 0.026 educación 0.011 empleados 0.024 maestros 0.010 sus 0.024 empleados 0.010 no 0.023 caso 0.010 federal 0.021 sus 0.009 trabajadores 0.020 apelaciones 0.008 educación 0.020 sindicato 0.007 Tabla 5: Calidad del término de retroalimentación cuando no se utiliza una lista de paradas. Términos de retroalimentación para el tema 60 de TREC: pago por mérito vs antigüedad. el efecto de las palabras vacías automáticamente. En el ejemplo de la Tabla 5, la retroalimentación de remuestreo deja solo una palabra vacía (su) en las diez primeras posiciones. Observamos un comportamiento similar del término de retroalimentación en muchos otros temas. La razón de este efecto parece ser la interacción de la puntuación de selección de términos con el límite superior de términos m. Si bien la presencia e incluso la proporción de palabras vacías particulares es bastante estable en diferentes muestras de documentos, su posición relativa en la lista de los primeros m elementos no lo es, ya que se examinan conjuntos de documentos con diferentes números de candidatos de términos mejores y de menor frecuencia para cada muestra. Como resultado, si bien algunos stopwords pueden aparecer en cada conjunto de documentos muestreados, tiende a ocurrir que cualquier stopword dado caiga por debajo del umbral para múltiples muestras, lo que lleva a su clasificación como una característica de alta varianza y bajo peso. 4. TRABAJO RELACIONADO Nuestro enfoque está relacionado con trabajos previos de varias áreas de recuperación de información y aprendizaje automático. Nuestro uso de la variación de consultas fue inspirado por el trabajo de YomTov et al. [20], Carpineto et al. [5] y Amati et al. [2], entre otros. Estos estudios utilizan la idea de crear múltiples subconsultas y luego examinar la naturaleza de la superposición en los documentos y/o términos de expansión que resultan de cada subconsulta. La combinación de modelos se realiza utilizando heurísticas. En particular, los estudios de Amati et al. y Carpineto et al. investigaron la combinación de términos de métodos de distribución individuales utilizando una heurística de combinación de reordenamiento de términos. En un conjunto de temas de TREC encontraron una amplia variación promedio en la distancia de rango de los términos de diferentes métodos de expansión. Su método de combinación proporcionó modestas mejoras positivas en la precisión promedio. La idea de examinar la superposición entre listas de términos sugeridos también ha sido utilizada en enfoques tempranos de expansión de consultas. El método de Análisis de Contexto Local (LCA) de Xu y Crofts [19] incluye un factor en la fórmula de ponderación derivada empíricamente que hace que se prefieran los términos de expansión que tienen conexiones con múltiples términos de consulta. En el lado del documento, el trabajo reciente de Zhou & Croft [21] exploró la idea de agregar ruido a los documentos, volver a puntuarlos y utilizar la estabilidad de las clasificaciones resultantes como una estimación de la dificultad de la consulta. Esto está relacionado con nuestro uso de muestreo de documentos para estimar el <br>riesgo</br> del modelo de retroalimentación construido a partir de los diferentes conjuntos de documentos recuperados en la parte superior. Sakai et al. [17] propusieron un enfoque para mejorar la robustez de la retroalimentación de relevancia pseudo utilizando un método que llaman muestreo selectivo. La esencia de su método es que permiten omitir algunos documentos de alta clasificación, basándose en un criterio de agrupación, para seleccionar un conjunto de documentos más variado e innovador más adelante en la clasificación para su uso por un método tradicional de retroalimentación pseudo. Su estudio no encontró mejoras significativas ni en robustez (RI) ni en MAP en sus corpora. Greiff, Morgan y Ponte [8] exploraron el papel de la varianza en la ponderación de términos. En una serie de simulaciones que simplificaron el problema a documentos de 2 características, encontraron que la precisión promedio disminuye a medida que la varianza de la frecuencia de términos - ruido alto - aumenta. La reducción del peso de los términos con alta varianza resultó en una mejora de la precisión promedio. Esto parece estar en concordancia con nuestros propios hallazgos para los modelos de retroalimentación individual. Las estimaciones de la varianza de la producción han sido utilizadas recientemente para mejorar la clasificación de textos. Lee et al. [11] utilizaron estimaciones de varianza específicas de la consulta de las salidas del clasificador para realizar una combinación de modelos mejorada. En lugar de utilizar muestreo, pudieron derivar expresiones en forma cerrada para la varianza del clasificador asumiendo clasificadores base utilizando tipos simples de redes de inferencia. Ando y Zhang propusieron un método que ellos llaman retroalimentación estructural [3] y mostraron cómo aplicarlo a la expansión de consultas para la pista de Genómica de TREC. Utilizaron variaciones de consultas r para obtener R conjuntos diferentes Sr de documentos mejor clasificados que se han intersectado con los documentos mejor clasificados obtenidos de la consulta original qorig. Para cada Si, se calcula el vector centróide normalizado ˆwi de los documentos. El análisis de componentes principales (PCA) se aplica luego a los ˆwi para obtener la matriz Φ de los vectores singulares izquierdos φh que se utilizan para obtener la nueva consulta expandida qexp = qorig + ΦT Φqorig. (7) En el caso de H = 1, tenemos un único vector singular izquierdo φ: qexp = qorig + (φT qorig)φ, de modo que el producto punto φT qorig es un tipo de peso dinámico en la consulta expandida que se basa en la similitud de la consulta original con la consulta expandida. El uso de la varianza como medida de calidad del modelo de retroalimentación ocurre de forma indirecta a través de la aplicación de PCA. Sería interesante estudiar las conexiones entre este enfoque y nuestro propio método de ajuste de modelos. Finalmente, en los enfoques de modelado del lenguaje para retroalimentación, Tao y Zhai [18] describen un método para una retroalimentación más robusta que permite que cada documento tenga un α de retroalimentación diferente. Los pesos de retroalimentación se derivan automáticamente utilizando EM regularizado. La condición de parada de EM implica un equilibrio aproximadamente igual entre el modelo de consulta y expansión. Proponen adaptar el parámetro de parada η basado en una función de alguna medida de calidad de los documentos de retroalimentación. CONCLUSIONES Hemos presentado un nuevo enfoque para la retroalimentación de relevancia pseudo basado en el muestreo de documentos y consultas. El uso del muestreo es un dispositivo muy flexible y poderoso, motivado por nuestro deseo general de ampliar los modelos actuales de recuperación mediante la estimación del <br>riesgo</br> o la varianza asociada con los parámetros o la salida de los procesos de recuperación. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "feedback model": {
            "translated_key": "modelo de retroalimentación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Estimation and Use of Uncertainty in Pseudo-relevance Feedback Kevyn Collins-Thompson and Jamie Callan Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213-8213 U.S.A. {kct | callan}@cs.cmu.edu ABSTRACT Existing pseudo-relevance feedback methods typically perform averaging over the top-retrieved documents, but ignore an important statistical dimension: the risk or variance associated with either the individual document models, or their combination.",
                "Treating the baseline feedback method as a black box, and the output <br>feedback model</br> as a random variable, we estimate a posterior distribution for the <br>feedback model</br> by resampling a given querys top-retrieved documents, using the posterior mean or mode as the enhanced feedback model.",
                "We then perform model combination over several enhanced models, each based on a slightly modified query sampled from the original query.",
                "We find that resampling documents helps increase individual <br>feedback model</br> precision by removing noise terms, while sampling from the query improves robustness (worst-case performance) by emphasizing terms related to multiple query aspects.",
                "The result is a meta-feedback algorithm that is both more robust and more precise than the original strong baseline method.",
                "Categories and Subject Descriptors: H.3.3 [Information Retrieval]: Retrieval Models General Terms: Algorithms, Experimentation 1.",
                "INTRODUCTION Uncertainty is an inherent feature of information retrieval.",
                "Not only do we not know the queries that will be presented to our retrieval algorithm ahead of time, but the users information need may be vague or incompletely specified by these queries.",
                "Even if the query were perfectly specified, language in the collection documents is inherently complex and ambiguous and matching such language effectively is a formidable problem by itself.",
                "With this in mind, we wish to treat many important quantities calculated by the retrieval system, whether a relevance score for a document, or a weight for a query expansion term, as random variables whose true value is uncertain but where the uncertainty about the true value may be quantified by replacing the fixed value with a probability distribution over possible values.",
                "In this way, retrieval algorithms may attempt to quantify the risk or uncertainty associated with their output rankings, or improve the stability or precision of their internal calculations.",
                "Current algorithms for pseudo-relevance feedback (PRF) tend to follow the same basic method whether we use vector space-based algorithms such as Rocchios formula [16], or more recent language modeling approaches such as Relevance Models [10].",
                "First, a set of top-retrieved documents is obtained from an initial query and assumed to approximate a set of relevant documents.",
                "Next, a single <br>feedback model</br> vector is computed according to some sort of average, centroid, or expectation over the set of possibly-relevant document models.",
                "For example, the document vectors may be combined with equal weighting, as in Rocchio, or by query likelihood, as may be done using the Relevance Model1 .",
                "The use of an expectation is reasonable for practical and theoretical reasons, but by itself ignores potentially valuable information about the risk of the <br>feedback model</br>.",
                "Our main hypothesis in this paper is that estimating the uncertainty in feedback is useful and leads to better individual feedback models and more robust combined models.",
                "Therefore, we propose a method for estimating uncertainty associated with an individual <br>feedback model</br> in terms of a posterior distribution over language models.",
                "To do this, we systematically vary the inputs to the baseline feedback method and fit a Dirichlet distribution to the output.",
                "We use the posterior mean or mode as the improved <br>feedback model</br> estimate.",
                "This process is shown in Figure 1.",
                "As we show later, the mean and mode may vary significantly from the single <br>feedback model</br> proposed by the baseline method.",
                "We also perform model combination using several improved feedback language models obtained by a small number of new queries sampled from the original query.",
                "A models weight combines two complementary factors: the models probability of generating the query, and the variance of the model, with high-variance models getting lower weight. 1 For example, an expected parameter vector conditioned on the query observation is formed from top-retrieved documents, which are treated as training strings (see [10], p. 62).",
                "Figure 1: Estimating the uncertainty of the <br>feedback model</br> for a single query. 2.",
                "SAMPLING-BASED FEEDBACK In Sections 2.1-2.5 we describe a general method for estimating a probability distribution over the set of possible language models.",
                "In Sections 2.6 and 2.7 we summarize how different query samples are used to generate multiple feedback models, which are then combined. 2.1 Modeling Feedback Uncertainty Given a query Q and a collection C, we assume a probabilistic retrieval system that assigns a real-valued document score f(D, Q) to each document D in C, such that the score is proportional to the estimated probability of relevance.",
                "We make no other assumptions about f(D, Q).",
                "The nature of f(D, Q) may be complex: for example, if the retrieval system supports structured query languages [12], then f(D, Q) may represent the output of an arbitrarily complex inference network defined by the structured query operators.",
                "In theory, the scoring function can vary from query to query, although in this study for simplicity we keep the scoring function the same for all queries.",
                "Our specific query method is given in Section 3.",
                "We treat the feedback algorithm as a black box and assume that the inputs to the feedback algorithm are the original query and the corresponding top-retrieved documents, with a score being given to each document.",
                "We assume that the output of the feedback algorithm is a vector of term weights to be used to add or reweight the terms in the representation of the original query, with the vector normalized to form a probability distribution.",
                "We view the the inputs to the feedback black box as random variables, and analyze the <br>feedback model</br> as a random variable that changes in response to changes in the inputs.",
                "Like the document scoring function f(D, Q), the feedback algorithm may implement a complex, non-linear scoring formula, and so as its inputs vary, the resulting feedback models may have a complex distribution over the space of feedback models (the sample space).",
                "Because of this potential complexity, we do not attempt to derive a posterior distribution in closed form, but instead use simulation.",
                "We call this distribution over possible feedback models the <br>feedback model</br> distribution.",
                "Our goal in this section is to estimate a useful approximation to the <br>feedback model</br> distribution.",
                "For a specific framework for experiments, we use the language modeling (LM) approach for information retrieval [15].",
                "The score of a document D with respect to a query Q and collection C is given by p(Q|D) with respect to language models ˆθQ and ˆθD estimated for the query and document respectively.",
                "We denote the set of k top-retrieved documents from collection C in response to Q by DQ(k, C).",
                "For simplicity, we assume that queries and documents are generated by multinomial distributions whose parameters are represented by unigram language models.",
                "To incorporate feedback in the LM approach, we assume a model-based scheme in which our goal is take the query and resulting ranked documents DQ(k, C) as input, and output an expansion language model ˆθE, which is then interpolated with the original query model ˆθQ: ˆθNew = (1 − α) · ˆθQ + α · ˆθE (1) This includes the possibility of α = 1 where the original query mode is completely replaced by the <br>feedback model</br>.",
                "Our sample space is the set of all possible language models LF that may be output as feedback models.",
                "Our approach is to take samples from this space and then fit a distribution to the samples using maximum likelihood.",
                "For simplicity, we start by assuming the latent feedback distribution has the form of a Dirichlet distribution.",
                "Although the Dirichlet is a unimodal distribution, and in general quite limited in its expressiveness in the sample space, it is a natural match for the multinomial language model, can be estimated quickly, and can capture the most salient features of confident and uncertain feedback models, such as the overall spread of the distibution. 2.2 Resampling document models We would like an approximation to the posterior distribution of the <br>feedback model</br> LF .",
                "To accomplish this, we apply a widely-used simulation technique called bootstrap sampling ([7], p. 474) on the input parameters, namely, the set of top-retrieved documents.",
                "Bootstrap sampling allows us to simulate the approximate effect of perturbing the parameters within the black box feedback algorithm by perturbing the inputs to that algorithm in a systematic way, while making no assumptions about the nature of the feedback algorithm.",
                "Specifically, we sample k documents with replacement from DQ(k, C), and calculate an expansion language model θb using the black box feedback method.",
                "We repeat this process B times to obtain a set of B feedback language models, to which we then fit a Dirichlet distribution.",
                "Typically B is in the range of 20 to 50 samples, with performance being relatively stable in this range.",
                "Note that instead of treating each top document as equally likely, we sample according to the estimated probabilities of relevance of each document in DQ(k, C).",
                "Thus, a document is more likely to be chosen the higher it is in the ranking. 2.3 Justification for a sampling approach The rationale for our sampling approach has two parts.",
                "First, we want to improve the quality of individual feedback models by smoothing out variation when the baseline <br>feedback model</br> is unstable.",
                "In this respect, our approach resembles bagging [4], an ensemble approach which generates multiple versions of a predictor by making bootstrap copies of the training set, and then averages the (numerical) predictors.",
                "In our application, top-retrieved documents can be seen as a kind of noisy training set for relevance.",
                "Second, sampling is an effective way to estimate basic properties of the feedback posterior distribution, which can then be used for improved model combination.",
                "For example, a model may be weighted by its prediction confidence, estimated as a function of the variability of the posterior around the model. foo2-401.map-Dim:5434,Size:12*12units,gaussianneighborhood (a) Topic 401 Foreign minorities, Germany foo2-402.map-Dim:5698,Size:12*12units,gaussianneighborhood (b) Topic 402 Behavioral genetics foo2-459.map-Dim:8969,Size:12*12units,gaussianneighborhood (c) Topic 459 When can a lender foreclose on property Figure 2: Visualization of expansion language model variance using self-organizing maps, showing the distribution of language models that results from resampling the inputs to the baseline expansion method.",
                "The language model that would have been chosen by the baseline expansion is at the center of each map.",
                "The similarity function is JensenShannon divergence. 2.4 Visualizing feedback distributions Before describing how we fit and use the Dirichlet distribution over feedback models, it is instructive to view some examples of actual <br>feedback model</br> distributions that result from bootstrap sampling the top-retrieved documents from different TREC topics.",
                "Each point in our sample space is a language model, which typically has several thousand dimensions.",
                "To help analyze the behavior of our method we used a Self-Organizing Map (via the SOM-PAK package [9]), to flatten and visualize the high-dimensional density function2 .",
                "The density maps for three TREC topics are shown in Figure 2 above.",
                "The dark areas represent regions of high similarity between language models.",
                "The light areas represent regions of low similarity - the valleys between clusters.",
                "Each diagram is centered on the language model that would have been chosen by the baseline expansion.",
                "A single peak (mode) is evident in some examples, but more complex structure appears in others.",
                "Also, while the distribution is usually close to the baseline <br>feedback model</br>, for some topics they are a significant distance apart (as measured by JensenShannon divergence), as in Subfigure 2c.",
                "In such cases, the mode or mean of the feedback distribution often performs significantly better than the baseline (and in a smaller proportion of cases, significantly worse). 2.5 Fitting a posterior feedback distribution After obtaining <br>feedback model</br> samples by resampling the <br>feedback model</br> inputs, we estimate the feedback distribution.",
                "We assume that the multinomial feedback models {ˆθ1, . . . , ˆθB} were generated by a latent Dirichlet distribution with parameters {α1, . . . , αN }.",
                "To estimate the {α1, . . . , αN }, we fit the Dirichlet parameters to the B language model samples according to maximum likelihood using a generalized Newton procedure, details of which are given in Minka [13].",
                "We assume a simple Dirichlet prior over the {α1, . . . , αN }, setting each to αi = μ · p(wi | C), where μ is a parameter and p(· | C) is the collection language model estimated from a set of documents from collection C. The parameter fitting converges very quickly - typically just 2 or 2 Because our points are language models in the multinomial simplex, we extended SOM-PAK to support JensenShannon divergence, a widely-used similarity measure between probability distributions. 3 iterations are enough - so that it is practical to apply at query-time when computational overhead must be small.",
                "In practice, we can restrict the calculation to the vocabulary of the top-retrieved documents, instead of the entire collection.",
                "Note that for this step we are re-using the existing retrieved documents and not performing additional queries.",
                "Given the parameters of an N-dimensional Dirichlet distribution Dir(α) the mean μ and mode x vectors are easy to calculate and are given respectively by μi = αiP αi (2) and xi = αi−1P αi−N . (3) We can then choose the language model at the mean or the mode of the posterior as the final enhanced <br>feedback model</br>. (We found the mode to give slightly better performance.)",
                "For information retrieval, the number of samples we will have available is likely to be quite small for performance reasons - usually less than ten.",
                "Moreover, while random sampling is useful in certain cases, it is perfectly acceptable to allow deterministic sampling distributions, but these must be designed carefully in order to approximate an accurate output variance.",
                "We leave this for future study. 2.6 Query variants We use the following methods for generating variants of the original query.",
                "Each variant corresponds to a different assumption about which aspects of the original query may be important.",
                "This is a form of deterministic sampling.",
                "We selected three simple methods that cover complimentary assumptions about the query.",
                "No-expansion Use only the original query.",
                "The assumption is that the given terms are a complete description of the information need.",
                "Leave-one-out A single term is left out of the original query.",
                "The assumption is that one of the query terms is a noise term.",
                "Single-term A single term is chosen from the original query.",
                "This assumes that only one aspect of the query, namely, that represented by the term, is most important.",
                "After generating a variant of the original query, we combine it with the original query using a weight αSUB so that we do not stray too far.",
                "In this study, we set αSUB = 0.5.",
                "For example, using the Indri [12] query language, a leave-oneout variant of the initial query that omits the term ireland for TREC topic 404 is: #weight(0.5 #combine(ireland peace talks) 0.5 #combine(peace talks)) 2.7 Combining enhanced feedback models from multiple query variants When using multiple query variants, the resulting enhanced feedback models are combined using Bayesian model combination.",
                "To do this, we treat each word as an item to be classified as belonging to a relevant or non-relevant class, and derive a class probability for each word by combining the scores from each query variant.",
                "Each score is given by that terms probability in the Dirichlet distribution.",
                "The term scores are weighted by the inverse of the variance of the term in the enhanced feedback models Dirichlet distribution.",
                "The prior probability of a words membership in the relevant class is given by the probability of the original query in the entire enhanced expansion model. 3.",
                "EVALUATION In this section we present results confirming the usefulness of estimating a <br>feedback model</br> distribution from weighted resampling of top-ranked documents, and of combining the feedback models obtained from different small changes in the original query. 3.1 General method We evaluated performance on a total of 350 queries derived from four sets of TREC topics: 51-200 (TREC-1&2), 351-400 (TREC-7), 401-450 (TREC-8), and 451-550 (wt10g, TREC-9&10).",
                "We chose these for their varied content and document properties.",
                "For example, wt10g documents are Web pages with a wide variety of subjects and styles while TREC-1&2 documents are more homogeneous news articles.",
                "Indexing and retrieval was performed using the Indri system in the Lemur toolkit [12] [1].",
                "Our queries were derived from the words in the title field of the TREC topics.",
                "Phrases were not used.",
                "To generate the baseline queries passed to Indri, we wrapped the query terms with Indris #combine operator.",
                "For example, the initial query for topic 404 is: #combine(ireland peace talks) We performed Krovetz stemming for all experiments.",
                "Because we found that the baseline (Indri) expansion method performed better using a stopword list with the <br>feedback model</br>, all experiments used a stoplist of 419 common English words.",
                "However, an interesting side-effect of our resampling approach is that it tends to remove many stopwords from the <br>feedback model</br>, making a stoplist less critical.",
                "This is discussed further in Section 3.6. 3.2 Baseline feedback method For our baseline expansion method, we use an algorithm included in Indri 1.0 as the default expansion method.",
                "This method first selects terms using a log-odds calculation described by Ponte [14], but assigns final term weights using Lavrenkos relevance model[10].",
                "We chose the Indri method because it gives a consistently strong baseline, is based on a language modeling approach, and is simple to experiment with.",
                "In a TREC evaluation using the GOV2 corpus [6], the method was one of the topperforming runs, achieving a 19.8% gain in MAP compared to using unexpanded queries.",
                "In this study, it achieves an average gain in MAP of 17.25% over the four collections.",
                "Indris expansion method first calculates a log-odds ratio o(v) for each potential expansion term v given by o(v) = X D log p(v|D) p(v|C) (4) over all documents D containing v, in collection C. Then, the expansion term candidates are sorted by descending o(v), and the top m are chosen.",
                "Finally, the term weights r(v) used in the expanded query are calculated based on the relevance model r(v) = X D p(q|D)p(v|D) p(v) p(D) (5) The quantity p(q|D) is the probability score assigned to the document in the initial retrieval.",
                "We use Dirichlet smoothing of p(v|D) with μ = 1000.",
                "This relevance model is then combined with the original query using linear interpolation, weighted by a parameter α.",
                "By default we used the top 50 documents for feedback and the top 20 expansion terms, with the feedback interpolation parameter α = 0.5 unless otherwise stated.",
                "For example, the baseline expanded query for topic 404 is: #weight(0.5 #combine(ireland peace talks) 0.5 #weight(0.10 ireland 0.08 peace 0.08 northern ...) 3.3 Expansion performance We measure our feedback algorithms effectiveness by two main criteria: precision, and robustness.",
                "Robustness, and the tradeoff between precision and robustness, is analyzed in Section 3.4.",
                "In this section, we examine average precision and precision in the top 10 documents (P10).",
                "We also include recall at 1,000 documents.",
                "For each query, we obtained a set of B feedback models using the Indri baseline.",
                "Each <br>feedback model</br> was obtained from a random sample of the top k documents taken with replacement.",
                "For these experiments, B = 30 and k = 50.",
                "Each <br>feedback model</br> contained 20 terms.",
                "On the query side, we used leave-one-out (LOO) sampling to create the query variants.",
                "Single-term query sampling had consistently worse performance across all collections and so our results here focus on LOO sampling.",
                "We used the methods described in Section 2 to estimate an enhanced <br>feedback model</br> from the Dirichlet posterior distribution for each query variant, and to combine the feedback models from all the query variants.",
                "We call our method resampling expansion and denote it as RS-FB here.",
                "We denote the Indri baseline feedback method as Base-FB.",
                "Results from applying both the baseline expansion method (Base-FB) and resampling expansion (RS-FB) are shown in Table 1.",
                "We observe several trends in this table.",
                "First, the average precision of RS-FB was comparable to Base-FB, achieving an average gain of 17.6% compared to using no expansion across the four collections.",
                "The Indri baseline expansion gain was 17.25%.",
                "Also, the RS-FB method achieved consistent improvements in P10 over Base-FB for every topic set, with an average improvement of 6.89% over Base-FB for all 350 topics.",
                "The lowest P10 gain over Base-FB was +3.82% for TREC-7 and the highest was +11.95% for wt10g.",
                "Finally, both Base-FB and RS-FB also consistently improved recall over using no expansion, with Base-FB achieving better recall than RS-FB for all topic sets. 3.4 Retrieval robustness We use the term robustness to mean the worst-case average precision performance of a feedback algorithm.",
                "Ideally, a robust feedback method would never perform worse than using the original query, while often performing better using the expansion.",
                "To evaluate robustness in this study, we use a very simple measure called the robustness index (RI)3 .",
                "For a set of queries Q, the RI measure is defined as: RI(Q) = n+ − n− |Q| (6) where n+ is the number of queries helped by the feedback method and n− is the number of queries hurt.",
                "Here, by helped we mean obtaining a higher average precision as a result of feedback.",
                "The value of RI ranges from a minimum 3 This is sometimes also called the reliability of improvement index and was used in Sakai et al. [17].",
                "Collection NoExp Base-FB RS-FB TREC 1&2 AvgP 0.1818 0.2419 (+33.04%) 0.2406 (+32.24%) P10 0.4443 0.4913 (+10.57%) 0.5363 (+17.83%) Recall 15084/37393 19172/37393 15396/37393 TREC 7 AvgP 0.1890 0.2175 (+15.07%) 0.2169 (+14.75%) P10 0.4200 0.4320 (+2.85%) 0.4480 (+6.67%) Recall 2179/4674 2608/4674 2487/4674 TREC 8 AvgP 0.2031 0.2361 (+16.25%) 0.2268 (+11.70%) P10 0.3960 0.4160 (+5.05%) 0.4340 (+9.59%) Recall 2144/4728 2642/4728 2485/4728 wt10g AvgP 0.1741 0.1829 (+5.06%) 0.1946 (+11.78%) P10 0.2760 0.2630 (-4.71%) 0.2960 (+7.24%) Recall 3361/5980 3725/5980 3664/5980 Table 1: Comparison of baseline (Base-FB) feedback and feedback using re-sampling (RS-FB).",
                "Improvement shown for BaseFB and RS-FB is relative to using no expansion. (a) TREC 1&2 (upper curve); TREC 8 (lower curve) (b) TREC 7 (upper curve); wt10g (lower curve) Figure 3: The trade-off between robustness and average precision for different corpora.",
                "The x-axis gives the change in MAP over using baseline expansion with α = 0.5.",
                "The yaxis gives the Robustness Index (RI).",
                "Each curve through uncircled points shows the RI/MAP tradeoff using the simple small-α strategy (see text) as α decreases from 0.5 to zero in the direction of the arrow.",
                "Circled points represent the tradeoffs obtained by resampling feedback for α = 0.5.",
                "Collection N Base-FB RS-FB n− RI n− RI TREC 1&2 103 26 +0.495 15 +0.709 TREC 7 46 14 +0.391 10 +0.565 TREC 8 44 12 +0.455 12 +0.455 wt10g 91 48 -0.055 39 +0.143 Combined 284 100 +0.296 76 +0.465 Table 2: Comparison of robustness index (RI) for baseline feedback (Base-FB) vs. resampling feedback (RS-FB).",
                "Also shown are the actual number of queries hurt by feedback (n−) for each method and collection.",
                "Queries for which initial average precision was negligible (≤ 0.01) were ignored, giving the remaining query count in column N. of −1.0, when all queries are hurt by the feedback method, to +1.0 when all queries are helped.",
                "The RI measure does not take into account the magnitude or distribution of the amount of change across the set Q.",
                "However, it is easy to understand as a general indication of robustness.",
                "One obvious way to improve the worst-case performance of feedback is simply to use a smaller fixed α interpolation parameter, such as α = 0.3, placing less weight on the (possibly risky) <br>feedback model</br> and more on the original query.",
                "We call this the small-α strategy.",
                "Since we are also reducing the potential gains when the <br>feedback model</br> is right, however, we would expect some trade-off between average precision and robustness.",
                "We therefore compared the precision/robustness trade-off between our resampling feedback algorithm, and the simple small-α method.",
                "The results are summarized in Figure 3.",
                "In the figure, the curve for each topic set interpolates between trade-off points, beginning at x=0, where α = 0.5, and continuing in the direction of the arrow as α decreases and the original query is given more and more weight.",
                "As expected, robustness continuously increases as we move along the curve, but mean average precision generally drops as the gains from feedback are eliminated.",
                "For comparison, the performance of resampling feedback at α = 0.5 is shown for each collection as the circled point.",
                "Higher and to the right is better.",
                "This figure shows that resampling feedback gives a somewhat better trade-off than the small-α approach for 3 of the 4 collections.",
                "Figure 4: Histogram showing improved robustness of resampling feedback (RS-FB) over baseline feedback (Base-FB) for all datasets combined.",
                "Queries are binned by % change in AP compared to the unexpanded query.",
                "Collection DS + QV DS + No QV TREC 1&2 AvgP 0.2406 0.2547 (+5.86%) P10 0.5263 0.5362 (+1.88%) RI 0.7087 0.6515 (-0.0572) TREC 7 AvgP 0.2169 0.2200 (+1.43%) P10 0.4480 0.4300 (-4.02%) RI 0.5652 0.2609 (-0.3043) TREC 8 AvgP 0.2268 0.2257 (-0.49%) P10 0.4340 0.4200 (-3.23%) RI 0.4545 0.4091 (-0.0454) wt10g AvgP 0.1946 0.1865 (-4.16%) P10 0.2960 0.2680 (-9.46%) RI 0.1429 0.0220 (-0.1209) Table 3: Comparison of resampling feedback using document sampling (DS) with (QV) and without (No QV) combining feedback models from multiple query variants.",
                "Table 2 gives the Robustness Index scores for Base-FB and RS-FB.",
                "The RS-FB feedback method obtained higher robustness than Base-FB on three of the four topic sets, with only slightly worse performance on TREC-8.",
                "A more detailed view showing the distribution over relative changes in AP is given by the histogram in Figure 4.",
                "Compared to Base-FB, the RS-FB method achieves a noticable reduction in the number of queries significantly hurt by expansion (i.e. where AP is hurt by 25% or more), while preserving positive gains in AP. 3.5 Effect of query and document sampling methods Given our algorithms improved robustness seen in Section 3.4, an important question is what component of our system is responsible.",
                "Is it the use of document re-sampling, the use of multiple query variants, or some other factor?",
                "The results in Table 3 suggest that the model combination based on query variants may be largely account for the improved robustness.",
                "When query variants are turned off and the original query is used by itself with document sampling, there is little net change in average precision, a small decrease in P10 for 3 out of the 4 topic sets, but a significant drop in robustness for all topic sets.",
                "In two cases, the RI measure drops by more than 50%.",
                "We also examined the effect of the document sampling method on retrieval effectiveness, using two different strategies.",
                "The uniform weighting strategy ignored the relevance scores from the initial retrieval and gave each document in the top k the same probability of selection.",
                "In contrast, the relevance-score weighting strategy chose documents with probability proportional to their relevance scores.",
                "In this way, documents that were more highly ranked were more likely to be selected.",
                "Results are shown in Table 4.",
                "The relevance-score weighting strategy performs better overall, with significantly higher RI and P10 scores on 3 of the 4 topic sets.",
                "The difference in average precision between the methods, however, is less marked.",
                "This suggests that uniform weighting acts to increase variance in retrieval results: when initial average precision is high, there are many relevant documents in the top k and uniform sampling may give a more representative relevance model than focusing on the highly-ranked items.",
                "On the other hand, when initial precision is low, there are few relevant documents in the bottom ranks and uniform sampling mixes in more of the non-relevant documents.",
                "For space reasons we only summarize our findings on sample size here.",
                "The number of samples has some effect on precision when less than 10, but performance stabilizes at around 15 to 20 samples.",
                "We used 30 samples for our experiments.",
                "Much beyond this level, the additional benefits of more samples decrease as the initial score distribution is more closely fit and the processing time increases. 3.6 The effect of resampling on expansion term quality Ideally, a retrieval model should not require a stopword list when estimating a model of relevance: a robust statistical model should down-weight stopwords automatically depending on context.",
                "Stopwords can harm feedback if selected as feedback terms, because they are typically poor discriminators and waste valuable term slots.",
                "In practice, however, because most term selection methods resemble a tf · idf type of weighting, terms with low idf but very high tf can sometimes be selected as expansion term candidates.",
                "This happens, for example, even with the Relevance Model approach that is part of our baseline feedback.",
                "To ensure as strong a baseline as possible, we use a stoplist for all experiments reported here.",
                "If we turn off the stopword list, however, we obtain results such as those shown in Table 5 where four of the top ten baseline feedback terms for TREC topic 60 (said, but, their, not) are stopwords using the BaseFB method. (The top 100 expansion terms were selected to generate this example.)",
                "Indris method attempts to address the stopword problem by applying an initial step based on Ponte [14] to select less-common terms that have high log-odds of being in the top-ranked documents compared to the whole collection.",
                "Nevertheless, this does not overcome the stopword problem completely, especially as the number of feedback terms grows.",
                "Using resampling feedback, however, appears to mitigate Collection QV + Uniform QV + Relevance-score weighting weighting TREC 1&2 AvgP 0.2545 0.2406 (-5.46%) P10 0.5369 0.5263 (-1.97%) RI 0.6212 0.7087 (+14.09%) TREC 7 AvgP 0.2174 0.2169 (-0.23%) P10 0.4320 0.4480 (+3.70%) RI 0.4783 0.5652 (+18.17%) TREC 8 AvgP 0.2267 0.2268 (+0.04%) P10 0.4120 0.4340 (+5.34%) RI 0.4545 0.4545 (+0.00%) wt10g AvgP 0.1808 0.1946 (+7.63%) P10 0.2680 0.2960 (+10.45%) RI 0.0220 0.1099 (+399.5%) Table 4: Comparison of uniform and relevance-weighted document sampling.",
                "The percentage change compared to uniform sampling is shown in parentheses.",
                "QV indicates that query variants were used in both runs.",
                "Baseline FB p(wi|R) Resampling FB p(wi|R) said 0.055 court 0.026 court 0.055 pay 0.018 pay 0.034 federal 0.012 but 0.026 education 0.011 employees 0.024 teachers 0.010 their 0.024 employees 0.010 not 0.023 case 0.010 federal 0.021 their 0.009 workers 0.020 appeals 0.008 education 0.020 union 0.007 Table 5: Feedback term quality when a stoplist is not used.",
                "Feedback terms for TREC topic 60: merit pay vs seniority. the effect of stopwords automatically.",
                "In the example of Table 5, resampling feedback leaves only one stopword (their) in the top ten.",
                "We observed similar feedback term behavior across many other topics.",
                "The reason for this effect appears to be the interaction of the term selection score with the top-m term cutoff.",
                "While the presence and even proportion of particular stopwords is fairly stable across different document samples, their relative position in the top-m list is not, as sets of documents with varying numbers of better, lower-frequency term candidates are examined for each sample.",
                "As a result, while some number of stopwords may appear in each sampled document set, any given stopword tends to fall below the cutoff for multiple samples, leading to its classification as a high-variance, low-weight feature. 4.",
                "RELATED WORK Our approach is related to previous work from several areas of information retrieval and machine learning.",
                "Our use of query variation was inspired by the work of YomTov et al. [20], Carpineto et al. [5], and Amati et al. [2], among others.",
                "These studies use the idea of creating multiple subqueries and then examining the nature of the overlap in the documents and/or expansion terms that result from each subquery.",
                "Model combination is performed using heuristics.",
                "In particular, the studies of Amati et al. and Carpineto et al. investigated combining terms from individual distributional methods using a term-reranking combination heuristic.",
                "In a set of TREC topics they found wide average variation in the rank-distance of terms from different expansion methods.",
                "Their combination method gave modest positive improvements in average precision.",
                "The idea of examining the overlap between lists of suggested terms has also been used in early query expansion approaches.",
                "Xu and Crofts method of Local Context Analysis (LCA) [19] includes a factor in the empirically-derived weighting formula that causes expansion terms to be preferred that have connections to multiple query terms.",
                "On the document side, recent work by Zhou & Croft [21] explored the idea of adding noise to documents, re-scoring them, and using the stability of the resulting rankings as an estimate of query difficulty.",
                "This is related to our use of document sampling to estimate the risk of the <br>feedback model</br> built from the different sets of top-retrieved documents.",
                "Sakai et al. [17] proposed an approach to improving the robustness of pseudo-relevance feedback using a method they call selective sampling.",
                "The essence of their method is that they allow skipping of some top-ranked documents, based on a clustering criterion, in order to select a more varied and novel set of documents later in the ranking for use by a traditional pseudo-feedback method.",
                "Their study did not find significant improvements in either robustness (RI) or MAP on their corpora.",
                "Greiff, Morgan and Ponte [8] explored the role of variance in term weighting.",
                "In a series of simulations that simplified the problem to 2-feature documents, they found that average precision degrades as term frequency variance - high noiseincreases.",
                "Downweighting terms with high variance resulted in improved average precision.",
                "This seems in accord with our own findings for individual feedback models.",
                "Estimates of output variance have recently been used for improved text classification.",
                "Lee et al. [11] used queryspecific variance estimates of classifier outputs to perform improved model combination.",
                "Instead of using sampling, they were able to derive closed-form expressions for classifier variance by assuming base classifiers using simple types of inference networks.",
                "Ando and Zhang proposed a method that they call structural feedback [3] and showed how to apply it to query expansion for the TREC Genomics Track.",
                "They used r query variations to obtain R different sets Sr of top-ranked documents that have been intersected with the top-ranked documents obtained from the original query qorig.",
                "For each Si, the normalized centroid vector ˆwi of the documents is calculated.",
                "Principal component analysis (PCA) is then applied to the ˆwi to obtain the matrix Φ of H left singular vectors φh that are used to obtain the new, expanded query qexp = qorig + ΦT Φqorig. (7) In the case H = 1, we have a single left singular vector φ: qexp = qorig + (φT qorig)φ so that the dot product φT qorig is a type of dynamic weight on the expanded query that is based on the similarity of the original query to the expanded query.",
                "The use of variance as a <br>feedback model</br> quality measure occurs indirectly through the application of PCA.",
                "It would be interesting to study the connections between this approach and our own modelfitting method.",
                "Finally, in language modeling approaches to feedback, Tao and Zhai [18] describe a method for more robust feedback that allows each document to have a different feedback α.",
                "The feedback weights are derived automatically using regularized EM.",
                "A roughly equal balance of query and expansion model is implied by their EM stopping condition.",
                "They propose tailoring the stopping parameter η based on a function of some quality measure of feedback documents. 5.",
                "CONCLUSIONS We have presented a new approach to pseudo-relevance feedback based on document and query sampling.",
                "The use of sampling is a very flexible and powerful device and is motivated by our general desire to extend current models of retrieval by estimating the risk or variance associated with the parameters or output of retrieval processes.",
                "Such variance estimates, for example, may be naturally used in a Bayesian framework for improved model estimation and combination.",
                "Applications such as selective expansion may then be implemented in a principled way.",
                "While our study uses the language modeling approach as a framework for experiments, we make few assumptions about the actual workings of the feedback algorithm.",
                "We believe it is likely that any reasonably effective baseline feedback algorithm would benefit from our approach.",
                "Our results on standard TREC collections show that our framework improves the robustness of a strong baseline feedback method across a variety of collections, without sacrificing average precision.",
                "It also gives small but consistent gains in top10 precision.",
                "In future work, we envision an investigation into how varying the set of sampling methods used and the number of samples controls the trade-off between robustness, accuracy, and efficiency.",
                "Acknowledgements We thank Paul Bennett for valuable discussions related to this work, which was supported by NSF grants #IIS-0534345 and #CNS-0454018, and U.S. Dept. of Education grant #R305G03123.",
                "Any opinions, findings, and conclusions or recommendations expressed in this material are the authors. and do not necessarily reflect those of the sponsors. 6.",
                "REFERENCES [1] The Lemur toolkit for language modeling and retrieval. http://www.lemurproject.org. [2] G. Amati, C. Carpineto, and G. Romano.",
                "Query difficulty, robustness, and selective application of query expansion.",
                "In Proc. of the 25th European Conf. on Information Retrieval (ECIR 2004), pages 127-137. [3] R. K. Ando and T. Zhang.",
                "A high-performance semi-supervised learning method for text chunking.",
                "In Proc. of the 43rd Annual Meeting of the ACL, pages 1-9, June 2005. [4] L. Breiman.",
                "Bagging predictors.",
                "Machine Learning, 24(2):123-140, 1996. [5] C. Carpineto, G. Romano, and V. Giannini.",
                "Improving retrieval feedback with multiple term-ranking function combination.",
                "ACM Trans.",
                "Info.",
                "Systems, 20(3):259 - 290. [6] K. Collins-Thompson, P. Ogilvie, and J. Callan.",
                "Initial results with structured queries and language models on half a terabyte of text.",
                "In Proc. of 2005 Text REtrieval Conference.",
                "NIST Special Publication. [7] R. O. Duda, P. E. Hart, and D. G. Stork.",
                "Pattern Classification.",
                "Wiley and Sons, 2nd edition, 2001. [8] W. R. Greiff, W. T. Morgan, and J. M. Ponte.",
                "The role of variance in term weighting for probabilistic information retrieval.",
                "In Proc. of the 11th Intl.",
                "Conf. on Info. and Knowledge Mgmt. (CIKM 2002), pages 252-259. [9] T. Kohonen, J. Hynninen, J. Kangas, and J. Laaksonen.",
                "SOMPAK: The self-organizing map program package.",
                "Technical Report A31, Helsinki University of Technology, 1996. http://www.cis.hut.fi/research/papers/som tr96.ps.Z. [10] V. Lavrenko.",
                "A Generative Theory of Relevance.",
                "PhD thesis, University of Massachusetts, Amherst, 2004. [11] C.-H. Lee, R. Greiner, and S. Wang.",
                "Using query-specific variance estimates to combine Bayesian classifiers.",
                "In Proc. of the 23rd Intl.",
                "Conf. on Machine Learning (ICML 2006), pages 529-536. [12] D. Metzler and W. B. Croft.",
                "Combining the language model and inference network approaches to retrieval.",
                "Info.",
                "Processing and Mgmt., 40(5):735-750, 2004. [13] T. Minka.",
                "Estimating a Dirichlet distribution.",
                "Technical report, 2000. http://research.microsoft.com/ minka/papers/dirichlet. [14] J. Ponte.",
                "Advances in Information Retrieval, chapter Language models for relevance feedback, pages 73-96. 2000.",
                "W.B.",
                "Croft, ed. [15] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In Proc. of the 1998 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 275-281. [16] J. Rocchio.",
                "The SMART Retrieval System, chapter Relevance Feedback in Information Retrieval, pages 313-323.",
                "Prentice-Hall, 1971.",
                "G. Salton, ed. [17] T. Sakai, T. Manabe, and M. Koyama.",
                "Flexible pseudo-relevance feedback via selective sampling.",
                "ACM Transactions on Asian Language Information Processing (TALIP), 4(2):111-135, 2005. [18] T. Tao and C. Zhai.",
                "Regularized estimation of mixture models for robust pseudo-relevance feedback.",
                "In Proc. of the 2006 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 162-169. [19] J. Xu and W. B. Croft.",
                "Improving the effectiveness of information retrieval with local context analysis.",
                "ACM Trans.",
                "Inf.",
                "Syst., 18(1):79-112, 2000. [20] E. YomTov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty.",
                "In Proc. of the 2005 ACM SIGIR Conf. on Research and Development in Information Retrieval, pages 512-519. [21] Y. Zhou and W. B. Croft.",
                "Ranking robustness: a novel framework to predict query performance.",
                "In Proc. of the 15th ACM Intl.",
                "Conf. on Information and Knowledge Mgmt. (CIKM 2006), pages 567-574."
            ],
            "original_annotated_samples": [
                "Treating the baseline feedback method as a black box, and the output <br>feedback model</br> as a random variable, we estimate a posterior distribution for the <br>feedback model</br> by resampling a given querys top-retrieved documents, using the posterior mean or mode as the enhanced feedback model.",
                "We find that resampling documents helps increase individual <br>feedback model</br> precision by removing noise terms, while sampling from the query improves robustness (worst-case performance) by emphasizing terms related to multiple query aspects.",
                "Next, a single <br>feedback model</br> vector is computed according to some sort of average, centroid, or expectation over the set of possibly-relevant document models.",
                "The use of an expectation is reasonable for practical and theoretical reasons, but by itself ignores potentially valuable information about the risk of the <br>feedback model</br>.",
                "Therefore, we propose a method for estimating uncertainty associated with an individual <br>feedback model</br> in terms of a posterior distribution over language models."
            ],
            "translated_annotated_samples": [
                "Tratando el método de retroalimentación de línea base como una caja negra, y el <br>modelo de retroalimentación</br> de salida como una variable aleatoria, estimamos una distribución posterior para el <br>modelo de retroalimentación</br> mediante el remuestreo de los documentos mejor recuperados de una consulta dada, utilizando la media o la moda posterior como el modelo de retroalimentación mejorado.",
                "Observamos que el remuestreo de documentos ayuda a aumentar la precisión del <br>modelo de retroalimentación</br> individual al eliminar términos de ruido, mientras que el muestreo de la consulta mejora la robustez (rendimiento en el peor de los casos) al enfatizar términos relacionados con múltiples aspectos de la consulta.",
                "A continuación, se calcula un vector de <br>modelo de retroalimentación</br> único según algún tipo de promedio, centroide o expectativa sobre el conjunto de modelos de documentos posiblemente relevantes.",
                "El uso de una expectativa es razonable por razones prácticas y teóricas, pero por sí sola ignora información potencialmente valiosa sobre el riesgo del <br>modelo de retroalimentación</br>.",
                "Por lo tanto, proponemos un método para estimar la incertidumbre asociada con un <br>modelo de retroalimentación</br> individual en términos de una distribución posterior sobre modelos de lenguaje."
            ],
            "translated_text": "Estimación y Uso de la Incertidumbre en la Retroalimentación de Pseudo-Relevancia Kevyn Collins-Thompson y Jamie Callan Instituto de Tecnologías del Lenguaje Escuela de Ciencias de la Computación Universidad Carnegie Mellon Pittsburgh, PA 15213-8213 EE. UU. {kct | callan}@cs.cmu.edu RESUMEN Los métodos existentes de retroalimentación de pseudo-relevancia suelen realizar un promedio de los documentos mejor clasificados, pero ignoran una dimensión estadística importante: el riesgo o la varianza asociada tanto a los modelos de documentos individuales como a su combinación. Tratando el método de retroalimentación de línea base como una caja negra, y el <br>modelo de retroalimentación</br> de salida como una variable aleatoria, estimamos una distribución posterior para el <br>modelo de retroalimentación</br> mediante el remuestreo de los documentos mejor recuperados de una consulta dada, utilizando la media o la moda posterior como el modelo de retroalimentación mejorado. Luego realizamos la combinación de modelos sobre varios modelos mejorados, cada uno basado en una consulta ligeramente modificada muestreada de la consulta original. Observamos que el remuestreo de documentos ayuda a aumentar la precisión del <br>modelo de retroalimentación</br> individual al eliminar términos de ruido, mientras que el muestreo de la consulta mejora la robustez (rendimiento en el peor de los casos) al enfatizar términos relacionados con múltiples aspectos de la consulta. El resultado es un algoritmo de meta-retroalimentación que es tanto más robusto como más preciso que el método de referencia original. Categorías y Descriptores de Asignaturas: H.3.3 [Recuperación de Información]: Modelos de Recuperación Términos Generales: Algoritmos, Experimentación 1. INTRODUCCIÓN La incertidumbre es una característica inherente de la recuperación de información. No solo no sabemos las consultas que se presentarán a nuestro algoritmo de recuperación con anticipación, sino que la necesidad de información de los usuarios puede ser vaga o estar incompletamente especificada por estas consultas. Incluso si la consulta estuviera perfectamente especificada, el lenguaje en los documentos de la colección es inherentemente complejo y ambiguo, y hacer coincidir dicho lenguaje de manera efectiva es un problema formidable por sí mismo. Con esto en mente, deseamos tratar muchas cantidades importantes calculadas por el sistema de recuperación, ya sea un puntaje de relevancia para un documento, o un peso para un término de expansión de consulta, como variables aleatorias cuyo valor real es incierto pero donde la incertidumbre sobre el valor real puede ser cuantificada reemplazando el valor fijo con una distribución de probabilidad sobre posibles valores. De esta manera, los algoritmos de recuperación pueden intentar cuantificar el riesgo o la incertidumbre asociados con sus clasificaciones de salida, o mejorar la estabilidad o precisión de sus cálculos internos. Los algoritmos actuales para la retroalimentación de pseudo relevancia (PRF) tienden a seguir el mismo método básico ya sea que utilicemos algoritmos basados en espacio vectorial como la fórmula de Rocchio [16], o enfoques más recientes de modelado de lenguaje como Modelos de Relevancia [10]. Primero, se obtiene un conjunto de documentos recuperados en la parte superior a partir de una consulta inicial y se asume que aproxima un conjunto de documentos relevantes. A continuación, se calcula un vector de <br>modelo de retroalimentación</br> único según algún tipo de promedio, centroide o expectativa sobre el conjunto de modelos de documentos posiblemente relevantes. Por ejemplo, los vectores de documentos pueden combinarse con igual ponderación, como en Rocchio, o por probabilidad de consulta, como se puede hacer utilizando el Modelo de Relevancia. El uso de una expectativa es razonable por razones prácticas y teóricas, pero por sí sola ignora información potencialmente valiosa sobre el riesgo del <br>modelo de retroalimentación</br>. Nuestra hipótesis principal en este artículo es que estimar la incertidumbre en la retroalimentación es útil y conduce a modelos de retroalimentación individual más efectivos y a modelos combinados más robustos. Por lo tanto, proponemos un método para estimar la incertidumbre asociada con un <br>modelo de retroalimentación</br> individual en términos de una distribución posterior sobre modelos de lenguaje. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "estimating uncertainty": {
            "translated_key": "incertidumbre",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Estimation and Use of Uncertainty in Pseudo-relevance Feedback Kevyn Collins-Thompson and Jamie Callan Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213-8213 U.S.A. {kct | callan}@cs.cmu.edu ABSTRACT Existing pseudo-relevance feedback methods typically perform averaging over the top-retrieved documents, but ignore an important statistical dimension: the risk or variance associated with either the individual document models, or their combination.",
                "Treating the baseline feedback method as a black box, and the output feedback model as a random variable, we estimate a posterior distribution for the feedback model by resampling a given querys top-retrieved documents, using the posterior mean or mode as the enhanced feedback model.",
                "We then perform model combination over several enhanced models, each based on a slightly modified query sampled from the original query.",
                "We find that resampling documents helps increase individual feedback model precision by removing noise terms, while sampling from the query improves robustness (worst-case performance) by emphasizing terms related to multiple query aspects.",
                "The result is a meta-feedback algorithm that is both more robust and more precise than the original strong baseline method.",
                "Categories and Subject Descriptors: H.3.3 [Information Retrieval]: Retrieval Models General Terms: Algorithms, Experimentation 1.",
                "INTRODUCTION Uncertainty is an inherent feature of information retrieval.",
                "Not only do we not know the queries that will be presented to our retrieval algorithm ahead of time, but the users information need may be vague or incompletely specified by these queries.",
                "Even if the query were perfectly specified, language in the collection documents is inherently complex and ambiguous and matching such language effectively is a formidable problem by itself.",
                "With this in mind, we wish to treat many important quantities calculated by the retrieval system, whether a relevance score for a document, or a weight for a query expansion term, as random variables whose true value is uncertain but where the uncertainty about the true value may be quantified by replacing the fixed value with a probability distribution over possible values.",
                "In this way, retrieval algorithms may attempt to quantify the risk or uncertainty associated with their output rankings, or improve the stability or precision of their internal calculations.",
                "Current algorithms for pseudo-relevance feedback (PRF) tend to follow the same basic method whether we use vector space-based algorithms such as Rocchios formula [16], or more recent language modeling approaches such as Relevance Models [10].",
                "First, a set of top-retrieved documents is obtained from an initial query and assumed to approximate a set of relevant documents.",
                "Next, a single feedback model vector is computed according to some sort of average, centroid, or expectation over the set of possibly-relevant document models.",
                "For example, the document vectors may be combined with equal weighting, as in Rocchio, or by query likelihood, as may be done using the Relevance Model1 .",
                "The use of an expectation is reasonable for practical and theoretical reasons, but by itself ignores potentially valuable information about the risk of the feedback model.",
                "Our main hypothesis in this paper is that estimating the uncertainty in feedback is useful and leads to better individual feedback models and more robust combined models.",
                "Therefore, we propose a method for <br>estimating uncertainty</br> associated with an individual feedback model in terms of a posterior distribution over language models.",
                "To do this, we systematically vary the inputs to the baseline feedback method and fit a Dirichlet distribution to the output.",
                "We use the posterior mean or mode as the improved feedback model estimate.",
                "This process is shown in Figure 1.",
                "As we show later, the mean and mode may vary significantly from the single feedback model proposed by the baseline method.",
                "We also perform model combination using several improved feedback language models obtained by a small number of new queries sampled from the original query.",
                "A models weight combines two complementary factors: the models probability of generating the query, and the variance of the model, with high-variance models getting lower weight. 1 For example, an expected parameter vector conditioned on the query observation is formed from top-retrieved documents, which are treated as training strings (see [10], p. 62).",
                "Figure 1: Estimating the uncertainty of the feedback model for a single query. 2.",
                "SAMPLING-BASED FEEDBACK In Sections 2.1-2.5 we describe a general method for estimating a probability distribution over the set of possible language models.",
                "In Sections 2.6 and 2.7 we summarize how different query samples are used to generate multiple feedback models, which are then combined. 2.1 Modeling Feedback Uncertainty Given a query Q and a collection C, we assume a probabilistic retrieval system that assigns a real-valued document score f(D, Q) to each document D in C, such that the score is proportional to the estimated probability of relevance.",
                "We make no other assumptions about f(D, Q).",
                "The nature of f(D, Q) may be complex: for example, if the retrieval system supports structured query languages [12], then f(D, Q) may represent the output of an arbitrarily complex inference network defined by the structured query operators.",
                "In theory, the scoring function can vary from query to query, although in this study for simplicity we keep the scoring function the same for all queries.",
                "Our specific query method is given in Section 3.",
                "We treat the feedback algorithm as a black box and assume that the inputs to the feedback algorithm are the original query and the corresponding top-retrieved documents, with a score being given to each document.",
                "We assume that the output of the feedback algorithm is a vector of term weights to be used to add or reweight the terms in the representation of the original query, with the vector normalized to form a probability distribution.",
                "We view the the inputs to the feedback black box as random variables, and analyze the feedback model as a random variable that changes in response to changes in the inputs.",
                "Like the document scoring function f(D, Q), the feedback algorithm may implement a complex, non-linear scoring formula, and so as its inputs vary, the resulting feedback models may have a complex distribution over the space of feedback models (the sample space).",
                "Because of this potential complexity, we do not attempt to derive a posterior distribution in closed form, but instead use simulation.",
                "We call this distribution over possible feedback models the feedback model distribution.",
                "Our goal in this section is to estimate a useful approximation to the feedback model distribution.",
                "For a specific framework for experiments, we use the language modeling (LM) approach for information retrieval [15].",
                "The score of a document D with respect to a query Q and collection C is given by p(Q|D) with respect to language models ˆθQ and ˆθD estimated for the query and document respectively.",
                "We denote the set of k top-retrieved documents from collection C in response to Q by DQ(k, C).",
                "For simplicity, we assume that queries and documents are generated by multinomial distributions whose parameters are represented by unigram language models.",
                "To incorporate feedback in the LM approach, we assume a model-based scheme in which our goal is take the query and resulting ranked documents DQ(k, C) as input, and output an expansion language model ˆθE, which is then interpolated with the original query model ˆθQ: ˆθNew = (1 − α) · ˆθQ + α · ˆθE (1) This includes the possibility of α = 1 where the original query mode is completely replaced by the feedback model.",
                "Our sample space is the set of all possible language models LF that may be output as feedback models.",
                "Our approach is to take samples from this space and then fit a distribution to the samples using maximum likelihood.",
                "For simplicity, we start by assuming the latent feedback distribution has the form of a Dirichlet distribution.",
                "Although the Dirichlet is a unimodal distribution, and in general quite limited in its expressiveness in the sample space, it is a natural match for the multinomial language model, can be estimated quickly, and can capture the most salient features of confident and uncertain feedback models, such as the overall spread of the distibution. 2.2 Resampling document models We would like an approximation to the posterior distribution of the feedback model LF .",
                "To accomplish this, we apply a widely-used simulation technique called bootstrap sampling ([7], p. 474) on the input parameters, namely, the set of top-retrieved documents.",
                "Bootstrap sampling allows us to simulate the approximate effect of perturbing the parameters within the black box feedback algorithm by perturbing the inputs to that algorithm in a systematic way, while making no assumptions about the nature of the feedback algorithm.",
                "Specifically, we sample k documents with replacement from DQ(k, C), and calculate an expansion language model θb using the black box feedback method.",
                "We repeat this process B times to obtain a set of B feedback language models, to which we then fit a Dirichlet distribution.",
                "Typically B is in the range of 20 to 50 samples, with performance being relatively stable in this range.",
                "Note that instead of treating each top document as equally likely, we sample according to the estimated probabilities of relevance of each document in DQ(k, C).",
                "Thus, a document is more likely to be chosen the higher it is in the ranking. 2.3 Justification for a sampling approach The rationale for our sampling approach has two parts.",
                "First, we want to improve the quality of individual feedback models by smoothing out variation when the baseline feedback model is unstable.",
                "In this respect, our approach resembles bagging [4], an ensemble approach which generates multiple versions of a predictor by making bootstrap copies of the training set, and then averages the (numerical) predictors.",
                "In our application, top-retrieved documents can be seen as a kind of noisy training set for relevance.",
                "Second, sampling is an effective way to estimate basic properties of the feedback posterior distribution, which can then be used for improved model combination.",
                "For example, a model may be weighted by its prediction confidence, estimated as a function of the variability of the posterior around the model. foo2-401.map-Dim:5434,Size:12*12units,gaussianneighborhood (a) Topic 401 Foreign minorities, Germany foo2-402.map-Dim:5698,Size:12*12units,gaussianneighborhood (b) Topic 402 Behavioral genetics foo2-459.map-Dim:8969,Size:12*12units,gaussianneighborhood (c) Topic 459 When can a lender foreclose on property Figure 2: Visualization of expansion language model variance using self-organizing maps, showing the distribution of language models that results from resampling the inputs to the baseline expansion method.",
                "The language model that would have been chosen by the baseline expansion is at the center of each map.",
                "The similarity function is JensenShannon divergence. 2.4 Visualizing feedback distributions Before describing how we fit and use the Dirichlet distribution over feedback models, it is instructive to view some examples of actual feedback model distributions that result from bootstrap sampling the top-retrieved documents from different TREC topics.",
                "Each point in our sample space is a language model, which typically has several thousand dimensions.",
                "To help analyze the behavior of our method we used a Self-Organizing Map (via the SOM-PAK package [9]), to flatten and visualize the high-dimensional density function2 .",
                "The density maps for three TREC topics are shown in Figure 2 above.",
                "The dark areas represent regions of high similarity between language models.",
                "The light areas represent regions of low similarity - the valleys between clusters.",
                "Each diagram is centered on the language model that would have been chosen by the baseline expansion.",
                "A single peak (mode) is evident in some examples, but more complex structure appears in others.",
                "Also, while the distribution is usually close to the baseline feedback model, for some topics they are a significant distance apart (as measured by JensenShannon divergence), as in Subfigure 2c.",
                "In such cases, the mode or mean of the feedback distribution often performs significantly better than the baseline (and in a smaller proportion of cases, significantly worse). 2.5 Fitting a posterior feedback distribution After obtaining feedback model samples by resampling the feedback model inputs, we estimate the feedback distribution.",
                "We assume that the multinomial feedback models {ˆθ1, . . . , ˆθB} were generated by a latent Dirichlet distribution with parameters {α1, . . . , αN }.",
                "To estimate the {α1, . . . , αN }, we fit the Dirichlet parameters to the B language model samples according to maximum likelihood using a generalized Newton procedure, details of which are given in Minka [13].",
                "We assume a simple Dirichlet prior over the {α1, . . . , αN }, setting each to αi = μ · p(wi | C), where μ is a parameter and p(· | C) is the collection language model estimated from a set of documents from collection C. The parameter fitting converges very quickly - typically just 2 or 2 Because our points are language models in the multinomial simplex, we extended SOM-PAK to support JensenShannon divergence, a widely-used similarity measure between probability distributions. 3 iterations are enough - so that it is practical to apply at query-time when computational overhead must be small.",
                "In practice, we can restrict the calculation to the vocabulary of the top-retrieved documents, instead of the entire collection.",
                "Note that for this step we are re-using the existing retrieved documents and not performing additional queries.",
                "Given the parameters of an N-dimensional Dirichlet distribution Dir(α) the mean μ and mode x vectors are easy to calculate and are given respectively by μi = αiP αi (2) and xi = αi−1P αi−N . (3) We can then choose the language model at the mean or the mode of the posterior as the final enhanced feedback model. (We found the mode to give slightly better performance.)",
                "For information retrieval, the number of samples we will have available is likely to be quite small for performance reasons - usually less than ten.",
                "Moreover, while random sampling is useful in certain cases, it is perfectly acceptable to allow deterministic sampling distributions, but these must be designed carefully in order to approximate an accurate output variance.",
                "We leave this for future study. 2.6 Query variants We use the following methods for generating variants of the original query.",
                "Each variant corresponds to a different assumption about which aspects of the original query may be important.",
                "This is a form of deterministic sampling.",
                "We selected three simple methods that cover complimentary assumptions about the query.",
                "No-expansion Use only the original query.",
                "The assumption is that the given terms are a complete description of the information need.",
                "Leave-one-out A single term is left out of the original query.",
                "The assumption is that one of the query terms is a noise term.",
                "Single-term A single term is chosen from the original query.",
                "This assumes that only one aspect of the query, namely, that represented by the term, is most important.",
                "After generating a variant of the original query, we combine it with the original query using a weight αSUB so that we do not stray too far.",
                "In this study, we set αSUB = 0.5.",
                "For example, using the Indri [12] query language, a leave-oneout variant of the initial query that omits the term ireland for TREC topic 404 is: #weight(0.5 #combine(ireland peace talks) 0.5 #combine(peace talks)) 2.7 Combining enhanced feedback models from multiple query variants When using multiple query variants, the resulting enhanced feedback models are combined using Bayesian model combination.",
                "To do this, we treat each word as an item to be classified as belonging to a relevant or non-relevant class, and derive a class probability for each word by combining the scores from each query variant.",
                "Each score is given by that terms probability in the Dirichlet distribution.",
                "The term scores are weighted by the inverse of the variance of the term in the enhanced feedback models Dirichlet distribution.",
                "The prior probability of a words membership in the relevant class is given by the probability of the original query in the entire enhanced expansion model. 3.",
                "EVALUATION In this section we present results confirming the usefulness of estimating a feedback model distribution from weighted resampling of top-ranked documents, and of combining the feedback models obtained from different small changes in the original query. 3.1 General method We evaluated performance on a total of 350 queries derived from four sets of TREC topics: 51-200 (TREC-1&2), 351-400 (TREC-7), 401-450 (TREC-8), and 451-550 (wt10g, TREC-9&10).",
                "We chose these for their varied content and document properties.",
                "For example, wt10g documents are Web pages with a wide variety of subjects and styles while TREC-1&2 documents are more homogeneous news articles.",
                "Indexing and retrieval was performed using the Indri system in the Lemur toolkit [12] [1].",
                "Our queries were derived from the words in the title field of the TREC topics.",
                "Phrases were not used.",
                "To generate the baseline queries passed to Indri, we wrapped the query terms with Indris #combine operator.",
                "For example, the initial query for topic 404 is: #combine(ireland peace talks) We performed Krovetz stemming for all experiments.",
                "Because we found that the baseline (Indri) expansion method performed better using a stopword list with the feedback model, all experiments used a stoplist of 419 common English words.",
                "However, an interesting side-effect of our resampling approach is that it tends to remove many stopwords from the feedback model, making a stoplist less critical.",
                "This is discussed further in Section 3.6. 3.2 Baseline feedback method For our baseline expansion method, we use an algorithm included in Indri 1.0 as the default expansion method.",
                "This method first selects terms using a log-odds calculation described by Ponte [14], but assigns final term weights using Lavrenkos relevance model[10].",
                "We chose the Indri method because it gives a consistently strong baseline, is based on a language modeling approach, and is simple to experiment with.",
                "In a TREC evaluation using the GOV2 corpus [6], the method was one of the topperforming runs, achieving a 19.8% gain in MAP compared to using unexpanded queries.",
                "In this study, it achieves an average gain in MAP of 17.25% over the four collections.",
                "Indris expansion method first calculates a log-odds ratio o(v) for each potential expansion term v given by o(v) = X D log p(v|D) p(v|C) (4) over all documents D containing v, in collection C. Then, the expansion term candidates are sorted by descending o(v), and the top m are chosen.",
                "Finally, the term weights r(v) used in the expanded query are calculated based on the relevance model r(v) = X D p(q|D)p(v|D) p(v) p(D) (5) The quantity p(q|D) is the probability score assigned to the document in the initial retrieval.",
                "We use Dirichlet smoothing of p(v|D) with μ = 1000.",
                "This relevance model is then combined with the original query using linear interpolation, weighted by a parameter α.",
                "By default we used the top 50 documents for feedback and the top 20 expansion terms, with the feedback interpolation parameter α = 0.5 unless otherwise stated.",
                "For example, the baseline expanded query for topic 404 is: #weight(0.5 #combine(ireland peace talks) 0.5 #weight(0.10 ireland 0.08 peace 0.08 northern ...) 3.3 Expansion performance We measure our feedback algorithms effectiveness by two main criteria: precision, and robustness.",
                "Robustness, and the tradeoff between precision and robustness, is analyzed in Section 3.4.",
                "In this section, we examine average precision and precision in the top 10 documents (P10).",
                "We also include recall at 1,000 documents.",
                "For each query, we obtained a set of B feedback models using the Indri baseline.",
                "Each feedback model was obtained from a random sample of the top k documents taken with replacement.",
                "For these experiments, B = 30 and k = 50.",
                "Each feedback model contained 20 terms.",
                "On the query side, we used leave-one-out (LOO) sampling to create the query variants.",
                "Single-term query sampling had consistently worse performance across all collections and so our results here focus on LOO sampling.",
                "We used the methods described in Section 2 to estimate an enhanced feedback model from the Dirichlet posterior distribution for each query variant, and to combine the feedback models from all the query variants.",
                "We call our method resampling expansion and denote it as RS-FB here.",
                "We denote the Indri baseline feedback method as Base-FB.",
                "Results from applying both the baseline expansion method (Base-FB) and resampling expansion (RS-FB) are shown in Table 1.",
                "We observe several trends in this table.",
                "First, the average precision of RS-FB was comparable to Base-FB, achieving an average gain of 17.6% compared to using no expansion across the four collections.",
                "The Indri baseline expansion gain was 17.25%.",
                "Also, the RS-FB method achieved consistent improvements in P10 over Base-FB for every topic set, with an average improvement of 6.89% over Base-FB for all 350 topics.",
                "The lowest P10 gain over Base-FB was +3.82% for TREC-7 and the highest was +11.95% for wt10g.",
                "Finally, both Base-FB and RS-FB also consistently improved recall over using no expansion, with Base-FB achieving better recall than RS-FB for all topic sets. 3.4 Retrieval robustness We use the term robustness to mean the worst-case average precision performance of a feedback algorithm.",
                "Ideally, a robust feedback method would never perform worse than using the original query, while often performing better using the expansion.",
                "To evaluate robustness in this study, we use a very simple measure called the robustness index (RI)3 .",
                "For a set of queries Q, the RI measure is defined as: RI(Q) = n+ − n− |Q| (6) where n+ is the number of queries helped by the feedback method and n− is the number of queries hurt.",
                "Here, by helped we mean obtaining a higher average precision as a result of feedback.",
                "The value of RI ranges from a minimum 3 This is sometimes also called the reliability of improvement index and was used in Sakai et al. [17].",
                "Collection NoExp Base-FB RS-FB TREC 1&2 AvgP 0.1818 0.2419 (+33.04%) 0.2406 (+32.24%) P10 0.4443 0.4913 (+10.57%) 0.5363 (+17.83%) Recall 15084/37393 19172/37393 15396/37393 TREC 7 AvgP 0.1890 0.2175 (+15.07%) 0.2169 (+14.75%) P10 0.4200 0.4320 (+2.85%) 0.4480 (+6.67%) Recall 2179/4674 2608/4674 2487/4674 TREC 8 AvgP 0.2031 0.2361 (+16.25%) 0.2268 (+11.70%) P10 0.3960 0.4160 (+5.05%) 0.4340 (+9.59%) Recall 2144/4728 2642/4728 2485/4728 wt10g AvgP 0.1741 0.1829 (+5.06%) 0.1946 (+11.78%) P10 0.2760 0.2630 (-4.71%) 0.2960 (+7.24%) Recall 3361/5980 3725/5980 3664/5980 Table 1: Comparison of baseline (Base-FB) feedback and feedback using re-sampling (RS-FB).",
                "Improvement shown for BaseFB and RS-FB is relative to using no expansion. (a) TREC 1&2 (upper curve); TREC 8 (lower curve) (b) TREC 7 (upper curve); wt10g (lower curve) Figure 3: The trade-off between robustness and average precision for different corpora.",
                "The x-axis gives the change in MAP over using baseline expansion with α = 0.5.",
                "The yaxis gives the Robustness Index (RI).",
                "Each curve through uncircled points shows the RI/MAP tradeoff using the simple small-α strategy (see text) as α decreases from 0.5 to zero in the direction of the arrow.",
                "Circled points represent the tradeoffs obtained by resampling feedback for α = 0.5.",
                "Collection N Base-FB RS-FB n− RI n− RI TREC 1&2 103 26 +0.495 15 +0.709 TREC 7 46 14 +0.391 10 +0.565 TREC 8 44 12 +0.455 12 +0.455 wt10g 91 48 -0.055 39 +0.143 Combined 284 100 +0.296 76 +0.465 Table 2: Comparison of robustness index (RI) for baseline feedback (Base-FB) vs. resampling feedback (RS-FB).",
                "Also shown are the actual number of queries hurt by feedback (n−) for each method and collection.",
                "Queries for which initial average precision was negligible (≤ 0.01) were ignored, giving the remaining query count in column N. of −1.0, when all queries are hurt by the feedback method, to +1.0 when all queries are helped.",
                "The RI measure does not take into account the magnitude or distribution of the amount of change across the set Q.",
                "However, it is easy to understand as a general indication of robustness.",
                "One obvious way to improve the worst-case performance of feedback is simply to use a smaller fixed α interpolation parameter, such as α = 0.3, placing less weight on the (possibly risky) feedback model and more on the original query.",
                "We call this the small-α strategy.",
                "Since we are also reducing the potential gains when the feedback model is right, however, we would expect some trade-off between average precision and robustness.",
                "We therefore compared the precision/robustness trade-off between our resampling feedback algorithm, and the simple small-α method.",
                "The results are summarized in Figure 3.",
                "In the figure, the curve for each topic set interpolates between trade-off points, beginning at x=0, where α = 0.5, and continuing in the direction of the arrow as α decreases and the original query is given more and more weight.",
                "As expected, robustness continuously increases as we move along the curve, but mean average precision generally drops as the gains from feedback are eliminated.",
                "For comparison, the performance of resampling feedback at α = 0.5 is shown for each collection as the circled point.",
                "Higher and to the right is better.",
                "This figure shows that resampling feedback gives a somewhat better trade-off than the small-α approach for 3 of the 4 collections.",
                "Figure 4: Histogram showing improved robustness of resampling feedback (RS-FB) over baseline feedback (Base-FB) for all datasets combined.",
                "Queries are binned by % change in AP compared to the unexpanded query.",
                "Collection DS + QV DS + No QV TREC 1&2 AvgP 0.2406 0.2547 (+5.86%) P10 0.5263 0.5362 (+1.88%) RI 0.7087 0.6515 (-0.0572) TREC 7 AvgP 0.2169 0.2200 (+1.43%) P10 0.4480 0.4300 (-4.02%) RI 0.5652 0.2609 (-0.3043) TREC 8 AvgP 0.2268 0.2257 (-0.49%) P10 0.4340 0.4200 (-3.23%) RI 0.4545 0.4091 (-0.0454) wt10g AvgP 0.1946 0.1865 (-4.16%) P10 0.2960 0.2680 (-9.46%) RI 0.1429 0.0220 (-0.1209) Table 3: Comparison of resampling feedback using document sampling (DS) with (QV) and without (No QV) combining feedback models from multiple query variants.",
                "Table 2 gives the Robustness Index scores for Base-FB and RS-FB.",
                "The RS-FB feedback method obtained higher robustness than Base-FB on three of the four topic sets, with only slightly worse performance on TREC-8.",
                "A more detailed view showing the distribution over relative changes in AP is given by the histogram in Figure 4.",
                "Compared to Base-FB, the RS-FB method achieves a noticable reduction in the number of queries significantly hurt by expansion (i.e. where AP is hurt by 25% or more), while preserving positive gains in AP. 3.5 Effect of query and document sampling methods Given our algorithms improved robustness seen in Section 3.4, an important question is what component of our system is responsible.",
                "Is it the use of document re-sampling, the use of multiple query variants, or some other factor?",
                "The results in Table 3 suggest that the model combination based on query variants may be largely account for the improved robustness.",
                "When query variants are turned off and the original query is used by itself with document sampling, there is little net change in average precision, a small decrease in P10 for 3 out of the 4 topic sets, but a significant drop in robustness for all topic sets.",
                "In two cases, the RI measure drops by more than 50%.",
                "We also examined the effect of the document sampling method on retrieval effectiveness, using two different strategies.",
                "The uniform weighting strategy ignored the relevance scores from the initial retrieval and gave each document in the top k the same probability of selection.",
                "In contrast, the relevance-score weighting strategy chose documents with probability proportional to their relevance scores.",
                "In this way, documents that were more highly ranked were more likely to be selected.",
                "Results are shown in Table 4.",
                "The relevance-score weighting strategy performs better overall, with significantly higher RI and P10 scores on 3 of the 4 topic sets.",
                "The difference in average precision between the methods, however, is less marked.",
                "This suggests that uniform weighting acts to increase variance in retrieval results: when initial average precision is high, there are many relevant documents in the top k and uniform sampling may give a more representative relevance model than focusing on the highly-ranked items.",
                "On the other hand, when initial precision is low, there are few relevant documents in the bottom ranks and uniform sampling mixes in more of the non-relevant documents.",
                "For space reasons we only summarize our findings on sample size here.",
                "The number of samples has some effect on precision when less than 10, but performance stabilizes at around 15 to 20 samples.",
                "We used 30 samples for our experiments.",
                "Much beyond this level, the additional benefits of more samples decrease as the initial score distribution is more closely fit and the processing time increases. 3.6 The effect of resampling on expansion term quality Ideally, a retrieval model should not require a stopword list when estimating a model of relevance: a robust statistical model should down-weight stopwords automatically depending on context.",
                "Stopwords can harm feedback if selected as feedback terms, because they are typically poor discriminators and waste valuable term slots.",
                "In practice, however, because most term selection methods resemble a tf · idf type of weighting, terms with low idf but very high tf can sometimes be selected as expansion term candidates.",
                "This happens, for example, even with the Relevance Model approach that is part of our baseline feedback.",
                "To ensure as strong a baseline as possible, we use a stoplist for all experiments reported here.",
                "If we turn off the stopword list, however, we obtain results such as those shown in Table 5 where four of the top ten baseline feedback terms for TREC topic 60 (said, but, their, not) are stopwords using the BaseFB method. (The top 100 expansion terms were selected to generate this example.)",
                "Indris method attempts to address the stopword problem by applying an initial step based on Ponte [14] to select less-common terms that have high log-odds of being in the top-ranked documents compared to the whole collection.",
                "Nevertheless, this does not overcome the stopword problem completely, especially as the number of feedback terms grows.",
                "Using resampling feedback, however, appears to mitigate Collection QV + Uniform QV + Relevance-score weighting weighting TREC 1&2 AvgP 0.2545 0.2406 (-5.46%) P10 0.5369 0.5263 (-1.97%) RI 0.6212 0.7087 (+14.09%) TREC 7 AvgP 0.2174 0.2169 (-0.23%) P10 0.4320 0.4480 (+3.70%) RI 0.4783 0.5652 (+18.17%) TREC 8 AvgP 0.2267 0.2268 (+0.04%) P10 0.4120 0.4340 (+5.34%) RI 0.4545 0.4545 (+0.00%) wt10g AvgP 0.1808 0.1946 (+7.63%) P10 0.2680 0.2960 (+10.45%) RI 0.0220 0.1099 (+399.5%) Table 4: Comparison of uniform and relevance-weighted document sampling.",
                "The percentage change compared to uniform sampling is shown in parentheses.",
                "QV indicates that query variants were used in both runs.",
                "Baseline FB p(wi|R) Resampling FB p(wi|R) said 0.055 court 0.026 court 0.055 pay 0.018 pay 0.034 federal 0.012 but 0.026 education 0.011 employees 0.024 teachers 0.010 their 0.024 employees 0.010 not 0.023 case 0.010 federal 0.021 their 0.009 workers 0.020 appeals 0.008 education 0.020 union 0.007 Table 5: Feedback term quality when a stoplist is not used.",
                "Feedback terms for TREC topic 60: merit pay vs seniority. the effect of stopwords automatically.",
                "In the example of Table 5, resampling feedback leaves only one stopword (their) in the top ten.",
                "We observed similar feedback term behavior across many other topics.",
                "The reason for this effect appears to be the interaction of the term selection score with the top-m term cutoff.",
                "While the presence and even proportion of particular stopwords is fairly stable across different document samples, their relative position in the top-m list is not, as sets of documents with varying numbers of better, lower-frequency term candidates are examined for each sample.",
                "As a result, while some number of stopwords may appear in each sampled document set, any given stopword tends to fall below the cutoff for multiple samples, leading to its classification as a high-variance, low-weight feature. 4.",
                "RELATED WORK Our approach is related to previous work from several areas of information retrieval and machine learning.",
                "Our use of query variation was inspired by the work of YomTov et al. [20], Carpineto et al. [5], and Amati et al. [2], among others.",
                "These studies use the idea of creating multiple subqueries and then examining the nature of the overlap in the documents and/or expansion terms that result from each subquery.",
                "Model combination is performed using heuristics.",
                "In particular, the studies of Amati et al. and Carpineto et al. investigated combining terms from individual distributional methods using a term-reranking combination heuristic.",
                "In a set of TREC topics they found wide average variation in the rank-distance of terms from different expansion methods.",
                "Their combination method gave modest positive improvements in average precision.",
                "The idea of examining the overlap between lists of suggested terms has also been used in early query expansion approaches.",
                "Xu and Crofts method of Local Context Analysis (LCA) [19] includes a factor in the empirically-derived weighting formula that causes expansion terms to be preferred that have connections to multiple query terms.",
                "On the document side, recent work by Zhou & Croft [21] explored the idea of adding noise to documents, re-scoring them, and using the stability of the resulting rankings as an estimate of query difficulty.",
                "This is related to our use of document sampling to estimate the risk of the feedback model built from the different sets of top-retrieved documents.",
                "Sakai et al. [17] proposed an approach to improving the robustness of pseudo-relevance feedback using a method they call selective sampling.",
                "The essence of their method is that they allow skipping of some top-ranked documents, based on a clustering criterion, in order to select a more varied and novel set of documents later in the ranking for use by a traditional pseudo-feedback method.",
                "Their study did not find significant improvements in either robustness (RI) or MAP on their corpora.",
                "Greiff, Morgan and Ponte [8] explored the role of variance in term weighting.",
                "In a series of simulations that simplified the problem to 2-feature documents, they found that average precision degrades as term frequency variance - high noiseincreases.",
                "Downweighting terms with high variance resulted in improved average precision.",
                "This seems in accord with our own findings for individual feedback models.",
                "Estimates of output variance have recently been used for improved text classification.",
                "Lee et al. [11] used queryspecific variance estimates of classifier outputs to perform improved model combination.",
                "Instead of using sampling, they were able to derive closed-form expressions for classifier variance by assuming base classifiers using simple types of inference networks.",
                "Ando and Zhang proposed a method that they call structural feedback [3] and showed how to apply it to query expansion for the TREC Genomics Track.",
                "They used r query variations to obtain R different sets Sr of top-ranked documents that have been intersected with the top-ranked documents obtained from the original query qorig.",
                "For each Si, the normalized centroid vector ˆwi of the documents is calculated.",
                "Principal component analysis (PCA) is then applied to the ˆwi to obtain the matrix Φ of H left singular vectors φh that are used to obtain the new, expanded query qexp = qorig + ΦT Φqorig. (7) In the case H = 1, we have a single left singular vector φ: qexp = qorig + (φT qorig)φ so that the dot product φT qorig is a type of dynamic weight on the expanded query that is based on the similarity of the original query to the expanded query.",
                "The use of variance as a feedback model quality measure occurs indirectly through the application of PCA.",
                "It would be interesting to study the connections between this approach and our own modelfitting method.",
                "Finally, in language modeling approaches to feedback, Tao and Zhai [18] describe a method for more robust feedback that allows each document to have a different feedback α.",
                "The feedback weights are derived automatically using regularized EM.",
                "A roughly equal balance of query and expansion model is implied by their EM stopping condition.",
                "They propose tailoring the stopping parameter η based on a function of some quality measure of feedback documents. 5.",
                "CONCLUSIONS We have presented a new approach to pseudo-relevance feedback based on document and query sampling.",
                "The use of sampling is a very flexible and powerful device and is motivated by our general desire to extend current models of retrieval by estimating the risk or variance associated with the parameters or output of retrieval processes.",
                "Such variance estimates, for example, may be naturally used in a Bayesian framework for improved model estimation and combination.",
                "Applications such as selective expansion may then be implemented in a principled way.",
                "While our study uses the language modeling approach as a framework for experiments, we make few assumptions about the actual workings of the feedback algorithm.",
                "We believe it is likely that any reasonably effective baseline feedback algorithm would benefit from our approach.",
                "Our results on standard TREC collections show that our framework improves the robustness of a strong baseline feedback method across a variety of collections, without sacrificing average precision.",
                "It also gives small but consistent gains in top10 precision.",
                "In future work, we envision an investigation into how varying the set of sampling methods used and the number of samples controls the trade-off between robustness, accuracy, and efficiency.",
                "Acknowledgements We thank Paul Bennett for valuable discussions related to this work, which was supported by NSF grants #IIS-0534345 and #CNS-0454018, and U.S. Dept. of Education grant #R305G03123.",
                "Any opinions, findings, and conclusions or recommendations expressed in this material are the authors. and do not necessarily reflect those of the sponsors. 6.",
                "REFERENCES [1] The Lemur toolkit for language modeling and retrieval. http://www.lemurproject.org. [2] G. Amati, C. Carpineto, and G. Romano.",
                "Query difficulty, robustness, and selective application of query expansion.",
                "In Proc. of the 25th European Conf. on Information Retrieval (ECIR 2004), pages 127-137. [3] R. K. Ando and T. Zhang.",
                "A high-performance semi-supervised learning method for text chunking.",
                "In Proc. of the 43rd Annual Meeting of the ACL, pages 1-9, June 2005. [4] L. Breiman.",
                "Bagging predictors.",
                "Machine Learning, 24(2):123-140, 1996. [5] C. Carpineto, G. Romano, and V. Giannini.",
                "Improving retrieval feedback with multiple term-ranking function combination.",
                "ACM Trans.",
                "Info.",
                "Systems, 20(3):259 - 290. [6] K. Collins-Thompson, P. Ogilvie, and J. Callan.",
                "Initial results with structured queries and language models on half a terabyte of text.",
                "In Proc. of 2005 Text REtrieval Conference.",
                "NIST Special Publication. [7] R. O. Duda, P. E. Hart, and D. G. Stork.",
                "Pattern Classification.",
                "Wiley and Sons, 2nd edition, 2001. [8] W. R. Greiff, W. T. Morgan, and J. M. Ponte.",
                "The role of variance in term weighting for probabilistic information retrieval.",
                "In Proc. of the 11th Intl.",
                "Conf. on Info. and Knowledge Mgmt. (CIKM 2002), pages 252-259. [9] T. Kohonen, J. Hynninen, J. Kangas, and J. Laaksonen.",
                "SOMPAK: The self-organizing map program package.",
                "Technical Report A31, Helsinki University of Technology, 1996. http://www.cis.hut.fi/research/papers/som tr96.ps.Z. [10] V. Lavrenko.",
                "A Generative Theory of Relevance.",
                "PhD thesis, University of Massachusetts, Amherst, 2004. [11] C.-H. Lee, R. Greiner, and S. Wang.",
                "Using query-specific variance estimates to combine Bayesian classifiers.",
                "In Proc. of the 23rd Intl.",
                "Conf. on Machine Learning (ICML 2006), pages 529-536. [12] D. Metzler and W. B. Croft.",
                "Combining the language model and inference network approaches to retrieval.",
                "Info.",
                "Processing and Mgmt., 40(5):735-750, 2004. [13] T. Minka.",
                "Estimating a Dirichlet distribution.",
                "Technical report, 2000. http://research.microsoft.com/ minka/papers/dirichlet. [14] J. Ponte.",
                "Advances in Information Retrieval, chapter Language models for relevance feedback, pages 73-96. 2000.",
                "W.B.",
                "Croft, ed. [15] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In Proc. of the 1998 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 275-281. [16] J. Rocchio.",
                "The SMART Retrieval System, chapter Relevance Feedback in Information Retrieval, pages 313-323.",
                "Prentice-Hall, 1971.",
                "G. Salton, ed. [17] T. Sakai, T. Manabe, and M. Koyama.",
                "Flexible pseudo-relevance feedback via selective sampling.",
                "ACM Transactions on Asian Language Information Processing (TALIP), 4(2):111-135, 2005. [18] T. Tao and C. Zhai.",
                "Regularized estimation of mixture models for robust pseudo-relevance feedback.",
                "In Proc. of the 2006 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 162-169. [19] J. Xu and W. B. Croft.",
                "Improving the effectiveness of information retrieval with local context analysis.",
                "ACM Trans.",
                "Inf.",
                "Syst., 18(1):79-112, 2000. [20] E. YomTov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty.",
                "In Proc. of the 2005 ACM SIGIR Conf. on Research and Development in Information Retrieval, pages 512-519. [21] Y. Zhou and W. B. Croft.",
                "Ranking robustness: a novel framework to predict query performance.",
                "In Proc. of the 15th ACM Intl.",
                "Conf. on Information and Knowledge Mgmt. (CIKM 2006), pages 567-574."
            ],
            "original_annotated_samples": [
                "Therefore, we propose a method for <br>estimating uncertainty</br> associated with an individual feedback model in terms of a posterior distribution over language models."
            ],
            "translated_annotated_samples": [
                "Por lo tanto, proponemos un método para estimar la <br>incertidumbre</br> asociada con un modelo de retroalimentación individual en términos de una distribución posterior sobre modelos de lenguaje."
            ],
            "translated_text": "Estimación y Uso de la Incertidumbre en la Retroalimentación de Pseudo-Relevancia Kevyn Collins-Thompson y Jamie Callan Instituto de Tecnologías del Lenguaje Escuela de Ciencias de la Computación Universidad Carnegie Mellon Pittsburgh, PA 15213-8213 EE. UU. {kct | callan}@cs.cmu.edu RESUMEN Los métodos existentes de retroalimentación de pseudo-relevancia suelen realizar un promedio de los documentos mejor clasificados, pero ignoran una dimensión estadística importante: el riesgo o la varianza asociada tanto a los modelos de documentos individuales como a su combinación. Tratando el método de retroalimentación de línea base como una caja negra, y el modelo de retroalimentación de salida como una variable aleatoria, estimamos una distribución posterior para el modelo de retroalimentación mediante el remuestreo de los documentos mejor recuperados de una consulta dada, utilizando la media o la moda posterior como el modelo de retroalimentación mejorado. Luego realizamos la combinación de modelos sobre varios modelos mejorados, cada uno basado en una consulta ligeramente modificada muestreada de la consulta original. Observamos que el remuestreo de documentos ayuda a aumentar la precisión del modelo de retroalimentación individual al eliminar términos de ruido, mientras que el muestreo de la consulta mejora la robustez (rendimiento en el peor de los casos) al enfatizar términos relacionados con múltiples aspectos de la consulta. El resultado es un algoritmo de meta-retroalimentación que es tanto más robusto como más preciso que el método de referencia original. Categorías y Descriptores de Asignaturas: H.3.3 [Recuperación de Información]: Modelos de Recuperación Términos Generales: Algoritmos, Experimentación 1. INTRODUCCIÓN La incertidumbre es una característica inherente de la recuperación de información. No solo no sabemos las consultas que se presentarán a nuestro algoritmo de recuperación con anticipación, sino que la necesidad de información de los usuarios puede ser vaga o estar incompletamente especificada por estas consultas. Incluso si la consulta estuviera perfectamente especificada, el lenguaje en los documentos de la colección es inherentemente complejo y ambiguo, y hacer coincidir dicho lenguaje de manera efectiva es un problema formidable por sí mismo. Con esto en mente, deseamos tratar muchas cantidades importantes calculadas por el sistema de recuperación, ya sea un puntaje de relevancia para un documento, o un peso para un término de expansión de consulta, como variables aleatorias cuyo valor real es incierto pero donde la incertidumbre sobre el valor real puede ser cuantificada reemplazando el valor fijo con una distribución de probabilidad sobre posibles valores. De esta manera, los algoritmos de recuperación pueden intentar cuantificar el riesgo o la incertidumbre asociados con sus clasificaciones de salida, o mejorar la estabilidad o precisión de sus cálculos internos. Los algoritmos actuales para la retroalimentación de pseudo relevancia (PRF) tienden a seguir el mismo método básico ya sea que utilicemos algoritmos basados en espacio vectorial como la fórmula de Rocchio [16], o enfoques más recientes de modelado de lenguaje como Modelos de Relevancia [10]. Primero, se obtiene un conjunto de documentos recuperados en la parte superior a partir de una consulta inicial y se asume que aproxima un conjunto de documentos relevantes. A continuación, se calcula un vector de modelo de retroalimentación único según algún tipo de promedio, centroide o expectativa sobre el conjunto de modelos de documentos posiblemente relevantes. Por ejemplo, los vectores de documentos pueden combinarse con igual ponderación, como en Rocchio, o por probabilidad de consulta, como se puede hacer utilizando el Modelo de Relevancia. El uso de una expectativa es razonable por razones prácticas y teóricas, pero por sí sola ignora información potencialmente valiosa sobre el riesgo del modelo de retroalimentación. Nuestra hipótesis principal en este artículo es que estimar la incertidumbre en la retroalimentación es útil y conduce a modelos de retroalimentación individual más efectivos y a modelos combinados más robustos. Por lo tanto, proponemos un método para estimar la <br>incertidumbre</br> asociada con un modelo de retroalimentación individual en términos de una distribución posterior sobre modelos de lenguaje. Para hacer esto, variamos sistemáticamente las entradas al método de retroalimentación base y ajustamos una distribución de Dirichlet a la salida. Utilizamos la media posterior o el modo como la estimación mejorada del modelo de retroalimentación. Este proceso se muestra en la Figura 1. Como mostraremos más adelante, la media y la moda pueden variar significativamente del modelo de retroalimentación única propuesto por el método de referencia. También realizamos la combinación de modelos utilizando varios modelos de lenguaje de retroalimentación mejorados obtenidos a partir de un pequeño número de nuevas consultas muestreadas de la consulta original. El peso de un modelo combina dos factores complementarios: la probabilidad del modelo de generar la consulta y la varianza del modelo, siendo los modelos con alta varianza los que obtienen un peso menor. Por ejemplo, un vector de parámetros esperado condicionado a la observación de la consulta se forma a partir de los documentos recuperados en la parte superior, que se tratan como cadenas de entrenamiento (ver [10], p. 62). Figura 1: Estimación de la incertidumbre del modelo de retroalimentación para una sola consulta. 2. En las Secciones 2.1-2.5 describimos un método general para estimar una distribución de probabilidad sobre el conjunto de posibles modelos de lenguaje basado en muestreo. En las Secciones 2.6 y 2.7 resumimos cómo se utilizan diferentes muestras de consultas para generar múltiples modelos de retroalimentación, que luego se combinan. 2.1 Modelado de la Incertidumbre de la Retroalimentación Dado una consulta Q y una colección C, asumimos un sistema de recuperación probabilístico que asigna un puntaje de documento de valor real f(D, Q) a cada documento D en C, de manera que el puntaje sea proporcional a la probabilidad estimada de relevancia. No hacemos ninguna otra suposición sobre f(D, Q). La naturaleza de f(D, Q) puede ser compleja: por ejemplo, si el sistema de recuperación admite lenguajes de consulta estructurados [12], entonces f(D, Q) puede representar la salida de una red de inferencia arbitrariamente compleja definida por los operadores de consulta estructurados. En teoría, la función de puntuación puede variar de una consulta a otra, aunque en este estudio por simplicidad mantenemos la misma función de puntuación para todas las consultas. Nuestro método de consulta específico se encuentra en la Sección 3. Tratamos el algoritmo de retroalimentación como una caja negra y asumimos que los insumos del algoritmo de retroalimentación son la consulta original y los documentos recuperados principales correspondientes, a los cuales se les asigna una puntuación a cada documento. Suponemos que la salida del algoritmo de retroalimentación es un vector de pesos de términos que se utilizarán para agregar o reponderar los términos en la representación de la consulta original, con el vector normalizado para formar una distribución de probabilidad. Consideramos las entradas a la caja negra de retroalimentación como variables aleatorias, y analizamos el modelo de retroalimentación como una variable aleatoria que cambia en respuesta a cambios en las entradas. Al igual que la función de puntuación del documento f(D, Q), el algoritmo de retroalimentación puede implementar una fórmula de puntuación compleja y no lineal, por lo que, a medida que sus entradas varían, los modelos de retroalimentación resultantes pueden tener una distribución compleja en el espacio de modelos de retroalimentación (el espacio muestral). Debido a esta complejidad potencial, no intentamos derivar una distribución posterior de forma cerrada, sino que en su lugar utilizamos simulación. Llamamos a esta distribución sobre posibles modelos de retroalimentación la distribución de modelos de retroalimentación. Nuestro objetivo en esta sección es estimar una aproximación útil a la distribución del modelo de retroalimentación. Para un marco específico de experimentos, utilizamos el enfoque de modelado de lenguaje (LM) para la recuperación de información [15]. El puntaje de un documento D con respecto a una consulta Q y una colección C se da por p(Q|D) con respecto a los modelos de lenguaje ˆθQ y ˆθD estimados para la consulta y el documento respectivamente. Denotamos el conjunto de los k documentos mejor recuperados de la colección C en respuesta a Q como DQ(k, C). Para simplificar, asumimos que las consultas y documentos son generados por distribuciones multinomiales cuyos parámetros están representados por modelos de lenguaje unigram. Para incorporar retroalimentación en el enfoque de LM, asumimos un esquema basado en modelos en el que nuestro objetivo es tomar la consulta y los documentos clasificados resultantes DQ(k, C) como entrada, y producir un modelo de lenguaje expandido ˆθE, que luego se interpola con el modelo de consulta original ˆθQ: ˆθNew = (1 − α) · ˆθQ + α · ˆθE (1). Esto incluye la posibilidad de α = 1 donde el modelo de consulta original es completamente reemplazado por el modelo de retroalimentación. Nuestro espacio muestral es el conjunto de todos los posibles modelos de lenguaje LF que pueden ser generados como modelos de retroalimentación. Nuestro enfoque es tomar muestras de este espacio y luego ajustar una distribución a las muestras utilizando máxima verosimilitud. Para simplificar, comenzamos asumiendo que la distribución de retroalimentación latente tiene la forma de una distribución de Dirichlet. Aunque la distribución de Dirichlet es unimodal y en general bastante limitada en su expresividad en el espacio muestral, es una coincidencia natural para el modelo de lenguaje multinomial, puede ser estimada rápidamente y capturar las características más relevantes de los modelos de retroalimentación confiada e incierta, como la dispersión general de la distribución. 2.2 Re-muestreo de modelos de documentos. Nos gustaría una aproximación a la distribución posterior del modelo de retroalimentación LF. Para lograr esto, aplicamos una técnica de simulación ampliamente utilizada llamada muestreo bootstrap ([7], p. 474) en los parámetros de entrada, es decir, el conjunto de documentos recuperados en la parte superior. El muestreo de arranque nos permite simular el efecto aproximado de perturbar los parámetros dentro del algoritmo de retroalimentación de la caja negra al perturbar los insumos de ese algoritmo de manera sistemática, sin hacer suposiciones sobre la naturaleza del algoritmo de retroalimentación. Específicamente, muestreamos k documentos con reemplazo de DQ(k, C), y calculamos un modelo de lenguaje de expansión θb utilizando el método de retroalimentación de caja negra. Repetimos este proceso B veces para obtener un conjunto de B modelos de lenguaje de retroalimentación, a los cuales luego ajustamos una distribución de Dirichlet. Normalmente, B se encuentra en el rango de 20 a 50 muestras, con un rendimiento relativamente estable en este rango. Ten en cuenta que en lugar de tratar cada documento superior como igualmente probable, muestreamos de acuerdo con las probabilidades estimadas de relevancia de cada documento en DQ(k, C). Por lo tanto, es más probable que un documento sea elegido cuanto más alto esté en la clasificación. 2.3 Justificación de un enfoque de muestreo La justificación de nuestro enfoque de muestreo tiene dos partes. Primero, queremos mejorar la calidad de los modelos de retroalimentación individual al suavizar la variación cuando el modelo de retroalimentación base es inestable. En este sentido, nuestro enfoque se asemeja al bagging [4], un enfoque de conjunto que genera múltiples versiones de un predictor haciendo copias de arranque del conjunto de entrenamiento, y luego promedia los predictores (numéricos). En nuestra aplicación, los documentos recuperados en la parte superior pueden ser vistos como una especie de conjunto de entrenamiento ruidoso para la relevancia. Segundo, el muestreo es una forma efectiva de estimar propiedades básicas de la distribución posterior de retroalimentación, las cuales pueden ser utilizadas para mejorar la combinación de modelos. Por ejemplo, un modelo puede ser ponderado por su confianza de predicción, estimada como una función de la variabilidad del posterior alrededor del modelo. foo2-401.map-Dim:5434,Size:12*12units, vecindario gaussiano (a) Tema 401 Minorías extranjeras, Alemania foo2-402.map-Dim:5698,Size:12*12units, vecindario gaussiano (b) Tema 402 Genética del comportamiento foo2-459.map-Dim:8969,Size:12*12units, vecindario gaussiano (c) Tema 459 ¿Cuándo puede un prestamista ejecutar una hipoteca sobre una propiedad? Figura 2: Visualización de la varianza del modelo de lenguaje de expansión utilizando mapas autoorganizados, mostrando la distribución de modelos de lenguaje que resulta de remuestrear las entradas al método de expansión de referencia. El modelo de lenguaje que habría sido elegido por la expansión de la línea base se encuentra en el centro de cada mapa. La función de similitud es la divergencia JensenShannon. 2.4 Visualización de distribuciones de retroalimentación Antes de describir cómo ajustamos y utilizamos la distribución de Dirichlet sobre modelos de retroalimentación, es instructivo ver algunos ejemplos de distribuciones de modelos de retroalimentación reales que resultan de muestrear por bootstrap los documentos más recuperados de diferentes temas de TREC. Cada punto en nuestro espacio muestral es un modelo de lenguaje, que típicamente tiene varias miles de dimensiones. Para ayudar a analizar el comportamiento de nuestro método, utilizamos un Mapa Auto-organizado (a través del paquete SOM-PAK [9]), para aplanar y visualizar la función de densidad de alta dimensión. Los mapas de densidad para tres temas de TREC se muestran en la Figura 2 arriba. Las áreas oscuras representan regiones de alta similitud entre los modelos de lenguaje. Las áreas claras representan regiones de baja similitud, los valles entre los grupos. Cada diagrama está centrado en el modelo de lenguaje que habría sido elegido por la expansión de referencia. Un solo pico (modo) es evidente en algunos ejemplos, pero en otros aparece una estructura más compleja. Además, aunque la distribución suele estar cerca del modelo de retroalimentación de referencia, para algunos temas están a una distancia significativa (según la divergencia de JensenShannon), como se muestra en la Subfigura 2c. En tales casos, la moda o la media de la distribución de retroalimentación a menudo funcionan significativamente mejor que el valor base (y en una proporción menor de casos, significativamente peor). 2.5 Ajuste de una distribución de retroalimentación posterior Después de obtener muestras del modelo de retroalimentación mediante el remuestreo de las entradas del modelo de retroalimentación, estimamos la distribución de retroalimentación. Suponemos que los modelos de retroalimentación multinomial {ˆθ1, . . . , ˆθB} fueron generados por una distribución de Dirichlet latente con parámetros {α1, . . . , αN}. Para estimar los {α1, . . . , αN }, ajustamos los parámetros de Dirichlet a las muestras del modelo de lenguaje B según la máxima verosimilitud utilizando un procedimiento de Newton generalizado, cuyos detalles se encuentran en Minka [13]. Suponemos una distribución de Dirichlet simple sobre los {α1, . . . , αN}, estableciendo cada uno como αi = μ · p(wi | C), donde μ es un parámetro y p(· | C) es el modelo de lenguaje de colección estimado a partir de un conjunto de documentos de la colección C. El ajuste de parámetros converge muy rápidamente, generalmente en solo 2 o 3 iteraciones. Dado que nuestros puntos son modelos de lenguaje en el simplex multinomial, extendimos SOM-PAK para admitir la divergencia de Jensen-Shannon, una medida de similitud ampliamente utilizada entre distribuciones de probabilidad. 3 iteraciones son suficientes, por lo que es práctico aplicarlo en tiempo de consulta cuando la sobrecarga computacional debe ser pequeña. En la práctica, podemos restringir el cálculo al vocabulario de los documentos recuperados en la parte superior, en lugar de toda la colección. Ten en cuenta que para este paso estamos reutilizando los documentos recuperados existentes y no realizando consultas adicionales. Dado los parámetros de una distribución de Dirichlet N-dimensional Dir(α), los vectores de media μ y moda x son fáciles de calcular y se dan respectivamente por μi = αiP αi (2) y xi = αi−1P αi−N . (3) Luego podemos elegir el modelo de lenguaje en la media o en la moda del posterior como el modelo de retroalimentación mejorado final. (Encontramos que la moda proporciona un rendimiento ligeramente mejor). Para la recuperación de información, es probable que el número de muestras disponibles sea bastante pequeño por razones de rendimiento, generalmente menos de diez. Además, si bien el muestreo aleatorio es útil en ciertos casos, es perfectamente aceptable permitir distribuciones de muestreo determinísticas, pero estas deben diseñarse cuidadosamente para aproximar una varianza de salida precisa. Dejamos esto para estudios futuros. 2.6 Variantes de consulta Utilizamos los siguientes métodos para generar variantes de la consulta original. Cada variante corresponde a una suposición diferente sobre qué aspectos de la consulta original pueden ser importantes. Esta es una forma de muestreo determinístico. Seleccionamos tres métodos simples que cubren suposiciones complementarias sobre la consulta. Uso sin expansión. La suposición es que los términos dados son una descripción completa de la necesidad de información. Deja uno fuera. Se deja fuera un término del query original. La suposición es que uno de los términos de la consulta es un término de ruido. Se elige un término único de la consulta original. Esto asume que solo un aspecto de la consulta, es decir, el representado por el término, es el más importante. Después de generar una variante de la consulta original, la combinamos con la consulta original utilizando un peso αSUB para no alejarnos demasiado. En este estudio, establecimos αSUB = 0.5. Por ejemplo, utilizando el lenguaje de consulta Indri [12], una variante de dejar uno fuera de la consulta inicial que omite el término \"ireland\" para el tema TREC 404 es: #weight(0.5 #combine(ireland peace talks) 0.5 #combine(peace talks)) 2.7 Combinando modelos de retroalimentación mejorados de múltiples variantes de consulta. Cuando se utilizan múltiples variantes de consulta, los modelos de retroalimentación mejorados resultantes se combinan utilizando la combinación de modelos bayesianos. Para hacer esto, tratamos cada palabra como un elemento a clasificar como perteneciente a una clase relevante o no relevante, y derivamos una probabilidad de clase para cada palabra combinando las puntuaciones de cada variante de consulta. Cada puntuación es dada por la probabilidad de ese término en la distribución de Dirichlet. Las puntuaciones de los términos se ponderan por el inverso de la varianza del término en los modelos de retroalimentación mejorada de la distribución de Dirichlet. La probabilidad previa de pertenencia de una palabra a la clase relevante se da por la probabilidad de la consulta original en todo el modelo de expansión mejorado. 3. EVALUACIÓN En esta sección presentamos resultados que confirman la utilidad de estimar una distribución del modelo de retroalimentación a partir del remuestreo ponderado de documentos mejor clasificados, y de combinar los modelos de retroalimentación obtenidos a partir de diferentes pequeños cambios en la consulta original. 3.1 Método general Evaluamos el rendimiento en un total de 350 consultas derivadas de cuatro conjuntos de temas de TREC: 51-200 (TREC-1&2), 351-400 (TREC-7), 401-450 (TREC-8) y 451-550 (wt10g, TREC-9&10). Elegimos estos por su contenido variado y propiedades del documento. Por ejemplo, los documentos wt10g son páginas web con una amplia variedad de temas y estilos, mientras que los documentos TREC-1&2 son artículos de noticias más homogéneos. La indexación y recuperación se realizó utilizando el sistema Indri en el kit de herramientas Lemur [12] [1]. Nuestras consultas se derivaron de las palabras en el campo del título de los temas de TREC. Las frases no fueron utilizadas. Para generar las consultas base que se pasaron a Indri, envolvimos los términos de la consulta con el operador #combine de Indri. Por ejemplo, la consulta inicial para el tema 404 es: #combine(ireland peace talks) Realizamos el stemming de Krovetz para todos los experimentos. Debido a que encontramos que el método de expansión de la base (Indri) funcionó mejor utilizando una lista de palabras vacías con el modelo de retroalimentación, todos los experimentos utilizaron una lista de 419 palabras comunes en inglés. Sin embargo, un efecto secundario interesante de nuestro enfoque de remuestreo es que tiende a eliminar muchas palabras vacías del modelo de retroalimentación, lo que hace que una lista de paradas sea menos crítica. Esto se discute más a fondo en la Sección 3.6. Método de retroalimentación de línea base 3.2 Para nuestro método de expansión de línea base, utilizamos un algoritmo incluido en Indri 1.0 como el método de expansión predeterminado. Este método primero selecciona términos utilizando un cálculo de logaritmo de probabilidades descrito por Ponte [14], pero asigna pesos finales a los términos utilizando el modelo de relevancia de Lavrenko [10]. Elegimos el método Indri porque proporciona una línea base consistentemente sólida, se basa en un enfoque de modelado del lenguaje y es fácil de experimentar con él. En una evaluación de TREC utilizando el corpus GOV2, el método fue uno de los mejores resultados, logrando un aumento del 19.8% en el MAP en comparación con el uso de consultas no expandidas. En este estudio, se logra una ganancia promedio en MAP del 17.25% en las cuatro colecciones. El método de expansión de Indri primero calcula una razón de logaritmos de probabilidades o(v) para cada término de expansión potencial v dado por o(v) = X D log p(v|D) p(v|C) (4) sobre todos los documentos D que contienen v, en la colección C. Luego, los candidatos a términos de expansión se ordenan por o(v) descendente, y se eligen los primeros m. Finalmente, los pesos de término r(v) utilizados en la consulta ampliada se calculan en base al modelo de relevancia r(v) = X D p(q|D)p(v|D) p(v) p(D) (5). La cantidad p(q|D) es la puntuación de probabilidad asignada al documento en la recuperación inicial. Utilizamos suavizado de Dirichlet de p(v|D) con μ = 1000. Este modelo de relevancia se combina luego con la consulta original utilizando interpolación lineal, ponderada por un parámetro α. Por defecto, utilizamos los 50 documentos principales para la retroalimentación y los 20 términos de expansión, con el parámetro de interpolación de retroalimentación α = 0.5 a menos que se indique lo contrario. Por ejemplo, la consulta expandida base para el tema 404 es: #weight(0.5 #combine(ireland peace talks) 0.5 #weight(0.10 ireland 0.08 peace 0.08 northern ...) 3.3 Rendimiento de la expansión. Medimos la efectividad de nuestros algoritmos de retroalimentación mediante dos criterios principales: precisión y robustez. La robustez, y el equilibrio entre precisión y robustez, se analiza en la Sección 3.4. En esta sección, examinamos la precisión promedio y la precisión en los 10 documentos principales (P10). También incluimos la recuperación de 1,000 documentos. Para cada consulta, obtuvimos un conjunto de B modelos de retroalimentación utilizando la línea base de Indri. Cada modelo de retroalimentación se obtuvo a partir de una muestra aleatoria de los k documentos principales tomados con reemplazo. Para estos experimentos, B = 30 y k = 50. Cada modelo de retroalimentación contenía 20 términos. En el lado de la consulta, utilizamos muestreo de dejar uno fuera (LOO) para crear las variantes de consulta. El muestreo de consulta de un solo término tuvo un rendimiento consistentemente peor en todas las colecciones, por lo que nuestros resultados se centran en el muestreo de LOO aquí. Utilizamos los métodos descritos en la Sección 2 para estimar un modelo de retroalimentación mejorado a partir de la distribución posterior de Dirichlet para cada variante de consulta, y para combinar los modelos de retroalimentación de todas las variantes de consulta. Llamamos a nuestro método expansión de remuestreo y lo denotamos como RS-FB aquí. Denominamos al método de retroalimentación de línea base Indri como Base-FB. Los resultados de aplicar tanto el método de expansión de línea base (Base-FB) como la expansión de remuestreo (RS-FB) se muestran en la Tabla 1. Observamos varias tendencias en esta tabla. Primero, la precisión promedio de RS-FB fue comparable a Base-FB, logrando una ganancia promedio del 17.6% en comparación con no utilizar expansión en las cuatro colecciones. La ganancia de expansión de la línea base de Indri fue del 17.25%. Además, el método RS-FB logró mejoras consistentes en P10 sobre Base-FB para cada conjunto de temas, con una mejora promedio del 6.89% sobre Base-FB para los 350 temas. La ganancia P10 más baja sobre Base-FB fue del +3.82% para TREC-7 y la más alta fue del +11.95% para wt10g. Finalmente, tanto Base-FB como RS-FB también mejoraron consistentemente la recuperación en comparación con no utilizar expansión, siendo Base-FB el que logró una mejor recuperación que RS-FB para todos los conjuntos de temas. 3.4 Robustez de recuperación. Utilizamos el término robustez para referirnos al rendimiento de precisión promedio en el peor de los casos de un algoritmo de retroalimentación. Idealmente, un método de retroalimentación sólido nunca debería funcionar peor que usar la consulta original, mientras que a menudo funciona mejor utilizando la expansión. Para evaluar la robustez en este estudio, utilizamos una medida muy simple llamada índice de robustez (RI). Para un conjunto de consultas Q, la medida de RI se define como: RI(Q) = n+ − n− |Q| (6) donde n+ es el número de consultas ayudadas por el método de retroalimentación y n− es el número de consultas perjudicadas. Aquí, por \"ayudar\" nos referimos a obtener una precisión promedio más alta como resultado de la retroalimentación. El valor de RI varía desde un mínimo de 3. A veces también se le llama índice de confiabilidad de mejora y fue utilizado en Sakai et al. [17]. Colección NoExp Base-FB RS-FB TREC 1&2 AvgP 0.1818 0.2419 (+33.04%) 0.2406 (+32.24%) P10 0.4443 0.4913 (+10.57%) 0.5363 (+17.83%) Recall 15084/37393 19172/37393 15396/37393 TREC 7 AvgP 0.1890 0.2175 (+15.07%) 0.2169 (+14.75%) P10 0.4200 0.4320 (+2.85%) 0.4480 (+6.67%) Recall 2179/4674 2608/4674 2487/4674 TREC 8 AvgP 0.2031 0.2361 (+16.25%) 0.2268 (+11.70%) P10 0.3960 0.4160 (+5.05%) 0.4340 (+9.59%) Recall 2144/4728 2642/4728 2485/4728 wt10g AvgP 0.1741 0.1829 (+5.06%) 0.1946 (+11.78%) P10 0.2760 0.2630 (-4.71%) 0.2960 (+7.24%) Recall 3361/5980 3725/5980 3664/5980 Tabla 1: Comparación de la retroalimentación base (Base-FB) y la retroalimentación utilizando re-muestreo (RS-FB). La mejora mostrada para BaseFB y RS-FB es relativa al uso de ninguna expansión. (a) TREC 1&2 (curva superior); TREC 8 (curva inferior) (b) TREC 7 (curva superior); wt10g (curva inferior) Figura 3: La compensación entre robustez y precisión promedio para diferentes corpora. El eje x muestra el cambio en la MAP al utilizar la expansión de la línea base con α = 0.5. El eje y muestra el Índice de Robustez (RI). Cada curva a través de los puntos no circulados muestra el compromiso entre RI/MAP utilizando la estrategia simple de pequeño α (ver texto) a medida que α disminuye de 0.5 a cero en la dirección de la flecha. Los puntos circulados representan los compromisos obtenidos al volver a muestrear la retroalimentación para α = 0.5. Colección N Base-FB RS-FB n− RI n− RI TREC 1&2 103 26 +0.495 15 +0.709 TREC 7 46 14 +0.391 10 +0.565 TREC 8 44 12 +0.455 12 +0.455 wt10g 91 48 -0.055 39 +0.143 Combinado 284 100 +0.296 76 +0.465 Tabla 2: Comparación del índice de robustez (RI) para retroalimentación de línea base (Base-FB) vs. retroalimentación de remuestreo (RS-FB). También se muestran el número real de consultas perjudicadas por la retroalimentación (n−) para cada método y colección. Las consultas para las cuales la precisión promedio inicial era insignificante (≤ 0.01) fueron ignoradas, dando como resultado el recuento de consultas restantes en la columna N, de -1.0, cuando todas las consultas son perjudicadas por el método de retroalimentación, a +1.0 cuando todas las consultas son ayudadas. La medida de RI no tiene en cuenta la magnitud o distribución de la cantidad de cambio en el conjunto Q. Sin embargo, es fácil de entender como una indicación general de robustez. Una forma obvia de mejorar el rendimiento en el peor de los casos de la retroalimentación es simplemente utilizar un parámetro de interpolación α fijo más pequeño, como α = 0.3, otorgando menos peso al modelo de retroalimentación (posiblemente arriesgado) y más al cuestionario original. Llamamos a esto la estrategia de pequeña α. Dado que también estamos reduciendo las ganancias potenciales cuando el modelo de retroalimentación es correcto, sin embargo, esperaríamos algún tipo de compensación entre la precisión promedio y la robustez. Por lo tanto, comparamos el equilibrio entre precisión y robustez entre nuestro algoritmo de retroalimentación de remuestreo y el método simple de pequeño α. Los resultados se resumen en la Figura 3. En la figura, la curva para cada conjunto de temas interpola entre los puntos de compensación, comenzando en x=0, donde α = 0.5, y continuando en la dirección de la flecha a medida que α disminuye y la consulta original recibe cada vez más peso. Como era de esperar, la robustez aumenta continuamente a medida que avanzamos a lo largo de la curva, pero la precisión media promedio generalmente disminuye a medida que se eliminan las ganancias del feedback. Para la comparación, se muestra el rendimiento del re-muestreo con retroalimentación en α = 0.5 para cada colección como el punto marcado con un círculo. Más alto y a la derecha es mejor. Esta figura muestra que el re-muestreo de retroalimentación ofrece un compromiso algo mejor que el enfoque de pequeño α para 3 de las 4 colecciones. Figura 4: Histograma que muestra la mayor robustez del re-muestreo de retroalimentación (RS-FB) sobre la retroalimentación base (Base-FB) para todos los conjuntos de datos combinados. Las consultas se agrupan por el % de cambio en AP en comparación con la consulta no expandida. Colección DS + QV DS + No QV TREC 1&2 AvgP 0.2406 0.2547 (+5.86%) P10 0.5263 0.5362 (+1.88%) RI 0.7087 0.6515 (-0.0572) TREC 7 AvgP 0.2169 0.2200 (+1.43%) P10 0.4480 0.4300 (-4.02%) RI 0.5652 0.2609 (-0.3043) TREC 8 AvgP 0.2268 0.2257 (-0.49%) P10 0.4340 0.4200 (-3.23%) RI 0.4545 0.4091 (-0.0454) wt10g AvgP 0.1946 0.1865 (-4.16%) P10 0.2960 0.2680 (-9.46%) RI 0.1429 0.0220 (-0.1209) Tabla 3: Comparación de la retroalimentación de remuestreo utilizando muestreo de documentos (DS) con (QV) y sin (No QV) combinar modelos de retroalimentación de múltiples variantes de consulta. La Tabla 2 muestra los puntajes del Índice de Robustez para Base-FB y RS-FB. El método de retroalimentación RS-FB obtuvo una mayor robustez que Base-FB en tres de los cuatro conjuntos de temas, con solo un rendimiento ligeramente peor en TREC-8. Una vista más detallada que muestra la distribución de los cambios relativos en AP se presenta en el histograma de la Figura 4. En comparación con Base-FB, el método RS-FB logra una reducción notable en el número de consultas significativamente afectadas por la expansión (es decir, donde AP se ve afectado en un 25% o más), al tiempo que conserva ganancias positivas en AP. 3.5 Efecto de los métodos de muestreo de consultas y documentos Dada la mayor robustez de nuestros algoritmos observada en la Sección 3.4, una pregunta importante es qué componente de nuestro sistema es responsable. ¿Es el uso de la re-muestreo de documentos, el uso de múltiples variantes de consulta, o algún otro factor? Los resultados en la Tabla 3 sugieren que la combinación de modelos basada en variantes de consulta puede ser en gran medida responsable de la mayor robustez. Cuando se desactivan las variantes de consulta y se utiliza la consulta original por sí sola con muestreo de documentos, hay poco cambio neto en la precisión promedio, una pequeña disminución en P10 para 3 de los 4 conjuntos de temas, pero una caída significativa en la robustez para todos los conjuntos de temas. En dos casos, la medida de RI disminuye en más del 50%. También examinamos el efecto del método de muestreo de documentos en la efectividad de la recuperación, utilizando dos estrategias diferentes. La estrategia de ponderación uniforme ignoró las puntuaciones de relevancia de la recuperación inicial y otorgó a cada documento en los primeros k la misma probabilidad de selección. Por el contrario, la estrategia de ponderación de puntajes de relevancia seleccionó documentos con una probabilidad proporcional a sus puntajes de relevancia. De esta manera, los documentos que tenían una clasificación más alta tenían más probabilidades de ser seleccionados. Los resultados se muestran en la Tabla 4. La estrategia de ponderación de la puntuación de relevancia tiene un mejor rendimiento en general, con puntajes de RI y P10 significativamente más altos en 3 de los 4 conjuntos de temas. La diferencia en la precisión promedio entre los métodos, sin embargo, es menos marcada. Esto sugiere que el peso uniforme actúa para aumentar la varianza en los resultados de recuperación: cuando la precisión promedio inicial es alta, hay muchos documentos relevantes en los primeros k y el muestreo uniforme puede proporcionar un modelo de relevancia más representativo que centrarse en los elementos altamente clasificados. Por otro lado, cuando la precisión inicial es baja, hay pocos documentos relevantes en las posiciones inferiores y el muestreo uniforme mezcla más de los documentos no relevantes. Por razones de espacio, solo resumimos nuestros hallazgos sobre el tamaño de la muestra aquí. El número de muestras tiene cierto efecto en la precisión cuando es menor a 10, pero el rendimiento se estabiliza alrededor de 15 a 20 muestras. Utilizamos 30 muestras para nuestros experimentos. Mucho más allá de este nivel, los beneficios adicionales de más muestras disminuyen a medida que la distribución inicial de puntajes se ajusta más estrechamente y aumenta el tiempo de procesamiento. 3.6 El efecto del remuestreo en la calidad del término de expansión Idealmente, un modelo de recuperación no debería requerir una lista de palabras vacías al estimar un modelo de relevancia: un modelo estadístico robusto debería reducir automáticamente el peso de las palabras vacías dependiendo del contexto. Las palabras vacías pueden perjudicar la retroalimentación si se seleccionan como términos de retroalimentación, ya que suelen ser malos discriminadores y desperdician espacios de términos valiosos. En la práctica, sin embargo, debido a que la mayoría de los métodos de selección de términos se asemejan a un tipo de ponderación tf · idf, a veces se pueden seleccionar términos con un idf bajo pero un tf muy alto como candidatos de términos de expansión. Esto sucede, por ejemplo, incluso con el enfoque del Modelo de Relevancia que forma parte de nuestra retroalimentación base. Para garantizar una línea base lo más sólida posible, utilizamos una lista de paradas para todos los experimentos reportados aquí. Si desactivamos la lista de palabras vacías, sin embargo, obtenemos resultados como los mostrados en la Tabla 5 donde cuatro de los diez términos de retroalimentación básica principales para el tema 60 de TREC (dijo, pero, su, no) son palabras vacías utilizando el método BaseFB. (Se seleccionaron los 100 términos de expansión principales para generar este ejemplo). El método Indris intenta abordar el problema de las palabras vacías aplicando un paso inicial basado en Ponte [14] para seleccionar términos menos comunes que tienen altas probabilidades logarítmicas de estar en los documentos mejor clasificados en comparación con toda la colección. Sin embargo, esto no resuelve completamente el problema de las palabras vacías, especialmente a medida que aumenta el número de términos de retroalimentación. Sin embargo, el uso de retroalimentación de remuestreo parece mitigar la ponderación de la colección QV + QV uniforme + ponderación de puntaje de relevancia TREC 1&2 AvgP 0.2545 0.2406 (-5.46%) P10 0.5369 0.5263 (-1.97%) RI 0.6212 0.7087 (+14.09%) TREC 7 AvgP 0.2174 0.2169 (-0.23%) P10 0.4320 0.4480 (+3.70%) RI 0.4783 0.5652 (+18.17%) TREC 8 AvgP 0.2267 0.2268 (+0.04%) P10 0.4120 0.4340 (+5.34%) RI 0.4545 0.4545 (+0.00%) wt10g AvgP 0.1808 0.1946 (+7.63%) P10 0.2680 0.2960 (+10.45%) RI 0.0220 0.1099 (+399.5%) Tabla 4: Comparación de muestreo de documentos uniforme y ponderado por relevancia. El cambio porcentual en comparación con el muestreo uniforme se muestra entre paréntesis. QV indica que se utilizaron variantes de consulta en ambas ejecuciones. FB de línea base p(wi|R) FB de remuestreo p(wi|R) dijo 0.055 corte 0.026 corte 0.055 pagar 0.018 pagar 0.034 federal 0.012 pero 0.026 educación 0.011 empleados 0.024 maestros 0.010 sus 0.024 empleados 0.010 no 0.023 caso 0.010 federal 0.021 sus 0.009 trabajadores 0.020 apelaciones 0.008 educación 0.020 sindicato 0.007 Tabla 5: Calidad del término de retroalimentación cuando no se utiliza una lista de paradas. Términos de retroalimentación para el tema 60 de TREC: pago por mérito vs antigüedad. el efecto de las palabras vacías automáticamente. En el ejemplo de la Tabla 5, la retroalimentación de remuestreo deja solo una palabra vacía (su) en las diez primeras posiciones. Observamos un comportamiento similar del término de retroalimentación en muchos otros temas. La razón de este efecto parece ser la interacción de la puntuación de selección de términos con el límite superior de términos m. Si bien la presencia e incluso la proporción de palabras vacías particulares es bastante estable en diferentes muestras de documentos, su posición relativa en la lista de los primeros m elementos no lo es, ya que se examinan conjuntos de documentos con diferentes números de candidatos de términos mejores y de menor frecuencia para cada muestra. Como resultado, si bien algunos stopwords pueden aparecer en cada conjunto de documentos muestreados, tiende a ocurrir que cualquier stopword dado caiga por debajo del umbral para múltiples muestras, lo que lleva a su clasificación como una característica de alta varianza y bajo peso. 4. TRABAJO RELACIONADO Nuestro enfoque está relacionado con trabajos previos de varias áreas de recuperación de información y aprendizaje automático. Nuestro uso de la variación de consultas fue inspirado por el trabajo de YomTov et al. [20], Carpineto et al. [5] y Amati et al. [2], entre otros. Estos estudios utilizan la idea de crear múltiples subconsultas y luego examinar la naturaleza de la superposición en los documentos y/o términos de expansión que resultan de cada subconsulta. La combinación de modelos se realiza utilizando heurísticas. En particular, los estudios de Amati et al. y Carpineto et al. investigaron la combinación de términos de métodos de distribución individuales utilizando una heurística de combinación de reordenamiento de términos. En un conjunto de temas de TREC encontraron una amplia variación promedio en la distancia de rango de los términos de diferentes métodos de expansión. Su método de combinación proporcionó modestas mejoras positivas en la precisión promedio. La idea de examinar la superposición entre listas de términos sugeridos también ha sido utilizada en enfoques tempranos de expansión de consultas. El método de Análisis de Contexto Local (LCA) de Xu y Crofts [19] incluye un factor en la fórmula de ponderación derivada empíricamente que hace que se prefieran los términos de expansión que tienen conexiones con múltiples términos de consulta. En el lado del documento, el trabajo reciente de Zhou & Croft [21] exploró la idea de agregar ruido a los documentos, volver a puntuarlos y utilizar la estabilidad de las clasificaciones resultantes como una estimación de la dificultad de la consulta. Esto está relacionado con nuestro uso de muestreo de documentos para estimar el riesgo del modelo de retroalimentación construido a partir de los diferentes conjuntos de documentos recuperados en la parte superior. Sakai et al. [17] propusieron un enfoque para mejorar la robustez de la retroalimentación de relevancia pseudo utilizando un método que llaman muestreo selectivo. La esencia de su método es que permiten omitir algunos documentos de alta clasificación, basándose en un criterio de agrupación, para seleccionar un conjunto de documentos más variado e innovador más adelante en la clasificación para su uso por un método tradicional de retroalimentación pseudo. Su estudio no encontró mejoras significativas ni en robustez (RI) ni en MAP en sus corpora. Greiff, Morgan y Ponte [8] exploraron el papel de la varianza en la ponderación de términos. En una serie de simulaciones que simplificaron el problema a documentos de 2 características, encontraron que la precisión promedio disminuye a medida que la varianza de la frecuencia de términos - ruido alto - aumenta. La reducción del peso de los términos con alta varianza resultó en una mejora de la precisión promedio. Esto parece estar en concordancia con nuestros propios hallazgos para los modelos de retroalimentación individual. Las estimaciones de la varianza de la producción han sido utilizadas recientemente para mejorar la clasificación de textos. Lee et al. [11] utilizaron estimaciones de varianza específicas de la consulta de las salidas del clasificador para realizar una combinación de modelos mejorada. En lugar de utilizar muestreo, pudieron derivar expresiones en forma cerrada para la varianza del clasificador asumiendo clasificadores base utilizando tipos simples de redes de inferencia. Ando y Zhang propusieron un método que ellos llaman retroalimentación estructural [3] y mostraron cómo aplicarlo a la expansión de consultas para la pista de Genómica de TREC. Utilizaron variaciones de consultas r para obtener R conjuntos diferentes Sr de documentos mejor clasificados que se han intersectado con los documentos mejor clasificados obtenidos de la consulta original qorig. Para cada Si, se calcula el vector centróide normalizado ˆwi de los documentos. El análisis de componentes principales (PCA) se aplica luego a los ˆwi para obtener la matriz Φ de los vectores singulares izquierdos φh que se utilizan para obtener la nueva consulta expandida qexp = qorig + ΦT Φqorig. (7) En el caso de H = 1, tenemos un único vector singular izquierdo φ: qexp = qorig + (φT qorig)φ, de modo que el producto punto φT qorig es un tipo de peso dinámico en la consulta expandida que se basa en la similitud de la consulta original con la consulta expandida. El uso de la varianza como medida de calidad del modelo de retroalimentación ocurre de forma indirecta a través de la aplicación de PCA. Sería interesante estudiar las conexiones entre este enfoque y nuestro propio método de ajuste de modelos. Finalmente, en los enfoques de modelado del lenguaje para retroalimentación, Tao y Zhai [18] describen un método para una retroalimentación más robusta que permite que cada documento tenga un α de retroalimentación diferente. Los pesos de retroalimentación se derivan automáticamente utilizando EM regularizado. La condición de parada de EM implica un equilibrio aproximadamente igual entre el modelo de consulta y expansión. Proponen adaptar el parámetro de parada η basado en una función de alguna medida de calidad de los documentos de retroalimentación. CONCLUSIONES Hemos presentado un nuevo enfoque para la retroalimentación de relevancia pseudo basado en el muestreo de documentos y consultas. El uso del muestreo es un dispositivo muy flexible y poderoso, motivado por nuestro deseo general de ampliar los modelos actuales de recuperación mediante la estimación del riesgo o la varianza asociada con los parámetros o la salida de los procesos de recuperación. Tales estimaciones de varianza, por ejemplo, pueden ser utilizadas de forma natural en un marco bayesiano para una mejor estimación y combinación de modelos. Aplicaciones como la expansión selectiva pueden ser implementadas de manera fundamentada. Si bien nuestro estudio utiliza el enfoque de modelado del lenguaje como marco para experimentos, hacemos pocas suposiciones sobre el funcionamiento real del algoritmo de retroalimentación. Creemos que es probable que cualquier algoritmo de retroalimentación de línea base razonablemente efectivo se beneficiaría de nuestro enfoque. Nuestros resultados en colecciones estándar de TREC muestran que nuestro marco mejora la robustez de un método de retroalimentación de referencia sólido en una variedad de colecciones, sin sacrificar la precisión promedio. También proporciona pequeñas pero consistentes mejoras en la precisión del top 10. En trabajos futuros, visualizamos una investigación sobre cómo variar el conjunto de métodos de muestreo utilizados y el número de muestras controla el equilibrio entre robustez, precisión y eficiencia. Agradecimientos Agradecemos a Paul Bennett por las valiosas discusiones relacionadas con este trabajo, el cual fue apoyado por las becas de la NSF #IIS-0534345 y #CNS-0454018, y la beca del Departamento de Educación de los EE. UU. #R305G03123. Cualquier opinión, hallazgo, conclusión o recomendación expresada en este material son del autor y no necesariamente reflejan las de los patrocinadores. REFERENCIAS [1] El kit de herramientas Lemur para modelado de lenguaje y recuperación. http://www.lemurproject.org. [2] G. Amati, C. Carpineto y G. Romano. Dificultad de la consulta, robustez y aplicación selectiva de la expansión de consultas. En Actas de la 25ª Conferencia Europea sobre Recuperación de Información (ECIR 2004), páginas 127-137. [3] R. K. Ando y T. Zhang. Un método de aprendizaje semisupervisado de alto rendimiento para segmentación de texto. En Actas de la 43ª Reunión Anual de la ACL, páginas 1-9, junio de 2005. [4] L. Breiman. Empaquetando predictores. Aprendizaje automático, 24(2):123-140, 1996. [5] C. Carpineto, G. Romano y V. Giannini. Mejorando la retroalimentación de recuperación con la combinación de múltiples funciones de clasificación de términos. ACM Trans. Información. Sistemas, 20(3):259 - 290. [6] K. Collins-Thompson, P. Ogilvie y J. Callan. Resultados iniciales con consultas estructuradas y modelos de lenguaje en medio terabyte de texto. En Actas de la Conferencia de Recuperación de Información de Texto de 2005. Publicación Especial del NIST. [7] R. O. Duda, P. E. Hart y D. G. Stork. Clasificación de patrones. Wiley and Sons, 2da edición, 2001. [8] W. R. Greiff, W. T. Morgan y J. M. Ponte. El papel de la varianza en la ponderación de términos para la recuperación de información probabilística. En Actas de la 11ª Conferencia Internacional. Conf. sobre Gestión de Información y Conocimiento (CIKM 2002), páginas 252-259. [9] T. Kohonen, J. Hynninen, J. Kangas y J. Laaksonen. SOMPAK: El paquete de programas de mapas autoorganizados. Informe técnico A31, Universidad de Tecnología de Helsinki, 1996. http://www.cis.hut.fi/research/papers/som tr96.ps.Z. [10] V. Lavrenko. Una teoría generativa de relevancia. Tesis doctoral, Universidad de Massachusetts, Amherst, 2004. [11] C.-H. Lee, R. Greiner y S. Wang. Utilizando estimaciones de varianza específicas de la consulta para combinar clasificadores bayesianos. En Actas del 23º Congreso Internacional. Conf. sobre Aprendizaje Automático (ICML 2006), páginas 529-536. [12] D. Metzler y W. B. Croft. Combinando el modelo de lenguaje y los enfoques de red de inferencia para la recuperación. Información. Procesamiento y Gestión, 40(5):735-750, 2004. [13] T. Minka. Estimando una distribución de Dirichlet. Informe técnico, 2000. http://research.microsoft.com/minka/papers/dirichlet. [14] J. Ponte. Avances en la Recuperación de Información, capítulo Modelos de lenguaje para retroalimentación de relevancia, páginas 73-96. 2000. W.B. Croft, ed. [15] J. M. Ponte y W. B. Croft. Un enfoque de modelado del lenguaje para la recuperación de información. En Actas de la Conferencia ACM SIGIR de 1998 sobre Investigación y Desarrollo en Recuperación de Información, páginas 275-281. [16] J. Rocchio. El Sistema de Recuperación SMART, capítulo Retroalimentación de Relevancia en la Recuperación de Información, páginas 313-323. Prentice-Hall, 1971. G. Salton, ed. [17] T. Sakai, T. Manabe y M. Koyama. Retroalimentación de pseudo-relevancia flexible a través de muestreo selectivo. ACM Transactions on Asian Language Information Processing (TALIP), 4(2):111-135, 2005. [18] T. Tao y C. Zhai. Estimación regularizada de modelos de mezcla para retroalimentación de pseudo relevancia robusta. En Actas de la Conferencia ACM SIGIR 2006 sobre Investigación y Desarrollo en Recuperación de Información, páginas 162-169. [19] J. Xu y W. B. Croft. Mejorando la efectividad de la recuperación de información con análisis de contexto local. ACM Trans. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate to Spanish? Syst., 18(1):79-112, 2000. [20] E. YomTov, S. Fine, D. Carmel, y A. Darlow. Aprendiendo a estimar la dificultad de la consulta. En Actas de la Conferencia ACM SIGIR 2005 sobre Investigación y Desarrollo en Recuperación de Información, páginas 512-519. [21] Y. Zhou y W. B. Croft. Robustez del ranking: un nuevo marco para predecir el rendimiento de la consulta. En Actas de la 15ª Conferencia Internacional de la ACM. Conferencia sobre Gestión de Información y Conocimiento (CIKM 2006), páginas 567-574. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "language modeling": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Estimation and Use of Uncertainty in Pseudo-relevance Feedback Kevyn Collins-Thompson and Jamie Callan Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213-8213 U.S.A. {kct | callan}@cs.cmu.edu ABSTRACT Existing pseudo-relevance feedback methods typically perform averaging over the top-retrieved documents, but ignore an important statistical dimension: the risk or variance associated with either the individual document models, or their combination.",
                "Treating the baseline feedback method as a black box, and the output feedback model as a random variable, we estimate a posterior distribution for the feedback model by resampling a given querys top-retrieved documents, using the posterior mean or mode as the enhanced feedback model.",
                "We then perform model combination over several enhanced models, each based on a slightly modified query sampled from the original query.",
                "We find that resampling documents helps increase individual feedback model precision by removing noise terms, while sampling from the query improves robustness (worst-case performance) by emphasizing terms related to multiple query aspects.",
                "The result is a meta-feedback algorithm that is both more robust and more precise than the original strong baseline method.",
                "Categories and Subject Descriptors: H.3.3 [Information Retrieval]: Retrieval Models General Terms: Algorithms, Experimentation 1.",
                "INTRODUCTION Uncertainty is an inherent feature of information retrieval.",
                "Not only do we not know the queries that will be presented to our retrieval algorithm ahead of time, but the users information need may be vague or incompletely specified by these queries.",
                "Even if the query were perfectly specified, language in the collection documents is inherently complex and ambiguous and matching such language effectively is a formidable problem by itself.",
                "With this in mind, we wish to treat many important quantities calculated by the retrieval system, whether a relevance score for a document, or a weight for a query expansion term, as random variables whose true value is uncertain but where the uncertainty about the true value may be quantified by replacing the fixed value with a probability distribution over possible values.",
                "In this way, retrieval algorithms may attempt to quantify the risk or uncertainty associated with their output rankings, or improve the stability or precision of their internal calculations.",
                "Current algorithms for pseudo-relevance feedback (PRF) tend to follow the same basic method whether we use vector space-based algorithms such as Rocchios formula [16], or more recent <br>language modeling</br> approaches such as Relevance Models [10].",
                "First, a set of top-retrieved documents is obtained from an initial query and assumed to approximate a set of relevant documents.",
                "Next, a single feedback model vector is computed according to some sort of average, centroid, or expectation over the set of possibly-relevant document models.",
                "For example, the document vectors may be combined with equal weighting, as in Rocchio, or by query likelihood, as may be done using the Relevance Model1 .",
                "The use of an expectation is reasonable for practical and theoretical reasons, but by itself ignores potentially valuable information about the risk of the feedback model.",
                "Our main hypothesis in this paper is that estimating the uncertainty in feedback is useful and leads to better individual feedback models and more robust combined models.",
                "Therefore, we propose a method for estimating uncertainty associated with an individual feedback model in terms of a posterior distribution over language models.",
                "To do this, we systematically vary the inputs to the baseline feedback method and fit a Dirichlet distribution to the output.",
                "We use the posterior mean or mode as the improved feedback model estimate.",
                "This process is shown in Figure 1.",
                "As we show later, the mean and mode may vary significantly from the single feedback model proposed by the baseline method.",
                "We also perform model combination using several improved feedback language models obtained by a small number of new queries sampled from the original query.",
                "A models weight combines two complementary factors: the models probability of generating the query, and the variance of the model, with high-variance models getting lower weight. 1 For example, an expected parameter vector conditioned on the query observation is formed from top-retrieved documents, which are treated as training strings (see [10], p. 62).",
                "Figure 1: Estimating the uncertainty of the feedback model for a single query. 2.",
                "SAMPLING-BASED FEEDBACK In Sections 2.1-2.5 we describe a general method for estimating a probability distribution over the set of possible language models.",
                "In Sections 2.6 and 2.7 we summarize how different query samples are used to generate multiple feedback models, which are then combined. 2.1 Modeling Feedback Uncertainty Given a query Q and a collection C, we assume a probabilistic retrieval system that assigns a real-valued document score f(D, Q) to each document D in C, such that the score is proportional to the estimated probability of relevance.",
                "We make no other assumptions about f(D, Q).",
                "The nature of f(D, Q) may be complex: for example, if the retrieval system supports structured query languages [12], then f(D, Q) may represent the output of an arbitrarily complex inference network defined by the structured query operators.",
                "In theory, the scoring function can vary from query to query, although in this study for simplicity we keep the scoring function the same for all queries.",
                "Our specific query method is given in Section 3.",
                "We treat the feedback algorithm as a black box and assume that the inputs to the feedback algorithm are the original query and the corresponding top-retrieved documents, with a score being given to each document.",
                "We assume that the output of the feedback algorithm is a vector of term weights to be used to add or reweight the terms in the representation of the original query, with the vector normalized to form a probability distribution.",
                "We view the the inputs to the feedback black box as random variables, and analyze the feedback model as a random variable that changes in response to changes in the inputs.",
                "Like the document scoring function f(D, Q), the feedback algorithm may implement a complex, non-linear scoring formula, and so as its inputs vary, the resulting feedback models may have a complex distribution over the space of feedback models (the sample space).",
                "Because of this potential complexity, we do not attempt to derive a posterior distribution in closed form, but instead use simulation.",
                "We call this distribution over possible feedback models the feedback model distribution.",
                "Our goal in this section is to estimate a useful approximation to the feedback model distribution.",
                "For a specific framework for experiments, we use the <br>language modeling</br> (LM) approach for information retrieval [15].",
                "The score of a document D with respect to a query Q and collection C is given by p(Q|D) with respect to language models ˆθQ and ˆθD estimated for the query and document respectively.",
                "We denote the set of k top-retrieved documents from collection C in response to Q by DQ(k, C).",
                "For simplicity, we assume that queries and documents are generated by multinomial distributions whose parameters are represented by unigram language models.",
                "To incorporate feedback in the LM approach, we assume a model-based scheme in which our goal is take the query and resulting ranked documents DQ(k, C) as input, and output an expansion language model ˆθE, which is then interpolated with the original query model ˆθQ: ˆθNew = (1 − α) · ˆθQ + α · ˆθE (1) This includes the possibility of α = 1 where the original query mode is completely replaced by the feedback model.",
                "Our sample space is the set of all possible language models LF that may be output as feedback models.",
                "Our approach is to take samples from this space and then fit a distribution to the samples using maximum likelihood.",
                "For simplicity, we start by assuming the latent feedback distribution has the form of a Dirichlet distribution.",
                "Although the Dirichlet is a unimodal distribution, and in general quite limited in its expressiveness in the sample space, it is a natural match for the multinomial language model, can be estimated quickly, and can capture the most salient features of confident and uncertain feedback models, such as the overall spread of the distibution. 2.2 Resampling document models We would like an approximation to the posterior distribution of the feedback model LF .",
                "To accomplish this, we apply a widely-used simulation technique called bootstrap sampling ([7], p. 474) on the input parameters, namely, the set of top-retrieved documents.",
                "Bootstrap sampling allows us to simulate the approximate effect of perturbing the parameters within the black box feedback algorithm by perturbing the inputs to that algorithm in a systematic way, while making no assumptions about the nature of the feedback algorithm.",
                "Specifically, we sample k documents with replacement from DQ(k, C), and calculate an expansion language model θb using the black box feedback method.",
                "We repeat this process B times to obtain a set of B feedback language models, to which we then fit a Dirichlet distribution.",
                "Typically B is in the range of 20 to 50 samples, with performance being relatively stable in this range.",
                "Note that instead of treating each top document as equally likely, we sample according to the estimated probabilities of relevance of each document in DQ(k, C).",
                "Thus, a document is more likely to be chosen the higher it is in the ranking. 2.3 Justification for a sampling approach The rationale for our sampling approach has two parts.",
                "First, we want to improve the quality of individual feedback models by smoothing out variation when the baseline feedback model is unstable.",
                "In this respect, our approach resembles bagging [4], an ensemble approach which generates multiple versions of a predictor by making bootstrap copies of the training set, and then averages the (numerical) predictors.",
                "In our application, top-retrieved documents can be seen as a kind of noisy training set for relevance.",
                "Second, sampling is an effective way to estimate basic properties of the feedback posterior distribution, which can then be used for improved model combination.",
                "For example, a model may be weighted by its prediction confidence, estimated as a function of the variability of the posterior around the model. foo2-401.map-Dim:5434,Size:12*12units,gaussianneighborhood (a) Topic 401 Foreign minorities, Germany foo2-402.map-Dim:5698,Size:12*12units,gaussianneighborhood (b) Topic 402 Behavioral genetics foo2-459.map-Dim:8969,Size:12*12units,gaussianneighborhood (c) Topic 459 When can a lender foreclose on property Figure 2: Visualization of expansion language model variance using self-organizing maps, showing the distribution of language models that results from resampling the inputs to the baseline expansion method.",
                "The language model that would have been chosen by the baseline expansion is at the center of each map.",
                "The similarity function is JensenShannon divergence. 2.4 Visualizing feedback distributions Before describing how we fit and use the Dirichlet distribution over feedback models, it is instructive to view some examples of actual feedback model distributions that result from bootstrap sampling the top-retrieved documents from different TREC topics.",
                "Each point in our sample space is a language model, which typically has several thousand dimensions.",
                "To help analyze the behavior of our method we used a Self-Organizing Map (via the SOM-PAK package [9]), to flatten and visualize the high-dimensional density function2 .",
                "The density maps for three TREC topics are shown in Figure 2 above.",
                "The dark areas represent regions of high similarity between language models.",
                "The light areas represent regions of low similarity - the valleys between clusters.",
                "Each diagram is centered on the language model that would have been chosen by the baseline expansion.",
                "A single peak (mode) is evident in some examples, but more complex structure appears in others.",
                "Also, while the distribution is usually close to the baseline feedback model, for some topics they are a significant distance apart (as measured by JensenShannon divergence), as in Subfigure 2c.",
                "In such cases, the mode or mean of the feedback distribution often performs significantly better than the baseline (and in a smaller proportion of cases, significantly worse). 2.5 Fitting a posterior feedback distribution After obtaining feedback model samples by resampling the feedback model inputs, we estimate the feedback distribution.",
                "We assume that the multinomial feedback models {ˆθ1, . . . , ˆθB} were generated by a latent Dirichlet distribution with parameters {α1, . . . , αN }.",
                "To estimate the {α1, . . . , αN }, we fit the Dirichlet parameters to the B language model samples according to maximum likelihood using a generalized Newton procedure, details of which are given in Minka [13].",
                "We assume a simple Dirichlet prior over the {α1, . . . , αN }, setting each to αi = μ · p(wi | C), where μ is a parameter and p(· | C) is the collection language model estimated from a set of documents from collection C. The parameter fitting converges very quickly - typically just 2 or 2 Because our points are language models in the multinomial simplex, we extended SOM-PAK to support JensenShannon divergence, a widely-used similarity measure between probability distributions. 3 iterations are enough - so that it is practical to apply at query-time when computational overhead must be small.",
                "In practice, we can restrict the calculation to the vocabulary of the top-retrieved documents, instead of the entire collection.",
                "Note that for this step we are re-using the existing retrieved documents and not performing additional queries.",
                "Given the parameters of an N-dimensional Dirichlet distribution Dir(α) the mean μ and mode x vectors are easy to calculate and are given respectively by μi = αiP αi (2) and xi = αi−1P αi−N . (3) We can then choose the language model at the mean or the mode of the posterior as the final enhanced feedback model. (We found the mode to give slightly better performance.)",
                "For information retrieval, the number of samples we will have available is likely to be quite small for performance reasons - usually less than ten.",
                "Moreover, while random sampling is useful in certain cases, it is perfectly acceptable to allow deterministic sampling distributions, but these must be designed carefully in order to approximate an accurate output variance.",
                "We leave this for future study. 2.6 Query variants We use the following methods for generating variants of the original query.",
                "Each variant corresponds to a different assumption about which aspects of the original query may be important.",
                "This is a form of deterministic sampling.",
                "We selected three simple methods that cover complimentary assumptions about the query.",
                "No-expansion Use only the original query.",
                "The assumption is that the given terms are a complete description of the information need.",
                "Leave-one-out A single term is left out of the original query.",
                "The assumption is that one of the query terms is a noise term.",
                "Single-term A single term is chosen from the original query.",
                "This assumes that only one aspect of the query, namely, that represented by the term, is most important.",
                "After generating a variant of the original query, we combine it with the original query using a weight αSUB so that we do not stray too far.",
                "In this study, we set αSUB = 0.5.",
                "For example, using the Indri [12] query language, a leave-oneout variant of the initial query that omits the term ireland for TREC topic 404 is: #weight(0.5 #combine(ireland peace talks) 0.5 #combine(peace talks)) 2.7 Combining enhanced feedback models from multiple query variants When using multiple query variants, the resulting enhanced feedback models are combined using Bayesian model combination.",
                "To do this, we treat each word as an item to be classified as belonging to a relevant or non-relevant class, and derive a class probability for each word by combining the scores from each query variant.",
                "Each score is given by that terms probability in the Dirichlet distribution.",
                "The term scores are weighted by the inverse of the variance of the term in the enhanced feedback models Dirichlet distribution.",
                "The prior probability of a words membership in the relevant class is given by the probability of the original query in the entire enhanced expansion model. 3.",
                "EVALUATION In this section we present results confirming the usefulness of estimating a feedback model distribution from weighted resampling of top-ranked documents, and of combining the feedback models obtained from different small changes in the original query. 3.1 General method We evaluated performance on a total of 350 queries derived from four sets of TREC topics: 51-200 (TREC-1&2), 351-400 (TREC-7), 401-450 (TREC-8), and 451-550 (wt10g, TREC-9&10).",
                "We chose these for their varied content and document properties.",
                "For example, wt10g documents are Web pages with a wide variety of subjects and styles while TREC-1&2 documents are more homogeneous news articles.",
                "Indexing and retrieval was performed using the Indri system in the Lemur toolkit [12] [1].",
                "Our queries were derived from the words in the title field of the TREC topics.",
                "Phrases were not used.",
                "To generate the baseline queries passed to Indri, we wrapped the query terms with Indris #combine operator.",
                "For example, the initial query for topic 404 is: #combine(ireland peace talks) We performed Krovetz stemming for all experiments.",
                "Because we found that the baseline (Indri) expansion method performed better using a stopword list with the feedback model, all experiments used a stoplist of 419 common English words.",
                "However, an interesting side-effect of our resampling approach is that it tends to remove many stopwords from the feedback model, making a stoplist less critical.",
                "This is discussed further in Section 3.6. 3.2 Baseline feedback method For our baseline expansion method, we use an algorithm included in Indri 1.0 as the default expansion method.",
                "This method first selects terms using a log-odds calculation described by Ponte [14], but assigns final term weights using Lavrenkos relevance model[10].",
                "We chose the Indri method because it gives a consistently strong baseline, is based on a <br>language modeling</br> approach, and is simple to experiment with.",
                "In a TREC evaluation using the GOV2 corpus [6], the method was one of the topperforming runs, achieving a 19.8% gain in MAP compared to using unexpanded queries.",
                "In this study, it achieves an average gain in MAP of 17.25% over the four collections.",
                "Indris expansion method first calculates a log-odds ratio o(v) for each potential expansion term v given by o(v) = X D log p(v|D) p(v|C) (4) over all documents D containing v, in collection C. Then, the expansion term candidates are sorted by descending o(v), and the top m are chosen.",
                "Finally, the term weights r(v) used in the expanded query are calculated based on the relevance model r(v) = X D p(q|D)p(v|D) p(v) p(D) (5) The quantity p(q|D) is the probability score assigned to the document in the initial retrieval.",
                "We use Dirichlet smoothing of p(v|D) with μ = 1000.",
                "This relevance model is then combined with the original query using linear interpolation, weighted by a parameter α.",
                "By default we used the top 50 documents for feedback and the top 20 expansion terms, with the feedback interpolation parameter α = 0.5 unless otherwise stated.",
                "For example, the baseline expanded query for topic 404 is: #weight(0.5 #combine(ireland peace talks) 0.5 #weight(0.10 ireland 0.08 peace 0.08 northern ...) 3.3 Expansion performance We measure our feedback algorithms effectiveness by two main criteria: precision, and robustness.",
                "Robustness, and the tradeoff between precision and robustness, is analyzed in Section 3.4.",
                "In this section, we examine average precision and precision in the top 10 documents (P10).",
                "We also include recall at 1,000 documents.",
                "For each query, we obtained a set of B feedback models using the Indri baseline.",
                "Each feedback model was obtained from a random sample of the top k documents taken with replacement.",
                "For these experiments, B = 30 and k = 50.",
                "Each feedback model contained 20 terms.",
                "On the query side, we used leave-one-out (LOO) sampling to create the query variants.",
                "Single-term query sampling had consistently worse performance across all collections and so our results here focus on LOO sampling.",
                "We used the methods described in Section 2 to estimate an enhanced feedback model from the Dirichlet posterior distribution for each query variant, and to combine the feedback models from all the query variants.",
                "We call our method resampling expansion and denote it as RS-FB here.",
                "We denote the Indri baseline feedback method as Base-FB.",
                "Results from applying both the baseline expansion method (Base-FB) and resampling expansion (RS-FB) are shown in Table 1.",
                "We observe several trends in this table.",
                "First, the average precision of RS-FB was comparable to Base-FB, achieving an average gain of 17.6% compared to using no expansion across the four collections.",
                "The Indri baseline expansion gain was 17.25%.",
                "Also, the RS-FB method achieved consistent improvements in P10 over Base-FB for every topic set, with an average improvement of 6.89% over Base-FB for all 350 topics.",
                "The lowest P10 gain over Base-FB was +3.82% for TREC-7 and the highest was +11.95% for wt10g.",
                "Finally, both Base-FB and RS-FB also consistently improved recall over using no expansion, with Base-FB achieving better recall than RS-FB for all topic sets. 3.4 Retrieval robustness We use the term robustness to mean the worst-case average precision performance of a feedback algorithm.",
                "Ideally, a robust feedback method would never perform worse than using the original query, while often performing better using the expansion.",
                "To evaluate robustness in this study, we use a very simple measure called the robustness index (RI)3 .",
                "For a set of queries Q, the RI measure is defined as: RI(Q) = n+ − n− |Q| (6) where n+ is the number of queries helped by the feedback method and n− is the number of queries hurt.",
                "Here, by helped we mean obtaining a higher average precision as a result of feedback.",
                "The value of RI ranges from a minimum 3 This is sometimes also called the reliability of improvement index and was used in Sakai et al. [17].",
                "Collection NoExp Base-FB RS-FB TREC 1&2 AvgP 0.1818 0.2419 (+33.04%) 0.2406 (+32.24%) P10 0.4443 0.4913 (+10.57%) 0.5363 (+17.83%) Recall 15084/37393 19172/37393 15396/37393 TREC 7 AvgP 0.1890 0.2175 (+15.07%) 0.2169 (+14.75%) P10 0.4200 0.4320 (+2.85%) 0.4480 (+6.67%) Recall 2179/4674 2608/4674 2487/4674 TREC 8 AvgP 0.2031 0.2361 (+16.25%) 0.2268 (+11.70%) P10 0.3960 0.4160 (+5.05%) 0.4340 (+9.59%) Recall 2144/4728 2642/4728 2485/4728 wt10g AvgP 0.1741 0.1829 (+5.06%) 0.1946 (+11.78%) P10 0.2760 0.2630 (-4.71%) 0.2960 (+7.24%) Recall 3361/5980 3725/5980 3664/5980 Table 1: Comparison of baseline (Base-FB) feedback and feedback using re-sampling (RS-FB).",
                "Improvement shown for BaseFB and RS-FB is relative to using no expansion. (a) TREC 1&2 (upper curve); TREC 8 (lower curve) (b) TREC 7 (upper curve); wt10g (lower curve) Figure 3: The trade-off between robustness and average precision for different corpora.",
                "The x-axis gives the change in MAP over using baseline expansion with α = 0.5.",
                "The yaxis gives the Robustness Index (RI).",
                "Each curve through uncircled points shows the RI/MAP tradeoff using the simple small-α strategy (see text) as α decreases from 0.5 to zero in the direction of the arrow.",
                "Circled points represent the tradeoffs obtained by resampling feedback for α = 0.5.",
                "Collection N Base-FB RS-FB n− RI n− RI TREC 1&2 103 26 +0.495 15 +0.709 TREC 7 46 14 +0.391 10 +0.565 TREC 8 44 12 +0.455 12 +0.455 wt10g 91 48 -0.055 39 +0.143 Combined 284 100 +0.296 76 +0.465 Table 2: Comparison of robustness index (RI) for baseline feedback (Base-FB) vs. resampling feedback (RS-FB).",
                "Also shown are the actual number of queries hurt by feedback (n−) for each method and collection.",
                "Queries for which initial average precision was negligible (≤ 0.01) were ignored, giving the remaining query count in column N. of −1.0, when all queries are hurt by the feedback method, to +1.0 when all queries are helped.",
                "The RI measure does not take into account the magnitude or distribution of the amount of change across the set Q.",
                "However, it is easy to understand as a general indication of robustness.",
                "One obvious way to improve the worst-case performance of feedback is simply to use a smaller fixed α interpolation parameter, such as α = 0.3, placing less weight on the (possibly risky) feedback model and more on the original query.",
                "We call this the small-α strategy.",
                "Since we are also reducing the potential gains when the feedback model is right, however, we would expect some trade-off between average precision and robustness.",
                "We therefore compared the precision/robustness trade-off between our resampling feedback algorithm, and the simple small-α method.",
                "The results are summarized in Figure 3.",
                "In the figure, the curve for each topic set interpolates between trade-off points, beginning at x=0, where α = 0.5, and continuing in the direction of the arrow as α decreases and the original query is given more and more weight.",
                "As expected, robustness continuously increases as we move along the curve, but mean average precision generally drops as the gains from feedback are eliminated.",
                "For comparison, the performance of resampling feedback at α = 0.5 is shown for each collection as the circled point.",
                "Higher and to the right is better.",
                "This figure shows that resampling feedback gives a somewhat better trade-off than the small-α approach for 3 of the 4 collections.",
                "Figure 4: Histogram showing improved robustness of resampling feedback (RS-FB) over baseline feedback (Base-FB) for all datasets combined.",
                "Queries are binned by % change in AP compared to the unexpanded query.",
                "Collection DS + QV DS + No QV TREC 1&2 AvgP 0.2406 0.2547 (+5.86%) P10 0.5263 0.5362 (+1.88%) RI 0.7087 0.6515 (-0.0572) TREC 7 AvgP 0.2169 0.2200 (+1.43%) P10 0.4480 0.4300 (-4.02%) RI 0.5652 0.2609 (-0.3043) TREC 8 AvgP 0.2268 0.2257 (-0.49%) P10 0.4340 0.4200 (-3.23%) RI 0.4545 0.4091 (-0.0454) wt10g AvgP 0.1946 0.1865 (-4.16%) P10 0.2960 0.2680 (-9.46%) RI 0.1429 0.0220 (-0.1209) Table 3: Comparison of resampling feedback using document sampling (DS) with (QV) and without (No QV) combining feedback models from multiple query variants.",
                "Table 2 gives the Robustness Index scores for Base-FB and RS-FB.",
                "The RS-FB feedback method obtained higher robustness than Base-FB on three of the four topic sets, with only slightly worse performance on TREC-8.",
                "A more detailed view showing the distribution over relative changes in AP is given by the histogram in Figure 4.",
                "Compared to Base-FB, the RS-FB method achieves a noticable reduction in the number of queries significantly hurt by expansion (i.e. where AP is hurt by 25% or more), while preserving positive gains in AP. 3.5 Effect of query and document sampling methods Given our algorithms improved robustness seen in Section 3.4, an important question is what component of our system is responsible.",
                "Is it the use of document re-sampling, the use of multiple query variants, or some other factor?",
                "The results in Table 3 suggest that the model combination based on query variants may be largely account for the improved robustness.",
                "When query variants are turned off and the original query is used by itself with document sampling, there is little net change in average precision, a small decrease in P10 for 3 out of the 4 topic sets, but a significant drop in robustness for all topic sets.",
                "In two cases, the RI measure drops by more than 50%.",
                "We also examined the effect of the document sampling method on retrieval effectiveness, using two different strategies.",
                "The uniform weighting strategy ignored the relevance scores from the initial retrieval and gave each document in the top k the same probability of selection.",
                "In contrast, the relevance-score weighting strategy chose documents with probability proportional to their relevance scores.",
                "In this way, documents that were more highly ranked were more likely to be selected.",
                "Results are shown in Table 4.",
                "The relevance-score weighting strategy performs better overall, with significantly higher RI and P10 scores on 3 of the 4 topic sets.",
                "The difference in average precision between the methods, however, is less marked.",
                "This suggests that uniform weighting acts to increase variance in retrieval results: when initial average precision is high, there are many relevant documents in the top k and uniform sampling may give a more representative relevance model than focusing on the highly-ranked items.",
                "On the other hand, when initial precision is low, there are few relevant documents in the bottom ranks and uniform sampling mixes in more of the non-relevant documents.",
                "For space reasons we only summarize our findings on sample size here.",
                "The number of samples has some effect on precision when less than 10, but performance stabilizes at around 15 to 20 samples.",
                "We used 30 samples for our experiments.",
                "Much beyond this level, the additional benefits of more samples decrease as the initial score distribution is more closely fit and the processing time increases. 3.6 The effect of resampling on expansion term quality Ideally, a retrieval model should not require a stopword list when estimating a model of relevance: a robust statistical model should down-weight stopwords automatically depending on context.",
                "Stopwords can harm feedback if selected as feedback terms, because they are typically poor discriminators and waste valuable term slots.",
                "In practice, however, because most term selection methods resemble a tf · idf type of weighting, terms with low idf but very high tf can sometimes be selected as expansion term candidates.",
                "This happens, for example, even with the Relevance Model approach that is part of our baseline feedback.",
                "To ensure as strong a baseline as possible, we use a stoplist for all experiments reported here.",
                "If we turn off the stopword list, however, we obtain results such as those shown in Table 5 where four of the top ten baseline feedback terms for TREC topic 60 (said, but, their, not) are stopwords using the BaseFB method. (The top 100 expansion terms were selected to generate this example.)",
                "Indris method attempts to address the stopword problem by applying an initial step based on Ponte [14] to select less-common terms that have high log-odds of being in the top-ranked documents compared to the whole collection.",
                "Nevertheless, this does not overcome the stopword problem completely, especially as the number of feedback terms grows.",
                "Using resampling feedback, however, appears to mitigate Collection QV + Uniform QV + Relevance-score weighting weighting TREC 1&2 AvgP 0.2545 0.2406 (-5.46%) P10 0.5369 0.5263 (-1.97%) RI 0.6212 0.7087 (+14.09%) TREC 7 AvgP 0.2174 0.2169 (-0.23%) P10 0.4320 0.4480 (+3.70%) RI 0.4783 0.5652 (+18.17%) TREC 8 AvgP 0.2267 0.2268 (+0.04%) P10 0.4120 0.4340 (+5.34%) RI 0.4545 0.4545 (+0.00%) wt10g AvgP 0.1808 0.1946 (+7.63%) P10 0.2680 0.2960 (+10.45%) RI 0.0220 0.1099 (+399.5%) Table 4: Comparison of uniform and relevance-weighted document sampling.",
                "The percentage change compared to uniform sampling is shown in parentheses.",
                "QV indicates that query variants were used in both runs.",
                "Baseline FB p(wi|R) Resampling FB p(wi|R) said 0.055 court 0.026 court 0.055 pay 0.018 pay 0.034 federal 0.012 but 0.026 education 0.011 employees 0.024 teachers 0.010 their 0.024 employees 0.010 not 0.023 case 0.010 federal 0.021 their 0.009 workers 0.020 appeals 0.008 education 0.020 union 0.007 Table 5: Feedback term quality when a stoplist is not used.",
                "Feedback terms for TREC topic 60: merit pay vs seniority. the effect of stopwords automatically.",
                "In the example of Table 5, resampling feedback leaves only one stopword (their) in the top ten.",
                "We observed similar feedback term behavior across many other topics.",
                "The reason for this effect appears to be the interaction of the term selection score with the top-m term cutoff.",
                "While the presence and even proportion of particular stopwords is fairly stable across different document samples, their relative position in the top-m list is not, as sets of documents with varying numbers of better, lower-frequency term candidates are examined for each sample.",
                "As a result, while some number of stopwords may appear in each sampled document set, any given stopword tends to fall below the cutoff for multiple samples, leading to its classification as a high-variance, low-weight feature. 4.",
                "RELATED WORK Our approach is related to previous work from several areas of information retrieval and machine learning.",
                "Our use of query variation was inspired by the work of YomTov et al. [20], Carpineto et al. [5], and Amati et al. [2], among others.",
                "These studies use the idea of creating multiple subqueries and then examining the nature of the overlap in the documents and/or expansion terms that result from each subquery.",
                "Model combination is performed using heuristics.",
                "In particular, the studies of Amati et al. and Carpineto et al. investigated combining terms from individual distributional methods using a term-reranking combination heuristic.",
                "In a set of TREC topics they found wide average variation in the rank-distance of terms from different expansion methods.",
                "Their combination method gave modest positive improvements in average precision.",
                "The idea of examining the overlap between lists of suggested terms has also been used in early query expansion approaches.",
                "Xu and Crofts method of Local Context Analysis (LCA) [19] includes a factor in the empirically-derived weighting formula that causes expansion terms to be preferred that have connections to multiple query terms.",
                "On the document side, recent work by Zhou & Croft [21] explored the idea of adding noise to documents, re-scoring them, and using the stability of the resulting rankings as an estimate of query difficulty.",
                "This is related to our use of document sampling to estimate the risk of the feedback model built from the different sets of top-retrieved documents.",
                "Sakai et al. [17] proposed an approach to improving the robustness of pseudo-relevance feedback using a method they call selective sampling.",
                "The essence of their method is that they allow skipping of some top-ranked documents, based on a clustering criterion, in order to select a more varied and novel set of documents later in the ranking for use by a traditional pseudo-feedback method.",
                "Their study did not find significant improvements in either robustness (RI) or MAP on their corpora.",
                "Greiff, Morgan and Ponte [8] explored the role of variance in term weighting.",
                "In a series of simulations that simplified the problem to 2-feature documents, they found that average precision degrades as term frequency variance - high noiseincreases.",
                "Downweighting terms with high variance resulted in improved average precision.",
                "This seems in accord with our own findings for individual feedback models.",
                "Estimates of output variance have recently been used for improved text classification.",
                "Lee et al. [11] used queryspecific variance estimates of classifier outputs to perform improved model combination.",
                "Instead of using sampling, they were able to derive closed-form expressions for classifier variance by assuming base classifiers using simple types of inference networks.",
                "Ando and Zhang proposed a method that they call structural feedback [3] and showed how to apply it to query expansion for the TREC Genomics Track.",
                "They used r query variations to obtain R different sets Sr of top-ranked documents that have been intersected with the top-ranked documents obtained from the original query qorig.",
                "For each Si, the normalized centroid vector ˆwi of the documents is calculated.",
                "Principal component analysis (PCA) is then applied to the ˆwi to obtain the matrix Φ of H left singular vectors φh that are used to obtain the new, expanded query qexp = qorig + ΦT Φqorig. (7) In the case H = 1, we have a single left singular vector φ: qexp = qorig + (φT qorig)φ so that the dot product φT qorig is a type of dynamic weight on the expanded query that is based on the similarity of the original query to the expanded query.",
                "The use of variance as a feedback model quality measure occurs indirectly through the application of PCA.",
                "It would be interesting to study the connections between this approach and our own modelfitting method.",
                "Finally, in <br>language modeling</br> approaches to feedback, Tao and Zhai [18] describe a method for more robust feedback that allows each document to have a different feedback α.",
                "The feedback weights are derived automatically using regularized EM.",
                "A roughly equal balance of query and expansion model is implied by their EM stopping condition.",
                "They propose tailoring the stopping parameter η based on a function of some quality measure of feedback documents. 5.",
                "CONCLUSIONS We have presented a new approach to pseudo-relevance feedback based on document and query sampling.",
                "The use of sampling is a very flexible and powerful device and is motivated by our general desire to extend current models of retrieval by estimating the risk or variance associated with the parameters or output of retrieval processes.",
                "Such variance estimates, for example, may be naturally used in a Bayesian framework for improved model estimation and combination.",
                "Applications such as selective expansion may then be implemented in a principled way.",
                "While our study uses the <br>language modeling</br> approach as a framework for experiments, we make few assumptions about the actual workings of the feedback algorithm.",
                "We believe it is likely that any reasonably effective baseline feedback algorithm would benefit from our approach.",
                "Our results on standard TREC collections show that our framework improves the robustness of a strong baseline feedback method across a variety of collections, without sacrificing average precision.",
                "It also gives small but consistent gains in top10 precision.",
                "In future work, we envision an investigation into how varying the set of sampling methods used and the number of samples controls the trade-off between robustness, accuracy, and efficiency.",
                "Acknowledgements We thank Paul Bennett for valuable discussions related to this work, which was supported by NSF grants #IIS-0534345 and #CNS-0454018, and U.S. Dept. of Education grant #R305G03123.",
                "Any opinions, findings, and conclusions or recommendations expressed in this material are the authors. and do not necessarily reflect those of the sponsors. 6.",
                "REFERENCES [1] The Lemur toolkit for <br>language modeling</br> and retrieval. http://www.lemurproject.org. [2] G. Amati, C. Carpineto, and G. Romano.",
                "Query difficulty, robustness, and selective application of query expansion.",
                "In Proc. of the 25th European Conf. on Information Retrieval (ECIR 2004), pages 127-137. [3] R. K. Ando and T. Zhang.",
                "A high-performance semi-supervised learning method for text chunking.",
                "In Proc. of the 43rd Annual Meeting of the ACL, pages 1-9, June 2005. [4] L. Breiman.",
                "Bagging predictors.",
                "Machine Learning, 24(2):123-140, 1996. [5] C. Carpineto, G. Romano, and V. Giannini.",
                "Improving retrieval feedback with multiple term-ranking function combination.",
                "ACM Trans.",
                "Info.",
                "Systems, 20(3):259 - 290. [6] K. Collins-Thompson, P. Ogilvie, and J. Callan.",
                "Initial results with structured queries and language models on half a terabyte of text.",
                "In Proc. of 2005 Text REtrieval Conference.",
                "NIST Special Publication. [7] R. O. Duda, P. E. Hart, and D. G. Stork.",
                "Pattern Classification.",
                "Wiley and Sons, 2nd edition, 2001. [8] W. R. Greiff, W. T. Morgan, and J. M. Ponte.",
                "The role of variance in term weighting for probabilistic information retrieval.",
                "In Proc. of the 11th Intl.",
                "Conf. on Info. and Knowledge Mgmt. (CIKM 2002), pages 252-259. [9] T. Kohonen, J. Hynninen, J. Kangas, and J. Laaksonen.",
                "SOMPAK: The self-organizing map program package.",
                "Technical Report A31, Helsinki University of Technology, 1996. http://www.cis.hut.fi/research/papers/som tr96.ps.Z. [10] V. Lavrenko.",
                "A Generative Theory of Relevance.",
                "PhD thesis, University of Massachusetts, Amherst, 2004. [11] C.-H. Lee, R. Greiner, and S. Wang.",
                "Using query-specific variance estimates to combine Bayesian classifiers.",
                "In Proc. of the 23rd Intl.",
                "Conf. on Machine Learning (ICML 2006), pages 529-536. [12] D. Metzler and W. B. Croft.",
                "Combining the language model and inference network approaches to retrieval.",
                "Info.",
                "Processing and Mgmt., 40(5):735-750, 2004. [13] T. Minka.",
                "Estimating a Dirichlet distribution.",
                "Technical report, 2000. http://research.microsoft.com/ minka/papers/dirichlet. [14] J. Ponte.",
                "Advances in Information Retrieval, chapter Language models for relevance feedback, pages 73-96. 2000.",
                "W.B.",
                "Croft, ed. [15] J. M. Ponte and W. B. Croft.",
                "A <br>language modeling</br> approach to information retrieval.",
                "In Proc. of the 1998 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 275-281. [16] J. Rocchio.",
                "The SMART Retrieval System, chapter Relevance Feedback in Information Retrieval, pages 313-323.",
                "Prentice-Hall, 1971.",
                "G. Salton, ed. [17] T. Sakai, T. Manabe, and M. Koyama.",
                "Flexible pseudo-relevance feedback via selective sampling.",
                "ACM Transactions on Asian Language Information Processing (TALIP), 4(2):111-135, 2005. [18] T. Tao and C. Zhai.",
                "Regularized estimation of mixture models for robust pseudo-relevance feedback.",
                "In Proc. of the 2006 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 162-169. [19] J. Xu and W. B. Croft.",
                "Improving the effectiveness of information retrieval with local context analysis.",
                "ACM Trans.",
                "Inf.",
                "Syst., 18(1):79-112, 2000. [20] E. YomTov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty.",
                "In Proc. of the 2005 ACM SIGIR Conf. on Research and Development in Information Retrieval, pages 512-519. [21] Y. Zhou and W. B. Croft.",
                "Ranking robustness: a novel framework to predict query performance.",
                "In Proc. of the 15th ACM Intl.",
                "Conf. on Information and Knowledge Mgmt. (CIKM 2006), pages 567-574."
            ],
            "original_annotated_samples": [
                "Current algorithms for pseudo-relevance feedback (PRF) tend to follow the same basic method whether we use vector space-based algorithms such as Rocchios formula [16], or more recent <br>language modeling</br> approaches such as Relevance Models [10].",
                "For a specific framework for experiments, we use the <br>language modeling</br> (LM) approach for information retrieval [15].",
                "We chose the Indri method because it gives a consistently strong baseline, is based on a <br>language modeling</br> approach, and is simple to experiment with.",
                "Finally, in <br>language modeling</br> approaches to feedback, Tao and Zhai [18] describe a method for more robust feedback that allows each document to have a different feedback α.",
                "While our study uses the <br>language modeling</br> approach as a framework for experiments, we make few assumptions about the actual workings of the feedback algorithm."
            ],
            "translated_annotated_samples": [
                "Los algoritmos actuales para la retroalimentación de pseudo relevancia (PRF) tienden a seguir el mismo método básico ya sea que utilicemos algoritmos basados en espacio vectorial como la fórmula de Rocchio [16], o enfoques más recientes de <br>modelado de lenguaje</br> como Modelos de Relevancia [10].",
                "Para un marco específico de experimentos, utilizamos el enfoque de <br>modelado de lenguaje</br> (LM) para la recuperación de información [15].",
                "Elegimos el método Indri porque proporciona una línea base consistentemente sólida, se basa en un enfoque de <br>modelado del lenguaje</br> y es fácil de experimentar con él.",
                "Finalmente, en los enfoques de <br>modelado del lenguaje</br> para retroalimentación, Tao y Zhai [18] describen un método para una retroalimentación más robusta que permite que cada documento tenga un α de retroalimentación diferente.",
                "Si bien nuestro estudio utiliza el enfoque de <br>modelado del lenguaje</br> como marco para experimentos, hacemos pocas suposiciones sobre el funcionamiento real del algoritmo de retroalimentación."
            ],
            "translated_text": "Estimación y Uso de la Incertidumbre en la Retroalimentación de Pseudo-Relevancia Kevyn Collins-Thompson y Jamie Callan Instituto de Tecnologías del Lenguaje Escuela de Ciencias de la Computación Universidad Carnegie Mellon Pittsburgh, PA 15213-8213 EE. UU. {kct | callan}@cs.cmu.edu RESUMEN Los métodos existentes de retroalimentación de pseudo-relevancia suelen realizar un promedio de los documentos mejor clasificados, pero ignoran una dimensión estadística importante: el riesgo o la varianza asociada tanto a los modelos de documentos individuales como a su combinación. Tratando el método de retroalimentación de línea base como una caja negra, y el modelo de retroalimentación de salida como una variable aleatoria, estimamos una distribución posterior para el modelo de retroalimentación mediante el remuestreo de los documentos mejor recuperados de una consulta dada, utilizando la media o la moda posterior como el modelo de retroalimentación mejorado. Luego realizamos la combinación de modelos sobre varios modelos mejorados, cada uno basado en una consulta ligeramente modificada muestreada de la consulta original. Observamos que el remuestreo de documentos ayuda a aumentar la precisión del modelo de retroalimentación individual al eliminar términos de ruido, mientras que el muestreo de la consulta mejora la robustez (rendimiento en el peor de los casos) al enfatizar términos relacionados con múltiples aspectos de la consulta. El resultado es un algoritmo de meta-retroalimentación que es tanto más robusto como más preciso que el método de referencia original. Categorías y Descriptores de Asignaturas: H.3.3 [Recuperación de Información]: Modelos de Recuperación Términos Generales: Algoritmos, Experimentación 1. INTRODUCCIÓN La incertidumbre es una característica inherente de la recuperación de información. No solo no sabemos las consultas que se presentarán a nuestro algoritmo de recuperación con anticipación, sino que la necesidad de información de los usuarios puede ser vaga o estar incompletamente especificada por estas consultas. Incluso si la consulta estuviera perfectamente especificada, el lenguaje en los documentos de la colección es inherentemente complejo y ambiguo, y hacer coincidir dicho lenguaje de manera efectiva es un problema formidable por sí mismo. Con esto en mente, deseamos tratar muchas cantidades importantes calculadas por el sistema de recuperación, ya sea un puntaje de relevancia para un documento, o un peso para un término de expansión de consulta, como variables aleatorias cuyo valor real es incierto pero donde la incertidumbre sobre el valor real puede ser cuantificada reemplazando el valor fijo con una distribución de probabilidad sobre posibles valores. De esta manera, los algoritmos de recuperación pueden intentar cuantificar el riesgo o la incertidumbre asociados con sus clasificaciones de salida, o mejorar la estabilidad o precisión de sus cálculos internos. Los algoritmos actuales para la retroalimentación de pseudo relevancia (PRF) tienden a seguir el mismo método básico ya sea que utilicemos algoritmos basados en espacio vectorial como la fórmula de Rocchio [16], o enfoques más recientes de <br>modelado de lenguaje</br> como Modelos de Relevancia [10]. Primero, se obtiene un conjunto de documentos recuperados en la parte superior a partir de una consulta inicial y se asume que aproxima un conjunto de documentos relevantes. A continuación, se calcula un vector de modelo de retroalimentación único según algún tipo de promedio, centroide o expectativa sobre el conjunto de modelos de documentos posiblemente relevantes. Por ejemplo, los vectores de documentos pueden combinarse con igual ponderación, como en Rocchio, o por probabilidad de consulta, como se puede hacer utilizando el Modelo de Relevancia. El uso de una expectativa es razonable por razones prácticas y teóricas, pero por sí sola ignora información potencialmente valiosa sobre el riesgo del modelo de retroalimentación. Nuestra hipótesis principal en este artículo es que estimar la incertidumbre en la retroalimentación es útil y conduce a modelos de retroalimentación individual más efectivos y a modelos combinados más robustos. Por lo tanto, proponemos un método para estimar la incertidumbre asociada con un modelo de retroalimentación individual en términos de una distribución posterior sobre modelos de lenguaje. Para hacer esto, variamos sistemáticamente las entradas al método de retroalimentación base y ajustamos una distribución de Dirichlet a la salida. Utilizamos la media posterior o el modo como la estimación mejorada del modelo de retroalimentación. Este proceso se muestra en la Figura 1. Como mostraremos más adelante, la media y la moda pueden variar significativamente del modelo de retroalimentación única propuesto por el método de referencia. También realizamos la combinación de modelos utilizando varios modelos de lenguaje de retroalimentación mejorados obtenidos a partir de un pequeño número de nuevas consultas muestreadas de la consulta original. El peso de un modelo combina dos factores complementarios: la probabilidad del modelo de generar la consulta y la varianza del modelo, siendo los modelos con alta varianza los que obtienen un peso menor. Por ejemplo, un vector de parámetros esperado condicionado a la observación de la consulta se forma a partir de los documentos recuperados en la parte superior, que se tratan como cadenas de entrenamiento (ver [10], p. 62). Figura 1: Estimación de la incertidumbre del modelo de retroalimentación para una sola consulta. 2. En las Secciones 2.1-2.5 describimos un método general para estimar una distribución de probabilidad sobre el conjunto de posibles modelos de lenguaje basado en muestreo. En las Secciones 2.6 y 2.7 resumimos cómo se utilizan diferentes muestras de consultas para generar múltiples modelos de retroalimentación, que luego se combinan. 2.1 Modelado de la Incertidumbre de la Retroalimentación Dado una consulta Q y una colección C, asumimos un sistema de recuperación probabilístico que asigna un puntaje de documento de valor real f(D, Q) a cada documento D en C, de manera que el puntaje sea proporcional a la probabilidad estimada de relevancia. No hacemos ninguna otra suposición sobre f(D, Q). La naturaleza de f(D, Q) puede ser compleja: por ejemplo, si el sistema de recuperación admite lenguajes de consulta estructurados [12], entonces f(D, Q) puede representar la salida de una red de inferencia arbitrariamente compleja definida por los operadores de consulta estructurados. En teoría, la función de puntuación puede variar de una consulta a otra, aunque en este estudio por simplicidad mantenemos la misma función de puntuación para todas las consultas. Nuestro método de consulta específico se encuentra en la Sección 3. Tratamos el algoritmo de retroalimentación como una caja negra y asumimos que los insumos del algoritmo de retroalimentación son la consulta original y los documentos recuperados principales correspondientes, a los cuales se les asigna una puntuación a cada documento. Suponemos que la salida del algoritmo de retroalimentación es un vector de pesos de términos que se utilizarán para agregar o reponderar los términos en la representación de la consulta original, con el vector normalizado para formar una distribución de probabilidad. Consideramos las entradas a la caja negra de retroalimentación como variables aleatorias, y analizamos el modelo de retroalimentación como una variable aleatoria que cambia en respuesta a cambios en las entradas. Al igual que la función de puntuación del documento f(D, Q), el algoritmo de retroalimentación puede implementar una fórmula de puntuación compleja y no lineal, por lo que, a medida que sus entradas varían, los modelos de retroalimentación resultantes pueden tener una distribución compleja en el espacio de modelos de retroalimentación (el espacio muestral). Debido a esta complejidad potencial, no intentamos derivar una distribución posterior de forma cerrada, sino que en su lugar utilizamos simulación. Llamamos a esta distribución sobre posibles modelos de retroalimentación la distribución de modelos de retroalimentación. Nuestro objetivo en esta sección es estimar una aproximación útil a la distribución del modelo de retroalimentación. Para un marco específico de experimentos, utilizamos el enfoque de <br>modelado de lenguaje</br> (LM) para la recuperación de información [15]. El puntaje de un documento D con respecto a una consulta Q y una colección C se da por p(Q|D) con respecto a los modelos de lenguaje ˆθQ y ˆθD estimados para la consulta y el documento respectivamente. Denotamos el conjunto de los k documentos mejor recuperados de la colección C en respuesta a Q como DQ(k, C). Para simplificar, asumimos que las consultas y documentos son generados por distribuciones multinomiales cuyos parámetros están representados por modelos de lenguaje unigram. Para incorporar retroalimentación en el enfoque de LM, asumimos un esquema basado en modelos en el que nuestro objetivo es tomar la consulta y los documentos clasificados resultantes DQ(k, C) como entrada, y producir un modelo de lenguaje expandido ˆθE, que luego se interpola con el modelo de consulta original ˆθQ: ˆθNew = (1 − α) · ˆθQ + α · ˆθE (1). Esto incluye la posibilidad de α = 1 donde el modelo de consulta original es completamente reemplazado por el modelo de retroalimentación. Nuestro espacio muestral es el conjunto de todos los posibles modelos de lenguaje LF que pueden ser generados como modelos de retroalimentación. Nuestro enfoque es tomar muestras de este espacio y luego ajustar una distribución a las muestras utilizando máxima verosimilitud. Para simplificar, comenzamos asumiendo que la distribución de retroalimentación latente tiene la forma de una distribución de Dirichlet. Aunque la distribución de Dirichlet es unimodal y en general bastante limitada en su expresividad en el espacio muestral, es una coincidencia natural para el modelo de lenguaje multinomial, puede ser estimada rápidamente y capturar las características más relevantes de los modelos de retroalimentación confiada e incierta, como la dispersión general de la distribución. 2.2 Re-muestreo de modelos de documentos. Nos gustaría una aproximación a la distribución posterior del modelo de retroalimentación LF. Para lograr esto, aplicamos una técnica de simulación ampliamente utilizada llamada muestreo bootstrap ([7], p. 474) en los parámetros de entrada, es decir, el conjunto de documentos recuperados en la parte superior. El muestreo de arranque nos permite simular el efecto aproximado de perturbar los parámetros dentro del algoritmo de retroalimentación de la caja negra al perturbar los insumos de ese algoritmo de manera sistemática, sin hacer suposiciones sobre la naturaleza del algoritmo de retroalimentación. Específicamente, muestreamos k documentos con reemplazo de DQ(k, C), y calculamos un modelo de lenguaje de expansión θb utilizando el método de retroalimentación de caja negra. Repetimos este proceso B veces para obtener un conjunto de B modelos de lenguaje de retroalimentación, a los cuales luego ajustamos una distribución de Dirichlet. Normalmente, B se encuentra en el rango de 20 a 50 muestras, con un rendimiento relativamente estable en este rango. Ten en cuenta que en lugar de tratar cada documento superior como igualmente probable, muestreamos de acuerdo con las probabilidades estimadas de relevancia de cada documento en DQ(k, C). Por lo tanto, es más probable que un documento sea elegido cuanto más alto esté en la clasificación. 2.3 Justificación de un enfoque de muestreo La justificación de nuestro enfoque de muestreo tiene dos partes. Primero, queremos mejorar la calidad de los modelos de retroalimentación individual al suavizar la variación cuando el modelo de retroalimentación base es inestable. En este sentido, nuestro enfoque se asemeja al bagging [4], un enfoque de conjunto que genera múltiples versiones de un predictor haciendo copias de arranque del conjunto de entrenamiento, y luego promedia los predictores (numéricos). En nuestra aplicación, los documentos recuperados en la parte superior pueden ser vistos como una especie de conjunto de entrenamiento ruidoso para la relevancia. Segundo, el muestreo es una forma efectiva de estimar propiedades básicas de la distribución posterior de retroalimentación, las cuales pueden ser utilizadas para mejorar la combinación de modelos. Por ejemplo, un modelo puede ser ponderado por su confianza de predicción, estimada como una función de la variabilidad del posterior alrededor del modelo. foo2-401.map-Dim:5434,Size:12*12units, vecindario gaussiano (a) Tema 401 Minorías extranjeras, Alemania foo2-402.map-Dim:5698,Size:12*12units, vecindario gaussiano (b) Tema 402 Genética del comportamiento foo2-459.map-Dim:8969,Size:12*12units, vecindario gaussiano (c) Tema 459 ¿Cuándo puede un prestamista ejecutar una hipoteca sobre una propiedad? Figura 2: Visualización de la varianza del modelo de lenguaje de expansión utilizando mapas autoorganizados, mostrando la distribución de modelos de lenguaje que resulta de remuestrear las entradas al método de expansión de referencia. El modelo de lenguaje que habría sido elegido por la expansión de la línea base se encuentra en el centro de cada mapa. La función de similitud es la divergencia JensenShannon. 2.4 Visualización de distribuciones de retroalimentación Antes de describir cómo ajustamos y utilizamos la distribución de Dirichlet sobre modelos de retroalimentación, es instructivo ver algunos ejemplos de distribuciones de modelos de retroalimentación reales que resultan de muestrear por bootstrap los documentos más recuperados de diferentes temas de TREC. Cada punto en nuestro espacio muestral es un modelo de lenguaje, que típicamente tiene varias miles de dimensiones. Para ayudar a analizar el comportamiento de nuestro método, utilizamos un Mapa Auto-organizado (a través del paquete SOM-PAK [9]), para aplanar y visualizar la función de densidad de alta dimensión. Los mapas de densidad para tres temas de TREC se muestran en la Figura 2 arriba. Las áreas oscuras representan regiones de alta similitud entre los modelos de lenguaje. Las áreas claras representan regiones de baja similitud, los valles entre los grupos. Cada diagrama está centrado en el modelo de lenguaje que habría sido elegido por la expansión de referencia. Un solo pico (modo) es evidente en algunos ejemplos, pero en otros aparece una estructura más compleja. Además, aunque la distribución suele estar cerca del modelo de retroalimentación de referencia, para algunos temas están a una distancia significativa (según la divergencia de JensenShannon), como se muestra en la Subfigura 2c. En tales casos, la moda o la media de la distribución de retroalimentación a menudo funcionan significativamente mejor que el valor base (y en una proporción menor de casos, significativamente peor). 2.5 Ajuste de una distribución de retroalimentación posterior Después de obtener muestras del modelo de retroalimentación mediante el remuestreo de las entradas del modelo de retroalimentación, estimamos la distribución de retroalimentación. Suponemos que los modelos de retroalimentación multinomial {ˆθ1, . . . , ˆθB} fueron generados por una distribución de Dirichlet latente con parámetros {α1, . . . , αN}. Para estimar los {α1, . . . , αN }, ajustamos los parámetros de Dirichlet a las muestras del modelo de lenguaje B según la máxima verosimilitud utilizando un procedimiento de Newton generalizado, cuyos detalles se encuentran en Minka [13]. Suponemos una distribución de Dirichlet simple sobre los {α1, . . . , αN}, estableciendo cada uno como αi = μ · p(wi | C), donde μ es un parámetro y p(· | C) es el modelo de lenguaje de colección estimado a partir de un conjunto de documentos de la colección C. El ajuste de parámetros converge muy rápidamente, generalmente en solo 2 o 3 iteraciones. Dado que nuestros puntos son modelos de lenguaje en el simplex multinomial, extendimos SOM-PAK para admitir la divergencia de Jensen-Shannon, una medida de similitud ampliamente utilizada entre distribuciones de probabilidad. 3 iteraciones son suficientes, por lo que es práctico aplicarlo en tiempo de consulta cuando la sobrecarga computacional debe ser pequeña. En la práctica, podemos restringir el cálculo al vocabulario de los documentos recuperados en la parte superior, en lugar de toda la colección. Ten en cuenta que para este paso estamos reutilizando los documentos recuperados existentes y no realizando consultas adicionales. Dado los parámetros de una distribución de Dirichlet N-dimensional Dir(α), los vectores de media μ y moda x son fáciles de calcular y se dan respectivamente por μi = αiP αi (2) y xi = αi−1P αi−N . (3) Luego podemos elegir el modelo de lenguaje en la media o en la moda del posterior como el modelo de retroalimentación mejorado final. (Encontramos que la moda proporciona un rendimiento ligeramente mejor). Para la recuperación de información, es probable que el número de muestras disponibles sea bastante pequeño por razones de rendimiento, generalmente menos de diez. Además, si bien el muestreo aleatorio es útil en ciertos casos, es perfectamente aceptable permitir distribuciones de muestreo determinísticas, pero estas deben diseñarse cuidadosamente para aproximar una varianza de salida precisa. Dejamos esto para estudios futuros. 2.6 Variantes de consulta Utilizamos los siguientes métodos para generar variantes de la consulta original. Cada variante corresponde a una suposición diferente sobre qué aspectos de la consulta original pueden ser importantes. Esta es una forma de muestreo determinístico. Seleccionamos tres métodos simples que cubren suposiciones complementarias sobre la consulta. Uso sin expansión. La suposición es que los términos dados son una descripción completa de la necesidad de información. Deja uno fuera. Se deja fuera un término del query original. La suposición es que uno de los términos de la consulta es un término de ruido. Se elige un término único de la consulta original. Esto asume que solo un aspecto de la consulta, es decir, el representado por el término, es el más importante. Después de generar una variante de la consulta original, la combinamos con la consulta original utilizando un peso αSUB para no alejarnos demasiado. En este estudio, establecimos αSUB = 0.5. Por ejemplo, utilizando el lenguaje de consulta Indri [12], una variante de dejar uno fuera de la consulta inicial que omite el término \"ireland\" para el tema TREC 404 es: #weight(0.5 #combine(ireland peace talks) 0.5 #combine(peace talks)) 2.7 Combinando modelos de retroalimentación mejorados de múltiples variantes de consulta. Cuando se utilizan múltiples variantes de consulta, los modelos de retroalimentación mejorados resultantes se combinan utilizando la combinación de modelos bayesianos. Para hacer esto, tratamos cada palabra como un elemento a clasificar como perteneciente a una clase relevante o no relevante, y derivamos una probabilidad de clase para cada palabra combinando las puntuaciones de cada variante de consulta. Cada puntuación es dada por la probabilidad de ese término en la distribución de Dirichlet. Las puntuaciones de los términos se ponderan por el inverso de la varianza del término en los modelos de retroalimentación mejorada de la distribución de Dirichlet. La probabilidad previa de pertenencia de una palabra a la clase relevante se da por la probabilidad de la consulta original en todo el modelo de expansión mejorado. 3. EVALUACIÓN En esta sección presentamos resultados que confirman la utilidad de estimar una distribución del modelo de retroalimentación a partir del remuestreo ponderado de documentos mejor clasificados, y de combinar los modelos de retroalimentación obtenidos a partir de diferentes pequeños cambios en la consulta original. 3.1 Método general Evaluamos el rendimiento en un total de 350 consultas derivadas de cuatro conjuntos de temas de TREC: 51-200 (TREC-1&2), 351-400 (TREC-7), 401-450 (TREC-8) y 451-550 (wt10g, TREC-9&10). Elegimos estos por su contenido variado y propiedades del documento. Por ejemplo, los documentos wt10g son páginas web con una amplia variedad de temas y estilos, mientras que los documentos TREC-1&2 son artículos de noticias más homogéneos. La indexación y recuperación se realizó utilizando el sistema Indri en el kit de herramientas Lemur [12] [1]. Nuestras consultas se derivaron de las palabras en el campo del título de los temas de TREC. Las frases no fueron utilizadas. Para generar las consultas base que se pasaron a Indri, envolvimos los términos de la consulta con el operador #combine de Indri. Por ejemplo, la consulta inicial para el tema 404 es: #combine(ireland peace talks) Realizamos el stemming de Krovetz para todos los experimentos. Debido a que encontramos que el método de expansión de la base (Indri) funcionó mejor utilizando una lista de palabras vacías con el modelo de retroalimentación, todos los experimentos utilizaron una lista de 419 palabras comunes en inglés. Sin embargo, un efecto secundario interesante de nuestro enfoque de remuestreo es que tiende a eliminar muchas palabras vacías del modelo de retroalimentación, lo que hace que una lista de paradas sea menos crítica. Esto se discute más a fondo en la Sección 3.6. Método de retroalimentación de línea base 3.2 Para nuestro método de expansión de línea base, utilizamos un algoritmo incluido en Indri 1.0 como el método de expansión predeterminado. Este método primero selecciona términos utilizando un cálculo de logaritmo de probabilidades descrito por Ponte [14], pero asigna pesos finales a los términos utilizando el modelo de relevancia de Lavrenko [10]. Elegimos el método Indri porque proporciona una línea base consistentemente sólida, se basa en un enfoque de <br>modelado del lenguaje</br> y es fácil de experimentar con él. En una evaluación de TREC utilizando el corpus GOV2, el método fue uno de los mejores resultados, logrando un aumento del 19.8% en el MAP en comparación con el uso de consultas no expandidas. En este estudio, se logra una ganancia promedio en MAP del 17.25% en las cuatro colecciones. El método de expansión de Indri primero calcula una razón de logaritmos de probabilidades o(v) para cada término de expansión potencial v dado por o(v) = X D log p(v|D) p(v|C) (4) sobre todos los documentos D que contienen v, en la colección C. Luego, los candidatos a términos de expansión se ordenan por o(v) descendente, y se eligen los primeros m. Finalmente, los pesos de término r(v) utilizados en la consulta ampliada se calculan en base al modelo de relevancia r(v) = X D p(q|D)p(v|D) p(v) p(D) (5). La cantidad p(q|D) es la puntuación de probabilidad asignada al documento en la recuperación inicial. Utilizamos suavizado de Dirichlet de p(v|D) con μ = 1000. Este modelo de relevancia se combina luego con la consulta original utilizando interpolación lineal, ponderada por un parámetro α. Por defecto, utilizamos los 50 documentos principales para la retroalimentación y los 20 términos de expansión, con el parámetro de interpolación de retroalimentación α = 0.5 a menos que se indique lo contrario. Por ejemplo, la consulta expandida base para el tema 404 es: #weight(0.5 #combine(ireland peace talks) 0.5 #weight(0.10 ireland 0.08 peace 0.08 northern ...) 3.3 Rendimiento de la expansión. Medimos la efectividad de nuestros algoritmos de retroalimentación mediante dos criterios principales: precisión y robustez. La robustez, y el equilibrio entre precisión y robustez, se analiza en la Sección 3.4. En esta sección, examinamos la precisión promedio y la precisión en los 10 documentos principales (P10). También incluimos la recuperación de 1,000 documentos. Para cada consulta, obtuvimos un conjunto de B modelos de retroalimentación utilizando la línea base de Indri. Cada modelo de retroalimentación se obtuvo a partir de una muestra aleatoria de los k documentos principales tomados con reemplazo. Para estos experimentos, B = 30 y k = 50. Cada modelo de retroalimentación contenía 20 términos. En el lado de la consulta, utilizamos muestreo de dejar uno fuera (LOO) para crear las variantes de consulta. El muestreo de consulta de un solo término tuvo un rendimiento consistentemente peor en todas las colecciones, por lo que nuestros resultados se centran en el muestreo de LOO aquí. Utilizamos los métodos descritos en la Sección 2 para estimar un modelo de retroalimentación mejorado a partir de la distribución posterior de Dirichlet para cada variante de consulta, y para combinar los modelos de retroalimentación de todas las variantes de consulta. Llamamos a nuestro método expansión de remuestreo y lo denotamos como RS-FB aquí. Denominamos al método de retroalimentación de línea base Indri como Base-FB. Los resultados de aplicar tanto el método de expansión de línea base (Base-FB) como la expansión de remuestreo (RS-FB) se muestran en la Tabla 1. Observamos varias tendencias en esta tabla. Primero, la precisión promedio de RS-FB fue comparable a Base-FB, logrando una ganancia promedio del 17.6% en comparación con no utilizar expansión en las cuatro colecciones. La ganancia de expansión de la línea base de Indri fue del 17.25%. Además, el método RS-FB logró mejoras consistentes en P10 sobre Base-FB para cada conjunto de temas, con una mejora promedio del 6.89% sobre Base-FB para los 350 temas. La ganancia P10 más baja sobre Base-FB fue del +3.82% para TREC-7 y la más alta fue del +11.95% para wt10g. Finalmente, tanto Base-FB como RS-FB también mejoraron consistentemente la recuperación en comparación con no utilizar expansión, siendo Base-FB el que logró una mejor recuperación que RS-FB para todos los conjuntos de temas. 3.4 Robustez de recuperación. Utilizamos el término robustez para referirnos al rendimiento de precisión promedio en el peor de los casos de un algoritmo de retroalimentación. Idealmente, un método de retroalimentación sólido nunca debería funcionar peor que usar la consulta original, mientras que a menudo funciona mejor utilizando la expansión. Para evaluar la robustez en este estudio, utilizamos una medida muy simple llamada índice de robustez (RI). Para un conjunto de consultas Q, la medida de RI se define como: RI(Q) = n+ − n− |Q| (6) donde n+ es el número de consultas ayudadas por el método de retroalimentación y n− es el número de consultas perjudicadas. Aquí, por \"ayudar\" nos referimos a obtener una precisión promedio más alta como resultado de la retroalimentación. El valor de RI varía desde un mínimo de 3. A veces también se le llama índice de confiabilidad de mejora y fue utilizado en Sakai et al. [17]. Colección NoExp Base-FB RS-FB TREC 1&2 AvgP 0.1818 0.2419 (+33.04%) 0.2406 (+32.24%) P10 0.4443 0.4913 (+10.57%) 0.5363 (+17.83%) Recall 15084/37393 19172/37393 15396/37393 TREC 7 AvgP 0.1890 0.2175 (+15.07%) 0.2169 (+14.75%) P10 0.4200 0.4320 (+2.85%) 0.4480 (+6.67%) Recall 2179/4674 2608/4674 2487/4674 TREC 8 AvgP 0.2031 0.2361 (+16.25%) 0.2268 (+11.70%) P10 0.3960 0.4160 (+5.05%) 0.4340 (+9.59%) Recall 2144/4728 2642/4728 2485/4728 wt10g AvgP 0.1741 0.1829 (+5.06%) 0.1946 (+11.78%) P10 0.2760 0.2630 (-4.71%) 0.2960 (+7.24%) Recall 3361/5980 3725/5980 3664/5980 Tabla 1: Comparación de la retroalimentación base (Base-FB) y la retroalimentación utilizando re-muestreo (RS-FB). La mejora mostrada para BaseFB y RS-FB es relativa al uso de ninguna expansión. (a) TREC 1&2 (curva superior); TREC 8 (curva inferior) (b) TREC 7 (curva superior); wt10g (curva inferior) Figura 3: La compensación entre robustez y precisión promedio para diferentes corpora. El eje x muestra el cambio en la MAP al utilizar la expansión de la línea base con α = 0.5. El eje y muestra el Índice de Robustez (RI). Cada curva a través de los puntos no circulados muestra el compromiso entre RI/MAP utilizando la estrategia simple de pequeño α (ver texto) a medida que α disminuye de 0.5 a cero en la dirección de la flecha. Los puntos circulados representan los compromisos obtenidos al volver a muestrear la retroalimentación para α = 0.5. Colección N Base-FB RS-FB n− RI n− RI TREC 1&2 103 26 +0.495 15 +0.709 TREC 7 46 14 +0.391 10 +0.565 TREC 8 44 12 +0.455 12 +0.455 wt10g 91 48 -0.055 39 +0.143 Combinado 284 100 +0.296 76 +0.465 Tabla 2: Comparación del índice de robustez (RI) para retroalimentación de línea base (Base-FB) vs. retroalimentación de remuestreo (RS-FB). También se muestran el número real de consultas perjudicadas por la retroalimentación (n−) para cada método y colección. Las consultas para las cuales la precisión promedio inicial era insignificante (≤ 0.01) fueron ignoradas, dando como resultado el recuento de consultas restantes en la columna N, de -1.0, cuando todas las consultas son perjudicadas por el método de retroalimentación, a +1.0 cuando todas las consultas son ayudadas. La medida de RI no tiene en cuenta la magnitud o distribución de la cantidad de cambio en el conjunto Q. Sin embargo, es fácil de entender como una indicación general de robustez. Una forma obvia de mejorar el rendimiento en el peor de los casos de la retroalimentación es simplemente utilizar un parámetro de interpolación α fijo más pequeño, como α = 0.3, otorgando menos peso al modelo de retroalimentación (posiblemente arriesgado) y más al cuestionario original. Llamamos a esto la estrategia de pequeña α. Dado que también estamos reduciendo las ganancias potenciales cuando el modelo de retroalimentación es correcto, sin embargo, esperaríamos algún tipo de compensación entre la precisión promedio y la robustez. Por lo tanto, comparamos el equilibrio entre precisión y robustez entre nuestro algoritmo de retroalimentación de remuestreo y el método simple de pequeño α. Los resultados se resumen en la Figura 3. En la figura, la curva para cada conjunto de temas interpola entre los puntos de compensación, comenzando en x=0, donde α = 0.5, y continuando en la dirección de la flecha a medida que α disminuye y la consulta original recibe cada vez más peso. Como era de esperar, la robustez aumenta continuamente a medida que avanzamos a lo largo de la curva, pero la precisión media promedio generalmente disminuye a medida que se eliminan las ganancias del feedback. Para la comparación, se muestra el rendimiento del re-muestreo con retroalimentación en α = 0.5 para cada colección como el punto marcado con un círculo. Más alto y a la derecha es mejor. Esta figura muestra que el re-muestreo de retroalimentación ofrece un compromiso algo mejor que el enfoque de pequeño α para 3 de las 4 colecciones. Figura 4: Histograma que muestra la mayor robustez del re-muestreo de retroalimentación (RS-FB) sobre la retroalimentación base (Base-FB) para todos los conjuntos de datos combinados. Las consultas se agrupan por el % de cambio en AP en comparación con la consulta no expandida. Colección DS + QV DS + No QV TREC 1&2 AvgP 0.2406 0.2547 (+5.86%) P10 0.5263 0.5362 (+1.88%) RI 0.7087 0.6515 (-0.0572) TREC 7 AvgP 0.2169 0.2200 (+1.43%) P10 0.4480 0.4300 (-4.02%) RI 0.5652 0.2609 (-0.3043) TREC 8 AvgP 0.2268 0.2257 (-0.49%) P10 0.4340 0.4200 (-3.23%) RI 0.4545 0.4091 (-0.0454) wt10g AvgP 0.1946 0.1865 (-4.16%) P10 0.2960 0.2680 (-9.46%) RI 0.1429 0.0220 (-0.1209) Tabla 3: Comparación de la retroalimentación de remuestreo utilizando muestreo de documentos (DS) con (QV) y sin (No QV) combinar modelos de retroalimentación de múltiples variantes de consulta. La Tabla 2 muestra los puntajes del Índice de Robustez para Base-FB y RS-FB. El método de retroalimentación RS-FB obtuvo una mayor robustez que Base-FB en tres de los cuatro conjuntos de temas, con solo un rendimiento ligeramente peor en TREC-8. Una vista más detallada que muestra la distribución de los cambios relativos en AP se presenta en el histograma de la Figura 4. En comparación con Base-FB, el método RS-FB logra una reducción notable en el número de consultas significativamente afectadas por la expansión (es decir, donde AP se ve afectado en un 25% o más), al tiempo que conserva ganancias positivas en AP. 3.5 Efecto de los métodos de muestreo de consultas y documentos Dada la mayor robustez de nuestros algoritmos observada en la Sección 3.4, una pregunta importante es qué componente de nuestro sistema es responsable. ¿Es el uso de la re-muestreo de documentos, el uso de múltiples variantes de consulta, o algún otro factor? Los resultados en la Tabla 3 sugieren que la combinación de modelos basada en variantes de consulta puede ser en gran medida responsable de la mayor robustez. Cuando se desactivan las variantes de consulta y se utiliza la consulta original por sí sola con muestreo de documentos, hay poco cambio neto en la precisión promedio, una pequeña disminución en P10 para 3 de los 4 conjuntos de temas, pero una caída significativa en la robustez para todos los conjuntos de temas. En dos casos, la medida de RI disminuye en más del 50%. También examinamos el efecto del método de muestreo de documentos en la efectividad de la recuperación, utilizando dos estrategias diferentes. La estrategia de ponderación uniforme ignoró las puntuaciones de relevancia de la recuperación inicial y otorgó a cada documento en los primeros k la misma probabilidad de selección. Por el contrario, la estrategia de ponderación de puntajes de relevancia seleccionó documentos con una probabilidad proporcional a sus puntajes de relevancia. De esta manera, los documentos que tenían una clasificación más alta tenían más probabilidades de ser seleccionados. Los resultados se muestran en la Tabla 4. La estrategia de ponderación de la puntuación de relevancia tiene un mejor rendimiento en general, con puntajes de RI y P10 significativamente más altos en 3 de los 4 conjuntos de temas. La diferencia en la precisión promedio entre los métodos, sin embargo, es menos marcada. Esto sugiere que el peso uniforme actúa para aumentar la varianza en los resultados de recuperación: cuando la precisión promedio inicial es alta, hay muchos documentos relevantes en los primeros k y el muestreo uniforme puede proporcionar un modelo de relevancia más representativo que centrarse en los elementos altamente clasificados. Por otro lado, cuando la precisión inicial es baja, hay pocos documentos relevantes en las posiciones inferiores y el muestreo uniforme mezcla más de los documentos no relevantes. Por razones de espacio, solo resumimos nuestros hallazgos sobre el tamaño de la muestra aquí. El número de muestras tiene cierto efecto en la precisión cuando es menor a 10, pero el rendimiento se estabiliza alrededor de 15 a 20 muestras. Utilizamos 30 muestras para nuestros experimentos. Mucho más allá de este nivel, los beneficios adicionales de más muestras disminuyen a medida que la distribución inicial de puntajes se ajusta más estrechamente y aumenta el tiempo de procesamiento. 3.6 El efecto del remuestreo en la calidad del término de expansión Idealmente, un modelo de recuperación no debería requerir una lista de palabras vacías al estimar un modelo de relevancia: un modelo estadístico robusto debería reducir automáticamente el peso de las palabras vacías dependiendo del contexto. Las palabras vacías pueden perjudicar la retroalimentación si se seleccionan como términos de retroalimentación, ya que suelen ser malos discriminadores y desperdician espacios de términos valiosos. En la práctica, sin embargo, debido a que la mayoría de los métodos de selección de términos se asemejan a un tipo de ponderación tf · idf, a veces se pueden seleccionar términos con un idf bajo pero un tf muy alto como candidatos de términos de expansión. Esto sucede, por ejemplo, incluso con el enfoque del Modelo de Relevancia que forma parte de nuestra retroalimentación base. Para garantizar una línea base lo más sólida posible, utilizamos una lista de paradas para todos los experimentos reportados aquí. Si desactivamos la lista de palabras vacías, sin embargo, obtenemos resultados como los mostrados en la Tabla 5 donde cuatro de los diez términos de retroalimentación básica principales para el tema 60 de TREC (dijo, pero, su, no) son palabras vacías utilizando el método BaseFB. (Se seleccionaron los 100 términos de expansión principales para generar este ejemplo). El método Indris intenta abordar el problema de las palabras vacías aplicando un paso inicial basado en Ponte [14] para seleccionar términos menos comunes que tienen altas probabilidades logarítmicas de estar en los documentos mejor clasificados en comparación con toda la colección. Sin embargo, esto no resuelve completamente el problema de las palabras vacías, especialmente a medida que aumenta el número de términos de retroalimentación. Sin embargo, el uso de retroalimentación de remuestreo parece mitigar la ponderación de la colección QV + QV uniforme + ponderación de puntaje de relevancia TREC 1&2 AvgP 0.2545 0.2406 (-5.46%) P10 0.5369 0.5263 (-1.97%) RI 0.6212 0.7087 (+14.09%) TREC 7 AvgP 0.2174 0.2169 (-0.23%) P10 0.4320 0.4480 (+3.70%) RI 0.4783 0.5652 (+18.17%) TREC 8 AvgP 0.2267 0.2268 (+0.04%) P10 0.4120 0.4340 (+5.34%) RI 0.4545 0.4545 (+0.00%) wt10g AvgP 0.1808 0.1946 (+7.63%) P10 0.2680 0.2960 (+10.45%) RI 0.0220 0.1099 (+399.5%) Tabla 4: Comparación de muestreo de documentos uniforme y ponderado por relevancia. El cambio porcentual en comparación con el muestreo uniforme se muestra entre paréntesis. QV indica que se utilizaron variantes de consulta en ambas ejecuciones. FB de línea base p(wi|R) FB de remuestreo p(wi|R) dijo 0.055 corte 0.026 corte 0.055 pagar 0.018 pagar 0.034 federal 0.012 pero 0.026 educación 0.011 empleados 0.024 maestros 0.010 sus 0.024 empleados 0.010 no 0.023 caso 0.010 federal 0.021 sus 0.009 trabajadores 0.020 apelaciones 0.008 educación 0.020 sindicato 0.007 Tabla 5: Calidad del término de retroalimentación cuando no se utiliza una lista de paradas. Términos de retroalimentación para el tema 60 de TREC: pago por mérito vs antigüedad. el efecto de las palabras vacías automáticamente. En el ejemplo de la Tabla 5, la retroalimentación de remuestreo deja solo una palabra vacía (su) en las diez primeras posiciones. Observamos un comportamiento similar del término de retroalimentación en muchos otros temas. La razón de este efecto parece ser la interacción de la puntuación de selección de términos con el límite superior de términos m. Si bien la presencia e incluso la proporción de palabras vacías particulares es bastante estable en diferentes muestras de documentos, su posición relativa en la lista de los primeros m elementos no lo es, ya que se examinan conjuntos de documentos con diferentes números de candidatos de términos mejores y de menor frecuencia para cada muestra. Como resultado, si bien algunos stopwords pueden aparecer en cada conjunto de documentos muestreados, tiende a ocurrir que cualquier stopword dado caiga por debajo del umbral para múltiples muestras, lo que lleva a su clasificación como una característica de alta varianza y bajo peso. 4. TRABAJO RELACIONADO Nuestro enfoque está relacionado con trabajos previos de varias áreas de recuperación de información y aprendizaje automático. Nuestro uso de la variación de consultas fue inspirado por el trabajo de YomTov et al. [20], Carpineto et al. [5] y Amati et al. [2], entre otros. Estos estudios utilizan la idea de crear múltiples subconsultas y luego examinar la naturaleza de la superposición en los documentos y/o términos de expansión que resultan de cada subconsulta. La combinación de modelos se realiza utilizando heurísticas. En particular, los estudios de Amati et al. y Carpineto et al. investigaron la combinación de términos de métodos de distribución individuales utilizando una heurística de combinación de reordenamiento de términos. En un conjunto de temas de TREC encontraron una amplia variación promedio en la distancia de rango de los términos de diferentes métodos de expansión. Su método de combinación proporcionó modestas mejoras positivas en la precisión promedio. La idea de examinar la superposición entre listas de términos sugeridos también ha sido utilizada en enfoques tempranos de expansión de consultas. El método de Análisis de Contexto Local (LCA) de Xu y Crofts [19] incluye un factor en la fórmula de ponderación derivada empíricamente que hace que se prefieran los términos de expansión que tienen conexiones con múltiples términos de consulta. En el lado del documento, el trabajo reciente de Zhou & Croft [21] exploró la idea de agregar ruido a los documentos, volver a puntuarlos y utilizar la estabilidad de las clasificaciones resultantes como una estimación de la dificultad de la consulta. Esto está relacionado con nuestro uso de muestreo de documentos para estimar el riesgo del modelo de retroalimentación construido a partir de los diferentes conjuntos de documentos recuperados en la parte superior. Sakai et al. [17] propusieron un enfoque para mejorar la robustez de la retroalimentación de relevancia pseudo utilizando un método que llaman muestreo selectivo. La esencia de su método es que permiten omitir algunos documentos de alta clasificación, basándose en un criterio de agrupación, para seleccionar un conjunto de documentos más variado e innovador más adelante en la clasificación para su uso por un método tradicional de retroalimentación pseudo. Su estudio no encontró mejoras significativas ni en robustez (RI) ni en MAP en sus corpora. Greiff, Morgan y Ponte [8] exploraron el papel de la varianza en la ponderación de términos. En una serie de simulaciones que simplificaron el problema a documentos de 2 características, encontraron que la precisión promedio disminuye a medida que la varianza de la frecuencia de términos - ruido alto - aumenta. La reducción del peso de los términos con alta varianza resultó en una mejora de la precisión promedio. Esto parece estar en concordancia con nuestros propios hallazgos para los modelos de retroalimentación individual. Las estimaciones de la varianza de la producción han sido utilizadas recientemente para mejorar la clasificación de textos. Lee et al. [11] utilizaron estimaciones de varianza específicas de la consulta de las salidas del clasificador para realizar una combinación de modelos mejorada. En lugar de utilizar muestreo, pudieron derivar expresiones en forma cerrada para la varianza del clasificador asumiendo clasificadores base utilizando tipos simples de redes de inferencia. Ando y Zhang propusieron un método que ellos llaman retroalimentación estructural [3] y mostraron cómo aplicarlo a la expansión de consultas para la pista de Genómica de TREC. Utilizaron variaciones de consultas r para obtener R conjuntos diferentes Sr de documentos mejor clasificados que se han intersectado con los documentos mejor clasificados obtenidos de la consulta original qorig. Para cada Si, se calcula el vector centróide normalizado ˆwi de los documentos. El análisis de componentes principales (PCA) se aplica luego a los ˆwi para obtener la matriz Φ de los vectores singulares izquierdos φh que se utilizan para obtener la nueva consulta expandida qexp = qorig + ΦT Φqorig. (7) En el caso de H = 1, tenemos un único vector singular izquierdo φ: qexp = qorig + (φT qorig)φ, de modo que el producto punto φT qorig es un tipo de peso dinámico en la consulta expandida que se basa en la similitud de la consulta original con la consulta expandida. El uso de la varianza como medida de calidad del modelo de retroalimentación ocurre de forma indirecta a través de la aplicación de PCA. Sería interesante estudiar las conexiones entre este enfoque y nuestro propio método de ajuste de modelos. Finalmente, en los enfoques de <br>modelado del lenguaje</br> para retroalimentación, Tao y Zhai [18] describen un método para una retroalimentación más robusta que permite que cada documento tenga un α de retroalimentación diferente. Los pesos de retroalimentación se derivan automáticamente utilizando EM regularizado. La condición de parada de EM implica un equilibrio aproximadamente igual entre el modelo de consulta y expansión. Proponen adaptar el parámetro de parada η basado en una función de alguna medida de calidad de los documentos de retroalimentación. CONCLUSIONES Hemos presentado un nuevo enfoque para la retroalimentación de relevancia pseudo basado en el muestreo de documentos y consultas. El uso del muestreo es un dispositivo muy flexible y poderoso, motivado por nuestro deseo general de ampliar los modelos actuales de recuperación mediante la estimación del riesgo o la varianza asociada con los parámetros o la salida de los procesos de recuperación. Tales estimaciones de varianza, por ejemplo, pueden ser utilizadas de forma natural en un marco bayesiano para una mejor estimación y combinación de modelos. Aplicaciones como la expansión selectiva pueden ser implementadas de manera fundamentada. Si bien nuestro estudio utiliza el enfoque de <br>modelado del lenguaje</br> como marco para experimentos, hacemos pocas suposiciones sobre el funcionamiento real del algoritmo de retroalimentación. ",
            "candidates": [],
            "error": [
                [
                    "modelado de lenguaje",
                    "modelado de lenguaje",
                    "modelado del lenguaje",
                    "modelado del lenguaje",
                    "modelado del lenguaje"
                ]
            ]
        },
        "feedback distribution": {
            "translated_key": "distribución de retroalimentación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Estimation and Use of Uncertainty in Pseudo-relevance Feedback Kevyn Collins-Thompson and Jamie Callan Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213-8213 U.S.A. {kct | callan}@cs.cmu.edu ABSTRACT Existing pseudo-relevance feedback methods typically perform averaging over the top-retrieved documents, but ignore an important statistical dimension: the risk or variance associated with either the individual document models, or their combination.",
                "Treating the baseline feedback method as a black box, and the output feedback model as a random variable, we estimate a posterior distribution for the feedback model by resampling a given querys top-retrieved documents, using the posterior mean or mode as the enhanced feedback model.",
                "We then perform model combination over several enhanced models, each based on a slightly modified query sampled from the original query.",
                "We find that resampling documents helps increase individual feedback model precision by removing noise terms, while sampling from the query improves robustness (worst-case performance) by emphasizing terms related to multiple query aspects.",
                "The result is a meta-feedback algorithm that is both more robust and more precise than the original strong baseline method.",
                "Categories and Subject Descriptors: H.3.3 [Information Retrieval]: Retrieval Models General Terms: Algorithms, Experimentation 1.",
                "INTRODUCTION Uncertainty is an inherent feature of information retrieval.",
                "Not only do we not know the queries that will be presented to our retrieval algorithm ahead of time, but the users information need may be vague or incompletely specified by these queries.",
                "Even if the query were perfectly specified, language in the collection documents is inherently complex and ambiguous and matching such language effectively is a formidable problem by itself.",
                "With this in mind, we wish to treat many important quantities calculated by the retrieval system, whether a relevance score for a document, or a weight for a query expansion term, as random variables whose true value is uncertain but where the uncertainty about the true value may be quantified by replacing the fixed value with a probability distribution over possible values.",
                "In this way, retrieval algorithms may attempt to quantify the risk or uncertainty associated with their output rankings, or improve the stability or precision of their internal calculations.",
                "Current algorithms for pseudo-relevance feedback (PRF) tend to follow the same basic method whether we use vector space-based algorithms such as Rocchios formula [16], or more recent language modeling approaches such as Relevance Models [10].",
                "First, a set of top-retrieved documents is obtained from an initial query and assumed to approximate a set of relevant documents.",
                "Next, a single feedback model vector is computed according to some sort of average, centroid, or expectation over the set of possibly-relevant document models.",
                "For example, the document vectors may be combined with equal weighting, as in Rocchio, or by query likelihood, as may be done using the Relevance Model1 .",
                "The use of an expectation is reasonable for practical and theoretical reasons, but by itself ignores potentially valuable information about the risk of the feedback model.",
                "Our main hypothesis in this paper is that estimating the uncertainty in feedback is useful and leads to better individual feedback models and more robust combined models.",
                "Therefore, we propose a method for estimating uncertainty associated with an individual feedback model in terms of a posterior distribution over language models.",
                "To do this, we systematically vary the inputs to the baseline feedback method and fit a Dirichlet distribution to the output.",
                "We use the posterior mean or mode as the improved feedback model estimate.",
                "This process is shown in Figure 1.",
                "As we show later, the mean and mode may vary significantly from the single feedback model proposed by the baseline method.",
                "We also perform model combination using several improved feedback language models obtained by a small number of new queries sampled from the original query.",
                "A models weight combines two complementary factors: the models probability of generating the query, and the variance of the model, with high-variance models getting lower weight. 1 For example, an expected parameter vector conditioned on the query observation is formed from top-retrieved documents, which are treated as training strings (see [10], p. 62).",
                "Figure 1: Estimating the uncertainty of the feedback model for a single query. 2.",
                "SAMPLING-BASED FEEDBACK In Sections 2.1-2.5 we describe a general method for estimating a probability distribution over the set of possible language models.",
                "In Sections 2.6 and 2.7 we summarize how different query samples are used to generate multiple feedback models, which are then combined. 2.1 Modeling Feedback Uncertainty Given a query Q and a collection C, we assume a probabilistic retrieval system that assigns a real-valued document score f(D, Q) to each document D in C, such that the score is proportional to the estimated probability of relevance.",
                "We make no other assumptions about f(D, Q).",
                "The nature of f(D, Q) may be complex: for example, if the retrieval system supports structured query languages [12], then f(D, Q) may represent the output of an arbitrarily complex inference network defined by the structured query operators.",
                "In theory, the scoring function can vary from query to query, although in this study for simplicity we keep the scoring function the same for all queries.",
                "Our specific query method is given in Section 3.",
                "We treat the feedback algorithm as a black box and assume that the inputs to the feedback algorithm are the original query and the corresponding top-retrieved documents, with a score being given to each document.",
                "We assume that the output of the feedback algorithm is a vector of term weights to be used to add or reweight the terms in the representation of the original query, with the vector normalized to form a probability distribution.",
                "We view the the inputs to the feedback black box as random variables, and analyze the feedback model as a random variable that changes in response to changes in the inputs.",
                "Like the document scoring function f(D, Q), the feedback algorithm may implement a complex, non-linear scoring formula, and so as its inputs vary, the resulting feedback models may have a complex distribution over the space of feedback models (the sample space).",
                "Because of this potential complexity, we do not attempt to derive a posterior distribution in closed form, but instead use simulation.",
                "We call this distribution over possible feedback models the feedback model distribution.",
                "Our goal in this section is to estimate a useful approximation to the feedback model distribution.",
                "For a specific framework for experiments, we use the language modeling (LM) approach for information retrieval [15].",
                "The score of a document D with respect to a query Q and collection C is given by p(Q|D) with respect to language models ˆθQ and ˆθD estimated for the query and document respectively.",
                "We denote the set of k top-retrieved documents from collection C in response to Q by DQ(k, C).",
                "For simplicity, we assume that queries and documents are generated by multinomial distributions whose parameters are represented by unigram language models.",
                "To incorporate feedback in the LM approach, we assume a model-based scheme in which our goal is take the query and resulting ranked documents DQ(k, C) as input, and output an expansion language model ˆθE, which is then interpolated with the original query model ˆθQ: ˆθNew = (1 − α) · ˆθQ + α · ˆθE (1) This includes the possibility of α = 1 where the original query mode is completely replaced by the feedback model.",
                "Our sample space is the set of all possible language models LF that may be output as feedback models.",
                "Our approach is to take samples from this space and then fit a distribution to the samples using maximum likelihood.",
                "For simplicity, we start by assuming the latent <br>feedback distribution</br> has the form of a Dirichlet distribution.",
                "Although the Dirichlet is a unimodal distribution, and in general quite limited in its expressiveness in the sample space, it is a natural match for the multinomial language model, can be estimated quickly, and can capture the most salient features of confident and uncertain feedback models, such as the overall spread of the distibution. 2.2 Resampling document models We would like an approximation to the posterior distribution of the feedback model LF .",
                "To accomplish this, we apply a widely-used simulation technique called bootstrap sampling ([7], p. 474) on the input parameters, namely, the set of top-retrieved documents.",
                "Bootstrap sampling allows us to simulate the approximate effect of perturbing the parameters within the black box feedback algorithm by perturbing the inputs to that algorithm in a systematic way, while making no assumptions about the nature of the feedback algorithm.",
                "Specifically, we sample k documents with replacement from DQ(k, C), and calculate an expansion language model θb using the black box feedback method.",
                "We repeat this process B times to obtain a set of B feedback language models, to which we then fit a Dirichlet distribution.",
                "Typically B is in the range of 20 to 50 samples, with performance being relatively stable in this range.",
                "Note that instead of treating each top document as equally likely, we sample according to the estimated probabilities of relevance of each document in DQ(k, C).",
                "Thus, a document is more likely to be chosen the higher it is in the ranking. 2.3 Justification for a sampling approach The rationale for our sampling approach has two parts.",
                "First, we want to improve the quality of individual feedback models by smoothing out variation when the baseline feedback model is unstable.",
                "In this respect, our approach resembles bagging [4], an ensemble approach which generates multiple versions of a predictor by making bootstrap copies of the training set, and then averages the (numerical) predictors.",
                "In our application, top-retrieved documents can be seen as a kind of noisy training set for relevance.",
                "Second, sampling is an effective way to estimate basic properties of the feedback posterior distribution, which can then be used for improved model combination.",
                "For example, a model may be weighted by its prediction confidence, estimated as a function of the variability of the posterior around the model. foo2-401.map-Dim:5434,Size:12*12units,gaussianneighborhood (a) Topic 401 Foreign minorities, Germany foo2-402.map-Dim:5698,Size:12*12units,gaussianneighborhood (b) Topic 402 Behavioral genetics foo2-459.map-Dim:8969,Size:12*12units,gaussianneighborhood (c) Topic 459 When can a lender foreclose on property Figure 2: Visualization of expansion language model variance using self-organizing maps, showing the distribution of language models that results from resampling the inputs to the baseline expansion method.",
                "The language model that would have been chosen by the baseline expansion is at the center of each map.",
                "The similarity function is JensenShannon divergence. 2.4 Visualizing feedback distributions Before describing how we fit and use the Dirichlet distribution over feedback models, it is instructive to view some examples of actual feedback model distributions that result from bootstrap sampling the top-retrieved documents from different TREC topics.",
                "Each point in our sample space is a language model, which typically has several thousand dimensions.",
                "To help analyze the behavior of our method we used a Self-Organizing Map (via the SOM-PAK package [9]), to flatten and visualize the high-dimensional density function2 .",
                "The density maps for three TREC topics are shown in Figure 2 above.",
                "The dark areas represent regions of high similarity between language models.",
                "The light areas represent regions of low similarity - the valleys between clusters.",
                "Each diagram is centered on the language model that would have been chosen by the baseline expansion.",
                "A single peak (mode) is evident in some examples, but more complex structure appears in others.",
                "Also, while the distribution is usually close to the baseline feedback model, for some topics they are a significant distance apart (as measured by JensenShannon divergence), as in Subfigure 2c.",
                "In such cases, the mode or mean of the <br>feedback distribution</br> often performs significantly better than the baseline (and in a smaller proportion of cases, significantly worse). 2.5 Fitting a posterior <br>feedback distribution</br> After obtaining feedback model samples by resampling the feedback model inputs, we estimate the feedback distribution.",
                "We assume that the multinomial feedback models {ˆθ1, . . . , ˆθB} were generated by a latent Dirichlet distribution with parameters {α1, . . . , αN }.",
                "To estimate the {α1, . . . , αN }, we fit the Dirichlet parameters to the B language model samples according to maximum likelihood using a generalized Newton procedure, details of which are given in Minka [13].",
                "We assume a simple Dirichlet prior over the {α1, . . . , αN }, setting each to αi = μ · p(wi | C), where μ is a parameter and p(· | C) is the collection language model estimated from a set of documents from collection C. The parameter fitting converges very quickly - typically just 2 or 2 Because our points are language models in the multinomial simplex, we extended SOM-PAK to support JensenShannon divergence, a widely-used similarity measure between probability distributions. 3 iterations are enough - so that it is practical to apply at query-time when computational overhead must be small.",
                "In practice, we can restrict the calculation to the vocabulary of the top-retrieved documents, instead of the entire collection.",
                "Note that for this step we are re-using the existing retrieved documents and not performing additional queries.",
                "Given the parameters of an N-dimensional Dirichlet distribution Dir(α) the mean μ and mode x vectors are easy to calculate and are given respectively by μi = αiP αi (2) and xi = αi−1P αi−N . (3) We can then choose the language model at the mean or the mode of the posterior as the final enhanced feedback model. (We found the mode to give slightly better performance.)",
                "For information retrieval, the number of samples we will have available is likely to be quite small for performance reasons - usually less than ten.",
                "Moreover, while random sampling is useful in certain cases, it is perfectly acceptable to allow deterministic sampling distributions, but these must be designed carefully in order to approximate an accurate output variance.",
                "We leave this for future study. 2.6 Query variants We use the following methods for generating variants of the original query.",
                "Each variant corresponds to a different assumption about which aspects of the original query may be important.",
                "This is a form of deterministic sampling.",
                "We selected three simple methods that cover complimentary assumptions about the query.",
                "No-expansion Use only the original query.",
                "The assumption is that the given terms are a complete description of the information need.",
                "Leave-one-out A single term is left out of the original query.",
                "The assumption is that one of the query terms is a noise term.",
                "Single-term A single term is chosen from the original query.",
                "This assumes that only one aspect of the query, namely, that represented by the term, is most important.",
                "After generating a variant of the original query, we combine it with the original query using a weight αSUB so that we do not stray too far.",
                "In this study, we set αSUB = 0.5.",
                "For example, using the Indri [12] query language, a leave-oneout variant of the initial query that omits the term ireland for TREC topic 404 is: #weight(0.5 #combine(ireland peace talks) 0.5 #combine(peace talks)) 2.7 Combining enhanced feedback models from multiple query variants When using multiple query variants, the resulting enhanced feedback models are combined using Bayesian model combination.",
                "To do this, we treat each word as an item to be classified as belonging to a relevant or non-relevant class, and derive a class probability for each word by combining the scores from each query variant.",
                "Each score is given by that terms probability in the Dirichlet distribution.",
                "The term scores are weighted by the inverse of the variance of the term in the enhanced feedback models Dirichlet distribution.",
                "The prior probability of a words membership in the relevant class is given by the probability of the original query in the entire enhanced expansion model. 3.",
                "EVALUATION In this section we present results confirming the usefulness of estimating a feedback model distribution from weighted resampling of top-ranked documents, and of combining the feedback models obtained from different small changes in the original query. 3.1 General method We evaluated performance on a total of 350 queries derived from four sets of TREC topics: 51-200 (TREC-1&2), 351-400 (TREC-7), 401-450 (TREC-8), and 451-550 (wt10g, TREC-9&10).",
                "We chose these for their varied content and document properties.",
                "For example, wt10g documents are Web pages with a wide variety of subjects and styles while TREC-1&2 documents are more homogeneous news articles.",
                "Indexing and retrieval was performed using the Indri system in the Lemur toolkit [12] [1].",
                "Our queries were derived from the words in the title field of the TREC topics.",
                "Phrases were not used.",
                "To generate the baseline queries passed to Indri, we wrapped the query terms with Indris #combine operator.",
                "For example, the initial query for topic 404 is: #combine(ireland peace talks) We performed Krovetz stemming for all experiments.",
                "Because we found that the baseline (Indri) expansion method performed better using a stopword list with the feedback model, all experiments used a stoplist of 419 common English words.",
                "However, an interesting side-effect of our resampling approach is that it tends to remove many stopwords from the feedback model, making a stoplist less critical.",
                "This is discussed further in Section 3.6. 3.2 Baseline feedback method For our baseline expansion method, we use an algorithm included in Indri 1.0 as the default expansion method.",
                "This method first selects terms using a log-odds calculation described by Ponte [14], but assigns final term weights using Lavrenkos relevance model[10].",
                "We chose the Indri method because it gives a consistently strong baseline, is based on a language modeling approach, and is simple to experiment with.",
                "In a TREC evaluation using the GOV2 corpus [6], the method was one of the topperforming runs, achieving a 19.8% gain in MAP compared to using unexpanded queries.",
                "In this study, it achieves an average gain in MAP of 17.25% over the four collections.",
                "Indris expansion method first calculates a log-odds ratio o(v) for each potential expansion term v given by o(v) = X D log p(v|D) p(v|C) (4) over all documents D containing v, in collection C. Then, the expansion term candidates are sorted by descending o(v), and the top m are chosen.",
                "Finally, the term weights r(v) used in the expanded query are calculated based on the relevance model r(v) = X D p(q|D)p(v|D) p(v) p(D) (5) The quantity p(q|D) is the probability score assigned to the document in the initial retrieval.",
                "We use Dirichlet smoothing of p(v|D) with μ = 1000.",
                "This relevance model is then combined with the original query using linear interpolation, weighted by a parameter α.",
                "By default we used the top 50 documents for feedback and the top 20 expansion terms, with the feedback interpolation parameter α = 0.5 unless otherwise stated.",
                "For example, the baseline expanded query for topic 404 is: #weight(0.5 #combine(ireland peace talks) 0.5 #weight(0.10 ireland 0.08 peace 0.08 northern ...) 3.3 Expansion performance We measure our feedback algorithms effectiveness by two main criteria: precision, and robustness.",
                "Robustness, and the tradeoff between precision and robustness, is analyzed in Section 3.4.",
                "In this section, we examine average precision and precision in the top 10 documents (P10).",
                "We also include recall at 1,000 documents.",
                "For each query, we obtained a set of B feedback models using the Indri baseline.",
                "Each feedback model was obtained from a random sample of the top k documents taken with replacement.",
                "For these experiments, B = 30 and k = 50.",
                "Each feedback model contained 20 terms.",
                "On the query side, we used leave-one-out (LOO) sampling to create the query variants.",
                "Single-term query sampling had consistently worse performance across all collections and so our results here focus on LOO sampling.",
                "We used the methods described in Section 2 to estimate an enhanced feedback model from the Dirichlet posterior distribution for each query variant, and to combine the feedback models from all the query variants.",
                "We call our method resampling expansion and denote it as RS-FB here.",
                "We denote the Indri baseline feedback method as Base-FB.",
                "Results from applying both the baseline expansion method (Base-FB) and resampling expansion (RS-FB) are shown in Table 1.",
                "We observe several trends in this table.",
                "First, the average precision of RS-FB was comparable to Base-FB, achieving an average gain of 17.6% compared to using no expansion across the four collections.",
                "The Indri baseline expansion gain was 17.25%.",
                "Also, the RS-FB method achieved consistent improvements in P10 over Base-FB for every topic set, with an average improvement of 6.89% over Base-FB for all 350 topics.",
                "The lowest P10 gain over Base-FB was +3.82% for TREC-7 and the highest was +11.95% for wt10g.",
                "Finally, both Base-FB and RS-FB also consistently improved recall over using no expansion, with Base-FB achieving better recall than RS-FB for all topic sets. 3.4 Retrieval robustness We use the term robustness to mean the worst-case average precision performance of a feedback algorithm.",
                "Ideally, a robust feedback method would never perform worse than using the original query, while often performing better using the expansion.",
                "To evaluate robustness in this study, we use a very simple measure called the robustness index (RI)3 .",
                "For a set of queries Q, the RI measure is defined as: RI(Q) = n+ − n− |Q| (6) where n+ is the number of queries helped by the feedback method and n− is the number of queries hurt.",
                "Here, by helped we mean obtaining a higher average precision as a result of feedback.",
                "The value of RI ranges from a minimum 3 This is sometimes also called the reliability of improvement index and was used in Sakai et al. [17].",
                "Collection NoExp Base-FB RS-FB TREC 1&2 AvgP 0.1818 0.2419 (+33.04%) 0.2406 (+32.24%) P10 0.4443 0.4913 (+10.57%) 0.5363 (+17.83%) Recall 15084/37393 19172/37393 15396/37393 TREC 7 AvgP 0.1890 0.2175 (+15.07%) 0.2169 (+14.75%) P10 0.4200 0.4320 (+2.85%) 0.4480 (+6.67%) Recall 2179/4674 2608/4674 2487/4674 TREC 8 AvgP 0.2031 0.2361 (+16.25%) 0.2268 (+11.70%) P10 0.3960 0.4160 (+5.05%) 0.4340 (+9.59%) Recall 2144/4728 2642/4728 2485/4728 wt10g AvgP 0.1741 0.1829 (+5.06%) 0.1946 (+11.78%) P10 0.2760 0.2630 (-4.71%) 0.2960 (+7.24%) Recall 3361/5980 3725/5980 3664/5980 Table 1: Comparison of baseline (Base-FB) feedback and feedback using re-sampling (RS-FB).",
                "Improvement shown for BaseFB and RS-FB is relative to using no expansion. (a) TREC 1&2 (upper curve); TREC 8 (lower curve) (b) TREC 7 (upper curve); wt10g (lower curve) Figure 3: The trade-off between robustness and average precision for different corpora.",
                "The x-axis gives the change in MAP over using baseline expansion with α = 0.5.",
                "The yaxis gives the Robustness Index (RI).",
                "Each curve through uncircled points shows the RI/MAP tradeoff using the simple small-α strategy (see text) as α decreases from 0.5 to zero in the direction of the arrow.",
                "Circled points represent the tradeoffs obtained by resampling feedback for α = 0.5.",
                "Collection N Base-FB RS-FB n− RI n− RI TREC 1&2 103 26 +0.495 15 +0.709 TREC 7 46 14 +0.391 10 +0.565 TREC 8 44 12 +0.455 12 +0.455 wt10g 91 48 -0.055 39 +0.143 Combined 284 100 +0.296 76 +0.465 Table 2: Comparison of robustness index (RI) for baseline feedback (Base-FB) vs. resampling feedback (RS-FB).",
                "Also shown are the actual number of queries hurt by feedback (n−) for each method and collection.",
                "Queries for which initial average precision was negligible (≤ 0.01) were ignored, giving the remaining query count in column N. of −1.0, when all queries are hurt by the feedback method, to +1.0 when all queries are helped.",
                "The RI measure does not take into account the magnitude or distribution of the amount of change across the set Q.",
                "However, it is easy to understand as a general indication of robustness.",
                "One obvious way to improve the worst-case performance of feedback is simply to use a smaller fixed α interpolation parameter, such as α = 0.3, placing less weight on the (possibly risky) feedback model and more on the original query.",
                "We call this the small-α strategy.",
                "Since we are also reducing the potential gains when the feedback model is right, however, we would expect some trade-off between average precision and robustness.",
                "We therefore compared the precision/robustness trade-off between our resampling feedback algorithm, and the simple small-α method.",
                "The results are summarized in Figure 3.",
                "In the figure, the curve for each topic set interpolates between trade-off points, beginning at x=0, where α = 0.5, and continuing in the direction of the arrow as α decreases and the original query is given more and more weight.",
                "As expected, robustness continuously increases as we move along the curve, but mean average precision generally drops as the gains from feedback are eliminated.",
                "For comparison, the performance of resampling feedback at α = 0.5 is shown for each collection as the circled point.",
                "Higher and to the right is better.",
                "This figure shows that resampling feedback gives a somewhat better trade-off than the small-α approach for 3 of the 4 collections.",
                "Figure 4: Histogram showing improved robustness of resampling feedback (RS-FB) over baseline feedback (Base-FB) for all datasets combined.",
                "Queries are binned by % change in AP compared to the unexpanded query.",
                "Collection DS + QV DS + No QV TREC 1&2 AvgP 0.2406 0.2547 (+5.86%) P10 0.5263 0.5362 (+1.88%) RI 0.7087 0.6515 (-0.0572) TREC 7 AvgP 0.2169 0.2200 (+1.43%) P10 0.4480 0.4300 (-4.02%) RI 0.5652 0.2609 (-0.3043) TREC 8 AvgP 0.2268 0.2257 (-0.49%) P10 0.4340 0.4200 (-3.23%) RI 0.4545 0.4091 (-0.0454) wt10g AvgP 0.1946 0.1865 (-4.16%) P10 0.2960 0.2680 (-9.46%) RI 0.1429 0.0220 (-0.1209) Table 3: Comparison of resampling feedback using document sampling (DS) with (QV) and without (No QV) combining feedback models from multiple query variants.",
                "Table 2 gives the Robustness Index scores for Base-FB and RS-FB.",
                "The RS-FB feedback method obtained higher robustness than Base-FB on three of the four topic sets, with only slightly worse performance on TREC-8.",
                "A more detailed view showing the distribution over relative changes in AP is given by the histogram in Figure 4.",
                "Compared to Base-FB, the RS-FB method achieves a noticable reduction in the number of queries significantly hurt by expansion (i.e. where AP is hurt by 25% or more), while preserving positive gains in AP. 3.5 Effect of query and document sampling methods Given our algorithms improved robustness seen in Section 3.4, an important question is what component of our system is responsible.",
                "Is it the use of document re-sampling, the use of multiple query variants, or some other factor?",
                "The results in Table 3 suggest that the model combination based on query variants may be largely account for the improved robustness.",
                "When query variants are turned off and the original query is used by itself with document sampling, there is little net change in average precision, a small decrease in P10 for 3 out of the 4 topic sets, but a significant drop in robustness for all topic sets.",
                "In two cases, the RI measure drops by more than 50%.",
                "We also examined the effect of the document sampling method on retrieval effectiveness, using two different strategies.",
                "The uniform weighting strategy ignored the relevance scores from the initial retrieval and gave each document in the top k the same probability of selection.",
                "In contrast, the relevance-score weighting strategy chose documents with probability proportional to their relevance scores.",
                "In this way, documents that were more highly ranked were more likely to be selected.",
                "Results are shown in Table 4.",
                "The relevance-score weighting strategy performs better overall, with significantly higher RI and P10 scores on 3 of the 4 topic sets.",
                "The difference in average precision between the methods, however, is less marked.",
                "This suggests that uniform weighting acts to increase variance in retrieval results: when initial average precision is high, there are many relevant documents in the top k and uniform sampling may give a more representative relevance model than focusing on the highly-ranked items.",
                "On the other hand, when initial precision is low, there are few relevant documents in the bottom ranks and uniform sampling mixes in more of the non-relevant documents.",
                "For space reasons we only summarize our findings on sample size here.",
                "The number of samples has some effect on precision when less than 10, but performance stabilizes at around 15 to 20 samples.",
                "We used 30 samples for our experiments.",
                "Much beyond this level, the additional benefits of more samples decrease as the initial score distribution is more closely fit and the processing time increases. 3.6 The effect of resampling on expansion term quality Ideally, a retrieval model should not require a stopword list when estimating a model of relevance: a robust statistical model should down-weight stopwords automatically depending on context.",
                "Stopwords can harm feedback if selected as feedback terms, because they are typically poor discriminators and waste valuable term slots.",
                "In practice, however, because most term selection methods resemble a tf · idf type of weighting, terms with low idf but very high tf can sometimes be selected as expansion term candidates.",
                "This happens, for example, even with the Relevance Model approach that is part of our baseline feedback.",
                "To ensure as strong a baseline as possible, we use a stoplist for all experiments reported here.",
                "If we turn off the stopword list, however, we obtain results such as those shown in Table 5 where four of the top ten baseline feedback terms for TREC topic 60 (said, but, their, not) are stopwords using the BaseFB method. (The top 100 expansion terms were selected to generate this example.)",
                "Indris method attempts to address the stopword problem by applying an initial step based on Ponte [14] to select less-common terms that have high log-odds of being in the top-ranked documents compared to the whole collection.",
                "Nevertheless, this does not overcome the stopword problem completely, especially as the number of feedback terms grows.",
                "Using resampling feedback, however, appears to mitigate Collection QV + Uniform QV + Relevance-score weighting weighting TREC 1&2 AvgP 0.2545 0.2406 (-5.46%) P10 0.5369 0.5263 (-1.97%) RI 0.6212 0.7087 (+14.09%) TREC 7 AvgP 0.2174 0.2169 (-0.23%) P10 0.4320 0.4480 (+3.70%) RI 0.4783 0.5652 (+18.17%) TREC 8 AvgP 0.2267 0.2268 (+0.04%) P10 0.4120 0.4340 (+5.34%) RI 0.4545 0.4545 (+0.00%) wt10g AvgP 0.1808 0.1946 (+7.63%) P10 0.2680 0.2960 (+10.45%) RI 0.0220 0.1099 (+399.5%) Table 4: Comparison of uniform and relevance-weighted document sampling.",
                "The percentage change compared to uniform sampling is shown in parentheses.",
                "QV indicates that query variants were used in both runs.",
                "Baseline FB p(wi|R) Resampling FB p(wi|R) said 0.055 court 0.026 court 0.055 pay 0.018 pay 0.034 federal 0.012 but 0.026 education 0.011 employees 0.024 teachers 0.010 their 0.024 employees 0.010 not 0.023 case 0.010 federal 0.021 their 0.009 workers 0.020 appeals 0.008 education 0.020 union 0.007 Table 5: Feedback term quality when a stoplist is not used.",
                "Feedback terms for TREC topic 60: merit pay vs seniority. the effect of stopwords automatically.",
                "In the example of Table 5, resampling feedback leaves only one stopword (their) in the top ten.",
                "We observed similar feedback term behavior across many other topics.",
                "The reason for this effect appears to be the interaction of the term selection score with the top-m term cutoff.",
                "While the presence and even proportion of particular stopwords is fairly stable across different document samples, their relative position in the top-m list is not, as sets of documents with varying numbers of better, lower-frequency term candidates are examined for each sample.",
                "As a result, while some number of stopwords may appear in each sampled document set, any given stopword tends to fall below the cutoff for multiple samples, leading to its classification as a high-variance, low-weight feature. 4.",
                "RELATED WORK Our approach is related to previous work from several areas of information retrieval and machine learning.",
                "Our use of query variation was inspired by the work of YomTov et al. [20], Carpineto et al. [5], and Amati et al. [2], among others.",
                "These studies use the idea of creating multiple subqueries and then examining the nature of the overlap in the documents and/or expansion terms that result from each subquery.",
                "Model combination is performed using heuristics.",
                "In particular, the studies of Amati et al. and Carpineto et al. investigated combining terms from individual distributional methods using a term-reranking combination heuristic.",
                "In a set of TREC topics they found wide average variation in the rank-distance of terms from different expansion methods.",
                "Their combination method gave modest positive improvements in average precision.",
                "The idea of examining the overlap between lists of suggested terms has also been used in early query expansion approaches.",
                "Xu and Crofts method of Local Context Analysis (LCA) [19] includes a factor in the empirically-derived weighting formula that causes expansion terms to be preferred that have connections to multiple query terms.",
                "On the document side, recent work by Zhou & Croft [21] explored the idea of adding noise to documents, re-scoring them, and using the stability of the resulting rankings as an estimate of query difficulty.",
                "This is related to our use of document sampling to estimate the risk of the feedback model built from the different sets of top-retrieved documents.",
                "Sakai et al. [17] proposed an approach to improving the robustness of pseudo-relevance feedback using a method they call selective sampling.",
                "The essence of their method is that they allow skipping of some top-ranked documents, based on a clustering criterion, in order to select a more varied and novel set of documents later in the ranking for use by a traditional pseudo-feedback method.",
                "Their study did not find significant improvements in either robustness (RI) or MAP on their corpora.",
                "Greiff, Morgan and Ponte [8] explored the role of variance in term weighting.",
                "In a series of simulations that simplified the problem to 2-feature documents, they found that average precision degrades as term frequency variance - high noiseincreases.",
                "Downweighting terms with high variance resulted in improved average precision.",
                "This seems in accord with our own findings for individual feedback models.",
                "Estimates of output variance have recently been used for improved text classification.",
                "Lee et al. [11] used queryspecific variance estimates of classifier outputs to perform improved model combination.",
                "Instead of using sampling, they were able to derive closed-form expressions for classifier variance by assuming base classifiers using simple types of inference networks.",
                "Ando and Zhang proposed a method that they call structural feedback [3] and showed how to apply it to query expansion for the TREC Genomics Track.",
                "They used r query variations to obtain R different sets Sr of top-ranked documents that have been intersected with the top-ranked documents obtained from the original query qorig.",
                "For each Si, the normalized centroid vector ˆwi of the documents is calculated.",
                "Principal component analysis (PCA) is then applied to the ˆwi to obtain the matrix Φ of H left singular vectors φh that are used to obtain the new, expanded query qexp = qorig + ΦT Φqorig. (7) In the case H = 1, we have a single left singular vector φ: qexp = qorig + (φT qorig)φ so that the dot product φT qorig is a type of dynamic weight on the expanded query that is based on the similarity of the original query to the expanded query.",
                "The use of variance as a feedback model quality measure occurs indirectly through the application of PCA.",
                "It would be interesting to study the connections between this approach and our own modelfitting method.",
                "Finally, in language modeling approaches to feedback, Tao and Zhai [18] describe a method for more robust feedback that allows each document to have a different feedback α.",
                "The feedback weights are derived automatically using regularized EM.",
                "A roughly equal balance of query and expansion model is implied by their EM stopping condition.",
                "They propose tailoring the stopping parameter η based on a function of some quality measure of feedback documents. 5.",
                "CONCLUSIONS We have presented a new approach to pseudo-relevance feedback based on document and query sampling.",
                "The use of sampling is a very flexible and powerful device and is motivated by our general desire to extend current models of retrieval by estimating the risk or variance associated with the parameters or output of retrieval processes.",
                "Such variance estimates, for example, may be naturally used in a Bayesian framework for improved model estimation and combination.",
                "Applications such as selective expansion may then be implemented in a principled way.",
                "While our study uses the language modeling approach as a framework for experiments, we make few assumptions about the actual workings of the feedback algorithm.",
                "We believe it is likely that any reasonably effective baseline feedback algorithm would benefit from our approach.",
                "Our results on standard TREC collections show that our framework improves the robustness of a strong baseline feedback method across a variety of collections, without sacrificing average precision.",
                "It also gives small but consistent gains in top10 precision.",
                "In future work, we envision an investigation into how varying the set of sampling methods used and the number of samples controls the trade-off between robustness, accuracy, and efficiency.",
                "Acknowledgements We thank Paul Bennett for valuable discussions related to this work, which was supported by NSF grants #IIS-0534345 and #CNS-0454018, and U.S. Dept. of Education grant #R305G03123.",
                "Any opinions, findings, and conclusions or recommendations expressed in this material are the authors. and do not necessarily reflect those of the sponsors. 6.",
                "REFERENCES [1] The Lemur toolkit for language modeling and retrieval. http://www.lemurproject.org. [2] G. Amati, C. Carpineto, and G. Romano.",
                "Query difficulty, robustness, and selective application of query expansion.",
                "In Proc. of the 25th European Conf. on Information Retrieval (ECIR 2004), pages 127-137. [3] R. K. Ando and T. Zhang.",
                "A high-performance semi-supervised learning method for text chunking.",
                "In Proc. of the 43rd Annual Meeting of the ACL, pages 1-9, June 2005. [4] L. Breiman.",
                "Bagging predictors.",
                "Machine Learning, 24(2):123-140, 1996. [5] C. Carpineto, G. Romano, and V. Giannini.",
                "Improving retrieval feedback with multiple term-ranking function combination.",
                "ACM Trans.",
                "Info.",
                "Systems, 20(3):259 - 290. [6] K. Collins-Thompson, P. Ogilvie, and J. Callan.",
                "Initial results with structured queries and language models on half a terabyte of text.",
                "In Proc. of 2005 Text REtrieval Conference.",
                "NIST Special Publication. [7] R. O. Duda, P. E. Hart, and D. G. Stork.",
                "Pattern Classification.",
                "Wiley and Sons, 2nd edition, 2001. [8] W. R. Greiff, W. T. Morgan, and J. M. Ponte.",
                "The role of variance in term weighting for probabilistic information retrieval.",
                "In Proc. of the 11th Intl.",
                "Conf. on Info. and Knowledge Mgmt. (CIKM 2002), pages 252-259. [9] T. Kohonen, J. Hynninen, J. Kangas, and J. Laaksonen.",
                "SOMPAK: The self-organizing map program package.",
                "Technical Report A31, Helsinki University of Technology, 1996. http://www.cis.hut.fi/research/papers/som tr96.ps.Z. [10] V. Lavrenko.",
                "A Generative Theory of Relevance.",
                "PhD thesis, University of Massachusetts, Amherst, 2004. [11] C.-H. Lee, R. Greiner, and S. Wang.",
                "Using query-specific variance estimates to combine Bayesian classifiers.",
                "In Proc. of the 23rd Intl.",
                "Conf. on Machine Learning (ICML 2006), pages 529-536. [12] D. Metzler and W. B. Croft.",
                "Combining the language model and inference network approaches to retrieval.",
                "Info.",
                "Processing and Mgmt., 40(5):735-750, 2004. [13] T. Minka.",
                "Estimating a Dirichlet distribution.",
                "Technical report, 2000. http://research.microsoft.com/ minka/papers/dirichlet. [14] J. Ponte.",
                "Advances in Information Retrieval, chapter Language models for relevance feedback, pages 73-96. 2000.",
                "W.B.",
                "Croft, ed. [15] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In Proc. of the 1998 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 275-281. [16] J. Rocchio.",
                "The SMART Retrieval System, chapter Relevance Feedback in Information Retrieval, pages 313-323.",
                "Prentice-Hall, 1971.",
                "G. Salton, ed. [17] T. Sakai, T. Manabe, and M. Koyama.",
                "Flexible pseudo-relevance feedback via selective sampling.",
                "ACM Transactions on Asian Language Information Processing (TALIP), 4(2):111-135, 2005. [18] T. Tao and C. Zhai.",
                "Regularized estimation of mixture models for robust pseudo-relevance feedback.",
                "In Proc. of the 2006 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 162-169. [19] J. Xu and W. B. Croft.",
                "Improving the effectiveness of information retrieval with local context analysis.",
                "ACM Trans.",
                "Inf.",
                "Syst., 18(1):79-112, 2000. [20] E. YomTov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty.",
                "In Proc. of the 2005 ACM SIGIR Conf. on Research and Development in Information Retrieval, pages 512-519. [21] Y. Zhou and W. B. Croft.",
                "Ranking robustness: a novel framework to predict query performance.",
                "In Proc. of the 15th ACM Intl.",
                "Conf. on Information and Knowledge Mgmt. (CIKM 2006), pages 567-574."
            ],
            "original_annotated_samples": [
                "For simplicity, we start by assuming the latent <br>feedback distribution</br> has the form of a Dirichlet distribution.",
                "In such cases, the mode or mean of the <br>feedback distribution</br> often performs significantly better than the baseline (and in a smaller proportion of cases, significantly worse). 2.5 Fitting a posterior <br>feedback distribution</br> After obtaining feedback model samples by resampling the feedback model inputs, we estimate the feedback distribution."
            ],
            "translated_annotated_samples": [
                "Para simplificar, comenzamos asumiendo que la <br>distribución de retroalimentación</br> latente tiene la forma de una distribución de Dirichlet.",
                "En tales casos, la moda o la media de la <br>distribución de retroalimentación</br> a menudo funcionan significativamente mejor que el valor base (y en una proporción menor de casos, significativamente peor). 2.5 Ajuste de una <br>distribución de retroalimentación</br> posterior Después de obtener muestras del modelo de retroalimentación mediante el remuestreo de las entradas del modelo de retroalimentación, estimamos la distribución de retroalimentación."
            ],
            "translated_text": "Estimación y Uso de la Incertidumbre en la Retroalimentación de Pseudo-Relevancia Kevyn Collins-Thompson y Jamie Callan Instituto de Tecnologías del Lenguaje Escuela de Ciencias de la Computación Universidad Carnegie Mellon Pittsburgh, PA 15213-8213 EE. UU. {kct | callan}@cs.cmu.edu RESUMEN Los métodos existentes de retroalimentación de pseudo-relevancia suelen realizar un promedio de los documentos mejor clasificados, pero ignoran una dimensión estadística importante: el riesgo o la varianza asociada tanto a los modelos de documentos individuales como a su combinación. Tratando el método de retroalimentación de línea base como una caja negra, y el modelo de retroalimentación de salida como una variable aleatoria, estimamos una distribución posterior para el modelo de retroalimentación mediante el remuestreo de los documentos mejor recuperados de una consulta dada, utilizando la media o la moda posterior como el modelo de retroalimentación mejorado. Luego realizamos la combinación de modelos sobre varios modelos mejorados, cada uno basado en una consulta ligeramente modificada muestreada de la consulta original. Observamos que el remuestreo de documentos ayuda a aumentar la precisión del modelo de retroalimentación individual al eliminar términos de ruido, mientras que el muestreo de la consulta mejora la robustez (rendimiento en el peor de los casos) al enfatizar términos relacionados con múltiples aspectos de la consulta. El resultado es un algoritmo de meta-retroalimentación que es tanto más robusto como más preciso que el método de referencia original. Categorías y Descriptores de Asignaturas: H.3.3 [Recuperación de Información]: Modelos de Recuperación Términos Generales: Algoritmos, Experimentación 1. INTRODUCCIÓN La incertidumbre es una característica inherente de la recuperación de información. No solo no sabemos las consultas que se presentarán a nuestro algoritmo de recuperación con anticipación, sino que la necesidad de información de los usuarios puede ser vaga o estar incompletamente especificada por estas consultas. Incluso si la consulta estuviera perfectamente especificada, el lenguaje en los documentos de la colección es inherentemente complejo y ambiguo, y hacer coincidir dicho lenguaje de manera efectiva es un problema formidable por sí mismo. Con esto en mente, deseamos tratar muchas cantidades importantes calculadas por el sistema de recuperación, ya sea un puntaje de relevancia para un documento, o un peso para un término de expansión de consulta, como variables aleatorias cuyo valor real es incierto pero donde la incertidumbre sobre el valor real puede ser cuantificada reemplazando el valor fijo con una distribución de probabilidad sobre posibles valores. De esta manera, los algoritmos de recuperación pueden intentar cuantificar el riesgo o la incertidumbre asociados con sus clasificaciones de salida, o mejorar la estabilidad o precisión de sus cálculos internos. Los algoritmos actuales para la retroalimentación de pseudo relevancia (PRF) tienden a seguir el mismo método básico ya sea que utilicemos algoritmos basados en espacio vectorial como la fórmula de Rocchio [16], o enfoques más recientes de modelado de lenguaje como Modelos de Relevancia [10]. Primero, se obtiene un conjunto de documentos recuperados en la parte superior a partir de una consulta inicial y se asume que aproxima un conjunto de documentos relevantes. A continuación, se calcula un vector de modelo de retroalimentación único según algún tipo de promedio, centroide o expectativa sobre el conjunto de modelos de documentos posiblemente relevantes. Por ejemplo, los vectores de documentos pueden combinarse con igual ponderación, como en Rocchio, o por probabilidad de consulta, como se puede hacer utilizando el Modelo de Relevancia. El uso de una expectativa es razonable por razones prácticas y teóricas, pero por sí sola ignora información potencialmente valiosa sobre el riesgo del modelo de retroalimentación. Nuestra hipótesis principal en este artículo es que estimar la incertidumbre en la retroalimentación es útil y conduce a modelos de retroalimentación individual más efectivos y a modelos combinados más robustos. Por lo tanto, proponemos un método para estimar la incertidumbre asociada con un modelo de retroalimentación individual en términos de una distribución posterior sobre modelos de lenguaje. Para hacer esto, variamos sistemáticamente las entradas al método de retroalimentación base y ajustamos una distribución de Dirichlet a la salida. Utilizamos la media posterior o el modo como la estimación mejorada del modelo de retroalimentación. Este proceso se muestra en la Figura 1. Como mostraremos más adelante, la media y la moda pueden variar significativamente del modelo de retroalimentación única propuesto por el método de referencia. También realizamos la combinación de modelos utilizando varios modelos de lenguaje de retroalimentación mejorados obtenidos a partir de un pequeño número de nuevas consultas muestreadas de la consulta original. El peso de un modelo combina dos factores complementarios: la probabilidad del modelo de generar la consulta y la varianza del modelo, siendo los modelos con alta varianza los que obtienen un peso menor. Por ejemplo, un vector de parámetros esperado condicionado a la observación de la consulta se forma a partir de los documentos recuperados en la parte superior, que se tratan como cadenas de entrenamiento (ver [10], p. 62). Figura 1: Estimación de la incertidumbre del modelo de retroalimentación para una sola consulta. 2. En las Secciones 2.1-2.5 describimos un método general para estimar una distribución de probabilidad sobre el conjunto de posibles modelos de lenguaje basado en muestreo. En las Secciones 2.6 y 2.7 resumimos cómo se utilizan diferentes muestras de consultas para generar múltiples modelos de retroalimentación, que luego se combinan. 2.1 Modelado de la Incertidumbre de la Retroalimentación Dado una consulta Q y una colección C, asumimos un sistema de recuperación probabilístico que asigna un puntaje de documento de valor real f(D, Q) a cada documento D en C, de manera que el puntaje sea proporcional a la probabilidad estimada de relevancia. No hacemos ninguna otra suposición sobre f(D, Q). La naturaleza de f(D, Q) puede ser compleja: por ejemplo, si el sistema de recuperación admite lenguajes de consulta estructurados [12], entonces f(D, Q) puede representar la salida de una red de inferencia arbitrariamente compleja definida por los operadores de consulta estructurados. En teoría, la función de puntuación puede variar de una consulta a otra, aunque en este estudio por simplicidad mantenemos la misma función de puntuación para todas las consultas. Nuestro método de consulta específico se encuentra en la Sección 3. Tratamos el algoritmo de retroalimentación como una caja negra y asumimos que los insumos del algoritmo de retroalimentación son la consulta original y los documentos recuperados principales correspondientes, a los cuales se les asigna una puntuación a cada documento. Suponemos que la salida del algoritmo de retroalimentación es un vector de pesos de términos que se utilizarán para agregar o reponderar los términos en la representación de la consulta original, con el vector normalizado para formar una distribución de probabilidad. Consideramos las entradas a la caja negra de retroalimentación como variables aleatorias, y analizamos el modelo de retroalimentación como una variable aleatoria que cambia en respuesta a cambios en las entradas. Al igual que la función de puntuación del documento f(D, Q), el algoritmo de retroalimentación puede implementar una fórmula de puntuación compleja y no lineal, por lo que, a medida que sus entradas varían, los modelos de retroalimentación resultantes pueden tener una distribución compleja en el espacio de modelos de retroalimentación (el espacio muestral). Debido a esta complejidad potencial, no intentamos derivar una distribución posterior de forma cerrada, sino que en su lugar utilizamos simulación. Llamamos a esta distribución sobre posibles modelos de retroalimentación la distribución de modelos de retroalimentación. Nuestro objetivo en esta sección es estimar una aproximación útil a la distribución del modelo de retroalimentación. Para un marco específico de experimentos, utilizamos el enfoque de modelado de lenguaje (LM) para la recuperación de información [15]. El puntaje de un documento D con respecto a una consulta Q y una colección C se da por p(Q|D) con respecto a los modelos de lenguaje ˆθQ y ˆθD estimados para la consulta y el documento respectivamente. Denotamos el conjunto de los k documentos mejor recuperados de la colección C en respuesta a Q como DQ(k, C). Para simplificar, asumimos que las consultas y documentos son generados por distribuciones multinomiales cuyos parámetros están representados por modelos de lenguaje unigram. Para incorporar retroalimentación en el enfoque de LM, asumimos un esquema basado en modelos en el que nuestro objetivo es tomar la consulta y los documentos clasificados resultantes DQ(k, C) como entrada, y producir un modelo de lenguaje expandido ˆθE, que luego se interpola con el modelo de consulta original ˆθQ: ˆθNew = (1 − α) · ˆθQ + α · ˆθE (1). Esto incluye la posibilidad de α = 1 donde el modelo de consulta original es completamente reemplazado por el modelo de retroalimentación. Nuestro espacio muestral es el conjunto de todos los posibles modelos de lenguaje LF que pueden ser generados como modelos de retroalimentación. Nuestro enfoque es tomar muestras de este espacio y luego ajustar una distribución a las muestras utilizando máxima verosimilitud. Para simplificar, comenzamos asumiendo que la <br>distribución de retroalimentación</br> latente tiene la forma de una distribución de Dirichlet. Aunque la distribución de Dirichlet es unimodal y en general bastante limitada en su expresividad en el espacio muestral, es una coincidencia natural para el modelo de lenguaje multinomial, puede ser estimada rápidamente y capturar las características más relevantes de los modelos de retroalimentación confiada e incierta, como la dispersión general de la distribución. 2.2 Re-muestreo de modelos de documentos. Nos gustaría una aproximación a la distribución posterior del modelo de retroalimentación LF. Para lograr esto, aplicamos una técnica de simulación ampliamente utilizada llamada muestreo bootstrap ([7], p. 474) en los parámetros de entrada, es decir, el conjunto de documentos recuperados en la parte superior. El muestreo de arranque nos permite simular el efecto aproximado de perturbar los parámetros dentro del algoritmo de retroalimentación de la caja negra al perturbar los insumos de ese algoritmo de manera sistemática, sin hacer suposiciones sobre la naturaleza del algoritmo de retroalimentación. Específicamente, muestreamos k documentos con reemplazo de DQ(k, C), y calculamos un modelo de lenguaje de expansión θb utilizando el método de retroalimentación de caja negra. Repetimos este proceso B veces para obtener un conjunto de B modelos de lenguaje de retroalimentación, a los cuales luego ajustamos una distribución de Dirichlet. Normalmente, B se encuentra en el rango de 20 a 50 muestras, con un rendimiento relativamente estable en este rango. Ten en cuenta que en lugar de tratar cada documento superior como igualmente probable, muestreamos de acuerdo con las probabilidades estimadas de relevancia de cada documento en DQ(k, C). Por lo tanto, es más probable que un documento sea elegido cuanto más alto esté en la clasificación. 2.3 Justificación de un enfoque de muestreo La justificación de nuestro enfoque de muestreo tiene dos partes. Primero, queremos mejorar la calidad de los modelos de retroalimentación individual al suavizar la variación cuando el modelo de retroalimentación base es inestable. En este sentido, nuestro enfoque se asemeja al bagging [4], un enfoque de conjunto que genera múltiples versiones de un predictor haciendo copias de arranque del conjunto de entrenamiento, y luego promedia los predictores (numéricos). En nuestra aplicación, los documentos recuperados en la parte superior pueden ser vistos como una especie de conjunto de entrenamiento ruidoso para la relevancia. Segundo, el muestreo es una forma efectiva de estimar propiedades básicas de la distribución posterior de retroalimentación, las cuales pueden ser utilizadas para mejorar la combinación de modelos. Por ejemplo, un modelo puede ser ponderado por su confianza de predicción, estimada como una función de la variabilidad del posterior alrededor del modelo. foo2-401.map-Dim:5434,Size:12*12units, vecindario gaussiano (a) Tema 401 Minorías extranjeras, Alemania foo2-402.map-Dim:5698,Size:12*12units, vecindario gaussiano (b) Tema 402 Genética del comportamiento foo2-459.map-Dim:8969,Size:12*12units, vecindario gaussiano (c) Tema 459 ¿Cuándo puede un prestamista ejecutar una hipoteca sobre una propiedad? Figura 2: Visualización de la varianza del modelo de lenguaje de expansión utilizando mapas autoorganizados, mostrando la distribución de modelos de lenguaje que resulta de remuestrear las entradas al método de expansión de referencia. El modelo de lenguaje que habría sido elegido por la expansión de la línea base se encuentra en el centro de cada mapa. La función de similitud es la divergencia JensenShannon. 2.4 Visualización de distribuciones de retroalimentación Antes de describir cómo ajustamos y utilizamos la distribución de Dirichlet sobre modelos de retroalimentación, es instructivo ver algunos ejemplos de distribuciones de modelos de retroalimentación reales que resultan de muestrear por bootstrap los documentos más recuperados de diferentes temas de TREC. Cada punto en nuestro espacio muestral es un modelo de lenguaje, que típicamente tiene varias miles de dimensiones. Para ayudar a analizar el comportamiento de nuestro método, utilizamos un Mapa Auto-organizado (a través del paquete SOM-PAK [9]), para aplanar y visualizar la función de densidad de alta dimensión. Los mapas de densidad para tres temas de TREC se muestran en la Figura 2 arriba. Las áreas oscuras representan regiones de alta similitud entre los modelos de lenguaje. Las áreas claras representan regiones de baja similitud, los valles entre los grupos. Cada diagrama está centrado en el modelo de lenguaje que habría sido elegido por la expansión de referencia. Un solo pico (modo) es evidente en algunos ejemplos, pero en otros aparece una estructura más compleja. Además, aunque la distribución suele estar cerca del modelo de retroalimentación de referencia, para algunos temas están a una distancia significativa (según la divergencia de JensenShannon), como se muestra en la Subfigura 2c. En tales casos, la moda o la media de la <br>distribución de retroalimentación</br> a menudo funcionan significativamente mejor que el valor base (y en una proporción menor de casos, significativamente peor). 2.5 Ajuste de una <br>distribución de retroalimentación</br> posterior Después de obtener muestras del modelo de retroalimentación mediante el remuestreo de las entradas del modelo de retroalimentación, estimamos la distribución de retroalimentación. Suponemos que los modelos de retroalimentación multinomial {ˆθ1, . . . , ˆθB} fueron generados por una distribución de Dirichlet latente con parámetros {α1, . . . , αN}. Para estimar los {α1, . . . , αN }, ajustamos los parámetros de Dirichlet a las muestras del modelo de lenguaje B según la máxima verosimilitud utilizando un procedimiento de Newton generalizado, cuyos detalles se encuentran en Minka [13]. Suponemos una distribución de Dirichlet simple sobre los {α1, . . . , αN}, estableciendo cada uno como αi = μ · p(wi | C), donde μ es un parámetro y p(· | C) es el modelo de lenguaje de colección estimado a partir de un conjunto de documentos de la colección C. El ajuste de parámetros converge muy rápidamente, generalmente en solo 2 o 3 iteraciones. Dado que nuestros puntos son modelos de lenguaje en el simplex multinomial, extendimos SOM-PAK para admitir la divergencia de Jensen-Shannon, una medida de similitud ampliamente utilizada entre distribuciones de probabilidad. 3 iteraciones son suficientes, por lo que es práctico aplicarlo en tiempo de consulta cuando la sobrecarga computacional debe ser pequeña. En la práctica, podemos restringir el cálculo al vocabulario de los documentos recuperados en la parte superior, en lugar de toda la colección. Ten en cuenta que para este paso estamos reutilizando los documentos recuperados existentes y no realizando consultas adicionales. Dado los parámetros de una distribución de Dirichlet N-dimensional Dir(α), los vectores de media μ y moda x son fáciles de calcular y se dan respectivamente por μi = αiP αi (2) y xi = αi−1P αi−N . (3) Luego podemos elegir el modelo de lenguaje en la media o en la moda del posterior como el modelo de retroalimentación mejorado final. (Encontramos que la moda proporciona un rendimiento ligeramente mejor). Para la recuperación de información, es probable que el número de muestras disponibles sea bastante pequeño por razones de rendimiento, generalmente menos de diez. Además, si bien el muestreo aleatorio es útil en ciertos casos, es perfectamente aceptable permitir distribuciones de muestreo determinísticas, pero estas deben diseñarse cuidadosamente para aproximar una varianza de salida precisa. Dejamos esto para estudios futuros. 2.6 Variantes de consulta Utilizamos los siguientes métodos para generar variantes de la consulta original. Cada variante corresponde a una suposición diferente sobre qué aspectos de la consulta original pueden ser importantes. Esta es una forma de muestreo determinístico. Seleccionamos tres métodos simples que cubren suposiciones complementarias sobre la consulta. Uso sin expansión. La suposición es que los términos dados son una descripción completa de la necesidad de información. Deja uno fuera. Se deja fuera un término del query original. La suposición es que uno de los términos de la consulta es un término de ruido. Se elige un término único de la consulta original. Esto asume que solo un aspecto de la consulta, es decir, el representado por el término, es el más importante. Después de generar una variante de la consulta original, la combinamos con la consulta original utilizando un peso αSUB para no alejarnos demasiado. En este estudio, establecimos αSUB = 0.5. Por ejemplo, utilizando el lenguaje de consulta Indri [12], una variante de dejar uno fuera de la consulta inicial que omite el término \"ireland\" para el tema TREC 404 es: #weight(0.5 #combine(ireland peace talks) 0.5 #combine(peace talks)) 2.7 Combinando modelos de retroalimentación mejorados de múltiples variantes de consulta. Cuando se utilizan múltiples variantes de consulta, los modelos de retroalimentación mejorados resultantes se combinan utilizando la combinación de modelos bayesianos. Para hacer esto, tratamos cada palabra como un elemento a clasificar como perteneciente a una clase relevante o no relevante, y derivamos una probabilidad de clase para cada palabra combinando las puntuaciones de cada variante de consulta. Cada puntuación es dada por la probabilidad de ese término en la distribución de Dirichlet. Las puntuaciones de los términos se ponderan por el inverso de la varianza del término en los modelos de retroalimentación mejorada de la distribución de Dirichlet. La probabilidad previa de pertenencia de una palabra a la clase relevante se da por la probabilidad de la consulta original en todo el modelo de expansión mejorado. 3. EVALUACIÓN En esta sección presentamos resultados que confirman la utilidad de estimar una distribución del modelo de retroalimentación a partir del remuestreo ponderado de documentos mejor clasificados, y de combinar los modelos de retroalimentación obtenidos a partir de diferentes pequeños cambios en la consulta original. 3.1 Método general Evaluamos el rendimiento en un total de 350 consultas derivadas de cuatro conjuntos de temas de TREC: 51-200 (TREC-1&2), 351-400 (TREC-7), 401-450 (TREC-8) y 451-550 (wt10g, TREC-9&10). Elegimos estos por su contenido variado y propiedades del documento. Por ejemplo, los documentos wt10g son páginas web con una amplia variedad de temas y estilos, mientras que los documentos TREC-1&2 son artículos de noticias más homogéneos. La indexación y recuperación se realizó utilizando el sistema Indri en el kit de herramientas Lemur [12] [1]. Nuestras consultas se derivaron de las palabras en el campo del título de los temas de TREC. Las frases no fueron utilizadas. Para generar las consultas base que se pasaron a Indri, envolvimos los términos de la consulta con el operador #combine de Indri. Por ejemplo, la consulta inicial para el tema 404 es: #combine(ireland peace talks) Realizamos el stemming de Krovetz para todos los experimentos. Debido a que encontramos que el método de expansión de la base (Indri) funcionó mejor utilizando una lista de palabras vacías con el modelo de retroalimentación, todos los experimentos utilizaron una lista de 419 palabras comunes en inglés. Sin embargo, un efecto secundario interesante de nuestro enfoque de remuestreo es que tiende a eliminar muchas palabras vacías del modelo de retroalimentación, lo que hace que una lista de paradas sea menos crítica. Esto se discute más a fondo en la Sección 3.6. Método de retroalimentación de línea base 3.2 Para nuestro método de expansión de línea base, utilizamos un algoritmo incluido en Indri 1.0 como el método de expansión predeterminado. Este método primero selecciona términos utilizando un cálculo de logaritmo de probabilidades descrito por Ponte [14], pero asigna pesos finales a los términos utilizando el modelo de relevancia de Lavrenko [10]. Elegimos el método Indri porque proporciona una línea base consistentemente sólida, se basa en un enfoque de modelado del lenguaje y es fácil de experimentar con él. En una evaluación de TREC utilizando el corpus GOV2, el método fue uno de los mejores resultados, logrando un aumento del 19.8% en el MAP en comparación con el uso de consultas no expandidas. En este estudio, se logra una ganancia promedio en MAP del 17.25% en las cuatro colecciones. El método de expansión de Indri primero calcula una razón de logaritmos de probabilidades o(v) para cada término de expansión potencial v dado por o(v) = X D log p(v|D) p(v|C) (4) sobre todos los documentos D que contienen v, en la colección C. Luego, los candidatos a términos de expansión se ordenan por o(v) descendente, y se eligen los primeros m. Finalmente, los pesos de término r(v) utilizados en la consulta ampliada se calculan en base al modelo de relevancia r(v) = X D p(q|D)p(v|D) p(v) p(D) (5). La cantidad p(q|D) es la puntuación de probabilidad asignada al documento en la recuperación inicial. Utilizamos suavizado de Dirichlet de p(v|D) con μ = 1000. Este modelo de relevancia se combina luego con la consulta original utilizando interpolación lineal, ponderada por un parámetro α. Por defecto, utilizamos los 50 documentos principales para la retroalimentación y los 20 términos de expansión, con el parámetro de interpolación de retroalimentación α = 0.5 a menos que se indique lo contrario. Por ejemplo, la consulta expandida base para el tema 404 es: #weight(0.5 #combine(ireland peace talks) 0.5 #weight(0.10 ireland 0.08 peace 0.08 northern ...) 3.3 Rendimiento de la expansión. Medimos la efectividad de nuestros algoritmos de retroalimentación mediante dos criterios principales: precisión y robustez. La robustez, y el equilibrio entre precisión y robustez, se analiza en la Sección 3.4. En esta sección, examinamos la precisión promedio y la precisión en los 10 documentos principales (P10). También incluimos la recuperación de 1,000 documentos. Para cada consulta, obtuvimos un conjunto de B modelos de retroalimentación utilizando la línea base de Indri. Cada modelo de retroalimentación se obtuvo a partir de una muestra aleatoria de los k documentos principales tomados con reemplazo. Para estos experimentos, B = 30 y k = 50. Cada modelo de retroalimentación contenía 20 términos. En el lado de la consulta, utilizamos muestreo de dejar uno fuera (LOO) para crear las variantes de consulta. El muestreo de consulta de un solo término tuvo un rendimiento consistentemente peor en todas las colecciones, por lo que nuestros resultados se centran en el muestreo de LOO aquí. Utilizamos los métodos descritos en la Sección 2 para estimar un modelo de retroalimentación mejorado a partir de la distribución posterior de Dirichlet para cada variante de consulta, y para combinar los modelos de retroalimentación de todas las variantes de consulta. Llamamos a nuestro método expansión de remuestreo y lo denotamos como RS-FB aquí. Denominamos al método de retroalimentación de línea base Indri como Base-FB. Los resultados de aplicar tanto el método de expansión de línea base (Base-FB) como la expansión de remuestreo (RS-FB) se muestran en la Tabla 1. Observamos varias tendencias en esta tabla. Primero, la precisión promedio de RS-FB fue comparable a Base-FB, logrando una ganancia promedio del 17.6% en comparación con no utilizar expansión en las cuatro colecciones. La ganancia de expansión de la línea base de Indri fue del 17.25%. Además, el método RS-FB logró mejoras consistentes en P10 sobre Base-FB para cada conjunto de temas, con una mejora promedio del 6.89% sobre Base-FB para los 350 temas. La ganancia P10 más baja sobre Base-FB fue del +3.82% para TREC-7 y la más alta fue del +11.95% para wt10g. Finalmente, tanto Base-FB como RS-FB también mejoraron consistentemente la recuperación en comparación con no utilizar expansión, siendo Base-FB el que logró una mejor recuperación que RS-FB para todos los conjuntos de temas. 3.4 Robustez de recuperación. Utilizamos el término robustez para referirnos al rendimiento de precisión promedio en el peor de los casos de un algoritmo de retroalimentación. Idealmente, un método de retroalimentación sólido nunca debería funcionar peor que usar la consulta original, mientras que a menudo funciona mejor utilizando la expansión. Para evaluar la robustez en este estudio, utilizamos una medida muy simple llamada índice de robustez (RI). Para un conjunto de consultas Q, la medida de RI se define como: RI(Q) = n+ − n− |Q| (6) donde n+ es el número de consultas ayudadas por el método de retroalimentación y n− es el número de consultas perjudicadas. Aquí, por \"ayudar\" nos referimos a obtener una precisión promedio más alta como resultado de la retroalimentación. El valor de RI varía desde un mínimo de 3. A veces también se le llama índice de confiabilidad de mejora y fue utilizado en Sakai et al. [17]. Colección NoExp Base-FB RS-FB TREC 1&2 AvgP 0.1818 0.2419 (+33.04%) 0.2406 (+32.24%) P10 0.4443 0.4913 (+10.57%) 0.5363 (+17.83%) Recall 15084/37393 19172/37393 15396/37393 TREC 7 AvgP 0.1890 0.2175 (+15.07%) 0.2169 (+14.75%) P10 0.4200 0.4320 (+2.85%) 0.4480 (+6.67%) Recall 2179/4674 2608/4674 2487/4674 TREC 8 AvgP 0.2031 0.2361 (+16.25%) 0.2268 (+11.70%) P10 0.3960 0.4160 (+5.05%) 0.4340 (+9.59%) Recall 2144/4728 2642/4728 2485/4728 wt10g AvgP 0.1741 0.1829 (+5.06%) 0.1946 (+11.78%) P10 0.2760 0.2630 (-4.71%) 0.2960 (+7.24%) Recall 3361/5980 3725/5980 3664/5980 Tabla 1: Comparación de la retroalimentación base (Base-FB) y la retroalimentación utilizando re-muestreo (RS-FB). La mejora mostrada para BaseFB y RS-FB es relativa al uso de ninguna expansión. (a) TREC 1&2 (curva superior); TREC 8 (curva inferior) (b) TREC 7 (curva superior); wt10g (curva inferior) Figura 3: La compensación entre robustez y precisión promedio para diferentes corpora. El eje x muestra el cambio en la MAP al utilizar la expansión de la línea base con α = 0.5. El eje y muestra el Índice de Robustez (RI). Cada curva a través de los puntos no circulados muestra el compromiso entre RI/MAP utilizando la estrategia simple de pequeño α (ver texto) a medida que α disminuye de 0.5 a cero en la dirección de la flecha. Los puntos circulados representan los compromisos obtenidos al volver a muestrear la retroalimentación para α = 0.5. Colección N Base-FB RS-FB n− RI n− RI TREC 1&2 103 26 +0.495 15 +0.709 TREC 7 46 14 +0.391 10 +0.565 TREC 8 44 12 +0.455 12 +0.455 wt10g 91 48 -0.055 39 +0.143 Combinado 284 100 +0.296 76 +0.465 Tabla 2: Comparación del índice de robustez (RI) para retroalimentación de línea base (Base-FB) vs. retroalimentación de remuestreo (RS-FB). También se muestran el número real de consultas perjudicadas por la retroalimentación (n−) para cada método y colección. Las consultas para las cuales la precisión promedio inicial era insignificante (≤ 0.01) fueron ignoradas, dando como resultado el recuento de consultas restantes en la columna N, de -1.0, cuando todas las consultas son perjudicadas por el método de retroalimentación, a +1.0 cuando todas las consultas son ayudadas. La medida de RI no tiene en cuenta la magnitud o distribución de la cantidad de cambio en el conjunto Q. Sin embargo, es fácil de entender como una indicación general de robustez. Una forma obvia de mejorar el rendimiento en el peor de los casos de la retroalimentación es simplemente utilizar un parámetro de interpolación α fijo más pequeño, como α = 0.3, otorgando menos peso al modelo de retroalimentación (posiblemente arriesgado) y más al cuestionario original. Llamamos a esto la estrategia de pequeña α. Dado que también estamos reduciendo las ganancias potenciales cuando el modelo de retroalimentación es correcto, sin embargo, esperaríamos algún tipo de compensación entre la precisión promedio y la robustez. Por lo tanto, comparamos el equilibrio entre precisión y robustez entre nuestro algoritmo de retroalimentación de remuestreo y el método simple de pequeño α. Los resultados se resumen en la Figura 3. En la figura, la curva para cada conjunto de temas interpola entre los puntos de compensación, comenzando en x=0, donde α = 0.5, y continuando en la dirección de la flecha a medida que α disminuye y la consulta original recibe cada vez más peso. Como era de esperar, la robustez aumenta continuamente a medida que avanzamos a lo largo de la curva, pero la precisión media promedio generalmente disminuye a medida que se eliminan las ganancias del feedback. Para la comparación, se muestra el rendimiento del re-muestreo con retroalimentación en α = 0.5 para cada colección como el punto marcado con un círculo. Más alto y a la derecha es mejor. Esta figura muestra que el re-muestreo de retroalimentación ofrece un compromiso algo mejor que el enfoque de pequeño α para 3 de las 4 colecciones. Figura 4: Histograma que muestra la mayor robustez del re-muestreo de retroalimentación (RS-FB) sobre la retroalimentación base (Base-FB) para todos los conjuntos de datos combinados. Las consultas se agrupan por el % de cambio en AP en comparación con la consulta no expandida. Colección DS + QV DS + No QV TREC 1&2 AvgP 0.2406 0.2547 (+5.86%) P10 0.5263 0.5362 (+1.88%) RI 0.7087 0.6515 (-0.0572) TREC 7 AvgP 0.2169 0.2200 (+1.43%) P10 0.4480 0.4300 (-4.02%) RI 0.5652 0.2609 (-0.3043) TREC 8 AvgP 0.2268 0.2257 (-0.49%) P10 0.4340 0.4200 (-3.23%) RI 0.4545 0.4091 (-0.0454) wt10g AvgP 0.1946 0.1865 (-4.16%) P10 0.2960 0.2680 (-9.46%) RI 0.1429 0.0220 (-0.1209) Tabla 3: Comparación de la retroalimentación de remuestreo utilizando muestreo de documentos (DS) con (QV) y sin (No QV) combinar modelos de retroalimentación de múltiples variantes de consulta. La Tabla 2 muestra los puntajes del Índice de Robustez para Base-FB y RS-FB. El método de retroalimentación RS-FB obtuvo una mayor robustez que Base-FB en tres de los cuatro conjuntos de temas, con solo un rendimiento ligeramente peor en TREC-8. Una vista más detallada que muestra la distribución de los cambios relativos en AP se presenta en el histograma de la Figura 4. En comparación con Base-FB, el método RS-FB logra una reducción notable en el número de consultas significativamente afectadas por la expansión (es decir, donde AP se ve afectado en un 25% o más), al tiempo que conserva ganancias positivas en AP. 3.5 Efecto de los métodos de muestreo de consultas y documentos Dada la mayor robustez de nuestros algoritmos observada en la Sección 3.4, una pregunta importante es qué componente de nuestro sistema es responsable. ¿Es el uso de la re-muestreo de documentos, el uso de múltiples variantes de consulta, o algún otro factor? Los resultados en la Tabla 3 sugieren que la combinación de modelos basada en variantes de consulta puede ser en gran medida responsable de la mayor robustez. Cuando se desactivan las variantes de consulta y se utiliza la consulta original por sí sola con muestreo de documentos, hay poco cambio neto en la precisión promedio, una pequeña disminución en P10 para 3 de los 4 conjuntos de temas, pero una caída significativa en la robustez para todos los conjuntos de temas. En dos casos, la medida de RI disminuye en más del 50%. También examinamos el efecto del método de muestreo de documentos en la efectividad de la recuperación, utilizando dos estrategias diferentes. La estrategia de ponderación uniforme ignoró las puntuaciones de relevancia de la recuperación inicial y otorgó a cada documento en los primeros k la misma probabilidad de selección. Por el contrario, la estrategia de ponderación de puntajes de relevancia seleccionó documentos con una probabilidad proporcional a sus puntajes de relevancia. De esta manera, los documentos que tenían una clasificación más alta tenían más probabilidades de ser seleccionados. Los resultados se muestran en la Tabla 4. La estrategia de ponderación de la puntuación de relevancia tiene un mejor rendimiento en general, con puntajes de RI y P10 significativamente más altos en 3 de los 4 conjuntos de temas. La diferencia en la precisión promedio entre los métodos, sin embargo, es menos marcada. Esto sugiere que el peso uniforme actúa para aumentar la varianza en los resultados de recuperación: cuando la precisión promedio inicial es alta, hay muchos documentos relevantes en los primeros k y el muestreo uniforme puede proporcionar un modelo de relevancia más representativo que centrarse en los elementos altamente clasificados. Por otro lado, cuando la precisión inicial es baja, hay pocos documentos relevantes en las posiciones inferiores y el muestreo uniforme mezcla más de los documentos no relevantes. Por razones de espacio, solo resumimos nuestros hallazgos sobre el tamaño de la muestra aquí. El número de muestras tiene cierto efecto en la precisión cuando es menor a 10, pero el rendimiento se estabiliza alrededor de 15 a 20 muestras. Utilizamos 30 muestras para nuestros experimentos. Mucho más allá de este nivel, los beneficios adicionales de más muestras disminuyen a medida que la distribución inicial de puntajes se ajusta más estrechamente y aumenta el tiempo de procesamiento. 3.6 El efecto del remuestreo en la calidad del término de expansión Idealmente, un modelo de recuperación no debería requerir una lista de palabras vacías al estimar un modelo de relevancia: un modelo estadístico robusto debería reducir automáticamente el peso de las palabras vacías dependiendo del contexto. Las palabras vacías pueden perjudicar la retroalimentación si se seleccionan como términos de retroalimentación, ya que suelen ser malos discriminadores y desperdician espacios de términos valiosos. En la práctica, sin embargo, debido a que la mayoría de los métodos de selección de términos se asemejan a un tipo de ponderación tf · idf, a veces se pueden seleccionar términos con un idf bajo pero un tf muy alto como candidatos de términos de expansión. Esto sucede, por ejemplo, incluso con el enfoque del Modelo de Relevancia que forma parte de nuestra retroalimentación base. Para garantizar una línea base lo más sólida posible, utilizamos una lista de paradas para todos los experimentos reportados aquí. Si desactivamos la lista de palabras vacías, sin embargo, obtenemos resultados como los mostrados en la Tabla 5 donde cuatro de los diez términos de retroalimentación básica principales para el tema 60 de TREC (dijo, pero, su, no) son palabras vacías utilizando el método BaseFB. (Se seleccionaron los 100 términos de expansión principales para generar este ejemplo). El método Indris intenta abordar el problema de las palabras vacías aplicando un paso inicial basado en Ponte [14] para seleccionar términos menos comunes que tienen altas probabilidades logarítmicas de estar en los documentos mejor clasificados en comparación con toda la colección. Sin embargo, esto no resuelve completamente el problema de las palabras vacías, especialmente a medida que aumenta el número de términos de retroalimentación. Sin embargo, el uso de retroalimentación de remuestreo parece mitigar la ponderación de la colección QV + QV uniforme + ponderación de puntaje de relevancia TREC 1&2 AvgP 0.2545 0.2406 (-5.46%) P10 0.5369 0.5263 (-1.97%) RI 0.6212 0.7087 (+14.09%) TREC 7 AvgP 0.2174 0.2169 (-0.23%) P10 0.4320 0.4480 (+3.70%) RI 0.4783 0.5652 (+18.17%) TREC 8 AvgP 0.2267 0.2268 (+0.04%) P10 0.4120 0.4340 (+5.34%) RI 0.4545 0.4545 (+0.00%) wt10g AvgP 0.1808 0.1946 (+7.63%) P10 0.2680 0.2960 (+10.45%) RI 0.0220 0.1099 (+399.5%) Tabla 4: Comparación de muestreo de documentos uniforme y ponderado por relevancia. El cambio porcentual en comparación con el muestreo uniforme se muestra entre paréntesis. QV indica que se utilizaron variantes de consulta en ambas ejecuciones. FB de línea base p(wi|R) FB de remuestreo p(wi|R) dijo 0.055 corte 0.026 corte 0.055 pagar 0.018 pagar 0.034 federal 0.012 pero 0.026 educación 0.011 empleados 0.024 maestros 0.010 sus 0.024 empleados 0.010 no 0.023 caso 0.010 federal 0.021 sus 0.009 trabajadores 0.020 apelaciones 0.008 educación 0.020 sindicato 0.007 Tabla 5: Calidad del término de retroalimentación cuando no se utiliza una lista de paradas. Términos de retroalimentación para el tema 60 de TREC: pago por mérito vs antigüedad. el efecto de las palabras vacías automáticamente. En el ejemplo de la Tabla 5, la retroalimentación de remuestreo deja solo una palabra vacía (su) en las diez primeras posiciones. Observamos un comportamiento similar del término de retroalimentación en muchos otros temas. La razón de este efecto parece ser la interacción de la puntuación de selección de términos con el límite superior de términos m. Si bien la presencia e incluso la proporción de palabras vacías particulares es bastante estable en diferentes muestras de documentos, su posición relativa en la lista de los primeros m elementos no lo es, ya que se examinan conjuntos de documentos con diferentes números de candidatos de términos mejores y de menor frecuencia para cada muestra. Como resultado, si bien algunos stopwords pueden aparecer en cada conjunto de documentos muestreados, tiende a ocurrir que cualquier stopword dado caiga por debajo del umbral para múltiples muestras, lo que lleva a su clasificación como una característica de alta varianza y bajo peso. 4. TRABAJO RELACIONADO Nuestro enfoque está relacionado con trabajos previos de varias áreas de recuperación de información y aprendizaje automático. Nuestro uso de la variación de consultas fue inspirado por el trabajo de YomTov et al. [20], Carpineto et al. [5] y Amati et al. [2], entre otros. Estos estudios utilizan la idea de crear múltiples subconsultas y luego examinar la naturaleza de la superposición en los documentos y/o términos de expansión que resultan de cada subconsulta. La combinación de modelos se realiza utilizando heurísticas. En particular, los estudios de Amati et al. y Carpineto et al. investigaron la combinación de términos de métodos de distribución individuales utilizando una heurística de combinación de reordenamiento de términos. En un conjunto de temas de TREC encontraron una amplia variación promedio en la distancia de rango de los términos de diferentes métodos de expansión. Su método de combinación proporcionó modestas mejoras positivas en la precisión promedio. La idea de examinar la superposición entre listas de términos sugeridos también ha sido utilizada en enfoques tempranos de expansión de consultas. El método de Análisis de Contexto Local (LCA) de Xu y Crofts [19] incluye un factor en la fórmula de ponderación derivada empíricamente que hace que se prefieran los términos de expansión que tienen conexiones con múltiples términos de consulta. En el lado del documento, el trabajo reciente de Zhou & Croft [21] exploró la idea de agregar ruido a los documentos, volver a puntuarlos y utilizar la estabilidad de las clasificaciones resultantes como una estimación de la dificultad de la consulta. Esto está relacionado con nuestro uso de muestreo de documentos para estimar el riesgo del modelo de retroalimentación construido a partir de los diferentes conjuntos de documentos recuperados en la parte superior. Sakai et al. [17] propusieron un enfoque para mejorar la robustez de la retroalimentación de relevancia pseudo utilizando un método que llaman muestreo selectivo. La esencia de su método es que permiten omitir algunos documentos de alta clasificación, basándose en un criterio de agrupación, para seleccionar un conjunto de documentos más variado e innovador más adelante en la clasificación para su uso por un método tradicional de retroalimentación pseudo. Su estudio no encontró mejoras significativas ni en robustez (RI) ni en MAP en sus corpora. Greiff, Morgan y Ponte [8] exploraron el papel de la varianza en la ponderación de términos. En una serie de simulaciones que simplificaron el problema a documentos de 2 características, encontraron que la precisión promedio disminuye a medida que la varianza de la frecuencia de términos - ruido alto - aumenta. La reducción del peso de los términos con alta varianza resultó en una mejora de la precisión promedio. Esto parece estar en concordancia con nuestros propios hallazgos para los modelos de retroalimentación individual. Las estimaciones de la varianza de la producción han sido utilizadas recientemente para mejorar la clasificación de textos. Lee et al. [11] utilizaron estimaciones de varianza específicas de la consulta de las salidas del clasificador para realizar una combinación de modelos mejorada. En lugar de utilizar muestreo, pudieron derivar expresiones en forma cerrada para la varianza del clasificador asumiendo clasificadores base utilizando tipos simples de redes de inferencia. Ando y Zhang propusieron un método que ellos llaman retroalimentación estructural [3] y mostraron cómo aplicarlo a la expansión de consultas para la pista de Genómica de TREC. Utilizaron variaciones de consultas r para obtener R conjuntos diferentes Sr de documentos mejor clasificados que se han intersectado con los documentos mejor clasificados obtenidos de la consulta original qorig. Para cada Si, se calcula el vector centróide normalizado ˆwi de los documentos. El análisis de componentes principales (PCA) se aplica luego a los ˆwi para obtener la matriz Φ de los vectores singulares izquierdos φh que se utilizan para obtener la nueva consulta expandida qexp = qorig + ΦT Φqorig. (7) En el caso de H = 1, tenemos un único vector singular izquierdo φ: qexp = qorig + (φT qorig)φ, de modo que el producto punto φT qorig es un tipo de peso dinámico en la consulta expandida que se basa en la similitud de la consulta original con la consulta expandida. El uso de la varianza como medida de calidad del modelo de retroalimentación ocurre de forma indirecta a través de la aplicación de PCA. Sería interesante estudiar las conexiones entre este enfoque y nuestro propio método de ajuste de modelos. Finalmente, en los enfoques de modelado del lenguaje para retroalimentación, Tao y Zhai [18] describen un método para una retroalimentación más robusta que permite que cada documento tenga un α de retroalimentación diferente. Los pesos de retroalimentación se derivan automáticamente utilizando EM regularizado. La condición de parada de EM implica un equilibrio aproximadamente igual entre el modelo de consulta y expansión. Proponen adaptar el parámetro de parada η basado en una función de alguna medida de calidad de los documentos de retroalimentación. CONCLUSIONES Hemos presentado un nuevo enfoque para la retroalimentación de relevancia pseudo basado en el muestreo de documentos y consultas. El uso del muestreo es un dispositivo muy flexible y poderoso, motivado por nuestro deseo general de ampliar los modelos actuales de recuperación mediante la estimación del riesgo o la varianza asociada con los parámetros o la salida de los procesos de recuperación. Tales estimaciones de varianza, por ejemplo, pueden ser utilizadas de forma natural en un marco bayesiano para una mejor estimación y combinación de modelos. Aplicaciones como la expansión selectiva pueden ser implementadas de manera fundamentada. Si bien nuestro estudio utiliza el enfoque de modelado del lenguaje como marco para experimentos, hacemos pocas suposiciones sobre el funcionamiento real del algoritmo de retroalimentación. Creemos que es probable que cualquier algoritmo de retroalimentación de línea base razonablemente efectivo se beneficiaría de nuestro enfoque. Nuestros resultados en colecciones estándar de TREC muestran que nuestro marco mejora la robustez de un método de retroalimentación de referencia sólido en una variedad de colecciones, sin sacrificar la precisión promedio. También proporciona pequeñas pero consistentes mejoras en la precisión del top 10. En trabajos futuros, visualizamos una investigación sobre cómo variar el conjunto de métodos de muestreo utilizados y el número de muestras controla el equilibrio entre robustez, precisión y eficiencia. Agradecimientos Agradecemos a Paul Bennett por las valiosas discusiones relacionadas con este trabajo, el cual fue apoyado por las becas de la NSF #IIS-0534345 y #CNS-0454018, y la beca del Departamento de Educación de los EE. UU. #R305G03123. Cualquier opinión, hallazgo, conclusión o recomendación expresada en este material son del autor y no necesariamente reflejan las de los patrocinadores. REFERENCIAS [1] El kit de herramientas Lemur para modelado de lenguaje y recuperación. http://www.lemurproject.org. [2] G. Amati, C. Carpineto y G. Romano. Dificultad de la consulta, robustez y aplicación selectiva de la expansión de consultas. En Actas de la 25ª Conferencia Europea sobre Recuperación de Información (ECIR 2004), páginas 127-137. [3] R. K. Ando y T. Zhang. Un método de aprendizaje semisupervisado de alto rendimiento para segmentación de texto. En Actas de la 43ª Reunión Anual de la ACL, páginas 1-9, junio de 2005. [4] L. Breiman. Empaquetando predictores. Aprendizaje automático, 24(2):123-140, 1996. [5] C. Carpineto, G. Romano y V. Giannini. Mejorando la retroalimentación de recuperación con la combinación de múltiples funciones de clasificación de términos. ACM Trans. Información. Sistemas, 20(3):259 - 290. [6] K. Collins-Thompson, P. Ogilvie y J. Callan. Resultados iniciales con consultas estructuradas y modelos de lenguaje en medio terabyte de texto. En Actas de la Conferencia de Recuperación de Información de Texto de 2005. Publicación Especial del NIST. [7] R. O. Duda, P. E. Hart y D. G. Stork. Clasificación de patrones. Wiley and Sons, 2da edición, 2001. [8] W. R. Greiff, W. T. Morgan y J. M. Ponte. El papel de la varianza en la ponderación de términos para la recuperación de información probabilística. En Actas de la 11ª Conferencia Internacional. Conf. sobre Gestión de Información y Conocimiento (CIKM 2002), páginas 252-259. [9] T. Kohonen, J. Hynninen, J. Kangas y J. Laaksonen. SOMPAK: El paquete de programas de mapas autoorganizados. Informe técnico A31, Universidad de Tecnología de Helsinki, 1996. http://www.cis.hut.fi/research/papers/som tr96.ps.Z. [10] V. Lavrenko. Una teoría generativa de relevancia. Tesis doctoral, Universidad de Massachusetts, Amherst, 2004. [11] C.-H. Lee, R. Greiner y S. Wang. Utilizando estimaciones de varianza específicas de la consulta para combinar clasificadores bayesianos. En Actas del 23º Congreso Internacional. Conf. sobre Aprendizaje Automático (ICML 2006), páginas 529-536. [12] D. Metzler y W. B. Croft. Combinando el modelo de lenguaje y los enfoques de red de inferencia para la recuperación. Información. Procesamiento y Gestión, 40(5):735-750, 2004. [13] T. Minka. Estimando una distribución de Dirichlet. Informe técnico, 2000. http://research.microsoft.com/minka/papers/dirichlet. [14] J. Ponte. Avances en la Recuperación de Información, capítulo Modelos de lenguaje para retroalimentación de relevancia, páginas 73-96. 2000. W.B. Croft, ed. [15] J. M. Ponte y W. B. Croft. Un enfoque de modelado del lenguaje para la recuperación de información. En Actas de la Conferencia ACM SIGIR de 1998 sobre Investigación y Desarrollo en Recuperación de Información, páginas 275-281. [16] J. Rocchio. El Sistema de Recuperación SMART, capítulo Retroalimentación de Relevancia en la Recuperación de Información, páginas 313-323. Prentice-Hall, 1971. G. Salton, ed. [17] T. Sakai, T. Manabe y M. Koyama. Retroalimentación de pseudo-relevancia flexible a través de muestreo selectivo. ACM Transactions on Asian Language Information Processing (TALIP), 4(2):111-135, 2005. [18] T. Tao y C. Zhai. Estimación regularizada de modelos de mezcla para retroalimentación de pseudo relevancia robusta. En Actas de la Conferencia ACM SIGIR 2006 sobre Investigación y Desarrollo en Recuperación de Información, páginas 162-169. [19] J. Xu y W. B. Croft. Mejorando la efectividad de la recuperación de información con análisis de contexto local. ACM Trans. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate to Spanish? Syst., 18(1):79-112, 2000. [20] E. YomTov, S. Fine, D. Carmel, y A. Darlow. Aprendiendo a estimar la dificultad de la consulta. En Actas de la Conferencia ACM SIGIR 2005 sobre Investigación y Desarrollo en Recuperación de Información, páginas 512-519. [21] Y. Zhou y W. B. Croft. Robustez del ranking: un nuevo marco para predecir el rendimiento de la consulta. En Actas de la 15ª Conferencia Internacional de la ACM. Conferencia sobre Gestión de Información y Conocimiento (CIKM 2006), páginas 567-574. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        }
    }
}