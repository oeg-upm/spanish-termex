{
    "id": "I-43",
    "original_text": "Dynamics Based Control with an Application to Area-Sweeping Problems Zinovi Rabinovich Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel jeff@cs.huji.ac.il Gal A. Kaminka The MAVERICK Group Department of Computer Science Bar Ilan University, Israel galk@cs.biu.ac.il ABSTRACT In this paper we introduce Dynamics Based Control (DBC), an approach to planning and control of an agent in stochastic environments. Unlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics. We show that a recently developed planning and control approach, Extended Markov Tracking (EMT) is an instantiation of DBC. EMT employs greedy action selection to provide an efficient control algorithm in Markovian environments. We exploit this efficiency in a set of experiments that applied multitarget EMT to a class of area-sweeping problems (searching for moving targets). We show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation. Categories and Subject Descriptors I.2.8 [Problem Solving, Control Methods, and Search]: Control Theory; I.2.9 [Robotics]; I.2.11 [Distributed Artificial Intelligence]: Intelligent Agents General Terms Algorithms, Theory 1. INTRODUCTION Planning and control constitutes a central research area in multiagent systems and artificial intelligence. In recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in stochastic environments. In this framework, the planning and control problem is often addressed by imposing a reward function, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility. While theoretically attractive, the complexity of optimally solving a POMDP is prohibitive [8, 7]. We take an alternative view of planning in stochastic environments. We do not use a (state-based) reward function, but instead optimize over a different criterion, a transition-based specification of the desired system dynamics. The idea here is to view planexecution as a process that compels a (stochastic) system to change, and a plan as a dynamic process that shapes that change according to desired criteria. We call this general planning framework Dynamics Based Control (DBC). In DBC, the goal of a planning (or control) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) target dynamics. As actual system behavior may deviate from that which is specified by target dynamics (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16]. Here, optimality is measured in terms of probability of deviation magnitudes. In this paper, we present the structure of Dynamics Based Control. We show that the recently developed Extended Markov Tracking (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC. EMT is an efficient instantiation of DBC. To evaluate DBC, we carried out a set of experiments applying multi-target EMT to the Tag Game [11]; this is a variant on the area sweeping problem, where an agent is trying to tag a moving target (quarry) whose position is not known with certainty. Experimental data demonstrates that even with a simple model of the environment and a simple design of target dynamics, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agents position). The paper is organized as follows. In Section 2 we motivate DBC using area-sweeping problems, and discuss related work. Section 3 introduces the Dynamics Based Control (DBC) structure, and its specialization to Markovian environments. This is followed by a review of the Extended Markov Tracking (EMT) approach as a DBC-structured control regimen in Section 4. That section also discusses the limitations of EMT-based control relative to the general DBC framework. Experimental settings and results are then presented in Section 5. Section 6 provides a short discussion of the overall approach, and Section 7 gives some concluding remarks and directions for future work. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2. MOTIVATION AND RELATED WORK Many real-life scenarios naturally have a stochastic target dynamics specification, especially those domains where there exists no ultimate goal, but rather system behavior (with specific properties) that has to be continually supported. For example, security guards perform persistent sweeps of an area to detect any sign of intrusion. Cunning thieves will attempt to track these sweeps, and time their operation to key points of the guards motion. It is thus advisable to make the guards motion dynamics appear irregular and random. Recent work by Paruchuri et al. [10] has addressed such randomization in the context of single-agent and distributed POMDPs. The goal in that work was to generate policies that provide a measure of action-selection randomization, while maintaining rewards within some acceptable levels. Our focus differs from this work in that DBC does not optimize expected rewards-indeed we do not consider rewards at all-but instead maintains desired dynamics (including, but not limited to, randomization). The Game of Tag is another example of the applicability of the approach. It was introduced in the work by Pineau et al. [11]. There are two agents that can move about an area, which is divided into a grid. The grid may have blocked cells (holes) into which no agent can move. One agent (the hunter) seeks to move into a cell occupied by the other (the quarry), such that they are co-located (this is a successful tag). The quarry seeks to avoid the hunter agent, and is always aware of the hunters position, but does not know how the hunter will behave, which opens up the possibility for a hunter to surprise the prey. The hunter knows the quarrys probabilistic law of motion, but does not know its current location. Tag is an instance of a family of area-sweeping (pursuit-evasion) problems. In [11], the hunter modeled the problem using a POMDP. A reward function was defined, to reflect the desire to tag the quarry, and an action policy was computed to optimize the reward collected over time. Due to the intractable complexity of determining the optimal policy, the action policy computed in that paper was essentially an approximation. In this paper, instead of formulating a reward function, we use EMT to solve the problem, by directly specifying the target dynamics. In fact, any search problem with randomized motion, the socalled class of area sweeping problems, can be described through specification of such target system dynamics. Dynamics Based Control provides a natural approach to solving these problems. 3. DYNAMICS BASED CONTROL The specification of Dynamics Based Control (DBC) can be broken into three interacting levels: Environment Design Level, User Level, and Agent Level. • Environment Design Level is concerned with the formal specification and modeling of the environment. For example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation constant. • User Level in turn relies on the environment model produced by Environment Design to specify the target system dynamics it wishes to observe. The User Level also specifies the estimation or learning procedure for system dynamics, and the measure of deviation. In the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping. • Agent Level in turn combines the environment model from the Environment Design level, the dynamics estimation procedure, the deviation measure and the target dynamics specification from User Level, to produce a sequence of actions that create system dynamics as close as possible to the targeted specification. As we are interested in the continual development of a stochastic system, such as happens in classical control theory [16] and continual planning [4], as well as in our example of museum sweeps, the question becomes how the Agent Level is to treat the deviation measurements over time. To this end, we use a probability threshold-that is, we would like the Agent Level to maximize the probability that the deviation measure will remain below a certain threshold. Specific action selection then depends on system formalization. One possibility would be to create a mixture of available system trends, much like that which happens in Behavior-Based Robotic architectures [1]. The other alternative would be to rely on the estimation procedure provided by the User Level-to utilize the Environment Design Level model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a certain dynamics has been achieved. Notice that this manipulation is not direct, but via the environment. Thus, for strong enough estimator algorithms, successful manipulation would mean a successful simulation of the specified target dynamics (i.e., beyond discerning via the available sensory input). DBC levels can also have a back-flow of information (see Figure 1). For instance, the Agent Level could provide data about target dynamics feasibility, allowing the User Level to modify the requirement, perhaps focusing on attainable features of system behavior. Data would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the User Level, this can provide an important tool for the environment model calibration at the Environment Design Level. UserEnv. Design Agent Model Ideal Dynamics Estimator Estimator Dynamics Feasibility System Response Data Figure 1: Data flow of the DBC framework Extending upon the idea of Actor-Critic algorithms [5], DBC data flow can provide a good basis for the design of a learning algorithm. For example, the User Level can operate as an exploratory device for a learning algorithm, inferring an ideal dynamics target from the environment model at hand that would expose and verify most critical features of system behavior. In this case, feasibility and system response data from the Agent Level would provide key information for an environment model update. In fact, the combination of feasibility and response data can provide a basis for the application of strong learning algorithms such as EM [2, 9]. 3.1 DBC for Markovian Environments For a Partially Observable Markovian Environment, DBC can be specified in a more rigorous manner. Notice how DBC discards rewards, and replaces it by another optimality criterion (structural differences are summarized in Table 1): • Environment Design level is to specify a tuple < S, A, T, O, Ω, s0 >, where: - S is the set of all possible environment states; - s0 is the initial state of the environment (which can also be viewed as a probability distribution over S); The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 791 - A is the set of all possible actions applicable in the environment; - T is the environments probabilistic transition function: T : S ×A → Π(S). That is, T(s |a, s) is the probability that the environment will move from state s to state s under action a; - O is the set of all possible observations. This is what the sensor input would look like for an outside observer; - Ω is the observation probability function: Ω : S × A × S → Π(O). That is, Ω(o|s , a, s) is the probability that one will observe o given that the environment has moved from state s to state s under action a. • User Level, in the case of Markovian environment, operates on the set of system dynamics described by a family of conditional probabilities F = {τ : S × A → Π(S)}. Thus specification of target dynamics can be expressed by q ∈ F, and the learning or tracking algorithm can be represented as a function L : O×(A×O)∗ → F; that is, it maps sequences of observations and actions performed so far into an estimate τ ∈ F of system dynamics. There are many possible variations available at the User Level to define divergence between system dynamics; several of them are: - Trace distance or L1 distance between two distributions p and q defined by D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Fidelity measure of distance F(p(·), q(·)) = x p(x)q(x) - Kullback-Leibler divergence DKL(p(·) q(·)) = x p(x) log p(x) q(x) Notice that the latter two are not actually metrics over the space of possible distributions, but nevertheless have meaningful and important interpretations. For instance, KullbackLeibler divergence is an important tool of information theory [3] that allows one to measure the price of encoding an information source governed by q, while assuming that it is governed by p. The User Level also defines the threshold of dynamics deviation probability θ. • Agent Level is then faced with a problem of selecting a control signal function a∗ to satisfy a minimization problem as follows: a∗ = arg min a Pr(d(τa, q) > θ) where d(τa, q) is a random variable describing deviation of the dynamics estimate τa, created by L under control signal a, from the ideal dynamics q. Implicit in this minimization problem is that L is manipulated via the environment, based on the environment model produced by the Environment Design Level. 3.2 DBC View of the State Space It is important to note the complementary view that DBC and POMDPs take on the state space of the environment. POMDPs regard state as a stationary snap-shot of the environment; whatever attributes of state sequencing one seeks are reached through properties of the control process, in this case reward accumulation. This can be viewed as if the sequencing of states and the attributes of that sequencing are only introduced by and for the controlling mechanism-the POMDP policy. DBC concentrates on the underlying principle of state sequencing, the system dynamics. DBCs target dynamics specification can use the environments state space as a means to describe, discern, and preserve changes that occur within the system. As a result, DBC has a greater ability to express state sequencing properties, which are grounded in the environment model and its state space definition. For example, consider the task of moving through rough terrain towards a goal and reaching it as fast as possible. POMDPs would encode terrain as state space points, while speed would be ensured by negative reward for every step taken without reaching the goalaccumulating higher reward can be reached only by faster motion. Alternatively, the state space could directly include the notion of speed. For POMDPs, this would mean that the same concept is encoded twice, in some sense: directly in the state space, and indirectly within reward accumulation. Now, even if the reward function would encode more, and finer, details of the properties of motion, the POMDP solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure. On the other hand, the tactical target expression of variations and correlations between position and speed of motion is now grounded in the state space representation. In this situation, any further constraints, e.g., smoothness of motion, speed limits in different locations, or speed reductions during sharp turns, are explicitly and uniformly expressed by the tactical target, and can result in faster and more effective action selection by a DBC algorithm. 4. EMT-BASED CONTROL AS A DBC Recently, a control algorithm was introduced called EMT-based Control [13], which instantiates the DBC framework. Although it provides an approximate greedy solution in the DBC sense, initial experiments using EMT-based control have been encouraging [14, 15]. EMT-based control is based on the Markovian environment definition, as in the case with POMDPs, but its User and Agent Levels are of the Markovian DBC type of optimality. • User Level of EMT-based control defines a limited-case target system dynamics independent of action: qEMT : S → Π(S). It then utilizes the Kullback-Leibler divergence measure to compose a momentary system dynamics estimator-the Extended Markov Tracking (EMT) algorithm. The algorithm keeps a system dynamics estimate τt EMT that is capable of explaining recent change in an auxiliary Bayesian system state estimator from pt−1 to pt, and updates it conservatively using Kullback-Leibler divergence. Since τt EMT and pt−1,t are respectively the conditional and marginal probabilities over the systems state space, explanation simply means that pt(s ) = s τt EMT (s |s)pt−1(s), and the dynamics estimate update is performed by solving a 792 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: Structure of POMDP vs. Dynamics-Based Control in Markovian Environment Level Approach MDP Markovian DBC Environment < S, A, T, O, Ω >,where S - set of states A - set of actions Design T : S × A → Π(S) - transition O - observation set Ω : S × A × S → Π(O) User r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - reward function q - ideal dynamics F - reward remodeling L - dynamics estimator θ - threshold Agent π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) minimization problem: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • Agent Level in EMT-based control is suboptimal with respect to DBC (though it remains within the DBC framework), performing greedy action selection based on prediction of EMTs reaction. The prediction is based on the environment model provided by the Environment Design level, so that if we denote by Ta the environments transition function limited to action a, and pt−1 is the auxiliary Bayesian system state estimator, then the EMT-based control choice is described by a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1) Note that this follows the Markovian DBC framework precisely: the rewarding optimality of POMDPs is discarded, and in its place a dynamics estimator (EMT in this case) is manipulated via action effects on the environment to produce an estimate close to the specified target system dynamics. Yet as we mentioned, naive EMTbased control is suboptimal in the DBC sense, and has several additional limitations that do not exist in the general DBC framework (discussed in Section 4.2). 4.1 Multi-Target EMT At times, there may exist several behavioral preferences. For example, in the case of museum guards, some art items are more heavily guarded, requiring that the guards stay more often in their close vicinity. On the other hand, no corner of the museum is to be left unchecked, which demands constant motion. Successful museum security would demand that the guards adhere to, and balance, both of these behaviors. For EMT-based control, this would mean facing several tactical targets {qk}K k=1, and the question becomes how to merge and balance them. A balancing mechanism can be applied to resolve this issue. Note that EMT-based control, while selecting an action, creates a preference vector over the set of actions based on their predicted performance with respect to a given target. If these preference vectors are normalized, they can be combined into a single unified preference. This requires replacement of standard EMT-based action selection by the algorithm below [15]: • Given: - a set of target dynamics {qk}K k=1, - vector of weights w(k) • Select action as follows - For each action a ∈ A predict the future state distribution ¯pa t+1 = Ta ∗ pt; - For each action, compute Da = H(¯pa t+1, pt, PDt) - For each a ∈ A and qk tactical target, denote V (a, k) = DKL (Da qk) pt . Let Vk(a) = 1 Zk V (a, k), where Zk = a∈A V (a, k) is a normalization factor. - Select a∗ = arg min a k k=1 w(k)Vk(a) The weights vector w = (w1, ..., wK ) allows the additional tuning of importance among target dynamics without the need to redesign the targets themselves. This balancing method is also seamlessly integrated into the EMT-based control flow of operation. 4.2 EMT-based Control Limitations EMT-based control is a sub-optimal (in the DBC sense) representative of the DBC structure. It limits the User by forcing EMT to be its dynamic tracking algorithm, and replaces Agent optimization by greedy action selection. This kind of combination, however, is common for on-line algorithms. Although further development of EMT-based controllers is necessary, evidence so far suggests that even the simplest form of the algorithm possesses a great deal of power, and displays trends that are optimal in the DBC sense of the word. There are two further, EMT-specific, limitations to EMT-based control that are evident at this point. Both already have partial solutions and are subjects of ongoing research. The first limitation is the problem of negative preference. In the POMDP framework for example, this is captured simply, through The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 793 the appearance of values with different signs within the reward structure. For EMT-based control, however, negative preference means that one would like to avoid a certain distribution over system development sequences; EMT-based control, however, concentrates on getting as close as possible to a distribution. Avoidance is thus unnatural in native EMT-based control. The second limitation comes from the fact that standard environment modeling can create pure sensory actions-actions that do not change the state of the world, and differ only in the way observations are received and the quality of observations received. Since the world state does not change, EMT-based control would not be able to differentiate between different sensory actions. Notice that both of these limitations of EMT-based control are absent from the general DBC framework, since it may have a tracking algorithm capable of considering pure sensory actions and, unlike Kullback-Leibler divergence, a distribution deviation measure that is capable of dealing with negative preference. 5. EMT PLAYING TAG The Game of Tag was first introduced in [11]. It is a single agent problem of capturing a quarry, and belongs to the class of area sweeping problems. An example domain is shown in Figure 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figure 2: Tag domain; an agent (A) attempts to seek and capture a quarry (Q) The Game of Tag extremely limits the agents perception, so that the agent is able to detect the quarry only if they are co-located in the same cell of the grid world. In the classical version of the game, co-location leads to a special observation, and the Tag action can be performed. We slightly modify this setting: the moment that both agents occupy the same cell, the game ends. As a result, both the agent and its quarry have the same motion capability, which allows them to move in four directions, North, South, East, and West. These form a formal space of actions within a Markovian environment. The state space of the formal Markovian environment is described by the cross-product of the agent and quarrys positions. For Figure 2, it would be S = {s0, ..., s23} × {s0, ..., s23}. The effects of an action taken by the agent are deterministic, but the environment in general has a stochastic response due to the motion of the quarry. With probability q0 1 it stays put, and with probability 1 − q0 it moves to an adjacent cell further away from the 1 In our experiments this was taken to be q0 = 0.2. agent. So for the instance shown in Figure 2 and q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Although the evasive behavior of the quarry is known to the agent, the quarrys position is not. The only sensory information available to the agent is its own location. We use EMT and directly specify the target dynamics. For the Game of Tag, we can easily formulate three major trends: catching the quarry, staying mobile, and stalking the quarry. This results in the following three target dynamics: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 otherwise Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 otherwise Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Note that none of the above targets are directly achievable; for instance, if Qt = s9 and At = s11, there is no action that can move the agent to At+1 = s9 as required by the Tcatch target dynamics. We ran several experiments to evaluate EMT performance in the Tag Game. Three configurations of the domain shown in Figure 3 were used, each posing a different challenge to the agent due to partial observability. In each setting, a set of 1000 runs was performed with a time limit of 100 steps. In every run, the initial position of both the agent and its quarry was selected at random; this means that as far as the agent was concerned, the quarrys initial position was uniformly distributed over the entire domain cell space. We also used two variations of the environment observability function. In the first version, observability function mapped all joint positions of hunter and quarry into the position of the hunter as an observation. In the second, only those joint positions in which hunter and quarry occupied different locations were mapped into the hunters location. The second version in fact utilized and expressed the fact that once hunter and quarry occupy the same cell the game ends. The results of these experiments are shown in Table 2. Balancing [15] the catch, move, and stalk target dynamics described in the previous section by the weight vector [0.8, 0.1, 0.1], EMT produced stable performance in all three domains. Although direct comparisons are difficult to make, the EMT performance displayed notable efficiency vis-`a-vis the POMDP approach. In spite of a simple and inefficient Matlab implementation of the EMT algorithm, the decision time for any given step averaged significantly below 1 second in all experiments. For the irregular open arena domain, which proved to be the most difficult, 1000 experiment runs bounded by 100 steps each, a total of 42411 steps, were completed in slightly under 6 hours. That is, over 4 × 104 online steps took an order of magnitude less time than the offline computation of POMDP policy in [11]. The significance of this differential is made even more prominent by the fact that, should the environment model parameters change, the online nature of EMT would allow it to maintain its performance time, while the POMDP policy would need to be recomputed, requiring yet again a large overhead of computation. We also tested the behavior cell frequency entropy, empirical measures from trial data. As Figure 4 and Figure 5 show, empir794 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) A Q Q A 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 A Q Figure 3: These configurations of the Tag Game space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor Table 2: Performance of the EMT-based solution in three Tag Game domains and two observability models: I) omniposition quarry, II) quarry is not at hunters position Model Domain Capture% E(Steps) Time/Step I Dead-ends 100 14.8 72(mSec) Arena 80.2 42.4 500(mSec) Circle 91.4 34.6 187(mSec) II Dead-ends 100 13.2 91(mSec) Arena 96.8 28.67 396(mSec) Circle 94.4 31.63 204(mSec) ical entropy grows with the length of interaction. For runs where the quarry was not captured immediately, the entropy reaches between 0.85 and 0.952 for different runs and scenarios. As the agent actively seeks the quarry, the entropy never reaches its maximum. One characteristic of the entropy graph for the open arena scenario particularly caught our attention in the case of the omniposition quarry observation model. Near the maximum limit of trial length (100 steps), entropy suddenly dropped. Further analysis of the data showed that under certain circumstances, a fluctuating behavior occurs in which the agent faces equally viable versions of quarry-following behavior. Since the EMT algorithm has greedy action selection, and the state space does not encode any form of commitment (not even speed or acceleration), the agent is locked within a small portion of cells. It is essentially attempting to simultaneously follow several courses of action, all of which are consistent with the target dynamics. This behavior did not occur in our second observation model, since it significantly reduced the set of eligible courses of action-essentially contributing to tie-breaking among them. 6. DISCUSSION The design of the EMT solution for the Tag Game exposes the core difference in approach to planning and control between EMT or DBC, on the one hand, and the more familiar POMDP approach, on the other. POMDP defines a reward structure to optimize, and influences system dynamics indirectly through that optimization. EMT discards any reward scheme, and instead measures and influences system dynamics directly. 2 Entropy was calculated using log base equal to the number of possible locations within the domain; this properly scales entropy expression into the range [0, 1] for all domains. Thus for the Tag Game, we did not search for a reward function that would encode and express our preference over the agents behavior, but rather directly set three (heuristic) behavior preferences as the basis for target dynamics to be maintained. Experimental data shows that these targets need not be directly achievable via the agents actions. However, the ratio between EMT performance and achievability of target dynamics remains to be explored. The tag game experiment data also revealed the different emphasis DBC and POMDPs place on the formulation of an environment state space. POMDPs rely entirely on the mechanism of reward accumulation maximization, i.e., formation of the action selection procedure to achieve necessary state sequencing. DBC, on the other hand, has two sources of sequencing specification: through the properties of an action selection procedure, and through direct specification within the target dynamics. The importance of the second source was underlined by the Tag Game experiment data, in which the greedy EMT algorithm, applied to a POMDP-type state space specification, failed, since target description over such a state space was incapable of encoding the necessary behavior tendencies, e.g., tie-breaking and commitment to directed motion. The structural differences between DBC (and EMT in particular), and POMDPs, prohibits direct performance comparison, and places them on complementary tracks, each within a suitable niche. For instance, POMDPs could be seen as a much more natural formulation of economic sequential decision-making problems, while EMT is a better fit for continual demand for stochastic change, as happens in many robotic or embodied-agent problems. The complementary properties of POMDPs and EMT can be further exploited. There is recent interest in using POMDPs in hybrid solutions [17], in which the POMDPs can be used together with other control approaches to provide results not easily achievable with either approach by itself. DBC can be an effective partner in such a hybrid solution. For instance, POMDPs have prohibitively large off-line time requirements for policy computation, but can be readily used in simpler settings to expose beneficial behavioral trends; this can serve as a form of target dynamics that are provided to EMT in a larger domain for on-line operation. 7. CONCLUSIONS AND FUTURE WORK In this paper, we have presented a novel perspective on the process of planning and control in stochastic environments, in the form of the Dynamics Based Control (DBC) framework. DBC formulates the task of planning as support of a specified target system dynamics, which describes the necessary properties of change within the environment. Optimality of DBC plans of action are measured The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 4: Observation Model I: Omniposition quarry. Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 5: Observation Model II: quarry not observed at hunters position. Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. with respect to the deviation of actual system dynamics from the target dynamics. We show that a recently developed technique of Extended Markov Tracking (EMT) [13] is an instantiation of DBC. In fact, EMT can be seen as a specific case of DBC parameterization, which employs a greedy action selection procedure. Since EMT exhibits the key features of the general DBC framework, as well as polynomial time complexity, we used the multitarget version of EMT [15] to demonstrate that the class of area sweeping problems naturally lends itself to dynamics-based descriptions, as instantiated by our experiments in the Tag Game domain. As enumerated in Section 4.2, EMT has a number of limitations, such as difficulty in dealing with negative dynamic preference. This prevents direct application of EMT to such problems as Rendezvous-Evasion Games (e.g., [6]). However, DBC in general has no such limitations, and readily enables the formulation of evasion games. In future work, we intend to proceed with the development of dynamics-based controllers for these problems. 8. ACKNOWLEDGMENT The work of the first two authors was partially supported by Israel Science Foundation grant #898/05, and the third author was partially supported by a grant from Israels Ministry of Science and Technology. 9. REFERENCES [1] R. C. Arkin. Behavior-Based Robotics. MIT Press, 1998. [2] J. A. Bilmes. A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and Hidden Markov Models. Technical Report TR-97-021, Department of Electrical Engeineering and Computer Science, University of California at Berkeley, 1998. [3] T. M. Cover and J. A. Thomas. Elements of information theory. Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz, and M. J. Wolverton. A survey of research in distributed, continual planning. AI Magazine, 4:13-22, 1999. [5] V. R. Konda and J. N. Tsitsiklis. Actor-Critic algorithms. SIAM Journal on Control and Optimization, 42(4):1143-1166, 2003. [6] W. S. Lim. A rendezvous-evasion game on discrete locations with joint randomization. Advances in Applied Probability, 29(4):1004-1017, December 1997. [7] M. L. Littman, T. L. Dean, and L. P. Kaelbling. On the complexity of solving Markov decision problems. In Proceedings of the 11th Annual Conference on Uncertainty in Artificial Intelligence (UAI-95), pages 394-402, 1995. [8] O. Madani, S. Hanks, and A. Condon. On the undecidability of probabilistic planning and related stochastic optimization problems. Artificial Intelligence Journal, 147(1-2):5-34, July 2003. [9] R. M. Neal and G. E. Hinton. A view of the EM algorithm 796 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) that justifies incremental, sparse, and other variants. In M. I. Jordan, editor, Learning in Graphical Models, pages 355-368. Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus. Security in multiagent systems by policy randomization. In Proceeding of AAMAS 2006, 2006. [11] J. Pineau, G. Gordon, and S. Thrun. Point-based value iteration: An anytime algorithm for pomdps. In International Joint Conference on Artificial Intelligence (IJCAI), pages 1025-1032, August 2003. [12] M. L. Puterman. Markov Decision Processes. Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics Section. Wiley-Interscience Publication, New York, 1994. [13] Z. Rabinovich and J. S. Rosenschein. Extended Markov Tracking with an application to control. In The Workshop on Agent Tracking: Modeling Other Agents from Observations, at the Third International Joint Conference on Autonomous Agents and Multiagent Systems, pages 95-100, New-York, July 2004. [14] Z. Rabinovich and J. S. Rosenschein. Multiagent coordination by Extended Markov Tracking. In The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 431-438, Utrecht, The Netherlands, July 2005. [15] Z. Rabinovich and J. S. Rosenschein. On the response of EMT-based control to interacting targets and models. In The Fifth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 465-470, Hakodate, Japan, May 2006. [16] R. F. Stengel. Optimal Control and Estimation. Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, and P. Varakantham. Conflicts in teamwork: Hybrids to the The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 797",
    "original_translation": "En este artículo presentamos el Control Basado en Dinámicas (DBC), un enfoque para la planificación y control de un agente en entornos estocásticos. A diferencia de los enfoques existentes, que buscan optimizar las recompensas esperadas (por ejemplo, en Problemas de Decisión de Markov Parcialmente Observables (POMDP)), DBC optimiza el comportamiento del sistema hacia las dinámicas del sistema especificadas. Mostramos que un enfoque de planificación y control recientemente desarrollado, Seguimiento Extendido de Markov (EMT), es una instancia de DBC. EMT emplea la selección de acciones codiciosa para proporcionar un algoritmo de control eficiente en entornos markovianos. Explotamos esta eficiencia en un conjunto de experimentos que aplicaron EMT multitarget a una clase de problemas de barrido de área (búsqueda de objetivos en movimiento). Mostramos que tales problemas pueden ser definidos de forma natural y resueltos eficientemente utilizando el marco de DBC y su instanciación de EMT. Categorías y Descriptores de Asignaturas I.2.8 [Resolución de Problemas, Métodos de Control y Búsqueda]: Teoría de Control; I.2.9 [Robótica]; I.2.11 [Inteligencia Artificial Distribuida]: Agentes Inteligentes Términos Generales Algoritmos, Teoría 1. INTRODUCCIÓN La planificación y el control constituyen un área de investigación central en sistemas multiagentes e inteligencia artificial. En los últimos años, los Procesos de Decisión de Markov Parcialmente Observables (POMDPs) [12] se han convertido en una base formal popular para la planificación en entornos estocásticos. En este marco, el problema de planificación y control suele abordarse imponiendo una función de recompensa y calculando una política (de elección de acciones) que sea óptima, en el sentido de que resultará en la mayor utilidad esperada. Si bien teóricamente atractivo, la complejidad de resolver óptimamente un POMDP es prohibitiva [8, 7]. Tomamos una visión alternativa de la planificación en entornos estocásticos. No utilizamos una función de recompensa basada en el estado, sino que optimizamos según un criterio diferente, una especificación basada en transiciones de la dinámica del sistema deseado. La idea aquí es ver la ejecución del plan como un proceso que obliga a un sistema (estocástico) a cambiar, y un plan como un proceso dinámico que moldea ese cambio de acuerdo con criterios deseados. Llamamos a este marco de planificación general Control Basado en Dinámicas (DBC). En DBC, el objetivo de un proceso de planificación (o control) es garantizar que el sistema cambiará de acuerdo con dinámicas objetivo específicas (potencialmente estocásticas). Dado que el comportamiento real del sistema puede desviarse de lo especificado por la dinámica objetivo (debido a la naturaleza estocástica del sistema), la planificación en tales entornos debe ser continua, de manera similar a los controladores en lazo cerrado clásicos. Aquí, la optimalidad se mide en términos de la probabilidad de magnitudes de desviación. En este documento, presentamos la estructura del Control Basado en Dinámicas. Mostramos que el enfoque de Seguimiento Extendido de Markov (EMT) recientemente desarrollado [13, 14, 15] está subsumido por DBC, con EMT empleando selección de acciones codiciosa, que es una parametrización específica entre las opciones posibles dentro de DBC. EMT es una implementación eficiente de DBC. Para evaluar DBC, llevamos a cabo un conjunto de experimentos aplicando EMT de múltiples objetivos al Juego de Etiquetas [11]; esta es una variante del problema de barrido de área, donde un agente intenta etiquetar a un objetivo móvil (presa) cuya posición no se conoce con certeza. Los datos experimentales demuestran que incluso con un modelo simple del entorno y un diseño simple de la dinámica del objetivo, se pueden lograr altas tasas de éxito tanto en la captura de la presa como en sorprender a la presa (como se expresa por la entropía observada de la posición de los agentes controlados). El documento está organizado de la siguiente manera. En la Sección 2 motivamos DBC utilizando problemas de barrido de área, y discutimos trabajos relacionados. La Sección 3 introduce la estructura de Control Basado en Dinámicas (DBC) y su especialización en entornos Markovianos. Esto es seguido por una revisión del enfoque de Seguimiento Markov Extendido (EMT) como un régimen de control estructurado por DBC en la Sección 4. Esa sección también discute las limitaciones del control basado en EMT en relación con el marco general de DBC. Los ajustes experimentales y los resultados se presentan a continuación en la Sección 5. La Sección 6 proporciona una breve discusión del enfoque general, y la Sección 7 ofrece algunas observaciones finales y direcciones para trabajos futuros. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2. MOTIVACIÓN Y TRABAJO RELACIONADO Muchos escenarios de la vida real tienen naturalmente una especificación de dinámica de destino estocástica, especialmente en aquellos dominios donde no existe un objetivo final, sino más bien un comportamiento del sistema (con propiedades específicas) que debe ser continuamente respaldado. Por ejemplo, los guardias de seguridad realizan barridos persistentes de un área para detectar cualquier signo de intrusión. Los ladrones astutos intentarán rastrear estos barridos y sincronizar su operación con puntos clave del movimiento de los guardias. Por lo tanto, es recomendable que la dinámica de movimiento de los guardias parezca irregular y aleatoria. El trabajo reciente de Paruchuri et al. [10] ha abordado dicha aleatorización en el contexto de POMDPs de agente único y distribuidos. El objetivo de ese trabajo era generar políticas que proporcionen una medida de aleatorización en la selección de acciones, manteniendo al mismo tiempo las recompensas dentro de niveles aceptables. Nuestro enfoque difiere de este trabajo en que DBC no optimiza recompensas esperadas, de hecho, no consideramos recompensas en absoluto, sino que en su lugar mantenemos dinámicas deseadas (incluyendo, pero no limitado a, la aleatorización). El juego de la mancha es otro ejemplo de la aplicabilidad del enfoque. Fue introducido en el trabajo de Pineau et al. [11]. Hay dos agentes que pueden moverse por un área, la cual está dividida en una cuadrícula. La cuadrícula puede tener celdas bloqueadas (agujeros) en las cuales ningún agente puede moverse. Un agente (el cazador) intenta moverse a una celda ocupada por el otro (la presa), de modo que estén ubicados en el mismo lugar (esto es una etiqueta exitosa). La presa busca evitar al cazador, siempre está al tanto de la posición del cazador, pero no sabe cómo se comportará el cazador, lo que abre la posibilidad de que un cazador sorprenda a la presa. El cazador conoce la ley de movimiento probabilístico de la presa, pero no conoce su ubicación actual. Tag es una instancia de una familia de problemas de barrido de área (persecución-evasión). En [11], el cazador modeló el problema utilizando un POMDP. Se definió una función de recompensa para reflejar el deseo de marcar la presa, y se calculó una política de acción para optimizar la recompensa recolectada con el tiempo. Debido a la complejidad intratable de determinar la política óptima, la política de acción calculada en ese documento fue esencialmente una aproximación. En este artículo, en lugar de formular una función de recompensa, utilizamos EMT para resolver el problema, especificando directamente la dinámica objetivo. De hecho, cualquier problema de búsqueda con movimiento aleatorio, la llamada clase de problemas de barrido de área, puede ser descrito a través de la especificación de la dinámica de dicho sistema objetivo. El Control Basado en Dinámicas proporciona un enfoque natural para resolver estos problemas. CONTROL BASADO EN DINÁMICA La especificación del Control Basado en Dinámica (DBC) se puede dividir en tres niveles interactivos: Nivel de Diseño del Entorno, Nivel de Usuario y Nivel de Agente. • El Nivel de Diseño del Entorno se ocupa de la especificación formal y modelado del entorno. Por ejemplo, este nivel especificaría las leyes de la física dentro del sistema y establecería sus parámetros, como la constante de gravitación. El Nivel de Usuario, a su vez, se basa en el modelo de entorno producido por el Diseño de Entorno para especificar la dinámica del sistema objetivo que desea observar. El Nivel de Usuario también especifica el procedimiento de estimación o aprendizaje para la dinámica del sistema, y la medida de desviación. En el escenario del guardia de museo mencionado anteriormente, estos corresponderían a un horario de barrido estocástico y una medida de sorpresa relativa entre el barrido especificado y el real. • El Nivel de Agente, a su vez, combina el modelo del entorno del Nivel de Diseño de Entorno, el procedimiento de estimación de la dinámica, la medida de desviación y la especificación de la dinámica objetivo del Nivel de Usuario, para producir una secuencia de acciones que creen dinámicas del sistema lo más cercanas posible a la especificación deseada. Dado nuestro interés en el desarrollo continuo de un sistema estocástico, como ocurre en la teoría de control clásica [16] y la planificación continua [4], así como en nuestro ejemplo de recorridos por museos, la pregunta es cómo el Nivel de Agente debe tratar las mediciones de desviación a lo largo del tiempo. Con este fin, utilizamos un umbral de probabilidad, es decir, nos gustaría que el Nivel del Agente maximice la probabilidad de que la medida de desviación permanezca por debajo de cierto umbral. La selección de acciones específicas depende entonces de la formalización del sistema. Una posibilidad sería crear una mezcla de tendencias de sistemas disponibles, similar a lo que sucede en las arquitecturas robóticas basadas en el comportamiento [1]. La otra alternativa sería depender del procedimiento de estimación proporcionado por el Nivel de Usuario para utilizar el modelo del Nivel de Diseño del Entorno del entorno para elegir acciones, de manera que se manipule al estimador de dinámicas para que crea que se ha logrado cierta dinámica. Ten en cuenta que esta manipulación no es directa, sino a través del entorno. Por lo tanto, para algoritmos de estimación lo suficientemente fuertes, la manipulación exitosa significaría una simulación exitosa de la dinámica objetivo especificada (es decir, más allá de discernir a través de la entrada sensorial disponible). Los niveles de DBC también pueden tener un retroceso de información (ver Figura 1). Por ejemplo, el Nivel de Agente podría proporcionar datos sobre la viabilidad de la dinámica del objetivo, permitiendo al Nivel de Usuario modificar el requisito, quizás centrándose en las características alcanzables del comportamiento del sistema. Los datos también estarían disponibles sobre la respuesta del sistema a diferentes acciones realizadas; combinados con un estimador de dinámica definido por el Nivel de Usuario, esto puede proporcionar una herramienta importante para la calibración del modelo del entorno en el Nivel de Diseño del Entorno. UserEnv. El modelo de agente de diseño Estimador de Dinámicas Ideales Estimador de Dinámicas Factibilidad de Respuesta del Sistema de Datos Figura 1: Flujo de datos del marco de trabajo DBC Ampliando la idea de los algoritmos Actor-Crítico [5], el flujo de datos de DBC puede proporcionar una buena base para el diseño de un algoritmo de aprendizaje. Por ejemplo, el Nivel de Usuario puede funcionar como un dispositivo exploratorio para un algoritmo de aprendizaje, inferir un objetivo dinámico ideal a partir del modelo del entorno en cuestión que expondría y verificaría las características más críticas del comportamiento del sistema. En este caso, los datos de viabilidad y respuesta del sistema a nivel de agente proporcionarían información clave para una actualización del modelo del entorno. De hecho, la combinación de datos de viabilidad y respuesta puede proporcionar una base para la aplicación de algoritmos de aprendizaje sólidos como EM [2, 9]. 3.1 DBC para entornos markovianos Para un entorno markoviano parcialmente observable, DBC puede especificarse de una manera más rigurosa. Observa cómo DBC descarta las recompensas y las reemplaza por otro criterio de optimalidad (las diferencias estructurales se resumen en la Tabla 1): • El nivel de Diseño del Entorno es especificar una tupla < S, A, T, O, Ω, s0 >, donde: - S es el conjunto de todos los posibles estados del entorno; - s0 es el estado inicial del entorno (que también puede ser visto como una distribución de probabilidad sobre S); La Sexta Conferencia Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 791 - A es el conjunto de todas las acciones posibles aplicables en el entorno; - T es la función de transición probabilística del entorno: T: S × A → Π(S). Es decir, T(s | a, s) es la probabilidad de que el entorno se mueva del estado s al estado s bajo la acción a; - O es el conjunto de todas las observaciones posibles. Así es como se vería la entrada del sensor para un observador externo; - Ω es la función de probabilidad de observación: Ω: S × A × S → Π(O). Es decir, Ω(o|s, a, s) es la probabilidad de observar o dado que el entorno ha pasado del estado s al estado s bajo la acción a. • Nivel de Usuario, en el caso de un entorno markoviano, opera en el conjunto de dinámicas del sistema descritas por una familia de probabilidades condicionales F = {τ: S × A → Π(S)}. Por lo tanto, la especificación de la dinámica del objetivo puede expresarse como q ∈ F, y el algoritmo de aprendizaje o seguimiento puede representarse como una función L : O×(A×O)∗ → F; es decir, mapea secuencias de observaciones y acciones realizadas hasta el momento en una estimación τ ∈ F de la dinámica del sistema. Hay muchas variaciones posibles disponibles a nivel de usuario para definir la divergencia entre la dinámica del sistema; varias de ellas son: - Distancia de traza o distancia L1 entre dos distribuciones p y q definida por D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Medida de fidelidad de distancia F(p(·), q(·)) = x p(x)q(x) - Divergencia de Kullback-Leibler DKL(p(·) q(·)) = x p(x) log p(x) q(x) Nótese que las dos últimas no son en realidad métricas sobre el espacio de distribuciones posibles, pero tienen interpretaciones significativas e importantes. Por ejemplo, la divergencia de Kullback-Leibler es una herramienta importante de la teoría de la información [3] que permite medir el costo de codificar una fuente de información gobernada por q, asumiendo que está gobernada por p. El Nivel de Usuario también define el umbral de probabilidad de desviación de la dinámica θ. • El Nivel de Agente se enfrenta entonces a un problema de seleccionar una función de señal de control a∗ para satisfacer un problema de minimización de la siguiente manera: a∗ = arg min a Pr(d(τa, q) > θ) donde d(τa, q) es una variable aleatoria que describe la desviación de la estimación de la dinámica τa, creada por L bajo la señal de control a, de la dinámica ideal q. Implícito en este problema de minimización es que L es manipulado a través del entorno, basado en el modelo de entorno producido por el Nivel de Diseño del Entorno. 3.2 Vista DBC del Espacio de Estados Es importante tener en cuenta la visión complementaria que DBC y POMDPs tienen sobre el espacio de estados del entorno. Los POMDP consideran el estado como una instantánea estacionaria del entorno; cualquier atributo de la secuencia de estados que se busque se alcanza a través de las propiedades del proceso de control, en este caso la acumulación de recompensas. Esto se puede ver como si la secuencia de estados y los atributos de esa secuencia solo fueran introducidos por y para el mecanismo de control, la política POMDP. DBC se enfoca en el principio subyacente de la secuenciación de estados, la dinámica del sistema. La especificación de la dinámica de los DBCs puede utilizar el espacio de estado del entorno como un medio para describir, discernir y preservar los cambios que ocurren dentro del sistema. Como resultado, DBC tiene una mayor capacidad para expresar propiedades de secuenciación de estados, las cuales están fundamentadas en el modelo del entorno y su definición del espacio de estados. Por ejemplo, considera la tarea de moverse a través de terreno accidentado hacia un objetivo y alcanzarlo lo más rápido posible. Los POMDPs codificarían el terreno como puntos en el espacio de estados, mientras que la velocidad se garantizaría mediante una recompensa negativa por cada paso dado sin alcanzar la meta. Acumular una recompensa más alta solo se puede lograr mediante un movimiento más rápido. Alternativamente, el espacio de estados podría incluir directamente la noción de velocidad. Para los POMDPs, esto significaría que el mismo concepto está codificado dos veces, en cierto sentido: directamente en el espacio de estados e indirectamente en la acumulación de recompensas. Ahora, incluso si la función de recompensa codificara más y detalles más finos de las propiedades del movimiento, la solución POMDP tendría que buscar en un espacio mucho más grande de políticas, aunque aún esté guiada por el concepto implícito del procedimiento de acumulación de recompensas. Por otro lado, la expresión del objetivo táctico de variaciones y correlaciones entre la posición y la velocidad del movimiento ahora se basa en la representación del espacio de estados. En esta situación, cualquier restricción adicional, como la suavidad del movimiento, los límites de velocidad en diferentes ubicaciones o las reducciones de velocidad durante giros bruscos, son expresadas explícita y uniformemente por el objetivo táctico, y pueden resultar en una selección de acciones más rápida y efectiva por un algoritmo DBC. 4. El Control Basado en EMT como un DBC Recientemente, se introdujo un algoritmo de control llamado Control Basado en EMT [13], que instancia el marco de trabajo DBC. Aunque proporciona una solución ávida aproximada en el sentido de DBC, los experimentos iniciales utilizando el control basado en EMT han sido alentadores [14, 15]. El control basado en EMT se basa en la definición del entorno markoviano, como en el caso de los POMDP, pero sus Niveles de Usuario y Agente son del tipo de optimalidad DBC markoviana. • El Nivel de Usuario del control basado en EMT define una dinámica del sistema objetivo en un caso limitado independiente de la acción: qEMT: S → Π(S). Luego utiliza la medida de divergencia de Kullback-Leibler para componer un estimador de dinámica de sistemas momentáneos: el algoritmo de Seguimiento Extendido de Markov (EMT). El algoritmo mantiene una estimación de la dinámica del sistema τt EMT que es capaz de explicar el cambio reciente en un estimador de estado auxiliar bayesiano de pt−1 a pt, y lo actualiza de manera conservadora utilizando la divergencia de Kullback-Leibler. Dado que τt EMT y pt−1,t son respectivamente las probabilidades condicionales y marginales sobre el espacio de estados del sistema, la explicación simplemente significa que pt(s) = s τt EMT (s | s)pt−1(s), y la actualización de la estimación de la dinámica se realiza resolviendo un 792 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Tabla 1: Estructura de POMDP vs. Control basado en Dinámicas en Entorno Markoviano Nivel Enfoque MDP Markoviano DBC Entorno < S, A, T, O, Ω >, donde S - conjunto de estados A - conjunto de acciones Diseño T : S × A → Π(S) - transición O - conjunto de observaciones Ω : S × A × S → Π(O) Usuario r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - función de recompensa q - dinámicas ideales F - remodelación de recompensa L - estimador de dinámicas θ - umbral Agente π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) problema de minimización: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • El nivel de Agente en el control basado en EMT es subóptimo con respecto a DBC (aunque permanece dentro del marco de DBC), realizando selección de acciones codiciosas basadas en la predicción de la reacción de EMT. La predicción se basa en el modelo del entorno proporcionado por el nivel de Diseño del Entorno, de modo que si denotamos por Ta la función de transición de entornos limitada a la acción a, y pt−1 es el estimador de estado auxiliar del sistema bayesiano, entonces la elección de control basada en EMT se describe por a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1). Nótese que esto sigue precisamente el marco DBC Markoviano: la optimalidad recompensante de los POMDPs se descarta, y en su lugar se manipula un estimador de dinámicas (EMT en este caso) a través de los efectos de la acción en el entorno para producir una estimación cercana a la dinámica del sistema objetivo especificado. Sin embargo, como mencionamos, el control EMT ingenuo es subóptimo en el sentido de DBC y tiene varias limitaciones adicionales que no existen en el marco general de DBC (discutido en la Sección 4.2). 4.1 EMT de múltiples objetivos. En ocasiones, puede haber varias preferencias de comportamiento. Por ejemplo, en el caso de los guardias de museo, algunos objetos de arte están más fuertemente custodiados, lo que requiere que los guardias permanezcan más a menudo en su cercanía. Por otro lado, no se debe dejar ningún rincón del museo sin revisar, lo que requiere un movimiento constante. El éxito de la seguridad en un museo requeriría que los guardias se adhieran y equilibren ambos comportamientos. Para el control basado en EMT, esto significaría enfrentar varios objetivos tácticos {qk}K k=1, y la pregunta sería cómo fusionar y equilibrarlos. Un mecanismo de equilibrio se puede aplicar para resolver este problema. Ten en cuenta que el control basado en EMT, al seleccionar una acción, crea un vector de preferencia sobre el conjunto de acciones basado en su rendimiento predicho con respecto a un objetivo dado. Si estos vectores de preferencia están normalizados, pueden combinarse en una única preferencia unificada. Esto requiere reemplazar la selección de acciones basada en EMT estándar por el algoritmo a continuación [15]: • Dado: - un conjunto de dinámicas objetivo {qk}K k=1, - vector de pesos w(k) • Seleccionar la acción de la siguiente manera - Para cada acción a ∈ A predecir la distribución de estado futuro ¯pa t+1 = Ta ∗ pt; - Para cada acción, calcular Da = H(¯pa t+1, pt, PDt) - Para cada a ∈ A y objetivo táctico qk, denotar V (a, k) = DKL (Da qk) pt. Sea Vk(a) = 1 Zk V (a, k), donde Zk = a∈A V (a, k) es un factor de normalización. - Selecciona a∗ = arg min a k k=1 w(k)Vk(a) El vector de pesos w = (w1, ..., wK ) permite ajustar la importancia entre las dinámicas objetivo sin necesidad de rediseñar los objetivos mismos. Este método de equilibrio también está integrado de forma transparente en el flujo de operación basado en EMT. Limitaciones del Control Basado en EMT El control basado en EMT es una representación subóptima (en el sentido de DBC) de la estructura DBC. Limita al Usuario al obligar a EMT a ser su algoritmo de seguimiento dinámico, y reemplaza la optimización del Agente por la selección de acciones codiciosa. Este tipo de combinación, sin embargo, es común en algoritmos en línea. Aunque es necesario un mayor desarrollo de los controladores basados en EMT, la evidencia hasta ahora sugiere que incluso la forma más simple del algoritmo posee una gran cantidad de potencia y muestra tendencias que son óptimas en el sentido de DBC. Hay dos limitaciones adicionales específicas de los técnicos en emergencias médicas (EMT, por sus siglas en inglés) al control basado en EMT que son evidentes en este momento. Ambos ya tienen soluciones parciales y son objeto de investigación continua. La primera limitación es el problema de la preferencia negativa. En el marco de POMDP, por ejemplo, esto se captura de manera sencilla, a través de The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 793 la aparición de valores con signos diferentes dentro de la estructura de recompensa. Para el control basado en EMT, sin embargo, la preferencia negativa significa que uno quisiera evitar cierta distribución sobre las secuencias de desarrollo del sistema; sin embargo, el control basado en EMT se concentra en acercarse lo más posible a una distribución. La evitación es, por lo tanto, antinatural en el control basado en EMT nativo. La segunda limitación proviene del hecho de que el modelado del entorno estándar puede crear acciones sensoriales puras, acciones que no cambian el estado del mundo y solo difieren en la forma en que se reciben las observaciones y la calidad de las observaciones recibidas. Dado que el estado del mundo no cambia, el control basado en EMT no sería capaz de diferenciar entre diferentes acciones sensoriales. Ten en cuenta que ambas limitaciones del control basado en EMT están ausentes en el marco general de DBC, ya que puede tener un algoritmo de seguimiento capaz de considerar acciones sensoriales puras y, a diferencia de la divergencia de Kullback-Leibler, una medida de desviación de distribución capaz de manejar preferencias negativas. 5. El juego de la \"carrera de relevos\" fue introducido por primera vez en [11]. Es un problema de agente único de capturar una presa, y pertenece a la clase de problemas de barrido de área. Un ejemplo de dominio se muestra en la Figura 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figura 2: Dominio de etiquetas; un agente (A) intenta buscar y capturar una presa (Q). El juego de etiquetas limita extremadamente la percepción de los agentes, de modo que el agente solo puede detectar la presa si están ubicados en la misma celda del mundo de cuadrícula. En la versión clásica del juego, la co-locación conduce a una observación especial, y la acción de Etiqueta se puede realizar. Modificamos ligeramente esta configuración: el momento en que ambos agentes ocupan la misma celda, el juego termina. Como resultado, tanto el agente como su presa tienen la misma capacidad de movimiento, lo que les permite moverse en cuatro direcciones, Norte, Sur, Este y Oeste. Estos forman un espacio formal de acciones dentro de un entorno markoviano. El espacio de estados del entorno formal markoviano está descrito por el producto cruz de las posiciones del agente y la presa. Para la Figura 2, sería S = {s0, ..., s23} × {s0, ..., s23}. Los efectos de una acción tomada por el agente son deterministas, pero el entorno en general tiene una respuesta estocástica debido al movimiento de la presa. Con probabilidad q0 1 se queda en su lugar, y con probabilidad 1 − q0 se mueve a una celda adyacente más lejos del agente 1. En nuestros experimentos, esto se tomó como q0 = 0.2. Por lo tanto, para la instancia mostrada en la Figura 2 y q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Aunque el comportamiento evasivo de la presa es conocido por el agente, la posición de la presa no lo es. La única información sensorial disponible para el agente es su propia ubicación. Utilizamos EMT y especificamos directamente la dinámica del objetivo. Para el juego de la mancha, podemos formular fácilmente tres tendencias principales: atrapar al perseguido, mantenerse en movimiento y acechar al perseguido. Esto resulta en las siguientes tres dinámicas de objetivo: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 en otro caso Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 en otro caso Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Nótese que ninguno de los objetivos anteriores es directamente alcanzable; por ejemplo, si Qt = s9 y At = s11, no hay ninguna acción que pueda llevar al agente a At+1 = s9 como lo requiere la dinámica de objetivo Tcatch. Realizamos varios experimentos para evaluar el rendimiento de EMT en el Juego de Etiquetas. Se utilizaron tres configuraciones del dominio mostrado en la Figura 3, cada una planteando un desafío diferente para el agente debido a la observabilidad parcial. En cada escenario, se realizó un conjunto de 1000 ejecuciones con un límite de tiempo de 100 pasos. En cada ejecución, la posición inicial tanto del agente como de su presa fue seleccionada al azar; esto significa que, en lo que respecta al agente, la posición inicial de la presa estaba distribuida de forma uniforme en todo el espacio de celdas del dominio. También utilizamos dos variaciones de la función de observabilidad del entorno. En la primera versión, la función de observabilidad mapeaba todas las posiciones articulares del cazador y la presa en la posición del cazador como observación. En el segundo, solo aquellas posiciones conjuntas en las que el cazador y la presa ocupaban ubicaciones diferentes fueron mapeadas en la ubicación del cazador. La segunda versión de hecho utilizó y expresó el hecho de que una vez que el cazador y la presa ocupan la misma celda, el juego termina. Los resultados de estos experimentos se muestran en la Tabla 2. Equilibrando la dinámica de captura, movimiento y seguimiento descrita en la sección anterior mediante el vector de peso [0.8, 0.1, 0.1], EMT logró un rendimiento estable en los tres dominios. Aunque las comparaciones directas son difíciles de hacer, el rendimiento de los EMT mostró una notable eficiencia en comparación con el enfoque POMDP. A pesar de una implementación simple e ineficiente del algoritmo EMT en Matlab, el tiempo de decisión para cualquier paso dado promedió significativamente por debajo de 1 segundo en todos los experimentos. Para el dominio de la arena abierta irregular, que resultó ser el más difícil, se completaron 1000 ejecuciones de experimentos limitadas a 100 pasos cada una, un total de 42411 pasos, en poco menos de 6 horas. Es decir, más de 4 × 104 pasos en línea tomaron una orden de magnitud menos tiempo que la computación sin conexión de la política POMDP en [11]. La importancia de esta diferencia se destaca aún más por el hecho de que, en caso de que cambien los parámetros del modelo del entorno, la naturaleza en línea de EMT le permitiría mantener su rendimiento, mientras que la política POMDP necesitaría ser recalculada, lo que requeriría una vez más un gran costo computacional adicional. También probamos la entropía de la frecuencia de las células de comportamiento, medidas empíricas de los datos de prueba. Como muestran la Figura 4 y la Figura 5, empir794 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 3: Se utilizaron estas configuraciones del espacio de juego de Tag: a) múltiples callejones sin salida, b) arena abierta irregular, c) pasillo circular. Tabla 2: Rendimiento de la solución basada en EMT en tres dominios de juego de Tag y dos modelos de observabilidad: I) cantera de omniposición, II) la cantera no está en la posición de los cazadores. Modelo Dominio Captura% E(Pasos) Tiempo/Paso I Callejones sin salida 100 14.8 72(mSeg) Arena 80.2 42.4 500(mSeg) Círculo 91.4 34.6 187(mSeg) II Callejones sin salida 100 13.2 91(mSeg) Arena 96.8 28.67 396(mSeg) Círculo 94.4 31.63 204(mSeg) La entropía lógica crece con la duración de la interacción. Para las carreras en las que la presa no fue capturada de inmediato, la entropía alcanza valores entre 0.85 y 0.952 para diferentes carreras y escenarios. A medida que el agente busca activamente la presa, la entropía nunca alcanza su máximo. Una característica del gráfico de entropía para el escenario de arena abierta llamó especialmente nuestra atención en el caso del modelo de observación de cantera omniposicional. Cerca del límite máximo de longitud de prueba (100 pasos), la entropía cayó repentinamente. Un análisis adicional de los datos mostró que bajo ciertas circunstancias, se produce un comportamiento fluctuante en el que el agente se enfrenta a versiones igualmente viables del comportamiento de seguir a la presa. Dado que el algoritmo EMT tiene una selección de acciones codiciosa, y el espacio de estados no codifica ninguna forma de compromiso (ni siquiera velocidad o aceleración), el agente queda atrapado dentro de una pequeña porción de celdas. Básicamente, se intenta seguir simultáneamente varios cursos de acción, todos los cuales son consistentes con la dinámica objetivo. Este comportamiento no ocurrió en nuestro segundo modelo de observación, ya que redujo significativamente el conjunto de cursos de acción elegibles, contribuyendo esencialmente a romper el empate entre ellos. 6. DISCUSIÓN El diseño de la solución EMT para el Juego de Etiquetas expone la diferencia fundamental en el enfoque de planificación y control entre EMT o DBC, por un lado, y el enfoque más familiar de POMDP, por otro lado. POMDP define una estructura de recompensa para optimizar e influencia la dinámica del sistema de forma indirecta a través de esa optimización. EMT descarta cualquier esquema de recompensa y, en cambio, mide e influye directamente en la dinámica del sistema. Se calculó la entropía 2 utilizando el logaritmo con base igual al número de ubicaciones posibles dentro del dominio; esto escala adecuadamente la expresión de entropía en el rango [0, 1] para todos los dominios. Por lo tanto, para el Juego de Etiquetas, no buscamos una función de recompensa que codificara y expresara nuestra preferencia sobre el comportamiento de los agentes, sino que establecimos directamente tres preferencias de comportamiento (heurísticas) como base para mantener la dinámica del objetivo. Los datos experimentales muestran que estos objetivos no necesitan ser directamente alcanzables a través de las acciones de los agentes. Sin embargo, aún queda por explorar la relación entre el rendimiento de la EMT y la alcanzabilidad de la dinámica objetivo. Los datos del experimento del juego de etiquetas también revelaron la diferente importancia que DBC y POMDPs otorgan a la formulación del espacio de estados del entorno. Los POMDPs dependen enteramente del mecanismo de maximización de la acumulación de recompensas, es decir, la formación del procedimiento de selección de acciones para lograr la secuenciación de estados necesaria. DBC, por otro lado, tiene dos fuentes de especificación de secuenciación: a través de las propiedades de un procedimiento de selección de acciones y a través de una especificación directa dentro de la dinámica objetivo. La importancia de la segunda fuente fue subrayada por los datos del experimento del Juego de Etiquetas, en el que el algoritmo EMT codicioso, aplicado a una especificación de espacio de estados tipo POMDP, falló, ya que la descripción del objetivo sobre dicho espacio de estados no era capaz de codificar las tendencias de comportamiento necesarias, por ejemplo, la resolución de empates y el compromiso con el movimiento dirigido. Las diferencias estructurales entre DBC (y EMT en particular) y POMDPs impiden la comparación directa de rendimiento y los sitúan en pistas complementarias, cada uno dentro de un nicho adecuado. Por ejemplo, los POMDP podrían ser vistos como una formulación mucho más natural de problemas económicos de toma de decisiones secuenciales, mientras que EMT se ajusta mejor a la demanda continua de cambios estocásticos, como ocurre en muchos problemas de robótica o de agentes incorporados. Las propiedades complementarias de POMDPs y EMT pueden ser aún más explotadas. Existe un interés reciente en utilizar POMDPs en soluciones híbridas [17], en las cuales los POMDPs pueden ser utilizados junto con otros enfoques de control para proporcionar resultados que no son fácilmente alcanzables con ninguno de los enfoques por sí solos. DBC puede ser un socio efectivo en una solución híbrida como esta. Por ejemplo, los POMDP tienen requisitos de tiempo fuera de línea prohibitivamente grandes para el cálculo de políticas, pero pueden ser fácilmente utilizados en entornos más simples para exponer tendencias de comportamiento beneficiosas; esto puede servir como una forma de dinámica objetivo que se proporciona a EMT en un dominio más grande para su operación en línea. 7. CONCLUSIONES Y TRABAJO FUTURO En este artículo, hemos presentado una nueva perspectiva sobre el proceso de planificación y control en entornos estocásticos, en forma del marco de Control Basado en Dinámicas (DBC). DBC formula la tarea de planificar como el apoyo a la dinámica de un sistema objetivo especificado, que describe las propiedades necesarias del cambio dentro del entorno. La optimalidad de los planes de acción de DBC se mide en el Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Calles sin salida 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Círculo Figura 4: Modelo de Observación I: Cantera omniposicional. Desarrollo de entropía con longitud de juego de etiqueta para los tres escenarios experimentales: a) múltiples callejones sin salida, b) arena abierta irregular, c) pasillo circular. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Callejones sin salida 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Círculo Figura 5: Modelo de observación II: cantera no observada en la posición de los cazadores. Desarrollo de la entropía con la longitud del juego de etiquetas para los tres escenarios experimentales: a) múltiples callejones sin salida, b) arena abierta irregular, c) pasillo circular, con respecto a la desviación de la dinámica del sistema real de la dinámica objetivo. Mostramos que una técnica recientemente desarrollada de Seguimiento Markov Extendido (EMT) [13] es una instancia de DBC. De hecho, EMT puede ser visto como un caso específico de la parametrización DBC, que emplea un procedimiento de selección de acciones codicioso. Dado que EMT exhibe las características clave del marco general DBC, así como una complejidad temporal polinómica, utilizamos la versión multitarget de EMT [15] para demostrar que la clase de problemas de barrido de área se presta naturalmente a descripciones basadas en dinámicas, tal como se ejemplifica en nuestros experimentos en el dominio del Juego de Etiquetas. Como se enumera en la Sección 4.2, EMT tiene varias limitaciones, como la dificultad para manejar la preferencia dinámica negativa. Esto evita la aplicación directa de la EMT a problemas como los Juegos de Rendezvous-Evasión (por ejemplo, [6]). Sin embargo, DBC en general no tiene tales limitaciones y permite fácilmente la formulación de juegos de evasión. En trabajos futuros, tenemos la intención de continuar con el desarrollo de controladores basados en dinámica para estos problemas. 8. AGRADECIMIENTO El trabajo de los dos primeros autores fue parcialmente apoyado por la subvención #898/05 de la Fundación para la Ciencia de Israel, y el tercer autor fue parcialmente apoyado por una subvención del Ministerio de Ciencia y Tecnología de Israel. REFERENCIAS [1] R. C. Arkin. Robótica basada en el comportamiento. MIT Press, 1998. [2] J. \n\nMIT Press, 1998. [2] J. A. Bilmes. Un tutorial detallado del algoritmo EM y su aplicación en la estimación de parámetros para mezclas gaussianas y modelos ocultos de Markov. Informe técnico TR-97-021, Departamento de Ingeniería Eléctrica e Informática, Universidad de California en Berkeley, 1998. [3] T. M. Cover y J. A. Thomas. Elementos de teoría de la información. Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz y M. J. Wolverton. Una encuesta de investigación en planificación distribuida y continua. Revista de Inteligencia Artificial, 4:13-22, 1999. [5] V. R. Konda y J. N. Tsitsiklis. Algoritmos Actor-Crítico. Revista SIAM de Control y Optimización, 42(4):1143-1166, 2003. [6] W. S. Lim. Un juego de encuentro-evasión en ubicaciones discretas con aleatorización conjunta. Avances en Probabilidad Aplicada, 29(4):1004-1017, diciembre de 1997. [7] M. L. Littman, T. L. Dean y L. P. Kaelbling. Sobre la complejidad de resolver problemas de decisión de Markov. En Actas de la 11ª Conferencia Anual sobre Incertidumbre en Inteligencia Artificial (UAI-95), páginas 394-402, 1995. [8] O. Madani, S. Hanks y A. Condon. Sobre la indecidibilidad de la planificación probabilística y problemas relacionados de optimización estocástica. Revista de Inteligencia Artificial, 147(1-2):5-34, julio de 2003. [9] R. M. Neal y G. E. Hinton. Una vista del algoritmo EM 796 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) que justifica enfoques incrementales, dispersos y otras variantes. En M. I. Jordan, editor, Aprendizaje en Modelos Gráficos, páginas 355-368. Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez y S. Kraus. Seguridad en sistemas multiagentes mediante la aleatorización de políticas. En Actas de AAMAS 2006, 2006. [11] J. Pineau, G. Gordon y S. Thrun. Iteración de valor basada en puntos: Un algoritmo en cualquier momento para POMDPs. En la Conferencia Conjunta Internacional de Inteligencia Artificial (IJCAI), páginas 1025-1032, agosto de 2003. [12] M. L. Puterman. Procesos de Decisión de Markov. Serie Wiley en Probabilidad y Estadística Matemática: Sección de Probabilidad y Estadística Aplicada. Publicación de Wiley-Interscience, Nueva York, 1994. [13] Z. Rabinovich y J. S. Rosenschein. Seguimiento Markov extendido con una aplicación al control. En el taller sobre seguimiento de agentes: modelando otros agentes a partir de observaciones, en la Tercera Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, páginas 95-100, Nueva York, julio de 2004. [14] Z. Rabinovich y J. S. Rosenschein. Coordinación multiagente mediante Seguimiento Markov Extendido. En la Cuarta Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, páginas 431-438, Utrecht, Países Bajos, julio de 2005. [15] Z. Rabinovich y J. S. Rosenschein. Sobre la respuesta del control basado en EMT a objetivos y modelos interactivos. En la Quinta Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, páginas 465-470, Hakodate, Japón, mayo de 2006. [16] R. F. Stengel. Control óptimo y estimación. Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, y P. Varakantham. Conflictos en el trabajo en equipo: Híbridos para el Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 797",
    "original_sentences": [
        "Dynamics Based Control with an Application to Area-Sweeping Problems Zinovi Rabinovich Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel jeff@cs.huji.ac.il Gal A. Kaminka The MAVERICK Group Department of Computer Science Bar Ilan University, Israel galk@cs.biu.ac.il ABSTRACT In this paper we introduce Dynamics Based Control (DBC), an approach to planning and control of an agent in stochastic environments.",
        "Unlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics.",
        "We show that a recently developed planning and control approach, Extended Markov Tracking (EMT) is an instantiation of DBC.",
        "EMT employs greedy action selection to provide an efficient control algorithm in Markovian environments.",
        "We exploit this efficiency in a set of experiments that applied multitarget EMT to a class of area-sweeping problems (searching for moving targets).",
        "We show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation.",
        "Categories and Subject Descriptors I.2.8 [Problem Solving, Control Methods, and Search]: Control Theory; I.2.9 [Robotics]; I.2.11 [Distributed Artificial Intelligence]: Intelligent Agents General Terms Algorithms, Theory 1.",
        "INTRODUCTION Planning and control constitutes a central research area in multiagent systems and artificial intelligence.",
        "In recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in stochastic environments.",
        "In this framework, the planning and control problem is often addressed by imposing a reward function, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility.",
        "While theoretically attractive, the complexity of optimally solving a POMDP is prohibitive [8, 7].",
        "We take an alternative view of planning in stochastic environments.",
        "We do not use a (state-based) reward function, but instead optimize over a different criterion, a transition-based specification of the desired system dynamics.",
        "The idea here is to view planexecution as a process that compels a (stochastic) system to change, and a plan as a dynamic process that shapes that change according to desired criteria.",
        "We call this general planning framework Dynamics Based Control (DBC).",
        "In DBC, the goal of a planning (or control) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) target dynamics.",
        "As actual system behavior may deviate from that which is specified by target dynamics (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16].",
        "Here, optimality is measured in terms of probability of deviation magnitudes.",
        "In this paper, we present the structure of Dynamics Based Control.",
        "We show that the recently developed Extended Markov Tracking (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC.",
        "EMT is an efficient instantiation of DBC.",
        "To evaluate DBC, we carried out a set of experiments applying multi-target EMT to the Tag Game [11]; this is a variant on the area sweeping problem, where an agent is trying to tag a moving target (quarry) whose position is not known with certainty.",
        "Experimental data demonstrates that even with a simple model of the environment and a simple design of target dynamics, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agents position).",
        "The paper is organized as follows.",
        "In Section 2 we motivate DBC using area-sweeping problems, and discuss related work.",
        "Section 3 introduces the Dynamics Based Control (DBC) structure, and its specialization to Markovian environments.",
        "This is followed by a review of the Extended Markov Tracking (EMT) approach as a DBC-structured control regimen in Section 4.",
        "That section also discusses the limitations of EMT-based control relative to the general DBC framework.",
        "Experimental settings and results are then presented in Section 5.",
        "Section 6 provides a short discussion of the overall approach, and Section 7 gives some concluding remarks and directions for future work. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
        "MOTIVATION AND RELATED WORK Many real-life scenarios naturally have a stochastic target dynamics specification, especially those domains where there exists no ultimate goal, but rather system behavior (with specific properties) that has to be continually supported.",
        "For example, security guards perform persistent sweeps of an area to detect any sign of intrusion.",
        "Cunning thieves will attempt to track these sweeps, and time their operation to key points of the guards motion.",
        "It is thus advisable to make the guards motion dynamics appear irregular and random.",
        "Recent work by Paruchuri et al. [10] has addressed such randomization in the context of single-agent and distributed POMDPs.",
        "The goal in that work was to generate policies that provide a measure of action-selection randomization, while maintaining rewards within some acceptable levels.",
        "Our focus differs from this work in that DBC does not optimize expected rewards-indeed we do not consider rewards at all-but instead maintains desired dynamics (including, but not limited to, randomization).",
        "The Game of Tag is another example of the applicability of the approach.",
        "It was introduced in the work by Pineau et al. [11].",
        "There are two agents that can move about an area, which is divided into a grid.",
        "The grid may have blocked cells (holes) into which no agent can move.",
        "One agent (the hunter) seeks to move into a cell occupied by the other (the quarry), such that they are co-located (this is a successful tag).",
        "The quarry seeks to avoid the hunter agent, and is always aware of the hunters position, but does not know how the hunter will behave, which opens up the possibility for a hunter to surprise the prey.",
        "The hunter knows the quarrys probabilistic law of motion, but does not know its current location.",
        "Tag is an instance of a family of area-sweeping (pursuit-evasion) problems.",
        "In [11], the hunter modeled the problem using a POMDP.",
        "A reward function was defined, to reflect the desire to tag the quarry, and an action policy was computed to optimize the reward collected over time.",
        "Due to the intractable complexity of determining the optimal policy, the action policy computed in that paper was essentially an approximation.",
        "In this paper, instead of formulating a reward function, we use EMT to solve the problem, by directly specifying the target dynamics.",
        "In fact, any search problem with randomized motion, the socalled class of area sweeping problems, can be described through specification of such target system dynamics.",
        "Dynamics Based Control provides a natural approach to solving these problems. 3.",
        "DYNAMICS BASED CONTROL The specification of Dynamics Based Control (DBC) can be broken into three interacting levels: Environment Design Level, User Level, and Agent Level. • Environment Design Level is concerned with the formal specification and modeling of the environment.",
        "For example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation constant. • User Level in turn relies on the environment model produced by Environment Design to specify the target system dynamics it wishes to observe.",
        "The User Level also specifies the estimation or learning procedure for system dynamics, and the measure of deviation.",
        "In the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping. • Agent Level in turn combines the environment model from the Environment Design level, the dynamics estimation procedure, the deviation measure and the target dynamics specification from User Level, to produce a sequence of actions that create system dynamics as close as possible to the targeted specification.",
        "As we are interested in the continual development of a stochastic system, such as happens in classical control theory [16] and continual planning [4], as well as in our example of museum sweeps, the question becomes how the Agent Level is to treat the deviation measurements over time.",
        "To this end, we use a probability threshold-that is, we would like the Agent Level to maximize the probability that the deviation measure will remain below a certain threshold.",
        "Specific action selection then depends on system formalization.",
        "One possibility would be to create a mixture of available system trends, much like that which happens in Behavior-Based Robotic architectures [1].",
        "The other alternative would be to rely on the estimation procedure provided by the User Level-to utilize the Environment Design Level model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a certain dynamics has been achieved.",
        "Notice that this manipulation is not direct, but via the environment.",
        "Thus, for strong enough estimator algorithms, successful manipulation would mean a successful simulation of the specified target dynamics (i.e., beyond discerning via the available sensory input).",
        "DBC levels can also have a back-flow of information (see Figure 1).",
        "For instance, the Agent Level could provide data about target dynamics feasibility, allowing the User Level to modify the requirement, perhaps focusing on attainable features of system behavior.",
        "Data would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the User Level, this can provide an important tool for the environment model calibration at the Environment Design Level.",
        "UserEnv.",
        "Design Agent Model Ideal Dynamics Estimator Estimator Dynamics Feasibility System Response Data Figure 1: Data flow of the DBC framework Extending upon the idea of Actor-Critic algorithms [5], DBC data flow can provide a good basis for the design of a learning algorithm.",
        "For example, the User Level can operate as an exploratory device for a learning algorithm, inferring an ideal dynamics target from the environment model at hand that would expose and verify most critical features of system behavior.",
        "In this case, feasibility and system response data from the Agent Level would provide key information for an environment model update.",
        "In fact, the combination of feasibility and response data can provide a basis for the application of strong learning algorithms such as EM [2, 9]. 3.1 DBC for Markovian Environments For a Partially Observable Markovian Environment, DBC can be specified in a more rigorous manner.",
        "Notice how DBC discards rewards, and replaces it by another optimality criterion (structural differences are summarized in Table 1): • Environment Design level is to specify a tuple < S, A, T, O, Ω, s0 >, where: - S is the set of all possible environment states; - s0 is the initial state of the environment (which can also be viewed as a probability distribution over S); The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 791 - A is the set of all possible actions applicable in the environment; - T is the environments probabilistic transition function: T : S ×A → Π(S).",
        "That is, T(s |a, s) is the probability that the environment will move from state s to state s under action a; - O is the set of all possible observations.",
        "This is what the sensor input would look like for an outside observer; - Ω is the observation probability function: Ω : S × A × S → Π(O).",
        "That is, Ω(o|s , a, s) is the probability that one will observe o given that the environment has moved from state s to state s under action a. • User Level, in the case of Markovian environment, operates on the set of system dynamics described by a family of conditional probabilities F = {τ : S × A → Π(S)}.",
        "Thus specification of target dynamics can be expressed by q ∈ F, and the learning or tracking algorithm can be represented as a function L : O×(A×O)∗ → F; that is, it maps sequences of observations and actions performed so far into an estimate τ ∈ F of system dynamics.",
        "There are many possible variations available at the User Level to define divergence between system dynamics; several of them are: - Trace distance or L1 distance between two distributions p and q defined by D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Fidelity measure of distance F(p(·), q(·)) = x p(x)q(x) - Kullback-Leibler divergence DKL(p(·) q(·)) = x p(x) log p(x) q(x) Notice that the latter two are not actually metrics over the space of possible distributions, but nevertheless have meaningful and important interpretations.",
        "For instance, KullbackLeibler divergence is an important tool of information theory [3] that allows one to measure the price of encoding an information source governed by q, while assuming that it is governed by p. The User Level also defines the threshold of dynamics deviation probability θ. • Agent Level is then faced with a problem of selecting a control signal function a∗ to satisfy a minimization problem as follows: a∗ = arg min a Pr(d(τa, q) > θ) where d(τa, q) is a random variable describing deviation of the dynamics estimate τa, created by L under control signal a, from the ideal dynamics q.",
        "Implicit in this minimization problem is that L is manipulated via the environment, based on the environment model produced by the Environment Design Level. 3.2 DBC View of the State Space It is important to note the complementary view that DBC and POMDPs take on the state space of the environment.",
        "POMDPs regard state as a stationary snap-shot of the environment; whatever attributes of state sequencing one seeks are reached through properties of the control process, in this case reward accumulation.",
        "This can be viewed as if the sequencing of states and the attributes of that sequencing are only introduced by and for the controlling mechanism-the POMDP policy.",
        "DBC concentrates on the underlying principle of state sequencing, the system dynamics.",
        "DBCs target dynamics specification can use the environments state space as a means to describe, discern, and preserve changes that occur within the system.",
        "As a result, DBC has a greater ability to express state sequencing properties, which are grounded in the environment model and its state space definition.",
        "For example, consider the task of moving through rough terrain towards a goal and reaching it as fast as possible.",
        "POMDPs would encode terrain as state space points, while speed would be ensured by negative reward for every step taken without reaching the goalaccumulating higher reward can be reached only by faster motion.",
        "Alternatively, the state space could directly include the notion of speed.",
        "For POMDPs, this would mean that the same concept is encoded twice, in some sense: directly in the state space, and indirectly within reward accumulation.",
        "Now, even if the reward function would encode more, and finer, details of the properties of motion, the POMDP solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure.",
        "On the other hand, the tactical target expression of variations and correlations between position and speed of motion is now grounded in the state space representation.",
        "In this situation, any further constraints, e.g., smoothness of motion, speed limits in different locations, or speed reductions during sharp turns, are explicitly and uniformly expressed by the tactical target, and can result in faster and more effective action selection by a DBC algorithm. 4.",
        "EMT-BASED CONTROL AS A DBC Recently, a control algorithm was introduced called EMT-based Control [13], which instantiates the DBC framework.",
        "Although it provides an approximate greedy solution in the DBC sense, initial experiments using EMT-based control have been encouraging [14, 15].",
        "EMT-based control is based on the Markovian environment definition, as in the case with POMDPs, but its User and Agent Levels are of the Markovian DBC type of optimality. • User Level of EMT-based control defines a limited-case target system dynamics independent of action: qEMT : S → Π(S).",
        "It then utilizes the Kullback-Leibler divergence measure to compose a momentary system dynamics estimator-the Extended Markov Tracking (EMT) algorithm.",
        "The algorithm keeps a system dynamics estimate τt EMT that is capable of explaining recent change in an auxiliary Bayesian system state estimator from pt−1 to pt, and updates it conservatively using Kullback-Leibler divergence.",
        "Since τt EMT and pt−1,t are respectively the conditional and marginal probabilities over the systems state space, explanation simply means that pt(s ) = s τt EMT (s |s)pt−1(s), and the dynamics estimate update is performed by solving a 792 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: Structure of POMDP vs. Dynamics-Based Control in Markovian Environment Level Approach MDP Markovian DBC Environment < S, A, T, O, Ω >,where S - set of states A - set of actions Design T : S × A → Π(S) - transition O - observation set Ω : S × A × S → Π(O) User r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - reward function q - ideal dynamics F - reward remodeling L - dynamics estimator θ - threshold Agent π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) minimization problem: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • Agent Level in EMT-based control is suboptimal with respect to DBC (though it remains within the DBC framework), performing greedy action selection based on prediction of EMTs reaction.",
        "The prediction is based on the environment model provided by the Environment Design level, so that if we denote by Ta the environments transition function limited to action a, and pt−1 is the auxiliary Bayesian system state estimator, then the EMT-based control choice is described by a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1) Note that this follows the Markovian DBC framework precisely: the rewarding optimality of POMDPs is discarded, and in its place a dynamics estimator (EMT in this case) is manipulated via action effects on the environment to produce an estimate close to the specified target system dynamics.",
        "Yet as we mentioned, naive EMTbased control is suboptimal in the DBC sense, and has several additional limitations that do not exist in the general DBC framework (discussed in Section 4.2). 4.1 Multi-Target EMT At times, there may exist several behavioral preferences.",
        "For example, in the case of museum guards, some art items are more heavily guarded, requiring that the guards stay more often in their close vicinity.",
        "On the other hand, no corner of the museum is to be left unchecked, which demands constant motion.",
        "Successful museum security would demand that the guards adhere to, and balance, both of these behaviors.",
        "For EMT-based control, this would mean facing several tactical targets {qk}K k=1, and the question becomes how to merge and balance them.",
        "A balancing mechanism can be applied to resolve this issue.",
        "Note that EMT-based control, while selecting an action, creates a preference vector over the set of actions based on their predicted performance with respect to a given target.",
        "If these preference vectors are normalized, they can be combined into a single unified preference.",
        "This requires replacement of standard EMT-based action selection by the algorithm below [15]: • Given: - a set of target dynamics {qk}K k=1, - vector of weights w(k) • Select action as follows - For each action a ∈ A predict the future state distribution ¯pa t+1 = Ta ∗ pt; - For each action, compute Da = H(¯pa t+1, pt, PDt) - For each a ∈ A and qk tactical target, denote V (a, k) = DKL (Da qk) pt .",
        "Let Vk(a) = 1 Zk V (a, k), where Zk = a∈A V (a, k) is a normalization factor. - Select a∗ = arg min a k k=1 w(k)Vk(a) The weights vector w = (w1, ..., wK ) allows the additional tuning of importance among target dynamics without the need to redesign the targets themselves.",
        "This balancing method is also seamlessly integrated into the EMT-based control flow of operation. 4.2 EMT-based Control Limitations EMT-based control is a sub-optimal (in the DBC sense) representative of the DBC structure.",
        "It limits the User by forcing EMT to be its dynamic tracking algorithm, and replaces Agent optimization by greedy action selection.",
        "This kind of combination, however, is common for on-line algorithms.",
        "Although further development of EMT-based controllers is necessary, evidence so far suggests that even the simplest form of the algorithm possesses a great deal of power, and displays trends that are optimal in the DBC sense of the word.",
        "There are two further, EMT-specific, limitations to EMT-based control that are evident at this point.",
        "Both already have partial solutions and are subjects of ongoing research.",
        "The first limitation is the problem of negative preference.",
        "In the POMDP framework for example, this is captured simply, through The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 793 the appearance of values with different signs within the reward structure.",
        "For EMT-based control, however, negative preference means that one would like to avoid a certain distribution over system development sequences; EMT-based control, however, concentrates on getting as close as possible to a distribution.",
        "Avoidance is thus unnatural in native EMT-based control.",
        "The second limitation comes from the fact that standard environment modeling can create pure sensory actions-actions that do not change the state of the world, and differ only in the way observations are received and the quality of observations received.",
        "Since the world state does not change, EMT-based control would not be able to differentiate between different sensory actions.",
        "Notice that both of these limitations of EMT-based control are absent from the general DBC framework, since it may have a tracking algorithm capable of considering pure sensory actions and, unlike Kullback-Leibler divergence, a distribution deviation measure that is capable of dealing with negative preference. 5.",
        "EMT PLAYING TAG The Game of Tag was first introduced in [11].",
        "It is a single agent problem of capturing a quarry, and belongs to the class of area sweeping problems.",
        "An example domain is shown in Figure 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figure 2: Tag domain; an agent (A) attempts to seek and capture a quarry (Q) The Game of Tag extremely limits the agents perception, so that the agent is able to detect the quarry only if they are co-located in the same cell of the grid world.",
        "In the classical version of the game, co-location leads to a special observation, and the Tag action can be performed.",
        "We slightly modify this setting: the moment that both agents occupy the same cell, the game ends.",
        "As a result, both the agent and its quarry have the same motion capability, which allows them to move in four directions, North, South, East, and West.",
        "These form a formal space of actions within a Markovian environment.",
        "The state space of the formal Markovian environment is described by the cross-product of the agent and quarrys positions.",
        "For Figure 2, it would be S = {s0, ..., s23} × {s0, ..., s23}.",
        "The effects of an action taken by the agent are deterministic, but the environment in general has a stochastic response due to the motion of the quarry.",
        "With probability q0 1 it stays put, and with probability 1 − q0 it moves to an adjacent cell further away from the 1 In our experiments this was taken to be q0 = 0.2. agent.",
        "So for the instance shown in Figure 2 and q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Although the evasive behavior of the quarry is known to the agent, the quarrys position is not.",
        "The only sensory information available to the agent is its own location.",
        "We use EMT and directly specify the target dynamics.",
        "For the Game of Tag, we can easily formulate three major trends: catching the quarry, staying mobile, and stalking the quarry.",
        "This results in the following three target dynamics: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 otherwise Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 otherwise Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Note that none of the above targets are directly achievable; for instance, if Qt = s9 and At = s11, there is no action that can move the agent to At+1 = s9 as required by the Tcatch target dynamics.",
        "We ran several experiments to evaluate EMT performance in the Tag Game.",
        "Three configurations of the domain shown in Figure 3 were used, each posing a different challenge to the agent due to partial observability.",
        "In each setting, a set of 1000 runs was performed with a time limit of 100 steps.",
        "In every run, the initial position of both the agent and its quarry was selected at random; this means that as far as the agent was concerned, the quarrys initial position was uniformly distributed over the entire domain cell space.",
        "We also used two variations of the environment observability function.",
        "In the first version, observability function mapped all joint positions of hunter and quarry into the position of the hunter as an observation.",
        "In the second, only those joint positions in which hunter and quarry occupied different locations were mapped into the hunters location.",
        "The second version in fact utilized and expressed the fact that once hunter and quarry occupy the same cell the game ends.",
        "The results of these experiments are shown in Table 2.",
        "Balancing [15] the catch, move, and stalk target dynamics described in the previous section by the weight vector [0.8, 0.1, 0.1], EMT produced stable performance in all three domains.",
        "Although direct comparisons are difficult to make, the EMT performance displayed notable efficiency vis-`a-vis the POMDP approach.",
        "In spite of a simple and inefficient Matlab implementation of the EMT algorithm, the decision time for any given step averaged significantly below 1 second in all experiments.",
        "For the irregular open arena domain, which proved to be the most difficult, 1000 experiment runs bounded by 100 steps each, a total of 42411 steps, were completed in slightly under 6 hours.",
        "That is, over 4 × 104 online steps took an order of magnitude less time than the offline computation of POMDP policy in [11].",
        "The significance of this differential is made even more prominent by the fact that, should the environment model parameters change, the online nature of EMT would allow it to maintain its performance time, while the POMDP policy would need to be recomputed, requiring yet again a large overhead of computation.",
        "We also tested the behavior cell frequency entropy, empirical measures from trial data.",
        "As Figure 4 and Figure 5 show, empir794 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) A Q Q A 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 A Q Figure 3: These configurations of the Tag Game space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor Table 2: Performance of the EMT-based solution in three Tag Game domains and two observability models: I) omniposition quarry, II) quarry is not at hunters position Model Domain Capture% E(Steps) Time/Step I Dead-ends 100 14.8 72(mSec) Arena 80.2 42.4 500(mSec) Circle 91.4 34.6 187(mSec) II Dead-ends 100 13.2 91(mSec) Arena 96.8 28.67 396(mSec) Circle 94.4 31.63 204(mSec) ical entropy grows with the length of interaction.",
        "For runs where the quarry was not captured immediately, the entropy reaches between 0.85 and 0.952 for different runs and scenarios.",
        "As the agent actively seeks the quarry, the entropy never reaches its maximum.",
        "One characteristic of the entropy graph for the open arena scenario particularly caught our attention in the case of the omniposition quarry observation model.",
        "Near the maximum limit of trial length (100 steps), entropy suddenly dropped.",
        "Further analysis of the data showed that under certain circumstances, a fluctuating behavior occurs in which the agent faces equally viable versions of quarry-following behavior.",
        "Since the EMT algorithm has greedy action selection, and the state space does not encode any form of commitment (not even speed or acceleration), the agent is locked within a small portion of cells.",
        "It is essentially attempting to simultaneously follow several courses of action, all of which are consistent with the target dynamics.",
        "This behavior did not occur in our second observation model, since it significantly reduced the set of eligible courses of action-essentially contributing to tie-breaking among them. 6.",
        "DISCUSSION The design of the EMT solution for the Tag Game exposes the core difference in approach to planning and control between EMT or DBC, on the one hand, and the more familiar POMDP approach, on the other.",
        "POMDP defines a reward structure to optimize, and influences system dynamics indirectly through that optimization.",
        "EMT discards any reward scheme, and instead measures and influences system dynamics directly. 2 Entropy was calculated using log base equal to the number of possible locations within the domain; this properly scales entropy expression into the range [0, 1] for all domains.",
        "Thus for the Tag Game, we did not search for a reward function that would encode and express our preference over the agents behavior, but rather directly set three (heuristic) behavior preferences as the basis for target dynamics to be maintained.",
        "Experimental data shows that these targets need not be directly achievable via the agents actions.",
        "However, the ratio between EMT performance and achievability of target dynamics remains to be explored.",
        "The tag game experiment data also revealed the different emphasis DBC and POMDPs place on the formulation of an environment state space.",
        "POMDPs rely entirely on the mechanism of reward accumulation maximization, i.e., formation of the action selection procedure to achieve necessary state sequencing.",
        "DBC, on the other hand, has two sources of sequencing specification: through the properties of an action selection procedure, and through direct specification within the target dynamics.",
        "The importance of the second source was underlined by the Tag Game experiment data, in which the greedy EMT algorithm, applied to a POMDP-type state space specification, failed, since target description over such a state space was incapable of encoding the necessary behavior tendencies, e.g., tie-breaking and commitment to directed motion.",
        "The structural differences between DBC (and EMT in particular), and POMDPs, prohibits direct performance comparison, and places them on complementary tracks, each within a suitable niche.",
        "For instance, POMDPs could be seen as a much more natural formulation of economic sequential decision-making problems, while EMT is a better fit for continual demand for stochastic change, as happens in many robotic or embodied-agent problems.",
        "The complementary properties of POMDPs and EMT can be further exploited.",
        "There is recent interest in using POMDPs in hybrid solutions [17], in which the POMDPs can be used together with other control approaches to provide results not easily achievable with either approach by itself.",
        "DBC can be an effective partner in such a hybrid solution.",
        "For instance, POMDPs have prohibitively large off-line time requirements for policy computation, but can be readily used in simpler settings to expose beneficial behavioral trends; this can serve as a form of target dynamics that are provided to EMT in a larger domain for on-line operation. 7.",
        "CONCLUSIONS AND FUTURE WORK In this paper, we have presented a novel perspective on the process of planning and control in stochastic environments, in the form of the Dynamics Based Control (DBC) framework.",
        "DBC formulates the task of planning as support of a specified target system dynamics, which describes the necessary properties of change within the environment.",
        "Optimality of DBC plans of action are measured The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 4: Observation Model I: Omniposition quarry.",
        "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 5: Observation Model II: quarry not observed at hunters position.",
        "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. with respect to the deviation of actual system dynamics from the target dynamics.",
        "We show that a recently developed technique of Extended Markov Tracking (EMT) [13] is an instantiation of DBC.",
        "In fact, EMT can be seen as a specific case of DBC parameterization, which employs a greedy action selection procedure.",
        "Since EMT exhibits the key features of the general DBC framework, as well as polynomial time complexity, we used the multitarget version of EMT [15] to demonstrate that the class of area sweeping problems naturally lends itself to dynamics-based descriptions, as instantiated by our experiments in the Tag Game domain.",
        "As enumerated in Section 4.2, EMT has a number of limitations, such as difficulty in dealing with negative dynamic preference.",
        "This prevents direct application of EMT to such problems as Rendezvous-Evasion Games (e.g., [6]).",
        "However, DBC in general has no such limitations, and readily enables the formulation of evasion games.",
        "In future work, we intend to proceed with the development of dynamics-based controllers for these problems. 8.",
        "ACKNOWLEDGMENT The work of the first two authors was partially supported by Israel Science Foundation grant #898/05, and the third author was partially supported by a grant from Israels Ministry of Science and Technology. 9.",
        "REFERENCES [1] R. C. Arkin.",
        "Behavior-Based Robotics.",
        "MIT Press, 1998. [2] J.",
        "A. Bilmes.",
        "A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and Hidden Markov Models.",
        "Technical Report TR-97-021, Department of Electrical Engeineering and Computer Science, University of California at Berkeley, 1998. [3] T. M. Cover and J.",
        "A. Thomas.",
        "Elements of information theory.",
        "Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz, and M. J. Wolverton.",
        "A survey of research in distributed, continual planning.",
        "AI Magazine, 4:13-22, 1999. [5] V. R. Konda and J. N. Tsitsiklis.",
        "Actor-Critic algorithms.",
        "SIAM Journal on Control and Optimization, 42(4):1143-1166, 2003. [6] W. S. Lim.",
        "A rendezvous-evasion game on discrete locations with joint randomization.",
        "Advances in Applied Probability, 29(4):1004-1017, December 1997. [7] M. L. Littman, T. L. Dean, and L. P. Kaelbling.",
        "On the complexity of solving Markov decision problems.",
        "In Proceedings of the 11th Annual Conference on Uncertainty in Artificial Intelligence (UAI-95), pages 394-402, 1995. [8] O. Madani, S. Hanks, and A. Condon.",
        "On the undecidability of probabilistic planning and related stochastic optimization problems.",
        "Artificial Intelligence Journal, 147(1-2):5-34, July 2003. [9] R. M. Neal and G. E. Hinton.",
        "A view of the EM algorithm 796 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) that justifies incremental, sparse, and other variants.",
        "In M. I. Jordan, editor, Learning in Graphical Models, pages 355-368.",
        "Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
        "Security in multiagent systems by policy randomization.",
        "In Proceeding of AAMAS 2006, 2006. [11] J. Pineau, G. Gordon, and S. Thrun.",
        "Point-based value iteration: An anytime algorithm for pomdps.",
        "In International Joint Conference on Artificial Intelligence (IJCAI), pages 1025-1032, August 2003. [12] M. L. Puterman.",
        "Markov Decision Processes.",
        "Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics Section.",
        "Wiley-Interscience Publication, New York, 1994. [13] Z. Rabinovich and J. S. Rosenschein.",
        "Extended Markov Tracking with an application to control.",
        "In The Workshop on Agent Tracking: Modeling Other Agents from Observations, at the Third International Joint Conference on Autonomous Agents and Multiagent Systems, pages 95-100, New-York, July 2004. [14] Z. Rabinovich and J. S. Rosenschein.",
        "Multiagent coordination by Extended Markov Tracking.",
        "In The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 431-438, Utrecht, The Netherlands, July 2005. [15] Z. Rabinovich and J. S. Rosenschein.",
        "On the response of EMT-based control to interacting targets and models.",
        "In The Fifth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 465-470, Hakodate, Japan, May 2006. [16] R. F. Stengel.",
        "Optimal Control and Estimation.",
        "Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, and P. Varakantham.",
        "Conflicts in teamwork: Hybrids to the The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 797"
    ],
    "translated_text_sentences": [
        "En este artículo presentamos el Control Basado en Dinámicas (DBC), un enfoque para la planificación y control de un agente en entornos estocásticos.",
        "A diferencia de los enfoques existentes, que buscan optimizar las recompensas esperadas (por ejemplo, en Problemas de Decisión de Markov Parcialmente Observables (POMDP)), DBC optimiza el comportamiento del sistema hacia las dinámicas del sistema especificadas.",
        "Mostramos que un enfoque de planificación y control recientemente desarrollado, Seguimiento Extendido de Markov (EMT), es una instancia de DBC.",
        "EMT emplea la selección de acciones codiciosa para proporcionar un algoritmo de control eficiente en entornos markovianos.",
        "Explotamos esta eficiencia en un conjunto de experimentos que aplicaron EMT multitarget a una clase de problemas de barrido de área (búsqueda de objetivos en movimiento).",
        "Mostramos que tales problemas pueden ser definidos de forma natural y resueltos eficientemente utilizando el marco de DBC y su instanciación de EMT.",
        "Categorías y Descriptores de Asignaturas I.2.8 [Resolución de Problemas, Métodos de Control y Búsqueda]: Teoría de Control; I.2.9 [Robótica]; I.2.11 [Inteligencia Artificial Distribuida]: Agentes Inteligentes Términos Generales Algoritmos, Teoría 1.",
        "INTRODUCCIÓN La planificación y el control constituyen un área de investigación central en sistemas multiagentes e inteligencia artificial.",
        "En los últimos años, los Procesos de Decisión de Markov Parcialmente Observables (POMDPs) [12] se han convertido en una base formal popular para la planificación en entornos estocásticos.",
        "En este marco, el problema de planificación y control suele abordarse imponiendo una función de recompensa y calculando una política (de elección de acciones) que sea óptima, en el sentido de que resultará en la mayor utilidad esperada.",
        "Si bien teóricamente atractivo, la complejidad de resolver óptimamente un POMDP es prohibitiva [8, 7].",
        "Tomamos una visión alternativa de la planificación en entornos estocásticos.",
        "No utilizamos una función de recompensa basada en el estado, sino que optimizamos según un criterio diferente, una especificación basada en transiciones de la dinámica del sistema deseado.",
        "La idea aquí es ver la ejecución del plan como un proceso que obliga a un sistema (estocástico) a cambiar, y un plan como un proceso dinámico que moldea ese cambio de acuerdo con criterios deseados.",
        "Llamamos a este marco de planificación general Control Basado en Dinámicas (DBC).",
        "En DBC, el objetivo de un proceso de planificación (o control) es garantizar que el sistema cambiará de acuerdo con dinámicas objetivo específicas (potencialmente estocásticas).",
        "Dado que el comportamiento real del sistema puede desviarse de lo especificado por la dinámica objetivo (debido a la naturaleza estocástica del sistema), la planificación en tales entornos debe ser continua, de manera similar a los controladores en lazo cerrado clásicos.",
        "Aquí, la optimalidad se mide en términos de la probabilidad de magnitudes de desviación.",
        "En este documento, presentamos la estructura del Control Basado en Dinámicas.",
        "Mostramos que el enfoque de Seguimiento Extendido de Markov (EMT) recientemente desarrollado [13, 14, 15] está subsumido por DBC, con EMT empleando selección de acciones codiciosa, que es una parametrización específica entre las opciones posibles dentro de DBC.",
        "EMT es una implementación eficiente de DBC.",
        "Para evaluar DBC, llevamos a cabo un conjunto de experimentos aplicando EMT de múltiples objetivos al Juego de Etiquetas [11]; esta es una variante del problema de barrido de área, donde un agente intenta etiquetar a un objetivo móvil (presa) cuya posición no se conoce con certeza.",
        "Los datos experimentales demuestran que incluso con un modelo simple del entorno y un diseño simple de la dinámica del objetivo, se pueden lograr altas tasas de éxito tanto en la captura de la presa como en sorprender a la presa (como se expresa por la entropía observada de la posición de los agentes controlados).",
        "El documento está organizado de la siguiente manera.",
        "En la Sección 2 motivamos DBC utilizando problemas de barrido de área, y discutimos trabajos relacionados.",
        "La Sección 3 introduce la estructura de Control Basado en Dinámicas (DBC) y su especialización en entornos Markovianos.",
        "Esto es seguido por una revisión del enfoque de Seguimiento Markov Extendido (EMT) como un régimen de control estructurado por DBC en la Sección 4.",
        "Esa sección también discute las limitaciones del control basado en EMT en relación con el marco general de DBC.",
        "Los ajustes experimentales y los resultados se presentan a continuación en la Sección 5.",
        "La Sección 6 proporciona una breve discusión del enfoque general, y la Sección 7 ofrece algunas observaciones finales y direcciones para trabajos futuros. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
        "MOTIVACIÓN Y TRABAJO RELACIONADO Muchos escenarios de la vida real tienen naturalmente una especificación de dinámica de destino estocástica, especialmente en aquellos dominios donde no existe un objetivo final, sino más bien un comportamiento del sistema (con propiedades específicas) que debe ser continuamente respaldado.",
        "Por ejemplo, los guardias de seguridad realizan barridos persistentes de un área para detectar cualquier signo de intrusión.",
        "Los ladrones astutos intentarán rastrear estos barridos y sincronizar su operación con puntos clave del movimiento de los guardias.",
        "Por lo tanto, es recomendable que la dinámica de movimiento de los guardias parezca irregular y aleatoria.",
        "El trabajo reciente de Paruchuri et al. [10] ha abordado dicha aleatorización en el contexto de POMDPs de agente único y distribuidos.",
        "El objetivo de ese trabajo era generar políticas que proporcionen una medida de aleatorización en la selección de acciones, manteniendo al mismo tiempo las recompensas dentro de niveles aceptables.",
        "Nuestro enfoque difiere de este trabajo en que DBC no optimiza recompensas esperadas, de hecho, no consideramos recompensas en absoluto, sino que en su lugar mantenemos dinámicas deseadas (incluyendo, pero no limitado a, la aleatorización).",
        "El juego de la mancha es otro ejemplo de la aplicabilidad del enfoque.",
        "Fue introducido en el trabajo de Pineau et al. [11].",
        "Hay dos agentes que pueden moverse por un área, la cual está dividida en una cuadrícula.",
        "La cuadrícula puede tener celdas bloqueadas (agujeros) en las cuales ningún agente puede moverse.",
        "Un agente (el cazador) intenta moverse a una celda ocupada por el otro (la presa), de modo que estén ubicados en el mismo lugar (esto es una etiqueta exitosa).",
        "La presa busca evitar al cazador, siempre está al tanto de la posición del cazador, pero no sabe cómo se comportará el cazador, lo que abre la posibilidad de que un cazador sorprenda a la presa.",
        "El cazador conoce la ley de movimiento probabilístico de la presa, pero no conoce su ubicación actual.",
        "Tag es una instancia de una familia de problemas de barrido de área (persecución-evasión).",
        "En [11], el cazador modeló el problema utilizando un POMDP.",
        "Se definió una función de recompensa para reflejar el deseo de marcar la presa, y se calculó una política de acción para optimizar la recompensa recolectada con el tiempo.",
        "Debido a la complejidad intratable de determinar la política óptima, la política de acción calculada en ese documento fue esencialmente una aproximación.",
        "En este artículo, en lugar de formular una función de recompensa, utilizamos EMT para resolver el problema, especificando directamente la dinámica objetivo.",
        "De hecho, cualquier problema de búsqueda con movimiento aleatorio, la llamada clase de problemas de barrido de área, puede ser descrito a través de la especificación de la dinámica de dicho sistema objetivo.",
        "El Control Basado en Dinámicas proporciona un enfoque natural para resolver estos problemas.",
        "CONTROL BASADO EN DINÁMICA La especificación del Control Basado en Dinámica (DBC) se puede dividir en tres niveles interactivos: Nivel de Diseño del Entorno, Nivel de Usuario y Nivel de Agente. • El Nivel de Diseño del Entorno se ocupa de la especificación formal y modelado del entorno.",
        "Por ejemplo, este nivel especificaría las leyes de la física dentro del sistema y establecería sus parámetros, como la constante de gravitación. El Nivel de Usuario, a su vez, se basa en el modelo de entorno producido por el Diseño de Entorno para especificar la dinámica del sistema objetivo que desea observar.",
        "El Nivel de Usuario también especifica el procedimiento de estimación o aprendizaje para la dinámica del sistema, y la medida de desviación.",
        "En el escenario del guardia de museo mencionado anteriormente, estos corresponderían a un horario de barrido estocástico y una medida de sorpresa relativa entre el barrido especificado y el real. • El Nivel de Agente, a su vez, combina el modelo del entorno del Nivel de Diseño de Entorno, el procedimiento de estimación de la dinámica, la medida de desviación y la especificación de la dinámica objetivo del Nivel de Usuario, para producir una secuencia de acciones que creen dinámicas del sistema lo más cercanas posible a la especificación deseada.",
        "Dado nuestro interés en el desarrollo continuo de un sistema estocástico, como ocurre en la teoría de control clásica [16] y la planificación continua [4], así como en nuestro ejemplo de recorridos por museos, la pregunta es cómo el Nivel de Agente debe tratar las mediciones de desviación a lo largo del tiempo.",
        "Con este fin, utilizamos un umbral de probabilidad, es decir, nos gustaría que el Nivel del Agente maximice la probabilidad de que la medida de desviación permanezca por debajo de cierto umbral.",
        "La selección de acciones específicas depende entonces de la formalización del sistema.",
        "Una posibilidad sería crear una mezcla de tendencias de sistemas disponibles, similar a lo que sucede en las arquitecturas robóticas basadas en el comportamiento [1].",
        "La otra alternativa sería depender del procedimiento de estimación proporcionado por el Nivel de Usuario para utilizar el modelo del Nivel de Diseño del Entorno del entorno para elegir acciones, de manera que se manipule al estimador de dinámicas para que crea que se ha logrado cierta dinámica.",
        "Ten en cuenta que esta manipulación no es directa, sino a través del entorno.",
        "Por lo tanto, para algoritmos de estimación lo suficientemente fuertes, la manipulación exitosa significaría una simulación exitosa de la dinámica objetivo especificada (es decir, más allá de discernir a través de la entrada sensorial disponible).",
        "Los niveles de DBC también pueden tener un retroceso de información (ver Figura 1).",
        "Por ejemplo, el Nivel de Agente podría proporcionar datos sobre la viabilidad de la dinámica del objetivo, permitiendo al Nivel de Usuario modificar el requisito, quizás centrándose en las características alcanzables del comportamiento del sistema.",
        "Los datos también estarían disponibles sobre la respuesta del sistema a diferentes acciones realizadas; combinados con un estimador de dinámica definido por el Nivel de Usuario, esto puede proporcionar una herramienta importante para la calibración del modelo del entorno en el Nivel de Diseño del Entorno.",
        "UserEnv.",
        "El modelo de agente de diseño Estimador de Dinámicas Ideales Estimador de Dinámicas Factibilidad de Respuesta del Sistema de Datos Figura 1: Flujo de datos del marco de trabajo DBC Ampliando la idea de los algoritmos Actor-Crítico [5], el flujo de datos de DBC puede proporcionar una buena base para el diseño de un algoritmo de aprendizaje.",
        "Por ejemplo, el Nivel de Usuario puede funcionar como un dispositivo exploratorio para un algoritmo de aprendizaje, inferir un objetivo dinámico ideal a partir del modelo del entorno en cuestión que expondría y verificaría las características más críticas del comportamiento del sistema.",
        "En este caso, los datos de viabilidad y respuesta del sistema a nivel de agente proporcionarían información clave para una actualización del modelo del entorno.",
        "De hecho, la combinación de datos de viabilidad y respuesta puede proporcionar una base para la aplicación de algoritmos de aprendizaje sólidos como EM [2, 9]. 3.1 DBC para entornos markovianos Para un entorno markoviano parcialmente observable, DBC puede especificarse de una manera más rigurosa.",
        "Observa cómo DBC descarta las recompensas y las reemplaza por otro criterio de optimalidad (las diferencias estructurales se resumen en la Tabla 1): • El nivel de Diseño del Entorno es especificar una tupla < S, A, T, O, Ω, s0 >, donde: - S es el conjunto de todos los posibles estados del entorno; - s0 es el estado inicial del entorno (que también puede ser visto como una distribución de probabilidad sobre S); La Sexta Conferencia Internacional.",
        "La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 791 - A es el conjunto de todas las acciones posibles aplicables en el entorno; - T es la función de transición probabilística del entorno: T: S × A → Π(S).",
        "Es decir, T(s | a, s) es la probabilidad de que el entorno se mueva del estado s al estado s bajo la acción a; - O es el conjunto de todas las observaciones posibles.",
        "Así es como se vería la entrada del sensor para un observador externo; - Ω es la función de probabilidad de observación: Ω: S × A × S → Π(O).",
        "Es decir, Ω(o|s, a, s) es la probabilidad de observar o dado que el entorno ha pasado del estado s al estado s bajo la acción a. • Nivel de Usuario, en el caso de un entorno markoviano, opera en el conjunto de dinámicas del sistema descritas por una familia de probabilidades condicionales F = {τ: S × A → Π(S)}.",
        "Por lo tanto, la especificación de la dinámica del objetivo puede expresarse como q ∈ F, y el algoritmo de aprendizaje o seguimiento puede representarse como una función L : O×(A×O)∗ → F; es decir, mapea secuencias de observaciones y acciones realizadas hasta el momento en una estimación τ ∈ F de la dinámica del sistema.",
        "Hay muchas variaciones posibles disponibles a nivel de usuario para definir la divergencia entre la dinámica del sistema; varias de ellas son: - Distancia de traza o distancia L1 entre dos distribuciones p y q definida por D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Medida de fidelidad de distancia F(p(·), q(·)) = x p(x)q(x) - Divergencia de Kullback-Leibler DKL(p(·) q(·)) = x p(x) log p(x) q(x) Nótese que las dos últimas no son en realidad métricas sobre el espacio de distribuciones posibles, pero tienen interpretaciones significativas e importantes.",
        "Por ejemplo, la divergencia de Kullback-Leibler es una herramienta importante de la teoría de la información [3] que permite medir el costo de codificar una fuente de información gobernada por q, asumiendo que está gobernada por p. El Nivel de Usuario también define el umbral de probabilidad de desviación de la dinámica θ. • El Nivel de Agente se enfrenta entonces a un problema de seleccionar una función de señal de control a∗ para satisfacer un problema de minimización de la siguiente manera: a∗ = arg min a Pr(d(τa, q) > θ) donde d(τa, q) es una variable aleatoria que describe la desviación de la estimación de la dinámica τa, creada por L bajo la señal de control a, de la dinámica ideal q.",
        "Implícito en este problema de minimización es que L es manipulado a través del entorno, basado en el modelo de entorno producido por el Nivel de Diseño del Entorno. 3.2 Vista DBC del Espacio de Estados Es importante tener en cuenta la visión complementaria que DBC y POMDPs tienen sobre el espacio de estados del entorno.",
        "Los POMDP consideran el estado como una instantánea estacionaria del entorno; cualquier atributo de la secuencia de estados que se busque se alcanza a través de las propiedades del proceso de control, en este caso la acumulación de recompensas.",
        "Esto se puede ver como si la secuencia de estados y los atributos de esa secuencia solo fueran introducidos por y para el mecanismo de control, la política POMDP.",
        "DBC se enfoca en el principio subyacente de la secuenciación de estados, la dinámica del sistema.",
        "La especificación de la dinámica de los DBCs puede utilizar el espacio de estado del entorno como un medio para describir, discernir y preservar los cambios que ocurren dentro del sistema.",
        "Como resultado, DBC tiene una mayor capacidad para expresar propiedades de secuenciación de estados, las cuales están fundamentadas en el modelo del entorno y su definición del espacio de estados.",
        "Por ejemplo, considera la tarea de moverse a través de terreno accidentado hacia un objetivo y alcanzarlo lo más rápido posible.",
        "Los POMDPs codificarían el terreno como puntos en el espacio de estados, mientras que la velocidad se garantizaría mediante una recompensa negativa por cada paso dado sin alcanzar la meta. Acumular una recompensa más alta solo se puede lograr mediante un movimiento más rápido.",
        "Alternativamente, el espacio de estados podría incluir directamente la noción de velocidad.",
        "Para los POMDPs, esto significaría que el mismo concepto está codificado dos veces, en cierto sentido: directamente en el espacio de estados e indirectamente en la acumulación de recompensas.",
        "Ahora, incluso si la función de recompensa codificara más y detalles más finos de las propiedades del movimiento, la solución POMDP tendría que buscar en un espacio mucho más grande de políticas, aunque aún esté guiada por el concepto implícito del procedimiento de acumulación de recompensas.",
        "Por otro lado, la expresión del objetivo táctico de variaciones y correlaciones entre la posición y la velocidad del movimiento ahora se basa en la representación del espacio de estados.",
        "En esta situación, cualquier restricción adicional, como la suavidad del movimiento, los límites de velocidad en diferentes ubicaciones o las reducciones de velocidad durante giros bruscos, son expresadas explícita y uniformemente por el objetivo táctico, y pueden resultar en una selección de acciones más rápida y efectiva por un algoritmo DBC. 4.",
        "El Control Basado en EMT como un DBC Recientemente, se introdujo un algoritmo de control llamado Control Basado en EMT [13], que instancia el marco de trabajo DBC.",
        "Aunque proporciona una solución ávida aproximada en el sentido de DBC, los experimentos iniciales utilizando el control basado en EMT han sido alentadores [14, 15].",
        "El control basado en EMT se basa en la definición del entorno markoviano, como en el caso de los POMDP, pero sus Niveles de Usuario y Agente son del tipo de optimalidad DBC markoviana. • El Nivel de Usuario del control basado en EMT define una dinámica del sistema objetivo en un caso limitado independiente de la acción: qEMT: S → Π(S).",
        "Luego utiliza la medida de divergencia de Kullback-Leibler para componer un estimador de dinámica de sistemas momentáneos: el algoritmo de Seguimiento Extendido de Markov (EMT).",
        "El algoritmo mantiene una estimación de la dinámica del sistema τt EMT que es capaz de explicar el cambio reciente en un estimador de estado auxiliar bayesiano de pt−1 a pt, y lo actualiza de manera conservadora utilizando la divergencia de Kullback-Leibler.",
        "Dado que τt EMT y pt−1,t son respectivamente las probabilidades condicionales y marginales sobre el espacio de estados del sistema, la explicación simplemente significa que pt(s) = s τt EMT (s | s)pt−1(s), y la actualización de la estimación de la dinámica se realiza resolviendo un 792 The Sixth Intl.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Tabla 1: Estructura de POMDP vs. Control basado en Dinámicas en Entorno Markoviano Nivel Enfoque MDP Markoviano DBC Entorno < S, A, T, O, Ω >, donde S - conjunto de estados A - conjunto de acciones Diseño T : S × A → Π(S) - transición O - conjunto de observaciones Ω : S × A × S → Π(O) Usuario r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - función de recompensa q - dinámicas ideales F - remodelación de recompensa L - estimador de dinámicas θ - umbral Agente π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) problema de minimización: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • El nivel de Agente en el control basado en EMT es subóptimo con respecto a DBC (aunque permanece dentro del marco de DBC), realizando selección de acciones codiciosas basadas en la predicción de la reacción de EMT.",
        "La predicción se basa en el modelo del entorno proporcionado por el nivel de Diseño del Entorno, de modo que si denotamos por Ta la función de transición de entornos limitada a la acción a, y pt−1 es el estimador de estado auxiliar del sistema bayesiano, entonces la elección de control basada en EMT se describe por a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1). Nótese que esto sigue precisamente el marco DBC Markoviano: la optimalidad recompensante de los POMDPs se descarta, y en su lugar se manipula un estimador de dinámicas (EMT en este caso) a través de los efectos de la acción en el entorno para producir una estimación cercana a la dinámica del sistema objetivo especificado.",
        "Sin embargo, como mencionamos, el control EMT ingenuo es subóptimo en el sentido de DBC y tiene varias limitaciones adicionales que no existen en el marco general de DBC (discutido en la Sección 4.2). 4.1 EMT de múltiples objetivos. En ocasiones, puede haber varias preferencias de comportamiento.",
        "Por ejemplo, en el caso de los guardias de museo, algunos objetos de arte están más fuertemente custodiados, lo que requiere que los guardias permanezcan más a menudo en su cercanía.",
        "Por otro lado, no se debe dejar ningún rincón del museo sin revisar, lo que requiere un movimiento constante.",
        "El éxito de la seguridad en un museo requeriría que los guardias se adhieran y equilibren ambos comportamientos.",
        "Para el control basado en EMT, esto significaría enfrentar varios objetivos tácticos {qk}K k=1, y la pregunta sería cómo fusionar y equilibrarlos.",
        "Un mecanismo de equilibrio se puede aplicar para resolver este problema.",
        "Ten en cuenta que el control basado en EMT, al seleccionar una acción, crea un vector de preferencia sobre el conjunto de acciones basado en su rendimiento predicho con respecto a un objetivo dado.",
        "Si estos vectores de preferencia están normalizados, pueden combinarse en una única preferencia unificada.",
        "Esto requiere reemplazar la selección de acciones basada en EMT estándar por el algoritmo a continuación [15]: • Dado: - un conjunto de dinámicas objetivo {qk}K k=1, - vector de pesos w(k) • Seleccionar la acción de la siguiente manera - Para cada acción a ∈ A predecir la distribución de estado futuro ¯pa t+1 = Ta ∗ pt; - Para cada acción, calcular Da = H(¯pa t+1, pt, PDt) - Para cada a ∈ A y objetivo táctico qk, denotar V (a, k) = DKL (Da qk) pt.",
        "Sea Vk(a) = 1 Zk V (a, k), donde Zk = a∈A V (a, k) es un factor de normalización. - Selecciona a∗ = arg min a k k=1 w(k)Vk(a) El vector de pesos w = (w1, ..., wK ) permite ajustar la importancia entre las dinámicas objetivo sin necesidad de rediseñar los objetivos mismos.",
        "Este método de equilibrio también está integrado de forma transparente en el flujo de operación basado en EMT. Limitaciones del Control Basado en EMT El control basado en EMT es una representación subóptima (en el sentido de DBC) de la estructura DBC.",
        "Limita al Usuario al obligar a EMT a ser su algoritmo de seguimiento dinámico, y reemplaza la optimización del Agente por la selección de acciones codiciosa.",
        "Este tipo de combinación, sin embargo, es común en algoritmos en línea.",
        "Aunque es necesario un mayor desarrollo de los controladores basados en EMT, la evidencia hasta ahora sugiere que incluso la forma más simple del algoritmo posee una gran cantidad de potencia y muestra tendencias que son óptimas en el sentido de DBC.",
        "Hay dos limitaciones adicionales específicas de los técnicos en emergencias médicas (EMT, por sus siglas en inglés) al control basado en EMT que son evidentes en este momento.",
        "Ambos ya tienen soluciones parciales y son objeto de investigación continua.",
        "La primera limitación es el problema de la preferencia negativa.",
        "En el marco de POMDP, por ejemplo, esto se captura de manera sencilla, a través de The Sixth Intl.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 793 la aparición de valores con signos diferentes dentro de la estructura de recompensa.",
        "Para el control basado en EMT, sin embargo, la preferencia negativa significa que uno quisiera evitar cierta distribución sobre las secuencias de desarrollo del sistema; sin embargo, el control basado en EMT se concentra en acercarse lo más posible a una distribución.",
        "La evitación es, por lo tanto, antinatural en el control basado en EMT nativo.",
        "La segunda limitación proviene del hecho de que el modelado del entorno estándar puede crear acciones sensoriales puras, acciones que no cambian el estado del mundo y solo difieren en la forma en que se reciben las observaciones y la calidad de las observaciones recibidas.",
        "Dado que el estado del mundo no cambia, el control basado en EMT no sería capaz de diferenciar entre diferentes acciones sensoriales.",
        "Ten en cuenta que ambas limitaciones del control basado en EMT están ausentes en el marco general de DBC, ya que puede tener un algoritmo de seguimiento capaz de considerar acciones sensoriales puras y, a diferencia de la divergencia de Kullback-Leibler, una medida de desviación de distribución capaz de manejar preferencias negativas. 5.",
        "El juego de la \"carrera de relevos\" fue introducido por primera vez en [11].",
        "Es un problema de agente único de capturar una presa, y pertenece a la clase de problemas de barrido de área.",
        "Un ejemplo de dominio se muestra en la Figura 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figura 2: Dominio de etiquetas; un agente (A) intenta buscar y capturar una presa (Q). El juego de etiquetas limita extremadamente la percepción de los agentes, de modo que el agente solo puede detectar la presa si están ubicados en la misma celda del mundo de cuadrícula.",
        "En la versión clásica del juego, la co-locación conduce a una observación especial, y la acción de Etiqueta se puede realizar.",
        "Modificamos ligeramente esta configuración: el momento en que ambos agentes ocupan la misma celda, el juego termina.",
        "Como resultado, tanto el agente como su presa tienen la misma capacidad de movimiento, lo que les permite moverse en cuatro direcciones, Norte, Sur, Este y Oeste.",
        "Estos forman un espacio formal de acciones dentro de un entorno markoviano.",
        "El espacio de estados del entorno formal markoviano está descrito por el producto cruz de las posiciones del agente y la presa.",
        "Para la Figura 2, sería S = {s0, ..., s23} × {s0, ..., s23}.",
        "Los efectos de una acción tomada por el agente son deterministas, pero el entorno en general tiene una respuesta estocástica debido al movimiento de la presa.",
        "Con probabilidad q0 1 se queda en su lugar, y con probabilidad 1 − q0 se mueve a una celda adyacente más lejos del agente 1. En nuestros experimentos, esto se tomó como q0 = 0.2.",
        "Por lo tanto, para la instancia mostrada en la Figura 2 y q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Aunque el comportamiento evasivo de la presa es conocido por el agente, la posición de la presa no lo es.",
        "La única información sensorial disponible para el agente es su propia ubicación.",
        "Utilizamos EMT y especificamos directamente la dinámica del objetivo.",
        "Para el juego de la mancha, podemos formular fácilmente tres tendencias principales: atrapar al perseguido, mantenerse en movimiento y acechar al perseguido.",
        "Esto resulta en las siguientes tres dinámicas de objetivo: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 en otro caso Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 en otro caso Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Nótese que ninguno de los objetivos anteriores es directamente alcanzable; por ejemplo, si Qt = s9 y At = s11, no hay ninguna acción que pueda llevar al agente a At+1 = s9 como lo requiere la dinámica de objetivo Tcatch.",
        "Realizamos varios experimentos para evaluar el rendimiento de EMT en el Juego de Etiquetas.",
        "Se utilizaron tres configuraciones del dominio mostrado en la Figura 3, cada una planteando un desafío diferente para el agente debido a la observabilidad parcial.",
        "En cada escenario, se realizó un conjunto de 1000 ejecuciones con un límite de tiempo de 100 pasos.",
        "En cada ejecución, la posición inicial tanto del agente como de su presa fue seleccionada al azar; esto significa que, en lo que respecta al agente, la posición inicial de la presa estaba distribuida de forma uniforme en todo el espacio de celdas del dominio.",
        "También utilizamos dos variaciones de la función de observabilidad del entorno.",
        "En la primera versión, la función de observabilidad mapeaba todas las posiciones articulares del cazador y la presa en la posición del cazador como observación.",
        "En el segundo, solo aquellas posiciones conjuntas en las que el cazador y la presa ocupaban ubicaciones diferentes fueron mapeadas en la ubicación del cazador.",
        "La segunda versión de hecho utilizó y expresó el hecho de que una vez que el cazador y la presa ocupan la misma celda, el juego termina.",
        "Los resultados de estos experimentos se muestran en la Tabla 2.",
        "Equilibrando la dinámica de captura, movimiento y seguimiento descrita en la sección anterior mediante el vector de peso [0.8, 0.1, 0.1], EMT logró un rendimiento estable en los tres dominios.",
        "Aunque las comparaciones directas son difíciles de hacer, el rendimiento de los EMT mostró una notable eficiencia en comparación con el enfoque POMDP.",
        "A pesar de una implementación simple e ineficiente del algoritmo EMT en Matlab, el tiempo de decisión para cualquier paso dado promedió significativamente por debajo de 1 segundo en todos los experimentos.",
        "Para el dominio de la arena abierta irregular, que resultó ser el más difícil, se completaron 1000 ejecuciones de experimentos limitadas a 100 pasos cada una, un total de 42411 pasos, en poco menos de 6 horas.",
        "Es decir, más de 4 × 104 pasos en línea tomaron una orden de magnitud menos tiempo que la computación sin conexión de la política POMDP en [11].",
        "La importancia de esta diferencia se destaca aún más por el hecho de que, en caso de que cambien los parámetros del modelo del entorno, la naturaleza en línea de EMT le permitiría mantener su rendimiento, mientras que la política POMDP necesitaría ser recalculada, lo que requeriría una vez más un gran costo computacional adicional.",
        "También probamos la entropía de la frecuencia de las células de comportamiento, medidas empíricas de los datos de prueba.",
        "Como muestran la Figura 4 y la Figura 5, empir794 The Sixth Intl.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 3: Se utilizaron estas configuraciones del espacio de juego de Tag: a) múltiples callejones sin salida, b) arena abierta irregular, c) pasillo circular. Tabla 2: Rendimiento de la solución basada en EMT en tres dominios de juego de Tag y dos modelos de observabilidad: I) cantera de omniposición, II) la cantera no está en la posición de los cazadores. Modelo Dominio Captura% E(Pasos) Tiempo/Paso I Callejones sin salida 100 14.8 72(mSeg) Arena 80.2 42.4 500(mSeg) Círculo 91.4 34.6 187(mSeg) II Callejones sin salida 100 13.2 91(mSeg) Arena 96.8 28.67 396(mSeg) Círculo 94.4 31.63 204(mSeg) La entropía lógica crece con la duración de la interacción.",
        "Para las carreras en las que la presa no fue capturada de inmediato, la entropía alcanza valores entre 0.85 y 0.952 para diferentes carreras y escenarios.",
        "A medida que el agente busca activamente la presa, la entropía nunca alcanza su máximo.",
        "Una característica del gráfico de entropía para el escenario de arena abierta llamó especialmente nuestra atención en el caso del modelo de observación de cantera omniposicional.",
        "Cerca del límite máximo de longitud de prueba (100 pasos), la entropía cayó repentinamente.",
        "Un análisis adicional de los datos mostró que bajo ciertas circunstancias, se produce un comportamiento fluctuante en el que el agente se enfrenta a versiones igualmente viables del comportamiento de seguir a la presa.",
        "Dado que el algoritmo EMT tiene una selección de acciones codiciosa, y el espacio de estados no codifica ninguna forma de compromiso (ni siquiera velocidad o aceleración), el agente queda atrapado dentro de una pequeña porción de celdas.",
        "Básicamente, se intenta seguir simultáneamente varios cursos de acción, todos los cuales son consistentes con la dinámica objetivo.",
        "Este comportamiento no ocurrió en nuestro segundo modelo de observación, ya que redujo significativamente el conjunto de cursos de acción elegibles, contribuyendo esencialmente a romper el empate entre ellos. 6.",
        "DISCUSIÓN El diseño de la solución EMT para el Juego de Etiquetas expone la diferencia fundamental en el enfoque de planificación y control entre EMT o DBC, por un lado, y el enfoque más familiar de POMDP, por otro lado.",
        "POMDP define una estructura de recompensa para optimizar e influencia la dinámica del sistema de forma indirecta a través de esa optimización.",
        "EMT descarta cualquier esquema de recompensa y, en cambio, mide e influye directamente en la dinámica del sistema. Se calculó la entropía 2 utilizando el logaritmo con base igual al número de ubicaciones posibles dentro del dominio; esto escala adecuadamente la expresión de entropía en el rango [0, 1] para todos los dominios.",
        "Por lo tanto, para el Juego de Etiquetas, no buscamos una función de recompensa que codificara y expresara nuestra preferencia sobre el comportamiento de los agentes, sino que establecimos directamente tres preferencias de comportamiento (heurísticas) como base para mantener la dinámica del objetivo.",
        "Los datos experimentales muestran que estos objetivos no necesitan ser directamente alcanzables a través de las acciones de los agentes.",
        "Sin embargo, aún queda por explorar la relación entre el rendimiento de la EMT y la alcanzabilidad de la dinámica objetivo.",
        "Los datos del experimento del juego de etiquetas también revelaron la diferente importancia que DBC y POMDPs otorgan a la formulación del espacio de estados del entorno.",
        "Los POMDPs dependen enteramente del mecanismo de maximización de la acumulación de recompensas, es decir, la formación del procedimiento de selección de acciones para lograr la secuenciación de estados necesaria.",
        "DBC, por otro lado, tiene dos fuentes de especificación de secuenciación: a través de las propiedades de un procedimiento de selección de acciones y a través de una especificación directa dentro de la dinámica objetivo.",
        "La importancia de la segunda fuente fue subrayada por los datos del experimento del Juego de Etiquetas, en el que el algoritmo EMT codicioso, aplicado a una especificación de espacio de estados tipo POMDP, falló, ya que la descripción del objetivo sobre dicho espacio de estados no era capaz de codificar las tendencias de comportamiento necesarias, por ejemplo, la resolución de empates y el compromiso con el movimiento dirigido.",
        "Las diferencias estructurales entre DBC (y EMT en particular) y POMDPs impiden la comparación directa de rendimiento y los sitúan en pistas complementarias, cada uno dentro de un nicho adecuado.",
        "Por ejemplo, los POMDP podrían ser vistos como una formulación mucho más natural de problemas económicos de toma de decisiones secuenciales, mientras que EMT se ajusta mejor a la demanda continua de cambios estocásticos, como ocurre en muchos problemas de robótica o de agentes incorporados.",
        "Las propiedades complementarias de POMDPs y EMT pueden ser aún más explotadas.",
        "Existe un interés reciente en utilizar POMDPs en soluciones híbridas [17], en las cuales los POMDPs pueden ser utilizados junto con otros enfoques de control para proporcionar resultados que no son fácilmente alcanzables con ninguno de los enfoques por sí solos.",
        "DBC puede ser un socio efectivo en una solución híbrida como esta.",
        "Por ejemplo, los POMDP tienen requisitos de tiempo fuera de línea prohibitivamente grandes para el cálculo de políticas, pero pueden ser fácilmente utilizados en entornos más simples para exponer tendencias de comportamiento beneficiosas; esto puede servir como una forma de dinámica objetivo que se proporciona a EMT en un dominio más grande para su operación en línea. 7.",
        "CONCLUSIONES Y TRABAJO FUTURO En este artículo, hemos presentado una nueva perspectiva sobre el proceso de planificación y control en entornos estocásticos, en forma del marco de Control Basado en Dinámicas (DBC).",
        "DBC formula la tarea de planificar como el apoyo a la dinámica de un sistema objetivo especificado, que describe las propiedades necesarias del cambio dentro del entorno.",
        "La optimalidad de los planes de acción de DBC se mide en el Sexto Internacional.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Calles sin salida 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Círculo Figura 4: Modelo de Observación I: Cantera omniposicional.",
        "Desarrollo de entropía con longitud de juego de etiqueta para los tres escenarios experimentales: a) múltiples callejones sin salida, b) arena abierta irregular, c) pasillo circular. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Callejones sin salida 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Círculo Figura 5: Modelo de observación II: cantera no observada en la posición de los cazadores.",
        "Desarrollo de la entropía con la longitud del juego de etiquetas para los tres escenarios experimentales: a) múltiples callejones sin salida, b) arena abierta irregular, c) pasillo circular, con respecto a la desviación de la dinámica del sistema real de la dinámica objetivo.",
        "Mostramos que una técnica recientemente desarrollada de Seguimiento Markov Extendido (EMT) [13] es una instancia de DBC.",
        "De hecho, EMT puede ser visto como un caso específico de la parametrización DBC, que emplea un procedimiento de selección de acciones codicioso.",
        "Dado que EMT exhibe las características clave del marco general DBC, así como una complejidad temporal polinómica, utilizamos la versión multitarget de EMT [15] para demostrar que la clase de problemas de barrido de área se presta naturalmente a descripciones basadas en dinámicas, tal como se ejemplifica en nuestros experimentos en el dominio del Juego de Etiquetas.",
        "Como se enumera en la Sección 4.2, EMT tiene varias limitaciones, como la dificultad para manejar la preferencia dinámica negativa.",
        "Esto evita la aplicación directa de la EMT a problemas como los Juegos de Rendezvous-Evasión (por ejemplo, [6]).",
        "Sin embargo, DBC en general no tiene tales limitaciones y permite fácilmente la formulación de juegos de evasión.",
        "En trabajos futuros, tenemos la intención de continuar con el desarrollo de controladores basados en dinámica para estos problemas. 8.",
        "AGRADECIMIENTO El trabajo de los dos primeros autores fue parcialmente apoyado por la subvención #898/05 de la Fundación para la Ciencia de Israel, y el tercer autor fue parcialmente apoyado por una subvención del Ministerio de Ciencia y Tecnología de Israel.",
        "REFERENCIAS [1] R. C. Arkin.",
        "Robótica basada en el comportamiento.",
        "MIT Press, 1998. [2] J. \n\nMIT Press, 1998. [2] J.",
        "A. Bilmes.",
        "Un tutorial detallado del algoritmo EM y su aplicación en la estimación de parámetros para mezclas gaussianas y modelos ocultos de Markov.",
        "Informe técnico TR-97-021, Departamento de Ingeniería Eléctrica e Informática, Universidad de California en Berkeley, 1998. [3] T. M. Cover y J.",
        "A. Thomas.",
        "Elementos de teoría de la información.",
        "Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz y M. J. Wolverton.",
        "Una encuesta de investigación en planificación distribuida y continua.",
        "Revista de Inteligencia Artificial, 4:13-22, 1999. [5] V. R. Konda y J. N. Tsitsiklis.",
        "Algoritmos Actor-Crítico.",
        "Revista SIAM de Control y Optimización, 42(4):1143-1166, 2003. [6] W. S. Lim.",
        "Un juego de encuentro-evasión en ubicaciones discretas con aleatorización conjunta.",
        "Avances en Probabilidad Aplicada, 29(4):1004-1017, diciembre de 1997. [7] M. L. Littman, T. L. Dean y L. P. Kaelbling.",
        "Sobre la complejidad de resolver problemas de decisión de Markov.",
        "En Actas de la 11ª Conferencia Anual sobre Incertidumbre en Inteligencia Artificial (UAI-95), páginas 394-402, 1995. [8] O. Madani, S. Hanks y A. Condon.",
        "Sobre la indecidibilidad de la planificación probabilística y problemas relacionados de optimización estocástica.",
        "Revista de Inteligencia Artificial, 147(1-2):5-34, julio de 2003. [9] R. M. Neal y G. E. Hinton.",
        "Una vista del algoritmo EM 796 El Sexto Internacional.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) que justifica enfoques incrementales, dispersos y otras variantes.",
        "En M. I. Jordan, editor, Aprendizaje en Modelos Gráficos, páginas 355-368.",
        "Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez y S. Kraus.",
        "Seguridad en sistemas multiagentes mediante la aleatorización de políticas.",
        "En Actas de AAMAS 2006, 2006. [11] J. Pineau, G. Gordon y S. Thrun.",
        "Iteración de valor basada en puntos: Un algoritmo en cualquier momento para POMDPs.",
        "En la Conferencia Conjunta Internacional de Inteligencia Artificial (IJCAI), páginas 1025-1032, agosto de 2003. [12] M. L. Puterman.",
        "Procesos de Decisión de Markov.",
        "Serie Wiley en Probabilidad y Estadística Matemática: Sección de Probabilidad y Estadística Aplicada.",
        "Publicación de Wiley-Interscience, Nueva York, 1994. [13] Z. Rabinovich y J. S. Rosenschein.",
        "Seguimiento Markov extendido con una aplicación al control.",
        "En el taller sobre seguimiento de agentes: modelando otros agentes a partir de observaciones, en la Tercera Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, páginas 95-100, Nueva York, julio de 2004. [14] Z. Rabinovich y J. S. Rosenschein.",
        "Coordinación multiagente mediante Seguimiento Markov Extendido.",
        "En la Cuarta Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, páginas 431-438, Utrecht, Países Bajos, julio de 2005. [15] Z. Rabinovich y J. S. Rosenschein.",
        "Sobre la respuesta del control basado en EMT a objetivos y modelos interactivos.",
        "En la Quinta Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, páginas 465-470, Hakodate, Japón, mayo de 2006. [16] R. F. Stengel.",
        "Control óptimo y estimación.",
        "Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, y P. Varakantham.",
        "Conflictos en el trabajo en equipo: Híbridos para el Sexto Internacional.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 797"
    ],
    "error_count": 11,
    "keys": {
        "dynamics based control": {
            "translated_key": "Control Basado en Dinámicas",
            "is_in_text": true,
            "original_annotated_sentences": [
                "<br>dynamics based control</br> with an Application to Area-Sweeping Problems Zinovi Rabinovich Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel jeff@cs.huji.ac.il Gal A. Kaminka The MAVERICK Group Department of Computer Science Bar Ilan University, Israel galk@cs.biu.ac.il ABSTRACT In this paper we introduce <br>dynamics based control</br> (DBC), an approach to planning and control of an agent in stochastic environments.",
                "Unlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics.",
                "We show that a recently developed planning and control approach, Extended Markov Tracking (EMT) is an instantiation of DBC.",
                "EMT employs greedy action selection to provide an efficient control algorithm in Markovian environments.",
                "We exploit this efficiency in a set of experiments that applied multitarget EMT to a class of area-sweeping problems (searching for moving targets).",
                "We show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation.",
                "Categories and Subject Descriptors I.2.8 [Problem Solving, Control Methods, and Search]: Control Theory; I.2.9 [Robotics]; I.2.11 [Distributed Artificial Intelligence]: Intelligent Agents General Terms Algorithms, Theory 1.",
                "INTRODUCTION Planning and control constitutes a central research area in multiagent systems and artificial intelligence.",
                "In recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in stochastic environments.",
                "In this framework, the planning and control problem is often addressed by imposing a reward function, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility.",
                "While theoretically attractive, the complexity of optimally solving a POMDP is prohibitive [8, 7].",
                "We take an alternative view of planning in stochastic environments.",
                "We do not use a (state-based) reward function, but instead optimize over a different criterion, a transition-based specification of the desired system dynamics.",
                "The idea here is to view planexecution as a process that compels a (stochastic) system to change, and a plan as a dynamic process that shapes that change according to desired criteria.",
                "We call this general planning framework <br>dynamics based control</br> (DBC).",
                "In DBC, the goal of a planning (or control) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) target dynamics.",
                "As actual system behavior may deviate from that which is specified by target dynamics (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16].",
                "Here, optimality is measured in terms of probability of deviation magnitudes.",
                "In this paper, we present the structure of <br>dynamics based control</br>.",
                "We show that the recently developed Extended Markov Tracking (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC.",
                "EMT is an efficient instantiation of DBC.",
                "To evaluate DBC, we carried out a set of experiments applying multi-target EMT to the Tag Game [11]; this is a variant on the area sweeping problem, where an agent is trying to tag a moving target (quarry) whose position is not known with certainty.",
                "Experimental data demonstrates that even with a simple model of the environment and a simple design of target dynamics, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agents position).",
                "The paper is organized as follows.",
                "In Section 2 we motivate DBC using area-sweeping problems, and discuss related work.",
                "Section 3 introduces the <br>dynamics based control</br> (DBC) structure, and its specialization to Markovian environments.",
                "This is followed by a review of the Extended Markov Tracking (EMT) approach as a DBC-structured control regimen in Section 4.",
                "That section also discusses the limitations of EMT-based control relative to the general DBC framework.",
                "Experimental settings and results are then presented in Section 5.",
                "Section 6 provides a short discussion of the overall approach, and Section 7 gives some concluding remarks and directions for future work. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "MOTIVATION AND RELATED WORK Many real-life scenarios naturally have a stochastic target dynamics specification, especially those domains where there exists no ultimate goal, but rather system behavior (with specific properties) that has to be continually supported.",
                "For example, security guards perform persistent sweeps of an area to detect any sign of intrusion.",
                "Cunning thieves will attempt to track these sweeps, and time their operation to key points of the guards motion.",
                "It is thus advisable to make the guards motion dynamics appear irregular and random.",
                "Recent work by Paruchuri et al. [10] has addressed such randomization in the context of single-agent and distributed POMDPs.",
                "The goal in that work was to generate policies that provide a measure of action-selection randomization, while maintaining rewards within some acceptable levels.",
                "Our focus differs from this work in that DBC does not optimize expected rewards-indeed we do not consider rewards at all-but instead maintains desired dynamics (including, but not limited to, randomization).",
                "The Game of Tag is another example of the applicability of the approach.",
                "It was introduced in the work by Pineau et al. [11].",
                "There are two agents that can move about an area, which is divided into a grid.",
                "The grid may have blocked cells (holes) into which no agent can move.",
                "One agent (the hunter) seeks to move into a cell occupied by the other (the quarry), such that they are co-located (this is a successful tag).",
                "The quarry seeks to avoid the hunter agent, and is always aware of the hunters position, but does not know how the hunter will behave, which opens up the possibility for a hunter to surprise the prey.",
                "The hunter knows the quarrys probabilistic law of motion, but does not know its current location.",
                "Tag is an instance of a family of area-sweeping (pursuit-evasion) problems.",
                "In [11], the hunter modeled the problem using a POMDP.",
                "A reward function was defined, to reflect the desire to tag the quarry, and an action policy was computed to optimize the reward collected over time.",
                "Due to the intractable complexity of determining the optimal policy, the action policy computed in that paper was essentially an approximation.",
                "In this paper, instead of formulating a reward function, we use EMT to solve the problem, by directly specifying the target dynamics.",
                "In fact, any search problem with randomized motion, the socalled class of area sweeping problems, can be described through specification of such target system dynamics.",
                "<br>dynamics based control</br> provides a natural approach to solving these problems. 3.",
                "<br>dynamics based control</br> The specification of <br>dynamics based control</br> (DBC) can be broken into three interacting levels: Environment Design Level, User Level, and Agent Level. • Environment Design Level is concerned with the formal specification and modeling of the environment.",
                "For example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation constant. • User Level in turn relies on the environment model produced by Environment Design to specify the target system dynamics it wishes to observe.",
                "The User Level also specifies the estimation or learning procedure for system dynamics, and the measure of deviation.",
                "In the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping. • Agent Level in turn combines the environment model from the Environment Design level, the dynamics estimation procedure, the deviation measure and the target dynamics specification from User Level, to produce a sequence of actions that create system dynamics as close as possible to the targeted specification.",
                "As we are interested in the continual development of a stochastic system, such as happens in classical control theory [16] and continual planning [4], as well as in our example of museum sweeps, the question becomes how the Agent Level is to treat the deviation measurements over time.",
                "To this end, we use a probability threshold-that is, we would like the Agent Level to maximize the probability that the deviation measure will remain below a certain threshold.",
                "Specific action selection then depends on system formalization.",
                "One possibility would be to create a mixture of available system trends, much like that which happens in Behavior-Based Robotic architectures [1].",
                "The other alternative would be to rely on the estimation procedure provided by the User Level-to utilize the Environment Design Level model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a certain dynamics has been achieved.",
                "Notice that this manipulation is not direct, but via the environment.",
                "Thus, for strong enough estimator algorithms, successful manipulation would mean a successful simulation of the specified target dynamics (i.e., beyond discerning via the available sensory input).",
                "DBC levels can also have a back-flow of information (see Figure 1).",
                "For instance, the Agent Level could provide data about target dynamics feasibility, allowing the User Level to modify the requirement, perhaps focusing on attainable features of system behavior.",
                "Data would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the User Level, this can provide an important tool for the environment model calibration at the Environment Design Level.",
                "UserEnv.",
                "Design Agent Model Ideal Dynamics Estimator Estimator Dynamics Feasibility System Response Data Figure 1: Data flow of the DBC framework Extending upon the idea of Actor-Critic algorithms [5], DBC data flow can provide a good basis for the design of a learning algorithm.",
                "For example, the User Level can operate as an exploratory device for a learning algorithm, inferring an ideal dynamics target from the environment model at hand that would expose and verify most critical features of system behavior.",
                "In this case, feasibility and system response data from the Agent Level would provide key information for an environment model update.",
                "In fact, the combination of feasibility and response data can provide a basis for the application of strong learning algorithms such as EM [2, 9]. 3.1 DBC for Markovian Environments For a Partially Observable Markovian Environment, DBC can be specified in a more rigorous manner.",
                "Notice how DBC discards rewards, and replaces it by another optimality criterion (structural differences are summarized in Table 1): • Environment Design level is to specify a tuple < S, A, T, O, Ω, s0 >, where: - S is the set of all possible environment states; - s0 is the initial state of the environment (which can also be viewed as a probability distribution over S); The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 791 - A is the set of all possible actions applicable in the environment; - T is the environments probabilistic transition function: T : S ×A → Π(S).",
                "That is, T(s |a, s) is the probability that the environment will move from state s to state s under action a; - O is the set of all possible observations.",
                "This is what the sensor input would look like for an outside observer; - Ω is the observation probability function: Ω : S × A × S → Π(O).",
                "That is, Ω(o|s , a, s) is the probability that one will observe o given that the environment has moved from state s to state s under action a. • User Level, in the case of Markovian environment, operates on the set of system dynamics described by a family of conditional probabilities F = {τ : S × A → Π(S)}.",
                "Thus specification of target dynamics can be expressed by q ∈ F, and the learning or tracking algorithm can be represented as a function L : O×(A×O)∗ → F; that is, it maps sequences of observations and actions performed so far into an estimate τ ∈ F of system dynamics.",
                "There are many possible variations available at the User Level to define divergence between system dynamics; several of them are: - Trace distance or L1 distance between two distributions p and q defined by D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Fidelity measure of distance F(p(·), q(·)) = x p(x)q(x) - Kullback-Leibler divergence DKL(p(·) q(·)) = x p(x) log p(x) q(x) Notice that the latter two are not actually metrics over the space of possible distributions, but nevertheless have meaningful and important interpretations.",
                "For instance, KullbackLeibler divergence is an important tool of information theory [3] that allows one to measure the price of encoding an information source governed by q, while assuming that it is governed by p. The User Level also defines the threshold of dynamics deviation probability θ. • Agent Level is then faced with a problem of selecting a control signal function a∗ to satisfy a minimization problem as follows: a∗ = arg min a Pr(d(τa, q) > θ) where d(τa, q) is a random variable describing deviation of the dynamics estimate τa, created by L under control signal a, from the ideal dynamics q.",
                "Implicit in this minimization problem is that L is manipulated via the environment, based on the environment model produced by the Environment Design Level. 3.2 DBC View of the State Space It is important to note the complementary view that DBC and POMDPs take on the state space of the environment.",
                "POMDPs regard state as a stationary snap-shot of the environment; whatever attributes of state sequencing one seeks are reached through properties of the control process, in this case reward accumulation.",
                "This can be viewed as if the sequencing of states and the attributes of that sequencing are only introduced by and for the controlling mechanism-the POMDP policy.",
                "DBC concentrates on the underlying principle of state sequencing, the system dynamics.",
                "DBCs target dynamics specification can use the environments state space as a means to describe, discern, and preserve changes that occur within the system.",
                "As a result, DBC has a greater ability to express state sequencing properties, which are grounded in the environment model and its state space definition.",
                "For example, consider the task of moving through rough terrain towards a goal and reaching it as fast as possible.",
                "POMDPs would encode terrain as state space points, while speed would be ensured by negative reward for every step taken without reaching the goalaccumulating higher reward can be reached only by faster motion.",
                "Alternatively, the state space could directly include the notion of speed.",
                "For POMDPs, this would mean that the same concept is encoded twice, in some sense: directly in the state space, and indirectly within reward accumulation.",
                "Now, even if the reward function would encode more, and finer, details of the properties of motion, the POMDP solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure.",
                "On the other hand, the tactical target expression of variations and correlations between position and speed of motion is now grounded in the state space representation.",
                "In this situation, any further constraints, e.g., smoothness of motion, speed limits in different locations, or speed reductions during sharp turns, are explicitly and uniformly expressed by the tactical target, and can result in faster and more effective action selection by a DBC algorithm. 4.",
                "EMT-BASED CONTROL AS A DBC Recently, a control algorithm was introduced called EMT-based Control [13], which instantiates the DBC framework.",
                "Although it provides an approximate greedy solution in the DBC sense, initial experiments using EMT-based control have been encouraging [14, 15].",
                "EMT-based control is based on the Markovian environment definition, as in the case with POMDPs, but its User and Agent Levels are of the Markovian DBC type of optimality. • User Level of EMT-based control defines a limited-case target system dynamics independent of action: qEMT : S → Π(S).",
                "It then utilizes the Kullback-Leibler divergence measure to compose a momentary system dynamics estimator-the Extended Markov Tracking (EMT) algorithm.",
                "The algorithm keeps a system dynamics estimate τt EMT that is capable of explaining recent change in an auxiliary Bayesian system state estimator from pt−1 to pt, and updates it conservatively using Kullback-Leibler divergence.",
                "Since τt EMT and pt−1,t are respectively the conditional and marginal probabilities over the systems state space, explanation simply means that pt(s ) = s τt EMT (s |s)pt−1(s), and the dynamics estimate update is performed by solving a 792 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: Structure of POMDP vs. Dynamics-Based Control in Markovian Environment Level Approach MDP Markovian DBC Environment < S, A, T, O, Ω >,where S - set of states A - set of actions Design T : S × A → Π(S) - transition O - observation set Ω : S × A × S → Π(O) User r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - reward function q - ideal dynamics F - reward remodeling L - dynamics estimator θ - threshold Agent π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) minimization problem: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • Agent Level in EMT-based control is suboptimal with respect to DBC (though it remains within the DBC framework), performing greedy action selection based on prediction of EMTs reaction.",
                "The prediction is based on the environment model provided by the Environment Design level, so that if we denote by Ta the environments transition function limited to action a, and pt−1 is the auxiliary Bayesian system state estimator, then the EMT-based control choice is described by a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1) Note that this follows the Markovian DBC framework precisely: the rewarding optimality of POMDPs is discarded, and in its place a dynamics estimator (EMT in this case) is manipulated via action effects on the environment to produce an estimate close to the specified target system dynamics.",
                "Yet as we mentioned, naive EMTbased control is suboptimal in the DBC sense, and has several additional limitations that do not exist in the general DBC framework (discussed in Section 4.2). 4.1 Multi-Target EMT At times, there may exist several behavioral preferences.",
                "For example, in the case of museum guards, some art items are more heavily guarded, requiring that the guards stay more often in their close vicinity.",
                "On the other hand, no corner of the museum is to be left unchecked, which demands constant motion.",
                "Successful museum security would demand that the guards adhere to, and balance, both of these behaviors.",
                "For EMT-based control, this would mean facing several tactical targets {qk}K k=1, and the question becomes how to merge and balance them.",
                "A balancing mechanism can be applied to resolve this issue.",
                "Note that EMT-based control, while selecting an action, creates a preference vector over the set of actions based on their predicted performance with respect to a given target.",
                "If these preference vectors are normalized, they can be combined into a single unified preference.",
                "This requires replacement of standard EMT-based action selection by the algorithm below [15]: • Given: - a set of target dynamics {qk}K k=1, - vector of weights w(k) • Select action as follows - For each action a ∈ A predict the future state distribution ¯pa t+1 = Ta ∗ pt; - For each action, compute Da = H(¯pa t+1, pt, PDt) - For each a ∈ A and qk tactical target, denote V (a, k) = DKL (Da qk) pt .",
                "Let Vk(a) = 1 Zk V (a, k), where Zk = a∈A V (a, k) is a normalization factor. - Select a∗ = arg min a k k=1 w(k)Vk(a) The weights vector w = (w1, ..., wK ) allows the additional tuning of importance among target dynamics without the need to redesign the targets themselves.",
                "This balancing method is also seamlessly integrated into the EMT-based control flow of operation. 4.2 EMT-based Control Limitations EMT-based control is a sub-optimal (in the DBC sense) representative of the DBC structure.",
                "It limits the User by forcing EMT to be its dynamic tracking algorithm, and replaces Agent optimization by greedy action selection.",
                "This kind of combination, however, is common for on-line algorithms.",
                "Although further development of EMT-based controllers is necessary, evidence so far suggests that even the simplest form of the algorithm possesses a great deal of power, and displays trends that are optimal in the DBC sense of the word.",
                "There are two further, EMT-specific, limitations to EMT-based control that are evident at this point.",
                "Both already have partial solutions and are subjects of ongoing research.",
                "The first limitation is the problem of negative preference.",
                "In the POMDP framework for example, this is captured simply, through The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 793 the appearance of values with different signs within the reward structure.",
                "For EMT-based control, however, negative preference means that one would like to avoid a certain distribution over system development sequences; EMT-based control, however, concentrates on getting as close as possible to a distribution.",
                "Avoidance is thus unnatural in native EMT-based control.",
                "The second limitation comes from the fact that standard environment modeling can create pure sensory actions-actions that do not change the state of the world, and differ only in the way observations are received and the quality of observations received.",
                "Since the world state does not change, EMT-based control would not be able to differentiate between different sensory actions.",
                "Notice that both of these limitations of EMT-based control are absent from the general DBC framework, since it may have a tracking algorithm capable of considering pure sensory actions and, unlike Kullback-Leibler divergence, a distribution deviation measure that is capable of dealing with negative preference. 5.",
                "EMT PLAYING TAG The Game of Tag was first introduced in [11].",
                "It is a single agent problem of capturing a quarry, and belongs to the class of area sweeping problems.",
                "An example domain is shown in Figure 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figure 2: Tag domain; an agent (A) attempts to seek and capture a quarry (Q) The Game of Tag extremely limits the agents perception, so that the agent is able to detect the quarry only if they are co-located in the same cell of the grid world.",
                "In the classical version of the game, co-location leads to a special observation, and the Tag action can be performed.",
                "We slightly modify this setting: the moment that both agents occupy the same cell, the game ends.",
                "As a result, both the agent and its quarry have the same motion capability, which allows them to move in four directions, North, South, East, and West.",
                "These form a formal space of actions within a Markovian environment.",
                "The state space of the formal Markovian environment is described by the cross-product of the agent and quarrys positions.",
                "For Figure 2, it would be S = {s0, ..., s23} × {s0, ..., s23}.",
                "The effects of an action taken by the agent are deterministic, but the environment in general has a stochastic response due to the motion of the quarry.",
                "With probability q0 1 it stays put, and with probability 1 − q0 it moves to an adjacent cell further away from the 1 In our experiments this was taken to be q0 = 0.2. agent.",
                "So for the instance shown in Figure 2 and q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Although the evasive behavior of the quarry is known to the agent, the quarrys position is not.",
                "The only sensory information available to the agent is its own location.",
                "We use EMT and directly specify the target dynamics.",
                "For the Game of Tag, we can easily formulate three major trends: catching the quarry, staying mobile, and stalking the quarry.",
                "This results in the following three target dynamics: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 otherwise Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 otherwise Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Note that none of the above targets are directly achievable; for instance, if Qt = s9 and At = s11, there is no action that can move the agent to At+1 = s9 as required by the Tcatch target dynamics.",
                "We ran several experiments to evaluate EMT performance in the Tag Game.",
                "Three configurations of the domain shown in Figure 3 were used, each posing a different challenge to the agent due to partial observability.",
                "In each setting, a set of 1000 runs was performed with a time limit of 100 steps.",
                "In every run, the initial position of both the agent and its quarry was selected at random; this means that as far as the agent was concerned, the quarrys initial position was uniformly distributed over the entire domain cell space.",
                "We also used two variations of the environment observability function.",
                "In the first version, observability function mapped all joint positions of hunter and quarry into the position of the hunter as an observation.",
                "In the second, only those joint positions in which hunter and quarry occupied different locations were mapped into the hunters location.",
                "The second version in fact utilized and expressed the fact that once hunter and quarry occupy the same cell the game ends.",
                "The results of these experiments are shown in Table 2.",
                "Balancing [15] the catch, move, and stalk target dynamics described in the previous section by the weight vector [0.8, 0.1, 0.1], EMT produced stable performance in all three domains.",
                "Although direct comparisons are difficult to make, the EMT performance displayed notable efficiency vis-`a-vis the POMDP approach.",
                "In spite of a simple and inefficient Matlab implementation of the EMT algorithm, the decision time for any given step averaged significantly below 1 second in all experiments.",
                "For the irregular open arena domain, which proved to be the most difficult, 1000 experiment runs bounded by 100 steps each, a total of 42411 steps, were completed in slightly under 6 hours.",
                "That is, over 4 × 104 online steps took an order of magnitude less time than the offline computation of POMDP policy in [11].",
                "The significance of this differential is made even more prominent by the fact that, should the environment model parameters change, the online nature of EMT would allow it to maintain its performance time, while the POMDP policy would need to be recomputed, requiring yet again a large overhead of computation.",
                "We also tested the behavior cell frequency entropy, empirical measures from trial data.",
                "As Figure 4 and Figure 5 show, empir794 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) A Q Q A 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 A Q Figure 3: These configurations of the Tag Game space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor Table 2: Performance of the EMT-based solution in three Tag Game domains and two observability models: I) omniposition quarry, II) quarry is not at hunters position Model Domain Capture% E(Steps) Time/Step I Dead-ends 100 14.8 72(mSec) Arena 80.2 42.4 500(mSec) Circle 91.4 34.6 187(mSec) II Dead-ends 100 13.2 91(mSec) Arena 96.8 28.67 396(mSec) Circle 94.4 31.63 204(mSec) ical entropy grows with the length of interaction.",
                "For runs where the quarry was not captured immediately, the entropy reaches between 0.85 and 0.952 for different runs and scenarios.",
                "As the agent actively seeks the quarry, the entropy never reaches its maximum.",
                "One characteristic of the entropy graph for the open arena scenario particularly caught our attention in the case of the omniposition quarry observation model.",
                "Near the maximum limit of trial length (100 steps), entropy suddenly dropped.",
                "Further analysis of the data showed that under certain circumstances, a fluctuating behavior occurs in which the agent faces equally viable versions of quarry-following behavior.",
                "Since the EMT algorithm has greedy action selection, and the state space does not encode any form of commitment (not even speed or acceleration), the agent is locked within a small portion of cells.",
                "It is essentially attempting to simultaneously follow several courses of action, all of which are consistent with the target dynamics.",
                "This behavior did not occur in our second observation model, since it significantly reduced the set of eligible courses of action-essentially contributing to tie-breaking among them. 6.",
                "DISCUSSION The design of the EMT solution for the Tag Game exposes the core difference in approach to planning and control between EMT or DBC, on the one hand, and the more familiar POMDP approach, on the other.",
                "POMDP defines a reward structure to optimize, and influences system dynamics indirectly through that optimization.",
                "EMT discards any reward scheme, and instead measures and influences system dynamics directly. 2 Entropy was calculated using log base equal to the number of possible locations within the domain; this properly scales entropy expression into the range [0, 1] for all domains.",
                "Thus for the Tag Game, we did not search for a reward function that would encode and express our preference over the agents behavior, but rather directly set three (heuristic) behavior preferences as the basis for target dynamics to be maintained.",
                "Experimental data shows that these targets need not be directly achievable via the agents actions.",
                "However, the ratio between EMT performance and achievability of target dynamics remains to be explored.",
                "The tag game experiment data also revealed the different emphasis DBC and POMDPs place on the formulation of an environment state space.",
                "POMDPs rely entirely on the mechanism of reward accumulation maximization, i.e., formation of the action selection procedure to achieve necessary state sequencing.",
                "DBC, on the other hand, has two sources of sequencing specification: through the properties of an action selection procedure, and through direct specification within the target dynamics.",
                "The importance of the second source was underlined by the Tag Game experiment data, in which the greedy EMT algorithm, applied to a POMDP-type state space specification, failed, since target description over such a state space was incapable of encoding the necessary behavior tendencies, e.g., tie-breaking and commitment to directed motion.",
                "The structural differences between DBC (and EMT in particular), and POMDPs, prohibits direct performance comparison, and places them on complementary tracks, each within a suitable niche.",
                "For instance, POMDPs could be seen as a much more natural formulation of economic sequential decision-making problems, while EMT is a better fit for continual demand for stochastic change, as happens in many robotic or embodied-agent problems.",
                "The complementary properties of POMDPs and EMT can be further exploited.",
                "There is recent interest in using POMDPs in hybrid solutions [17], in which the POMDPs can be used together with other control approaches to provide results not easily achievable with either approach by itself.",
                "DBC can be an effective partner in such a hybrid solution.",
                "For instance, POMDPs have prohibitively large off-line time requirements for policy computation, but can be readily used in simpler settings to expose beneficial behavioral trends; this can serve as a form of target dynamics that are provided to EMT in a larger domain for on-line operation. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have presented a novel perspective on the process of planning and control in stochastic environments, in the form of the <br>dynamics based control</br> (DBC) framework.",
                "DBC formulates the task of planning as support of a specified target system dynamics, which describes the necessary properties of change within the environment.",
                "Optimality of DBC plans of action are measured The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 4: Observation Model I: Omniposition quarry.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 5: Observation Model II: quarry not observed at hunters position.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. with respect to the deviation of actual system dynamics from the target dynamics.",
                "We show that a recently developed technique of Extended Markov Tracking (EMT) [13] is an instantiation of DBC.",
                "In fact, EMT can be seen as a specific case of DBC parameterization, which employs a greedy action selection procedure.",
                "Since EMT exhibits the key features of the general DBC framework, as well as polynomial time complexity, we used the multitarget version of EMT [15] to demonstrate that the class of area sweeping problems naturally lends itself to dynamics-based descriptions, as instantiated by our experiments in the Tag Game domain.",
                "As enumerated in Section 4.2, EMT has a number of limitations, such as difficulty in dealing with negative dynamic preference.",
                "This prevents direct application of EMT to such problems as Rendezvous-Evasion Games (e.g., [6]).",
                "However, DBC in general has no such limitations, and readily enables the formulation of evasion games.",
                "In future work, we intend to proceed with the development of dynamics-based controllers for these problems. 8.",
                "ACKNOWLEDGMENT The work of the first two authors was partially supported by Israel Science Foundation grant #898/05, and the third author was partially supported by a grant from Israels Ministry of Science and Technology. 9.",
                "REFERENCES [1] R. C. Arkin.",
                "Behavior-Based Robotics.",
                "MIT Press, 1998. [2] J.",
                "A. Bilmes.",
                "A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and Hidden Markov Models.",
                "Technical Report TR-97-021, Department of Electrical Engeineering and Computer Science, University of California at Berkeley, 1998. [3] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of information theory.",
                "Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz, and M. J. Wolverton.",
                "A survey of research in distributed, continual planning.",
                "AI Magazine, 4:13-22, 1999. [5] V. R. Konda and J. N. Tsitsiklis.",
                "Actor-Critic algorithms.",
                "SIAM Journal on Control and Optimization, 42(4):1143-1166, 2003. [6] W. S. Lim.",
                "A rendezvous-evasion game on discrete locations with joint randomization.",
                "Advances in Applied Probability, 29(4):1004-1017, December 1997. [7] M. L. Littman, T. L. Dean, and L. P. Kaelbling.",
                "On the complexity of solving Markov decision problems.",
                "In Proceedings of the 11th Annual Conference on Uncertainty in Artificial Intelligence (UAI-95), pages 394-402, 1995. [8] O. Madani, S. Hanks, and A. Condon.",
                "On the undecidability of probabilistic planning and related stochastic optimization problems.",
                "Artificial Intelligence Journal, 147(1-2):5-34, July 2003. [9] R. M. Neal and G. E. Hinton.",
                "A view of the EM algorithm 796 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) that justifies incremental, sparse, and other variants.",
                "In M. I. Jordan, editor, Learning in Graphical Models, pages 355-368.",
                "Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In Proceeding of AAMAS 2006, 2006. [11] J. Pineau, G. Gordon, and S. Thrun.",
                "Point-based value iteration: An anytime algorithm for pomdps.",
                "In International Joint Conference on Artificial Intelligence (IJCAI), pages 1025-1032, August 2003. [12] M. L. Puterman.",
                "Markov Decision Processes.",
                "Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics Section.",
                "Wiley-Interscience Publication, New York, 1994. [13] Z. Rabinovich and J. S. Rosenschein.",
                "Extended Markov Tracking with an application to control.",
                "In The Workshop on Agent Tracking: Modeling Other Agents from Observations, at the Third International Joint Conference on Autonomous Agents and Multiagent Systems, pages 95-100, New-York, July 2004. [14] Z. Rabinovich and J. S. Rosenschein.",
                "Multiagent coordination by Extended Markov Tracking.",
                "In The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 431-438, Utrecht, The Netherlands, July 2005. [15] Z. Rabinovich and J. S. Rosenschein.",
                "On the response of EMT-based control to interacting targets and models.",
                "In The Fifth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 465-470, Hakodate, Japan, May 2006. [16] R. F. Stengel.",
                "Optimal Control and Estimation.",
                "Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, and P. Varakantham.",
                "Conflicts in teamwork: Hybrids to the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 797"
            ],
            "original_annotated_samples": [
                "<br>dynamics based control</br> with an Application to Area-Sweeping Problems Zinovi Rabinovich Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel jeff@cs.huji.ac.il Gal A. Kaminka The MAVERICK Group Department of Computer Science Bar Ilan University, Israel galk@cs.biu.ac.il ABSTRACT In this paper we introduce <br>dynamics based control</br> (DBC), an approach to planning and control of an agent in stochastic environments.",
                "We call this general planning framework <br>dynamics based control</br> (DBC).",
                "In this paper, we present the structure of <br>dynamics based control</br>.",
                "Section 3 introduces the <br>dynamics based control</br> (DBC) structure, and its specialization to Markovian environments.",
                "<br>dynamics based control</br> provides a natural approach to solving these problems. 3."
            ],
            "translated_annotated_samples": [
                "En este artículo presentamos el <br>Control Basado en Dinámicas</br> (DBC), un enfoque para la planificación y control de un agente en entornos estocásticos.",
                "Llamamos a este marco de planificación general <br>Control Basado en Dinámicas</br> (DBC).",
                "En este documento, presentamos la estructura del <br>Control Basado en Dinámicas</br>.",
                "La Sección 3 introduce la estructura de <br>Control Basado en Dinámicas</br> (DBC) y su especialización en entornos Markovianos.",
                "El <br>Control Basado en Dinámicas</br> proporciona un enfoque natural para resolver estos problemas."
            ],
            "translated_text": "En este artículo presentamos el <br>Control Basado en Dinámicas</br> (DBC), un enfoque para la planificación y control de un agente en entornos estocásticos. A diferencia de los enfoques existentes, que buscan optimizar las recompensas esperadas (por ejemplo, en Problemas de Decisión de Markov Parcialmente Observables (POMDP)), DBC optimiza el comportamiento del sistema hacia las dinámicas del sistema especificadas. Mostramos que un enfoque de planificación y control recientemente desarrollado, Seguimiento Extendido de Markov (EMT), es una instancia de DBC. EMT emplea la selección de acciones codiciosa para proporcionar un algoritmo de control eficiente en entornos markovianos. Explotamos esta eficiencia en un conjunto de experimentos que aplicaron EMT multitarget a una clase de problemas de barrido de área (búsqueda de objetivos en movimiento). Mostramos que tales problemas pueden ser definidos de forma natural y resueltos eficientemente utilizando el marco de DBC y su instanciación de EMT. Categorías y Descriptores de Asignaturas I.2.8 [Resolución de Problemas, Métodos de Control y Búsqueda]: Teoría de Control; I.2.9 [Robótica]; I.2.11 [Inteligencia Artificial Distribuida]: Agentes Inteligentes Términos Generales Algoritmos, Teoría 1. INTRODUCCIÓN La planificación y el control constituyen un área de investigación central en sistemas multiagentes e inteligencia artificial. En los últimos años, los Procesos de Decisión de Markov Parcialmente Observables (POMDPs) [12] se han convertido en una base formal popular para la planificación en entornos estocásticos. En este marco, el problema de planificación y control suele abordarse imponiendo una función de recompensa y calculando una política (de elección de acciones) que sea óptima, en el sentido de que resultará en la mayor utilidad esperada. Si bien teóricamente atractivo, la complejidad de resolver óptimamente un POMDP es prohibitiva [8, 7]. Tomamos una visión alternativa de la planificación en entornos estocásticos. No utilizamos una función de recompensa basada en el estado, sino que optimizamos según un criterio diferente, una especificación basada en transiciones de la dinámica del sistema deseado. La idea aquí es ver la ejecución del plan como un proceso que obliga a un sistema (estocástico) a cambiar, y un plan como un proceso dinámico que moldea ese cambio de acuerdo con criterios deseados. Llamamos a este marco de planificación general <br>Control Basado en Dinámicas</br> (DBC). En DBC, el objetivo de un proceso de planificación (o control) es garantizar que el sistema cambiará de acuerdo con dinámicas objetivo específicas (potencialmente estocásticas). Dado que el comportamiento real del sistema puede desviarse de lo especificado por la dinámica objetivo (debido a la naturaleza estocástica del sistema), la planificación en tales entornos debe ser continua, de manera similar a los controladores en lazo cerrado clásicos. Aquí, la optimalidad se mide en términos de la probabilidad de magnitudes de desviación. En este documento, presentamos la estructura del <br>Control Basado en Dinámicas</br>. Mostramos que el enfoque de Seguimiento Extendido de Markov (EMT) recientemente desarrollado [13, 14, 15] está subsumido por DBC, con EMT empleando selección de acciones codiciosa, que es una parametrización específica entre las opciones posibles dentro de DBC. EMT es una implementación eficiente de DBC. Para evaluar DBC, llevamos a cabo un conjunto de experimentos aplicando EMT de múltiples objetivos al Juego de Etiquetas [11]; esta es una variante del problema de barrido de área, donde un agente intenta etiquetar a un objetivo móvil (presa) cuya posición no se conoce con certeza. Los datos experimentales demuestran que incluso con un modelo simple del entorno y un diseño simple de la dinámica del objetivo, se pueden lograr altas tasas de éxito tanto en la captura de la presa como en sorprender a la presa (como se expresa por la entropía observada de la posición de los agentes controlados). El documento está organizado de la siguiente manera. En la Sección 2 motivamos DBC utilizando problemas de barrido de área, y discutimos trabajos relacionados. La Sección 3 introduce la estructura de <br>Control Basado en Dinámicas</br> (DBC) y su especialización en entornos Markovianos. Esto es seguido por una revisión del enfoque de Seguimiento Markov Extendido (EMT) como un régimen de control estructurado por DBC en la Sección 4. Esa sección también discute las limitaciones del control basado en EMT en relación con el marco general de DBC. Los ajustes experimentales y los resultados se presentan a continuación en la Sección 5. La Sección 6 proporciona una breve discusión del enfoque general, y la Sección 7 ofrece algunas observaciones finales y direcciones para trabajos futuros. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2. MOTIVACIÓN Y TRABAJO RELACIONADO Muchos escenarios de la vida real tienen naturalmente una especificación de dinámica de destino estocástica, especialmente en aquellos dominios donde no existe un objetivo final, sino más bien un comportamiento del sistema (con propiedades específicas) que debe ser continuamente respaldado. Por ejemplo, los guardias de seguridad realizan barridos persistentes de un área para detectar cualquier signo de intrusión. Los ladrones astutos intentarán rastrear estos barridos y sincronizar su operación con puntos clave del movimiento de los guardias. Por lo tanto, es recomendable que la dinámica de movimiento de los guardias parezca irregular y aleatoria. El trabajo reciente de Paruchuri et al. [10] ha abordado dicha aleatorización en el contexto de POMDPs de agente único y distribuidos. El objetivo de ese trabajo era generar políticas que proporcionen una medida de aleatorización en la selección de acciones, manteniendo al mismo tiempo las recompensas dentro de niveles aceptables. Nuestro enfoque difiere de este trabajo en que DBC no optimiza recompensas esperadas, de hecho, no consideramos recompensas en absoluto, sino que en su lugar mantenemos dinámicas deseadas (incluyendo, pero no limitado a, la aleatorización). El juego de la mancha es otro ejemplo de la aplicabilidad del enfoque. Fue introducido en el trabajo de Pineau et al. [11]. Hay dos agentes que pueden moverse por un área, la cual está dividida en una cuadrícula. La cuadrícula puede tener celdas bloqueadas (agujeros) en las cuales ningún agente puede moverse. Un agente (el cazador) intenta moverse a una celda ocupada por el otro (la presa), de modo que estén ubicados en el mismo lugar (esto es una etiqueta exitosa). La presa busca evitar al cazador, siempre está al tanto de la posición del cazador, pero no sabe cómo se comportará el cazador, lo que abre la posibilidad de que un cazador sorprenda a la presa. El cazador conoce la ley de movimiento probabilístico de la presa, pero no conoce su ubicación actual. Tag es una instancia de una familia de problemas de barrido de área (persecución-evasión). En [11], el cazador modeló el problema utilizando un POMDP. Se definió una función de recompensa para reflejar el deseo de marcar la presa, y se calculó una política de acción para optimizar la recompensa recolectada con el tiempo. Debido a la complejidad intratable de determinar la política óptima, la política de acción calculada en ese documento fue esencialmente una aproximación. En este artículo, en lugar de formular una función de recompensa, utilizamos EMT para resolver el problema, especificando directamente la dinámica objetivo. De hecho, cualquier problema de búsqueda con movimiento aleatorio, la llamada clase de problemas de barrido de área, puede ser descrito a través de la especificación de la dinámica de dicho sistema objetivo. El <br>Control Basado en Dinámicas</br> proporciona un enfoque natural para resolver estos problemas. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "area-sweeping problem": {
            "translated_key": "problemas de barrido de área",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Dynamics Based Control with an Application to Area-Sweeping Problems Zinovi Rabinovich Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel jeff@cs.huji.ac.il Gal A. Kaminka The MAVERICK Group Department of Computer Science Bar Ilan University, Israel galk@cs.biu.ac.il ABSTRACT In this paper we introduce Dynamics Based Control (DBC), an approach to planning and control of an agent in stochastic environments.",
                "Unlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics.",
                "We show that a recently developed planning and control approach, Extended Markov Tracking (EMT) is an instantiation of DBC.",
                "EMT employs greedy action selection to provide an efficient control algorithm in Markovian environments.",
                "We exploit this efficiency in a set of experiments that applied multitarget EMT to a class of <br>area-sweeping problem</br>s (searching for moving targets).",
                "We show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation.",
                "Categories and Subject Descriptors I.2.8 [Problem Solving, Control Methods, and Search]: Control Theory; I.2.9 [Robotics]; I.2.11 [Distributed Artificial Intelligence]: Intelligent Agents General Terms Algorithms, Theory 1.",
                "INTRODUCTION Planning and control constitutes a central research area in multiagent systems and artificial intelligence.",
                "In recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in stochastic environments.",
                "In this framework, the planning and control problem is often addressed by imposing a reward function, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility.",
                "While theoretically attractive, the complexity of optimally solving a POMDP is prohibitive [8, 7].",
                "We take an alternative view of planning in stochastic environments.",
                "We do not use a (state-based) reward function, but instead optimize over a different criterion, a transition-based specification of the desired system dynamics.",
                "The idea here is to view planexecution as a process that compels a (stochastic) system to change, and a plan as a dynamic process that shapes that change according to desired criteria.",
                "We call this general planning framework Dynamics Based Control (DBC).",
                "In DBC, the goal of a planning (or control) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) target dynamics.",
                "As actual system behavior may deviate from that which is specified by target dynamics (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16].",
                "Here, optimality is measured in terms of probability of deviation magnitudes.",
                "In this paper, we present the structure of Dynamics Based Control.",
                "We show that the recently developed Extended Markov Tracking (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC.",
                "EMT is an efficient instantiation of DBC.",
                "To evaluate DBC, we carried out a set of experiments applying multi-target EMT to the Tag Game [11]; this is a variant on the area sweeping problem, where an agent is trying to tag a moving target (quarry) whose position is not known with certainty.",
                "Experimental data demonstrates that even with a simple model of the environment and a simple design of target dynamics, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agents position).",
                "The paper is organized as follows.",
                "In Section 2 we motivate DBC using <br>area-sweeping problem</br>s, and discuss related work.",
                "Section 3 introduces the Dynamics Based Control (DBC) structure, and its specialization to Markovian environments.",
                "This is followed by a review of the Extended Markov Tracking (EMT) approach as a DBC-structured control regimen in Section 4.",
                "That section also discusses the limitations of EMT-based control relative to the general DBC framework.",
                "Experimental settings and results are then presented in Section 5.",
                "Section 6 provides a short discussion of the overall approach, and Section 7 gives some concluding remarks and directions for future work. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "MOTIVATION AND RELATED WORK Many real-life scenarios naturally have a stochastic target dynamics specification, especially those domains where there exists no ultimate goal, but rather system behavior (with specific properties) that has to be continually supported.",
                "For example, security guards perform persistent sweeps of an area to detect any sign of intrusion.",
                "Cunning thieves will attempt to track these sweeps, and time their operation to key points of the guards motion.",
                "It is thus advisable to make the guards motion dynamics appear irregular and random.",
                "Recent work by Paruchuri et al. [10] has addressed such randomization in the context of single-agent and distributed POMDPs.",
                "The goal in that work was to generate policies that provide a measure of action-selection randomization, while maintaining rewards within some acceptable levels.",
                "Our focus differs from this work in that DBC does not optimize expected rewards-indeed we do not consider rewards at all-but instead maintains desired dynamics (including, but not limited to, randomization).",
                "The Game of Tag is another example of the applicability of the approach.",
                "It was introduced in the work by Pineau et al. [11].",
                "There are two agents that can move about an area, which is divided into a grid.",
                "The grid may have blocked cells (holes) into which no agent can move.",
                "One agent (the hunter) seeks to move into a cell occupied by the other (the quarry), such that they are co-located (this is a successful tag).",
                "The quarry seeks to avoid the hunter agent, and is always aware of the hunters position, but does not know how the hunter will behave, which opens up the possibility for a hunter to surprise the prey.",
                "The hunter knows the quarrys probabilistic law of motion, but does not know its current location.",
                "Tag is an instance of a family of area-sweeping (pursuit-evasion) problems.",
                "In [11], the hunter modeled the problem using a POMDP.",
                "A reward function was defined, to reflect the desire to tag the quarry, and an action policy was computed to optimize the reward collected over time.",
                "Due to the intractable complexity of determining the optimal policy, the action policy computed in that paper was essentially an approximation.",
                "In this paper, instead of formulating a reward function, we use EMT to solve the problem, by directly specifying the target dynamics.",
                "In fact, any search problem with randomized motion, the socalled class of area sweeping problems, can be described through specification of such target system dynamics.",
                "Dynamics Based Control provides a natural approach to solving these problems. 3.",
                "DYNAMICS BASED CONTROL The specification of Dynamics Based Control (DBC) can be broken into three interacting levels: Environment Design Level, User Level, and Agent Level. • Environment Design Level is concerned with the formal specification and modeling of the environment.",
                "For example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation constant. • User Level in turn relies on the environment model produced by Environment Design to specify the target system dynamics it wishes to observe.",
                "The User Level also specifies the estimation or learning procedure for system dynamics, and the measure of deviation.",
                "In the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping. • Agent Level in turn combines the environment model from the Environment Design level, the dynamics estimation procedure, the deviation measure and the target dynamics specification from User Level, to produce a sequence of actions that create system dynamics as close as possible to the targeted specification.",
                "As we are interested in the continual development of a stochastic system, such as happens in classical control theory [16] and continual planning [4], as well as in our example of museum sweeps, the question becomes how the Agent Level is to treat the deviation measurements over time.",
                "To this end, we use a probability threshold-that is, we would like the Agent Level to maximize the probability that the deviation measure will remain below a certain threshold.",
                "Specific action selection then depends on system formalization.",
                "One possibility would be to create a mixture of available system trends, much like that which happens in Behavior-Based Robotic architectures [1].",
                "The other alternative would be to rely on the estimation procedure provided by the User Level-to utilize the Environment Design Level model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a certain dynamics has been achieved.",
                "Notice that this manipulation is not direct, but via the environment.",
                "Thus, for strong enough estimator algorithms, successful manipulation would mean a successful simulation of the specified target dynamics (i.e., beyond discerning via the available sensory input).",
                "DBC levels can also have a back-flow of information (see Figure 1).",
                "For instance, the Agent Level could provide data about target dynamics feasibility, allowing the User Level to modify the requirement, perhaps focusing on attainable features of system behavior.",
                "Data would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the User Level, this can provide an important tool for the environment model calibration at the Environment Design Level.",
                "UserEnv.",
                "Design Agent Model Ideal Dynamics Estimator Estimator Dynamics Feasibility System Response Data Figure 1: Data flow of the DBC framework Extending upon the idea of Actor-Critic algorithms [5], DBC data flow can provide a good basis for the design of a learning algorithm.",
                "For example, the User Level can operate as an exploratory device for a learning algorithm, inferring an ideal dynamics target from the environment model at hand that would expose and verify most critical features of system behavior.",
                "In this case, feasibility and system response data from the Agent Level would provide key information for an environment model update.",
                "In fact, the combination of feasibility and response data can provide a basis for the application of strong learning algorithms such as EM [2, 9]. 3.1 DBC for Markovian Environments For a Partially Observable Markovian Environment, DBC can be specified in a more rigorous manner.",
                "Notice how DBC discards rewards, and replaces it by another optimality criterion (structural differences are summarized in Table 1): • Environment Design level is to specify a tuple < S, A, T, O, Ω, s0 >, where: - S is the set of all possible environment states; - s0 is the initial state of the environment (which can also be viewed as a probability distribution over S); The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 791 - A is the set of all possible actions applicable in the environment; - T is the environments probabilistic transition function: T : S ×A → Π(S).",
                "That is, T(s |a, s) is the probability that the environment will move from state s to state s under action a; - O is the set of all possible observations.",
                "This is what the sensor input would look like for an outside observer; - Ω is the observation probability function: Ω : S × A × S → Π(O).",
                "That is, Ω(o|s , a, s) is the probability that one will observe o given that the environment has moved from state s to state s under action a. • User Level, in the case of Markovian environment, operates on the set of system dynamics described by a family of conditional probabilities F = {τ : S × A → Π(S)}.",
                "Thus specification of target dynamics can be expressed by q ∈ F, and the learning or tracking algorithm can be represented as a function L : O×(A×O)∗ → F; that is, it maps sequences of observations and actions performed so far into an estimate τ ∈ F of system dynamics.",
                "There are many possible variations available at the User Level to define divergence between system dynamics; several of them are: - Trace distance or L1 distance between two distributions p and q defined by D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Fidelity measure of distance F(p(·), q(·)) = x p(x)q(x) - Kullback-Leibler divergence DKL(p(·) q(·)) = x p(x) log p(x) q(x) Notice that the latter two are not actually metrics over the space of possible distributions, but nevertheless have meaningful and important interpretations.",
                "For instance, KullbackLeibler divergence is an important tool of information theory [3] that allows one to measure the price of encoding an information source governed by q, while assuming that it is governed by p. The User Level also defines the threshold of dynamics deviation probability θ. • Agent Level is then faced with a problem of selecting a control signal function a∗ to satisfy a minimization problem as follows: a∗ = arg min a Pr(d(τa, q) > θ) where d(τa, q) is a random variable describing deviation of the dynamics estimate τa, created by L under control signal a, from the ideal dynamics q.",
                "Implicit in this minimization problem is that L is manipulated via the environment, based on the environment model produced by the Environment Design Level. 3.2 DBC View of the State Space It is important to note the complementary view that DBC and POMDPs take on the state space of the environment.",
                "POMDPs regard state as a stationary snap-shot of the environment; whatever attributes of state sequencing one seeks are reached through properties of the control process, in this case reward accumulation.",
                "This can be viewed as if the sequencing of states and the attributes of that sequencing are only introduced by and for the controlling mechanism-the POMDP policy.",
                "DBC concentrates on the underlying principle of state sequencing, the system dynamics.",
                "DBCs target dynamics specification can use the environments state space as a means to describe, discern, and preserve changes that occur within the system.",
                "As a result, DBC has a greater ability to express state sequencing properties, which are grounded in the environment model and its state space definition.",
                "For example, consider the task of moving through rough terrain towards a goal and reaching it as fast as possible.",
                "POMDPs would encode terrain as state space points, while speed would be ensured by negative reward for every step taken without reaching the goalaccumulating higher reward can be reached only by faster motion.",
                "Alternatively, the state space could directly include the notion of speed.",
                "For POMDPs, this would mean that the same concept is encoded twice, in some sense: directly in the state space, and indirectly within reward accumulation.",
                "Now, even if the reward function would encode more, and finer, details of the properties of motion, the POMDP solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure.",
                "On the other hand, the tactical target expression of variations and correlations between position and speed of motion is now grounded in the state space representation.",
                "In this situation, any further constraints, e.g., smoothness of motion, speed limits in different locations, or speed reductions during sharp turns, are explicitly and uniformly expressed by the tactical target, and can result in faster and more effective action selection by a DBC algorithm. 4.",
                "EMT-BASED CONTROL AS A DBC Recently, a control algorithm was introduced called EMT-based Control [13], which instantiates the DBC framework.",
                "Although it provides an approximate greedy solution in the DBC sense, initial experiments using EMT-based control have been encouraging [14, 15].",
                "EMT-based control is based on the Markovian environment definition, as in the case with POMDPs, but its User and Agent Levels are of the Markovian DBC type of optimality. • User Level of EMT-based control defines a limited-case target system dynamics independent of action: qEMT : S → Π(S).",
                "It then utilizes the Kullback-Leibler divergence measure to compose a momentary system dynamics estimator-the Extended Markov Tracking (EMT) algorithm.",
                "The algorithm keeps a system dynamics estimate τt EMT that is capable of explaining recent change in an auxiliary Bayesian system state estimator from pt−1 to pt, and updates it conservatively using Kullback-Leibler divergence.",
                "Since τt EMT and pt−1,t are respectively the conditional and marginal probabilities over the systems state space, explanation simply means that pt(s ) = s τt EMT (s |s)pt−1(s), and the dynamics estimate update is performed by solving a 792 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: Structure of POMDP vs. Dynamics-Based Control in Markovian Environment Level Approach MDP Markovian DBC Environment < S, A, T, O, Ω >,where S - set of states A - set of actions Design T : S × A → Π(S) - transition O - observation set Ω : S × A × S → Π(O) User r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - reward function q - ideal dynamics F - reward remodeling L - dynamics estimator θ - threshold Agent π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) minimization problem: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • Agent Level in EMT-based control is suboptimal with respect to DBC (though it remains within the DBC framework), performing greedy action selection based on prediction of EMTs reaction.",
                "The prediction is based on the environment model provided by the Environment Design level, so that if we denote by Ta the environments transition function limited to action a, and pt−1 is the auxiliary Bayesian system state estimator, then the EMT-based control choice is described by a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1) Note that this follows the Markovian DBC framework precisely: the rewarding optimality of POMDPs is discarded, and in its place a dynamics estimator (EMT in this case) is manipulated via action effects on the environment to produce an estimate close to the specified target system dynamics.",
                "Yet as we mentioned, naive EMTbased control is suboptimal in the DBC sense, and has several additional limitations that do not exist in the general DBC framework (discussed in Section 4.2). 4.1 Multi-Target EMT At times, there may exist several behavioral preferences.",
                "For example, in the case of museum guards, some art items are more heavily guarded, requiring that the guards stay more often in their close vicinity.",
                "On the other hand, no corner of the museum is to be left unchecked, which demands constant motion.",
                "Successful museum security would demand that the guards adhere to, and balance, both of these behaviors.",
                "For EMT-based control, this would mean facing several tactical targets {qk}K k=1, and the question becomes how to merge and balance them.",
                "A balancing mechanism can be applied to resolve this issue.",
                "Note that EMT-based control, while selecting an action, creates a preference vector over the set of actions based on their predicted performance with respect to a given target.",
                "If these preference vectors are normalized, they can be combined into a single unified preference.",
                "This requires replacement of standard EMT-based action selection by the algorithm below [15]: • Given: - a set of target dynamics {qk}K k=1, - vector of weights w(k) • Select action as follows - For each action a ∈ A predict the future state distribution ¯pa t+1 = Ta ∗ pt; - For each action, compute Da = H(¯pa t+1, pt, PDt) - For each a ∈ A and qk tactical target, denote V (a, k) = DKL (Da qk) pt .",
                "Let Vk(a) = 1 Zk V (a, k), where Zk = a∈A V (a, k) is a normalization factor. - Select a∗ = arg min a k k=1 w(k)Vk(a) The weights vector w = (w1, ..., wK ) allows the additional tuning of importance among target dynamics without the need to redesign the targets themselves.",
                "This balancing method is also seamlessly integrated into the EMT-based control flow of operation. 4.2 EMT-based Control Limitations EMT-based control is a sub-optimal (in the DBC sense) representative of the DBC structure.",
                "It limits the User by forcing EMT to be its dynamic tracking algorithm, and replaces Agent optimization by greedy action selection.",
                "This kind of combination, however, is common for on-line algorithms.",
                "Although further development of EMT-based controllers is necessary, evidence so far suggests that even the simplest form of the algorithm possesses a great deal of power, and displays trends that are optimal in the DBC sense of the word.",
                "There are two further, EMT-specific, limitations to EMT-based control that are evident at this point.",
                "Both already have partial solutions and are subjects of ongoing research.",
                "The first limitation is the problem of negative preference.",
                "In the POMDP framework for example, this is captured simply, through The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 793 the appearance of values with different signs within the reward structure.",
                "For EMT-based control, however, negative preference means that one would like to avoid a certain distribution over system development sequences; EMT-based control, however, concentrates on getting as close as possible to a distribution.",
                "Avoidance is thus unnatural in native EMT-based control.",
                "The second limitation comes from the fact that standard environment modeling can create pure sensory actions-actions that do not change the state of the world, and differ only in the way observations are received and the quality of observations received.",
                "Since the world state does not change, EMT-based control would not be able to differentiate between different sensory actions.",
                "Notice that both of these limitations of EMT-based control are absent from the general DBC framework, since it may have a tracking algorithm capable of considering pure sensory actions and, unlike Kullback-Leibler divergence, a distribution deviation measure that is capable of dealing with negative preference. 5.",
                "EMT PLAYING TAG The Game of Tag was first introduced in [11].",
                "It is a single agent problem of capturing a quarry, and belongs to the class of area sweeping problems.",
                "An example domain is shown in Figure 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figure 2: Tag domain; an agent (A) attempts to seek and capture a quarry (Q) The Game of Tag extremely limits the agents perception, so that the agent is able to detect the quarry only if they are co-located in the same cell of the grid world.",
                "In the classical version of the game, co-location leads to a special observation, and the Tag action can be performed.",
                "We slightly modify this setting: the moment that both agents occupy the same cell, the game ends.",
                "As a result, both the agent and its quarry have the same motion capability, which allows them to move in four directions, North, South, East, and West.",
                "These form a formal space of actions within a Markovian environment.",
                "The state space of the formal Markovian environment is described by the cross-product of the agent and quarrys positions.",
                "For Figure 2, it would be S = {s0, ..., s23} × {s0, ..., s23}.",
                "The effects of an action taken by the agent are deterministic, but the environment in general has a stochastic response due to the motion of the quarry.",
                "With probability q0 1 it stays put, and with probability 1 − q0 it moves to an adjacent cell further away from the 1 In our experiments this was taken to be q0 = 0.2. agent.",
                "So for the instance shown in Figure 2 and q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Although the evasive behavior of the quarry is known to the agent, the quarrys position is not.",
                "The only sensory information available to the agent is its own location.",
                "We use EMT and directly specify the target dynamics.",
                "For the Game of Tag, we can easily formulate three major trends: catching the quarry, staying mobile, and stalking the quarry.",
                "This results in the following three target dynamics: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 otherwise Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 otherwise Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Note that none of the above targets are directly achievable; for instance, if Qt = s9 and At = s11, there is no action that can move the agent to At+1 = s9 as required by the Tcatch target dynamics.",
                "We ran several experiments to evaluate EMT performance in the Tag Game.",
                "Three configurations of the domain shown in Figure 3 were used, each posing a different challenge to the agent due to partial observability.",
                "In each setting, a set of 1000 runs was performed with a time limit of 100 steps.",
                "In every run, the initial position of both the agent and its quarry was selected at random; this means that as far as the agent was concerned, the quarrys initial position was uniformly distributed over the entire domain cell space.",
                "We also used two variations of the environment observability function.",
                "In the first version, observability function mapped all joint positions of hunter and quarry into the position of the hunter as an observation.",
                "In the second, only those joint positions in which hunter and quarry occupied different locations were mapped into the hunters location.",
                "The second version in fact utilized and expressed the fact that once hunter and quarry occupy the same cell the game ends.",
                "The results of these experiments are shown in Table 2.",
                "Balancing [15] the catch, move, and stalk target dynamics described in the previous section by the weight vector [0.8, 0.1, 0.1], EMT produced stable performance in all three domains.",
                "Although direct comparisons are difficult to make, the EMT performance displayed notable efficiency vis-`a-vis the POMDP approach.",
                "In spite of a simple and inefficient Matlab implementation of the EMT algorithm, the decision time for any given step averaged significantly below 1 second in all experiments.",
                "For the irregular open arena domain, which proved to be the most difficult, 1000 experiment runs bounded by 100 steps each, a total of 42411 steps, were completed in slightly under 6 hours.",
                "That is, over 4 × 104 online steps took an order of magnitude less time than the offline computation of POMDP policy in [11].",
                "The significance of this differential is made even more prominent by the fact that, should the environment model parameters change, the online nature of EMT would allow it to maintain its performance time, while the POMDP policy would need to be recomputed, requiring yet again a large overhead of computation.",
                "We also tested the behavior cell frequency entropy, empirical measures from trial data.",
                "As Figure 4 and Figure 5 show, empir794 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) A Q Q A 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 A Q Figure 3: These configurations of the Tag Game space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor Table 2: Performance of the EMT-based solution in three Tag Game domains and two observability models: I) omniposition quarry, II) quarry is not at hunters position Model Domain Capture% E(Steps) Time/Step I Dead-ends 100 14.8 72(mSec) Arena 80.2 42.4 500(mSec) Circle 91.4 34.6 187(mSec) II Dead-ends 100 13.2 91(mSec) Arena 96.8 28.67 396(mSec) Circle 94.4 31.63 204(mSec) ical entropy grows with the length of interaction.",
                "For runs where the quarry was not captured immediately, the entropy reaches between 0.85 and 0.952 for different runs and scenarios.",
                "As the agent actively seeks the quarry, the entropy never reaches its maximum.",
                "One characteristic of the entropy graph for the open arena scenario particularly caught our attention in the case of the omniposition quarry observation model.",
                "Near the maximum limit of trial length (100 steps), entropy suddenly dropped.",
                "Further analysis of the data showed that under certain circumstances, a fluctuating behavior occurs in which the agent faces equally viable versions of quarry-following behavior.",
                "Since the EMT algorithm has greedy action selection, and the state space does not encode any form of commitment (not even speed or acceleration), the agent is locked within a small portion of cells.",
                "It is essentially attempting to simultaneously follow several courses of action, all of which are consistent with the target dynamics.",
                "This behavior did not occur in our second observation model, since it significantly reduced the set of eligible courses of action-essentially contributing to tie-breaking among them. 6.",
                "DISCUSSION The design of the EMT solution for the Tag Game exposes the core difference in approach to planning and control between EMT or DBC, on the one hand, and the more familiar POMDP approach, on the other.",
                "POMDP defines a reward structure to optimize, and influences system dynamics indirectly through that optimization.",
                "EMT discards any reward scheme, and instead measures and influences system dynamics directly. 2 Entropy was calculated using log base equal to the number of possible locations within the domain; this properly scales entropy expression into the range [0, 1] for all domains.",
                "Thus for the Tag Game, we did not search for a reward function that would encode and express our preference over the agents behavior, but rather directly set three (heuristic) behavior preferences as the basis for target dynamics to be maintained.",
                "Experimental data shows that these targets need not be directly achievable via the agents actions.",
                "However, the ratio between EMT performance and achievability of target dynamics remains to be explored.",
                "The tag game experiment data also revealed the different emphasis DBC and POMDPs place on the formulation of an environment state space.",
                "POMDPs rely entirely on the mechanism of reward accumulation maximization, i.e., formation of the action selection procedure to achieve necessary state sequencing.",
                "DBC, on the other hand, has two sources of sequencing specification: through the properties of an action selection procedure, and through direct specification within the target dynamics.",
                "The importance of the second source was underlined by the Tag Game experiment data, in which the greedy EMT algorithm, applied to a POMDP-type state space specification, failed, since target description over such a state space was incapable of encoding the necessary behavior tendencies, e.g., tie-breaking and commitment to directed motion.",
                "The structural differences between DBC (and EMT in particular), and POMDPs, prohibits direct performance comparison, and places them on complementary tracks, each within a suitable niche.",
                "For instance, POMDPs could be seen as a much more natural formulation of economic sequential decision-making problems, while EMT is a better fit for continual demand for stochastic change, as happens in many robotic or embodied-agent problems.",
                "The complementary properties of POMDPs and EMT can be further exploited.",
                "There is recent interest in using POMDPs in hybrid solutions [17], in which the POMDPs can be used together with other control approaches to provide results not easily achievable with either approach by itself.",
                "DBC can be an effective partner in such a hybrid solution.",
                "For instance, POMDPs have prohibitively large off-line time requirements for policy computation, but can be readily used in simpler settings to expose beneficial behavioral trends; this can serve as a form of target dynamics that are provided to EMT in a larger domain for on-line operation. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have presented a novel perspective on the process of planning and control in stochastic environments, in the form of the Dynamics Based Control (DBC) framework.",
                "DBC formulates the task of planning as support of a specified target system dynamics, which describes the necessary properties of change within the environment.",
                "Optimality of DBC plans of action are measured The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 4: Observation Model I: Omniposition quarry.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 5: Observation Model II: quarry not observed at hunters position.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. with respect to the deviation of actual system dynamics from the target dynamics.",
                "We show that a recently developed technique of Extended Markov Tracking (EMT) [13] is an instantiation of DBC.",
                "In fact, EMT can be seen as a specific case of DBC parameterization, which employs a greedy action selection procedure.",
                "Since EMT exhibits the key features of the general DBC framework, as well as polynomial time complexity, we used the multitarget version of EMT [15] to demonstrate that the class of area sweeping problems naturally lends itself to dynamics-based descriptions, as instantiated by our experiments in the Tag Game domain.",
                "As enumerated in Section 4.2, EMT has a number of limitations, such as difficulty in dealing with negative dynamic preference.",
                "This prevents direct application of EMT to such problems as Rendezvous-Evasion Games (e.g., [6]).",
                "However, DBC in general has no such limitations, and readily enables the formulation of evasion games.",
                "In future work, we intend to proceed with the development of dynamics-based controllers for these problems. 8.",
                "ACKNOWLEDGMENT The work of the first two authors was partially supported by Israel Science Foundation grant #898/05, and the third author was partially supported by a grant from Israels Ministry of Science and Technology. 9.",
                "REFERENCES [1] R. C. Arkin.",
                "Behavior-Based Robotics.",
                "MIT Press, 1998. [2] J.",
                "A. Bilmes.",
                "A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and Hidden Markov Models.",
                "Technical Report TR-97-021, Department of Electrical Engeineering and Computer Science, University of California at Berkeley, 1998. [3] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of information theory.",
                "Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz, and M. J. Wolverton.",
                "A survey of research in distributed, continual planning.",
                "AI Magazine, 4:13-22, 1999. [5] V. R. Konda and J. N. Tsitsiklis.",
                "Actor-Critic algorithms.",
                "SIAM Journal on Control and Optimization, 42(4):1143-1166, 2003. [6] W. S. Lim.",
                "A rendezvous-evasion game on discrete locations with joint randomization.",
                "Advances in Applied Probability, 29(4):1004-1017, December 1997. [7] M. L. Littman, T. L. Dean, and L. P. Kaelbling.",
                "On the complexity of solving Markov decision problems.",
                "In Proceedings of the 11th Annual Conference on Uncertainty in Artificial Intelligence (UAI-95), pages 394-402, 1995. [8] O. Madani, S. Hanks, and A. Condon.",
                "On the undecidability of probabilistic planning and related stochastic optimization problems.",
                "Artificial Intelligence Journal, 147(1-2):5-34, July 2003. [9] R. M. Neal and G. E. Hinton.",
                "A view of the EM algorithm 796 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) that justifies incremental, sparse, and other variants.",
                "In M. I. Jordan, editor, Learning in Graphical Models, pages 355-368.",
                "Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In Proceeding of AAMAS 2006, 2006. [11] J. Pineau, G. Gordon, and S. Thrun.",
                "Point-based value iteration: An anytime algorithm for pomdps.",
                "In International Joint Conference on Artificial Intelligence (IJCAI), pages 1025-1032, August 2003. [12] M. L. Puterman.",
                "Markov Decision Processes.",
                "Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics Section.",
                "Wiley-Interscience Publication, New York, 1994. [13] Z. Rabinovich and J. S. Rosenschein.",
                "Extended Markov Tracking with an application to control.",
                "In The Workshop on Agent Tracking: Modeling Other Agents from Observations, at the Third International Joint Conference on Autonomous Agents and Multiagent Systems, pages 95-100, New-York, July 2004. [14] Z. Rabinovich and J. S. Rosenschein.",
                "Multiagent coordination by Extended Markov Tracking.",
                "In The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 431-438, Utrecht, The Netherlands, July 2005. [15] Z. Rabinovich and J. S. Rosenschein.",
                "On the response of EMT-based control to interacting targets and models.",
                "In The Fifth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 465-470, Hakodate, Japan, May 2006. [16] R. F. Stengel.",
                "Optimal Control and Estimation.",
                "Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, and P. Varakantham.",
                "Conflicts in teamwork: Hybrids to the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 797"
            ],
            "original_annotated_samples": [
                "We exploit this efficiency in a set of experiments that applied multitarget EMT to a class of <br>area-sweeping problem</br>s (searching for moving targets).",
                "In Section 2 we motivate DBC using <br>area-sweeping problem</br>s, and discuss related work."
            ],
            "translated_annotated_samples": [
                "Explotamos esta eficiencia en un conjunto de experimentos que aplicaron EMT multitarget a una clase de <br>problemas de barrido de área</br> (búsqueda de objetivos en movimiento).",
                "En la Sección 2 motivamos DBC utilizando <br>problemas de barrido de área</br>, y discutimos trabajos relacionados."
            ],
            "translated_text": "En este artículo presentamos el Control Basado en Dinámicas (DBC), un enfoque para la planificación y control de un agente en entornos estocásticos. A diferencia de los enfoques existentes, que buscan optimizar las recompensas esperadas (por ejemplo, en Problemas de Decisión de Markov Parcialmente Observables (POMDP)), DBC optimiza el comportamiento del sistema hacia las dinámicas del sistema especificadas. Mostramos que un enfoque de planificación y control recientemente desarrollado, Seguimiento Extendido de Markov (EMT), es una instancia de DBC. EMT emplea la selección de acciones codiciosa para proporcionar un algoritmo de control eficiente en entornos markovianos. Explotamos esta eficiencia en un conjunto de experimentos que aplicaron EMT multitarget a una clase de <br>problemas de barrido de área</br> (búsqueda de objetivos en movimiento). Mostramos que tales problemas pueden ser definidos de forma natural y resueltos eficientemente utilizando el marco de DBC y su instanciación de EMT. Categorías y Descriptores de Asignaturas I.2.8 [Resolución de Problemas, Métodos de Control y Búsqueda]: Teoría de Control; I.2.9 [Robótica]; I.2.11 [Inteligencia Artificial Distribuida]: Agentes Inteligentes Términos Generales Algoritmos, Teoría 1. INTRODUCCIÓN La planificación y el control constituyen un área de investigación central en sistemas multiagentes e inteligencia artificial. En los últimos años, los Procesos de Decisión de Markov Parcialmente Observables (POMDPs) [12] se han convertido en una base formal popular para la planificación en entornos estocásticos. En este marco, el problema de planificación y control suele abordarse imponiendo una función de recompensa y calculando una política (de elección de acciones) que sea óptima, en el sentido de que resultará en la mayor utilidad esperada. Si bien teóricamente atractivo, la complejidad de resolver óptimamente un POMDP es prohibitiva [8, 7]. Tomamos una visión alternativa de la planificación en entornos estocásticos. No utilizamos una función de recompensa basada en el estado, sino que optimizamos según un criterio diferente, una especificación basada en transiciones de la dinámica del sistema deseado. La idea aquí es ver la ejecución del plan como un proceso que obliga a un sistema (estocástico) a cambiar, y un plan como un proceso dinámico que moldea ese cambio de acuerdo con criterios deseados. Llamamos a este marco de planificación general Control Basado en Dinámicas (DBC). En DBC, el objetivo de un proceso de planificación (o control) es garantizar que el sistema cambiará de acuerdo con dinámicas objetivo específicas (potencialmente estocásticas). Dado que el comportamiento real del sistema puede desviarse de lo especificado por la dinámica objetivo (debido a la naturaleza estocástica del sistema), la planificación en tales entornos debe ser continua, de manera similar a los controladores en lazo cerrado clásicos. Aquí, la optimalidad se mide en términos de la probabilidad de magnitudes de desviación. En este documento, presentamos la estructura del Control Basado en Dinámicas. Mostramos que el enfoque de Seguimiento Extendido de Markov (EMT) recientemente desarrollado [13, 14, 15] está subsumido por DBC, con EMT empleando selección de acciones codiciosa, que es una parametrización específica entre las opciones posibles dentro de DBC. EMT es una implementación eficiente de DBC. Para evaluar DBC, llevamos a cabo un conjunto de experimentos aplicando EMT de múltiples objetivos al Juego de Etiquetas [11]; esta es una variante del problema de barrido de área, donde un agente intenta etiquetar a un objetivo móvil (presa) cuya posición no se conoce con certeza. Los datos experimentales demuestran que incluso con un modelo simple del entorno y un diseño simple de la dinámica del objetivo, se pueden lograr altas tasas de éxito tanto en la captura de la presa como en sorprender a la presa (como se expresa por la entropía observada de la posición de los agentes controlados). El documento está organizado de la siguiente manera. En la Sección 2 motivamos DBC utilizando <br>problemas de barrido de área</br>, y discutimos trabajos relacionados. La Sección 3 introduce la estructura de Control Basado en Dinámicas (DBC) y su especialización en entornos Markovianos. Esto es seguido por una revisión del enfoque de Seguimiento Markov Extendido (EMT) como un régimen de control estructurado por DBC en la Sección 4. Esa sección también discute las limitaciones del control basado en EMT en relación con el marco general de DBC. Los ajustes experimentales y los resultados se presentan a continuación en la Sección 5. La Sección 6 proporciona una breve discusión del enfoque general, y la Sección 7 ofrece algunas observaciones finales y direcciones para trabajos futuros. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2. MOTIVACIÓN Y TRABAJO RELACIONADO Muchos escenarios de la vida real tienen naturalmente una especificación de dinámica de destino estocástica, especialmente en aquellos dominios donde no existe un objetivo final, sino más bien un comportamiento del sistema (con propiedades específicas) que debe ser continuamente respaldado. Por ejemplo, los guardias de seguridad realizan barridos persistentes de un área para detectar cualquier signo de intrusión. Los ladrones astutos intentarán rastrear estos barridos y sincronizar su operación con puntos clave del movimiento de los guardias. Por lo tanto, es recomendable que la dinámica de movimiento de los guardias parezca irregular y aleatoria. El trabajo reciente de Paruchuri et al. [10] ha abordado dicha aleatorización en el contexto de POMDPs de agente único y distribuidos. El objetivo de ese trabajo era generar políticas que proporcionen una medida de aleatorización en la selección de acciones, manteniendo al mismo tiempo las recompensas dentro de niveles aceptables. Nuestro enfoque difiere de este trabajo en que DBC no optimiza recompensas esperadas, de hecho, no consideramos recompensas en absoluto, sino que en su lugar mantenemos dinámicas deseadas (incluyendo, pero no limitado a, la aleatorización). El juego de la mancha es otro ejemplo de la aplicabilidad del enfoque. Fue introducido en el trabajo de Pineau et al. [11]. Hay dos agentes que pueden moverse por un área, la cual está dividida en una cuadrícula. La cuadrícula puede tener celdas bloqueadas (agujeros) en las cuales ningún agente puede moverse. Un agente (el cazador) intenta moverse a una celda ocupada por el otro (la presa), de modo que estén ubicados en el mismo lugar (esto es una etiqueta exitosa). La presa busca evitar al cazador, siempre está al tanto de la posición del cazador, pero no sabe cómo se comportará el cazador, lo que abre la posibilidad de que un cazador sorprenda a la presa. El cazador conoce la ley de movimiento probabilístico de la presa, pero no conoce su ubicación actual. Tag es una instancia de una familia de problemas de barrido de área (persecución-evasión). En [11], el cazador modeló el problema utilizando un POMDP. Se definió una función de recompensa para reflejar el deseo de marcar la presa, y se calculó una política de acción para optimizar la recompensa recolectada con el tiempo. Debido a la complejidad intratable de determinar la política óptima, la política de acción calculada en ese documento fue esencialmente una aproximación. En este artículo, en lugar de formular una función de recompensa, utilizamos EMT para resolver el problema, especificando directamente la dinámica objetivo. De hecho, cualquier problema de búsqueda con movimiento aleatorio, la llamada clase de problemas de barrido de área, puede ser descrito a través de la especificación de la dinámica de dicho sistema objetivo. El Control Basado en Dinámicas proporciona un enfoque natural para resolver estos problemas. CONTROL BASADO EN DINÁMICA La especificación del Control Basado en Dinámica (DBC) se puede dividir en tres niveles interactivos: Nivel de Diseño del Entorno, Nivel de Usuario y Nivel de Agente. • El Nivel de Diseño del Entorno se ocupa de la especificación formal y modelado del entorno. Por ejemplo, este nivel especificaría las leyes de la física dentro del sistema y establecería sus parámetros, como la constante de gravitación. El Nivel de Usuario, a su vez, se basa en el modelo de entorno producido por el Diseño de Entorno para especificar la dinámica del sistema objetivo que desea observar. El Nivel de Usuario también especifica el procedimiento de estimación o aprendizaje para la dinámica del sistema, y la medida de desviación. En el escenario del guardia de museo mencionado anteriormente, estos corresponderían a un horario de barrido estocástico y una medida de sorpresa relativa entre el barrido especificado y el real. • El Nivel de Agente, a su vez, combina el modelo del entorno del Nivel de Diseño de Entorno, el procedimiento de estimación de la dinámica, la medida de desviación y la especificación de la dinámica objetivo del Nivel de Usuario, para producir una secuencia de acciones que creen dinámicas del sistema lo más cercanas posible a la especificación deseada. Dado nuestro interés en el desarrollo continuo de un sistema estocástico, como ocurre en la teoría de control clásica [16] y la planificación continua [4], así como en nuestro ejemplo de recorridos por museos, la pregunta es cómo el Nivel de Agente debe tratar las mediciones de desviación a lo largo del tiempo. Con este fin, utilizamos un umbral de probabilidad, es decir, nos gustaría que el Nivel del Agente maximice la probabilidad de que la medida de desviación permanezca por debajo de cierto umbral. La selección de acciones específicas depende entonces de la formalización del sistema. Una posibilidad sería crear una mezcla de tendencias de sistemas disponibles, similar a lo que sucede en las arquitecturas robóticas basadas en el comportamiento [1]. La otra alternativa sería depender del procedimiento de estimación proporcionado por el Nivel de Usuario para utilizar el modelo del Nivel de Diseño del Entorno del entorno para elegir acciones, de manera que se manipule al estimador de dinámicas para que crea que se ha logrado cierta dinámica. Ten en cuenta que esta manipulación no es directa, sino a través del entorno. Por lo tanto, para algoritmos de estimación lo suficientemente fuertes, la manipulación exitosa significaría una simulación exitosa de la dinámica objetivo especificada (es decir, más allá de discernir a través de la entrada sensorial disponible). Los niveles de DBC también pueden tener un retroceso de información (ver Figura 1). Por ejemplo, el Nivel de Agente podría proporcionar datos sobre la viabilidad de la dinámica del objetivo, permitiendo al Nivel de Usuario modificar el requisito, quizás centrándose en las características alcanzables del comportamiento del sistema. Los datos también estarían disponibles sobre la respuesta del sistema a diferentes acciones realizadas; combinados con un estimador de dinámica definido por el Nivel de Usuario, esto puede proporcionar una herramienta importante para la calibración del modelo del entorno en el Nivel de Diseño del Entorno. UserEnv. El modelo de agente de diseño Estimador de Dinámicas Ideales Estimador de Dinámicas Factibilidad de Respuesta del Sistema de Datos Figura 1: Flujo de datos del marco de trabajo DBC Ampliando la idea de los algoritmos Actor-Crítico [5], el flujo de datos de DBC puede proporcionar una buena base para el diseño de un algoritmo de aprendizaje. Por ejemplo, el Nivel de Usuario puede funcionar como un dispositivo exploratorio para un algoritmo de aprendizaje, inferir un objetivo dinámico ideal a partir del modelo del entorno en cuestión que expondría y verificaría las características más críticas del comportamiento del sistema. En este caso, los datos de viabilidad y respuesta del sistema a nivel de agente proporcionarían información clave para una actualización del modelo del entorno. De hecho, la combinación de datos de viabilidad y respuesta puede proporcionar una base para la aplicación de algoritmos de aprendizaje sólidos como EM [2, 9]. 3.1 DBC para entornos markovianos Para un entorno markoviano parcialmente observable, DBC puede especificarse de una manera más rigurosa. Observa cómo DBC descarta las recompensas y las reemplaza por otro criterio de optimalidad (las diferencias estructurales se resumen en la Tabla 1): • El nivel de Diseño del Entorno es especificar una tupla < S, A, T, O, Ω, s0 >, donde: - S es el conjunto de todos los posibles estados del entorno; - s0 es el estado inicial del entorno (que también puede ser visto como una distribución de probabilidad sobre S); La Sexta Conferencia Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 791 - A es el conjunto de todas las acciones posibles aplicables en el entorno; - T es la función de transición probabilística del entorno: T: S × A → Π(S). Es decir, T(s | a, s) es la probabilidad de que el entorno se mueva del estado s al estado s bajo la acción a; - O es el conjunto de todas las observaciones posibles. Así es como se vería la entrada del sensor para un observador externo; - Ω es la función de probabilidad de observación: Ω: S × A × S → Π(O). Es decir, Ω(o|s, a, s) es la probabilidad de observar o dado que el entorno ha pasado del estado s al estado s bajo la acción a. • Nivel de Usuario, en el caso de un entorno markoviano, opera en el conjunto de dinámicas del sistema descritas por una familia de probabilidades condicionales F = {τ: S × A → Π(S)}. Por lo tanto, la especificación de la dinámica del objetivo puede expresarse como q ∈ F, y el algoritmo de aprendizaje o seguimiento puede representarse como una función L : O×(A×O)∗ → F; es decir, mapea secuencias de observaciones y acciones realizadas hasta el momento en una estimación τ ∈ F de la dinámica del sistema. Hay muchas variaciones posibles disponibles a nivel de usuario para definir la divergencia entre la dinámica del sistema; varias de ellas son: - Distancia de traza o distancia L1 entre dos distribuciones p y q definida por D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Medida de fidelidad de distancia F(p(·), q(·)) = x p(x)q(x) - Divergencia de Kullback-Leibler DKL(p(·) q(·)) = x p(x) log p(x) q(x) Nótese que las dos últimas no son en realidad métricas sobre el espacio de distribuciones posibles, pero tienen interpretaciones significativas e importantes. Por ejemplo, la divergencia de Kullback-Leibler es una herramienta importante de la teoría de la información [3] que permite medir el costo de codificar una fuente de información gobernada por q, asumiendo que está gobernada por p. El Nivel de Usuario también define el umbral de probabilidad de desviación de la dinámica θ. • El Nivel de Agente se enfrenta entonces a un problema de seleccionar una función de señal de control a∗ para satisfacer un problema de minimización de la siguiente manera: a∗ = arg min a Pr(d(τa, q) > θ) donde d(τa, q) es una variable aleatoria que describe la desviación de la estimación de la dinámica τa, creada por L bajo la señal de control a, de la dinámica ideal q. Implícito en este problema de minimización es que L es manipulado a través del entorno, basado en el modelo de entorno producido por el Nivel de Diseño del Entorno. 3.2 Vista DBC del Espacio de Estados Es importante tener en cuenta la visión complementaria que DBC y POMDPs tienen sobre el espacio de estados del entorno. Los POMDP consideran el estado como una instantánea estacionaria del entorno; cualquier atributo de la secuencia de estados que se busque se alcanza a través de las propiedades del proceso de control, en este caso la acumulación de recompensas. Esto se puede ver como si la secuencia de estados y los atributos de esa secuencia solo fueran introducidos por y para el mecanismo de control, la política POMDP. DBC se enfoca en el principio subyacente de la secuenciación de estados, la dinámica del sistema. La especificación de la dinámica de los DBCs puede utilizar el espacio de estado del entorno como un medio para describir, discernir y preservar los cambios que ocurren dentro del sistema. Como resultado, DBC tiene una mayor capacidad para expresar propiedades de secuenciación de estados, las cuales están fundamentadas en el modelo del entorno y su definición del espacio de estados. Por ejemplo, considera la tarea de moverse a través de terreno accidentado hacia un objetivo y alcanzarlo lo más rápido posible. Los POMDPs codificarían el terreno como puntos en el espacio de estados, mientras que la velocidad se garantizaría mediante una recompensa negativa por cada paso dado sin alcanzar la meta. Acumular una recompensa más alta solo se puede lograr mediante un movimiento más rápido. Alternativamente, el espacio de estados podría incluir directamente la noción de velocidad. Para los POMDPs, esto significaría que el mismo concepto está codificado dos veces, en cierto sentido: directamente en el espacio de estados e indirectamente en la acumulación de recompensas. Ahora, incluso si la función de recompensa codificara más y detalles más finos de las propiedades del movimiento, la solución POMDP tendría que buscar en un espacio mucho más grande de políticas, aunque aún esté guiada por el concepto implícito del procedimiento de acumulación de recompensas. Por otro lado, la expresión del objetivo táctico de variaciones y correlaciones entre la posición y la velocidad del movimiento ahora se basa en la representación del espacio de estados. En esta situación, cualquier restricción adicional, como la suavidad del movimiento, los límites de velocidad en diferentes ubicaciones o las reducciones de velocidad durante giros bruscos, son expresadas explícita y uniformemente por el objetivo táctico, y pueden resultar en una selección de acciones más rápida y efectiva por un algoritmo DBC. 4. El Control Basado en EMT como un DBC Recientemente, se introdujo un algoritmo de control llamado Control Basado en EMT [13], que instancia el marco de trabajo DBC. Aunque proporciona una solución ávida aproximada en el sentido de DBC, los experimentos iniciales utilizando el control basado en EMT han sido alentadores [14, 15]. El control basado en EMT se basa en la definición del entorno markoviano, como en el caso de los POMDP, pero sus Niveles de Usuario y Agente son del tipo de optimalidad DBC markoviana. • El Nivel de Usuario del control basado en EMT define una dinámica del sistema objetivo en un caso limitado independiente de la acción: qEMT: S → Π(S). Luego utiliza la medida de divergencia de Kullback-Leibler para componer un estimador de dinámica de sistemas momentáneos: el algoritmo de Seguimiento Extendido de Markov (EMT). El algoritmo mantiene una estimación de la dinámica del sistema τt EMT que es capaz de explicar el cambio reciente en un estimador de estado auxiliar bayesiano de pt−1 a pt, y lo actualiza de manera conservadora utilizando la divergencia de Kullback-Leibler. Dado que τt EMT y pt−1,t son respectivamente las probabilidades condicionales y marginales sobre el espacio de estados del sistema, la explicación simplemente significa que pt(s) = s τt EMT (s | s)pt−1(s), y la actualización de la estimación de la dinámica se realiza resolviendo un 792 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Tabla 1: Estructura de POMDP vs. Control basado en Dinámicas en Entorno Markoviano Nivel Enfoque MDP Markoviano DBC Entorno < S, A, T, O, Ω >, donde S - conjunto de estados A - conjunto de acciones Diseño T : S × A → Π(S) - transición O - conjunto de observaciones Ω : S × A × S → Π(O) Usuario r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - función de recompensa q - dinámicas ideales F - remodelación de recompensa L - estimador de dinámicas θ - umbral Agente π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) problema de minimización: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • El nivel de Agente en el control basado en EMT es subóptimo con respecto a DBC (aunque permanece dentro del marco de DBC), realizando selección de acciones codiciosas basadas en la predicción de la reacción de EMT. La predicción se basa en el modelo del entorno proporcionado por el nivel de Diseño del Entorno, de modo que si denotamos por Ta la función de transición de entornos limitada a la acción a, y pt−1 es el estimador de estado auxiliar del sistema bayesiano, entonces la elección de control basada en EMT se describe por a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1). Nótese que esto sigue precisamente el marco DBC Markoviano: la optimalidad recompensante de los POMDPs se descarta, y en su lugar se manipula un estimador de dinámicas (EMT en este caso) a través de los efectos de la acción en el entorno para producir una estimación cercana a la dinámica del sistema objetivo especificado. Sin embargo, como mencionamos, el control EMT ingenuo es subóptimo en el sentido de DBC y tiene varias limitaciones adicionales que no existen en el marco general de DBC (discutido en la Sección 4.2). 4.1 EMT de múltiples objetivos. En ocasiones, puede haber varias preferencias de comportamiento. Por ejemplo, en el caso de los guardias de museo, algunos objetos de arte están más fuertemente custodiados, lo que requiere que los guardias permanezcan más a menudo en su cercanía. Por otro lado, no se debe dejar ningún rincón del museo sin revisar, lo que requiere un movimiento constante. El éxito de la seguridad en un museo requeriría que los guardias se adhieran y equilibren ambos comportamientos. Para el control basado en EMT, esto significaría enfrentar varios objetivos tácticos {qk}K k=1, y la pregunta sería cómo fusionar y equilibrarlos. Un mecanismo de equilibrio se puede aplicar para resolver este problema. Ten en cuenta que el control basado en EMT, al seleccionar una acción, crea un vector de preferencia sobre el conjunto de acciones basado en su rendimiento predicho con respecto a un objetivo dado. Si estos vectores de preferencia están normalizados, pueden combinarse en una única preferencia unificada. Esto requiere reemplazar la selección de acciones basada en EMT estándar por el algoritmo a continuación [15]: • Dado: - un conjunto de dinámicas objetivo {qk}K k=1, - vector de pesos w(k) • Seleccionar la acción de la siguiente manera - Para cada acción a ∈ A predecir la distribución de estado futuro ¯pa t+1 = Ta ∗ pt; - Para cada acción, calcular Da = H(¯pa t+1, pt, PDt) - Para cada a ∈ A y objetivo táctico qk, denotar V (a, k) = DKL (Da qk) pt. Sea Vk(a) = 1 Zk V (a, k), donde Zk = a∈A V (a, k) es un factor de normalización. - Selecciona a∗ = arg min a k k=1 w(k)Vk(a) El vector de pesos w = (w1, ..., wK ) permite ajustar la importancia entre las dinámicas objetivo sin necesidad de rediseñar los objetivos mismos. Este método de equilibrio también está integrado de forma transparente en el flujo de operación basado en EMT. Limitaciones del Control Basado en EMT El control basado en EMT es una representación subóptima (en el sentido de DBC) de la estructura DBC. Limita al Usuario al obligar a EMT a ser su algoritmo de seguimiento dinámico, y reemplaza la optimización del Agente por la selección de acciones codiciosa. Este tipo de combinación, sin embargo, es común en algoritmos en línea. Aunque es necesario un mayor desarrollo de los controladores basados en EMT, la evidencia hasta ahora sugiere que incluso la forma más simple del algoritmo posee una gran cantidad de potencia y muestra tendencias que son óptimas en el sentido de DBC. Hay dos limitaciones adicionales específicas de los técnicos en emergencias médicas (EMT, por sus siglas en inglés) al control basado en EMT que son evidentes en este momento. Ambos ya tienen soluciones parciales y son objeto de investigación continua. La primera limitación es el problema de la preferencia negativa. En el marco de POMDP, por ejemplo, esto se captura de manera sencilla, a través de The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 793 la aparición de valores con signos diferentes dentro de la estructura de recompensa. Para el control basado en EMT, sin embargo, la preferencia negativa significa que uno quisiera evitar cierta distribución sobre las secuencias de desarrollo del sistema; sin embargo, el control basado en EMT se concentra en acercarse lo más posible a una distribución. La evitación es, por lo tanto, antinatural en el control basado en EMT nativo. La segunda limitación proviene del hecho de que el modelado del entorno estándar puede crear acciones sensoriales puras, acciones que no cambian el estado del mundo y solo difieren en la forma en que se reciben las observaciones y la calidad de las observaciones recibidas. Dado que el estado del mundo no cambia, el control basado en EMT no sería capaz de diferenciar entre diferentes acciones sensoriales. Ten en cuenta que ambas limitaciones del control basado en EMT están ausentes en el marco general de DBC, ya que puede tener un algoritmo de seguimiento capaz de considerar acciones sensoriales puras y, a diferencia de la divergencia de Kullback-Leibler, una medida de desviación de distribución capaz de manejar preferencias negativas. 5. El juego de la \"carrera de relevos\" fue introducido por primera vez en [11]. Es un problema de agente único de capturar una presa, y pertenece a la clase de problemas de barrido de área. Un ejemplo de dominio se muestra en la Figura 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figura 2: Dominio de etiquetas; un agente (A) intenta buscar y capturar una presa (Q). El juego de etiquetas limita extremadamente la percepción de los agentes, de modo que el agente solo puede detectar la presa si están ubicados en la misma celda del mundo de cuadrícula. En la versión clásica del juego, la co-locación conduce a una observación especial, y la acción de Etiqueta se puede realizar. Modificamos ligeramente esta configuración: el momento en que ambos agentes ocupan la misma celda, el juego termina. Como resultado, tanto el agente como su presa tienen la misma capacidad de movimiento, lo que les permite moverse en cuatro direcciones, Norte, Sur, Este y Oeste. Estos forman un espacio formal de acciones dentro de un entorno markoviano. El espacio de estados del entorno formal markoviano está descrito por el producto cruz de las posiciones del agente y la presa. Para la Figura 2, sería S = {s0, ..., s23} × {s0, ..., s23}. Los efectos de una acción tomada por el agente son deterministas, pero el entorno en general tiene una respuesta estocástica debido al movimiento de la presa. Con probabilidad q0 1 se queda en su lugar, y con probabilidad 1 − q0 se mueve a una celda adyacente más lejos del agente 1. En nuestros experimentos, esto se tomó como q0 = 0.2. Por lo tanto, para la instancia mostrada en la Figura 2 y q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Aunque el comportamiento evasivo de la presa es conocido por el agente, la posición de la presa no lo es. La única información sensorial disponible para el agente es su propia ubicación. Utilizamos EMT y especificamos directamente la dinámica del objetivo. Para el juego de la mancha, podemos formular fácilmente tres tendencias principales: atrapar al perseguido, mantenerse en movimiento y acechar al perseguido. Esto resulta en las siguientes tres dinámicas de objetivo: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 en otro caso Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 en otro caso Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Nótese que ninguno de los objetivos anteriores es directamente alcanzable; por ejemplo, si Qt = s9 y At = s11, no hay ninguna acción que pueda llevar al agente a At+1 = s9 como lo requiere la dinámica de objetivo Tcatch. Realizamos varios experimentos para evaluar el rendimiento de EMT en el Juego de Etiquetas. Se utilizaron tres configuraciones del dominio mostrado en la Figura 3, cada una planteando un desafío diferente para el agente debido a la observabilidad parcial. En cada escenario, se realizó un conjunto de 1000 ejecuciones con un límite de tiempo de 100 pasos. En cada ejecución, la posición inicial tanto del agente como de su presa fue seleccionada al azar; esto significa que, en lo que respecta al agente, la posición inicial de la presa estaba distribuida de forma uniforme en todo el espacio de celdas del dominio. También utilizamos dos variaciones de la función de observabilidad del entorno. En la primera versión, la función de observabilidad mapeaba todas las posiciones articulares del cazador y la presa en la posición del cazador como observación. En el segundo, solo aquellas posiciones conjuntas en las que el cazador y la presa ocupaban ubicaciones diferentes fueron mapeadas en la ubicación del cazador. La segunda versión de hecho utilizó y expresó el hecho de que una vez que el cazador y la presa ocupan la misma celda, el juego termina. Los resultados de estos experimentos se muestran en la Tabla 2. Equilibrando la dinámica de captura, movimiento y seguimiento descrita en la sección anterior mediante el vector de peso [0.8, 0.1, 0.1], EMT logró un rendimiento estable en los tres dominios. Aunque las comparaciones directas son difíciles de hacer, el rendimiento de los EMT mostró una notable eficiencia en comparación con el enfoque POMDP. A pesar de una implementación simple e ineficiente del algoritmo EMT en Matlab, el tiempo de decisión para cualquier paso dado promedió significativamente por debajo de 1 segundo en todos los experimentos. Para el dominio de la arena abierta irregular, que resultó ser el más difícil, se completaron 1000 ejecuciones de experimentos limitadas a 100 pasos cada una, un total de 42411 pasos, en poco menos de 6 horas. Es decir, más de 4 × 104 pasos en línea tomaron una orden de magnitud menos tiempo que la computación sin conexión de la política POMDP en [11]. La importancia de esta diferencia se destaca aún más por el hecho de que, en caso de que cambien los parámetros del modelo del entorno, la naturaleza en línea de EMT le permitiría mantener su rendimiento, mientras que la política POMDP necesitaría ser recalculada, lo que requeriría una vez más un gran costo computacional adicional. También probamos la entropía de la frecuencia de las células de comportamiento, medidas empíricas de los datos de prueba. Como muestran la Figura 4 y la Figura 5, empir794 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 3: Se utilizaron estas configuraciones del espacio de juego de Tag: a) múltiples callejones sin salida, b) arena abierta irregular, c) pasillo circular. Tabla 2: Rendimiento de la solución basada en EMT en tres dominios de juego de Tag y dos modelos de observabilidad: I) cantera de omniposición, II) la cantera no está en la posición de los cazadores. Modelo Dominio Captura% E(Pasos) Tiempo/Paso I Callejones sin salida 100 14.8 72(mSeg) Arena 80.2 42.4 500(mSeg) Círculo 91.4 34.6 187(mSeg) II Callejones sin salida 100 13.2 91(mSeg) Arena 96.8 28.67 396(mSeg) Círculo 94.4 31.63 204(mSeg) La entropía lógica crece con la duración de la interacción. Para las carreras en las que la presa no fue capturada de inmediato, la entropía alcanza valores entre 0.85 y 0.952 para diferentes carreras y escenarios. A medida que el agente busca activamente la presa, la entropía nunca alcanza su máximo. Una característica del gráfico de entropía para el escenario de arena abierta llamó especialmente nuestra atención en el caso del modelo de observación de cantera omniposicional. Cerca del límite máximo de longitud de prueba (100 pasos), la entropía cayó repentinamente. Un análisis adicional de los datos mostró que bajo ciertas circunstancias, se produce un comportamiento fluctuante en el que el agente se enfrenta a versiones igualmente viables del comportamiento de seguir a la presa. Dado que el algoritmo EMT tiene una selección de acciones codiciosa, y el espacio de estados no codifica ninguna forma de compromiso (ni siquiera velocidad o aceleración), el agente queda atrapado dentro de una pequeña porción de celdas. Básicamente, se intenta seguir simultáneamente varios cursos de acción, todos los cuales son consistentes con la dinámica objetivo. Este comportamiento no ocurrió en nuestro segundo modelo de observación, ya que redujo significativamente el conjunto de cursos de acción elegibles, contribuyendo esencialmente a romper el empate entre ellos. 6. DISCUSIÓN El diseño de la solución EMT para el Juego de Etiquetas expone la diferencia fundamental en el enfoque de planificación y control entre EMT o DBC, por un lado, y el enfoque más familiar de POMDP, por otro lado. POMDP define una estructura de recompensa para optimizar e influencia la dinámica del sistema de forma indirecta a través de esa optimización. EMT descarta cualquier esquema de recompensa y, en cambio, mide e influye directamente en la dinámica del sistema. Se calculó la entropía 2 utilizando el logaritmo con base igual al número de ubicaciones posibles dentro del dominio; esto escala adecuadamente la expresión de entropía en el rango [0, 1] para todos los dominios. Por lo tanto, para el Juego de Etiquetas, no buscamos una función de recompensa que codificara y expresara nuestra preferencia sobre el comportamiento de los agentes, sino que establecimos directamente tres preferencias de comportamiento (heurísticas) como base para mantener la dinámica del objetivo. Los datos experimentales muestran que estos objetivos no necesitan ser directamente alcanzables a través de las acciones de los agentes. Sin embargo, aún queda por explorar la relación entre el rendimiento de la EMT y la alcanzabilidad de la dinámica objetivo. Los datos del experimento del juego de etiquetas también revelaron la diferente importancia que DBC y POMDPs otorgan a la formulación del espacio de estados del entorno. Los POMDPs dependen enteramente del mecanismo de maximización de la acumulación de recompensas, es decir, la formación del procedimiento de selección de acciones para lograr la secuenciación de estados necesaria. DBC, por otro lado, tiene dos fuentes de especificación de secuenciación: a través de las propiedades de un procedimiento de selección de acciones y a través de una especificación directa dentro de la dinámica objetivo. La importancia de la segunda fuente fue subrayada por los datos del experimento del Juego de Etiquetas, en el que el algoritmo EMT codicioso, aplicado a una especificación de espacio de estados tipo POMDP, falló, ya que la descripción del objetivo sobre dicho espacio de estados no era capaz de codificar las tendencias de comportamiento necesarias, por ejemplo, la resolución de empates y el compromiso con el movimiento dirigido. Las diferencias estructurales entre DBC (y EMT en particular) y POMDPs impiden la comparación directa de rendimiento y los sitúan en pistas complementarias, cada uno dentro de un nicho adecuado. Por ejemplo, los POMDP podrían ser vistos como una formulación mucho más natural de problemas económicos de toma de decisiones secuenciales, mientras que EMT se ajusta mejor a la demanda continua de cambios estocásticos, como ocurre en muchos problemas de robótica o de agentes incorporados. Las propiedades complementarias de POMDPs y EMT pueden ser aún más explotadas. Existe un interés reciente en utilizar POMDPs en soluciones híbridas [17], en las cuales los POMDPs pueden ser utilizados junto con otros enfoques de control para proporcionar resultados que no son fácilmente alcanzables con ninguno de los enfoques por sí solos. DBC puede ser un socio efectivo en una solución híbrida como esta. Por ejemplo, los POMDP tienen requisitos de tiempo fuera de línea prohibitivamente grandes para el cálculo de políticas, pero pueden ser fácilmente utilizados en entornos más simples para exponer tendencias de comportamiento beneficiosas; esto puede servir como una forma de dinámica objetivo que se proporciona a EMT en un dominio más grande para su operación en línea. 7. CONCLUSIONES Y TRABAJO FUTURO En este artículo, hemos presentado una nueva perspectiva sobre el proceso de planificación y control en entornos estocásticos, en forma del marco de Control Basado en Dinámicas (DBC). DBC formula la tarea de planificar como el apoyo a la dinámica de un sistema objetivo especificado, que describe las propiedades necesarias del cambio dentro del entorno. La optimalidad de los planes de acción de DBC se mide en el Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Calles sin salida 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Círculo Figura 4: Modelo de Observación I: Cantera omniposicional. Desarrollo de entropía con longitud de juego de etiqueta para los tres escenarios experimentales: a) múltiples callejones sin salida, b) arena abierta irregular, c) pasillo circular. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Callejones sin salida 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Círculo Figura 5: Modelo de observación II: cantera no observada en la posición de los cazadores. Desarrollo de la entropía con la longitud del juego de etiquetas para los tres escenarios experimentales: a) múltiples callejones sin salida, b) arena abierta irregular, c) pasillo circular, con respecto a la desviación de la dinámica del sistema real de la dinámica objetivo. Mostramos que una técnica recientemente desarrollada de Seguimiento Markov Extendido (EMT) [13] es una instancia de DBC. De hecho, EMT puede ser visto como un caso específico de la parametrización DBC, que emplea un procedimiento de selección de acciones codicioso. Dado que EMT exhibe las características clave del marco general DBC, así como una complejidad temporal polinómica, utilizamos la versión multitarget de EMT [15] para demostrar que la clase de problemas de barrido de área se presta naturalmente a descripciones basadas en dinámicas, tal como se ejemplifica en nuestros experimentos en el dominio del Juego de Etiquetas. Como se enumera en la Sección 4.2, EMT tiene varias limitaciones, como la dificultad para manejar la preferencia dinámica negativa. Esto evita la aplicación directa de la EMT a problemas como los Juegos de Rendezvous-Evasión (por ejemplo, [6]). Sin embargo, DBC en general no tiene tales limitaciones y permite fácilmente la formulación de juegos de evasión. En trabajos futuros, tenemos la intención de continuar con el desarrollo de controladores basados en dinámica para estos problemas. 8. AGRADECIMIENTO El trabajo de los dos primeros autores fue parcialmente apoyado por la subvención #898/05 de la Fundación para la Ciencia de Israel, y el tercer autor fue parcialmente apoyado por una subvención del Ministerio de Ciencia y Tecnología de Israel. REFERENCIAS [1] R. C. Arkin. Robótica basada en el comportamiento. MIT Press, 1998. [2] J. \n\nMIT Press, 1998. [2] J. A. Bilmes. Un tutorial detallado del algoritmo EM y su aplicación en la estimación de parámetros para mezclas gaussianas y modelos ocultos de Markov. Informe técnico TR-97-021, Departamento de Ingeniería Eléctrica e Informática, Universidad de California en Berkeley, 1998. [3] T. M. Cover y J. A. Thomas. Elementos de teoría de la información. Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz y M. J. Wolverton. Una encuesta de investigación en planificación distribuida y continua. Revista de Inteligencia Artificial, 4:13-22, 1999. [5] V. R. Konda y J. N. Tsitsiklis. Algoritmos Actor-Crítico. Revista SIAM de Control y Optimización, 42(4):1143-1166, 2003. [6] W. S. Lim. Un juego de encuentro-evasión en ubicaciones discretas con aleatorización conjunta. Avances en Probabilidad Aplicada, 29(4):1004-1017, diciembre de 1997. [7] M. L. Littman, T. L. Dean y L. P. Kaelbling. Sobre la complejidad de resolver problemas de decisión de Markov. En Actas de la 11ª Conferencia Anual sobre Incertidumbre en Inteligencia Artificial (UAI-95), páginas 394-402, 1995. [8] O. Madani, S. Hanks y A. Condon. Sobre la indecidibilidad de la planificación probabilística y problemas relacionados de optimización estocástica. Revista de Inteligencia Artificial, 147(1-2):5-34, julio de 2003. [9] R. M. Neal y G. E. Hinton. Una vista del algoritmo EM 796 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) que justifica enfoques incrementales, dispersos y otras variantes. En M. I. Jordan, editor, Aprendizaje en Modelos Gráficos, páginas 355-368. Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez y S. Kraus. Seguridad en sistemas multiagentes mediante la aleatorización de políticas. En Actas de AAMAS 2006, 2006. [11] J. Pineau, G. Gordon y S. Thrun. Iteración de valor basada en puntos: Un algoritmo en cualquier momento para POMDPs. En la Conferencia Conjunta Internacional de Inteligencia Artificial (IJCAI), páginas 1025-1032, agosto de 2003. [12] M. L. Puterman. Procesos de Decisión de Markov. Serie Wiley en Probabilidad y Estadística Matemática: Sección de Probabilidad y Estadística Aplicada. Publicación de Wiley-Interscience, Nueva York, 1994. [13] Z. Rabinovich y J. S. Rosenschein. Seguimiento Markov extendido con una aplicación al control. En el taller sobre seguimiento de agentes: modelando otros agentes a partir de observaciones, en la Tercera Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, páginas 95-100, Nueva York, julio de 2004. [14] Z. Rabinovich y J. S. Rosenschein. Coordinación multiagente mediante Seguimiento Markov Extendido. En la Cuarta Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, páginas 431-438, Utrecht, Países Bajos, julio de 2005. [15] Z. Rabinovich y J. S. Rosenschein. Sobre la respuesta del control basado en EMT a objetivos y modelos interactivos. En la Quinta Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, páginas 465-470, Hakodate, Japón, mayo de 2006. [16] R. F. Stengel. Control óptimo y estimación. Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, y P. Varakantham. Conflictos en el trabajo en equipo: Híbridos para el Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 797 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "stochastic environment": {
            "translated_key": "entornos estocásticos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Dynamics Based Control with an Application to Area-Sweeping Problems Zinovi Rabinovich Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel jeff@cs.huji.ac.il Gal A. Kaminka The MAVERICK Group Department of Computer Science Bar Ilan University, Israel galk@cs.biu.ac.il ABSTRACT In this paper we introduce Dynamics Based Control (DBC), an approach to planning and control of an agent in <br>stochastic environment</br>s.",
                "Unlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics.",
                "We show that a recently developed planning and control approach, Extended Markov Tracking (EMT) is an instantiation of DBC.",
                "EMT employs greedy action selection to provide an efficient control algorithm in Markovian environments.",
                "We exploit this efficiency in a set of experiments that applied multitarget EMT to a class of area-sweeping problems (searching for moving targets).",
                "We show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation.",
                "Categories and Subject Descriptors I.2.8 [Problem Solving, Control Methods, and Search]: Control Theory; I.2.9 [Robotics]; I.2.11 [Distributed Artificial Intelligence]: Intelligent Agents General Terms Algorithms, Theory 1.",
                "INTRODUCTION Planning and control constitutes a central research area in multiagent systems and artificial intelligence.",
                "In recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in <br>stochastic environment</br>s.",
                "In this framework, the planning and control problem is often addressed by imposing a reward function, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility.",
                "While theoretically attractive, the complexity of optimally solving a POMDP is prohibitive [8, 7].",
                "We take an alternative view of planning in <br>stochastic environment</br>s.",
                "We do not use a (state-based) reward function, but instead optimize over a different criterion, a transition-based specification of the desired system dynamics.",
                "The idea here is to view planexecution as a process that compels a (stochastic) system to change, and a plan as a dynamic process that shapes that change according to desired criteria.",
                "We call this general planning framework Dynamics Based Control (DBC).",
                "In DBC, the goal of a planning (or control) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) target dynamics.",
                "As actual system behavior may deviate from that which is specified by target dynamics (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16].",
                "Here, optimality is measured in terms of probability of deviation magnitudes.",
                "In this paper, we present the structure of Dynamics Based Control.",
                "We show that the recently developed Extended Markov Tracking (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC.",
                "EMT is an efficient instantiation of DBC.",
                "To evaluate DBC, we carried out a set of experiments applying multi-target EMT to the Tag Game [11]; this is a variant on the area sweeping problem, where an agent is trying to tag a moving target (quarry) whose position is not known with certainty.",
                "Experimental data demonstrates that even with a simple model of the environment and a simple design of target dynamics, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agents position).",
                "The paper is organized as follows.",
                "In Section 2 we motivate DBC using area-sweeping problems, and discuss related work.",
                "Section 3 introduces the Dynamics Based Control (DBC) structure, and its specialization to Markovian environments.",
                "This is followed by a review of the Extended Markov Tracking (EMT) approach as a DBC-structured control regimen in Section 4.",
                "That section also discusses the limitations of EMT-based control relative to the general DBC framework.",
                "Experimental settings and results are then presented in Section 5.",
                "Section 6 provides a short discussion of the overall approach, and Section 7 gives some concluding remarks and directions for future work. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "MOTIVATION AND RELATED WORK Many real-life scenarios naturally have a stochastic target dynamics specification, especially those domains where there exists no ultimate goal, but rather system behavior (with specific properties) that has to be continually supported.",
                "For example, security guards perform persistent sweeps of an area to detect any sign of intrusion.",
                "Cunning thieves will attempt to track these sweeps, and time their operation to key points of the guards motion.",
                "It is thus advisable to make the guards motion dynamics appear irregular and random.",
                "Recent work by Paruchuri et al. [10] has addressed such randomization in the context of single-agent and distributed POMDPs.",
                "The goal in that work was to generate policies that provide a measure of action-selection randomization, while maintaining rewards within some acceptable levels.",
                "Our focus differs from this work in that DBC does not optimize expected rewards-indeed we do not consider rewards at all-but instead maintains desired dynamics (including, but not limited to, randomization).",
                "The Game of Tag is another example of the applicability of the approach.",
                "It was introduced in the work by Pineau et al. [11].",
                "There are two agents that can move about an area, which is divided into a grid.",
                "The grid may have blocked cells (holes) into which no agent can move.",
                "One agent (the hunter) seeks to move into a cell occupied by the other (the quarry), such that they are co-located (this is a successful tag).",
                "The quarry seeks to avoid the hunter agent, and is always aware of the hunters position, but does not know how the hunter will behave, which opens up the possibility for a hunter to surprise the prey.",
                "The hunter knows the quarrys probabilistic law of motion, but does not know its current location.",
                "Tag is an instance of a family of area-sweeping (pursuit-evasion) problems.",
                "In [11], the hunter modeled the problem using a POMDP.",
                "A reward function was defined, to reflect the desire to tag the quarry, and an action policy was computed to optimize the reward collected over time.",
                "Due to the intractable complexity of determining the optimal policy, the action policy computed in that paper was essentially an approximation.",
                "In this paper, instead of formulating a reward function, we use EMT to solve the problem, by directly specifying the target dynamics.",
                "In fact, any search problem with randomized motion, the socalled class of area sweeping problems, can be described through specification of such target system dynamics.",
                "Dynamics Based Control provides a natural approach to solving these problems. 3.",
                "DYNAMICS BASED CONTROL The specification of Dynamics Based Control (DBC) can be broken into three interacting levels: Environment Design Level, User Level, and Agent Level. • Environment Design Level is concerned with the formal specification and modeling of the environment.",
                "For example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation constant. • User Level in turn relies on the environment model produced by Environment Design to specify the target system dynamics it wishes to observe.",
                "The User Level also specifies the estimation or learning procedure for system dynamics, and the measure of deviation.",
                "In the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping. • Agent Level in turn combines the environment model from the Environment Design level, the dynamics estimation procedure, the deviation measure and the target dynamics specification from User Level, to produce a sequence of actions that create system dynamics as close as possible to the targeted specification.",
                "As we are interested in the continual development of a stochastic system, such as happens in classical control theory [16] and continual planning [4], as well as in our example of museum sweeps, the question becomes how the Agent Level is to treat the deviation measurements over time.",
                "To this end, we use a probability threshold-that is, we would like the Agent Level to maximize the probability that the deviation measure will remain below a certain threshold.",
                "Specific action selection then depends on system formalization.",
                "One possibility would be to create a mixture of available system trends, much like that which happens in Behavior-Based Robotic architectures [1].",
                "The other alternative would be to rely on the estimation procedure provided by the User Level-to utilize the Environment Design Level model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a certain dynamics has been achieved.",
                "Notice that this manipulation is not direct, but via the environment.",
                "Thus, for strong enough estimator algorithms, successful manipulation would mean a successful simulation of the specified target dynamics (i.e., beyond discerning via the available sensory input).",
                "DBC levels can also have a back-flow of information (see Figure 1).",
                "For instance, the Agent Level could provide data about target dynamics feasibility, allowing the User Level to modify the requirement, perhaps focusing on attainable features of system behavior.",
                "Data would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the User Level, this can provide an important tool for the environment model calibration at the Environment Design Level.",
                "UserEnv.",
                "Design Agent Model Ideal Dynamics Estimator Estimator Dynamics Feasibility System Response Data Figure 1: Data flow of the DBC framework Extending upon the idea of Actor-Critic algorithms [5], DBC data flow can provide a good basis for the design of a learning algorithm.",
                "For example, the User Level can operate as an exploratory device for a learning algorithm, inferring an ideal dynamics target from the environment model at hand that would expose and verify most critical features of system behavior.",
                "In this case, feasibility and system response data from the Agent Level would provide key information for an environment model update.",
                "In fact, the combination of feasibility and response data can provide a basis for the application of strong learning algorithms such as EM [2, 9]. 3.1 DBC for Markovian Environments For a Partially Observable Markovian Environment, DBC can be specified in a more rigorous manner.",
                "Notice how DBC discards rewards, and replaces it by another optimality criterion (structural differences are summarized in Table 1): • Environment Design level is to specify a tuple < S, A, T, O, Ω, s0 >, where: - S is the set of all possible environment states; - s0 is the initial state of the environment (which can also be viewed as a probability distribution over S); The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 791 - A is the set of all possible actions applicable in the environment; - T is the environments probabilistic transition function: T : S ×A → Π(S).",
                "That is, T(s |a, s) is the probability that the environment will move from state s to state s under action a; - O is the set of all possible observations.",
                "This is what the sensor input would look like for an outside observer; - Ω is the observation probability function: Ω : S × A × S → Π(O).",
                "That is, Ω(o|s , a, s) is the probability that one will observe o given that the environment has moved from state s to state s under action a. • User Level, in the case of Markovian environment, operates on the set of system dynamics described by a family of conditional probabilities F = {τ : S × A → Π(S)}.",
                "Thus specification of target dynamics can be expressed by q ∈ F, and the learning or tracking algorithm can be represented as a function L : O×(A×O)∗ → F; that is, it maps sequences of observations and actions performed so far into an estimate τ ∈ F of system dynamics.",
                "There are many possible variations available at the User Level to define divergence between system dynamics; several of them are: - Trace distance or L1 distance between two distributions p and q defined by D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Fidelity measure of distance F(p(·), q(·)) = x p(x)q(x) - Kullback-Leibler divergence DKL(p(·) q(·)) = x p(x) log p(x) q(x) Notice that the latter two are not actually metrics over the space of possible distributions, but nevertheless have meaningful and important interpretations.",
                "For instance, KullbackLeibler divergence is an important tool of information theory [3] that allows one to measure the price of encoding an information source governed by q, while assuming that it is governed by p. The User Level also defines the threshold of dynamics deviation probability θ. • Agent Level is then faced with a problem of selecting a control signal function a∗ to satisfy a minimization problem as follows: a∗ = arg min a Pr(d(τa, q) > θ) where d(τa, q) is a random variable describing deviation of the dynamics estimate τa, created by L under control signal a, from the ideal dynamics q.",
                "Implicit in this minimization problem is that L is manipulated via the environment, based on the environment model produced by the Environment Design Level. 3.2 DBC View of the State Space It is important to note the complementary view that DBC and POMDPs take on the state space of the environment.",
                "POMDPs regard state as a stationary snap-shot of the environment; whatever attributes of state sequencing one seeks are reached through properties of the control process, in this case reward accumulation.",
                "This can be viewed as if the sequencing of states and the attributes of that sequencing are only introduced by and for the controlling mechanism-the POMDP policy.",
                "DBC concentrates on the underlying principle of state sequencing, the system dynamics.",
                "DBCs target dynamics specification can use the environments state space as a means to describe, discern, and preserve changes that occur within the system.",
                "As a result, DBC has a greater ability to express state sequencing properties, which are grounded in the environment model and its state space definition.",
                "For example, consider the task of moving through rough terrain towards a goal and reaching it as fast as possible.",
                "POMDPs would encode terrain as state space points, while speed would be ensured by negative reward for every step taken without reaching the goalaccumulating higher reward can be reached only by faster motion.",
                "Alternatively, the state space could directly include the notion of speed.",
                "For POMDPs, this would mean that the same concept is encoded twice, in some sense: directly in the state space, and indirectly within reward accumulation.",
                "Now, even if the reward function would encode more, and finer, details of the properties of motion, the POMDP solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure.",
                "On the other hand, the tactical target expression of variations and correlations between position and speed of motion is now grounded in the state space representation.",
                "In this situation, any further constraints, e.g., smoothness of motion, speed limits in different locations, or speed reductions during sharp turns, are explicitly and uniformly expressed by the tactical target, and can result in faster and more effective action selection by a DBC algorithm. 4.",
                "EMT-BASED CONTROL AS A DBC Recently, a control algorithm was introduced called EMT-based Control [13], which instantiates the DBC framework.",
                "Although it provides an approximate greedy solution in the DBC sense, initial experiments using EMT-based control have been encouraging [14, 15].",
                "EMT-based control is based on the Markovian environment definition, as in the case with POMDPs, but its User and Agent Levels are of the Markovian DBC type of optimality. • User Level of EMT-based control defines a limited-case target system dynamics independent of action: qEMT : S → Π(S).",
                "It then utilizes the Kullback-Leibler divergence measure to compose a momentary system dynamics estimator-the Extended Markov Tracking (EMT) algorithm.",
                "The algorithm keeps a system dynamics estimate τt EMT that is capable of explaining recent change in an auxiliary Bayesian system state estimator from pt−1 to pt, and updates it conservatively using Kullback-Leibler divergence.",
                "Since τt EMT and pt−1,t are respectively the conditional and marginal probabilities over the systems state space, explanation simply means that pt(s ) = s τt EMT (s |s)pt−1(s), and the dynamics estimate update is performed by solving a 792 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: Structure of POMDP vs. Dynamics-Based Control in Markovian Environment Level Approach MDP Markovian DBC Environment < S, A, T, O, Ω >,where S - set of states A - set of actions Design T : S × A → Π(S) - transition O - observation set Ω : S × A × S → Π(O) User r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - reward function q - ideal dynamics F - reward remodeling L - dynamics estimator θ - threshold Agent π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) minimization problem: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • Agent Level in EMT-based control is suboptimal with respect to DBC (though it remains within the DBC framework), performing greedy action selection based on prediction of EMTs reaction.",
                "The prediction is based on the environment model provided by the Environment Design level, so that if we denote by Ta the environments transition function limited to action a, and pt−1 is the auxiliary Bayesian system state estimator, then the EMT-based control choice is described by a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1) Note that this follows the Markovian DBC framework precisely: the rewarding optimality of POMDPs is discarded, and in its place a dynamics estimator (EMT in this case) is manipulated via action effects on the environment to produce an estimate close to the specified target system dynamics.",
                "Yet as we mentioned, naive EMTbased control is suboptimal in the DBC sense, and has several additional limitations that do not exist in the general DBC framework (discussed in Section 4.2). 4.1 Multi-Target EMT At times, there may exist several behavioral preferences.",
                "For example, in the case of museum guards, some art items are more heavily guarded, requiring that the guards stay more often in their close vicinity.",
                "On the other hand, no corner of the museum is to be left unchecked, which demands constant motion.",
                "Successful museum security would demand that the guards adhere to, and balance, both of these behaviors.",
                "For EMT-based control, this would mean facing several tactical targets {qk}K k=1, and the question becomes how to merge and balance them.",
                "A balancing mechanism can be applied to resolve this issue.",
                "Note that EMT-based control, while selecting an action, creates a preference vector over the set of actions based on their predicted performance with respect to a given target.",
                "If these preference vectors are normalized, they can be combined into a single unified preference.",
                "This requires replacement of standard EMT-based action selection by the algorithm below [15]: • Given: - a set of target dynamics {qk}K k=1, - vector of weights w(k) • Select action as follows - For each action a ∈ A predict the future state distribution ¯pa t+1 = Ta ∗ pt; - For each action, compute Da = H(¯pa t+1, pt, PDt) - For each a ∈ A and qk tactical target, denote V (a, k) = DKL (Da qk) pt .",
                "Let Vk(a) = 1 Zk V (a, k), where Zk = a∈A V (a, k) is a normalization factor. - Select a∗ = arg min a k k=1 w(k)Vk(a) The weights vector w = (w1, ..., wK ) allows the additional tuning of importance among target dynamics without the need to redesign the targets themselves.",
                "This balancing method is also seamlessly integrated into the EMT-based control flow of operation. 4.2 EMT-based Control Limitations EMT-based control is a sub-optimal (in the DBC sense) representative of the DBC structure.",
                "It limits the User by forcing EMT to be its dynamic tracking algorithm, and replaces Agent optimization by greedy action selection.",
                "This kind of combination, however, is common for on-line algorithms.",
                "Although further development of EMT-based controllers is necessary, evidence so far suggests that even the simplest form of the algorithm possesses a great deal of power, and displays trends that are optimal in the DBC sense of the word.",
                "There are two further, EMT-specific, limitations to EMT-based control that are evident at this point.",
                "Both already have partial solutions and are subjects of ongoing research.",
                "The first limitation is the problem of negative preference.",
                "In the POMDP framework for example, this is captured simply, through The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 793 the appearance of values with different signs within the reward structure.",
                "For EMT-based control, however, negative preference means that one would like to avoid a certain distribution over system development sequences; EMT-based control, however, concentrates on getting as close as possible to a distribution.",
                "Avoidance is thus unnatural in native EMT-based control.",
                "The second limitation comes from the fact that standard environment modeling can create pure sensory actions-actions that do not change the state of the world, and differ only in the way observations are received and the quality of observations received.",
                "Since the world state does not change, EMT-based control would not be able to differentiate between different sensory actions.",
                "Notice that both of these limitations of EMT-based control are absent from the general DBC framework, since it may have a tracking algorithm capable of considering pure sensory actions and, unlike Kullback-Leibler divergence, a distribution deviation measure that is capable of dealing with negative preference. 5.",
                "EMT PLAYING TAG The Game of Tag was first introduced in [11].",
                "It is a single agent problem of capturing a quarry, and belongs to the class of area sweeping problems.",
                "An example domain is shown in Figure 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figure 2: Tag domain; an agent (A) attempts to seek and capture a quarry (Q) The Game of Tag extremely limits the agents perception, so that the agent is able to detect the quarry only if they are co-located in the same cell of the grid world.",
                "In the classical version of the game, co-location leads to a special observation, and the Tag action can be performed.",
                "We slightly modify this setting: the moment that both agents occupy the same cell, the game ends.",
                "As a result, both the agent and its quarry have the same motion capability, which allows them to move in four directions, North, South, East, and West.",
                "These form a formal space of actions within a Markovian environment.",
                "The state space of the formal Markovian environment is described by the cross-product of the agent and quarrys positions.",
                "For Figure 2, it would be S = {s0, ..., s23} × {s0, ..., s23}.",
                "The effects of an action taken by the agent are deterministic, but the environment in general has a stochastic response due to the motion of the quarry.",
                "With probability q0 1 it stays put, and with probability 1 − q0 it moves to an adjacent cell further away from the 1 In our experiments this was taken to be q0 = 0.2. agent.",
                "So for the instance shown in Figure 2 and q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Although the evasive behavior of the quarry is known to the agent, the quarrys position is not.",
                "The only sensory information available to the agent is its own location.",
                "We use EMT and directly specify the target dynamics.",
                "For the Game of Tag, we can easily formulate three major trends: catching the quarry, staying mobile, and stalking the quarry.",
                "This results in the following three target dynamics: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 otherwise Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 otherwise Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Note that none of the above targets are directly achievable; for instance, if Qt = s9 and At = s11, there is no action that can move the agent to At+1 = s9 as required by the Tcatch target dynamics.",
                "We ran several experiments to evaluate EMT performance in the Tag Game.",
                "Three configurations of the domain shown in Figure 3 were used, each posing a different challenge to the agent due to partial observability.",
                "In each setting, a set of 1000 runs was performed with a time limit of 100 steps.",
                "In every run, the initial position of both the agent and its quarry was selected at random; this means that as far as the agent was concerned, the quarrys initial position was uniformly distributed over the entire domain cell space.",
                "We also used two variations of the environment observability function.",
                "In the first version, observability function mapped all joint positions of hunter and quarry into the position of the hunter as an observation.",
                "In the second, only those joint positions in which hunter and quarry occupied different locations were mapped into the hunters location.",
                "The second version in fact utilized and expressed the fact that once hunter and quarry occupy the same cell the game ends.",
                "The results of these experiments are shown in Table 2.",
                "Balancing [15] the catch, move, and stalk target dynamics described in the previous section by the weight vector [0.8, 0.1, 0.1], EMT produced stable performance in all three domains.",
                "Although direct comparisons are difficult to make, the EMT performance displayed notable efficiency vis-`a-vis the POMDP approach.",
                "In spite of a simple and inefficient Matlab implementation of the EMT algorithm, the decision time for any given step averaged significantly below 1 second in all experiments.",
                "For the irregular open arena domain, which proved to be the most difficult, 1000 experiment runs bounded by 100 steps each, a total of 42411 steps, were completed in slightly under 6 hours.",
                "That is, over 4 × 104 online steps took an order of magnitude less time than the offline computation of POMDP policy in [11].",
                "The significance of this differential is made even more prominent by the fact that, should the environment model parameters change, the online nature of EMT would allow it to maintain its performance time, while the POMDP policy would need to be recomputed, requiring yet again a large overhead of computation.",
                "We also tested the behavior cell frequency entropy, empirical measures from trial data.",
                "As Figure 4 and Figure 5 show, empir794 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) A Q Q A 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 A Q Figure 3: These configurations of the Tag Game space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor Table 2: Performance of the EMT-based solution in three Tag Game domains and two observability models: I) omniposition quarry, II) quarry is not at hunters position Model Domain Capture% E(Steps) Time/Step I Dead-ends 100 14.8 72(mSec) Arena 80.2 42.4 500(mSec) Circle 91.4 34.6 187(mSec) II Dead-ends 100 13.2 91(mSec) Arena 96.8 28.67 396(mSec) Circle 94.4 31.63 204(mSec) ical entropy grows with the length of interaction.",
                "For runs where the quarry was not captured immediately, the entropy reaches between 0.85 and 0.952 for different runs and scenarios.",
                "As the agent actively seeks the quarry, the entropy never reaches its maximum.",
                "One characteristic of the entropy graph for the open arena scenario particularly caught our attention in the case of the omniposition quarry observation model.",
                "Near the maximum limit of trial length (100 steps), entropy suddenly dropped.",
                "Further analysis of the data showed that under certain circumstances, a fluctuating behavior occurs in which the agent faces equally viable versions of quarry-following behavior.",
                "Since the EMT algorithm has greedy action selection, and the state space does not encode any form of commitment (not even speed or acceleration), the agent is locked within a small portion of cells.",
                "It is essentially attempting to simultaneously follow several courses of action, all of which are consistent with the target dynamics.",
                "This behavior did not occur in our second observation model, since it significantly reduced the set of eligible courses of action-essentially contributing to tie-breaking among them. 6.",
                "DISCUSSION The design of the EMT solution for the Tag Game exposes the core difference in approach to planning and control between EMT or DBC, on the one hand, and the more familiar POMDP approach, on the other.",
                "POMDP defines a reward structure to optimize, and influences system dynamics indirectly through that optimization.",
                "EMT discards any reward scheme, and instead measures and influences system dynamics directly. 2 Entropy was calculated using log base equal to the number of possible locations within the domain; this properly scales entropy expression into the range [0, 1] for all domains.",
                "Thus for the Tag Game, we did not search for a reward function that would encode and express our preference over the agents behavior, but rather directly set three (heuristic) behavior preferences as the basis for target dynamics to be maintained.",
                "Experimental data shows that these targets need not be directly achievable via the agents actions.",
                "However, the ratio between EMT performance and achievability of target dynamics remains to be explored.",
                "The tag game experiment data also revealed the different emphasis DBC and POMDPs place on the formulation of an environment state space.",
                "POMDPs rely entirely on the mechanism of reward accumulation maximization, i.e., formation of the action selection procedure to achieve necessary state sequencing.",
                "DBC, on the other hand, has two sources of sequencing specification: through the properties of an action selection procedure, and through direct specification within the target dynamics.",
                "The importance of the second source was underlined by the Tag Game experiment data, in which the greedy EMT algorithm, applied to a POMDP-type state space specification, failed, since target description over such a state space was incapable of encoding the necessary behavior tendencies, e.g., tie-breaking and commitment to directed motion.",
                "The structural differences between DBC (and EMT in particular), and POMDPs, prohibits direct performance comparison, and places them on complementary tracks, each within a suitable niche.",
                "For instance, POMDPs could be seen as a much more natural formulation of economic sequential decision-making problems, while EMT is a better fit for continual demand for stochastic change, as happens in many robotic or embodied-agent problems.",
                "The complementary properties of POMDPs and EMT can be further exploited.",
                "There is recent interest in using POMDPs in hybrid solutions [17], in which the POMDPs can be used together with other control approaches to provide results not easily achievable with either approach by itself.",
                "DBC can be an effective partner in such a hybrid solution.",
                "For instance, POMDPs have prohibitively large off-line time requirements for policy computation, but can be readily used in simpler settings to expose beneficial behavioral trends; this can serve as a form of target dynamics that are provided to EMT in a larger domain for on-line operation. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have presented a novel perspective on the process of planning and control in <br>stochastic environment</br>s, in the form of the Dynamics Based Control (DBC) framework.",
                "DBC formulates the task of planning as support of a specified target system dynamics, which describes the necessary properties of change within the environment.",
                "Optimality of DBC plans of action are measured The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 4: Observation Model I: Omniposition quarry.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 5: Observation Model II: quarry not observed at hunters position.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. with respect to the deviation of actual system dynamics from the target dynamics.",
                "We show that a recently developed technique of Extended Markov Tracking (EMT) [13] is an instantiation of DBC.",
                "In fact, EMT can be seen as a specific case of DBC parameterization, which employs a greedy action selection procedure.",
                "Since EMT exhibits the key features of the general DBC framework, as well as polynomial time complexity, we used the multitarget version of EMT [15] to demonstrate that the class of area sweeping problems naturally lends itself to dynamics-based descriptions, as instantiated by our experiments in the Tag Game domain.",
                "As enumerated in Section 4.2, EMT has a number of limitations, such as difficulty in dealing with negative dynamic preference.",
                "This prevents direct application of EMT to such problems as Rendezvous-Evasion Games (e.g., [6]).",
                "However, DBC in general has no such limitations, and readily enables the formulation of evasion games.",
                "In future work, we intend to proceed with the development of dynamics-based controllers for these problems. 8.",
                "ACKNOWLEDGMENT The work of the first two authors was partially supported by Israel Science Foundation grant #898/05, and the third author was partially supported by a grant from Israels Ministry of Science and Technology. 9.",
                "REFERENCES [1] R. C. Arkin.",
                "Behavior-Based Robotics.",
                "MIT Press, 1998. [2] J.",
                "A. Bilmes.",
                "A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and Hidden Markov Models.",
                "Technical Report TR-97-021, Department of Electrical Engeineering and Computer Science, University of California at Berkeley, 1998. [3] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of information theory.",
                "Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz, and M. J. Wolverton.",
                "A survey of research in distributed, continual planning.",
                "AI Magazine, 4:13-22, 1999. [5] V. R. Konda and J. N. Tsitsiklis.",
                "Actor-Critic algorithms.",
                "SIAM Journal on Control and Optimization, 42(4):1143-1166, 2003. [6] W. S. Lim.",
                "A rendezvous-evasion game on discrete locations with joint randomization.",
                "Advances in Applied Probability, 29(4):1004-1017, December 1997. [7] M. L. Littman, T. L. Dean, and L. P. Kaelbling.",
                "On the complexity of solving Markov decision problems.",
                "In Proceedings of the 11th Annual Conference on Uncertainty in Artificial Intelligence (UAI-95), pages 394-402, 1995. [8] O. Madani, S. Hanks, and A. Condon.",
                "On the undecidability of probabilistic planning and related stochastic optimization problems.",
                "Artificial Intelligence Journal, 147(1-2):5-34, July 2003. [9] R. M. Neal and G. E. Hinton.",
                "A view of the EM algorithm 796 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) that justifies incremental, sparse, and other variants.",
                "In M. I. Jordan, editor, Learning in Graphical Models, pages 355-368.",
                "Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In Proceeding of AAMAS 2006, 2006. [11] J. Pineau, G. Gordon, and S. Thrun.",
                "Point-based value iteration: An anytime algorithm for pomdps.",
                "In International Joint Conference on Artificial Intelligence (IJCAI), pages 1025-1032, August 2003. [12] M. L. Puterman.",
                "Markov Decision Processes.",
                "Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics Section.",
                "Wiley-Interscience Publication, New York, 1994. [13] Z. Rabinovich and J. S. Rosenschein.",
                "Extended Markov Tracking with an application to control.",
                "In The Workshop on Agent Tracking: Modeling Other Agents from Observations, at the Third International Joint Conference on Autonomous Agents and Multiagent Systems, pages 95-100, New-York, July 2004. [14] Z. Rabinovich and J. S. Rosenschein.",
                "Multiagent coordination by Extended Markov Tracking.",
                "In The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 431-438, Utrecht, The Netherlands, July 2005. [15] Z. Rabinovich and J. S. Rosenschein.",
                "On the response of EMT-based control to interacting targets and models.",
                "In The Fifth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 465-470, Hakodate, Japan, May 2006. [16] R. F. Stengel.",
                "Optimal Control and Estimation.",
                "Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, and P. Varakantham.",
                "Conflicts in teamwork: Hybrids to the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 797"
            ],
            "original_annotated_samples": [
                "Dynamics Based Control with an Application to Area-Sweeping Problems Zinovi Rabinovich Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel jeff@cs.huji.ac.il Gal A. Kaminka The MAVERICK Group Department of Computer Science Bar Ilan University, Israel galk@cs.biu.ac.il ABSTRACT In this paper we introduce Dynamics Based Control (DBC), an approach to planning and control of an agent in <br>stochastic environment</br>s.",
                "In recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in <br>stochastic environment</br>s.",
                "We take an alternative view of planning in <br>stochastic environment</br>s.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have presented a novel perspective on the process of planning and control in <br>stochastic environment</br>s, in the form of the Dynamics Based Control (DBC) framework."
            ],
            "translated_annotated_samples": [
                "En este artículo presentamos el Control Basado en Dinámicas (DBC), un enfoque para la planificación y control de un agente en <br>entornos estocásticos</br>.",
                "En los últimos años, los Procesos de Decisión de Markov Parcialmente Observables (POMDPs) [12] se han convertido en una base formal popular para la planificación en <br>entornos estocásticos</br>.",
                "Tomamos una visión alternativa de la planificación en <br>entornos estocásticos</br>.",
                "CONCLUSIONES Y TRABAJO FUTURO En este artículo, hemos presentado una nueva perspectiva sobre el proceso de planificación y control en <br>entornos estocásticos</br>, en forma del marco de Control Basado en Dinámicas (DBC)."
            ],
            "translated_text": "En este artículo presentamos el Control Basado en Dinámicas (DBC), un enfoque para la planificación y control de un agente en <br>entornos estocásticos</br>. A diferencia de los enfoques existentes, que buscan optimizar las recompensas esperadas (por ejemplo, en Problemas de Decisión de Markov Parcialmente Observables (POMDP)), DBC optimiza el comportamiento del sistema hacia las dinámicas del sistema especificadas. Mostramos que un enfoque de planificación y control recientemente desarrollado, Seguimiento Extendido de Markov (EMT), es una instancia de DBC. EMT emplea la selección de acciones codiciosa para proporcionar un algoritmo de control eficiente en entornos markovianos. Explotamos esta eficiencia en un conjunto de experimentos que aplicaron EMT multitarget a una clase de problemas de barrido de área (búsqueda de objetivos en movimiento). Mostramos que tales problemas pueden ser definidos de forma natural y resueltos eficientemente utilizando el marco de DBC y su instanciación de EMT. Categorías y Descriptores de Asignaturas I.2.8 [Resolución de Problemas, Métodos de Control y Búsqueda]: Teoría de Control; I.2.9 [Robótica]; I.2.11 [Inteligencia Artificial Distribuida]: Agentes Inteligentes Términos Generales Algoritmos, Teoría 1. INTRODUCCIÓN La planificación y el control constituyen un área de investigación central en sistemas multiagentes e inteligencia artificial. En los últimos años, los Procesos de Decisión de Markov Parcialmente Observables (POMDPs) [12] se han convertido en una base formal popular para la planificación en <br>entornos estocásticos</br>. En este marco, el problema de planificación y control suele abordarse imponiendo una función de recompensa y calculando una política (de elección de acciones) que sea óptima, en el sentido de que resultará en la mayor utilidad esperada. Si bien teóricamente atractivo, la complejidad de resolver óptimamente un POMDP es prohibitiva [8, 7]. Tomamos una visión alternativa de la planificación en <br>entornos estocásticos</br>. No utilizamos una función de recompensa basada en el estado, sino que optimizamos según un criterio diferente, una especificación basada en transiciones de la dinámica del sistema deseado. La idea aquí es ver la ejecución del plan como un proceso que obliga a un sistema (estocástico) a cambiar, y un plan como un proceso dinámico que moldea ese cambio de acuerdo con criterios deseados. Llamamos a este marco de planificación general Control Basado en Dinámicas (DBC). En DBC, el objetivo de un proceso de planificación (o control) es garantizar que el sistema cambiará de acuerdo con dinámicas objetivo específicas (potencialmente estocásticas). Dado que el comportamiento real del sistema puede desviarse de lo especificado por la dinámica objetivo (debido a la naturaleza estocástica del sistema), la planificación en tales entornos debe ser continua, de manera similar a los controladores en lazo cerrado clásicos. Aquí, la optimalidad se mide en términos de la probabilidad de magnitudes de desviación. En este documento, presentamos la estructura del Control Basado en Dinámicas. Mostramos que el enfoque de Seguimiento Extendido de Markov (EMT) recientemente desarrollado [13, 14, 15] está subsumido por DBC, con EMT empleando selección de acciones codiciosa, que es una parametrización específica entre las opciones posibles dentro de DBC. EMT es una implementación eficiente de DBC. Para evaluar DBC, llevamos a cabo un conjunto de experimentos aplicando EMT de múltiples objetivos al Juego de Etiquetas [11]; esta es una variante del problema de barrido de área, donde un agente intenta etiquetar a un objetivo móvil (presa) cuya posición no se conoce con certeza. Los datos experimentales demuestran que incluso con un modelo simple del entorno y un diseño simple de la dinámica del objetivo, se pueden lograr altas tasas de éxito tanto en la captura de la presa como en sorprender a la presa (como se expresa por la entropía observada de la posición de los agentes controlados). El documento está organizado de la siguiente manera. En la Sección 2 motivamos DBC utilizando problemas de barrido de área, y discutimos trabajos relacionados. La Sección 3 introduce la estructura de Control Basado en Dinámicas (DBC) y su especialización en entornos Markovianos. Esto es seguido por una revisión del enfoque de Seguimiento Markov Extendido (EMT) como un régimen de control estructurado por DBC en la Sección 4. Esa sección también discute las limitaciones del control basado en EMT en relación con el marco general de DBC. Los ajustes experimentales y los resultados se presentan a continuación en la Sección 5. La Sección 6 proporciona una breve discusión del enfoque general, y la Sección 7 ofrece algunas observaciones finales y direcciones para trabajos futuros. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2. MOTIVACIÓN Y TRABAJO RELACIONADO Muchos escenarios de la vida real tienen naturalmente una especificación de dinámica de destino estocástica, especialmente en aquellos dominios donde no existe un objetivo final, sino más bien un comportamiento del sistema (con propiedades específicas) que debe ser continuamente respaldado. Por ejemplo, los guardias de seguridad realizan barridos persistentes de un área para detectar cualquier signo de intrusión. Los ladrones astutos intentarán rastrear estos barridos y sincronizar su operación con puntos clave del movimiento de los guardias. Por lo tanto, es recomendable que la dinámica de movimiento de los guardias parezca irregular y aleatoria. El trabajo reciente de Paruchuri et al. [10] ha abordado dicha aleatorización en el contexto de POMDPs de agente único y distribuidos. El objetivo de ese trabajo era generar políticas que proporcionen una medida de aleatorización en la selección de acciones, manteniendo al mismo tiempo las recompensas dentro de niveles aceptables. Nuestro enfoque difiere de este trabajo en que DBC no optimiza recompensas esperadas, de hecho, no consideramos recompensas en absoluto, sino que en su lugar mantenemos dinámicas deseadas (incluyendo, pero no limitado a, la aleatorización). El juego de la mancha es otro ejemplo de la aplicabilidad del enfoque. Fue introducido en el trabajo de Pineau et al. [11]. Hay dos agentes que pueden moverse por un área, la cual está dividida en una cuadrícula. La cuadrícula puede tener celdas bloqueadas (agujeros) en las cuales ningún agente puede moverse. Un agente (el cazador) intenta moverse a una celda ocupada por el otro (la presa), de modo que estén ubicados en el mismo lugar (esto es una etiqueta exitosa). La presa busca evitar al cazador, siempre está al tanto de la posición del cazador, pero no sabe cómo se comportará el cazador, lo que abre la posibilidad de que un cazador sorprenda a la presa. El cazador conoce la ley de movimiento probabilístico de la presa, pero no conoce su ubicación actual. Tag es una instancia de una familia de problemas de barrido de área (persecución-evasión). En [11], el cazador modeló el problema utilizando un POMDP. Se definió una función de recompensa para reflejar el deseo de marcar la presa, y se calculó una política de acción para optimizar la recompensa recolectada con el tiempo. Debido a la complejidad intratable de determinar la política óptima, la política de acción calculada en ese documento fue esencialmente una aproximación. En este artículo, en lugar de formular una función de recompensa, utilizamos EMT para resolver el problema, especificando directamente la dinámica objetivo. De hecho, cualquier problema de búsqueda con movimiento aleatorio, la llamada clase de problemas de barrido de área, puede ser descrito a través de la especificación de la dinámica de dicho sistema objetivo. El Control Basado en Dinámicas proporciona un enfoque natural para resolver estos problemas. CONTROL BASADO EN DINÁMICA La especificación del Control Basado en Dinámica (DBC) se puede dividir en tres niveles interactivos: Nivel de Diseño del Entorno, Nivel de Usuario y Nivel de Agente. • El Nivel de Diseño del Entorno se ocupa de la especificación formal y modelado del entorno. Por ejemplo, este nivel especificaría las leyes de la física dentro del sistema y establecería sus parámetros, como la constante de gravitación. El Nivel de Usuario, a su vez, se basa en el modelo de entorno producido por el Diseño de Entorno para especificar la dinámica del sistema objetivo que desea observar. El Nivel de Usuario también especifica el procedimiento de estimación o aprendizaje para la dinámica del sistema, y la medida de desviación. En el escenario del guardia de museo mencionado anteriormente, estos corresponderían a un horario de barrido estocástico y una medida de sorpresa relativa entre el barrido especificado y el real. • El Nivel de Agente, a su vez, combina el modelo del entorno del Nivel de Diseño de Entorno, el procedimiento de estimación de la dinámica, la medida de desviación y la especificación de la dinámica objetivo del Nivel de Usuario, para producir una secuencia de acciones que creen dinámicas del sistema lo más cercanas posible a la especificación deseada. Dado nuestro interés en el desarrollo continuo de un sistema estocástico, como ocurre en la teoría de control clásica [16] y la planificación continua [4], así como en nuestro ejemplo de recorridos por museos, la pregunta es cómo el Nivel de Agente debe tratar las mediciones de desviación a lo largo del tiempo. Con este fin, utilizamos un umbral de probabilidad, es decir, nos gustaría que el Nivel del Agente maximice la probabilidad de que la medida de desviación permanezca por debajo de cierto umbral. La selección de acciones específicas depende entonces de la formalización del sistema. Una posibilidad sería crear una mezcla de tendencias de sistemas disponibles, similar a lo que sucede en las arquitecturas robóticas basadas en el comportamiento [1]. La otra alternativa sería depender del procedimiento de estimación proporcionado por el Nivel de Usuario para utilizar el modelo del Nivel de Diseño del Entorno del entorno para elegir acciones, de manera que se manipule al estimador de dinámicas para que crea que se ha logrado cierta dinámica. Ten en cuenta que esta manipulación no es directa, sino a través del entorno. Por lo tanto, para algoritmos de estimación lo suficientemente fuertes, la manipulación exitosa significaría una simulación exitosa de la dinámica objetivo especificada (es decir, más allá de discernir a través de la entrada sensorial disponible). Los niveles de DBC también pueden tener un retroceso de información (ver Figura 1). Por ejemplo, el Nivel de Agente podría proporcionar datos sobre la viabilidad de la dinámica del objetivo, permitiendo al Nivel de Usuario modificar el requisito, quizás centrándose en las características alcanzables del comportamiento del sistema. Los datos también estarían disponibles sobre la respuesta del sistema a diferentes acciones realizadas; combinados con un estimador de dinámica definido por el Nivel de Usuario, esto puede proporcionar una herramienta importante para la calibración del modelo del entorno en el Nivel de Diseño del Entorno. UserEnv. El modelo de agente de diseño Estimador de Dinámicas Ideales Estimador de Dinámicas Factibilidad de Respuesta del Sistema de Datos Figura 1: Flujo de datos del marco de trabajo DBC Ampliando la idea de los algoritmos Actor-Crítico [5], el flujo de datos de DBC puede proporcionar una buena base para el diseño de un algoritmo de aprendizaje. Por ejemplo, el Nivel de Usuario puede funcionar como un dispositivo exploratorio para un algoritmo de aprendizaje, inferir un objetivo dinámico ideal a partir del modelo del entorno en cuestión que expondría y verificaría las características más críticas del comportamiento del sistema. En este caso, los datos de viabilidad y respuesta del sistema a nivel de agente proporcionarían información clave para una actualización del modelo del entorno. De hecho, la combinación de datos de viabilidad y respuesta puede proporcionar una base para la aplicación de algoritmos de aprendizaje sólidos como EM [2, 9]. 3.1 DBC para entornos markovianos Para un entorno markoviano parcialmente observable, DBC puede especificarse de una manera más rigurosa. Observa cómo DBC descarta las recompensas y las reemplaza por otro criterio de optimalidad (las diferencias estructurales se resumen en la Tabla 1): • El nivel de Diseño del Entorno es especificar una tupla < S, A, T, O, Ω, s0 >, donde: - S es el conjunto de todos los posibles estados del entorno; - s0 es el estado inicial del entorno (que también puede ser visto como una distribución de probabilidad sobre S); La Sexta Conferencia Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 791 - A es el conjunto de todas las acciones posibles aplicables en el entorno; - T es la función de transición probabilística del entorno: T: S × A → Π(S). Es decir, T(s | a, s) es la probabilidad de que el entorno se mueva del estado s al estado s bajo la acción a; - O es el conjunto de todas las observaciones posibles. Así es como se vería la entrada del sensor para un observador externo; - Ω es la función de probabilidad de observación: Ω: S × A × S → Π(O). Es decir, Ω(o|s, a, s) es la probabilidad de observar o dado que el entorno ha pasado del estado s al estado s bajo la acción a. • Nivel de Usuario, en el caso de un entorno markoviano, opera en el conjunto de dinámicas del sistema descritas por una familia de probabilidades condicionales F = {τ: S × A → Π(S)}. Por lo tanto, la especificación de la dinámica del objetivo puede expresarse como q ∈ F, y el algoritmo de aprendizaje o seguimiento puede representarse como una función L : O×(A×O)∗ → F; es decir, mapea secuencias de observaciones y acciones realizadas hasta el momento en una estimación τ ∈ F de la dinámica del sistema. Hay muchas variaciones posibles disponibles a nivel de usuario para definir la divergencia entre la dinámica del sistema; varias de ellas son: - Distancia de traza o distancia L1 entre dos distribuciones p y q definida por D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Medida de fidelidad de distancia F(p(·), q(·)) = x p(x)q(x) - Divergencia de Kullback-Leibler DKL(p(·) q(·)) = x p(x) log p(x) q(x) Nótese que las dos últimas no son en realidad métricas sobre el espacio de distribuciones posibles, pero tienen interpretaciones significativas e importantes. Por ejemplo, la divergencia de Kullback-Leibler es una herramienta importante de la teoría de la información [3] que permite medir el costo de codificar una fuente de información gobernada por q, asumiendo que está gobernada por p. El Nivel de Usuario también define el umbral de probabilidad de desviación de la dinámica θ. • El Nivel de Agente se enfrenta entonces a un problema de seleccionar una función de señal de control a∗ para satisfacer un problema de minimización de la siguiente manera: a∗ = arg min a Pr(d(τa, q) > θ) donde d(τa, q) es una variable aleatoria que describe la desviación de la estimación de la dinámica τa, creada por L bajo la señal de control a, de la dinámica ideal q. Implícito en este problema de minimización es que L es manipulado a través del entorno, basado en el modelo de entorno producido por el Nivel de Diseño del Entorno. 3.2 Vista DBC del Espacio de Estados Es importante tener en cuenta la visión complementaria que DBC y POMDPs tienen sobre el espacio de estados del entorno. Los POMDP consideran el estado como una instantánea estacionaria del entorno; cualquier atributo de la secuencia de estados que se busque se alcanza a través de las propiedades del proceso de control, en este caso la acumulación de recompensas. Esto se puede ver como si la secuencia de estados y los atributos de esa secuencia solo fueran introducidos por y para el mecanismo de control, la política POMDP. DBC se enfoca en el principio subyacente de la secuenciación de estados, la dinámica del sistema. La especificación de la dinámica de los DBCs puede utilizar el espacio de estado del entorno como un medio para describir, discernir y preservar los cambios que ocurren dentro del sistema. Como resultado, DBC tiene una mayor capacidad para expresar propiedades de secuenciación de estados, las cuales están fundamentadas en el modelo del entorno y su definición del espacio de estados. Por ejemplo, considera la tarea de moverse a través de terreno accidentado hacia un objetivo y alcanzarlo lo más rápido posible. Los POMDPs codificarían el terreno como puntos en el espacio de estados, mientras que la velocidad se garantizaría mediante una recompensa negativa por cada paso dado sin alcanzar la meta. Acumular una recompensa más alta solo se puede lograr mediante un movimiento más rápido. Alternativamente, el espacio de estados podría incluir directamente la noción de velocidad. Para los POMDPs, esto significaría que el mismo concepto está codificado dos veces, en cierto sentido: directamente en el espacio de estados e indirectamente en la acumulación de recompensas. Ahora, incluso si la función de recompensa codificara más y detalles más finos de las propiedades del movimiento, la solución POMDP tendría que buscar en un espacio mucho más grande de políticas, aunque aún esté guiada por el concepto implícito del procedimiento de acumulación de recompensas. Por otro lado, la expresión del objetivo táctico de variaciones y correlaciones entre la posición y la velocidad del movimiento ahora se basa en la representación del espacio de estados. En esta situación, cualquier restricción adicional, como la suavidad del movimiento, los límites de velocidad en diferentes ubicaciones o las reducciones de velocidad durante giros bruscos, son expresadas explícita y uniformemente por el objetivo táctico, y pueden resultar en una selección de acciones más rápida y efectiva por un algoritmo DBC. 4. El Control Basado en EMT como un DBC Recientemente, se introdujo un algoritmo de control llamado Control Basado en EMT [13], que instancia el marco de trabajo DBC. Aunque proporciona una solución ávida aproximada en el sentido de DBC, los experimentos iniciales utilizando el control basado en EMT han sido alentadores [14, 15]. El control basado en EMT se basa en la definición del entorno markoviano, como en el caso de los POMDP, pero sus Niveles de Usuario y Agente son del tipo de optimalidad DBC markoviana. • El Nivel de Usuario del control basado en EMT define una dinámica del sistema objetivo en un caso limitado independiente de la acción: qEMT: S → Π(S). Luego utiliza la medida de divergencia de Kullback-Leibler para componer un estimador de dinámica de sistemas momentáneos: el algoritmo de Seguimiento Extendido de Markov (EMT). El algoritmo mantiene una estimación de la dinámica del sistema τt EMT que es capaz de explicar el cambio reciente en un estimador de estado auxiliar bayesiano de pt−1 a pt, y lo actualiza de manera conservadora utilizando la divergencia de Kullback-Leibler. Dado que τt EMT y pt−1,t son respectivamente las probabilidades condicionales y marginales sobre el espacio de estados del sistema, la explicación simplemente significa que pt(s) = s τt EMT (s | s)pt−1(s), y la actualización de la estimación de la dinámica se realiza resolviendo un 792 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Tabla 1: Estructura de POMDP vs. Control basado en Dinámicas en Entorno Markoviano Nivel Enfoque MDP Markoviano DBC Entorno < S, A, T, O, Ω >, donde S - conjunto de estados A - conjunto de acciones Diseño T : S × A → Π(S) - transición O - conjunto de observaciones Ω : S × A × S → Π(O) Usuario r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - función de recompensa q - dinámicas ideales F - remodelación de recompensa L - estimador de dinámicas θ - umbral Agente π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) problema de minimización: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • El nivel de Agente en el control basado en EMT es subóptimo con respecto a DBC (aunque permanece dentro del marco de DBC), realizando selección de acciones codiciosas basadas en la predicción de la reacción de EMT. La predicción se basa en el modelo del entorno proporcionado por el nivel de Diseño del Entorno, de modo que si denotamos por Ta la función de transición de entornos limitada a la acción a, y pt−1 es el estimador de estado auxiliar del sistema bayesiano, entonces la elección de control basada en EMT se describe por a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1). Nótese que esto sigue precisamente el marco DBC Markoviano: la optimalidad recompensante de los POMDPs se descarta, y en su lugar se manipula un estimador de dinámicas (EMT en este caso) a través de los efectos de la acción en el entorno para producir una estimación cercana a la dinámica del sistema objetivo especificado. Sin embargo, como mencionamos, el control EMT ingenuo es subóptimo en el sentido de DBC y tiene varias limitaciones adicionales que no existen en el marco general de DBC (discutido en la Sección 4.2). 4.1 EMT de múltiples objetivos. En ocasiones, puede haber varias preferencias de comportamiento. Por ejemplo, en el caso de los guardias de museo, algunos objetos de arte están más fuertemente custodiados, lo que requiere que los guardias permanezcan más a menudo en su cercanía. Por otro lado, no se debe dejar ningún rincón del museo sin revisar, lo que requiere un movimiento constante. El éxito de la seguridad en un museo requeriría que los guardias se adhieran y equilibren ambos comportamientos. Para el control basado en EMT, esto significaría enfrentar varios objetivos tácticos {qk}K k=1, y la pregunta sería cómo fusionar y equilibrarlos. Un mecanismo de equilibrio se puede aplicar para resolver este problema. Ten en cuenta que el control basado en EMT, al seleccionar una acción, crea un vector de preferencia sobre el conjunto de acciones basado en su rendimiento predicho con respecto a un objetivo dado. Si estos vectores de preferencia están normalizados, pueden combinarse en una única preferencia unificada. Esto requiere reemplazar la selección de acciones basada en EMT estándar por el algoritmo a continuación [15]: • Dado: - un conjunto de dinámicas objetivo {qk}K k=1, - vector de pesos w(k) • Seleccionar la acción de la siguiente manera - Para cada acción a ∈ A predecir la distribución de estado futuro ¯pa t+1 = Ta ∗ pt; - Para cada acción, calcular Da = H(¯pa t+1, pt, PDt) - Para cada a ∈ A y objetivo táctico qk, denotar V (a, k) = DKL (Da qk) pt. Sea Vk(a) = 1 Zk V (a, k), donde Zk = a∈A V (a, k) es un factor de normalización. - Selecciona a∗ = arg min a k k=1 w(k)Vk(a) El vector de pesos w = (w1, ..., wK ) permite ajustar la importancia entre las dinámicas objetivo sin necesidad de rediseñar los objetivos mismos. Este método de equilibrio también está integrado de forma transparente en el flujo de operación basado en EMT. Limitaciones del Control Basado en EMT El control basado en EMT es una representación subóptima (en el sentido de DBC) de la estructura DBC. Limita al Usuario al obligar a EMT a ser su algoritmo de seguimiento dinámico, y reemplaza la optimización del Agente por la selección de acciones codiciosa. Este tipo de combinación, sin embargo, es común en algoritmos en línea. Aunque es necesario un mayor desarrollo de los controladores basados en EMT, la evidencia hasta ahora sugiere que incluso la forma más simple del algoritmo posee una gran cantidad de potencia y muestra tendencias que son óptimas en el sentido de DBC. Hay dos limitaciones adicionales específicas de los técnicos en emergencias médicas (EMT, por sus siglas en inglés) al control basado en EMT que son evidentes en este momento. Ambos ya tienen soluciones parciales y son objeto de investigación continua. La primera limitación es el problema de la preferencia negativa. En el marco de POMDP, por ejemplo, esto se captura de manera sencilla, a través de The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 793 la aparición de valores con signos diferentes dentro de la estructura de recompensa. Para el control basado en EMT, sin embargo, la preferencia negativa significa que uno quisiera evitar cierta distribución sobre las secuencias de desarrollo del sistema; sin embargo, el control basado en EMT se concentra en acercarse lo más posible a una distribución. La evitación es, por lo tanto, antinatural en el control basado en EMT nativo. La segunda limitación proviene del hecho de que el modelado del entorno estándar puede crear acciones sensoriales puras, acciones que no cambian el estado del mundo y solo difieren en la forma en que se reciben las observaciones y la calidad de las observaciones recibidas. Dado que el estado del mundo no cambia, el control basado en EMT no sería capaz de diferenciar entre diferentes acciones sensoriales. Ten en cuenta que ambas limitaciones del control basado en EMT están ausentes en el marco general de DBC, ya que puede tener un algoritmo de seguimiento capaz de considerar acciones sensoriales puras y, a diferencia de la divergencia de Kullback-Leibler, una medida de desviación de distribución capaz de manejar preferencias negativas. 5. El juego de la \"carrera de relevos\" fue introducido por primera vez en [11]. Es un problema de agente único de capturar una presa, y pertenece a la clase de problemas de barrido de área. Un ejemplo de dominio se muestra en la Figura 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figura 2: Dominio de etiquetas; un agente (A) intenta buscar y capturar una presa (Q). El juego de etiquetas limita extremadamente la percepción de los agentes, de modo que el agente solo puede detectar la presa si están ubicados en la misma celda del mundo de cuadrícula. En la versión clásica del juego, la co-locación conduce a una observación especial, y la acción de Etiqueta se puede realizar. Modificamos ligeramente esta configuración: el momento en que ambos agentes ocupan la misma celda, el juego termina. Como resultado, tanto el agente como su presa tienen la misma capacidad de movimiento, lo que les permite moverse en cuatro direcciones, Norte, Sur, Este y Oeste. Estos forman un espacio formal de acciones dentro de un entorno markoviano. El espacio de estados del entorno formal markoviano está descrito por el producto cruz de las posiciones del agente y la presa. Para la Figura 2, sería S = {s0, ..., s23} × {s0, ..., s23}. Los efectos de una acción tomada por el agente son deterministas, pero el entorno en general tiene una respuesta estocástica debido al movimiento de la presa. Con probabilidad q0 1 se queda en su lugar, y con probabilidad 1 − q0 se mueve a una celda adyacente más lejos del agente 1. En nuestros experimentos, esto se tomó como q0 = 0.2. Por lo tanto, para la instancia mostrada en la Figura 2 y q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Aunque el comportamiento evasivo de la presa es conocido por el agente, la posición de la presa no lo es. La única información sensorial disponible para el agente es su propia ubicación. Utilizamos EMT y especificamos directamente la dinámica del objetivo. Para el juego de la mancha, podemos formular fácilmente tres tendencias principales: atrapar al perseguido, mantenerse en movimiento y acechar al perseguido. Esto resulta en las siguientes tres dinámicas de objetivo: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 en otro caso Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 en otro caso Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Nótese que ninguno de los objetivos anteriores es directamente alcanzable; por ejemplo, si Qt = s9 y At = s11, no hay ninguna acción que pueda llevar al agente a At+1 = s9 como lo requiere la dinámica de objetivo Tcatch. Realizamos varios experimentos para evaluar el rendimiento de EMT en el Juego de Etiquetas. Se utilizaron tres configuraciones del dominio mostrado en la Figura 3, cada una planteando un desafío diferente para el agente debido a la observabilidad parcial. En cada escenario, se realizó un conjunto de 1000 ejecuciones con un límite de tiempo de 100 pasos. En cada ejecución, la posición inicial tanto del agente como de su presa fue seleccionada al azar; esto significa que, en lo que respecta al agente, la posición inicial de la presa estaba distribuida de forma uniforme en todo el espacio de celdas del dominio. También utilizamos dos variaciones de la función de observabilidad del entorno. En la primera versión, la función de observabilidad mapeaba todas las posiciones articulares del cazador y la presa en la posición del cazador como observación. En el segundo, solo aquellas posiciones conjuntas en las que el cazador y la presa ocupaban ubicaciones diferentes fueron mapeadas en la ubicación del cazador. La segunda versión de hecho utilizó y expresó el hecho de que una vez que el cazador y la presa ocupan la misma celda, el juego termina. Los resultados de estos experimentos se muestran en la Tabla 2. Equilibrando la dinámica de captura, movimiento y seguimiento descrita en la sección anterior mediante el vector de peso [0.8, 0.1, 0.1], EMT logró un rendimiento estable en los tres dominios. Aunque las comparaciones directas son difíciles de hacer, el rendimiento de los EMT mostró una notable eficiencia en comparación con el enfoque POMDP. A pesar de una implementación simple e ineficiente del algoritmo EMT en Matlab, el tiempo de decisión para cualquier paso dado promedió significativamente por debajo de 1 segundo en todos los experimentos. Para el dominio de la arena abierta irregular, que resultó ser el más difícil, se completaron 1000 ejecuciones de experimentos limitadas a 100 pasos cada una, un total de 42411 pasos, en poco menos de 6 horas. Es decir, más de 4 × 104 pasos en línea tomaron una orden de magnitud menos tiempo que la computación sin conexión de la política POMDP en [11]. La importancia de esta diferencia se destaca aún más por el hecho de que, en caso de que cambien los parámetros del modelo del entorno, la naturaleza en línea de EMT le permitiría mantener su rendimiento, mientras que la política POMDP necesitaría ser recalculada, lo que requeriría una vez más un gran costo computacional adicional. También probamos la entropía de la frecuencia de las células de comportamiento, medidas empíricas de los datos de prueba. Como muestran la Figura 4 y la Figura 5, empir794 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 3: Se utilizaron estas configuraciones del espacio de juego de Tag: a) múltiples callejones sin salida, b) arena abierta irregular, c) pasillo circular. Tabla 2: Rendimiento de la solución basada en EMT en tres dominios de juego de Tag y dos modelos de observabilidad: I) cantera de omniposición, II) la cantera no está en la posición de los cazadores. Modelo Dominio Captura% E(Pasos) Tiempo/Paso I Callejones sin salida 100 14.8 72(mSeg) Arena 80.2 42.4 500(mSeg) Círculo 91.4 34.6 187(mSeg) II Callejones sin salida 100 13.2 91(mSeg) Arena 96.8 28.67 396(mSeg) Círculo 94.4 31.63 204(mSeg) La entropía lógica crece con la duración de la interacción. Para las carreras en las que la presa no fue capturada de inmediato, la entropía alcanza valores entre 0.85 y 0.952 para diferentes carreras y escenarios. A medida que el agente busca activamente la presa, la entropía nunca alcanza su máximo. Una característica del gráfico de entropía para el escenario de arena abierta llamó especialmente nuestra atención en el caso del modelo de observación de cantera omniposicional. Cerca del límite máximo de longitud de prueba (100 pasos), la entropía cayó repentinamente. Un análisis adicional de los datos mostró que bajo ciertas circunstancias, se produce un comportamiento fluctuante en el que el agente se enfrenta a versiones igualmente viables del comportamiento de seguir a la presa. Dado que el algoritmo EMT tiene una selección de acciones codiciosa, y el espacio de estados no codifica ninguna forma de compromiso (ni siquiera velocidad o aceleración), el agente queda atrapado dentro de una pequeña porción de celdas. Básicamente, se intenta seguir simultáneamente varios cursos de acción, todos los cuales son consistentes con la dinámica objetivo. Este comportamiento no ocurrió en nuestro segundo modelo de observación, ya que redujo significativamente el conjunto de cursos de acción elegibles, contribuyendo esencialmente a romper el empate entre ellos. 6. DISCUSIÓN El diseño de la solución EMT para el Juego de Etiquetas expone la diferencia fundamental en el enfoque de planificación y control entre EMT o DBC, por un lado, y el enfoque más familiar de POMDP, por otro lado. POMDP define una estructura de recompensa para optimizar e influencia la dinámica del sistema de forma indirecta a través de esa optimización. EMT descarta cualquier esquema de recompensa y, en cambio, mide e influye directamente en la dinámica del sistema. Se calculó la entropía 2 utilizando el logaritmo con base igual al número de ubicaciones posibles dentro del dominio; esto escala adecuadamente la expresión de entropía en el rango [0, 1] para todos los dominios. Por lo tanto, para el Juego de Etiquetas, no buscamos una función de recompensa que codificara y expresara nuestra preferencia sobre el comportamiento de los agentes, sino que establecimos directamente tres preferencias de comportamiento (heurísticas) como base para mantener la dinámica del objetivo. Los datos experimentales muestran que estos objetivos no necesitan ser directamente alcanzables a través de las acciones de los agentes. Sin embargo, aún queda por explorar la relación entre el rendimiento de la EMT y la alcanzabilidad de la dinámica objetivo. Los datos del experimento del juego de etiquetas también revelaron la diferente importancia que DBC y POMDPs otorgan a la formulación del espacio de estados del entorno. Los POMDPs dependen enteramente del mecanismo de maximización de la acumulación de recompensas, es decir, la formación del procedimiento de selección de acciones para lograr la secuenciación de estados necesaria. DBC, por otro lado, tiene dos fuentes de especificación de secuenciación: a través de las propiedades de un procedimiento de selección de acciones y a través de una especificación directa dentro de la dinámica objetivo. La importancia de la segunda fuente fue subrayada por los datos del experimento del Juego de Etiquetas, en el que el algoritmo EMT codicioso, aplicado a una especificación de espacio de estados tipo POMDP, falló, ya que la descripción del objetivo sobre dicho espacio de estados no era capaz de codificar las tendencias de comportamiento necesarias, por ejemplo, la resolución de empates y el compromiso con el movimiento dirigido. Las diferencias estructurales entre DBC (y EMT en particular) y POMDPs impiden la comparación directa de rendimiento y los sitúan en pistas complementarias, cada uno dentro de un nicho adecuado. Por ejemplo, los POMDP podrían ser vistos como una formulación mucho más natural de problemas económicos de toma de decisiones secuenciales, mientras que EMT se ajusta mejor a la demanda continua de cambios estocásticos, como ocurre en muchos problemas de robótica o de agentes incorporados. Las propiedades complementarias de POMDPs y EMT pueden ser aún más explotadas. Existe un interés reciente en utilizar POMDPs en soluciones híbridas [17], en las cuales los POMDPs pueden ser utilizados junto con otros enfoques de control para proporcionar resultados que no son fácilmente alcanzables con ninguno de los enfoques por sí solos. DBC puede ser un socio efectivo en una solución híbrida como esta. Por ejemplo, los POMDP tienen requisitos de tiempo fuera de línea prohibitivamente grandes para el cálculo de políticas, pero pueden ser fácilmente utilizados en entornos más simples para exponer tendencias de comportamiento beneficiosas; esto puede servir como una forma de dinámica objetivo que se proporciona a EMT en un dominio más grande para su operación en línea. 7. CONCLUSIONES Y TRABAJO FUTURO En este artículo, hemos presentado una nueva perspectiva sobre el proceso de planificación y control en <br>entornos estocásticos</br>, en forma del marco de Control Basado en Dinámicas (DBC). DBC formula la tarea de planificar como el apoyo a la dinámica de un sistema objetivo especificado, que describe las propiedades necesarias del cambio dentro del entorno. La optimalidad de los planes de acción de DBC se mide en el Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Calles sin salida 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Círculo Figura 4: Modelo de Observación I: Cantera omniposicional. Desarrollo de entropía con longitud de juego de etiqueta para los tres escenarios experimentales: a) múltiples callejones sin salida, b) arena abierta irregular, c) pasillo circular. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Callejones sin salida 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Círculo Figura 5: Modelo de observación II: cantera no observada en la posición de los cazadores. Desarrollo de la entropía con la longitud del juego de etiquetas para los tres escenarios experimentales: a) múltiples callejones sin salida, b) arena abierta irregular, c) pasillo circular, con respecto a la desviación de la dinámica del sistema real de la dinámica objetivo. Mostramos que una técnica recientemente desarrollada de Seguimiento Markov Extendido (EMT) [13] es una instancia de DBC. De hecho, EMT puede ser visto como un caso específico de la parametrización DBC, que emplea un procedimiento de selección de acciones codicioso. Dado que EMT exhibe las características clave del marco general DBC, así como una complejidad temporal polinómica, utilizamos la versión multitarget de EMT [15] para demostrar que la clase de problemas de barrido de área se presta naturalmente a descripciones basadas en dinámicas, tal como se ejemplifica en nuestros experimentos en el dominio del Juego de Etiquetas. Como se enumera en la Sección 4.2, EMT tiene varias limitaciones, como la dificultad para manejar la preferencia dinámica negativa. Esto evita la aplicación directa de la EMT a problemas como los Juegos de Rendezvous-Evasión (por ejemplo, [6]). Sin embargo, DBC en general no tiene tales limitaciones y permite fácilmente la formulación de juegos de evasión. En trabajos futuros, tenemos la intención de continuar con el desarrollo de controladores basados en dinámica para estos problemas. 8. AGRADECIMIENTO El trabajo de los dos primeros autores fue parcialmente apoyado por la subvención #898/05 de la Fundación para la Ciencia de Israel, y el tercer autor fue parcialmente apoyado por una subvención del Ministerio de Ciencia y Tecnología de Israel. REFERENCIAS [1] R. C. Arkin. Robótica basada en el comportamiento. MIT Press, 1998. [2] J. \n\nMIT Press, 1998. [2] J. A. Bilmes. Un tutorial detallado del algoritmo EM y su aplicación en la estimación de parámetros para mezclas gaussianas y modelos ocultos de Markov. Informe técnico TR-97-021, Departamento de Ingeniería Eléctrica e Informática, Universidad de California en Berkeley, 1998. [3] T. M. Cover y J. A. Thomas. Elementos de teoría de la información. Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz y M. J. Wolverton. Una encuesta de investigación en planificación distribuida y continua. Revista de Inteligencia Artificial, 4:13-22, 1999. [5] V. R. Konda y J. N. Tsitsiklis. Algoritmos Actor-Crítico. Revista SIAM de Control y Optimización, 42(4):1143-1166, 2003. [6] W. S. Lim. Un juego de encuentro-evasión en ubicaciones discretas con aleatorización conjunta. Avances en Probabilidad Aplicada, 29(4):1004-1017, diciembre de 1997. [7] M. L. Littman, T. L. Dean y L. P. Kaelbling. Sobre la complejidad de resolver problemas de decisión de Markov. En Actas de la 11ª Conferencia Anual sobre Incertidumbre en Inteligencia Artificial (UAI-95), páginas 394-402, 1995. [8] O. Madani, S. Hanks y A. Condon. Sobre la indecidibilidad de la planificación probabilística y problemas relacionados de optimización estocástica. Revista de Inteligencia Artificial, 147(1-2):5-34, julio de 2003. [9] R. M. Neal y G. E. Hinton. Una vista del algoritmo EM 796 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) que justifica enfoques incrementales, dispersos y otras variantes. En M. I. Jordan, editor, Aprendizaje en Modelos Gráficos, páginas 355-368. Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez y S. Kraus. Seguridad en sistemas multiagentes mediante la aleatorización de políticas. En Actas de AAMAS 2006, 2006. [11] J. Pineau, G. Gordon y S. Thrun. Iteración de valor basada en puntos: Un algoritmo en cualquier momento para POMDPs. En la Conferencia Conjunta Internacional de Inteligencia Artificial (IJCAI), páginas 1025-1032, agosto de 2003. [12] M. L. Puterman. Procesos de Decisión de Markov. Serie Wiley en Probabilidad y Estadística Matemática: Sección de Probabilidad y Estadística Aplicada. Publicación de Wiley-Interscience, Nueva York, 1994. [13] Z. Rabinovich y J. S. Rosenschein. Seguimiento Markov extendido con una aplicación al control. En el taller sobre seguimiento de agentes: modelando otros agentes a partir de observaciones, en la Tercera Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, páginas 95-100, Nueva York, julio de 2004. [14] Z. Rabinovich y J. S. Rosenschein. Coordinación multiagente mediante Seguimiento Markov Extendido. En la Cuarta Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, páginas 431-438, Utrecht, Países Bajos, julio de 2005. [15] Z. Rabinovich y J. S. Rosenschein. Sobre la respuesta del control basado en EMT a objetivos y modelos interactivos. En la Quinta Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, páginas 465-470, Hakodate, Japón, mayo de 2006. [16] R. F. Stengel. Control óptimo y estimación. Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, y P. Varakantham. Conflictos en el trabajo en equipo: Híbridos para el Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 797 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "reward function": {
            "translated_key": "función de recompensa",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Dynamics Based Control with an Application to Area-Sweeping Problems Zinovi Rabinovich Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel jeff@cs.huji.ac.il Gal A. Kaminka The MAVERICK Group Department of Computer Science Bar Ilan University, Israel galk@cs.biu.ac.il ABSTRACT In this paper we introduce Dynamics Based Control (DBC), an approach to planning and control of an agent in stochastic environments.",
                "Unlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics.",
                "We show that a recently developed planning and control approach, Extended Markov Tracking (EMT) is an instantiation of DBC.",
                "EMT employs greedy action selection to provide an efficient control algorithm in Markovian environments.",
                "We exploit this efficiency in a set of experiments that applied multitarget EMT to a class of area-sweeping problems (searching for moving targets).",
                "We show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation.",
                "Categories and Subject Descriptors I.2.8 [Problem Solving, Control Methods, and Search]: Control Theory; I.2.9 [Robotics]; I.2.11 [Distributed Artificial Intelligence]: Intelligent Agents General Terms Algorithms, Theory 1.",
                "INTRODUCTION Planning and control constitutes a central research area in multiagent systems and artificial intelligence.",
                "In recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in stochastic environments.",
                "In this framework, the planning and control problem is often addressed by imposing a <br>reward function</br>, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility.",
                "While theoretically attractive, the complexity of optimally solving a POMDP is prohibitive [8, 7].",
                "We take an alternative view of planning in stochastic environments.",
                "We do not use a (state-based) <br>reward function</br>, but instead optimize over a different criterion, a transition-based specification of the desired system dynamics.",
                "The idea here is to view planexecution as a process that compels a (stochastic) system to change, and a plan as a dynamic process that shapes that change according to desired criteria.",
                "We call this general planning framework Dynamics Based Control (DBC).",
                "In DBC, the goal of a planning (or control) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) target dynamics.",
                "As actual system behavior may deviate from that which is specified by target dynamics (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16].",
                "Here, optimality is measured in terms of probability of deviation magnitudes.",
                "In this paper, we present the structure of Dynamics Based Control.",
                "We show that the recently developed Extended Markov Tracking (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC.",
                "EMT is an efficient instantiation of DBC.",
                "To evaluate DBC, we carried out a set of experiments applying multi-target EMT to the Tag Game [11]; this is a variant on the area sweeping problem, where an agent is trying to tag a moving target (quarry) whose position is not known with certainty.",
                "Experimental data demonstrates that even with a simple model of the environment and a simple design of target dynamics, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agents position).",
                "The paper is organized as follows.",
                "In Section 2 we motivate DBC using area-sweeping problems, and discuss related work.",
                "Section 3 introduces the Dynamics Based Control (DBC) structure, and its specialization to Markovian environments.",
                "This is followed by a review of the Extended Markov Tracking (EMT) approach as a DBC-structured control regimen in Section 4.",
                "That section also discusses the limitations of EMT-based control relative to the general DBC framework.",
                "Experimental settings and results are then presented in Section 5.",
                "Section 6 provides a short discussion of the overall approach, and Section 7 gives some concluding remarks and directions for future work. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "MOTIVATION AND RELATED WORK Many real-life scenarios naturally have a stochastic target dynamics specification, especially those domains where there exists no ultimate goal, but rather system behavior (with specific properties) that has to be continually supported.",
                "For example, security guards perform persistent sweeps of an area to detect any sign of intrusion.",
                "Cunning thieves will attempt to track these sweeps, and time their operation to key points of the guards motion.",
                "It is thus advisable to make the guards motion dynamics appear irregular and random.",
                "Recent work by Paruchuri et al. [10] has addressed such randomization in the context of single-agent and distributed POMDPs.",
                "The goal in that work was to generate policies that provide a measure of action-selection randomization, while maintaining rewards within some acceptable levels.",
                "Our focus differs from this work in that DBC does not optimize expected rewards-indeed we do not consider rewards at all-but instead maintains desired dynamics (including, but not limited to, randomization).",
                "The Game of Tag is another example of the applicability of the approach.",
                "It was introduced in the work by Pineau et al. [11].",
                "There are two agents that can move about an area, which is divided into a grid.",
                "The grid may have blocked cells (holes) into which no agent can move.",
                "One agent (the hunter) seeks to move into a cell occupied by the other (the quarry), such that they are co-located (this is a successful tag).",
                "The quarry seeks to avoid the hunter agent, and is always aware of the hunters position, but does not know how the hunter will behave, which opens up the possibility for a hunter to surprise the prey.",
                "The hunter knows the quarrys probabilistic law of motion, but does not know its current location.",
                "Tag is an instance of a family of area-sweeping (pursuit-evasion) problems.",
                "In [11], the hunter modeled the problem using a POMDP.",
                "A <br>reward function</br> was defined, to reflect the desire to tag the quarry, and an action policy was computed to optimize the reward collected over time.",
                "Due to the intractable complexity of determining the optimal policy, the action policy computed in that paper was essentially an approximation.",
                "In this paper, instead of formulating a <br>reward function</br>, we use EMT to solve the problem, by directly specifying the target dynamics.",
                "In fact, any search problem with randomized motion, the socalled class of area sweeping problems, can be described through specification of such target system dynamics.",
                "Dynamics Based Control provides a natural approach to solving these problems. 3.",
                "DYNAMICS BASED CONTROL The specification of Dynamics Based Control (DBC) can be broken into three interacting levels: Environment Design Level, User Level, and Agent Level. • Environment Design Level is concerned with the formal specification and modeling of the environment.",
                "For example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation constant. • User Level in turn relies on the environment model produced by Environment Design to specify the target system dynamics it wishes to observe.",
                "The User Level also specifies the estimation or learning procedure for system dynamics, and the measure of deviation.",
                "In the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping. • Agent Level in turn combines the environment model from the Environment Design level, the dynamics estimation procedure, the deviation measure and the target dynamics specification from User Level, to produce a sequence of actions that create system dynamics as close as possible to the targeted specification.",
                "As we are interested in the continual development of a stochastic system, such as happens in classical control theory [16] and continual planning [4], as well as in our example of museum sweeps, the question becomes how the Agent Level is to treat the deviation measurements over time.",
                "To this end, we use a probability threshold-that is, we would like the Agent Level to maximize the probability that the deviation measure will remain below a certain threshold.",
                "Specific action selection then depends on system formalization.",
                "One possibility would be to create a mixture of available system trends, much like that which happens in Behavior-Based Robotic architectures [1].",
                "The other alternative would be to rely on the estimation procedure provided by the User Level-to utilize the Environment Design Level model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a certain dynamics has been achieved.",
                "Notice that this manipulation is not direct, but via the environment.",
                "Thus, for strong enough estimator algorithms, successful manipulation would mean a successful simulation of the specified target dynamics (i.e., beyond discerning via the available sensory input).",
                "DBC levels can also have a back-flow of information (see Figure 1).",
                "For instance, the Agent Level could provide data about target dynamics feasibility, allowing the User Level to modify the requirement, perhaps focusing on attainable features of system behavior.",
                "Data would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the User Level, this can provide an important tool for the environment model calibration at the Environment Design Level.",
                "UserEnv.",
                "Design Agent Model Ideal Dynamics Estimator Estimator Dynamics Feasibility System Response Data Figure 1: Data flow of the DBC framework Extending upon the idea of Actor-Critic algorithms [5], DBC data flow can provide a good basis for the design of a learning algorithm.",
                "For example, the User Level can operate as an exploratory device for a learning algorithm, inferring an ideal dynamics target from the environment model at hand that would expose and verify most critical features of system behavior.",
                "In this case, feasibility and system response data from the Agent Level would provide key information for an environment model update.",
                "In fact, the combination of feasibility and response data can provide a basis for the application of strong learning algorithms such as EM [2, 9]. 3.1 DBC for Markovian Environments For a Partially Observable Markovian Environment, DBC can be specified in a more rigorous manner.",
                "Notice how DBC discards rewards, and replaces it by another optimality criterion (structural differences are summarized in Table 1): • Environment Design level is to specify a tuple < S, A, T, O, Ω, s0 >, where: - S is the set of all possible environment states; - s0 is the initial state of the environment (which can also be viewed as a probability distribution over S); The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 791 - A is the set of all possible actions applicable in the environment; - T is the environments probabilistic transition function: T : S ×A → Π(S).",
                "That is, T(s |a, s) is the probability that the environment will move from state s to state s under action a; - O is the set of all possible observations.",
                "This is what the sensor input would look like for an outside observer; - Ω is the observation probability function: Ω : S × A × S → Π(O).",
                "That is, Ω(o|s , a, s) is the probability that one will observe o given that the environment has moved from state s to state s under action a. • User Level, in the case of Markovian environment, operates on the set of system dynamics described by a family of conditional probabilities F = {τ : S × A → Π(S)}.",
                "Thus specification of target dynamics can be expressed by q ∈ F, and the learning or tracking algorithm can be represented as a function L : O×(A×O)∗ → F; that is, it maps sequences of observations and actions performed so far into an estimate τ ∈ F of system dynamics.",
                "There are many possible variations available at the User Level to define divergence between system dynamics; several of them are: - Trace distance or L1 distance between two distributions p and q defined by D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Fidelity measure of distance F(p(·), q(·)) = x p(x)q(x) - Kullback-Leibler divergence DKL(p(·) q(·)) = x p(x) log p(x) q(x) Notice that the latter two are not actually metrics over the space of possible distributions, but nevertheless have meaningful and important interpretations.",
                "For instance, KullbackLeibler divergence is an important tool of information theory [3] that allows one to measure the price of encoding an information source governed by q, while assuming that it is governed by p. The User Level also defines the threshold of dynamics deviation probability θ. • Agent Level is then faced with a problem of selecting a control signal function a∗ to satisfy a minimization problem as follows: a∗ = arg min a Pr(d(τa, q) > θ) where d(τa, q) is a random variable describing deviation of the dynamics estimate τa, created by L under control signal a, from the ideal dynamics q.",
                "Implicit in this minimization problem is that L is manipulated via the environment, based on the environment model produced by the Environment Design Level. 3.2 DBC View of the State Space It is important to note the complementary view that DBC and POMDPs take on the state space of the environment.",
                "POMDPs regard state as a stationary snap-shot of the environment; whatever attributes of state sequencing one seeks are reached through properties of the control process, in this case reward accumulation.",
                "This can be viewed as if the sequencing of states and the attributes of that sequencing are only introduced by and for the controlling mechanism-the POMDP policy.",
                "DBC concentrates on the underlying principle of state sequencing, the system dynamics.",
                "DBCs target dynamics specification can use the environments state space as a means to describe, discern, and preserve changes that occur within the system.",
                "As a result, DBC has a greater ability to express state sequencing properties, which are grounded in the environment model and its state space definition.",
                "For example, consider the task of moving through rough terrain towards a goal and reaching it as fast as possible.",
                "POMDPs would encode terrain as state space points, while speed would be ensured by negative reward for every step taken without reaching the goalaccumulating higher reward can be reached only by faster motion.",
                "Alternatively, the state space could directly include the notion of speed.",
                "For POMDPs, this would mean that the same concept is encoded twice, in some sense: directly in the state space, and indirectly within reward accumulation.",
                "Now, even if the <br>reward function</br> would encode more, and finer, details of the properties of motion, the POMDP solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure.",
                "On the other hand, the tactical target expression of variations and correlations between position and speed of motion is now grounded in the state space representation.",
                "In this situation, any further constraints, e.g., smoothness of motion, speed limits in different locations, or speed reductions during sharp turns, are explicitly and uniformly expressed by the tactical target, and can result in faster and more effective action selection by a DBC algorithm. 4.",
                "EMT-BASED CONTROL AS A DBC Recently, a control algorithm was introduced called EMT-based Control [13], which instantiates the DBC framework.",
                "Although it provides an approximate greedy solution in the DBC sense, initial experiments using EMT-based control have been encouraging [14, 15].",
                "EMT-based control is based on the Markovian environment definition, as in the case with POMDPs, but its User and Agent Levels are of the Markovian DBC type of optimality. • User Level of EMT-based control defines a limited-case target system dynamics independent of action: qEMT : S → Π(S).",
                "It then utilizes the Kullback-Leibler divergence measure to compose a momentary system dynamics estimator-the Extended Markov Tracking (EMT) algorithm.",
                "The algorithm keeps a system dynamics estimate τt EMT that is capable of explaining recent change in an auxiliary Bayesian system state estimator from pt−1 to pt, and updates it conservatively using Kullback-Leibler divergence.",
                "Since τt EMT and pt−1,t are respectively the conditional and marginal probabilities over the systems state space, explanation simply means that pt(s ) = s τt EMT (s |s)pt−1(s), and the dynamics estimate update is performed by solving a 792 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: Structure of POMDP vs. Dynamics-Based Control in Markovian Environment Level Approach MDP Markovian DBC Environment < S, A, T, O, Ω >,where S - set of states A - set of actions Design T : S × A → Π(S) - transition O - observation set Ω : S × A × S → Π(O) User r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - <br>reward function</br> q - ideal dynamics F - reward remodeling L - dynamics estimator θ - threshold Agent π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) minimization problem: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • Agent Level in EMT-based control is suboptimal with respect to DBC (though it remains within the DBC framework), performing greedy action selection based on prediction of EMTs reaction.",
                "The prediction is based on the environment model provided by the Environment Design level, so that if we denote by Ta the environments transition function limited to action a, and pt−1 is the auxiliary Bayesian system state estimator, then the EMT-based control choice is described by a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1) Note that this follows the Markovian DBC framework precisely: the rewarding optimality of POMDPs is discarded, and in its place a dynamics estimator (EMT in this case) is manipulated via action effects on the environment to produce an estimate close to the specified target system dynamics.",
                "Yet as we mentioned, naive EMTbased control is suboptimal in the DBC sense, and has several additional limitations that do not exist in the general DBC framework (discussed in Section 4.2). 4.1 Multi-Target EMT At times, there may exist several behavioral preferences.",
                "For example, in the case of museum guards, some art items are more heavily guarded, requiring that the guards stay more often in their close vicinity.",
                "On the other hand, no corner of the museum is to be left unchecked, which demands constant motion.",
                "Successful museum security would demand that the guards adhere to, and balance, both of these behaviors.",
                "For EMT-based control, this would mean facing several tactical targets {qk}K k=1, and the question becomes how to merge and balance them.",
                "A balancing mechanism can be applied to resolve this issue.",
                "Note that EMT-based control, while selecting an action, creates a preference vector over the set of actions based on their predicted performance with respect to a given target.",
                "If these preference vectors are normalized, they can be combined into a single unified preference.",
                "This requires replacement of standard EMT-based action selection by the algorithm below [15]: • Given: - a set of target dynamics {qk}K k=1, - vector of weights w(k) • Select action as follows - For each action a ∈ A predict the future state distribution ¯pa t+1 = Ta ∗ pt; - For each action, compute Da = H(¯pa t+1, pt, PDt) - For each a ∈ A and qk tactical target, denote V (a, k) = DKL (Da qk) pt .",
                "Let Vk(a) = 1 Zk V (a, k), where Zk = a∈A V (a, k) is a normalization factor. - Select a∗ = arg min a k k=1 w(k)Vk(a) The weights vector w = (w1, ..., wK ) allows the additional tuning of importance among target dynamics without the need to redesign the targets themselves.",
                "This balancing method is also seamlessly integrated into the EMT-based control flow of operation. 4.2 EMT-based Control Limitations EMT-based control is a sub-optimal (in the DBC sense) representative of the DBC structure.",
                "It limits the User by forcing EMT to be its dynamic tracking algorithm, and replaces Agent optimization by greedy action selection.",
                "This kind of combination, however, is common for on-line algorithms.",
                "Although further development of EMT-based controllers is necessary, evidence so far suggests that even the simplest form of the algorithm possesses a great deal of power, and displays trends that are optimal in the DBC sense of the word.",
                "There are two further, EMT-specific, limitations to EMT-based control that are evident at this point.",
                "Both already have partial solutions and are subjects of ongoing research.",
                "The first limitation is the problem of negative preference.",
                "In the POMDP framework for example, this is captured simply, through The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 793 the appearance of values with different signs within the reward structure.",
                "For EMT-based control, however, negative preference means that one would like to avoid a certain distribution over system development sequences; EMT-based control, however, concentrates on getting as close as possible to a distribution.",
                "Avoidance is thus unnatural in native EMT-based control.",
                "The second limitation comes from the fact that standard environment modeling can create pure sensory actions-actions that do not change the state of the world, and differ only in the way observations are received and the quality of observations received.",
                "Since the world state does not change, EMT-based control would not be able to differentiate between different sensory actions.",
                "Notice that both of these limitations of EMT-based control are absent from the general DBC framework, since it may have a tracking algorithm capable of considering pure sensory actions and, unlike Kullback-Leibler divergence, a distribution deviation measure that is capable of dealing with negative preference. 5.",
                "EMT PLAYING TAG The Game of Tag was first introduced in [11].",
                "It is a single agent problem of capturing a quarry, and belongs to the class of area sweeping problems.",
                "An example domain is shown in Figure 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figure 2: Tag domain; an agent (A) attempts to seek and capture a quarry (Q) The Game of Tag extremely limits the agents perception, so that the agent is able to detect the quarry only if they are co-located in the same cell of the grid world.",
                "In the classical version of the game, co-location leads to a special observation, and the Tag action can be performed.",
                "We slightly modify this setting: the moment that both agents occupy the same cell, the game ends.",
                "As a result, both the agent and its quarry have the same motion capability, which allows them to move in four directions, North, South, East, and West.",
                "These form a formal space of actions within a Markovian environment.",
                "The state space of the formal Markovian environment is described by the cross-product of the agent and quarrys positions.",
                "For Figure 2, it would be S = {s0, ..., s23} × {s0, ..., s23}.",
                "The effects of an action taken by the agent are deterministic, but the environment in general has a stochastic response due to the motion of the quarry.",
                "With probability q0 1 it stays put, and with probability 1 − q0 it moves to an adjacent cell further away from the 1 In our experiments this was taken to be q0 = 0.2. agent.",
                "So for the instance shown in Figure 2 and q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Although the evasive behavior of the quarry is known to the agent, the quarrys position is not.",
                "The only sensory information available to the agent is its own location.",
                "We use EMT and directly specify the target dynamics.",
                "For the Game of Tag, we can easily formulate three major trends: catching the quarry, staying mobile, and stalking the quarry.",
                "This results in the following three target dynamics: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 otherwise Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 otherwise Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Note that none of the above targets are directly achievable; for instance, if Qt = s9 and At = s11, there is no action that can move the agent to At+1 = s9 as required by the Tcatch target dynamics.",
                "We ran several experiments to evaluate EMT performance in the Tag Game.",
                "Three configurations of the domain shown in Figure 3 were used, each posing a different challenge to the agent due to partial observability.",
                "In each setting, a set of 1000 runs was performed with a time limit of 100 steps.",
                "In every run, the initial position of both the agent and its quarry was selected at random; this means that as far as the agent was concerned, the quarrys initial position was uniformly distributed over the entire domain cell space.",
                "We also used two variations of the environment observability function.",
                "In the first version, observability function mapped all joint positions of hunter and quarry into the position of the hunter as an observation.",
                "In the second, only those joint positions in which hunter and quarry occupied different locations were mapped into the hunters location.",
                "The second version in fact utilized and expressed the fact that once hunter and quarry occupy the same cell the game ends.",
                "The results of these experiments are shown in Table 2.",
                "Balancing [15] the catch, move, and stalk target dynamics described in the previous section by the weight vector [0.8, 0.1, 0.1], EMT produced stable performance in all three domains.",
                "Although direct comparisons are difficult to make, the EMT performance displayed notable efficiency vis-`a-vis the POMDP approach.",
                "In spite of a simple and inefficient Matlab implementation of the EMT algorithm, the decision time for any given step averaged significantly below 1 second in all experiments.",
                "For the irregular open arena domain, which proved to be the most difficult, 1000 experiment runs bounded by 100 steps each, a total of 42411 steps, were completed in slightly under 6 hours.",
                "That is, over 4 × 104 online steps took an order of magnitude less time than the offline computation of POMDP policy in [11].",
                "The significance of this differential is made even more prominent by the fact that, should the environment model parameters change, the online nature of EMT would allow it to maintain its performance time, while the POMDP policy would need to be recomputed, requiring yet again a large overhead of computation.",
                "We also tested the behavior cell frequency entropy, empirical measures from trial data.",
                "As Figure 4 and Figure 5 show, empir794 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) A Q Q A 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 A Q Figure 3: These configurations of the Tag Game space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor Table 2: Performance of the EMT-based solution in three Tag Game domains and two observability models: I) omniposition quarry, II) quarry is not at hunters position Model Domain Capture% E(Steps) Time/Step I Dead-ends 100 14.8 72(mSec) Arena 80.2 42.4 500(mSec) Circle 91.4 34.6 187(mSec) II Dead-ends 100 13.2 91(mSec) Arena 96.8 28.67 396(mSec) Circle 94.4 31.63 204(mSec) ical entropy grows with the length of interaction.",
                "For runs where the quarry was not captured immediately, the entropy reaches between 0.85 and 0.952 for different runs and scenarios.",
                "As the agent actively seeks the quarry, the entropy never reaches its maximum.",
                "One characteristic of the entropy graph for the open arena scenario particularly caught our attention in the case of the omniposition quarry observation model.",
                "Near the maximum limit of trial length (100 steps), entropy suddenly dropped.",
                "Further analysis of the data showed that under certain circumstances, a fluctuating behavior occurs in which the agent faces equally viable versions of quarry-following behavior.",
                "Since the EMT algorithm has greedy action selection, and the state space does not encode any form of commitment (not even speed or acceleration), the agent is locked within a small portion of cells.",
                "It is essentially attempting to simultaneously follow several courses of action, all of which are consistent with the target dynamics.",
                "This behavior did not occur in our second observation model, since it significantly reduced the set of eligible courses of action-essentially contributing to tie-breaking among them. 6.",
                "DISCUSSION The design of the EMT solution for the Tag Game exposes the core difference in approach to planning and control between EMT or DBC, on the one hand, and the more familiar POMDP approach, on the other.",
                "POMDP defines a reward structure to optimize, and influences system dynamics indirectly through that optimization.",
                "EMT discards any reward scheme, and instead measures and influences system dynamics directly. 2 Entropy was calculated using log base equal to the number of possible locations within the domain; this properly scales entropy expression into the range [0, 1] for all domains.",
                "Thus for the Tag Game, we did not search for a <br>reward function</br> that would encode and express our preference over the agents behavior, but rather directly set three (heuristic) behavior preferences as the basis for target dynamics to be maintained.",
                "Experimental data shows that these targets need not be directly achievable via the agents actions.",
                "However, the ratio between EMT performance and achievability of target dynamics remains to be explored.",
                "The tag game experiment data also revealed the different emphasis DBC and POMDPs place on the formulation of an environment state space.",
                "POMDPs rely entirely on the mechanism of reward accumulation maximization, i.e., formation of the action selection procedure to achieve necessary state sequencing.",
                "DBC, on the other hand, has two sources of sequencing specification: through the properties of an action selection procedure, and through direct specification within the target dynamics.",
                "The importance of the second source was underlined by the Tag Game experiment data, in which the greedy EMT algorithm, applied to a POMDP-type state space specification, failed, since target description over such a state space was incapable of encoding the necessary behavior tendencies, e.g., tie-breaking and commitment to directed motion.",
                "The structural differences between DBC (and EMT in particular), and POMDPs, prohibits direct performance comparison, and places them on complementary tracks, each within a suitable niche.",
                "For instance, POMDPs could be seen as a much more natural formulation of economic sequential decision-making problems, while EMT is a better fit for continual demand for stochastic change, as happens in many robotic or embodied-agent problems.",
                "The complementary properties of POMDPs and EMT can be further exploited.",
                "There is recent interest in using POMDPs in hybrid solutions [17], in which the POMDPs can be used together with other control approaches to provide results not easily achievable with either approach by itself.",
                "DBC can be an effective partner in such a hybrid solution.",
                "For instance, POMDPs have prohibitively large off-line time requirements for policy computation, but can be readily used in simpler settings to expose beneficial behavioral trends; this can serve as a form of target dynamics that are provided to EMT in a larger domain for on-line operation. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have presented a novel perspective on the process of planning and control in stochastic environments, in the form of the Dynamics Based Control (DBC) framework.",
                "DBC formulates the task of planning as support of a specified target system dynamics, which describes the necessary properties of change within the environment.",
                "Optimality of DBC plans of action are measured The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 4: Observation Model I: Omniposition quarry.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 5: Observation Model II: quarry not observed at hunters position.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. with respect to the deviation of actual system dynamics from the target dynamics.",
                "We show that a recently developed technique of Extended Markov Tracking (EMT) [13] is an instantiation of DBC.",
                "In fact, EMT can be seen as a specific case of DBC parameterization, which employs a greedy action selection procedure.",
                "Since EMT exhibits the key features of the general DBC framework, as well as polynomial time complexity, we used the multitarget version of EMT [15] to demonstrate that the class of area sweeping problems naturally lends itself to dynamics-based descriptions, as instantiated by our experiments in the Tag Game domain.",
                "As enumerated in Section 4.2, EMT has a number of limitations, such as difficulty in dealing with negative dynamic preference.",
                "This prevents direct application of EMT to such problems as Rendezvous-Evasion Games (e.g., [6]).",
                "However, DBC in general has no such limitations, and readily enables the formulation of evasion games.",
                "In future work, we intend to proceed with the development of dynamics-based controllers for these problems. 8.",
                "ACKNOWLEDGMENT The work of the first two authors was partially supported by Israel Science Foundation grant #898/05, and the third author was partially supported by a grant from Israels Ministry of Science and Technology. 9.",
                "REFERENCES [1] R. C. Arkin.",
                "Behavior-Based Robotics.",
                "MIT Press, 1998. [2] J.",
                "A. Bilmes.",
                "A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and Hidden Markov Models.",
                "Technical Report TR-97-021, Department of Electrical Engeineering and Computer Science, University of California at Berkeley, 1998. [3] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of information theory.",
                "Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz, and M. J. Wolverton.",
                "A survey of research in distributed, continual planning.",
                "AI Magazine, 4:13-22, 1999. [5] V. R. Konda and J. N. Tsitsiklis.",
                "Actor-Critic algorithms.",
                "SIAM Journal on Control and Optimization, 42(4):1143-1166, 2003. [6] W. S. Lim.",
                "A rendezvous-evasion game on discrete locations with joint randomization.",
                "Advances in Applied Probability, 29(4):1004-1017, December 1997. [7] M. L. Littman, T. L. Dean, and L. P. Kaelbling.",
                "On the complexity of solving Markov decision problems.",
                "In Proceedings of the 11th Annual Conference on Uncertainty in Artificial Intelligence (UAI-95), pages 394-402, 1995. [8] O. Madani, S. Hanks, and A. Condon.",
                "On the undecidability of probabilistic planning and related stochastic optimization problems.",
                "Artificial Intelligence Journal, 147(1-2):5-34, July 2003. [9] R. M. Neal and G. E. Hinton.",
                "A view of the EM algorithm 796 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) that justifies incremental, sparse, and other variants.",
                "In M. I. Jordan, editor, Learning in Graphical Models, pages 355-368.",
                "Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In Proceeding of AAMAS 2006, 2006. [11] J. Pineau, G. Gordon, and S. Thrun.",
                "Point-based value iteration: An anytime algorithm for pomdps.",
                "In International Joint Conference on Artificial Intelligence (IJCAI), pages 1025-1032, August 2003. [12] M. L. Puterman.",
                "Markov Decision Processes.",
                "Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics Section.",
                "Wiley-Interscience Publication, New York, 1994. [13] Z. Rabinovich and J. S. Rosenschein.",
                "Extended Markov Tracking with an application to control.",
                "In The Workshop on Agent Tracking: Modeling Other Agents from Observations, at the Third International Joint Conference on Autonomous Agents and Multiagent Systems, pages 95-100, New-York, July 2004. [14] Z. Rabinovich and J. S. Rosenschein.",
                "Multiagent coordination by Extended Markov Tracking.",
                "In The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 431-438, Utrecht, The Netherlands, July 2005. [15] Z. Rabinovich and J. S. Rosenschein.",
                "On the response of EMT-based control to interacting targets and models.",
                "In The Fifth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 465-470, Hakodate, Japan, May 2006. [16] R. F. Stengel.",
                "Optimal Control and Estimation.",
                "Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, and P. Varakantham.",
                "Conflicts in teamwork: Hybrids to the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 797"
            ],
            "original_annotated_samples": [
                "In this framework, the planning and control problem is often addressed by imposing a <br>reward function</br>, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility.",
                "We do not use a (state-based) <br>reward function</br>, but instead optimize over a different criterion, a transition-based specification of the desired system dynamics.",
                "A <br>reward function</br> was defined, to reflect the desire to tag the quarry, and an action policy was computed to optimize the reward collected over time.",
                "In this paper, instead of formulating a <br>reward function</br>, we use EMT to solve the problem, by directly specifying the target dynamics.",
                "Now, even if the <br>reward function</br> would encode more, and finer, details of the properties of motion, the POMDP solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure."
            ],
            "translated_annotated_samples": [
                "En este marco, el problema de planificación y control suele abordarse imponiendo una <br>función de recompensa</br> y calculando una política (de elección de acciones) que sea óptima, en el sentido de que resultará en la mayor utilidad esperada.",
                "No utilizamos una <br>función de recompensa</br> basada en el estado, sino que optimizamos según un criterio diferente, una especificación basada en transiciones de la dinámica del sistema deseado.",
                "Se definió una <br>función de recompensa</br> para reflejar el deseo de marcar la presa, y se calculó una política de acción para optimizar la recompensa recolectada con el tiempo.",
                "En este artículo, en lugar de formular una <br>función de recompensa</br>, utilizamos EMT para resolver el problema, especificando directamente la dinámica objetivo.",
                "Ahora, incluso si la <br>función de recompensa</br> codificara más y detalles más finos de las propiedades del movimiento, la solución POMDP tendría que buscar en un espacio mucho más grande de políticas, aunque aún esté guiada por el concepto implícito del procedimiento de acumulación de recompensas."
            ],
            "translated_text": "En este artículo presentamos el Control Basado en Dinámicas (DBC), un enfoque para la planificación y control de un agente en entornos estocásticos. A diferencia de los enfoques existentes, que buscan optimizar las recompensas esperadas (por ejemplo, en Problemas de Decisión de Markov Parcialmente Observables (POMDP)), DBC optimiza el comportamiento del sistema hacia las dinámicas del sistema especificadas. Mostramos que un enfoque de planificación y control recientemente desarrollado, Seguimiento Extendido de Markov (EMT), es una instancia de DBC. EMT emplea la selección de acciones codiciosa para proporcionar un algoritmo de control eficiente en entornos markovianos. Explotamos esta eficiencia en un conjunto de experimentos que aplicaron EMT multitarget a una clase de problemas de barrido de área (búsqueda de objetivos en movimiento). Mostramos que tales problemas pueden ser definidos de forma natural y resueltos eficientemente utilizando el marco de DBC y su instanciación de EMT. Categorías y Descriptores de Asignaturas I.2.8 [Resolución de Problemas, Métodos de Control y Búsqueda]: Teoría de Control; I.2.9 [Robótica]; I.2.11 [Inteligencia Artificial Distribuida]: Agentes Inteligentes Términos Generales Algoritmos, Teoría 1. INTRODUCCIÓN La planificación y el control constituyen un área de investigación central en sistemas multiagentes e inteligencia artificial. En los últimos años, los Procesos de Decisión de Markov Parcialmente Observables (POMDPs) [12] se han convertido en una base formal popular para la planificación en entornos estocásticos. En este marco, el problema de planificación y control suele abordarse imponiendo una <br>función de recompensa</br> y calculando una política (de elección de acciones) que sea óptima, en el sentido de que resultará en la mayor utilidad esperada. Si bien teóricamente atractivo, la complejidad de resolver óptimamente un POMDP es prohibitiva [8, 7]. Tomamos una visión alternativa de la planificación en entornos estocásticos. No utilizamos una <br>función de recompensa</br> basada en el estado, sino que optimizamos según un criterio diferente, una especificación basada en transiciones de la dinámica del sistema deseado. La idea aquí es ver la ejecución del plan como un proceso que obliga a un sistema (estocástico) a cambiar, y un plan como un proceso dinámico que moldea ese cambio de acuerdo con criterios deseados. Llamamos a este marco de planificación general Control Basado en Dinámicas (DBC). En DBC, el objetivo de un proceso de planificación (o control) es garantizar que el sistema cambiará de acuerdo con dinámicas objetivo específicas (potencialmente estocásticas). Dado que el comportamiento real del sistema puede desviarse de lo especificado por la dinámica objetivo (debido a la naturaleza estocástica del sistema), la planificación en tales entornos debe ser continua, de manera similar a los controladores en lazo cerrado clásicos. Aquí, la optimalidad se mide en términos de la probabilidad de magnitudes de desviación. En este documento, presentamos la estructura del Control Basado en Dinámicas. Mostramos que el enfoque de Seguimiento Extendido de Markov (EMT) recientemente desarrollado [13, 14, 15] está subsumido por DBC, con EMT empleando selección de acciones codiciosa, que es una parametrización específica entre las opciones posibles dentro de DBC. EMT es una implementación eficiente de DBC. Para evaluar DBC, llevamos a cabo un conjunto de experimentos aplicando EMT de múltiples objetivos al Juego de Etiquetas [11]; esta es una variante del problema de barrido de área, donde un agente intenta etiquetar a un objetivo móvil (presa) cuya posición no se conoce con certeza. Los datos experimentales demuestran que incluso con un modelo simple del entorno y un diseño simple de la dinámica del objetivo, se pueden lograr altas tasas de éxito tanto en la captura de la presa como en sorprender a la presa (como se expresa por la entropía observada de la posición de los agentes controlados). El documento está organizado de la siguiente manera. En la Sección 2 motivamos DBC utilizando problemas de barrido de área, y discutimos trabajos relacionados. La Sección 3 introduce la estructura de Control Basado en Dinámicas (DBC) y su especialización en entornos Markovianos. Esto es seguido por una revisión del enfoque de Seguimiento Markov Extendido (EMT) como un régimen de control estructurado por DBC en la Sección 4. Esa sección también discute las limitaciones del control basado en EMT en relación con el marco general de DBC. Los ajustes experimentales y los resultados se presentan a continuación en la Sección 5. La Sección 6 proporciona una breve discusión del enfoque general, y la Sección 7 ofrece algunas observaciones finales y direcciones para trabajos futuros. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2. MOTIVACIÓN Y TRABAJO RELACIONADO Muchos escenarios de la vida real tienen naturalmente una especificación de dinámica de destino estocástica, especialmente en aquellos dominios donde no existe un objetivo final, sino más bien un comportamiento del sistema (con propiedades específicas) que debe ser continuamente respaldado. Por ejemplo, los guardias de seguridad realizan barridos persistentes de un área para detectar cualquier signo de intrusión. Los ladrones astutos intentarán rastrear estos barridos y sincronizar su operación con puntos clave del movimiento de los guardias. Por lo tanto, es recomendable que la dinámica de movimiento de los guardias parezca irregular y aleatoria. El trabajo reciente de Paruchuri et al. [10] ha abordado dicha aleatorización en el contexto de POMDPs de agente único y distribuidos. El objetivo de ese trabajo era generar políticas que proporcionen una medida de aleatorización en la selección de acciones, manteniendo al mismo tiempo las recompensas dentro de niveles aceptables. Nuestro enfoque difiere de este trabajo en que DBC no optimiza recompensas esperadas, de hecho, no consideramos recompensas en absoluto, sino que en su lugar mantenemos dinámicas deseadas (incluyendo, pero no limitado a, la aleatorización). El juego de la mancha es otro ejemplo de la aplicabilidad del enfoque. Fue introducido en el trabajo de Pineau et al. [11]. Hay dos agentes que pueden moverse por un área, la cual está dividida en una cuadrícula. La cuadrícula puede tener celdas bloqueadas (agujeros) en las cuales ningún agente puede moverse. Un agente (el cazador) intenta moverse a una celda ocupada por el otro (la presa), de modo que estén ubicados en el mismo lugar (esto es una etiqueta exitosa). La presa busca evitar al cazador, siempre está al tanto de la posición del cazador, pero no sabe cómo se comportará el cazador, lo que abre la posibilidad de que un cazador sorprenda a la presa. El cazador conoce la ley de movimiento probabilístico de la presa, pero no conoce su ubicación actual. Tag es una instancia de una familia de problemas de barrido de área (persecución-evasión). En [11], el cazador modeló el problema utilizando un POMDP. Se definió una <br>función de recompensa</br> para reflejar el deseo de marcar la presa, y se calculó una política de acción para optimizar la recompensa recolectada con el tiempo. Debido a la complejidad intratable de determinar la política óptima, la política de acción calculada en ese documento fue esencialmente una aproximación. En este artículo, en lugar de formular una <br>función de recompensa</br>, utilizamos EMT para resolver el problema, especificando directamente la dinámica objetivo. De hecho, cualquier problema de búsqueda con movimiento aleatorio, la llamada clase de problemas de barrido de área, puede ser descrito a través de la especificación de la dinámica de dicho sistema objetivo. El Control Basado en Dinámicas proporciona un enfoque natural para resolver estos problemas. CONTROL BASADO EN DINÁMICA La especificación del Control Basado en Dinámica (DBC) se puede dividir en tres niveles interactivos: Nivel de Diseño del Entorno, Nivel de Usuario y Nivel de Agente. • El Nivel de Diseño del Entorno se ocupa de la especificación formal y modelado del entorno. Por ejemplo, este nivel especificaría las leyes de la física dentro del sistema y establecería sus parámetros, como la constante de gravitación. El Nivel de Usuario, a su vez, se basa en el modelo de entorno producido por el Diseño de Entorno para especificar la dinámica del sistema objetivo que desea observar. El Nivel de Usuario también especifica el procedimiento de estimación o aprendizaje para la dinámica del sistema, y la medida de desviación. En el escenario del guardia de museo mencionado anteriormente, estos corresponderían a un horario de barrido estocástico y una medida de sorpresa relativa entre el barrido especificado y el real. • El Nivel de Agente, a su vez, combina el modelo del entorno del Nivel de Diseño de Entorno, el procedimiento de estimación de la dinámica, la medida de desviación y la especificación de la dinámica objetivo del Nivel de Usuario, para producir una secuencia de acciones que creen dinámicas del sistema lo más cercanas posible a la especificación deseada. Dado nuestro interés en el desarrollo continuo de un sistema estocástico, como ocurre en la teoría de control clásica [16] y la planificación continua [4], así como en nuestro ejemplo de recorridos por museos, la pregunta es cómo el Nivel de Agente debe tratar las mediciones de desviación a lo largo del tiempo. Con este fin, utilizamos un umbral de probabilidad, es decir, nos gustaría que el Nivel del Agente maximice la probabilidad de que la medida de desviación permanezca por debajo de cierto umbral. La selección de acciones específicas depende entonces de la formalización del sistema. Una posibilidad sería crear una mezcla de tendencias de sistemas disponibles, similar a lo que sucede en las arquitecturas robóticas basadas en el comportamiento [1]. La otra alternativa sería depender del procedimiento de estimación proporcionado por el Nivel de Usuario para utilizar el modelo del Nivel de Diseño del Entorno del entorno para elegir acciones, de manera que se manipule al estimador de dinámicas para que crea que se ha logrado cierta dinámica. Ten en cuenta que esta manipulación no es directa, sino a través del entorno. Por lo tanto, para algoritmos de estimación lo suficientemente fuertes, la manipulación exitosa significaría una simulación exitosa de la dinámica objetivo especificada (es decir, más allá de discernir a través de la entrada sensorial disponible). Los niveles de DBC también pueden tener un retroceso de información (ver Figura 1). Por ejemplo, el Nivel de Agente podría proporcionar datos sobre la viabilidad de la dinámica del objetivo, permitiendo al Nivel de Usuario modificar el requisito, quizás centrándose en las características alcanzables del comportamiento del sistema. Los datos también estarían disponibles sobre la respuesta del sistema a diferentes acciones realizadas; combinados con un estimador de dinámica definido por el Nivel de Usuario, esto puede proporcionar una herramienta importante para la calibración del modelo del entorno en el Nivel de Diseño del Entorno. UserEnv. El modelo de agente de diseño Estimador de Dinámicas Ideales Estimador de Dinámicas Factibilidad de Respuesta del Sistema de Datos Figura 1: Flujo de datos del marco de trabajo DBC Ampliando la idea de los algoritmos Actor-Crítico [5], el flujo de datos de DBC puede proporcionar una buena base para el diseño de un algoritmo de aprendizaje. Por ejemplo, el Nivel de Usuario puede funcionar como un dispositivo exploratorio para un algoritmo de aprendizaje, inferir un objetivo dinámico ideal a partir del modelo del entorno en cuestión que expondría y verificaría las características más críticas del comportamiento del sistema. En este caso, los datos de viabilidad y respuesta del sistema a nivel de agente proporcionarían información clave para una actualización del modelo del entorno. De hecho, la combinación de datos de viabilidad y respuesta puede proporcionar una base para la aplicación de algoritmos de aprendizaje sólidos como EM [2, 9]. 3.1 DBC para entornos markovianos Para un entorno markoviano parcialmente observable, DBC puede especificarse de una manera más rigurosa. Observa cómo DBC descarta las recompensas y las reemplaza por otro criterio de optimalidad (las diferencias estructurales se resumen en la Tabla 1): • El nivel de Diseño del Entorno es especificar una tupla < S, A, T, O, Ω, s0 >, donde: - S es el conjunto de todos los posibles estados del entorno; - s0 es el estado inicial del entorno (que también puede ser visto como una distribución de probabilidad sobre S); La Sexta Conferencia Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 791 - A es el conjunto de todas las acciones posibles aplicables en el entorno; - T es la función de transición probabilística del entorno: T: S × A → Π(S). Es decir, T(s | a, s) es la probabilidad de que el entorno se mueva del estado s al estado s bajo la acción a; - O es el conjunto de todas las observaciones posibles. Así es como se vería la entrada del sensor para un observador externo; - Ω es la función de probabilidad de observación: Ω: S × A × S → Π(O). Es decir, Ω(o|s, a, s) es la probabilidad de observar o dado que el entorno ha pasado del estado s al estado s bajo la acción a. • Nivel de Usuario, en el caso de un entorno markoviano, opera en el conjunto de dinámicas del sistema descritas por una familia de probabilidades condicionales F = {τ: S × A → Π(S)}. Por lo tanto, la especificación de la dinámica del objetivo puede expresarse como q ∈ F, y el algoritmo de aprendizaje o seguimiento puede representarse como una función L : O×(A×O)∗ → F; es decir, mapea secuencias de observaciones y acciones realizadas hasta el momento en una estimación τ ∈ F de la dinámica del sistema. Hay muchas variaciones posibles disponibles a nivel de usuario para definir la divergencia entre la dinámica del sistema; varias de ellas son: - Distancia de traza o distancia L1 entre dos distribuciones p y q definida por D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Medida de fidelidad de distancia F(p(·), q(·)) = x p(x)q(x) - Divergencia de Kullback-Leibler DKL(p(·) q(·)) = x p(x) log p(x) q(x) Nótese que las dos últimas no son en realidad métricas sobre el espacio de distribuciones posibles, pero tienen interpretaciones significativas e importantes. Por ejemplo, la divergencia de Kullback-Leibler es una herramienta importante de la teoría de la información [3] que permite medir el costo de codificar una fuente de información gobernada por q, asumiendo que está gobernada por p. El Nivel de Usuario también define el umbral de probabilidad de desviación de la dinámica θ. • El Nivel de Agente se enfrenta entonces a un problema de seleccionar una función de señal de control a∗ para satisfacer un problema de minimización de la siguiente manera: a∗ = arg min a Pr(d(τa, q) > θ) donde d(τa, q) es una variable aleatoria que describe la desviación de la estimación de la dinámica τa, creada por L bajo la señal de control a, de la dinámica ideal q. Implícito en este problema de minimización es que L es manipulado a través del entorno, basado en el modelo de entorno producido por el Nivel de Diseño del Entorno. 3.2 Vista DBC del Espacio de Estados Es importante tener en cuenta la visión complementaria que DBC y POMDPs tienen sobre el espacio de estados del entorno. Los POMDP consideran el estado como una instantánea estacionaria del entorno; cualquier atributo de la secuencia de estados que se busque se alcanza a través de las propiedades del proceso de control, en este caso la acumulación de recompensas. Esto se puede ver como si la secuencia de estados y los atributos de esa secuencia solo fueran introducidos por y para el mecanismo de control, la política POMDP. DBC se enfoca en el principio subyacente de la secuenciación de estados, la dinámica del sistema. La especificación de la dinámica de los DBCs puede utilizar el espacio de estado del entorno como un medio para describir, discernir y preservar los cambios que ocurren dentro del sistema. Como resultado, DBC tiene una mayor capacidad para expresar propiedades de secuenciación de estados, las cuales están fundamentadas en el modelo del entorno y su definición del espacio de estados. Por ejemplo, considera la tarea de moverse a través de terreno accidentado hacia un objetivo y alcanzarlo lo más rápido posible. Los POMDPs codificarían el terreno como puntos en el espacio de estados, mientras que la velocidad se garantizaría mediante una recompensa negativa por cada paso dado sin alcanzar la meta. Acumular una recompensa más alta solo se puede lograr mediante un movimiento más rápido. Alternativamente, el espacio de estados podría incluir directamente la noción de velocidad. Para los POMDPs, esto significaría que el mismo concepto está codificado dos veces, en cierto sentido: directamente en el espacio de estados e indirectamente en la acumulación de recompensas. Ahora, incluso si la <br>función de recompensa</br> codificara más y detalles más finos de las propiedades del movimiento, la solución POMDP tendría que buscar en un espacio mucho más grande de políticas, aunque aún esté guiada por el concepto implícito del procedimiento de acumulación de recompensas. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "partially observable markov decision problem": {
            "translated_key": "problema de decisión de Markov parcialmente observable",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Dynamics Based Control with an Application to Area-Sweeping Problems Zinovi Rabinovich Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel jeff@cs.huji.ac.il Gal A. Kaminka The MAVERICK Group Department of Computer Science Bar Ilan University, Israel galk@cs.biu.ac.il ABSTRACT In this paper we introduce Dynamics Based Control (DBC), an approach to planning and control of an agent in stochastic environments.",
                "Unlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics.",
                "We show that a recently developed planning and control approach, Extended Markov Tracking (EMT) is an instantiation of DBC.",
                "EMT employs greedy action selection to provide an efficient control algorithm in Markovian environments.",
                "We exploit this efficiency in a set of experiments that applied multitarget EMT to a class of area-sweeping problems (searching for moving targets).",
                "We show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation.",
                "Categories and Subject Descriptors I.2.8 [Problem Solving, Control Methods, and Search]: Control Theory; I.2.9 [Robotics]; I.2.11 [Distributed Artificial Intelligence]: Intelligent Agents General Terms Algorithms, Theory 1.",
                "INTRODUCTION Planning and control constitutes a central research area in multiagent systems and artificial intelligence.",
                "In recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in stochastic environments.",
                "In this framework, the planning and control problem is often addressed by imposing a reward function, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility.",
                "While theoretically attractive, the complexity of optimally solving a POMDP is prohibitive [8, 7].",
                "We take an alternative view of planning in stochastic environments.",
                "We do not use a (state-based) reward function, but instead optimize over a different criterion, a transition-based specification of the desired system dynamics.",
                "The idea here is to view planexecution as a process that compels a (stochastic) system to change, and a plan as a dynamic process that shapes that change according to desired criteria.",
                "We call this general planning framework Dynamics Based Control (DBC).",
                "In DBC, the goal of a planning (or control) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) target dynamics.",
                "As actual system behavior may deviate from that which is specified by target dynamics (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16].",
                "Here, optimality is measured in terms of probability of deviation magnitudes.",
                "In this paper, we present the structure of Dynamics Based Control.",
                "We show that the recently developed Extended Markov Tracking (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC.",
                "EMT is an efficient instantiation of DBC.",
                "To evaluate DBC, we carried out a set of experiments applying multi-target EMT to the Tag Game [11]; this is a variant on the area sweeping problem, where an agent is trying to tag a moving target (quarry) whose position is not known with certainty.",
                "Experimental data demonstrates that even with a simple model of the environment and a simple design of target dynamics, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agents position).",
                "The paper is organized as follows.",
                "In Section 2 we motivate DBC using area-sweeping problems, and discuss related work.",
                "Section 3 introduces the Dynamics Based Control (DBC) structure, and its specialization to Markovian environments.",
                "This is followed by a review of the Extended Markov Tracking (EMT) approach as a DBC-structured control regimen in Section 4.",
                "That section also discusses the limitations of EMT-based control relative to the general DBC framework.",
                "Experimental settings and results are then presented in Section 5.",
                "Section 6 provides a short discussion of the overall approach, and Section 7 gives some concluding remarks and directions for future work. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "MOTIVATION AND RELATED WORK Many real-life scenarios naturally have a stochastic target dynamics specification, especially those domains where there exists no ultimate goal, but rather system behavior (with specific properties) that has to be continually supported.",
                "For example, security guards perform persistent sweeps of an area to detect any sign of intrusion.",
                "Cunning thieves will attempt to track these sweeps, and time their operation to key points of the guards motion.",
                "It is thus advisable to make the guards motion dynamics appear irregular and random.",
                "Recent work by Paruchuri et al. [10] has addressed such randomization in the context of single-agent and distributed POMDPs.",
                "The goal in that work was to generate policies that provide a measure of action-selection randomization, while maintaining rewards within some acceptable levels.",
                "Our focus differs from this work in that DBC does not optimize expected rewards-indeed we do not consider rewards at all-but instead maintains desired dynamics (including, but not limited to, randomization).",
                "The Game of Tag is another example of the applicability of the approach.",
                "It was introduced in the work by Pineau et al. [11].",
                "There are two agents that can move about an area, which is divided into a grid.",
                "The grid may have blocked cells (holes) into which no agent can move.",
                "One agent (the hunter) seeks to move into a cell occupied by the other (the quarry), such that they are co-located (this is a successful tag).",
                "The quarry seeks to avoid the hunter agent, and is always aware of the hunters position, but does not know how the hunter will behave, which opens up the possibility for a hunter to surprise the prey.",
                "The hunter knows the quarrys probabilistic law of motion, but does not know its current location.",
                "Tag is an instance of a family of area-sweeping (pursuit-evasion) problems.",
                "In [11], the hunter modeled the problem using a POMDP.",
                "A reward function was defined, to reflect the desire to tag the quarry, and an action policy was computed to optimize the reward collected over time.",
                "Due to the intractable complexity of determining the optimal policy, the action policy computed in that paper was essentially an approximation.",
                "In this paper, instead of formulating a reward function, we use EMT to solve the problem, by directly specifying the target dynamics.",
                "In fact, any search problem with randomized motion, the socalled class of area sweeping problems, can be described through specification of such target system dynamics.",
                "Dynamics Based Control provides a natural approach to solving these problems. 3.",
                "DYNAMICS BASED CONTROL The specification of Dynamics Based Control (DBC) can be broken into three interacting levels: Environment Design Level, User Level, and Agent Level. • Environment Design Level is concerned with the formal specification and modeling of the environment.",
                "For example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation constant. • User Level in turn relies on the environment model produced by Environment Design to specify the target system dynamics it wishes to observe.",
                "The User Level also specifies the estimation or learning procedure for system dynamics, and the measure of deviation.",
                "In the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping. • Agent Level in turn combines the environment model from the Environment Design level, the dynamics estimation procedure, the deviation measure and the target dynamics specification from User Level, to produce a sequence of actions that create system dynamics as close as possible to the targeted specification.",
                "As we are interested in the continual development of a stochastic system, such as happens in classical control theory [16] and continual planning [4], as well as in our example of museum sweeps, the question becomes how the Agent Level is to treat the deviation measurements over time.",
                "To this end, we use a probability threshold-that is, we would like the Agent Level to maximize the probability that the deviation measure will remain below a certain threshold.",
                "Specific action selection then depends on system formalization.",
                "One possibility would be to create a mixture of available system trends, much like that which happens in Behavior-Based Robotic architectures [1].",
                "The other alternative would be to rely on the estimation procedure provided by the User Level-to utilize the Environment Design Level model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a certain dynamics has been achieved.",
                "Notice that this manipulation is not direct, but via the environment.",
                "Thus, for strong enough estimator algorithms, successful manipulation would mean a successful simulation of the specified target dynamics (i.e., beyond discerning via the available sensory input).",
                "DBC levels can also have a back-flow of information (see Figure 1).",
                "For instance, the Agent Level could provide data about target dynamics feasibility, allowing the User Level to modify the requirement, perhaps focusing on attainable features of system behavior.",
                "Data would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the User Level, this can provide an important tool for the environment model calibration at the Environment Design Level.",
                "UserEnv.",
                "Design Agent Model Ideal Dynamics Estimator Estimator Dynamics Feasibility System Response Data Figure 1: Data flow of the DBC framework Extending upon the idea of Actor-Critic algorithms [5], DBC data flow can provide a good basis for the design of a learning algorithm.",
                "For example, the User Level can operate as an exploratory device for a learning algorithm, inferring an ideal dynamics target from the environment model at hand that would expose and verify most critical features of system behavior.",
                "In this case, feasibility and system response data from the Agent Level would provide key information for an environment model update.",
                "In fact, the combination of feasibility and response data can provide a basis for the application of strong learning algorithms such as EM [2, 9]. 3.1 DBC for Markovian Environments For a Partially Observable Markovian Environment, DBC can be specified in a more rigorous manner.",
                "Notice how DBC discards rewards, and replaces it by another optimality criterion (structural differences are summarized in Table 1): • Environment Design level is to specify a tuple < S, A, T, O, Ω, s0 >, where: - S is the set of all possible environment states; - s0 is the initial state of the environment (which can also be viewed as a probability distribution over S); The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 791 - A is the set of all possible actions applicable in the environment; - T is the environments probabilistic transition function: T : S ×A → Π(S).",
                "That is, T(s |a, s) is the probability that the environment will move from state s to state s under action a; - O is the set of all possible observations.",
                "This is what the sensor input would look like for an outside observer; - Ω is the observation probability function: Ω : S × A × S → Π(O).",
                "That is, Ω(o|s , a, s) is the probability that one will observe o given that the environment has moved from state s to state s under action a. • User Level, in the case of Markovian environment, operates on the set of system dynamics described by a family of conditional probabilities F = {τ : S × A → Π(S)}.",
                "Thus specification of target dynamics can be expressed by q ∈ F, and the learning or tracking algorithm can be represented as a function L : O×(A×O)∗ → F; that is, it maps sequences of observations and actions performed so far into an estimate τ ∈ F of system dynamics.",
                "There are many possible variations available at the User Level to define divergence between system dynamics; several of them are: - Trace distance or L1 distance between two distributions p and q defined by D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Fidelity measure of distance F(p(·), q(·)) = x p(x)q(x) - Kullback-Leibler divergence DKL(p(·) q(·)) = x p(x) log p(x) q(x) Notice that the latter two are not actually metrics over the space of possible distributions, but nevertheless have meaningful and important interpretations.",
                "For instance, KullbackLeibler divergence is an important tool of information theory [3] that allows one to measure the price of encoding an information source governed by q, while assuming that it is governed by p. The User Level also defines the threshold of dynamics deviation probability θ. • Agent Level is then faced with a problem of selecting a control signal function a∗ to satisfy a minimization problem as follows: a∗ = arg min a Pr(d(τa, q) > θ) where d(τa, q) is a random variable describing deviation of the dynamics estimate τa, created by L under control signal a, from the ideal dynamics q.",
                "Implicit in this minimization problem is that L is manipulated via the environment, based on the environment model produced by the Environment Design Level. 3.2 DBC View of the State Space It is important to note the complementary view that DBC and POMDPs take on the state space of the environment.",
                "POMDPs regard state as a stationary snap-shot of the environment; whatever attributes of state sequencing one seeks are reached through properties of the control process, in this case reward accumulation.",
                "This can be viewed as if the sequencing of states and the attributes of that sequencing are only introduced by and for the controlling mechanism-the POMDP policy.",
                "DBC concentrates on the underlying principle of state sequencing, the system dynamics.",
                "DBCs target dynamics specification can use the environments state space as a means to describe, discern, and preserve changes that occur within the system.",
                "As a result, DBC has a greater ability to express state sequencing properties, which are grounded in the environment model and its state space definition.",
                "For example, consider the task of moving through rough terrain towards a goal and reaching it as fast as possible.",
                "POMDPs would encode terrain as state space points, while speed would be ensured by negative reward for every step taken without reaching the goalaccumulating higher reward can be reached only by faster motion.",
                "Alternatively, the state space could directly include the notion of speed.",
                "For POMDPs, this would mean that the same concept is encoded twice, in some sense: directly in the state space, and indirectly within reward accumulation.",
                "Now, even if the reward function would encode more, and finer, details of the properties of motion, the POMDP solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure.",
                "On the other hand, the tactical target expression of variations and correlations between position and speed of motion is now grounded in the state space representation.",
                "In this situation, any further constraints, e.g., smoothness of motion, speed limits in different locations, or speed reductions during sharp turns, are explicitly and uniformly expressed by the tactical target, and can result in faster and more effective action selection by a DBC algorithm. 4.",
                "EMT-BASED CONTROL AS A DBC Recently, a control algorithm was introduced called EMT-based Control [13], which instantiates the DBC framework.",
                "Although it provides an approximate greedy solution in the DBC sense, initial experiments using EMT-based control have been encouraging [14, 15].",
                "EMT-based control is based on the Markovian environment definition, as in the case with POMDPs, but its User and Agent Levels are of the Markovian DBC type of optimality. • User Level of EMT-based control defines a limited-case target system dynamics independent of action: qEMT : S → Π(S).",
                "It then utilizes the Kullback-Leibler divergence measure to compose a momentary system dynamics estimator-the Extended Markov Tracking (EMT) algorithm.",
                "The algorithm keeps a system dynamics estimate τt EMT that is capable of explaining recent change in an auxiliary Bayesian system state estimator from pt−1 to pt, and updates it conservatively using Kullback-Leibler divergence.",
                "Since τt EMT and pt−1,t are respectively the conditional and marginal probabilities over the systems state space, explanation simply means that pt(s ) = s τt EMT (s |s)pt−1(s), and the dynamics estimate update is performed by solving a 792 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: Structure of POMDP vs. Dynamics-Based Control in Markovian Environment Level Approach MDP Markovian DBC Environment < S, A, T, O, Ω >,where S - set of states A - set of actions Design T : S × A → Π(S) - transition O - observation set Ω : S × A × S → Π(O) User r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - reward function q - ideal dynamics F - reward remodeling L - dynamics estimator θ - threshold Agent π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) minimization problem: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • Agent Level in EMT-based control is suboptimal with respect to DBC (though it remains within the DBC framework), performing greedy action selection based on prediction of EMTs reaction.",
                "The prediction is based on the environment model provided by the Environment Design level, so that if we denote by Ta the environments transition function limited to action a, and pt−1 is the auxiliary Bayesian system state estimator, then the EMT-based control choice is described by a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1) Note that this follows the Markovian DBC framework precisely: the rewarding optimality of POMDPs is discarded, and in its place a dynamics estimator (EMT in this case) is manipulated via action effects on the environment to produce an estimate close to the specified target system dynamics.",
                "Yet as we mentioned, naive EMTbased control is suboptimal in the DBC sense, and has several additional limitations that do not exist in the general DBC framework (discussed in Section 4.2). 4.1 Multi-Target EMT At times, there may exist several behavioral preferences.",
                "For example, in the case of museum guards, some art items are more heavily guarded, requiring that the guards stay more often in their close vicinity.",
                "On the other hand, no corner of the museum is to be left unchecked, which demands constant motion.",
                "Successful museum security would demand that the guards adhere to, and balance, both of these behaviors.",
                "For EMT-based control, this would mean facing several tactical targets {qk}K k=1, and the question becomes how to merge and balance them.",
                "A balancing mechanism can be applied to resolve this issue.",
                "Note that EMT-based control, while selecting an action, creates a preference vector over the set of actions based on their predicted performance with respect to a given target.",
                "If these preference vectors are normalized, they can be combined into a single unified preference.",
                "This requires replacement of standard EMT-based action selection by the algorithm below [15]: • Given: - a set of target dynamics {qk}K k=1, - vector of weights w(k) • Select action as follows - For each action a ∈ A predict the future state distribution ¯pa t+1 = Ta ∗ pt; - For each action, compute Da = H(¯pa t+1, pt, PDt) - For each a ∈ A and qk tactical target, denote V (a, k) = DKL (Da qk) pt .",
                "Let Vk(a) = 1 Zk V (a, k), where Zk = a∈A V (a, k) is a normalization factor. - Select a∗ = arg min a k k=1 w(k)Vk(a) The weights vector w = (w1, ..., wK ) allows the additional tuning of importance among target dynamics without the need to redesign the targets themselves.",
                "This balancing method is also seamlessly integrated into the EMT-based control flow of operation. 4.2 EMT-based Control Limitations EMT-based control is a sub-optimal (in the DBC sense) representative of the DBC structure.",
                "It limits the User by forcing EMT to be its dynamic tracking algorithm, and replaces Agent optimization by greedy action selection.",
                "This kind of combination, however, is common for on-line algorithms.",
                "Although further development of EMT-based controllers is necessary, evidence so far suggests that even the simplest form of the algorithm possesses a great deal of power, and displays trends that are optimal in the DBC sense of the word.",
                "There are two further, EMT-specific, limitations to EMT-based control that are evident at this point.",
                "Both already have partial solutions and are subjects of ongoing research.",
                "The first limitation is the problem of negative preference.",
                "In the POMDP framework for example, this is captured simply, through The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 793 the appearance of values with different signs within the reward structure.",
                "For EMT-based control, however, negative preference means that one would like to avoid a certain distribution over system development sequences; EMT-based control, however, concentrates on getting as close as possible to a distribution.",
                "Avoidance is thus unnatural in native EMT-based control.",
                "The second limitation comes from the fact that standard environment modeling can create pure sensory actions-actions that do not change the state of the world, and differ only in the way observations are received and the quality of observations received.",
                "Since the world state does not change, EMT-based control would not be able to differentiate between different sensory actions.",
                "Notice that both of these limitations of EMT-based control are absent from the general DBC framework, since it may have a tracking algorithm capable of considering pure sensory actions and, unlike Kullback-Leibler divergence, a distribution deviation measure that is capable of dealing with negative preference. 5.",
                "EMT PLAYING TAG The Game of Tag was first introduced in [11].",
                "It is a single agent problem of capturing a quarry, and belongs to the class of area sweeping problems.",
                "An example domain is shown in Figure 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figure 2: Tag domain; an agent (A) attempts to seek and capture a quarry (Q) The Game of Tag extremely limits the agents perception, so that the agent is able to detect the quarry only if they are co-located in the same cell of the grid world.",
                "In the classical version of the game, co-location leads to a special observation, and the Tag action can be performed.",
                "We slightly modify this setting: the moment that both agents occupy the same cell, the game ends.",
                "As a result, both the agent and its quarry have the same motion capability, which allows them to move in four directions, North, South, East, and West.",
                "These form a formal space of actions within a Markovian environment.",
                "The state space of the formal Markovian environment is described by the cross-product of the agent and quarrys positions.",
                "For Figure 2, it would be S = {s0, ..., s23} × {s0, ..., s23}.",
                "The effects of an action taken by the agent are deterministic, but the environment in general has a stochastic response due to the motion of the quarry.",
                "With probability q0 1 it stays put, and with probability 1 − q0 it moves to an adjacent cell further away from the 1 In our experiments this was taken to be q0 = 0.2. agent.",
                "So for the instance shown in Figure 2 and q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Although the evasive behavior of the quarry is known to the agent, the quarrys position is not.",
                "The only sensory information available to the agent is its own location.",
                "We use EMT and directly specify the target dynamics.",
                "For the Game of Tag, we can easily formulate three major trends: catching the quarry, staying mobile, and stalking the quarry.",
                "This results in the following three target dynamics: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 otherwise Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 otherwise Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Note that none of the above targets are directly achievable; for instance, if Qt = s9 and At = s11, there is no action that can move the agent to At+1 = s9 as required by the Tcatch target dynamics.",
                "We ran several experiments to evaluate EMT performance in the Tag Game.",
                "Three configurations of the domain shown in Figure 3 were used, each posing a different challenge to the agent due to partial observability.",
                "In each setting, a set of 1000 runs was performed with a time limit of 100 steps.",
                "In every run, the initial position of both the agent and its quarry was selected at random; this means that as far as the agent was concerned, the quarrys initial position was uniformly distributed over the entire domain cell space.",
                "We also used two variations of the environment observability function.",
                "In the first version, observability function mapped all joint positions of hunter and quarry into the position of the hunter as an observation.",
                "In the second, only those joint positions in which hunter and quarry occupied different locations were mapped into the hunters location.",
                "The second version in fact utilized and expressed the fact that once hunter and quarry occupy the same cell the game ends.",
                "The results of these experiments are shown in Table 2.",
                "Balancing [15] the catch, move, and stalk target dynamics described in the previous section by the weight vector [0.8, 0.1, 0.1], EMT produced stable performance in all three domains.",
                "Although direct comparisons are difficult to make, the EMT performance displayed notable efficiency vis-`a-vis the POMDP approach.",
                "In spite of a simple and inefficient Matlab implementation of the EMT algorithm, the decision time for any given step averaged significantly below 1 second in all experiments.",
                "For the irregular open arena domain, which proved to be the most difficult, 1000 experiment runs bounded by 100 steps each, a total of 42411 steps, were completed in slightly under 6 hours.",
                "That is, over 4 × 104 online steps took an order of magnitude less time than the offline computation of POMDP policy in [11].",
                "The significance of this differential is made even more prominent by the fact that, should the environment model parameters change, the online nature of EMT would allow it to maintain its performance time, while the POMDP policy would need to be recomputed, requiring yet again a large overhead of computation.",
                "We also tested the behavior cell frequency entropy, empirical measures from trial data.",
                "As Figure 4 and Figure 5 show, empir794 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) A Q Q A 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 A Q Figure 3: These configurations of the Tag Game space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor Table 2: Performance of the EMT-based solution in three Tag Game domains and two observability models: I) omniposition quarry, II) quarry is not at hunters position Model Domain Capture% E(Steps) Time/Step I Dead-ends 100 14.8 72(mSec) Arena 80.2 42.4 500(mSec) Circle 91.4 34.6 187(mSec) II Dead-ends 100 13.2 91(mSec) Arena 96.8 28.67 396(mSec) Circle 94.4 31.63 204(mSec) ical entropy grows with the length of interaction.",
                "For runs where the quarry was not captured immediately, the entropy reaches between 0.85 and 0.952 for different runs and scenarios.",
                "As the agent actively seeks the quarry, the entropy never reaches its maximum.",
                "One characteristic of the entropy graph for the open arena scenario particularly caught our attention in the case of the omniposition quarry observation model.",
                "Near the maximum limit of trial length (100 steps), entropy suddenly dropped.",
                "Further analysis of the data showed that under certain circumstances, a fluctuating behavior occurs in which the agent faces equally viable versions of quarry-following behavior.",
                "Since the EMT algorithm has greedy action selection, and the state space does not encode any form of commitment (not even speed or acceleration), the agent is locked within a small portion of cells.",
                "It is essentially attempting to simultaneously follow several courses of action, all of which are consistent with the target dynamics.",
                "This behavior did not occur in our second observation model, since it significantly reduced the set of eligible courses of action-essentially contributing to tie-breaking among them. 6.",
                "DISCUSSION The design of the EMT solution for the Tag Game exposes the core difference in approach to planning and control between EMT or DBC, on the one hand, and the more familiar POMDP approach, on the other.",
                "POMDP defines a reward structure to optimize, and influences system dynamics indirectly through that optimization.",
                "EMT discards any reward scheme, and instead measures and influences system dynamics directly. 2 Entropy was calculated using log base equal to the number of possible locations within the domain; this properly scales entropy expression into the range [0, 1] for all domains.",
                "Thus for the Tag Game, we did not search for a reward function that would encode and express our preference over the agents behavior, but rather directly set three (heuristic) behavior preferences as the basis for target dynamics to be maintained.",
                "Experimental data shows that these targets need not be directly achievable via the agents actions.",
                "However, the ratio between EMT performance and achievability of target dynamics remains to be explored.",
                "The tag game experiment data also revealed the different emphasis DBC and POMDPs place on the formulation of an environment state space.",
                "POMDPs rely entirely on the mechanism of reward accumulation maximization, i.e., formation of the action selection procedure to achieve necessary state sequencing.",
                "DBC, on the other hand, has two sources of sequencing specification: through the properties of an action selection procedure, and through direct specification within the target dynamics.",
                "The importance of the second source was underlined by the Tag Game experiment data, in which the greedy EMT algorithm, applied to a POMDP-type state space specification, failed, since target description over such a state space was incapable of encoding the necessary behavior tendencies, e.g., tie-breaking and commitment to directed motion.",
                "The structural differences between DBC (and EMT in particular), and POMDPs, prohibits direct performance comparison, and places them on complementary tracks, each within a suitable niche.",
                "For instance, POMDPs could be seen as a much more natural formulation of economic sequential decision-making problems, while EMT is a better fit for continual demand for stochastic change, as happens in many robotic or embodied-agent problems.",
                "The complementary properties of POMDPs and EMT can be further exploited.",
                "There is recent interest in using POMDPs in hybrid solutions [17], in which the POMDPs can be used together with other control approaches to provide results not easily achievable with either approach by itself.",
                "DBC can be an effective partner in such a hybrid solution.",
                "For instance, POMDPs have prohibitively large off-line time requirements for policy computation, but can be readily used in simpler settings to expose beneficial behavioral trends; this can serve as a form of target dynamics that are provided to EMT in a larger domain for on-line operation. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have presented a novel perspective on the process of planning and control in stochastic environments, in the form of the Dynamics Based Control (DBC) framework.",
                "DBC formulates the task of planning as support of a specified target system dynamics, which describes the necessary properties of change within the environment.",
                "Optimality of DBC plans of action are measured The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 4: Observation Model I: Omniposition quarry.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 5: Observation Model II: quarry not observed at hunters position.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. with respect to the deviation of actual system dynamics from the target dynamics.",
                "We show that a recently developed technique of Extended Markov Tracking (EMT) [13] is an instantiation of DBC.",
                "In fact, EMT can be seen as a specific case of DBC parameterization, which employs a greedy action selection procedure.",
                "Since EMT exhibits the key features of the general DBC framework, as well as polynomial time complexity, we used the multitarget version of EMT [15] to demonstrate that the class of area sweeping problems naturally lends itself to dynamics-based descriptions, as instantiated by our experiments in the Tag Game domain.",
                "As enumerated in Section 4.2, EMT has a number of limitations, such as difficulty in dealing with negative dynamic preference.",
                "This prevents direct application of EMT to such problems as Rendezvous-Evasion Games (e.g., [6]).",
                "However, DBC in general has no such limitations, and readily enables the formulation of evasion games.",
                "In future work, we intend to proceed with the development of dynamics-based controllers for these problems. 8.",
                "ACKNOWLEDGMENT The work of the first two authors was partially supported by Israel Science Foundation grant #898/05, and the third author was partially supported by a grant from Israels Ministry of Science and Technology. 9.",
                "REFERENCES [1] R. C. Arkin.",
                "Behavior-Based Robotics.",
                "MIT Press, 1998. [2] J.",
                "A. Bilmes.",
                "A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and Hidden Markov Models.",
                "Technical Report TR-97-021, Department of Electrical Engeineering and Computer Science, University of California at Berkeley, 1998. [3] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of information theory.",
                "Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz, and M. J. Wolverton.",
                "A survey of research in distributed, continual planning.",
                "AI Magazine, 4:13-22, 1999. [5] V. R. Konda and J. N. Tsitsiklis.",
                "Actor-Critic algorithms.",
                "SIAM Journal on Control and Optimization, 42(4):1143-1166, 2003. [6] W. S. Lim.",
                "A rendezvous-evasion game on discrete locations with joint randomization.",
                "Advances in Applied Probability, 29(4):1004-1017, December 1997. [7] M. L. Littman, T. L. Dean, and L. P. Kaelbling.",
                "On the complexity of solving Markov decision problems.",
                "In Proceedings of the 11th Annual Conference on Uncertainty in Artificial Intelligence (UAI-95), pages 394-402, 1995. [8] O. Madani, S. Hanks, and A. Condon.",
                "On the undecidability of probabilistic planning and related stochastic optimization problems.",
                "Artificial Intelligence Journal, 147(1-2):5-34, July 2003. [9] R. M. Neal and G. E. Hinton.",
                "A view of the EM algorithm 796 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) that justifies incremental, sparse, and other variants.",
                "In M. I. Jordan, editor, Learning in Graphical Models, pages 355-368.",
                "Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In Proceeding of AAMAS 2006, 2006. [11] J. Pineau, G. Gordon, and S. Thrun.",
                "Point-based value iteration: An anytime algorithm for pomdps.",
                "In International Joint Conference on Artificial Intelligence (IJCAI), pages 1025-1032, August 2003. [12] M. L. Puterman.",
                "Markov Decision Processes.",
                "Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics Section.",
                "Wiley-Interscience Publication, New York, 1994. [13] Z. Rabinovich and J. S. Rosenschein.",
                "Extended Markov Tracking with an application to control.",
                "In The Workshop on Agent Tracking: Modeling Other Agents from Observations, at the Third International Joint Conference on Autonomous Agents and Multiagent Systems, pages 95-100, New-York, July 2004. [14] Z. Rabinovich and J. S. Rosenschein.",
                "Multiagent coordination by Extended Markov Tracking.",
                "In The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 431-438, Utrecht, The Netherlands, July 2005. [15] Z. Rabinovich and J. S. Rosenschein.",
                "On the response of EMT-based control to interacting targets and models.",
                "In The Fifth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 465-470, Hakodate, Japan, May 2006. [16] R. F. Stengel.",
                "Optimal Control and Estimation.",
                "Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, and P. Varakantham.",
                "Conflicts in teamwork: Hybrids to the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 797"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "extended markov tracking": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Dynamics Based Control with an Application to Area-Sweeping Problems Zinovi Rabinovich Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel jeff@cs.huji.ac.il Gal A. Kaminka The MAVERICK Group Department of Computer Science Bar Ilan University, Israel galk@cs.biu.ac.il ABSTRACT In this paper we introduce Dynamics Based Control (DBC), an approach to planning and control of an agent in stochastic environments.",
                "Unlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics.",
                "We show that a recently developed planning and control approach, <br>extended markov tracking</br> (EMT) is an instantiation of DBC.",
                "EMT employs greedy action selection to provide an efficient control algorithm in Markovian environments.",
                "We exploit this efficiency in a set of experiments that applied multitarget EMT to a class of area-sweeping problems (searching for moving targets).",
                "We show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation.",
                "Categories and Subject Descriptors I.2.8 [Problem Solving, Control Methods, and Search]: Control Theory; I.2.9 [Robotics]; I.2.11 [Distributed Artificial Intelligence]: Intelligent Agents General Terms Algorithms, Theory 1.",
                "INTRODUCTION Planning and control constitutes a central research area in multiagent systems and artificial intelligence.",
                "In recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in stochastic environments.",
                "In this framework, the planning and control problem is often addressed by imposing a reward function, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility.",
                "While theoretically attractive, the complexity of optimally solving a POMDP is prohibitive [8, 7].",
                "We take an alternative view of planning in stochastic environments.",
                "We do not use a (state-based) reward function, but instead optimize over a different criterion, a transition-based specification of the desired system dynamics.",
                "The idea here is to view planexecution as a process that compels a (stochastic) system to change, and a plan as a dynamic process that shapes that change according to desired criteria.",
                "We call this general planning framework Dynamics Based Control (DBC).",
                "In DBC, the goal of a planning (or control) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) target dynamics.",
                "As actual system behavior may deviate from that which is specified by target dynamics (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16].",
                "Here, optimality is measured in terms of probability of deviation magnitudes.",
                "In this paper, we present the structure of Dynamics Based Control.",
                "We show that the recently developed <br>extended markov tracking</br> (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC.",
                "EMT is an efficient instantiation of DBC.",
                "To evaluate DBC, we carried out a set of experiments applying multi-target EMT to the Tag Game [11]; this is a variant on the area sweeping problem, where an agent is trying to tag a moving target (quarry) whose position is not known with certainty.",
                "Experimental data demonstrates that even with a simple model of the environment and a simple design of target dynamics, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agents position).",
                "The paper is organized as follows.",
                "In Section 2 we motivate DBC using area-sweeping problems, and discuss related work.",
                "Section 3 introduces the Dynamics Based Control (DBC) structure, and its specialization to Markovian environments.",
                "This is followed by a review of the <br>extended markov tracking</br> (EMT) approach as a DBC-structured control regimen in Section 4.",
                "That section also discusses the limitations of EMT-based control relative to the general DBC framework.",
                "Experimental settings and results are then presented in Section 5.",
                "Section 6 provides a short discussion of the overall approach, and Section 7 gives some concluding remarks and directions for future work. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "MOTIVATION AND RELATED WORK Many real-life scenarios naturally have a stochastic target dynamics specification, especially those domains where there exists no ultimate goal, but rather system behavior (with specific properties) that has to be continually supported.",
                "For example, security guards perform persistent sweeps of an area to detect any sign of intrusion.",
                "Cunning thieves will attempt to track these sweeps, and time their operation to key points of the guards motion.",
                "It is thus advisable to make the guards motion dynamics appear irregular and random.",
                "Recent work by Paruchuri et al. [10] has addressed such randomization in the context of single-agent and distributed POMDPs.",
                "The goal in that work was to generate policies that provide a measure of action-selection randomization, while maintaining rewards within some acceptable levels.",
                "Our focus differs from this work in that DBC does not optimize expected rewards-indeed we do not consider rewards at all-but instead maintains desired dynamics (including, but not limited to, randomization).",
                "The Game of Tag is another example of the applicability of the approach.",
                "It was introduced in the work by Pineau et al. [11].",
                "There are two agents that can move about an area, which is divided into a grid.",
                "The grid may have blocked cells (holes) into which no agent can move.",
                "One agent (the hunter) seeks to move into a cell occupied by the other (the quarry), such that they are co-located (this is a successful tag).",
                "The quarry seeks to avoid the hunter agent, and is always aware of the hunters position, but does not know how the hunter will behave, which opens up the possibility for a hunter to surprise the prey.",
                "The hunter knows the quarrys probabilistic law of motion, but does not know its current location.",
                "Tag is an instance of a family of area-sweeping (pursuit-evasion) problems.",
                "In [11], the hunter modeled the problem using a POMDP.",
                "A reward function was defined, to reflect the desire to tag the quarry, and an action policy was computed to optimize the reward collected over time.",
                "Due to the intractable complexity of determining the optimal policy, the action policy computed in that paper was essentially an approximation.",
                "In this paper, instead of formulating a reward function, we use EMT to solve the problem, by directly specifying the target dynamics.",
                "In fact, any search problem with randomized motion, the socalled class of area sweeping problems, can be described through specification of such target system dynamics.",
                "Dynamics Based Control provides a natural approach to solving these problems. 3.",
                "DYNAMICS BASED CONTROL The specification of Dynamics Based Control (DBC) can be broken into three interacting levels: Environment Design Level, User Level, and Agent Level. • Environment Design Level is concerned with the formal specification and modeling of the environment.",
                "For example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation constant. • User Level in turn relies on the environment model produced by Environment Design to specify the target system dynamics it wishes to observe.",
                "The User Level also specifies the estimation or learning procedure for system dynamics, and the measure of deviation.",
                "In the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping. • Agent Level in turn combines the environment model from the Environment Design level, the dynamics estimation procedure, the deviation measure and the target dynamics specification from User Level, to produce a sequence of actions that create system dynamics as close as possible to the targeted specification.",
                "As we are interested in the continual development of a stochastic system, such as happens in classical control theory [16] and continual planning [4], as well as in our example of museum sweeps, the question becomes how the Agent Level is to treat the deviation measurements over time.",
                "To this end, we use a probability threshold-that is, we would like the Agent Level to maximize the probability that the deviation measure will remain below a certain threshold.",
                "Specific action selection then depends on system formalization.",
                "One possibility would be to create a mixture of available system trends, much like that which happens in Behavior-Based Robotic architectures [1].",
                "The other alternative would be to rely on the estimation procedure provided by the User Level-to utilize the Environment Design Level model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a certain dynamics has been achieved.",
                "Notice that this manipulation is not direct, but via the environment.",
                "Thus, for strong enough estimator algorithms, successful manipulation would mean a successful simulation of the specified target dynamics (i.e., beyond discerning via the available sensory input).",
                "DBC levels can also have a back-flow of information (see Figure 1).",
                "For instance, the Agent Level could provide data about target dynamics feasibility, allowing the User Level to modify the requirement, perhaps focusing on attainable features of system behavior.",
                "Data would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the User Level, this can provide an important tool for the environment model calibration at the Environment Design Level.",
                "UserEnv.",
                "Design Agent Model Ideal Dynamics Estimator Estimator Dynamics Feasibility System Response Data Figure 1: Data flow of the DBC framework Extending upon the idea of Actor-Critic algorithms [5], DBC data flow can provide a good basis for the design of a learning algorithm.",
                "For example, the User Level can operate as an exploratory device for a learning algorithm, inferring an ideal dynamics target from the environment model at hand that would expose and verify most critical features of system behavior.",
                "In this case, feasibility and system response data from the Agent Level would provide key information for an environment model update.",
                "In fact, the combination of feasibility and response data can provide a basis for the application of strong learning algorithms such as EM [2, 9]. 3.1 DBC for Markovian Environments For a Partially Observable Markovian Environment, DBC can be specified in a more rigorous manner.",
                "Notice how DBC discards rewards, and replaces it by another optimality criterion (structural differences are summarized in Table 1): • Environment Design level is to specify a tuple < S, A, T, O, Ω, s0 >, where: - S is the set of all possible environment states; - s0 is the initial state of the environment (which can also be viewed as a probability distribution over S); The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 791 - A is the set of all possible actions applicable in the environment; - T is the environments probabilistic transition function: T : S ×A → Π(S).",
                "That is, T(s |a, s) is the probability that the environment will move from state s to state s under action a; - O is the set of all possible observations.",
                "This is what the sensor input would look like for an outside observer; - Ω is the observation probability function: Ω : S × A × S → Π(O).",
                "That is, Ω(o|s , a, s) is the probability that one will observe o given that the environment has moved from state s to state s under action a. • User Level, in the case of Markovian environment, operates on the set of system dynamics described by a family of conditional probabilities F = {τ : S × A → Π(S)}.",
                "Thus specification of target dynamics can be expressed by q ∈ F, and the learning or tracking algorithm can be represented as a function L : O×(A×O)∗ → F; that is, it maps sequences of observations and actions performed so far into an estimate τ ∈ F of system dynamics.",
                "There are many possible variations available at the User Level to define divergence between system dynamics; several of them are: - Trace distance or L1 distance between two distributions p and q defined by D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Fidelity measure of distance F(p(·), q(·)) = x p(x)q(x) - Kullback-Leibler divergence DKL(p(·) q(·)) = x p(x) log p(x) q(x) Notice that the latter two are not actually metrics over the space of possible distributions, but nevertheless have meaningful and important interpretations.",
                "For instance, KullbackLeibler divergence is an important tool of information theory [3] that allows one to measure the price of encoding an information source governed by q, while assuming that it is governed by p. The User Level also defines the threshold of dynamics deviation probability θ. • Agent Level is then faced with a problem of selecting a control signal function a∗ to satisfy a minimization problem as follows: a∗ = arg min a Pr(d(τa, q) > θ) where d(τa, q) is a random variable describing deviation of the dynamics estimate τa, created by L under control signal a, from the ideal dynamics q.",
                "Implicit in this minimization problem is that L is manipulated via the environment, based on the environment model produced by the Environment Design Level. 3.2 DBC View of the State Space It is important to note the complementary view that DBC and POMDPs take on the state space of the environment.",
                "POMDPs regard state as a stationary snap-shot of the environment; whatever attributes of state sequencing one seeks are reached through properties of the control process, in this case reward accumulation.",
                "This can be viewed as if the sequencing of states and the attributes of that sequencing are only introduced by and for the controlling mechanism-the POMDP policy.",
                "DBC concentrates on the underlying principle of state sequencing, the system dynamics.",
                "DBCs target dynamics specification can use the environments state space as a means to describe, discern, and preserve changes that occur within the system.",
                "As a result, DBC has a greater ability to express state sequencing properties, which are grounded in the environment model and its state space definition.",
                "For example, consider the task of moving through rough terrain towards a goal and reaching it as fast as possible.",
                "POMDPs would encode terrain as state space points, while speed would be ensured by negative reward for every step taken without reaching the goalaccumulating higher reward can be reached only by faster motion.",
                "Alternatively, the state space could directly include the notion of speed.",
                "For POMDPs, this would mean that the same concept is encoded twice, in some sense: directly in the state space, and indirectly within reward accumulation.",
                "Now, even if the reward function would encode more, and finer, details of the properties of motion, the POMDP solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure.",
                "On the other hand, the tactical target expression of variations and correlations between position and speed of motion is now grounded in the state space representation.",
                "In this situation, any further constraints, e.g., smoothness of motion, speed limits in different locations, or speed reductions during sharp turns, are explicitly and uniformly expressed by the tactical target, and can result in faster and more effective action selection by a DBC algorithm. 4.",
                "EMT-BASED CONTROL AS A DBC Recently, a control algorithm was introduced called EMT-based Control [13], which instantiates the DBC framework.",
                "Although it provides an approximate greedy solution in the DBC sense, initial experiments using EMT-based control have been encouraging [14, 15].",
                "EMT-based control is based on the Markovian environment definition, as in the case with POMDPs, but its User and Agent Levels are of the Markovian DBC type of optimality. • User Level of EMT-based control defines a limited-case target system dynamics independent of action: qEMT : S → Π(S).",
                "It then utilizes the Kullback-Leibler divergence measure to compose a momentary system dynamics estimator-the <br>extended markov tracking</br> (EMT) algorithm.",
                "The algorithm keeps a system dynamics estimate τt EMT that is capable of explaining recent change in an auxiliary Bayesian system state estimator from pt−1 to pt, and updates it conservatively using Kullback-Leibler divergence.",
                "Since τt EMT and pt−1,t are respectively the conditional and marginal probabilities over the systems state space, explanation simply means that pt(s ) = s τt EMT (s |s)pt−1(s), and the dynamics estimate update is performed by solving a 792 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: Structure of POMDP vs. Dynamics-Based Control in Markovian Environment Level Approach MDP Markovian DBC Environment < S, A, T, O, Ω >,where S - set of states A - set of actions Design T : S × A → Π(S) - transition O - observation set Ω : S × A × S → Π(O) User r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - reward function q - ideal dynamics F - reward remodeling L - dynamics estimator θ - threshold Agent π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) minimization problem: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • Agent Level in EMT-based control is suboptimal with respect to DBC (though it remains within the DBC framework), performing greedy action selection based on prediction of EMTs reaction.",
                "The prediction is based on the environment model provided by the Environment Design level, so that if we denote by Ta the environments transition function limited to action a, and pt−1 is the auxiliary Bayesian system state estimator, then the EMT-based control choice is described by a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1) Note that this follows the Markovian DBC framework precisely: the rewarding optimality of POMDPs is discarded, and in its place a dynamics estimator (EMT in this case) is manipulated via action effects on the environment to produce an estimate close to the specified target system dynamics.",
                "Yet as we mentioned, naive EMTbased control is suboptimal in the DBC sense, and has several additional limitations that do not exist in the general DBC framework (discussed in Section 4.2). 4.1 Multi-Target EMT At times, there may exist several behavioral preferences.",
                "For example, in the case of museum guards, some art items are more heavily guarded, requiring that the guards stay more often in their close vicinity.",
                "On the other hand, no corner of the museum is to be left unchecked, which demands constant motion.",
                "Successful museum security would demand that the guards adhere to, and balance, both of these behaviors.",
                "For EMT-based control, this would mean facing several tactical targets {qk}K k=1, and the question becomes how to merge and balance them.",
                "A balancing mechanism can be applied to resolve this issue.",
                "Note that EMT-based control, while selecting an action, creates a preference vector over the set of actions based on their predicted performance with respect to a given target.",
                "If these preference vectors are normalized, they can be combined into a single unified preference.",
                "This requires replacement of standard EMT-based action selection by the algorithm below [15]: • Given: - a set of target dynamics {qk}K k=1, - vector of weights w(k) • Select action as follows - For each action a ∈ A predict the future state distribution ¯pa t+1 = Ta ∗ pt; - For each action, compute Da = H(¯pa t+1, pt, PDt) - For each a ∈ A and qk tactical target, denote V (a, k) = DKL (Da qk) pt .",
                "Let Vk(a) = 1 Zk V (a, k), where Zk = a∈A V (a, k) is a normalization factor. - Select a∗ = arg min a k k=1 w(k)Vk(a) The weights vector w = (w1, ..., wK ) allows the additional tuning of importance among target dynamics without the need to redesign the targets themselves.",
                "This balancing method is also seamlessly integrated into the EMT-based control flow of operation. 4.2 EMT-based Control Limitations EMT-based control is a sub-optimal (in the DBC sense) representative of the DBC structure.",
                "It limits the User by forcing EMT to be its dynamic tracking algorithm, and replaces Agent optimization by greedy action selection.",
                "This kind of combination, however, is common for on-line algorithms.",
                "Although further development of EMT-based controllers is necessary, evidence so far suggests that even the simplest form of the algorithm possesses a great deal of power, and displays trends that are optimal in the DBC sense of the word.",
                "There are two further, EMT-specific, limitations to EMT-based control that are evident at this point.",
                "Both already have partial solutions and are subjects of ongoing research.",
                "The first limitation is the problem of negative preference.",
                "In the POMDP framework for example, this is captured simply, through The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 793 the appearance of values with different signs within the reward structure.",
                "For EMT-based control, however, negative preference means that one would like to avoid a certain distribution over system development sequences; EMT-based control, however, concentrates on getting as close as possible to a distribution.",
                "Avoidance is thus unnatural in native EMT-based control.",
                "The second limitation comes from the fact that standard environment modeling can create pure sensory actions-actions that do not change the state of the world, and differ only in the way observations are received and the quality of observations received.",
                "Since the world state does not change, EMT-based control would not be able to differentiate between different sensory actions.",
                "Notice that both of these limitations of EMT-based control are absent from the general DBC framework, since it may have a tracking algorithm capable of considering pure sensory actions and, unlike Kullback-Leibler divergence, a distribution deviation measure that is capable of dealing with negative preference. 5.",
                "EMT PLAYING TAG The Game of Tag was first introduced in [11].",
                "It is a single agent problem of capturing a quarry, and belongs to the class of area sweeping problems.",
                "An example domain is shown in Figure 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figure 2: Tag domain; an agent (A) attempts to seek and capture a quarry (Q) The Game of Tag extremely limits the agents perception, so that the agent is able to detect the quarry only if they are co-located in the same cell of the grid world.",
                "In the classical version of the game, co-location leads to a special observation, and the Tag action can be performed.",
                "We slightly modify this setting: the moment that both agents occupy the same cell, the game ends.",
                "As a result, both the agent and its quarry have the same motion capability, which allows them to move in four directions, North, South, East, and West.",
                "These form a formal space of actions within a Markovian environment.",
                "The state space of the formal Markovian environment is described by the cross-product of the agent and quarrys positions.",
                "For Figure 2, it would be S = {s0, ..., s23} × {s0, ..., s23}.",
                "The effects of an action taken by the agent are deterministic, but the environment in general has a stochastic response due to the motion of the quarry.",
                "With probability q0 1 it stays put, and with probability 1 − q0 it moves to an adjacent cell further away from the 1 In our experiments this was taken to be q0 = 0.2. agent.",
                "So for the instance shown in Figure 2 and q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Although the evasive behavior of the quarry is known to the agent, the quarrys position is not.",
                "The only sensory information available to the agent is its own location.",
                "We use EMT and directly specify the target dynamics.",
                "For the Game of Tag, we can easily formulate three major trends: catching the quarry, staying mobile, and stalking the quarry.",
                "This results in the following three target dynamics: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 otherwise Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 otherwise Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Note that none of the above targets are directly achievable; for instance, if Qt = s9 and At = s11, there is no action that can move the agent to At+1 = s9 as required by the Tcatch target dynamics.",
                "We ran several experiments to evaluate EMT performance in the Tag Game.",
                "Three configurations of the domain shown in Figure 3 were used, each posing a different challenge to the agent due to partial observability.",
                "In each setting, a set of 1000 runs was performed with a time limit of 100 steps.",
                "In every run, the initial position of both the agent and its quarry was selected at random; this means that as far as the agent was concerned, the quarrys initial position was uniformly distributed over the entire domain cell space.",
                "We also used two variations of the environment observability function.",
                "In the first version, observability function mapped all joint positions of hunter and quarry into the position of the hunter as an observation.",
                "In the second, only those joint positions in which hunter and quarry occupied different locations were mapped into the hunters location.",
                "The second version in fact utilized and expressed the fact that once hunter and quarry occupy the same cell the game ends.",
                "The results of these experiments are shown in Table 2.",
                "Balancing [15] the catch, move, and stalk target dynamics described in the previous section by the weight vector [0.8, 0.1, 0.1], EMT produced stable performance in all three domains.",
                "Although direct comparisons are difficult to make, the EMT performance displayed notable efficiency vis-`a-vis the POMDP approach.",
                "In spite of a simple and inefficient Matlab implementation of the EMT algorithm, the decision time for any given step averaged significantly below 1 second in all experiments.",
                "For the irregular open arena domain, which proved to be the most difficult, 1000 experiment runs bounded by 100 steps each, a total of 42411 steps, were completed in slightly under 6 hours.",
                "That is, over 4 × 104 online steps took an order of magnitude less time than the offline computation of POMDP policy in [11].",
                "The significance of this differential is made even more prominent by the fact that, should the environment model parameters change, the online nature of EMT would allow it to maintain its performance time, while the POMDP policy would need to be recomputed, requiring yet again a large overhead of computation.",
                "We also tested the behavior cell frequency entropy, empirical measures from trial data.",
                "As Figure 4 and Figure 5 show, empir794 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) A Q Q A 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 A Q Figure 3: These configurations of the Tag Game space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor Table 2: Performance of the EMT-based solution in three Tag Game domains and two observability models: I) omniposition quarry, II) quarry is not at hunters position Model Domain Capture% E(Steps) Time/Step I Dead-ends 100 14.8 72(mSec) Arena 80.2 42.4 500(mSec) Circle 91.4 34.6 187(mSec) II Dead-ends 100 13.2 91(mSec) Arena 96.8 28.67 396(mSec) Circle 94.4 31.63 204(mSec) ical entropy grows with the length of interaction.",
                "For runs where the quarry was not captured immediately, the entropy reaches between 0.85 and 0.952 for different runs and scenarios.",
                "As the agent actively seeks the quarry, the entropy never reaches its maximum.",
                "One characteristic of the entropy graph for the open arena scenario particularly caught our attention in the case of the omniposition quarry observation model.",
                "Near the maximum limit of trial length (100 steps), entropy suddenly dropped.",
                "Further analysis of the data showed that under certain circumstances, a fluctuating behavior occurs in which the agent faces equally viable versions of quarry-following behavior.",
                "Since the EMT algorithm has greedy action selection, and the state space does not encode any form of commitment (not even speed or acceleration), the agent is locked within a small portion of cells.",
                "It is essentially attempting to simultaneously follow several courses of action, all of which are consistent with the target dynamics.",
                "This behavior did not occur in our second observation model, since it significantly reduced the set of eligible courses of action-essentially contributing to tie-breaking among them. 6.",
                "DISCUSSION The design of the EMT solution for the Tag Game exposes the core difference in approach to planning and control between EMT or DBC, on the one hand, and the more familiar POMDP approach, on the other.",
                "POMDP defines a reward structure to optimize, and influences system dynamics indirectly through that optimization.",
                "EMT discards any reward scheme, and instead measures and influences system dynamics directly. 2 Entropy was calculated using log base equal to the number of possible locations within the domain; this properly scales entropy expression into the range [0, 1] for all domains.",
                "Thus for the Tag Game, we did not search for a reward function that would encode and express our preference over the agents behavior, but rather directly set three (heuristic) behavior preferences as the basis for target dynamics to be maintained.",
                "Experimental data shows that these targets need not be directly achievable via the agents actions.",
                "However, the ratio between EMT performance and achievability of target dynamics remains to be explored.",
                "The tag game experiment data also revealed the different emphasis DBC and POMDPs place on the formulation of an environment state space.",
                "POMDPs rely entirely on the mechanism of reward accumulation maximization, i.e., formation of the action selection procedure to achieve necessary state sequencing.",
                "DBC, on the other hand, has two sources of sequencing specification: through the properties of an action selection procedure, and through direct specification within the target dynamics.",
                "The importance of the second source was underlined by the Tag Game experiment data, in which the greedy EMT algorithm, applied to a POMDP-type state space specification, failed, since target description over such a state space was incapable of encoding the necessary behavior tendencies, e.g., tie-breaking and commitment to directed motion.",
                "The structural differences between DBC (and EMT in particular), and POMDPs, prohibits direct performance comparison, and places them on complementary tracks, each within a suitable niche.",
                "For instance, POMDPs could be seen as a much more natural formulation of economic sequential decision-making problems, while EMT is a better fit for continual demand for stochastic change, as happens in many robotic or embodied-agent problems.",
                "The complementary properties of POMDPs and EMT can be further exploited.",
                "There is recent interest in using POMDPs in hybrid solutions [17], in which the POMDPs can be used together with other control approaches to provide results not easily achievable with either approach by itself.",
                "DBC can be an effective partner in such a hybrid solution.",
                "For instance, POMDPs have prohibitively large off-line time requirements for policy computation, but can be readily used in simpler settings to expose beneficial behavioral trends; this can serve as a form of target dynamics that are provided to EMT in a larger domain for on-line operation. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have presented a novel perspective on the process of planning and control in stochastic environments, in the form of the Dynamics Based Control (DBC) framework.",
                "DBC formulates the task of planning as support of a specified target system dynamics, which describes the necessary properties of change within the environment.",
                "Optimality of DBC plans of action are measured The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 4: Observation Model I: Omniposition quarry.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 5: Observation Model II: quarry not observed at hunters position.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. with respect to the deviation of actual system dynamics from the target dynamics.",
                "We show that a recently developed technique of <br>extended markov tracking</br> (EMT) [13] is an instantiation of DBC.",
                "In fact, EMT can be seen as a specific case of DBC parameterization, which employs a greedy action selection procedure.",
                "Since EMT exhibits the key features of the general DBC framework, as well as polynomial time complexity, we used the multitarget version of EMT [15] to demonstrate that the class of area sweeping problems naturally lends itself to dynamics-based descriptions, as instantiated by our experiments in the Tag Game domain.",
                "As enumerated in Section 4.2, EMT has a number of limitations, such as difficulty in dealing with negative dynamic preference.",
                "This prevents direct application of EMT to such problems as Rendezvous-Evasion Games (e.g., [6]).",
                "However, DBC in general has no such limitations, and readily enables the formulation of evasion games.",
                "In future work, we intend to proceed with the development of dynamics-based controllers for these problems. 8.",
                "ACKNOWLEDGMENT The work of the first two authors was partially supported by Israel Science Foundation grant #898/05, and the third author was partially supported by a grant from Israels Ministry of Science and Technology. 9.",
                "REFERENCES [1] R. C. Arkin.",
                "Behavior-Based Robotics.",
                "MIT Press, 1998. [2] J.",
                "A. Bilmes.",
                "A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and Hidden Markov Models.",
                "Technical Report TR-97-021, Department of Electrical Engeineering and Computer Science, University of California at Berkeley, 1998. [3] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of information theory.",
                "Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz, and M. J. Wolverton.",
                "A survey of research in distributed, continual planning.",
                "AI Magazine, 4:13-22, 1999. [5] V. R. Konda and J. N. Tsitsiklis.",
                "Actor-Critic algorithms.",
                "SIAM Journal on Control and Optimization, 42(4):1143-1166, 2003. [6] W. S. Lim.",
                "A rendezvous-evasion game on discrete locations with joint randomization.",
                "Advances in Applied Probability, 29(4):1004-1017, December 1997. [7] M. L. Littman, T. L. Dean, and L. P. Kaelbling.",
                "On the complexity of solving Markov decision problems.",
                "In Proceedings of the 11th Annual Conference on Uncertainty in Artificial Intelligence (UAI-95), pages 394-402, 1995. [8] O. Madani, S. Hanks, and A. Condon.",
                "On the undecidability of probabilistic planning and related stochastic optimization problems.",
                "Artificial Intelligence Journal, 147(1-2):5-34, July 2003. [9] R. M. Neal and G. E. Hinton.",
                "A view of the EM algorithm 796 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) that justifies incremental, sparse, and other variants.",
                "In M. I. Jordan, editor, Learning in Graphical Models, pages 355-368.",
                "Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In Proceeding of AAMAS 2006, 2006. [11] J. Pineau, G. Gordon, and S. Thrun.",
                "Point-based value iteration: An anytime algorithm for pomdps.",
                "In International Joint Conference on Artificial Intelligence (IJCAI), pages 1025-1032, August 2003. [12] M. L. Puterman.",
                "Markov Decision Processes.",
                "Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics Section.",
                "Wiley-Interscience Publication, New York, 1994. [13] Z. Rabinovich and J. S. Rosenschein.",
                "<br>extended markov tracking</br> with an application to control.",
                "In The Workshop on Agent Tracking: Modeling Other Agents from Observations, at the Third International Joint Conference on Autonomous Agents and Multiagent Systems, pages 95-100, New-York, July 2004. [14] Z. Rabinovich and J. S. Rosenschein.",
                "Multiagent coordination by <br>extended markov tracking</br>.",
                "In The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 431-438, Utrecht, The Netherlands, July 2005. [15] Z. Rabinovich and J. S. Rosenschein.",
                "On the response of EMT-based control to interacting targets and models.",
                "In The Fifth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 465-470, Hakodate, Japan, May 2006. [16] R. F. Stengel.",
                "Optimal Control and Estimation.",
                "Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, and P. Varakantham.",
                "Conflicts in teamwork: Hybrids to the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 797"
            ],
            "original_annotated_samples": [
                "We show that a recently developed planning and control approach, <br>extended markov tracking</br> (EMT) is an instantiation of DBC.",
                "We show that the recently developed <br>extended markov tracking</br> (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC.",
                "This is followed by a review of the <br>extended markov tracking</br> (EMT) approach as a DBC-structured control regimen in Section 4.",
                "It then utilizes the Kullback-Leibler divergence measure to compose a momentary system dynamics estimator-the <br>extended markov tracking</br> (EMT) algorithm.",
                "We show that a recently developed technique of <br>extended markov tracking</br> (EMT) [13] is an instantiation of DBC."
            ],
            "translated_annotated_samples": [
                "Mostramos que un enfoque de planificación y control recientemente desarrollado, <br>Seguimiento Extendido de Markov</br> (EMT), es una instancia de DBC.",
                "Mostramos que el enfoque de <br>Seguimiento Extendido de Markov</br> (EMT) recientemente desarrollado [13, 14, 15] está subsumido por DBC, con EMT empleando selección de acciones codiciosa, que es una parametrización específica entre las opciones posibles dentro de DBC.",
                "Esto es seguido por una revisión del enfoque de <br>Seguimiento Markov Extendido</br> (EMT) como un régimen de control estructurado por DBC en la Sección 4.",
                "Luego utiliza la medida de divergencia de Kullback-Leibler para componer un estimador de dinámica de sistemas momentáneos: el algoritmo de <br>Seguimiento Extendido de Markov</br> (EMT).",
                "Mostramos que una técnica recientemente desarrollada de <br>Seguimiento Markov Extendido</br> (EMT) [13] es una instancia de DBC."
            ],
            "translated_text": "En este artículo presentamos el Control Basado en Dinámicas (DBC), un enfoque para la planificación y control de un agente en entornos estocásticos. A diferencia de los enfoques existentes, que buscan optimizar las recompensas esperadas (por ejemplo, en Problemas de Decisión de Markov Parcialmente Observables (POMDP)), DBC optimiza el comportamiento del sistema hacia las dinámicas del sistema especificadas. Mostramos que un enfoque de planificación y control recientemente desarrollado, <br>Seguimiento Extendido de Markov</br> (EMT), es una instancia de DBC. EMT emplea la selección de acciones codiciosa para proporcionar un algoritmo de control eficiente en entornos markovianos. Explotamos esta eficiencia en un conjunto de experimentos que aplicaron EMT multitarget a una clase de problemas de barrido de área (búsqueda de objetivos en movimiento). Mostramos que tales problemas pueden ser definidos de forma natural y resueltos eficientemente utilizando el marco de DBC y su instanciación de EMT. Categorías y Descriptores de Asignaturas I.2.8 [Resolución de Problemas, Métodos de Control y Búsqueda]: Teoría de Control; I.2.9 [Robótica]; I.2.11 [Inteligencia Artificial Distribuida]: Agentes Inteligentes Términos Generales Algoritmos, Teoría 1. INTRODUCCIÓN La planificación y el control constituyen un área de investigación central en sistemas multiagentes e inteligencia artificial. En los últimos años, los Procesos de Decisión de Markov Parcialmente Observables (POMDPs) [12] se han convertido en una base formal popular para la planificación en entornos estocásticos. En este marco, el problema de planificación y control suele abordarse imponiendo una función de recompensa y calculando una política (de elección de acciones) que sea óptima, en el sentido de que resultará en la mayor utilidad esperada. Si bien teóricamente atractivo, la complejidad de resolver óptimamente un POMDP es prohibitiva [8, 7]. Tomamos una visión alternativa de la planificación en entornos estocásticos. No utilizamos una función de recompensa basada en el estado, sino que optimizamos según un criterio diferente, una especificación basada en transiciones de la dinámica del sistema deseado. La idea aquí es ver la ejecución del plan como un proceso que obliga a un sistema (estocástico) a cambiar, y un plan como un proceso dinámico que moldea ese cambio de acuerdo con criterios deseados. Llamamos a este marco de planificación general Control Basado en Dinámicas (DBC). En DBC, el objetivo de un proceso de planificación (o control) es garantizar que el sistema cambiará de acuerdo con dinámicas objetivo específicas (potencialmente estocásticas). Dado que el comportamiento real del sistema puede desviarse de lo especificado por la dinámica objetivo (debido a la naturaleza estocástica del sistema), la planificación en tales entornos debe ser continua, de manera similar a los controladores en lazo cerrado clásicos. Aquí, la optimalidad se mide en términos de la probabilidad de magnitudes de desviación. En este documento, presentamos la estructura del Control Basado en Dinámicas. Mostramos que el enfoque de <br>Seguimiento Extendido de Markov</br> (EMT) recientemente desarrollado [13, 14, 15] está subsumido por DBC, con EMT empleando selección de acciones codiciosa, que es una parametrización específica entre las opciones posibles dentro de DBC. EMT es una implementación eficiente de DBC. Para evaluar DBC, llevamos a cabo un conjunto de experimentos aplicando EMT de múltiples objetivos al Juego de Etiquetas [11]; esta es una variante del problema de barrido de área, donde un agente intenta etiquetar a un objetivo móvil (presa) cuya posición no se conoce con certeza. Los datos experimentales demuestran que incluso con un modelo simple del entorno y un diseño simple de la dinámica del objetivo, se pueden lograr altas tasas de éxito tanto en la captura de la presa como en sorprender a la presa (como se expresa por la entropía observada de la posición de los agentes controlados). El documento está organizado de la siguiente manera. En la Sección 2 motivamos DBC utilizando problemas de barrido de área, y discutimos trabajos relacionados. La Sección 3 introduce la estructura de Control Basado en Dinámicas (DBC) y su especialización en entornos Markovianos. Esto es seguido por una revisión del enfoque de <br>Seguimiento Markov Extendido</br> (EMT) como un régimen de control estructurado por DBC en la Sección 4. Esa sección también discute las limitaciones del control basado en EMT en relación con el marco general de DBC. Los ajustes experimentales y los resultados se presentan a continuación en la Sección 5. La Sección 6 proporciona una breve discusión del enfoque general, y la Sección 7 ofrece algunas observaciones finales y direcciones para trabajos futuros. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2. MOTIVACIÓN Y TRABAJO RELACIONADO Muchos escenarios de la vida real tienen naturalmente una especificación de dinámica de destino estocástica, especialmente en aquellos dominios donde no existe un objetivo final, sino más bien un comportamiento del sistema (con propiedades específicas) que debe ser continuamente respaldado. Por ejemplo, los guardias de seguridad realizan barridos persistentes de un área para detectar cualquier signo de intrusión. Los ladrones astutos intentarán rastrear estos barridos y sincronizar su operación con puntos clave del movimiento de los guardias. Por lo tanto, es recomendable que la dinámica de movimiento de los guardias parezca irregular y aleatoria. El trabajo reciente de Paruchuri et al. [10] ha abordado dicha aleatorización en el contexto de POMDPs de agente único y distribuidos. El objetivo de ese trabajo era generar políticas que proporcionen una medida de aleatorización en la selección de acciones, manteniendo al mismo tiempo las recompensas dentro de niveles aceptables. Nuestro enfoque difiere de este trabajo en que DBC no optimiza recompensas esperadas, de hecho, no consideramos recompensas en absoluto, sino que en su lugar mantenemos dinámicas deseadas (incluyendo, pero no limitado a, la aleatorización). El juego de la mancha es otro ejemplo de la aplicabilidad del enfoque. Fue introducido en el trabajo de Pineau et al. [11]. Hay dos agentes que pueden moverse por un área, la cual está dividida en una cuadrícula. La cuadrícula puede tener celdas bloqueadas (agujeros) en las cuales ningún agente puede moverse. Un agente (el cazador) intenta moverse a una celda ocupada por el otro (la presa), de modo que estén ubicados en el mismo lugar (esto es una etiqueta exitosa). La presa busca evitar al cazador, siempre está al tanto de la posición del cazador, pero no sabe cómo se comportará el cazador, lo que abre la posibilidad de que un cazador sorprenda a la presa. El cazador conoce la ley de movimiento probabilístico de la presa, pero no conoce su ubicación actual. Tag es una instancia de una familia de problemas de barrido de área (persecución-evasión). En [11], el cazador modeló el problema utilizando un POMDP. Se definió una función de recompensa para reflejar el deseo de marcar la presa, y se calculó una política de acción para optimizar la recompensa recolectada con el tiempo. Debido a la complejidad intratable de determinar la política óptima, la política de acción calculada en ese documento fue esencialmente una aproximación. En este artículo, en lugar de formular una función de recompensa, utilizamos EMT para resolver el problema, especificando directamente la dinámica objetivo. De hecho, cualquier problema de búsqueda con movimiento aleatorio, la llamada clase de problemas de barrido de área, puede ser descrito a través de la especificación de la dinámica de dicho sistema objetivo. El Control Basado en Dinámicas proporciona un enfoque natural para resolver estos problemas. CONTROL BASADO EN DINÁMICA La especificación del Control Basado en Dinámica (DBC) se puede dividir en tres niveles interactivos: Nivel de Diseño del Entorno, Nivel de Usuario y Nivel de Agente. • El Nivel de Diseño del Entorno se ocupa de la especificación formal y modelado del entorno. Por ejemplo, este nivel especificaría las leyes de la física dentro del sistema y establecería sus parámetros, como la constante de gravitación. El Nivel de Usuario, a su vez, se basa en el modelo de entorno producido por el Diseño de Entorno para especificar la dinámica del sistema objetivo que desea observar. El Nivel de Usuario también especifica el procedimiento de estimación o aprendizaje para la dinámica del sistema, y la medida de desviación. En el escenario del guardia de museo mencionado anteriormente, estos corresponderían a un horario de barrido estocástico y una medida de sorpresa relativa entre el barrido especificado y el real. • El Nivel de Agente, a su vez, combina el modelo del entorno del Nivel de Diseño de Entorno, el procedimiento de estimación de la dinámica, la medida de desviación y la especificación de la dinámica objetivo del Nivel de Usuario, para producir una secuencia de acciones que creen dinámicas del sistema lo más cercanas posible a la especificación deseada. Dado nuestro interés en el desarrollo continuo de un sistema estocástico, como ocurre en la teoría de control clásica [16] y la planificación continua [4], así como en nuestro ejemplo de recorridos por museos, la pregunta es cómo el Nivel de Agente debe tratar las mediciones de desviación a lo largo del tiempo. Con este fin, utilizamos un umbral de probabilidad, es decir, nos gustaría que el Nivel del Agente maximice la probabilidad de que la medida de desviación permanezca por debajo de cierto umbral. La selección de acciones específicas depende entonces de la formalización del sistema. Una posibilidad sería crear una mezcla de tendencias de sistemas disponibles, similar a lo que sucede en las arquitecturas robóticas basadas en el comportamiento [1]. La otra alternativa sería depender del procedimiento de estimación proporcionado por el Nivel de Usuario para utilizar el modelo del Nivel de Diseño del Entorno del entorno para elegir acciones, de manera que se manipule al estimador de dinámicas para que crea que se ha logrado cierta dinámica. Ten en cuenta que esta manipulación no es directa, sino a través del entorno. Por lo tanto, para algoritmos de estimación lo suficientemente fuertes, la manipulación exitosa significaría una simulación exitosa de la dinámica objetivo especificada (es decir, más allá de discernir a través de la entrada sensorial disponible). Los niveles de DBC también pueden tener un retroceso de información (ver Figura 1). Por ejemplo, el Nivel de Agente podría proporcionar datos sobre la viabilidad de la dinámica del objetivo, permitiendo al Nivel de Usuario modificar el requisito, quizás centrándose en las características alcanzables del comportamiento del sistema. Los datos también estarían disponibles sobre la respuesta del sistema a diferentes acciones realizadas; combinados con un estimador de dinámica definido por el Nivel de Usuario, esto puede proporcionar una herramienta importante para la calibración del modelo del entorno en el Nivel de Diseño del Entorno. UserEnv. El modelo de agente de diseño Estimador de Dinámicas Ideales Estimador de Dinámicas Factibilidad de Respuesta del Sistema de Datos Figura 1: Flujo de datos del marco de trabajo DBC Ampliando la idea de los algoritmos Actor-Crítico [5], el flujo de datos de DBC puede proporcionar una buena base para el diseño de un algoritmo de aprendizaje. Por ejemplo, el Nivel de Usuario puede funcionar como un dispositivo exploratorio para un algoritmo de aprendizaje, inferir un objetivo dinámico ideal a partir del modelo del entorno en cuestión que expondría y verificaría las características más críticas del comportamiento del sistema. En este caso, los datos de viabilidad y respuesta del sistema a nivel de agente proporcionarían información clave para una actualización del modelo del entorno. De hecho, la combinación de datos de viabilidad y respuesta puede proporcionar una base para la aplicación de algoritmos de aprendizaje sólidos como EM [2, 9]. 3.1 DBC para entornos markovianos Para un entorno markoviano parcialmente observable, DBC puede especificarse de una manera más rigurosa. Observa cómo DBC descarta las recompensas y las reemplaza por otro criterio de optimalidad (las diferencias estructurales se resumen en la Tabla 1): • El nivel de Diseño del Entorno es especificar una tupla < S, A, T, O, Ω, s0 >, donde: - S es el conjunto de todos los posibles estados del entorno; - s0 es el estado inicial del entorno (que también puede ser visto como una distribución de probabilidad sobre S); La Sexta Conferencia Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 791 - A es el conjunto de todas las acciones posibles aplicables en el entorno; - T es la función de transición probabilística del entorno: T: S × A → Π(S). Es decir, T(s | a, s) es la probabilidad de que el entorno se mueva del estado s al estado s bajo la acción a; - O es el conjunto de todas las observaciones posibles. Así es como se vería la entrada del sensor para un observador externo; - Ω es la función de probabilidad de observación: Ω: S × A × S → Π(O). Es decir, Ω(o|s, a, s) es la probabilidad de observar o dado que el entorno ha pasado del estado s al estado s bajo la acción a. • Nivel de Usuario, en el caso de un entorno markoviano, opera en el conjunto de dinámicas del sistema descritas por una familia de probabilidades condicionales F = {τ: S × A → Π(S)}. Por lo tanto, la especificación de la dinámica del objetivo puede expresarse como q ∈ F, y el algoritmo de aprendizaje o seguimiento puede representarse como una función L : O×(A×O)∗ → F; es decir, mapea secuencias de observaciones y acciones realizadas hasta el momento en una estimación τ ∈ F de la dinámica del sistema. Hay muchas variaciones posibles disponibles a nivel de usuario para definir la divergencia entre la dinámica del sistema; varias de ellas son: - Distancia de traza o distancia L1 entre dos distribuciones p y q definida por D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Medida de fidelidad de distancia F(p(·), q(·)) = x p(x)q(x) - Divergencia de Kullback-Leibler DKL(p(·) q(·)) = x p(x) log p(x) q(x) Nótese que las dos últimas no son en realidad métricas sobre el espacio de distribuciones posibles, pero tienen interpretaciones significativas e importantes. Por ejemplo, la divergencia de Kullback-Leibler es una herramienta importante de la teoría de la información [3] que permite medir el costo de codificar una fuente de información gobernada por q, asumiendo que está gobernada por p. El Nivel de Usuario también define el umbral de probabilidad de desviación de la dinámica θ. • El Nivel de Agente se enfrenta entonces a un problema de seleccionar una función de señal de control a∗ para satisfacer un problema de minimización de la siguiente manera: a∗ = arg min a Pr(d(τa, q) > θ) donde d(τa, q) es una variable aleatoria que describe la desviación de la estimación de la dinámica τa, creada por L bajo la señal de control a, de la dinámica ideal q. Implícito en este problema de minimización es que L es manipulado a través del entorno, basado en el modelo de entorno producido por el Nivel de Diseño del Entorno. 3.2 Vista DBC del Espacio de Estados Es importante tener en cuenta la visión complementaria que DBC y POMDPs tienen sobre el espacio de estados del entorno. Los POMDP consideran el estado como una instantánea estacionaria del entorno; cualquier atributo de la secuencia de estados que se busque se alcanza a través de las propiedades del proceso de control, en este caso la acumulación de recompensas. Esto se puede ver como si la secuencia de estados y los atributos de esa secuencia solo fueran introducidos por y para el mecanismo de control, la política POMDP. DBC se enfoca en el principio subyacente de la secuenciación de estados, la dinámica del sistema. La especificación de la dinámica de los DBCs puede utilizar el espacio de estado del entorno como un medio para describir, discernir y preservar los cambios que ocurren dentro del sistema. Como resultado, DBC tiene una mayor capacidad para expresar propiedades de secuenciación de estados, las cuales están fundamentadas en el modelo del entorno y su definición del espacio de estados. Por ejemplo, considera la tarea de moverse a través de terreno accidentado hacia un objetivo y alcanzarlo lo más rápido posible. Los POMDPs codificarían el terreno como puntos en el espacio de estados, mientras que la velocidad se garantizaría mediante una recompensa negativa por cada paso dado sin alcanzar la meta. Acumular una recompensa más alta solo se puede lograr mediante un movimiento más rápido. Alternativamente, el espacio de estados podría incluir directamente la noción de velocidad. Para los POMDPs, esto significaría que el mismo concepto está codificado dos veces, en cierto sentido: directamente en el espacio de estados e indirectamente en la acumulación de recompensas. Ahora, incluso si la función de recompensa codificara más y detalles más finos de las propiedades del movimiento, la solución POMDP tendría que buscar en un espacio mucho más grande de políticas, aunque aún esté guiada por el concepto implícito del procedimiento de acumulación de recompensas. Por otro lado, la expresión del objetivo táctico de variaciones y correlaciones entre la posición y la velocidad del movimiento ahora se basa en la representación del espacio de estados. En esta situación, cualquier restricción adicional, como la suavidad del movimiento, los límites de velocidad en diferentes ubicaciones o las reducciones de velocidad durante giros bruscos, son expresadas explícita y uniformemente por el objetivo táctico, y pueden resultar en una selección de acciones más rápida y efectiva por un algoritmo DBC. 4. El Control Basado en EMT como un DBC Recientemente, se introdujo un algoritmo de control llamado Control Basado en EMT [13], que instancia el marco de trabajo DBC. Aunque proporciona una solución ávida aproximada en el sentido de DBC, los experimentos iniciales utilizando el control basado en EMT han sido alentadores [14, 15]. El control basado en EMT se basa en la definición del entorno markoviano, como en el caso de los POMDP, pero sus Niveles de Usuario y Agente son del tipo de optimalidad DBC markoviana. • El Nivel de Usuario del control basado en EMT define una dinámica del sistema objetivo en un caso limitado independiente de la acción: qEMT: S → Π(S). Luego utiliza la medida de divergencia de Kullback-Leibler para componer un estimador de dinámica de sistemas momentáneos: el algoritmo de <br>Seguimiento Extendido de Markov</br> (EMT). El algoritmo mantiene una estimación de la dinámica del sistema τt EMT que es capaz de explicar el cambio reciente en un estimador de estado auxiliar bayesiano de pt−1 a pt, y lo actualiza de manera conservadora utilizando la divergencia de Kullback-Leibler. Dado que τt EMT y pt−1,t son respectivamente las probabilidades condicionales y marginales sobre el espacio de estados del sistema, la explicación simplemente significa que pt(s) = s τt EMT (s | s)pt−1(s), y la actualización de la estimación de la dinámica se realiza resolviendo un 792 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Tabla 1: Estructura de POMDP vs. Control basado en Dinámicas en Entorno Markoviano Nivel Enfoque MDP Markoviano DBC Entorno < S, A, T, O, Ω >, donde S - conjunto de estados A - conjunto de acciones Diseño T : S × A → Π(S) - transición O - conjunto de observaciones Ω : S × A × S → Π(O) Usuario r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - función de recompensa q - dinámicas ideales F - remodelación de recompensa L - estimador de dinámicas θ - umbral Agente π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) problema de minimización: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • El nivel de Agente en el control basado en EMT es subóptimo con respecto a DBC (aunque permanece dentro del marco de DBC), realizando selección de acciones codiciosas basadas en la predicción de la reacción de EMT. La predicción se basa en el modelo del entorno proporcionado por el nivel de Diseño del Entorno, de modo que si denotamos por Ta la función de transición de entornos limitada a la acción a, y pt−1 es el estimador de estado auxiliar del sistema bayesiano, entonces la elección de control basada en EMT se describe por a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1). Nótese que esto sigue precisamente el marco DBC Markoviano: la optimalidad recompensante de los POMDPs se descarta, y en su lugar se manipula un estimador de dinámicas (EMT en este caso) a través de los efectos de la acción en el entorno para producir una estimación cercana a la dinámica del sistema objetivo especificado. Sin embargo, como mencionamos, el control EMT ingenuo es subóptimo en el sentido de DBC y tiene varias limitaciones adicionales que no existen en el marco general de DBC (discutido en la Sección 4.2). 4.1 EMT de múltiples objetivos. En ocasiones, puede haber varias preferencias de comportamiento. Por ejemplo, en el caso de los guardias de museo, algunos objetos de arte están más fuertemente custodiados, lo que requiere que los guardias permanezcan más a menudo en su cercanía. Por otro lado, no se debe dejar ningún rincón del museo sin revisar, lo que requiere un movimiento constante. El éxito de la seguridad en un museo requeriría que los guardias se adhieran y equilibren ambos comportamientos. Para el control basado en EMT, esto significaría enfrentar varios objetivos tácticos {qk}K k=1, y la pregunta sería cómo fusionar y equilibrarlos. Un mecanismo de equilibrio se puede aplicar para resolver este problema. Ten en cuenta que el control basado en EMT, al seleccionar una acción, crea un vector de preferencia sobre el conjunto de acciones basado en su rendimiento predicho con respecto a un objetivo dado. Si estos vectores de preferencia están normalizados, pueden combinarse en una única preferencia unificada. Esto requiere reemplazar la selección de acciones basada en EMT estándar por el algoritmo a continuación [15]: • Dado: - un conjunto de dinámicas objetivo {qk}K k=1, - vector de pesos w(k) • Seleccionar la acción de la siguiente manera - Para cada acción a ∈ A predecir la distribución de estado futuro ¯pa t+1 = Ta ∗ pt; - Para cada acción, calcular Da = H(¯pa t+1, pt, PDt) - Para cada a ∈ A y objetivo táctico qk, denotar V (a, k) = DKL (Da qk) pt. Sea Vk(a) = 1 Zk V (a, k), donde Zk = a∈A V (a, k) es un factor de normalización. - Selecciona a∗ = arg min a k k=1 w(k)Vk(a) El vector de pesos w = (w1, ..., wK ) permite ajustar la importancia entre las dinámicas objetivo sin necesidad de rediseñar los objetivos mismos. Este método de equilibrio también está integrado de forma transparente en el flujo de operación basado en EMT. Limitaciones del Control Basado en EMT El control basado en EMT es una representación subóptima (en el sentido de DBC) de la estructura DBC. Limita al Usuario al obligar a EMT a ser su algoritmo de seguimiento dinámico, y reemplaza la optimización del Agente por la selección de acciones codiciosa. Este tipo de combinación, sin embargo, es común en algoritmos en línea. Aunque es necesario un mayor desarrollo de los controladores basados en EMT, la evidencia hasta ahora sugiere que incluso la forma más simple del algoritmo posee una gran cantidad de potencia y muestra tendencias que son óptimas en el sentido de DBC. Hay dos limitaciones adicionales específicas de los técnicos en emergencias médicas (EMT, por sus siglas en inglés) al control basado en EMT que son evidentes en este momento. Ambos ya tienen soluciones parciales y son objeto de investigación continua. La primera limitación es el problema de la preferencia negativa. En el marco de POMDP, por ejemplo, esto se captura de manera sencilla, a través de The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 793 la aparición de valores con signos diferentes dentro de la estructura de recompensa. Para el control basado en EMT, sin embargo, la preferencia negativa significa que uno quisiera evitar cierta distribución sobre las secuencias de desarrollo del sistema; sin embargo, el control basado en EMT se concentra en acercarse lo más posible a una distribución. La evitación es, por lo tanto, antinatural en el control basado en EMT nativo. La segunda limitación proviene del hecho de que el modelado del entorno estándar puede crear acciones sensoriales puras, acciones que no cambian el estado del mundo y solo difieren en la forma en que se reciben las observaciones y la calidad de las observaciones recibidas. Dado que el estado del mundo no cambia, el control basado en EMT no sería capaz de diferenciar entre diferentes acciones sensoriales. Ten en cuenta que ambas limitaciones del control basado en EMT están ausentes en el marco general de DBC, ya que puede tener un algoritmo de seguimiento capaz de considerar acciones sensoriales puras y, a diferencia de la divergencia de Kullback-Leibler, una medida de desviación de distribución capaz de manejar preferencias negativas. 5. El juego de la \"carrera de relevos\" fue introducido por primera vez en [11]. Es un problema de agente único de capturar una presa, y pertenece a la clase de problemas de barrido de área. Un ejemplo de dominio se muestra en la Figura 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figura 2: Dominio de etiquetas; un agente (A) intenta buscar y capturar una presa (Q). El juego de etiquetas limita extremadamente la percepción de los agentes, de modo que el agente solo puede detectar la presa si están ubicados en la misma celda del mundo de cuadrícula. En la versión clásica del juego, la co-locación conduce a una observación especial, y la acción de Etiqueta se puede realizar. Modificamos ligeramente esta configuración: el momento en que ambos agentes ocupan la misma celda, el juego termina. Como resultado, tanto el agente como su presa tienen la misma capacidad de movimiento, lo que les permite moverse en cuatro direcciones, Norte, Sur, Este y Oeste. Estos forman un espacio formal de acciones dentro de un entorno markoviano. El espacio de estados del entorno formal markoviano está descrito por el producto cruz de las posiciones del agente y la presa. Para la Figura 2, sería S = {s0, ..., s23} × {s0, ..., s23}. Los efectos de una acción tomada por el agente son deterministas, pero el entorno en general tiene una respuesta estocástica debido al movimiento de la presa. Con probabilidad q0 1 se queda en su lugar, y con probabilidad 1 − q0 se mueve a una celda adyacente más lejos del agente 1. En nuestros experimentos, esto se tomó como q0 = 0.2. Por lo tanto, para la instancia mostrada en la Figura 2 y q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Aunque el comportamiento evasivo de la presa es conocido por el agente, la posición de la presa no lo es. La única información sensorial disponible para el agente es su propia ubicación. Utilizamos EMT y especificamos directamente la dinámica del objetivo. Para el juego de la mancha, podemos formular fácilmente tres tendencias principales: atrapar al perseguido, mantenerse en movimiento y acechar al perseguido. Esto resulta en las siguientes tres dinámicas de objetivo: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 en otro caso Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 en otro caso Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Nótese que ninguno de los objetivos anteriores es directamente alcanzable; por ejemplo, si Qt = s9 y At = s11, no hay ninguna acción que pueda llevar al agente a At+1 = s9 como lo requiere la dinámica de objetivo Tcatch. Realizamos varios experimentos para evaluar el rendimiento de EMT en el Juego de Etiquetas. Se utilizaron tres configuraciones del dominio mostrado en la Figura 3, cada una planteando un desafío diferente para el agente debido a la observabilidad parcial. En cada escenario, se realizó un conjunto de 1000 ejecuciones con un límite de tiempo de 100 pasos. En cada ejecución, la posición inicial tanto del agente como de su presa fue seleccionada al azar; esto significa que, en lo que respecta al agente, la posición inicial de la presa estaba distribuida de forma uniforme en todo el espacio de celdas del dominio. También utilizamos dos variaciones de la función de observabilidad del entorno. En la primera versión, la función de observabilidad mapeaba todas las posiciones articulares del cazador y la presa en la posición del cazador como observación. En el segundo, solo aquellas posiciones conjuntas en las que el cazador y la presa ocupaban ubicaciones diferentes fueron mapeadas en la ubicación del cazador. La segunda versión de hecho utilizó y expresó el hecho de que una vez que el cazador y la presa ocupan la misma celda, el juego termina. Los resultados de estos experimentos se muestran en la Tabla 2. Equilibrando la dinámica de captura, movimiento y seguimiento descrita en la sección anterior mediante el vector de peso [0.8, 0.1, 0.1], EMT logró un rendimiento estable en los tres dominios. Aunque las comparaciones directas son difíciles de hacer, el rendimiento de los EMT mostró una notable eficiencia en comparación con el enfoque POMDP. A pesar de una implementación simple e ineficiente del algoritmo EMT en Matlab, el tiempo de decisión para cualquier paso dado promedió significativamente por debajo de 1 segundo en todos los experimentos. Para el dominio de la arena abierta irregular, que resultó ser el más difícil, se completaron 1000 ejecuciones de experimentos limitadas a 100 pasos cada una, un total de 42411 pasos, en poco menos de 6 horas. Es decir, más de 4 × 104 pasos en línea tomaron una orden de magnitud menos tiempo que la computación sin conexión de la política POMDP en [11]. La importancia de esta diferencia se destaca aún más por el hecho de que, en caso de que cambien los parámetros del modelo del entorno, la naturaleza en línea de EMT le permitiría mantener su rendimiento, mientras que la política POMDP necesitaría ser recalculada, lo que requeriría una vez más un gran costo computacional adicional. También probamos la entropía de la frecuencia de las células de comportamiento, medidas empíricas de los datos de prueba. Como muestran la Figura 4 y la Figura 5, empir794 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 3: Se utilizaron estas configuraciones del espacio de juego de Tag: a) múltiples callejones sin salida, b) arena abierta irregular, c) pasillo circular. Tabla 2: Rendimiento de la solución basada en EMT en tres dominios de juego de Tag y dos modelos de observabilidad: I) cantera de omniposición, II) la cantera no está en la posición de los cazadores. Modelo Dominio Captura% E(Pasos) Tiempo/Paso I Callejones sin salida 100 14.8 72(mSeg) Arena 80.2 42.4 500(mSeg) Círculo 91.4 34.6 187(mSeg) II Callejones sin salida 100 13.2 91(mSeg) Arena 96.8 28.67 396(mSeg) Círculo 94.4 31.63 204(mSeg) La entropía lógica crece con la duración de la interacción. Para las carreras en las que la presa no fue capturada de inmediato, la entropía alcanza valores entre 0.85 y 0.952 para diferentes carreras y escenarios. A medida que el agente busca activamente la presa, la entropía nunca alcanza su máximo. Una característica del gráfico de entropía para el escenario de arena abierta llamó especialmente nuestra atención en el caso del modelo de observación de cantera omniposicional. Cerca del límite máximo de longitud de prueba (100 pasos), la entropía cayó repentinamente. Un análisis adicional de los datos mostró que bajo ciertas circunstancias, se produce un comportamiento fluctuante en el que el agente se enfrenta a versiones igualmente viables del comportamiento de seguir a la presa. Dado que el algoritmo EMT tiene una selección de acciones codiciosa, y el espacio de estados no codifica ninguna forma de compromiso (ni siquiera velocidad o aceleración), el agente queda atrapado dentro de una pequeña porción de celdas. Básicamente, se intenta seguir simultáneamente varios cursos de acción, todos los cuales son consistentes con la dinámica objetivo. Este comportamiento no ocurrió en nuestro segundo modelo de observación, ya que redujo significativamente el conjunto de cursos de acción elegibles, contribuyendo esencialmente a romper el empate entre ellos. 6. DISCUSIÓN El diseño de la solución EMT para el Juego de Etiquetas expone la diferencia fundamental en el enfoque de planificación y control entre EMT o DBC, por un lado, y el enfoque más familiar de POMDP, por otro lado. POMDP define una estructura de recompensa para optimizar e influencia la dinámica del sistema de forma indirecta a través de esa optimización. EMT descarta cualquier esquema de recompensa y, en cambio, mide e influye directamente en la dinámica del sistema. Se calculó la entropía 2 utilizando el logaritmo con base igual al número de ubicaciones posibles dentro del dominio; esto escala adecuadamente la expresión de entropía en el rango [0, 1] para todos los dominios. Por lo tanto, para el Juego de Etiquetas, no buscamos una función de recompensa que codificara y expresara nuestra preferencia sobre el comportamiento de los agentes, sino que establecimos directamente tres preferencias de comportamiento (heurísticas) como base para mantener la dinámica del objetivo. Los datos experimentales muestran que estos objetivos no necesitan ser directamente alcanzables a través de las acciones de los agentes. Sin embargo, aún queda por explorar la relación entre el rendimiento de la EMT y la alcanzabilidad de la dinámica objetivo. Los datos del experimento del juego de etiquetas también revelaron la diferente importancia que DBC y POMDPs otorgan a la formulación del espacio de estados del entorno. Los POMDPs dependen enteramente del mecanismo de maximización de la acumulación de recompensas, es decir, la formación del procedimiento de selección de acciones para lograr la secuenciación de estados necesaria. DBC, por otro lado, tiene dos fuentes de especificación de secuenciación: a través de las propiedades de un procedimiento de selección de acciones y a través de una especificación directa dentro de la dinámica objetivo. La importancia de la segunda fuente fue subrayada por los datos del experimento del Juego de Etiquetas, en el que el algoritmo EMT codicioso, aplicado a una especificación de espacio de estados tipo POMDP, falló, ya que la descripción del objetivo sobre dicho espacio de estados no era capaz de codificar las tendencias de comportamiento necesarias, por ejemplo, la resolución de empates y el compromiso con el movimiento dirigido. Las diferencias estructurales entre DBC (y EMT en particular) y POMDPs impiden la comparación directa de rendimiento y los sitúan en pistas complementarias, cada uno dentro de un nicho adecuado. Por ejemplo, los POMDP podrían ser vistos como una formulación mucho más natural de problemas económicos de toma de decisiones secuenciales, mientras que EMT se ajusta mejor a la demanda continua de cambios estocásticos, como ocurre en muchos problemas de robótica o de agentes incorporados. Las propiedades complementarias de POMDPs y EMT pueden ser aún más explotadas. Existe un interés reciente en utilizar POMDPs en soluciones híbridas [17], en las cuales los POMDPs pueden ser utilizados junto con otros enfoques de control para proporcionar resultados que no son fácilmente alcanzables con ninguno de los enfoques por sí solos. DBC puede ser un socio efectivo en una solución híbrida como esta. Por ejemplo, los POMDP tienen requisitos de tiempo fuera de línea prohibitivamente grandes para el cálculo de políticas, pero pueden ser fácilmente utilizados en entornos más simples para exponer tendencias de comportamiento beneficiosas; esto puede servir como una forma de dinámica objetivo que se proporciona a EMT en un dominio más grande para su operación en línea. 7. CONCLUSIONES Y TRABAJO FUTURO En este artículo, hemos presentado una nueva perspectiva sobre el proceso de planificación y control en entornos estocásticos, en forma del marco de Control Basado en Dinámicas (DBC). DBC formula la tarea de planificar como el apoyo a la dinámica de un sistema objetivo especificado, que describe las propiedades necesarias del cambio dentro del entorno. La optimalidad de los planes de acción de DBC se mide en el Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Calles sin salida 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Círculo Figura 4: Modelo de Observación I: Cantera omniposicional. Desarrollo de entropía con longitud de juego de etiqueta para los tres escenarios experimentales: a) múltiples callejones sin salida, b) arena abierta irregular, c) pasillo circular. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Callejones sin salida 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Círculo Figura 5: Modelo de observación II: cantera no observada en la posición de los cazadores. Desarrollo de la entropía con la longitud del juego de etiquetas para los tres escenarios experimentales: a) múltiples callejones sin salida, b) arena abierta irregular, c) pasillo circular, con respecto a la desviación de la dinámica del sistema real de la dinámica objetivo. Mostramos que una técnica recientemente desarrollada de <br>Seguimiento Markov Extendido</br> (EMT) [13] es una instancia de DBC. ",
            "candidates": [],
            "error": [
                [
                    "Seguimiento Extendido de Markov",
                    "Seguimiento Extendido de Markov",
                    "Seguimiento Markov Extendido",
                    "Seguimiento Extendido de Markov",
                    "Seguimiento Markov Extendido"
                ]
            ]
        },
        "multi-agent system": {
            "translated_key": "sistema multiagente",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Dynamics Based Control with an Application to Area-Sweeping Problems Zinovi Rabinovich Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel jeff@cs.huji.ac.il Gal A. Kaminka The MAVERICK Group Department of Computer Science Bar Ilan University, Israel galk@cs.biu.ac.il ABSTRACT In this paper we introduce Dynamics Based Control (DBC), an approach to planning and control of an agent in stochastic environments.",
                "Unlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics.",
                "We show that a recently developed planning and control approach, Extended Markov Tracking (EMT) is an instantiation of DBC.",
                "EMT employs greedy action selection to provide an efficient control algorithm in Markovian environments.",
                "We exploit this efficiency in a set of experiments that applied multitarget EMT to a class of area-sweeping problems (searching for moving targets).",
                "We show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation.",
                "Categories and Subject Descriptors I.2.8 [Problem Solving, Control Methods, and Search]: Control Theory; I.2.9 [Robotics]; I.2.11 [Distributed Artificial Intelligence]: Intelligent Agents General Terms Algorithms, Theory 1.",
                "INTRODUCTION Planning and control constitutes a central research area in multiagent systems and artificial intelligence.",
                "In recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in stochastic environments.",
                "In this framework, the planning and control problem is often addressed by imposing a reward function, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility.",
                "While theoretically attractive, the complexity of optimally solving a POMDP is prohibitive [8, 7].",
                "We take an alternative view of planning in stochastic environments.",
                "We do not use a (state-based) reward function, but instead optimize over a different criterion, a transition-based specification of the desired system dynamics.",
                "The idea here is to view planexecution as a process that compels a (stochastic) system to change, and a plan as a dynamic process that shapes that change according to desired criteria.",
                "We call this general planning framework Dynamics Based Control (DBC).",
                "In DBC, the goal of a planning (or control) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) target dynamics.",
                "As actual system behavior may deviate from that which is specified by target dynamics (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16].",
                "Here, optimality is measured in terms of probability of deviation magnitudes.",
                "In this paper, we present the structure of Dynamics Based Control.",
                "We show that the recently developed Extended Markov Tracking (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC.",
                "EMT is an efficient instantiation of DBC.",
                "To evaluate DBC, we carried out a set of experiments applying multi-target EMT to the Tag Game [11]; this is a variant on the area sweeping problem, where an agent is trying to tag a moving target (quarry) whose position is not known with certainty.",
                "Experimental data demonstrates that even with a simple model of the environment and a simple design of target dynamics, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agents position).",
                "The paper is organized as follows.",
                "In Section 2 we motivate DBC using area-sweeping problems, and discuss related work.",
                "Section 3 introduces the Dynamics Based Control (DBC) structure, and its specialization to Markovian environments.",
                "This is followed by a review of the Extended Markov Tracking (EMT) approach as a DBC-structured control regimen in Section 4.",
                "That section also discusses the limitations of EMT-based control relative to the general DBC framework.",
                "Experimental settings and results are then presented in Section 5.",
                "Section 6 provides a short discussion of the overall approach, and Section 7 gives some concluding remarks and directions for future work. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "MOTIVATION AND RELATED WORK Many real-life scenarios naturally have a stochastic target dynamics specification, especially those domains where there exists no ultimate goal, but rather system behavior (with specific properties) that has to be continually supported.",
                "For example, security guards perform persistent sweeps of an area to detect any sign of intrusion.",
                "Cunning thieves will attempt to track these sweeps, and time their operation to key points of the guards motion.",
                "It is thus advisable to make the guards motion dynamics appear irregular and random.",
                "Recent work by Paruchuri et al. [10] has addressed such randomization in the context of single-agent and distributed POMDPs.",
                "The goal in that work was to generate policies that provide a measure of action-selection randomization, while maintaining rewards within some acceptable levels.",
                "Our focus differs from this work in that DBC does not optimize expected rewards-indeed we do not consider rewards at all-but instead maintains desired dynamics (including, but not limited to, randomization).",
                "The Game of Tag is another example of the applicability of the approach.",
                "It was introduced in the work by Pineau et al. [11].",
                "There are two agents that can move about an area, which is divided into a grid.",
                "The grid may have blocked cells (holes) into which no agent can move.",
                "One agent (the hunter) seeks to move into a cell occupied by the other (the quarry), such that they are co-located (this is a successful tag).",
                "The quarry seeks to avoid the hunter agent, and is always aware of the hunters position, but does not know how the hunter will behave, which opens up the possibility for a hunter to surprise the prey.",
                "The hunter knows the quarrys probabilistic law of motion, but does not know its current location.",
                "Tag is an instance of a family of area-sweeping (pursuit-evasion) problems.",
                "In [11], the hunter modeled the problem using a POMDP.",
                "A reward function was defined, to reflect the desire to tag the quarry, and an action policy was computed to optimize the reward collected over time.",
                "Due to the intractable complexity of determining the optimal policy, the action policy computed in that paper was essentially an approximation.",
                "In this paper, instead of formulating a reward function, we use EMT to solve the problem, by directly specifying the target dynamics.",
                "In fact, any search problem with randomized motion, the socalled class of area sweeping problems, can be described through specification of such target system dynamics.",
                "Dynamics Based Control provides a natural approach to solving these problems. 3.",
                "DYNAMICS BASED CONTROL The specification of Dynamics Based Control (DBC) can be broken into three interacting levels: Environment Design Level, User Level, and Agent Level. • Environment Design Level is concerned with the formal specification and modeling of the environment.",
                "For example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation constant. • User Level in turn relies on the environment model produced by Environment Design to specify the target system dynamics it wishes to observe.",
                "The User Level also specifies the estimation or learning procedure for system dynamics, and the measure of deviation.",
                "In the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping. • Agent Level in turn combines the environment model from the Environment Design level, the dynamics estimation procedure, the deviation measure and the target dynamics specification from User Level, to produce a sequence of actions that create system dynamics as close as possible to the targeted specification.",
                "As we are interested in the continual development of a stochastic system, such as happens in classical control theory [16] and continual planning [4], as well as in our example of museum sweeps, the question becomes how the Agent Level is to treat the deviation measurements over time.",
                "To this end, we use a probability threshold-that is, we would like the Agent Level to maximize the probability that the deviation measure will remain below a certain threshold.",
                "Specific action selection then depends on system formalization.",
                "One possibility would be to create a mixture of available system trends, much like that which happens in Behavior-Based Robotic architectures [1].",
                "The other alternative would be to rely on the estimation procedure provided by the User Level-to utilize the Environment Design Level model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a certain dynamics has been achieved.",
                "Notice that this manipulation is not direct, but via the environment.",
                "Thus, for strong enough estimator algorithms, successful manipulation would mean a successful simulation of the specified target dynamics (i.e., beyond discerning via the available sensory input).",
                "DBC levels can also have a back-flow of information (see Figure 1).",
                "For instance, the Agent Level could provide data about target dynamics feasibility, allowing the User Level to modify the requirement, perhaps focusing on attainable features of system behavior.",
                "Data would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the User Level, this can provide an important tool for the environment model calibration at the Environment Design Level.",
                "UserEnv.",
                "Design Agent Model Ideal Dynamics Estimator Estimator Dynamics Feasibility System Response Data Figure 1: Data flow of the DBC framework Extending upon the idea of Actor-Critic algorithms [5], DBC data flow can provide a good basis for the design of a learning algorithm.",
                "For example, the User Level can operate as an exploratory device for a learning algorithm, inferring an ideal dynamics target from the environment model at hand that would expose and verify most critical features of system behavior.",
                "In this case, feasibility and system response data from the Agent Level would provide key information for an environment model update.",
                "In fact, the combination of feasibility and response data can provide a basis for the application of strong learning algorithms such as EM [2, 9]. 3.1 DBC for Markovian Environments For a Partially Observable Markovian Environment, DBC can be specified in a more rigorous manner.",
                "Notice how DBC discards rewards, and replaces it by another optimality criterion (structural differences are summarized in Table 1): • Environment Design level is to specify a tuple < S, A, T, O, Ω, s0 >, where: - S is the set of all possible environment states; - s0 is the initial state of the environment (which can also be viewed as a probability distribution over S); The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 791 - A is the set of all possible actions applicable in the environment; - T is the environments probabilistic transition function: T : S ×A → Π(S).",
                "That is, T(s |a, s) is the probability that the environment will move from state s to state s under action a; - O is the set of all possible observations.",
                "This is what the sensor input would look like for an outside observer; - Ω is the observation probability function: Ω : S × A × S → Π(O).",
                "That is, Ω(o|s , a, s) is the probability that one will observe o given that the environment has moved from state s to state s under action a. • User Level, in the case of Markovian environment, operates on the set of system dynamics described by a family of conditional probabilities F = {τ : S × A → Π(S)}.",
                "Thus specification of target dynamics can be expressed by q ∈ F, and the learning or tracking algorithm can be represented as a function L : O×(A×O)∗ → F; that is, it maps sequences of observations and actions performed so far into an estimate τ ∈ F of system dynamics.",
                "There are many possible variations available at the User Level to define divergence between system dynamics; several of them are: - Trace distance or L1 distance between two distributions p and q defined by D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Fidelity measure of distance F(p(·), q(·)) = x p(x)q(x) - Kullback-Leibler divergence DKL(p(·) q(·)) = x p(x) log p(x) q(x) Notice that the latter two are not actually metrics over the space of possible distributions, but nevertheless have meaningful and important interpretations.",
                "For instance, KullbackLeibler divergence is an important tool of information theory [3] that allows one to measure the price of encoding an information source governed by q, while assuming that it is governed by p. The User Level also defines the threshold of dynamics deviation probability θ. • Agent Level is then faced with a problem of selecting a control signal function a∗ to satisfy a minimization problem as follows: a∗ = arg min a Pr(d(τa, q) > θ) where d(τa, q) is a random variable describing deviation of the dynamics estimate τa, created by L under control signal a, from the ideal dynamics q.",
                "Implicit in this minimization problem is that L is manipulated via the environment, based on the environment model produced by the Environment Design Level. 3.2 DBC View of the State Space It is important to note the complementary view that DBC and POMDPs take on the state space of the environment.",
                "POMDPs regard state as a stationary snap-shot of the environment; whatever attributes of state sequencing one seeks are reached through properties of the control process, in this case reward accumulation.",
                "This can be viewed as if the sequencing of states and the attributes of that sequencing are only introduced by and for the controlling mechanism-the POMDP policy.",
                "DBC concentrates on the underlying principle of state sequencing, the system dynamics.",
                "DBCs target dynamics specification can use the environments state space as a means to describe, discern, and preserve changes that occur within the system.",
                "As a result, DBC has a greater ability to express state sequencing properties, which are grounded in the environment model and its state space definition.",
                "For example, consider the task of moving through rough terrain towards a goal and reaching it as fast as possible.",
                "POMDPs would encode terrain as state space points, while speed would be ensured by negative reward for every step taken without reaching the goalaccumulating higher reward can be reached only by faster motion.",
                "Alternatively, the state space could directly include the notion of speed.",
                "For POMDPs, this would mean that the same concept is encoded twice, in some sense: directly in the state space, and indirectly within reward accumulation.",
                "Now, even if the reward function would encode more, and finer, details of the properties of motion, the POMDP solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure.",
                "On the other hand, the tactical target expression of variations and correlations between position and speed of motion is now grounded in the state space representation.",
                "In this situation, any further constraints, e.g., smoothness of motion, speed limits in different locations, or speed reductions during sharp turns, are explicitly and uniformly expressed by the tactical target, and can result in faster and more effective action selection by a DBC algorithm. 4.",
                "EMT-BASED CONTROL AS A DBC Recently, a control algorithm was introduced called EMT-based Control [13], which instantiates the DBC framework.",
                "Although it provides an approximate greedy solution in the DBC sense, initial experiments using EMT-based control have been encouraging [14, 15].",
                "EMT-based control is based on the Markovian environment definition, as in the case with POMDPs, but its User and Agent Levels are of the Markovian DBC type of optimality. • User Level of EMT-based control defines a limited-case target system dynamics independent of action: qEMT : S → Π(S).",
                "It then utilizes the Kullback-Leibler divergence measure to compose a momentary system dynamics estimator-the Extended Markov Tracking (EMT) algorithm.",
                "The algorithm keeps a system dynamics estimate τt EMT that is capable of explaining recent change in an auxiliary Bayesian system state estimator from pt−1 to pt, and updates it conservatively using Kullback-Leibler divergence.",
                "Since τt EMT and pt−1,t are respectively the conditional and marginal probabilities over the systems state space, explanation simply means that pt(s ) = s τt EMT (s |s)pt−1(s), and the dynamics estimate update is performed by solving a 792 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: Structure of POMDP vs. Dynamics-Based Control in Markovian Environment Level Approach MDP Markovian DBC Environment < S, A, T, O, Ω >,where S - set of states A - set of actions Design T : S × A → Π(S) - transition O - observation set Ω : S × A × S → Π(O) User r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - reward function q - ideal dynamics F - reward remodeling L - dynamics estimator θ - threshold Agent π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) minimization problem: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • Agent Level in EMT-based control is suboptimal with respect to DBC (though it remains within the DBC framework), performing greedy action selection based on prediction of EMTs reaction.",
                "The prediction is based on the environment model provided by the Environment Design level, so that if we denote by Ta the environments transition function limited to action a, and pt−1 is the auxiliary Bayesian system state estimator, then the EMT-based control choice is described by a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1) Note that this follows the Markovian DBC framework precisely: the rewarding optimality of POMDPs is discarded, and in its place a dynamics estimator (EMT in this case) is manipulated via action effects on the environment to produce an estimate close to the specified target system dynamics.",
                "Yet as we mentioned, naive EMTbased control is suboptimal in the DBC sense, and has several additional limitations that do not exist in the general DBC framework (discussed in Section 4.2). 4.1 Multi-Target EMT At times, there may exist several behavioral preferences.",
                "For example, in the case of museum guards, some art items are more heavily guarded, requiring that the guards stay more often in their close vicinity.",
                "On the other hand, no corner of the museum is to be left unchecked, which demands constant motion.",
                "Successful museum security would demand that the guards adhere to, and balance, both of these behaviors.",
                "For EMT-based control, this would mean facing several tactical targets {qk}K k=1, and the question becomes how to merge and balance them.",
                "A balancing mechanism can be applied to resolve this issue.",
                "Note that EMT-based control, while selecting an action, creates a preference vector over the set of actions based on their predicted performance with respect to a given target.",
                "If these preference vectors are normalized, they can be combined into a single unified preference.",
                "This requires replacement of standard EMT-based action selection by the algorithm below [15]: • Given: - a set of target dynamics {qk}K k=1, - vector of weights w(k) • Select action as follows - For each action a ∈ A predict the future state distribution ¯pa t+1 = Ta ∗ pt; - For each action, compute Da = H(¯pa t+1, pt, PDt) - For each a ∈ A and qk tactical target, denote V (a, k) = DKL (Da qk) pt .",
                "Let Vk(a) = 1 Zk V (a, k), where Zk = a∈A V (a, k) is a normalization factor. - Select a∗ = arg min a k k=1 w(k)Vk(a) The weights vector w = (w1, ..., wK ) allows the additional tuning of importance among target dynamics without the need to redesign the targets themselves.",
                "This balancing method is also seamlessly integrated into the EMT-based control flow of operation. 4.2 EMT-based Control Limitations EMT-based control is a sub-optimal (in the DBC sense) representative of the DBC structure.",
                "It limits the User by forcing EMT to be its dynamic tracking algorithm, and replaces Agent optimization by greedy action selection.",
                "This kind of combination, however, is common for on-line algorithms.",
                "Although further development of EMT-based controllers is necessary, evidence so far suggests that even the simplest form of the algorithm possesses a great deal of power, and displays trends that are optimal in the DBC sense of the word.",
                "There are two further, EMT-specific, limitations to EMT-based control that are evident at this point.",
                "Both already have partial solutions and are subjects of ongoing research.",
                "The first limitation is the problem of negative preference.",
                "In the POMDP framework for example, this is captured simply, through The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 793 the appearance of values with different signs within the reward structure.",
                "For EMT-based control, however, negative preference means that one would like to avoid a certain distribution over system development sequences; EMT-based control, however, concentrates on getting as close as possible to a distribution.",
                "Avoidance is thus unnatural in native EMT-based control.",
                "The second limitation comes from the fact that standard environment modeling can create pure sensory actions-actions that do not change the state of the world, and differ only in the way observations are received and the quality of observations received.",
                "Since the world state does not change, EMT-based control would not be able to differentiate between different sensory actions.",
                "Notice that both of these limitations of EMT-based control are absent from the general DBC framework, since it may have a tracking algorithm capable of considering pure sensory actions and, unlike Kullback-Leibler divergence, a distribution deviation measure that is capable of dealing with negative preference. 5.",
                "EMT PLAYING TAG The Game of Tag was first introduced in [11].",
                "It is a single agent problem of capturing a quarry, and belongs to the class of area sweeping problems.",
                "An example domain is shown in Figure 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figure 2: Tag domain; an agent (A) attempts to seek and capture a quarry (Q) The Game of Tag extremely limits the agents perception, so that the agent is able to detect the quarry only if they are co-located in the same cell of the grid world.",
                "In the classical version of the game, co-location leads to a special observation, and the Tag action can be performed.",
                "We slightly modify this setting: the moment that both agents occupy the same cell, the game ends.",
                "As a result, both the agent and its quarry have the same motion capability, which allows them to move in four directions, North, South, East, and West.",
                "These form a formal space of actions within a Markovian environment.",
                "The state space of the formal Markovian environment is described by the cross-product of the agent and quarrys positions.",
                "For Figure 2, it would be S = {s0, ..., s23} × {s0, ..., s23}.",
                "The effects of an action taken by the agent are deterministic, but the environment in general has a stochastic response due to the motion of the quarry.",
                "With probability q0 1 it stays put, and with probability 1 − q0 it moves to an adjacent cell further away from the 1 In our experiments this was taken to be q0 = 0.2. agent.",
                "So for the instance shown in Figure 2 and q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Although the evasive behavior of the quarry is known to the agent, the quarrys position is not.",
                "The only sensory information available to the agent is its own location.",
                "We use EMT and directly specify the target dynamics.",
                "For the Game of Tag, we can easily formulate three major trends: catching the quarry, staying mobile, and stalking the quarry.",
                "This results in the following three target dynamics: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 otherwise Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 otherwise Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Note that none of the above targets are directly achievable; for instance, if Qt = s9 and At = s11, there is no action that can move the agent to At+1 = s9 as required by the Tcatch target dynamics.",
                "We ran several experiments to evaluate EMT performance in the Tag Game.",
                "Three configurations of the domain shown in Figure 3 were used, each posing a different challenge to the agent due to partial observability.",
                "In each setting, a set of 1000 runs was performed with a time limit of 100 steps.",
                "In every run, the initial position of both the agent and its quarry was selected at random; this means that as far as the agent was concerned, the quarrys initial position was uniformly distributed over the entire domain cell space.",
                "We also used two variations of the environment observability function.",
                "In the first version, observability function mapped all joint positions of hunter and quarry into the position of the hunter as an observation.",
                "In the second, only those joint positions in which hunter and quarry occupied different locations were mapped into the hunters location.",
                "The second version in fact utilized and expressed the fact that once hunter and quarry occupy the same cell the game ends.",
                "The results of these experiments are shown in Table 2.",
                "Balancing [15] the catch, move, and stalk target dynamics described in the previous section by the weight vector [0.8, 0.1, 0.1], EMT produced stable performance in all three domains.",
                "Although direct comparisons are difficult to make, the EMT performance displayed notable efficiency vis-`a-vis the POMDP approach.",
                "In spite of a simple and inefficient Matlab implementation of the EMT algorithm, the decision time for any given step averaged significantly below 1 second in all experiments.",
                "For the irregular open arena domain, which proved to be the most difficult, 1000 experiment runs bounded by 100 steps each, a total of 42411 steps, were completed in slightly under 6 hours.",
                "That is, over 4 × 104 online steps took an order of magnitude less time than the offline computation of POMDP policy in [11].",
                "The significance of this differential is made even more prominent by the fact that, should the environment model parameters change, the online nature of EMT would allow it to maintain its performance time, while the POMDP policy would need to be recomputed, requiring yet again a large overhead of computation.",
                "We also tested the behavior cell frequency entropy, empirical measures from trial data.",
                "As Figure 4 and Figure 5 show, empir794 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) A Q Q A 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 A Q Figure 3: These configurations of the Tag Game space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor Table 2: Performance of the EMT-based solution in three Tag Game domains and two observability models: I) omniposition quarry, II) quarry is not at hunters position Model Domain Capture% E(Steps) Time/Step I Dead-ends 100 14.8 72(mSec) Arena 80.2 42.4 500(mSec) Circle 91.4 34.6 187(mSec) II Dead-ends 100 13.2 91(mSec) Arena 96.8 28.67 396(mSec) Circle 94.4 31.63 204(mSec) ical entropy grows with the length of interaction.",
                "For runs where the quarry was not captured immediately, the entropy reaches between 0.85 and 0.952 for different runs and scenarios.",
                "As the agent actively seeks the quarry, the entropy never reaches its maximum.",
                "One characteristic of the entropy graph for the open arena scenario particularly caught our attention in the case of the omniposition quarry observation model.",
                "Near the maximum limit of trial length (100 steps), entropy suddenly dropped.",
                "Further analysis of the data showed that under certain circumstances, a fluctuating behavior occurs in which the agent faces equally viable versions of quarry-following behavior.",
                "Since the EMT algorithm has greedy action selection, and the state space does not encode any form of commitment (not even speed or acceleration), the agent is locked within a small portion of cells.",
                "It is essentially attempting to simultaneously follow several courses of action, all of which are consistent with the target dynamics.",
                "This behavior did not occur in our second observation model, since it significantly reduced the set of eligible courses of action-essentially contributing to tie-breaking among them. 6.",
                "DISCUSSION The design of the EMT solution for the Tag Game exposes the core difference in approach to planning and control between EMT or DBC, on the one hand, and the more familiar POMDP approach, on the other.",
                "POMDP defines a reward structure to optimize, and influences system dynamics indirectly through that optimization.",
                "EMT discards any reward scheme, and instead measures and influences system dynamics directly. 2 Entropy was calculated using log base equal to the number of possible locations within the domain; this properly scales entropy expression into the range [0, 1] for all domains.",
                "Thus for the Tag Game, we did not search for a reward function that would encode and express our preference over the agents behavior, but rather directly set three (heuristic) behavior preferences as the basis for target dynamics to be maintained.",
                "Experimental data shows that these targets need not be directly achievable via the agents actions.",
                "However, the ratio between EMT performance and achievability of target dynamics remains to be explored.",
                "The tag game experiment data also revealed the different emphasis DBC and POMDPs place on the formulation of an environment state space.",
                "POMDPs rely entirely on the mechanism of reward accumulation maximization, i.e., formation of the action selection procedure to achieve necessary state sequencing.",
                "DBC, on the other hand, has two sources of sequencing specification: through the properties of an action selection procedure, and through direct specification within the target dynamics.",
                "The importance of the second source was underlined by the Tag Game experiment data, in which the greedy EMT algorithm, applied to a POMDP-type state space specification, failed, since target description over such a state space was incapable of encoding the necessary behavior tendencies, e.g., tie-breaking and commitment to directed motion.",
                "The structural differences between DBC (and EMT in particular), and POMDPs, prohibits direct performance comparison, and places them on complementary tracks, each within a suitable niche.",
                "For instance, POMDPs could be seen as a much more natural formulation of economic sequential decision-making problems, while EMT is a better fit for continual demand for stochastic change, as happens in many robotic or embodied-agent problems.",
                "The complementary properties of POMDPs and EMT can be further exploited.",
                "There is recent interest in using POMDPs in hybrid solutions [17], in which the POMDPs can be used together with other control approaches to provide results not easily achievable with either approach by itself.",
                "DBC can be an effective partner in such a hybrid solution.",
                "For instance, POMDPs have prohibitively large off-line time requirements for policy computation, but can be readily used in simpler settings to expose beneficial behavioral trends; this can serve as a form of target dynamics that are provided to EMT in a larger domain for on-line operation. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have presented a novel perspective on the process of planning and control in stochastic environments, in the form of the Dynamics Based Control (DBC) framework.",
                "DBC formulates the task of planning as support of a specified target system dynamics, which describes the necessary properties of change within the environment.",
                "Optimality of DBC plans of action are measured The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 4: Observation Model I: Omniposition quarry.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 5: Observation Model II: quarry not observed at hunters position.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. with respect to the deviation of actual system dynamics from the target dynamics.",
                "We show that a recently developed technique of Extended Markov Tracking (EMT) [13] is an instantiation of DBC.",
                "In fact, EMT can be seen as a specific case of DBC parameterization, which employs a greedy action selection procedure.",
                "Since EMT exhibits the key features of the general DBC framework, as well as polynomial time complexity, we used the multitarget version of EMT [15] to demonstrate that the class of area sweeping problems naturally lends itself to dynamics-based descriptions, as instantiated by our experiments in the Tag Game domain.",
                "As enumerated in Section 4.2, EMT has a number of limitations, such as difficulty in dealing with negative dynamic preference.",
                "This prevents direct application of EMT to such problems as Rendezvous-Evasion Games (e.g., [6]).",
                "However, DBC in general has no such limitations, and readily enables the formulation of evasion games.",
                "In future work, we intend to proceed with the development of dynamics-based controllers for these problems. 8.",
                "ACKNOWLEDGMENT The work of the first two authors was partially supported by Israel Science Foundation grant #898/05, and the third author was partially supported by a grant from Israels Ministry of Science and Technology. 9.",
                "REFERENCES [1] R. C. Arkin.",
                "Behavior-Based Robotics.",
                "MIT Press, 1998. [2] J.",
                "A. Bilmes.",
                "A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and Hidden Markov Models.",
                "Technical Report TR-97-021, Department of Electrical Engeineering and Computer Science, University of California at Berkeley, 1998. [3] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of information theory.",
                "Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz, and M. J. Wolverton.",
                "A survey of research in distributed, continual planning.",
                "AI Magazine, 4:13-22, 1999. [5] V. R. Konda and J. N. Tsitsiklis.",
                "Actor-Critic algorithms.",
                "SIAM Journal on Control and Optimization, 42(4):1143-1166, 2003. [6] W. S. Lim.",
                "A rendezvous-evasion game on discrete locations with joint randomization.",
                "Advances in Applied Probability, 29(4):1004-1017, December 1997. [7] M. L. Littman, T. L. Dean, and L. P. Kaelbling.",
                "On the complexity of solving Markov decision problems.",
                "In Proceedings of the 11th Annual Conference on Uncertainty in Artificial Intelligence (UAI-95), pages 394-402, 1995. [8] O. Madani, S. Hanks, and A. Condon.",
                "On the undecidability of probabilistic planning and related stochastic optimization problems.",
                "Artificial Intelligence Journal, 147(1-2):5-34, July 2003. [9] R. M. Neal and G. E. Hinton.",
                "A view of the EM algorithm 796 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) that justifies incremental, sparse, and other variants.",
                "In M. I. Jordan, editor, Learning in Graphical Models, pages 355-368.",
                "Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In Proceeding of AAMAS 2006, 2006. [11] J. Pineau, G. Gordon, and S. Thrun.",
                "Point-based value iteration: An anytime algorithm for pomdps.",
                "In International Joint Conference on Artificial Intelligence (IJCAI), pages 1025-1032, August 2003. [12] M. L. Puterman.",
                "Markov Decision Processes.",
                "Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics Section.",
                "Wiley-Interscience Publication, New York, 1994. [13] Z. Rabinovich and J. S. Rosenschein.",
                "Extended Markov Tracking with an application to control.",
                "In The Workshop on Agent Tracking: Modeling Other Agents from Observations, at the Third International Joint Conference on Autonomous Agents and Multiagent Systems, pages 95-100, New-York, July 2004. [14] Z. Rabinovich and J. S. Rosenschein.",
                "Multiagent coordination by Extended Markov Tracking.",
                "In The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 431-438, Utrecht, The Netherlands, July 2005. [15] Z. Rabinovich and J. S. Rosenschein.",
                "On the response of EMT-based control to interacting targets and models.",
                "In The Fifth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 465-470, Hakodate, Japan, May 2006. [16] R. F. Stengel.",
                "Optimal Control and Estimation.",
                "Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, and P. Varakantham.",
                "Conflicts in teamwork: Hybrids to the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 797"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "target dynamics": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Dynamics Based Control with an Application to Area-Sweeping Problems Zinovi Rabinovich Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel jeff@cs.huji.ac.il Gal A. Kaminka The MAVERICK Group Department of Computer Science Bar Ilan University, Israel galk@cs.biu.ac.il ABSTRACT In this paper we introduce Dynamics Based Control (DBC), an approach to planning and control of an agent in stochastic environments.",
                "Unlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics.",
                "We show that a recently developed planning and control approach, Extended Markov Tracking (EMT) is an instantiation of DBC.",
                "EMT employs greedy action selection to provide an efficient control algorithm in Markovian environments.",
                "We exploit this efficiency in a set of experiments that applied multitarget EMT to a class of area-sweeping problems (searching for moving targets).",
                "We show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation.",
                "Categories and Subject Descriptors I.2.8 [Problem Solving, Control Methods, and Search]: Control Theory; I.2.9 [Robotics]; I.2.11 [Distributed Artificial Intelligence]: Intelligent Agents General Terms Algorithms, Theory 1.",
                "INTRODUCTION Planning and control constitutes a central research area in multiagent systems and artificial intelligence.",
                "In recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in stochastic environments.",
                "In this framework, the planning and control problem is often addressed by imposing a reward function, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility.",
                "While theoretically attractive, the complexity of optimally solving a POMDP is prohibitive [8, 7].",
                "We take an alternative view of planning in stochastic environments.",
                "We do not use a (state-based) reward function, but instead optimize over a different criterion, a transition-based specification of the desired system dynamics.",
                "The idea here is to view planexecution as a process that compels a (stochastic) system to change, and a plan as a dynamic process that shapes that change according to desired criteria.",
                "We call this general planning framework Dynamics Based Control (DBC).",
                "In DBC, the goal of a planning (or control) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) <br>target dynamics</br>.",
                "As actual system behavior may deviate from that which is specified by <br>target dynamics</br> (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16].",
                "Here, optimality is measured in terms of probability of deviation magnitudes.",
                "In this paper, we present the structure of Dynamics Based Control.",
                "We show that the recently developed Extended Markov Tracking (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC.",
                "EMT is an efficient instantiation of DBC.",
                "To evaluate DBC, we carried out a set of experiments applying multi-target EMT to the Tag Game [11]; this is a variant on the area sweeping problem, where an agent is trying to tag a moving target (quarry) whose position is not known with certainty.",
                "Experimental data demonstrates that even with a simple model of the environment and a simple design of <br>target dynamics</br>, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agents position).",
                "The paper is organized as follows.",
                "In Section 2 we motivate DBC using area-sweeping problems, and discuss related work.",
                "Section 3 introduces the Dynamics Based Control (DBC) structure, and its specialization to Markovian environments.",
                "This is followed by a review of the Extended Markov Tracking (EMT) approach as a DBC-structured control regimen in Section 4.",
                "That section also discusses the limitations of EMT-based control relative to the general DBC framework.",
                "Experimental settings and results are then presented in Section 5.",
                "Section 6 provides a short discussion of the overall approach, and Section 7 gives some concluding remarks and directions for future work. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "MOTIVATION AND RELATED WORK Many real-life scenarios naturally have a stochastic <br>target dynamics</br> specification, especially those domains where there exists no ultimate goal, but rather system behavior (with specific properties) that has to be continually supported.",
                "For example, security guards perform persistent sweeps of an area to detect any sign of intrusion.",
                "Cunning thieves will attempt to track these sweeps, and time their operation to key points of the guards motion.",
                "It is thus advisable to make the guards motion dynamics appear irregular and random.",
                "Recent work by Paruchuri et al. [10] has addressed such randomization in the context of single-agent and distributed POMDPs.",
                "The goal in that work was to generate policies that provide a measure of action-selection randomization, while maintaining rewards within some acceptable levels.",
                "Our focus differs from this work in that DBC does not optimize expected rewards-indeed we do not consider rewards at all-but instead maintains desired dynamics (including, but not limited to, randomization).",
                "The Game of Tag is another example of the applicability of the approach.",
                "It was introduced in the work by Pineau et al. [11].",
                "There are two agents that can move about an area, which is divided into a grid.",
                "The grid may have blocked cells (holes) into which no agent can move.",
                "One agent (the hunter) seeks to move into a cell occupied by the other (the quarry), such that they are co-located (this is a successful tag).",
                "The quarry seeks to avoid the hunter agent, and is always aware of the hunters position, but does not know how the hunter will behave, which opens up the possibility for a hunter to surprise the prey.",
                "The hunter knows the quarrys probabilistic law of motion, but does not know its current location.",
                "Tag is an instance of a family of area-sweeping (pursuit-evasion) problems.",
                "In [11], the hunter modeled the problem using a POMDP.",
                "A reward function was defined, to reflect the desire to tag the quarry, and an action policy was computed to optimize the reward collected over time.",
                "Due to the intractable complexity of determining the optimal policy, the action policy computed in that paper was essentially an approximation.",
                "In this paper, instead of formulating a reward function, we use EMT to solve the problem, by directly specifying the <br>target dynamics</br>.",
                "In fact, any search problem with randomized motion, the socalled class of area sweeping problems, can be described through specification of such target system dynamics.",
                "Dynamics Based Control provides a natural approach to solving these problems. 3.",
                "DYNAMICS BASED CONTROL The specification of Dynamics Based Control (DBC) can be broken into three interacting levels: Environment Design Level, User Level, and Agent Level. • Environment Design Level is concerned with the formal specification and modeling of the environment.",
                "For example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation constant. • User Level in turn relies on the environment model produced by Environment Design to specify the target system dynamics it wishes to observe.",
                "The User Level also specifies the estimation or learning procedure for system dynamics, and the measure of deviation.",
                "In the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping. • Agent Level in turn combines the environment model from the Environment Design level, the dynamics estimation procedure, the deviation measure and the <br>target dynamics</br> specification from User Level, to produce a sequence of actions that create system dynamics as close as possible to the targeted specification.",
                "As we are interested in the continual development of a stochastic system, such as happens in classical control theory [16] and continual planning [4], as well as in our example of museum sweeps, the question becomes how the Agent Level is to treat the deviation measurements over time.",
                "To this end, we use a probability threshold-that is, we would like the Agent Level to maximize the probability that the deviation measure will remain below a certain threshold.",
                "Specific action selection then depends on system formalization.",
                "One possibility would be to create a mixture of available system trends, much like that which happens in Behavior-Based Robotic architectures [1].",
                "The other alternative would be to rely on the estimation procedure provided by the User Level-to utilize the Environment Design Level model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a certain dynamics has been achieved.",
                "Notice that this manipulation is not direct, but via the environment.",
                "Thus, for strong enough estimator algorithms, successful manipulation would mean a successful simulation of the specified <br>target dynamics</br> (i.e., beyond discerning via the available sensory input).",
                "DBC levels can also have a back-flow of information (see Figure 1).",
                "For instance, the Agent Level could provide data about <br>target dynamics</br> feasibility, allowing the User Level to modify the requirement, perhaps focusing on attainable features of system behavior.",
                "Data would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the User Level, this can provide an important tool for the environment model calibration at the Environment Design Level.",
                "UserEnv.",
                "Design Agent Model Ideal Dynamics Estimator Estimator Dynamics Feasibility System Response Data Figure 1: Data flow of the DBC framework Extending upon the idea of Actor-Critic algorithms [5], DBC data flow can provide a good basis for the design of a learning algorithm.",
                "For example, the User Level can operate as an exploratory device for a learning algorithm, inferring an ideal dynamics target from the environment model at hand that would expose and verify most critical features of system behavior.",
                "In this case, feasibility and system response data from the Agent Level would provide key information for an environment model update.",
                "In fact, the combination of feasibility and response data can provide a basis for the application of strong learning algorithms such as EM [2, 9]. 3.1 DBC for Markovian Environments For a Partially Observable Markovian Environment, DBC can be specified in a more rigorous manner.",
                "Notice how DBC discards rewards, and replaces it by another optimality criterion (structural differences are summarized in Table 1): • Environment Design level is to specify a tuple < S, A, T, O, Ω, s0 >, where: - S is the set of all possible environment states; - s0 is the initial state of the environment (which can also be viewed as a probability distribution over S); The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 791 - A is the set of all possible actions applicable in the environment; - T is the environments probabilistic transition function: T : S ×A → Π(S).",
                "That is, T(s |a, s) is the probability that the environment will move from state s to state s under action a; - O is the set of all possible observations.",
                "This is what the sensor input would look like for an outside observer; - Ω is the observation probability function: Ω : S × A × S → Π(O).",
                "That is, Ω(o|s , a, s) is the probability that one will observe o given that the environment has moved from state s to state s under action a. • User Level, in the case of Markovian environment, operates on the set of system dynamics described by a family of conditional probabilities F = {τ : S × A → Π(S)}.",
                "Thus specification of <br>target dynamics</br> can be expressed by q ∈ F, and the learning or tracking algorithm can be represented as a function L : O×(A×O)∗ → F; that is, it maps sequences of observations and actions performed so far into an estimate τ ∈ F of system dynamics.",
                "There are many possible variations available at the User Level to define divergence between system dynamics; several of them are: - Trace distance or L1 distance between two distributions p and q defined by D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Fidelity measure of distance F(p(·), q(·)) = x p(x)q(x) - Kullback-Leibler divergence DKL(p(·) q(·)) = x p(x) log p(x) q(x) Notice that the latter two are not actually metrics over the space of possible distributions, but nevertheless have meaningful and important interpretations.",
                "For instance, KullbackLeibler divergence is an important tool of information theory [3] that allows one to measure the price of encoding an information source governed by q, while assuming that it is governed by p. The User Level also defines the threshold of dynamics deviation probability θ. • Agent Level is then faced with a problem of selecting a control signal function a∗ to satisfy a minimization problem as follows: a∗ = arg min a Pr(d(τa, q) > θ) where d(τa, q) is a random variable describing deviation of the dynamics estimate τa, created by L under control signal a, from the ideal dynamics q.",
                "Implicit in this minimization problem is that L is manipulated via the environment, based on the environment model produced by the Environment Design Level. 3.2 DBC View of the State Space It is important to note the complementary view that DBC and POMDPs take on the state space of the environment.",
                "POMDPs regard state as a stationary snap-shot of the environment; whatever attributes of state sequencing one seeks are reached through properties of the control process, in this case reward accumulation.",
                "This can be viewed as if the sequencing of states and the attributes of that sequencing are only introduced by and for the controlling mechanism-the POMDP policy.",
                "DBC concentrates on the underlying principle of state sequencing, the system dynamics.",
                "DBCs <br>target dynamics</br> specification can use the environments state space as a means to describe, discern, and preserve changes that occur within the system.",
                "As a result, DBC has a greater ability to express state sequencing properties, which are grounded in the environment model and its state space definition.",
                "For example, consider the task of moving through rough terrain towards a goal and reaching it as fast as possible.",
                "POMDPs would encode terrain as state space points, while speed would be ensured by negative reward for every step taken without reaching the goalaccumulating higher reward can be reached only by faster motion.",
                "Alternatively, the state space could directly include the notion of speed.",
                "For POMDPs, this would mean that the same concept is encoded twice, in some sense: directly in the state space, and indirectly within reward accumulation.",
                "Now, even if the reward function would encode more, and finer, details of the properties of motion, the POMDP solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure.",
                "On the other hand, the tactical target expression of variations and correlations between position and speed of motion is now grounded in the state space representation.",
                "In this situation, any further constraints, e.g., smoothness of motion, speed limits in different locations, or speed reductions during sharp turns, are explicitly and uniformly expressed by the tactical target, and can result in faster and more effective action selection by a DBC algorithm. 4.",
                "EMT-BASED CONTROL AS A DBC Recently, a control algorithm was introduced called EMT-based Control [13], which instantiates the DBC framework.",
                "Although it provides an approximate greedy solution in the DBC sense, initial experiments using EMT-based control have been encouraging [14, 15].",
                "EMT-based control is based on the Markovian environment definition, as in the case with POMDPs, but its User and Agent Levels are of the Markovian DBC type of optimality. • User Level of EMT-based control defines a limited-case target system dynamics independent of action: qEMT : S → Π(S).",
                "It then utilizes the Kullback-Leibler divergence measure to compose a momentary system dynamics estimator-the Extended Markov Tracking (EMT) algorithm.",
                "The algorithm keeps a system dynamics estimate τt EMT that is capable of explaining recent change in an auxiliary Bayesian system state estimator from pt−1 to pt, and updates it conservatively using Kullback-Leibler divergence.",
                "Since τt EMT and pt−1,t are respectively the conditional and marginal probabilities over the systems state space, explanation simply means that pt(s ) = s τt EMT (s |s)pt−1(s), and the dynamics estimate update is performed by solving a 792 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: Structure of POMDP vs. Dynamics-Based Control in Markovian Environment Level Approach MDP Markovian DBC Environment < S, A, T, O, Ω >,where S - set of states A - set of actions Design T : S × A → Π(S) - transition O - observation set Ω : S × A × S → Π(O) User r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - reward function q - ideal dynamics F - reward remodeling L - dynamics estimator θ - threshold Agent π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) minimization problem: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • Agent Level in EMT-based control is suboptimal with respect to DBC (though it remains within the DBC framework), performing greedy action selection based on prediction of EMTs reaction.",
                "The prediction is based on the environment model provided by the Environment Design level, so that if we denote by Ta the environments transition function limited to action a, and pt−1 is the auxiliary Bayesian system state estimator, then the EMT-based control choice is described by a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1) Note that this follows the Markovian DBC framework precisely: the rewarding optimality of POMDPs is discarded, and in its place a dynamics estimator (EMT in this case) is manipulated via action effects on the environment to produce an estimate close to the specified target system dynamics.",
                "Yet as we mentioned, naive EMTbased control is suboptimal in the DBC sense, and has several additional limitations that do not exist in the general DBC framework (discussed in Section 4.2). 4.1 Multi-Target EMT At times, there may exist several behavioral preferences.",
                "For example, in the case of museum guards, some art items are more heavily guarded, requiring that the guards stay more often in their close vicinity.",
                "On the other hand, no corner of the museum is to be left unchecked, which demands constant motion.",
                "Successful museum security would demand that the guards adhere to, and balance, both of these behaviors.",
                "For EMT-based control, this would mean facing several tactical targets {qk}K k=1, and the question becomes how to merge and balance them.",
                "A balancing mechanism can be applied to resolve this issue.",
                "Note that EMT-based control, while selecting an action, creates a preference vector over the set of actions based on their predicted performance with respect to a given target.",
                "If these preference vectors are normalized, they can be combined into a single unified preference.",
                "This requires replacement of standard EMT-based action selection by the algorithm below [15]: • Given: - a set of <br>target dynamics</br> {qk}K k=1, - vector of weights w(k) • Select action as follows - For each action a ∈ A predict the future state distribution ¯pa t+1 = Ta ∗ pt; - For each action, compute Da = H(¯pa t+1, pt, PDt) - For each a ∈ A and qk tactical target, denote V (a, k) = DKL (Da qk) pt .",
                "Let Vk(a) = 1 Zk V (a, k), where Zk = a∈A V (a, k) is a normalization factor. - Select a∗ = arg min a k k=1 w(k)Vk(a) The weights vector w = (w1, ..., wK ) allows the additional tuning of importance among <br>target dynamics</br> without the need to redesign the targets themselves.",
                "This balancing method is also seamlessly integrated into the EMT-based control flow of operation. 4.2 EMT-based Control Limitations EMT-based control is a sub-optimal (in the DBC sense) representative of the DBC structure.",
                "It limits the User by forcing EMT to be its dynamic tracking algorithm, and replaces Agent optimization by greedy action selection.",
                "This kind of combination, however, is common for on-line algorithms.",
                "Although further development of EMT-based controllers is necessary, evidence so far suggests that even the simplest form of the algorithm possesses a great deal of power, and displays trends that are optimal in the DBC sense of the word.",
                "There are two further, EMT-specific, limitations to EMT-based control that are evident at this point.",
                "Both already have partial solutions and are subjects of ongoing research.",
                "The first limitation is the problem of negative preference.",
                "In the POMDP framework for example, this is captured simply, through The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 793 the appearance of values with different signs within the reward structure.",
                "For EMT-based control, however, negative preference means that one would like to avoid a certain distribution over system development sequences; EMT-based control, however, concentrates on getting as close as possible to a distribution.",
                "Avoidance is thus unnatural in native EMT-based control.",
                "The second limitation comes from the fact that standard environment modeling can create pure sensory actions-actions that do not change the state of the world, and differ only in the way observations are received and the quality of observations received.",
                "Since the world state does not change, EMT-based control would not be able to differentiate between different sensory actions.",
                "Notice that both of these limitations of EMT-based control are absent from the general DBC framework, since it may have a tracking algorithm capable of considering pure sensory actions and, unlike Kullback-Leibler divergence, a distribution deviation measure that is capable of dealing with negative preference. 5.",
                "EMT PLAYING TAG The Game of Tag was first introduced in [11].",
                "It is a single agent problem of capturing a quarry, and belongs to the class of area sweeping problems.",
                "An example domain is shown in Figure 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figure 2: Tag domain; an agent (A) attempts to seek and capture a quarry (Q) The Game of Tag extremely limits the agents perception, so that the agent is able to detect the quarry only if they are co-located in the same cell of the grid world.",
                "In the classical version of the game, co-location leads to a special observation, and the Tag action can be performed.",
                "We slightly modify this setting: the moment that both agents occupy the same cell, the game ends.",
                "As a result, both the agent and its quarry have the same motion capability, which allows them to move in four directions, North, South, East, and West.",
                "These form a formal space of actions within a Markovian environment.",
                "The state space of the formal Markovian environment is described by the cross-product of the agent and quarrys positions.",
                "For Figure 2, it would be S = {s0, ..., s23} × {s0, ..., s23}.",
                "The effects of an action taken by the agent are deterministic, but the environment in general has a stochastic response due to the motion of the quarry.",
                "With probability q0 1 it stays put, and with probability 1 − q0 it moves to an adjacent cell further away from the 1 In our experiments this was taken to be q0 = 0.2. agent.",
                "So for the instance shown in Figure 2 and q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Although the evasive behavior of the quarry is known to the agent, the quarrys position is not.",
                "The only sensory information available to the agent is its own location.",
                "We use EMT and directly specify the <br>target dynamics</br>.",
                "For the Game of Tag, we can easily formulate three major trends: catching the quarry, staying mobile, and stalking the quarry.",
                "This results in the following three <br>target dynamics</br>: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 otherwise Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 otherwise Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Note that none of the above targets are directly achievable; for instance, if Qt = s9 and At = s11, there is no action that can move the agent to At+1 = s9 as required by the Tcatch <br>target dynamics</br>.",
                "We ran several experiments to evaluate EMT performance in the Tag Game.",
                "Three configurations of the domain shown in Figure 3 were used, each posing a different challenge to the agent due to partial observability.",
                "In each setting, a set of 1000 runs was performed with a time limit of 100 steps.",
                "In every run, the initial position of both the agent and its quarry was selected at random; this means that as far as the agent was concerned, the quarrys initial position was uniformly distributed over the entire domain cell space.",
                "We also used two variations of the environment observability function.",
                "In the first version, observability function mapped all joint positions of hunter and quarry into the position of the hunter as an observation.",
                "In the second, only those joint positions in which hunter and quarry occupied different locations were mapped into the hunters location.",
                "The second version in fact utilized and expressed the fact that once hunter and quarry occupy the same cell the game ends.",
                "The results of these experiments are shown in Table 2.",
                "Balancing [15] the catch, move, and stalk <br>target dynamics</br> described in the previous section by the weight vector [0.8, 0.1, 0.1], EMT produced stable performance in all three domains.",
                "Although direct comparisons are difficult to make, the EMT performance displayed notable efficiency vis-`a-vis the POMDP approach.",
                "In spite of a simple and inefficient Matlab implementation of the EMT algorithm, the decision time for any given step averaged significantly below 1 second in all experiments.",
                "For the irregular open arena domain, which proved to be the most difficult, 1000 experiment runs bounded by 100 steps each, a total of 42411 steps, were completed in slightly under 6 hours.",
                "That is, over 4 × 104 online steps took an order of magnitude less time than the offline computation of POMDP policy in [11].",
                "The significance of this differential is made even more prominent by the fact that, should the environment model parameters change, the online nature of EMT would allow it to maintain its performance time, while the POMDP policy would need to be recomputed, requiring yet again a large overhead of computation.",
                "We also tested the behavior cell frequency entropy, empirical measures from trial data.",
                "As Figure 4 and Figure 5 show, empir794 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) A Q Q A 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 A Q Figure 3: These configurations of the Tag Game space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor Table 2: Performance of the EMT-based solution in three Tag Game domains and two observability models: I) omniposition quarry, II) quarry is not at hunters position Model Domain Capture% E(Steps) Time/Step I Dead-ends 100 14.8 72(mSec) Arena 80.2 42.4 500(mSec) Circle 91.4 34.6 187(mSec) II Dead-ends 100 13.2 91(mSec) Arena 96.8 28.67 396(mSec) Circle 94.4 31.63 204(mSec) ical entropy grows with the length of interaction.",
                "For runs where the quarry was not captured immediately, the entropy reaches between 0.85 and 0.952 for different runs and scenarios.",
                "As the agent actively seeks the quarry, the entropy never reaches its maximum.",
                "One characteristic of the entropy graph for the open arena scenario particularly caught our attention in the case of the omniposition quarry observation model.",
                "Near the maximum limit of trial length (100 steps), entropy suddenly dropped.",
                "Further analysis of the data showed that under certain circumstances, a fluctuating behavior occurs in which the agent faces equally viable versions of quarry-following behavior.",
                "Since the EMT algorithm has greedy action selection, and the state space does not encode any form of commitment (not even speed or acceleration), the agent is locked within a small portion of cells.",
                "It is essentially attempting to simultaneously follow several courses of action, all of which are consistent with the <br>target dynamics</br>.",
                "This behavior did not occur in our second observation model, since it significantly reduced the set of eligible courses of action-essentially contributing to tie-breaking among them. 6.",
                "DISCUSSION The design of the EMT solution for the Tag Game exposes the core difference in approach to planning and control between EMT or DBC, on the one hand, and the more familiar POMDP approach, on the other.",
                "POMDP defines a reward structure to optimize, and influences system dynamics indirectly through that optimization.",
                "EMT discards any reward scheme, and instead measures and influences system dynamics directly. 2 Entropy was calculated using log base equal to the number of possible locations within the domain; this properly scales entropy expression into the range [0, 1] for all domains.",
                "Thus for the Tag Game, we did not search for a reward function that would encode and express our preference over the agents behavior, but rather directly set three (heuristic) behavior preferences as the basis for <br>target dynamics</br> to be maintained.",
                "Experimental data shows that these targets need not be directly achievable via the agents actions.",
                "However, the ratio between EMT performance and achievability of <br>target dynamics</br> remains to be explored.",
                "The tag game experiment data also revealed the different emphasis DBC and POMDPs place on the formulation of an environment state space.",
                "POMDPs rely entirely on the mechanism of reward accumulation maximization, i.e., formation of the action selection procedure to achieve necessary state sequencing.",
                "DBC, on the other hand, has two sources of sequencing specification: through the properties of an action selection procedure, and through direct specification within the <br>target dynamics</br>.",
                "The importance of the second source was underlined by the Tag Game experiment data, in which the greedy EMT algorithm, applied to a POMDP-type state space specification, failed, since target description over such a state space was incapable of encoding the necessary behavior tendencies, e.g., tie-breaking and commitment to directed motion.",
                "The structural differences between DBC (and EMT in particular), and POMDPs, prohibits direct performance comparison, and places them on complementary tracks, each within a suitable niche.",
                "For instance, POMDPs could be seen as a much more natural formulation of economic sequential decision-making problems, while EMT is a better fit for continual demand for stochastic change, as happens in many robotic or embodied-agent problems.",
                "The complementary properties of POMDPs and EMT can be further exploited.",
                "There is recent interest in using POMDPs in hybrid solutions [17], in which the POMDPs can be used together with other control approaches to provide results not easily achievable with either approach by itself.",
                "DBC can be an effective partner in such a hybrid solution.",
                "For instance, POMDPs have prohibitively large off-line time requirements for policy computation, but can be readily used in simpler settings to expose beneficial behavioral trends; this can serve as a form of <br>target dynamics</br> that are provided to EMT in a larger domain for on-line operation. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have presented a novel perspective on the process of planning and control in stochastic environments, in the form of the Dynamics Based Control (DBC) framework.",
                "DBC formulates the task of planning as support of a specified target system dynamics, which describes the necessary properties of change within the environment.",
                "Optimality of DBC plans of action are measured The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 4: Observation Model I: Omniposition quarry.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 5: Observation Model II: quarry not observed at hunters position.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. with respect to the deviation of actual system dynamics from the <br>target dynamics</br>.",
                "We show that a recently developed technique of Extended Markov Tracking (EMT) [13] is an instantiation of DBC.",
                "In fact, EMT can be seen as a specific case of DBC parameterization, which employs a greedy action selection procedure.",
                "Since EMT exhibits the key features of the general DBC framework, as well as polynomial time complexity, we used the multitarget version of EMT [15] to demonstrate that the class of area sweeping problems naturally lends itself to dynamics-based descriptions, as instantiated by our experiments in the Tag Game domain.",
                "As enumerated in Section 4.2, EMT has a number of limitations, such as difficulty in dealing with negative dynamic preference.",
                "This prevents direct application of EMT to such problems as Rendezvous-Evasion Games (e.g., [6]).",
                "However, DBC in general has no such limitations, and readily enables the formulation of evasion games.",
                "In future work, we intend to proceed with the development of dynamics-based controllers for these problems. 8.",
                "ACKNOWLEDGMENT The work of the first two authors was partially supported by Israel Science Foundation grant #898/05, and the third author was partially supported by a grant from Israels Ministry of Science and Technology. 9.",
                "REFERENCES [1] R. C. Arkin.",
                "Behavior-Based Robotics.",
                "MIT Press, 1998. [2] J.",
                "A. Bilmes.",
                "A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and Hidden Markov Models.",
                "Technical Report TR-97-021, Department of Electrical Engeineering and Computer Science, University of California at Berkeley, 1998. [3] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of information theory.",
                "Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz, and M. J. Wolverton.",
                "A survey of research in distributed, continual planning.",
                "AI Magazine, 4:13-22, 1999. [5] V. R. Konda and J. N. Tsitsiklis.",
                "Actor-Critic algorithms.",
                "SIAM Journal on Control and Optimization, 42(4):1143-1166, 2003. [6] W. S. Lim.",
                "A rendezvous-evasion game on discrete locations with joint randomization.",
                "Advances in Applied Probability, 29(4):1004-1017, December 1997. [7] M. L. Littman, T. L. Dean, and L. P. Kaelbling.",
                "On the complexity of solving Markov decision problems.",
                "In Proceedings of the 11th Annual Conference on Uncertainty in Artificial Intelligence (UAI-95), pages 394-402, 1995. [8] O. Madani, S. Hanks, and A. Condon.",
                "On the undecidability of probabilistic planning and related stochastic optimization problems.",
                "Artificial Intelligence Journal, 147(1-2):5-34, July 2003. [9] R. M. Neal and G. E. Hinton.",
                "A view of the EM algorithm 796 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) that justifies incremental, sparse, and other variants.",
                "In M. I. Jordan, editor, Learning in Graphical Models, pages 355-368.",
                "Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In Proceeding of AAMAS 2006, 2006. [11] J. Pineau, G. Gordon, and S. Thrun.",
                "Point-based value iteration: An anytime algorithm for pomdps.",
                "In International Joint Conference on Artificial Intelligence (IJCAI), pages 1025-1032, August 2003. [12] M. L. Puterman.",
                "Markov Decision Processes.",
                "Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics Section.",
                "Wiley-Interscience Publication, New York, 1994. [13] Z. Rabinovich and J. S. Rosenschein.",
                "Extended Markov Tracking with an application to control.",
                "In The Workshop on Agent Tracking: Modeling Other Agents from Observations, at the Third International Joint Conference on Autonomous Agents and Multiagent Systems, pages 95-100, New-York, July 2004. [14] Z. Rabinovich and J. S. Rosenschein.",
                "Multiagent coordination by Extended Markov Tracking.",
                "In The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 431-438, Utrecht, The Netherlands, July 2005. [15] Z. Rabinovich and J. S. Rosenschein.",
                "On the response of EMT-based control to interacting targets and models.",
                "In The Fifth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 465-470, Hakodate, Japan, May 2006. [16] R. F. Stengel.",
                "Optimal Control and Estimation.",
                "Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, and P. Varakantham.",
                "Conflicts in teamwork: Hybrids to the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 797"
            ],
            "original_annotated_samples": [
                "In DBC, the goal of a planning (or control) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) <br>target dynamics</br>.",
                "As actual system behavior may deviate from that which is specified by <br>target dynamics</br> (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16].",
                "Experimental data demonstrates that even with a simple model of the environment and a simple design of <br>target dynamics</br>, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agents position).",
                "MOTIVATION AND RELATED WORK Many real-life scenarios naturally have a stochastic <br>target dynamics</br> specification, especially those domains where there exists no ultimate goal, but rather system behavior (with specific properties) that has to be continually supported.",
                "In this paper, instead of formulating a reward function, we use EMT to solve the problem, by directly specifying the <br>target dynamics</br>."
            ],
            "translated_annotated_samples": [
                "En DBC, el objetivo de un proceso de planificación (o control) es garantizar que el sistema cambiará de acuerdo con <br>dinámicas objetivo</br> específicas (potencialmente estocásticas).",
                "Dado que el comportamiento real del sistema puede desviarse de lo especificado por la <br>dinámica objetivo</br> (debido a la naturaleza estocástica del sistema), la planificación en tales entornos debe ser continua, de manera similar a los controladores en lazo cerrado clásicos.",
                "Los datos experimentales demuestran que incluso con un modelo simple del entorno y un diseño simple de la <br>dinámica del objetivo</br>, se pueden lograr altas tasas de éxito tanto en la captura de la presa como en sorprender a la presa (como se expresa por la entropía observada de la posición de los agentes controlados).",
                "MOTIVACIÓN Y TRABAJO RELACIONADO Muchos escenarios de la vida real tienen naturalmente una especificación de <br>dinámica de destino</br> estocástica, especialmente en aquellos dominios donde no existe un objetivo final, sino más bien un comportamiento del sistema (con propiedades específicas) que debe ser continuamente respaldado.",
                "En este artículo, en lugar de formular una función de recompensa, utilizamos EMT para resolver el problema, especificando directamente la <br>dinámica objetivo</br>."
            ],
            "translated_text": "En este artículo presentamos el Control Basado en Dinámicas (DBC), un enfoque para la planificación y control de un agente en entornos estocásticos. A diferencia de los enfoques existentes, que buscan optimizar las recompensas esperadas (por ejemplo, en Problemas de Decisión de Markov Parcialmente Observables (POMDP)), DBC optimiza el comportamiento del sistema hacia las dinámicas del sistema especificadas. Mostramos que un enfoque de planificación y control recientemente desarrollado, Seguimiento Extendido de Markov (EMT), es una instancia de DBC. EMT emplea la selección de acciones codiciosa para proporcionar un algoritmo de control eficiente en entornos markovianos. Explotamos esta eficiencia en un conjunto de experimentos que aplicaron EMT multitarget a una clase de problemas de barrido de área (búsqueda de objetivos en movimiento). Mostramos que tales problemas pueden ser definidos de forma natural y resueltos eficientemente utilizando el marco de DBC y su instanciación de EMT. Categorías y Descriptores de Asignaturas I.2.8 [Resolución de Problemas, Métodos de Control y Búsqueda]: Teoría de Control; I.2.9 [Robótica]; I.2.11 [Inteligencia Artificial Distribuida]: Agentes Inteligentes Términos Generales Algoritmos, Teoría 1. INTRODUCCIÓN La planificación y el control constituyen un área de investigación central en sistemas multiagentes e inteligencia artificial. En los últimos años, los Procesos de Decisión de Markov Parcialmente Observables (POMDPs) [12] se han convertido en una base formal popular para la planificación en entornos estocásticos. En este marco, el problema de planificación y control suele abordarse imponiendo una función de recompensa y calculando una política (de elección de acciones) que sea óptima, en el sentido de que resultará en la mayor utilidad esperada. Si bien teóricamente atractivo, la complejidad de resolver óptimamente un POMDP es prohibitiva [8, 7]. Tomamos una visión alternativa de la planificación en entornos estocásticos. No utilizamos una función de recompensa basada en el estado, sino que optimizamos según un criterio diferente, una especificación basada en transiciones de la dinámica del sistema deseado. La idea aquí es ver la ejecución del plan como un proceso que obliga a un sistema (estocástico) a cambiar, y un plan como un proceso dinámico que moldea ese cambio de acuerdo con criterios deseados. Llamamos a este marco de planificación general Control Basado en Dinámicas (DBC). En DBC, el objetivo de un proceso de planificación (o control) es garantizar que el sistema cambiará de acuerdo con <br>dinámicas objetivo</br> específicas (potencialmente estocásticas). Dado que el comportamiento real del sistema puede desviarse de lo especificado por la <br>dinámica objetivo</br> (debido a la naturaleza estocástica del sistema), la planificación en tales entornos debe ser continua, de manera similar a los controladores en lazo cerrado clásicos. Aquí, la optimalidad se mide en términos de la probabilidad de magnitudes de desviación. En este documento, presentamos la estructura del Control Basado en Dinámicas. Mostramos que el enfoque de Seguimiento Extendido de Markov (EMT) recientemente desarrollado [13, 14, 15] está subsumido por DBC, con EMT empleando selección de acciones codiciosa, que es una parametrización específica entre las opciones posibles dentro de DBC. EMT es una implementación eficiente de DBC. Para evaluar DBC, llevamos a cabo un conjunto de experimentos aplicando EMT de múltiples objetivos al Juego de Etiquetas [11]; esta es una variante del problema de barrido de área, donde un agente intenta etiquetar a un objetivo móvil (presa) cuya posición no se conoce con certeza. Los datos experimentales demuestran que incluso con un modelo simple del entorno y un diseño simple de la <br>dinámica del objetivo</br>, se pueden lograr altas tasas de éxito tanto en la captura de la presa como en sorprender a la presa (como se expresa por la entropía observada de la posición de los agentes controlados). El documento está organizado de la siguiente manera. En la Sección 2 motivamos DBC utilizando problemas de barrido de área, y discutimos trabajos relacionados. La Sección 3 introduce la estructura de Control Basado en Dinámicas (DBC) y su especialización en entornos Markovianos. Esto es seguido por una revisión del enfoque de Seguimiento Markov Extendido (EMT) como un régimen de control estructurado por DBC en la Sección 4. Esa sección también discute las limitaciones del control basado en EMT en relación con el marco general de DBC. Los ajustes experimentales y los resultados se presentan a continuación en la Sección 5. La Sección 6 proporciona una breve discusión del enfoque general, y la Sección 7 ofrece algunas observaciones finales y direcciones para trabajos futuros. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2. MOTIVACIÓN Y TRABAJO RELACIONADO Muchos escenarios de la vida real tienen naturalmente una especificación de <br>dinámica de destino</br> estocástica, especialmente en aquellos dominios donde no existe un objetivo final, sino más bien un comportamiento del sistema (con propiedades específicas) que debe ser continuamente respaldado. Por ejemplo, los guardias de seguridad realizan barridos persistentes de un área para detectar cualquier signo de intrusión. Los ladrones astutos intentarán rastrear estos barridos y sincronizar su operación con puntos clave del movimiento de los guardias. Por lo tanto, es recomendable que la dinámica de movimiento de los guardias parezca irregular y aleatoria. El trabajo reciente de Paruchuri et al. [10] ha abordado dicha aleatorización en el contexto de POMDPs de agente único y distribuidos. El objetivo de ese trabajo era generar políticas que proporcionen una medida de aleatorización en la selección de acciones, manteniendo al mismo tiempo las recompensas dentro de niveles aceptables. Nuestro enfoque difiere de este trabajo en que DBC no optimiza recompensas esperadas, de hecho, no consideramos recompensas en absoluto, sino que en su lugar mantenemos dinámicas deseadas (incluyendo, pero no limitado a, la aleatorización). El juego de la mancha es otro ejemplo de la aplicabilidad del enfoque. Fue introducido en el trabajo de Pineau et al. [11]. Hay dos agentes que pueden moverse por un área, la cual está dividida en una cuadrícula. La cuadrícula puede tener celdas bloqueadas (agujeros) en las cuales ningún agente puede moverse. Un agente (el cazador) intenta moverse a una celda ocupada por el otro (la presa), de modo que estén ubicados en el mismo lugar (esto es una etiqueta exitosa). La presa busca evitar al cazador, siempre está al tanto de la posición del cazador, pero no sabe cómo se comportará el cazador, lo que abre la posibilidad de que un cazador sorprenda a la presa. El cazador conoce la ley de movimiento probabilístico de la presa, pero no conoce su ubicación actual. Tag es una instancia de una familia de problemas de barrido de área (persecución-evasión). En [11], el cazador modeló el problema utilizando un POMDP. Se definió una función de recompensa para reflejar el deseo de marcar la presa, y se calculó una política de acción para optimizar la recompensa recolectada con el tiempo. Debido a la complejidad intratable de determinar la política óptima, la política de acción calculada en ese documento fue esencialmente una aproximación. En este artículo, en lugar de formular una función de recompensa, utilizamos EMT para resolver el problema, especificando directamente la <br>dinámica objetivo</br>. ",
            "candidates": [],
            "error": [
                [
                    "dinámicas objetivo",
                    "dinámica objetivo",
                    "dinámica del objetivo",
                    "dinámica de destino",
                    "dinámica objetivo"
                ]
            ]
        },
        "action-selection randomization": {
            "translated_key": "aleatorización en la selección de acciones",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Dynamics Based Control with an Application to Area-Sweeping Problems Zinovi Rabinovich Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel jeff@cs.huji.ac.il Gal A. Kaminka The MAVERICK Group Department of Computer Science Bar Ilan University, Israel galk@cs.biu.ac.il ABSTRACT In this paper we introduce Dynamics Based Control (DBC), an approach to planning and control of an agent in stochastic environments.",
                "Unlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics.",
                "We show that a recently developed planning and control approach, Extended Markov Tracking (EMT) is an instantiation of DBC.",
                "EMT employs greedy action selection to provide an efficient control algorithm in Markovian environments.",
                "We exploit this efficiency in a set of experiments that applied multitarget EMT to a class of area-sweeping problems (searching for moving targets).",
                "We show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation.",
                "Categories and Subject Descriptors I.2.8 [Problem Solving, Control Methods, and Search]: Control Theory; I.2.9 [Robotics]; I.2.11 [Distributed Artificial Intelligence]: Intelligent Agents General Terms Algorithms, Theory 1.",
                "INTRODUCTION Planning and control constitutes a central research area in multiagent systems and artificial intelligence.",
                "In recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in stochastic environments.",
                "In this framework, the planning and control problem is often addressed by imposing a reward function, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility.",
                "While theoretically attractive, the complexity of optimally solving a POMDP is prohibitive [8, 7].",
                "We take an alternative view of planning in stochastic environments.",
                "We do not use a (state-based) reward function, but instead optimize over a different criterion, a transition-based specification of the desired system dynamics.",
                "The idea here is to view planexecution as a process that compels a (stochastic) system to change, and a plan as a dynamic process that shapes that change according to desired criteria.",
                "We call this general planning framework Dynamics Based Control (DBC).",
                "In DBC, the goal of a planning (or control) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) target dynamics.",
                "As actual system behavior may deviate from that which is specified by target dynamics (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16].",
                "Here, optimality is measured in terms of probability of deviation magnitudes.",
                "In this paper, we present the structure of Dynamics Based Control.",
                "We show that the recently developed Extended Markov Tracking (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC.",
                "EMT is an efficient instantiation of DBC.",
                "To evaluate DBC, we carried out a set of experiments applying multi-target EMT to the Tag Game [11]; this is a variant on the area sweeping problem, where an agent is trying to tag a moving target (quarry) whose position is not known with certainty.",
                "Experimental data demonstrates that even with a simple model of the environment and a simple design of target dynamics, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agents position).",
                "The paper is organized as follows.",
                "In Section 2 we motivate DBC using area-sweeping problems, and discuss related work.",
                "Section 3 introduces the Dynamics Based Control (DBC) structure, and its specialization to Markovian environments.",
                "This is followed by a review of the Extended Markov Tracking (EMT) approach as a DBC-structured control regimen in Section 4.",
                "That section also discusses the limitations of EMT-based control relative to the general DBC framework.",
                "Experimental settings and results are then presented in Section 5.",
                "Section 6 provides a short discussion of the overall approach, and Section 7 gives some concluding remarks and directions for future work. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "MOTIVATION AND RELATED WORK Many real-life scenarios naturally have a stochastic target dynamics specification, especially those domains where there exists no ultimate goal, but rather system behavior (with specific properties) that has to be continually supported.",
                "For example, security guards perform persistent sweeps of an area to detect any sign of intrusion.",
                "Cunning thieves will attempt to track these sweeps, and time their operation to key points of the guards motion.",
                "It is thus advisable to make the guards motion dynamics appear irregular and random.",
                "Recent work by Paruchuri et al. [10] has addressed such randomization in the context of single-agent and distributed POMDPs.",
                "The goal in that work was to generate policies that provide a measure of <br>action-selection randomization</br>, while maintaining rewards within some acceptable levels.",
                "Our focus differs from this work in that DBC does not optimize expected rewards-indeed we do not consider rewards at all-but instead maintains desired dynamics (including, but not limited to, randomization).",
                "The Game of Tag is another example of the applicability of the approach.",
                "It was introduced in the work by Pineau et al. [11].",
                "There are two agents that can move about an area, which is divided into a grid.",
                "The grid may have blocked cells (holes) into which no agent can move.",
                "One agent (the hunter) seeks to move into a cell occupied by the other (the quarry), such that they are co-located (this is a successful tag).",
                "The quarry seeks to avoid the hunter agent, and is always aware of the hunters position, but does not know how the hunter will behave, which opens up the possibility for a hunter to surprise the prey.",
                "The hunter knows the quarrys probabilistic law of motion, but does not know its current location.",
                "Tag is an instance of a family of area-sweeping (pursuit-evasion) problems.",
                "In [11], the hunter modeled the problem using a POMDP.",
                "A reward function was defined, to reflect the desire to tag the quarry, and an action policy was computed to optimize the reward collected over time.",
                "Due to the intractable complexity of determining the optimal policy, the action policy computed in that paper was essentially an approximation.",
                "In this paper, instead of formulating a reward function, we use EMT to solve the problem, by directly specifying the target dynamics.",
                "In fact, any search problem with randomized motion, the socalled class of area sweeping problems, can be described through specification of such target system dynamics.",
                "Dynamics Based Control provides a natural approach to solving these problems. 3.",
                "DYNAMICS BASED CONTROL The specification of Dynamics Based Control (DBC) can be broken into three interacting levels: Environment Design Level, User Level, and Agent Level. • Environment Design Level is concerned with the formal specification and modeling of the environment.",
                "For example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation constant. • User Level in turn relies on the environment model produced by Environment Design to specify the target system dynamics it wishes to observe.",
                "The User Level also specifies the estimation or learning procedure for system dynamics, and the measure of deviation.",
                "In the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping. • Agent Level in turn combines the environment model from the Environment Design level, the dynamics estimation procedure, the deviation measure and the target dynamics specification from User Level, to produce a sequence of actions that create system dynamics as close as possible to the targeted specification.",
                "As we are interested in the continual development of a stochastic system, such as happens in classical control theory [16] and continual planning [4], as well as in our example of museum sweeps, the question becomes how the Agent Level is to treat the deviation measurements over time.",
                "To this end, we use a probability threshold-that is, we would like the Agent Level to maximize the probability that the deviation measure will remain below a certain threshold.",
                "Specific action selection then depends on system formalization.",
                "One possibility would be to create a mixture of available system trends, much like that which happens in Behavior-Based Robotic architectures [1].",
                "The other alternative would be to rely on the estimation procedure provided by the User Level-to utilize the Environment Design Level model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a certain dynamics has been achieved.",
                "Notice that this manipulation is not direct, but via the environment.",
                "Thus, for strong enough estimator algorithms, successful manipulation would mean a successful simulation of the specified target dynamics (i.e., beyond discerning via the available sensory input).",
                "DBC levels can also have a back-flow of information (see Figure 1).",
                "For instance, the Agent Level could provide data about target dynamics feasibility, allowing the User Level to modify the requirement, perhaps focusing on attainable features of system behavior.",
                "Data would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the User Level, this can provide an important tool for the environment model calibration at the Environment Design Level.",
                "UserEnv.",
                "Design Agent Model Ideal Dynamics Estimator Estimator Dynamics Feasibility System Response Data Figure 1: Data flow of the DBC framework Extending upon the idea of Actor-Critic algorithms [5], DBC data flow can provide a good basis for the design of a learning algorithm.",
                "For example, the User Level can operate as an exploratory device for a learning algorithm, inferring an ideal dynamics target from the environment model at hand that would expose and verify most critical features of system behavior.",
                "In this case, feasibility and system response data from the Agent Level would provide key information for an environment model update.",
                "In fact, the combination of feasibility and response data can provide a basis for the application of strong learning algorithms such as EM [2, 9]. 3.1 DBC for Markovian Environments For a Partially Observable Markovian Environment, DBC can be specified in a more rigorous manner.",
                "Notice how DBC discards rewards, and replaces it by another optimality criterion (structural differences are summarized in Table 1): • Environment Design level is to specify a tuple < S, A, T, O, Ω, s0 >, where: - S is the set of all possible environment states; - s0 is the initial state of the environment (which can also be viewed as a probability distribution over S); The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 791 - A is the set of all possible actions applicable in the environment; - T is the environments probabilistic transition function: T : S ×A → Π(S).",
                "That is, T(s |a, s) is the probability that the environment will move from state s to state s under action a; - O is the set of all possible observations.",
                "This is what the sensor input would look like for an outside observer; - Ω is the observation probability function: Ω : S × A × S → Π(O).",
                "That is, Ω(o|s , a, s) is the probability that one will observe o given that the environment has moved from state s to state s under action a. • User Level, in the case of Markovian environment, operates on the set of system dynamics described by a family of conditional probabilities F = {τ : S × A → Π(S)}.",
                "Thus specification of target dynamics can be expressed by q ∈ F, and the learning or tracking algorithm can be represented as a function L : O×(A×O)∗ → F; that is, it maps sequences of observations and actions performed so far into an estimate τ ∈ F of system dynamics.",
                "There are many possible variations available at the User Level to define divergence between system dynamics; several of them are: - Trace distance or L1 distance between two distributions p and q defined by D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Fidelity measure of distance F(p(·), q(·)) = x p(x)q(x) - Kullback-Leibler divergence DKL(p(·) q(·)) = x p(x) log p(x) q(x) Notice that the latter two are not actually metrics over the space of possible distributions, but nevertheless have meaningful and important interpretations.",
                "For instance, KullbackLeibler divergence is an important tool of information theory [3] that allows one to measure the price of encoding an information source governed by q, while assuming that it is governed by p. The User Level also defines the threshold of dynamics deviation probability θ. • Agent Level is then faced with a problem of selecting a control signal function a∗ to satisfy a minimization problem as follows: a∗ = arg min a Pr(d(τa, q) > θ) where d(τa, q) is a random variable describing deviation of the dynamics estimate τa, created by L under control signal a, from the ideal dynamics q.",
                "Implicit in this minimization problem is that L is manipulated via the environment, based on the environment model produced by the Environment Design Level. 3.2 DBC View of the State Space It is important to note the complementary view that DBC and POMDPs take on the state space of the environment.",
                "POMDPs regard state as a stationary snap-shot of the environment; whatever attributes of state sequencing one seeks are reached through properties of the control process, in this case reward accumulation.",
                "This can be viewed as if the sequencing of states and the attributes of that sequencing are only introduced by and for the controlling mechanism-the POMDP policy.",
                "DBC concentrates on the underlying principle of state sequencing, the system dynamics.",
                "DBCs target dynamics specification can use the environments state space as a means to describe, discern, and preserve changes that occur within the system.",
                "As a result, DBC has a greater ability to express state sequencing properties, which are grounded in the environment model and its state space definition.",
                "For example, consider the task of moving through rough terrain towards a goal and reaching it as fast as possible.",
                "POMDPs would encode terrain as state space points, while speed would be ensured by negative reward for every step taken without reaching the goalaccumulating higher reward can be reached only by faster motion.",
                "Alternatively, the state space could directly include the notion of speed.",
                "For POMDPs, this would mean that the same concept is encoded twice, in some sense: directly in the state space, and indirectly within reward accumulation.",
                "Now, even if the reward function would encode more, and finer, details of the properties of motion, the POMDP solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure.",
                "On the other hand, the tactical target expression of variations and correlations between position and speed of motion is now grounded in the state space representation.",
                "In this situation, any further constraints, e.g., smoothness of motion, speed limits in different locations, or speed reductions during sharp turns, are explicitly and uniformly expressed by the tactical target, and can result in faster and more effective action selection by a DBC algorithm. 4.",
                "EMT-BASED CONTROL AS A DBC Recently, a control algorithm was introduced called EMT-based Control [13], which instantiates the DBC framework.",
                "Although it provides an approximate greedy solution in the DBC sense, initial experiments using EMT-based control have been encouraging [14, 15].",
                "EMT-based control is based on the Markovian environment definition, as in the case with POMDPs, but its User and Agent Levels are of the Markovian DBC type of optimality. • User Level of EMT-based control defines a limited-case target system dynamics independent of action: qEMT : S → Π(S).",
                "It then utilizes the Kullback-Leibler divergence measure to compose a momentary system dynamics estimator-the Extended Markov Tracking (EMT) algorithm.",
                "The algorithm keeps a system dynamics estimate τt EMT that is capable of explaining recent change in an auxiliary Bayesian system state estimator from pt−1 to pt, and updates it conservatively using Kullback-Leibler divergence.",
                "Since τt EMT and pt−1,t are respectively the conditional and marginal probabilities over the systems state space, explanation simply means that pt(s ) = s τt EMT (s |s)pt−1(s), and the dynamics estimate update is performed by solving a 792 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: Structure of POMDP vs. Dynamics-Based Control in Markovian Environment Level Approach MDP Markovian DBC Environment < S, A, T, O, Ω >,where S - set of states A - set of actions Design T : S × A → Π(S) - transition O - observation set Ω : S × A × S → Π(O) User r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - reward function q - ideal dynamics F - reward remodeling L - dynamics estimator θ - threshold Agent π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) minimization problem: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • Agent Level in EMT-based control is suboptimal with respect to DBC (though it remains within the DBC framework), performing greedy action selection based on prediction of EMTs reaction.",
                "The prediction is based on the environment model provided by the Environment Design level, so that if we denote by Ta the environments transition function limited to action a, and pt−1 is the auxiliary Bayesian system state estimator, then the EMT-based control choice is described by a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1) Note that this follows the Markovian DBC framework precisely: the rewarding optimality of POMDPs is discarded, and in its place a dynamics estimator (EMT in this case) is manipulated via action effects on the environment to produce an estimate close to the specified target system dynamics.",
                "Yet as we mentioned, naive EMTbased control is suboptimal in the DBC sense, and has several additional limitations that do not exist in the general DBC framework (discussed in Section 4.2). 4.1 Multi-Target EMT At times, there may exist several behavioral preferences.",
                "For example, in the case of museum guards, some art items are more heavily guarded, requiring that the guards stay more often in their close vicinity.",
                "On the other hand, no corner of the museum is to be left unchecked, which demands constant motion.",
                "Successful museum security would demand that the guards adhere to, and balance, both of these behaviors.",
                "For EMT-based control, this would mean facing several tactical targets {qk}K k=1, and the question becomes how to merge and balance them.",
                "A balancing mechanism can be applied to resolve this issue.",
                "Note that EMT-based control, while selecting an action, creates a preference vector over the set of actions based on their predicted performance with respect to a given target.",
                "If these preference vectors are normalized, they can be combined into a single unified preference.",
                "This requires replacement of standard EMT-based action selection by the algorithm below [15]: • Given: - a set of target dynamics {qk}K k=1, - vector of weights w(k) • Select action as follows - For each action a ∈ A predict the future state distribution ¯pa t+1 = Ta ∗ pt; - For each action, compute Da = H(¯pa t+1, pt, PDt) - For each a ∈ A and qk tactical target, denote V (a, k) = DKL (Da qk) pt .",
                "Let Vk(a) = 1 Zk V (a, k), where Zk = a∈A V (a, k) is a normalization factor. - Select a∗ = arg min a k k=1 w(k)Vk(a) The weights vector w = (w1, ..., wK ) allows the additional tuning of importance among target dynamics without the need to redesign the targets themselves.",
                "This balancing method is also seamlessly integrated into the EMT-based control flow of operation. 4.2 EMT-based Control Limitations EMT-based control is a sub-optimal (in the DBC sense) representative of the DBC structure.",
                "It limits the User by forcing EMT to be its dynamic tracking algorithm, and replaces Agent optimization by greedy action selection.",
                "This kind of combination, however, is common for on-line algorithms.",
                "Although further development of EMT-based controllers is necessary, evidence so far suggests that even the simplest form of the algorithm possesses a great deal of power, and displays trends that are optimal in the DBC sense of the word.",
                "There are two further, EMT-specific, limitations to EMT-based control that are evident at this point.",
                "Both already have partial solutions and are subjects of ongoing research.",
                "The first limitation is the problem of negative preference.",
                "In the POMDP framework for example, this is captured simply, through The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 793 the appearance of values with different signs within the reward structure.",
                "For EMT-based control, however, negative preference means that one would like to avoid a certain distribution over system development sequences; EMT-based control, however, concentrates on getting as close as possible to a distribution.",
                "Avoidance is thus unnatural in native EMT-based control.",
                "The second limitation comes from the fact that standard environment modeling can create pure sensory actions-actions that do not change the state of the world, and differ only in the way observations are received and the quality of observations received.",
                "Since the world state does not change, EMT-based control would not be able to differentiate between different sensory actions.",
                "Notice that both of these limitations of EMT-based control are absent from the general DBC framework, since it may have a tracking algorithm capable of considering pure sensory actions and, unlike Kullback-Leibler divergence, a distribution deviation measure that is capable of dealing with negative preference. 5.",
                "EMT PLAYING TAG The Game of Tag was first introduced in [11].",
                "It is a single agent problem of capturing a quarry, and belongs to the class of area sweeping problems.",
                "An example domain is shown in Figure 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figure 2: Tag domain; an agent (A) attempts to seek and capture a quarry (Q) The Game of Tag extremely limits the agents perception, so that the agent is able to detect the quarry only if they are co-located in the same cell of the grid world.",
                "In the classical version of the game, co-location leads to a special observation, and the Tag action can be performed.",
                "We slightly modify this setting: the moment that both agents occupy the same cell, the game ends.",
                "As a result, both the agent and its quarry have the same motion capability, which allows them to move in four directions, North, South, East, and West.",
                "These form a formal space of actions within a Markovian environment.",
                "The state space of the formal Markovian environment is described by the cross-product of the agent and quarrys positions.",
                "For Figure 2, it would be S = {s0, ..., s23} × {s0, ..., s23}.",
                "The effects of an action taken by the agent are deterministic, but the environment in general has a stochastic response due to the motion of the quarry.",
                "With probability q0 1 it stays put, and with probability 1 − q0 it moves to an adjacent cell further away from the 1 In our experiments this was taken to be q0 = 0.2. agent.",
                "So for the instance shown in Figure 2 and q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Although the evasive behavior of the quarry is known to the agent, the quarrys position is not.",
                "The only sensory information available to the agent is its own location.",
                "We use EMT and directly specify the target dynamics.",
                "For the Game of Tag, we can easily formulate three major trends: catching the quarry, staying mobile, and stalking the quarry.",
                "This results in the following three target dynamics: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 otherwise Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 otherwise Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Note that none of the above targets are directly achievable; for instance, if Qt = s9 and At = s11, there is no action that can move the agent to At+1 = s9 as required by the Tcatch target dynamics.",
                "We ran several experiments to evaluate EMT performance in the Tag Game.",
                "Three configurations of the domain shown in Figure 3 were used, each posing a different challenge to the agent due to partial observability.",
                "In each setting, a set of 1000 runs was performed with a time limit of 100 steps.",
                "In every run, the initial position of both the agent and its quarry was selected at random; this means that as far as the agent was concerned, the quarrys initial position was uniformly distributed over the entire domain cell space.",
                "We also used two variations of the environment observability function.",
                "In the first version, observability function mapped all joint positions of hunter and quarry into the position of the hunter as an observation.",
                "In the second, only those joint positions in which hunter and quarry occupied different locations were mapped into the hunters location.",
                "The second version in fact utilized and expressed the fact that once hunter and quarry occupy the same cell the game ends.",
                "The results of these experiments are shown in Table 2.",
                "Balancing [15] the catch, move, and stalk target dynamics described in the previous section by the weight vector [0.8, 0.1, 0.1], EMT produced stable performance in all three domains.",
                "Although direct comparisons are difficult to make, the EMT performance displayed notable efficiency vis-`a-vis the POMDP approach.",
                "In spite of a simple and inefficient Matlab implementation of the EMT algorithm, the decision time for any given step averaged significantly below 1 second in all experiments.",
                "For the irregular open arena domain, which proved to be the most difficult, 1000 experiment runs bounded by 100 steps each, a total of 42411 steps, were completed in slightly under 6 hours.",
                "That is, over 4 × 104 online steps took an order of magnitude less time than the offline computation of POMDP policy in [11].",
                "The significance of this differential is made even more prominent by the fact that, should the environment model parameters change, the online nature of EMT would allow it to maintain its performance time, while the POMDP policy would need to be recomputed, requiring yet again a large overhead of computation.",
                "We also tested the behavior cell frequency entropy, empirical measures from trial data.",
                "As Figure 4 and Figure 5 show, empir794 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) A Q Q A 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 A Q Figure 3: These configurations of the Tag Game space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor Table 2: Performance of the EMT-based solution in three Tag Game domains and two observability models: I) omniposition quarry, II) quarry is not at hunters position Model Domain Capture% E(Steps) Time/Step I Dead-ends 100 14.8 72(mSec) Arena 80.2 42.4 500(mSec) Circle 91.4 34.6 187(mSec) II Dead-ends 100 13.2 91(mSec) Arena 96.8 28.67 396(mSec) Circle 94.4 31.63 204(mSec) ical entropy grows with the length of interaction.",
                "For runs where the quarry was not captured immediately, the entropy reaches between 0.85 and 0.952 for different runs and scenarios.",
                "As the agent actively seeks the quarry, the entropy never reaches its maximum.",
                "One characteristic of the entropy graph for the open arena scenario particularly caught our attention in the case of the omniposition quarry observation model.",
                "Near the maximum limit of trial length (100 steps), entropy suddenly dropped.",
                "Further analysis of the data showed that under certain circumstances, a fluctuating behavior occurs in which the agent faces equally viable versions of quarry-following behavior.",
                "Since the EMT algorithm has greedy action selection, and the state space does not encode any form of commitment (not even speed or acceleration), the agent is locked within a small portion of cells.",
                "It is essentially attempting to simultaneously follow several courses of action, all of which are consistent with the target dynamics.",
                "This behavior did not occur in our second observation model, since it significantly reduced the set of eligible courses of action-essentially contributing to tie-breaking among them. 6.",
                "DISCUSSION The design of the EMT solution for the Tag Game exposes the core difference in approach to planning and control between EMT or DBC, on the one hand, and the more familiar POMDP approach, on the other.",
                "POMDP defines a reward structure to optimize, and influences system dynamics indirectly through that optimization.",
                "EMT discards any reward scheme, and instead measures and influences system dynamics directly. 2 Entropy was calculated using log base equal to the number of possible locations within the domain; this properly scales entropy expression into the range [0, 1] for all domains.",
                "Thus for the Tag Game, we did not search for a reward function that would encode and express our preference over the agents behavior, but rather directly set three (heuristic) behavior preferences as the basis for target dynamics to be maintained.",
                "Experimental data shows that these targets need not be directly achievable via the agents actions.",
                "However, the ratio between EMT performance and achievability of target dynamics remains to be explored.",
                "The tag game experiment data also revealed the different emphasis DBC and POMDPs place on the formulation of an environment state space.",
                "POMDPs rely entirely on the mechanism of reward accumulation maximization, i.e., formation of the action selection procedure to achieve necessary state sequencing.",
                "DBC, on the other hand, has two sources of sequencing specification: through the properties of an action selection procedure, and through direct specification within the target dynamics.",
                "The importance of the second source was underlined by the Tag Game experiment data, in which the greedy EMT algorithm, applied to a POMDP-type state space specification, failed, since target description over such a state space was incapable of encoding the necessary behavior tendencies, e.g., tie-breaking and commitment to directed motion.",
                "The structural differences between DBC (and EMT in particular), and POMDPs, prohibits direct performance comparison, and places them on complementary tracks, each within a suitable niche.",
                "For instance, POMDPs could be seen as a much more natural formulation of economic sequential decision-making problems, while EMT is a better fit for continual demand for stochastic change, as happens in many robotic or embodied-agent problems.",
                "The complementary properties of POMDPs and EMT can be further exploited.",
                "There is recent interest in using POMDPs in hybrid solutions [17], in which the POMDPs can be used together with other control approaches to provide results not easily achievable with either approach by itself.",
                "DBC can be an effective partner in such a hybrid solution.",
                "For instance, POMDPs have prohibitively large off-line time requirements for policy computation, but can be readily used in simpler settings to expose beneficial behavioral trends; this can serve as a form of target dynamics that are provided to EMT in a larger domain for on-line operation. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have presented a novel perspective on the process of planning and control in stochastic environments, in the form of the Dynamics Based Control (DBC) framework.",
                "DBC formulates the task of planning as support of a specified target system dynamics, which describes the necessary properties of change within the environment.",
                "Optimality of DBC plans of action are measured The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 4: Observation Model I: Omniposition quarry.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 5: Observation Model II: quarry not observed at hunters position.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. with respect to the deviation of actual system dynamics from the target dynamics.",
                "We show that a recently developed technique of Extended Markov Tracking (EMT) [13] is an instantiation of DBC.",
                "In fact, EMT can be seen as a specific case of DBC parameterization, which employs a greedy action selection procedure.",
                "Since EMT exhibits the key features of the general DBC framework, as well as polynomial time complexity, we used the multitarget version of EMT [15] to demonstrate that the class of area sweeping problems naturally lends itself to dynamics-based descriptions, as instantiated by our experiments in the Tag Game domain.",
                "As enumerated in Section 4.2, EMT has a number of limitations, such as difficulty in dealing with negative dynamic preference.",
                "This prevents direct application of EMT to such problems as Rendezvous-Evasion Games (e.g., [6]).",
                "However, DBC in general has no such limitations, and readily enables the formulation of evasion games.",
                "In future work, we intend to proceed with the development of dynamics-based controllers for these problems. 8.",
                "ACKNOWLEDGMENT The work of the first two authors was partially supported by Israel Science Foundation grant #898/05, and the third author was partially supported by a grant from Israels Ministry of Science and Technology. 9.",
                "REFERENCES [1] R. C. Arkin.",
                "Behavior-Based Robotics.",
                "MIT Press, 1998. [2] J.",
                "A. Bilmes.",
                "A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and Hidden Markov Models.",
                "Technical Report TR-97-021, Department of Electrical Engeineering and Computer Science, University of California at Berkeley, 1998. [3] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of information theory.",
                "Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz, and M. J. Wolverton.",
                "A survey of research in distributed, continual planning.",
                "AI Magazine, 4:13-22, 1999. [5] V. R. Konda and J. N. Tsitsiklis.",
                "Actor-Critic algorithms.",
                "SIAM Journal on Control and Optimization, 42(4):1143-1166, 2003. [6] W. S. Lim.",
                "A rendezvous-evasion game on discrete locations with joint randomization.",
                "Advances in Applied Probability, 29(4):1004-1017, December 1997. [7] M. L. Littman, T. L. Dean, and L. P. Kaelbling.",
                "On the complexity of solving Markov decision problems.",
                "In Proceedings of the 11th Annual Conference on Uncertainty in Artificial Intelligence (UAI-95), pages 394-402, 1995. [8] O. Madani, S. Hanks, and A. Condon.",
                "On the undecidability of probabilistic planning and related stochastic optimization problems.",
                "Artificial Intelligence Journal, 147(1-2):5-34, July 2003. [9] R. M. Neal and G. E. Hinton.",
                "A view of the EM algorithm 796 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) that justifies incremental, sparse, and other variants.",
                "In M. I. Jordan, editor, Learning in Graphical Models, pages 355-368.",
                "Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In Proceeding of AAMAS 2006, 2006. [11] J. Pineau, G. Gordon, and S. Thrun.",
                "Point-based value iteration: An anytime algorithm for pomdps.",
                "In International Joint Conference on Artificial Intelligence (IJCAI), pages 1025-1032, August 2003. [12] M. L. Puterman.",
                "Markov Decision Processes.",
                "Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics Section.",
                "Wiley-Interscience Publication, New York, 1994. [13] Z. Rabinovich and J. S. Rosenschein.",
                "Extended Markov Tracking with an application to control.",
                "In The Workshop on Agent Tracking: Modeling Other Agents from Observations, at the Third International Joint Conference on Autonomous Agents and Multiagent Systems, pages 95-100, New-York, July 2004. [14] Z. Rabinovich and J. S. Rosenschein.",
                "Multiagent coordination by Extended Markov Tracking.",
                "In The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 431-438, Utrecht, The Netherlands, July 2005. [15] Z. Rabinovich and J. S. Rosenschein.",
                "On the response of EMT-based control to interacting targets and models.",
                "In The Fifth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 465-470, Hakodate, Japan, May 2006. [16] R. F. Stengel.",
                "Optimal Control and Estimation.",
                "Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, and P. Varakantham.",
                "Conflicts in teamwork: Hybrids to the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 797"
            ],
            "original_annotated_samples": [
                "The goal in that work was to generate policies that provide a measure of <br>action-selection randomization</br>, while maintaining rewards within some acceptable levels."
            ],
            "translated_annotated_samples": [
                "El objetivo de ese trabajo era generar políticas que proporcionen una medida de <br>aleatorización en la selección de acciones</br>, manteniendo al mismo tiempo las recompensas dentro de niveles aceptables."
            ],
            "translated_text": "En este artículo presentamos el Control Basado en Dinámicas (DBC), un enfoque para la planificación y control de un agente en entornos estocásticos. A diferencia de los enfoques existentes, que buscan optimizar las recompensas esperadas (por ejemplo, en Problemas de Decisión de Markov Parcialmente Observables (POMDP)), DBC optimiza el comportamiento del sistema hacia las dinámicas del sistema especificadas. Mostramos que un enfoque de planificación y control recientemente desarrollado, Seguimiento Extendido de Markov (EMT), es una instancia de DBC. EMT emplea la selección de acciones codiciosa para proporcionar un algoritmo de control eficiente en entornos markovianos. Explotamos esta eficiencia en un conjunto de experimentos que aplicaron EMT multitarget a una clase de problemas de barrido de área (búsqueda de objetivos en movimiento). Mostramos que tales problemas pueden ser definidos de forma natural y resueltos eficientemente utilizando el marco de DBC y su instanciación de EMT. Categorías y Descriptores de Asignaturas I.2.8 [Resolución de Problemas, Métodos de Control y Búsqueda]: Teoría de Control; I.2.9 [Robótica]; I.2.11 [Inteligencia Artificial Distribuida]: Agentes Inteligentes Términos Generales Algoritmos, Teoría 1. INTRODUCCIÓN La planificación y el control constituyen un área de investigación central en sistemas multiagentes e inteligencia artificial. En los últimos años, los Procesos de Decisión de Markov Parcialmente Observables (POMDPs) [12] se han convertido en una base formal popular para la planificación en entornos estocásticos. En este marco, el problema de planificación y control suele abordarse imponiendo una función de recompensa y calculando una política (de elección de acciones) que sea óptima, en el sentido de que resultará en la mayor utilidad esperada. Si bien teóricamente atractivo, la complejidad de resolver óptimamente un POMDP es prohibitiva [8, 7]. Tomamos una visión alternativa de la planificación en entornos estocásticos. No utilizamos una función de recompensa basada en el estado, sino que optimizamos según un criterio diferente, una especificación basada en transiciones de la dinámica del sistema deseado. La idea aquí es ver la ejecución del plan como un proceso que obliga a un sistema (estocástico) a cambiar, y un plan como un proceso dinámico que moldea ese cambio de acuerdo con criterios deseados. Llamamos a este marco de planificación general Control Basado en Dinámicas (DBC). En DBC, el objetivo de un proceso de planificación (o control) es garantizar que el sistema cambiará de acuerdo con dinámicas objetivo específicas (potencialmente estocásticas). Dado que el comportamiento real del sistema puede desviarse de lo especificado por la dinámica objetivo (debido a la naturaleza estocástica del sistema), la planificación en tales entornos debe ser continua, de manera similar a los controladores en lazo cerrado clásicos. Aquí, la optimalidad se mide en términos de la probabilidad de magnitudes de desviación. En este documento, presentamos la estructura del Control Basado en Dinámicas. Mostramos que el enfoque de Seguimiento Extendido de Markov (EMT) recientemente desarrollado [13, 14, 15] está subsumido por DBC, con EMT empleando selección de acciones codiciosa, que es una parametrización específica entre las opciones posibles dentro de DBC. EMT es una implementación eficiente de DBC. Para evaluar DBC, llevamos a cabo un conjunto de experimentos aplicando EMT de múltiples objetivos al Juego de Etiquetas [11]; esta es una variante del problema de barrido de área, donde un agente intenta etiquetar a un objetivo móvil (presa) cuya posición no se conoce con certeza. Los datos experimentales demuestran que incluso con un modelo simple del entorno y un diseño simple de la dinámica del objetivo, se pueden lograr altas tasas de éxito tanto en la captura de la presa como en sorprender a la presa (como se expresa por la entropía observada de la posición de los agentes controlados). El documento está organizado de la siguiente manera. En la Sección 2 motivamos DBC utilizando problemas de barrido de área, y discutimos trabajos relacionados. La Sección 3 introduce la estructura de Control Basado en Dinámicas (DBC) y su especialización en entornos Markovianos. Esto es seguido por una revisión del enfoque de Seguimiento Markov Extendido (EMT) como un régimen de control estructurado por DBC en la Sección 4. Esa sección también discute las limitaciones del control basado en EMT en relación con el marco general de DBC. Los ajustes experimentales y los resultados se presentan a continuación en la Sección 5. La Sección 6 proporciona una breve discusión del enfoque general, y la Sección 7 ofrece algunas observaciones finales y direcciones para trabajos futuros. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2. MOTIVACIÓN Y TRABAJO RELACIONADO Muchos escenarios de la vida real tienen naturalmente una especificación de dinámica de destino estocástica, especialmente en aquellos dominios donde no existe un objetivo final, sino más bien un comportamiento del sistema (con propiedades específicas) que debe ser continuamente respaldado. Por ejemplo, los guardias de seguridad realizan barridos persistentes de un área para detectar cualquier signo de intrusión. Los ladrones astutos intentarán rastrear estos barridos y sincronizar su operación con puntos clave del movimiento de los guardias. Por lo tanto, es recomendable que la dinámica de movimiento de los guardias parezca irregular y aleatoria. El trabajo reciente de Paruchuri et al. [10] ha abordado dicha aleatorización en el contexto de POMDPs de agente único y distribuidos. El objetivo de ese trabajo era generar políticas que proporcionen una medida de <br>aleatorización en la selección de acciones</br>, manteniendo al mismo tiempo las recompensas dentro de niveles aceptables. Nuestro enfoque difiere de este trabajo en que DBC no optimiza recompensas esperadas, de hecho, no consideramos recompensas en absoluto, sino que en su lugar mantenemos dinámicas deseadas (incluyendo, pero no limitado a, la aleatorización). El juego de la mancha es otro ejemplo de la aplicabilidad del enfoque. Fue introducido en el trabajo de Pineau et al. [11]. Hay dos agentes que pueden moverse por un área, la cual está dividida en una cuadrícula. La cuadrícula puede tener celdas bloqueadas (agujeros) en las cuales ningún agente puede moverse. Un agente (el cazador) intenta moverse a una celda ocupada por el otro (la presa), de modo que estén ubicados en el mismo lugar (esto es una etiqueta exitosa). La presa busca evitar al cazador, siempre está al tanto de la posición del cazador, pero no sabe cómo se comportará el cazador, lo que abre la posibilidad de que un cazador sorprenda a la presa. El cazador conoce la ley de movimiento probabilístico de la presa, pero no conoce su ubicación actual. Tag es una instancia de una familia de problemas de barrido de área (persecución-evasión). En [11], el cazador modeló el problema utilizando un POMDP. Se definió una función de recompensa para reflejar el deseo de marcar la presa, y se calculó una política de acción para optimizar la recompensa recolectada con el tiempo. Debido a la complejidad intratable de determinar la política óptima, la política de acción calculada en ese documento fue esencialmente una aproximación. En este artículo, en lugar de formular una función de recompensa, utilizamos EMT para resolver el problema, especificando directamente la dinámica objetivo. De hecho, cualquier problema de búsqueda con movimiento aleatorio, la llamada clase de problemas de barrido de área, puede ser descrito a través de la especificación de la dinámica de dicho sistema objetivo. El Control Basado en Dinámicas proporciona un enfoque natural para resolver estos problemas. CONTROL BASADO EN DINÁMICA La especificación del Control Basado en Dinámica (DBC) se puede dividir en tres niveles interactivos: Nivel de Diseño del Entorno, Nivel de Usuario y Nivel de Agente. • El Nivel de Diseño del Entorno se ocupa de la especificación formal y modelado del entorno. Por ejemplo, este nivel especificaría las leyes de la física dentro del sistema y establecería sus parámetros, como la constante de gravitación. El Nivel de Usuario, a su vez, se basa en el modelo de entorno producido por el Diseño de Entorno para especificar la dinámica del sistema objetivo que desea observar. El Nivel de Usuario también especifica el procedimiento de estimación o aprendizaje para la dinámica del sistema, y la medida de desviación. En el escenario del guardia de museo mencionado anteriormente, estos corresponderían a un horario de barrido estocástico y una medida de sorpresa relativa entre el barrido especificado y el real. • El Nivel de Agente, a su vez, combina el modelo del entorno del Nivel de Diseño de Entorno, el procedimiento de estimación de la dinámica, la medida de desviación y la especificación de la dinámica objetivo del Nivel de Usuario, para producir una secuencia de acciones que creen dinámicas del sistema lo más cercanas posible a la especificación deseada. Dado nuestro interés en el desarrollo continuo de un sistema estocástico, como ocurre en la teoría de control clásica [16] y la planificación continua [4], así como en nuestro ejemplo de recorridos por museos, la pregunta es cómo el Nivel de Agente debe tratar las mediciones de desviación a lo largo del tiempo. Con este fin, utilizamos un umbral de probabilidad, es decir, nos gustaría que el Nivel del Agente maximice la probabilidad de que la medida de desviación permanezca por debajo de cierto umbral. La selección de acciones específicas depende entonces de la formalización del sistema. Una posibilidad sería crear una mezcla de tendencias de sistemas disponibles, similar a lo que sucede en las arquitecturas robóticas basadas en el comportamiento [1]. La otra alternativa sería depender del procedimiento de estimación proporcionado por el Nivel de Usuario para utilizar el modelo del Nivel de Diseño del Entorno del entorno para elegir acciones, de manera que se manipule al estimador de dinámicas para que crea que se ha logrado cierta dinámica. Ten en cuenta que esta manipulación no es directa, sino a través del entorno. Por lo tanto, para algoritmos de estimación lo suficientemente fuertes, la manipulación exitosa significaría una simulación exitosa de la dinámica objetivo especificada (es decir, más allá de discernir a través de la entrada sensorial disponible). Los niveles de DBC también pueden tener un retroceso de información (ver Figura 1). Por ejemplo, el Nivel de Agente podría proporcionar datos sobre la viabilidad de la dinámica del objetivo, permitiendo al Nivel de Usuario modificar el requisito, quizás centrándose en las características alcanzables del comportamiento del sistema. Los datos también estarían disponibles sobre la respuesta del sistema a diferentes acciones realizadas; combinados con un estimador de dinámica definido por el Nivel de Usuario, esto puede proporcionar una herramienta importante para la calibración del modelo del entorno en el Nivel de Diseño del Entorno. UserEnv. El modelo de agente de diseño Estimador de Dinámicas Ideales Estimador de Dinámicas Factibilidad de Respuesta del Sistema de Datos Figura 1: Flujo de datos del marco de trabajo DBC Ampliando la idea de los algoritmos Actor-Crítico [5], el flujo de datos de DBC puede proporcionar una buena base para el diseño de un algoritmo de aprendizaje. Por ejemplo, el Nivel de Usuario puede funcionar como un dispositivo exploratorio para un algoritmo de aprendizaje, inferir un objetivo dinámico ideal a partir del modelo del entorno en cuestión que expondría y verificaría las características más críticas del comportamiento del sistema. En este caso, los datos de viabilidad y respuesta del sistema a nivel de agente proporcionarían información clave para una actualización del modelo del entorno. De hecho, la combinación de datos de viabilidad y respuesta puede proporcionar una base para la aplicación de algoritmos de aprendizaje sólidos como EM [2, 9]. 3.1 DBC para entornos markovianos Para un entorno markoviano parcialmente observable, DBC puede especificarse de una manera más rigurosa. Observa cómo DBC descarta las recompensas y las reemplaza por otro criterio de optimalidad (las diferencias estructurales se resumen en la Tabla 1): • El nivel de Diseño del Entorno es especificar una tupla < S, A, T, O, Ω, s0 >, donde: - S es el conjunto de todos los posibles estados del entorno; - s0 es el estado inicial del entorno (que también puede ser visto como una distribución de probabilidad sobre S); La Sexta Conferencia Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 791 - A es el conjunto de todas las acciones posibles aplicables en el entorno; - T es la función de transición probabilística del entorno: T: S × A → Π(S). Es decir, T(s | a, s) es la probabilidad de que el entorno se mueva del estado s al estado s bajo la acción a; - O es el conjunto de todas las observaciones posibles. Así es como se vería la entrada del sensor para un observador externo; - Ω es la función de probabilidad de observación: Ω: S × A × S → Π(O). Es decir, Ω(o|s, a, s) es la probabilidad de observar o dado que el entorno ha pasado del estado s al estado s bajo la acción a. • Nivel de Usuario, en el caso de un entorno markoviano, opera en el conjunto de dinámicas del sistema descritas por una familia de probabilidades condicionales F = {τ: S × A → Π(S)}. Por lo tanto, la especificación de la dinámica del objetivo puede expresarse como q ∈ F, y el algoritmo de aprendizaje o seguimiento puede representarse como una función L : O×(A×O)∗ → F; es decir, mapea secuencias de observaciones y acciones realizadas hasta el momento en una estimación τ ∈ F de la dinámica del sistema. Hay muchas variaciones posibles disponibles a nivel de usuario para definir la divergencia entre la dinámica del sistema; varias de ellas son: - Distancia de traza o distancia L1 entre dos distribuciones p y q definida por D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Medida de fidelidad de distancia F(p(·), q(·)) = x p(x)q(x) - Divergencia de Kullback-Leibler DKL(p(·) q(·)) = x p(x) log p(x) q(x) Nótese que las dos últimas no son en realidad métricas sobre el espacio de distribuciones posibles, pero tienen interpretaciones significativas e importantes. Por ejemplo, la divergencia de Kullback-Leibler es una herramienta importante de la teoría de la información [3] que permite medir el costo de codificar una fuente de información gobernada por q, asumiendo que está gobernada por p. El Nivel de Usuario también define el umbral de probabilidad de desviación de la dinámica θ. • El Nivel de Agente se enfrenta entonces a un problema de seleccionar una función de señal de control a∗ para satisfacer un problema de minimización de la siguiente manera: a∗ = arg min a Pr(d(τa, q) > θ) donde d(τa, q) es una variable aleatoria que describe la desviación de la estimación de la dinámica τa, creada por L bajo la señal de control a, de la dinámica ideal q. Implícito en este problema de minimización es que L es manipulado a través del entorno, basado en el modelo de entorno producido por el Nivel de Diseño del Entorno. 3.2 Vista DBC del Espacio de Estados Es importante tener en cuenta la visión complementaria que DBC y POMDPs tienen sobre el espacio de estados del entorno. Los POMDP consideran el estado como una instantánea estacionaria del entorno; cualquier atributo de la secuencia de estados que se busque se alcanza a través de las propiedades del proceso de control, en este caso la acumulación de recompensas. Esto se puede ver como si la secuencia de estados y los atributos de esa secuencia solo fueran introducidos por y para el mecanismo de control, la política POMDP. DBC se enfoca en el principio subyacente de la secuenciación de estados, la dinámica del sistema. La especificación de la dinámica de los DBCs puede utilizar el espacio de estado del entorno como un medio para describir, discernir y preservar los cambios que ocurren dentro del sistema. Como resultado, DBC tiene una mayor capacidad para expresar propiedades de secuenciación de estados, las cuales están fundamentadas en el modelo del entorno y su definición del espacio de estados. Por ejemplo, considera la tarea de moverse a través de terreno accidentado hacia un objetivo y alcanzarlo lo más rápido posible. Los POMDPs codificarían el terreno como puntos en el espacio de estados, mientras que la velocidad se garantizaría mediante una recompensa negativa por cada paso dado sin alcanzar la meta. Acumular una recompensa más alta solo se puede lograr mediante un movimiento más rápido. Alternativamente, el espacio de estados podría incluir directamente la noción de velocidad. Para los POMDPs, esto significaría que el mismo concepto está codificado dos veces, en cierto sentido: directamente en el espacio de estados e indirectamente en la acumulación de recompensas. Ahora, incluso si la función de recompensa codificara más y detalles más finos de las propiedades del movimiento, la solución POMDP tendría que buscar en un espacio mucho más grande de políticas, aunque aún esté guiada por el concepto implícito del procedimiento de acumulación de recompensas. Por otro lado, la expresión del objetivo táctico de variaciones y correlaciones entre la posición y la velocidad del movimiento ahora se basa en la representación del espacio de estados. En esta situación, cualquier restricción adicional, como la suavidad del movimiento, los límites de velocidad en diferentes ubicaciones o las reducciones de velocidad durante giros bruscos, son expresadas explícita y uniformemente por el objetivo táctico, y pueden resultar en una selección de acciones más rápida y efectiva por un algoritmo DBC. 4. El Control Basado en EMT como un DBC Recientemente, se introdujo un algoritmo de control llamado Control Basado en EMT [13], que instancia el marco de trabajo DBC. Aunque proporciona una solución ávida aproximada en el sentido de DBC, los experimentos iniciales utilizando el control basado en EMT han sido alentadores [14, 15]. El control basado en EMT se basa en la definición del entorno markoviano, como en el caso de los POMDP, pero sus Niveles de Usuario y Agente son del tipo de optimalidad DBC markoviana. • El Nivel de Usuario del control basado en EMT define una dinámica del sistema objetivo en un caso limitado independiente de la acción: qEMT: S → Π(S). Luego utiliza la medida de divergencia de Kullback-Leibler para componer un estimador de dinámica de sistemas momentáneos: el algoritmo de Seguimiento Extendido de Markov (EMT). El algoritmo mantiene una estimación de la dinámica del sistema τt EMT que es capaz de explicar el cambio reciente en un estimador de estado auxiliar bayesiano de pt−1 a pt, y lo actualiza de manera conservadora utilizando la divergencia de Kullback-Leibler. Dado que τt EMT y pt−1,t son respectivamente las probabilidades condicionales y marginales sobre el espacio de estados del sistema, la explicación simplemente significa que pt(s) = s τt EMT (s | s)pt−1(s), y la actualización de la estimación de la dinámica se realiza resolviendo un 792 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Tabla 1: Estructura de POMDP vs. Control basado en Dinámicas en Entorno Markoviano Nivel Enfoque MDP Markoviano DBC Entorno < S, A, T, O, Ω >, donde S - conjunto de estados A - conjunto de acciones Diseño T : S × A → Π(S) - transición O - conjunto de observaciones Ω : S × A × S → Π(O) Usuario r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - función de recompensa q - dinámicas ideales F - remodelación de recompensa L - estimador de dinámicas θ - umbral Agente π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) problema de minimización: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • El nivel de Agente en el control basado en EMT es subóptimo con respecto a DBC (aunque permanece dentro del marco de DBC), realizando selección de acciones codiciosas basadas en la predicción de la reacción de EMT. La predicción se basa en el modelo del entorno proporcionado por el nivel de Diseño del Entorno, de modo que si denotamos por Ta la función de transición de entornos limitada a la acción a, y pt−1 es el estimador de estado auxiliar del sistema bayesiano, entonces la elección de control basada en EMT se describe por a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1). Nótese que esto sigue precisamente el marco DBC Markoviano: la optimalidad recompensante de los POMDPs se descarta, y en su lugar se manipula un estimador de dinámicas (EMT en este caso) a través de los efectos de la acción en el entorno para producir una estimación cercana a la dinámica del sistema objetivo especificado. Sin embargo, como mencionamos, el control EMT ingenuo es subóptimo en el sentido de DBC y tiene varias limitaciones adicionales que no existen en el marco general de DBC (discutido en la Sección 4.2). 4.1 EMT de múltiples objetivos. En ocasiones, puede haber varias preferencias de comportamiento. Por ejemplo, en el caso de los guardias de museo, algunos objetos de arte están más fuertemente custodiados, lo que requiere que los guardias permanezcan más a menudo en su cercanía. Por otro lado, no se debe dejar ningún rincón del museo sin revisar, lo que requiere un movimiento constante. El éxito de la seguridad en un museo requeriría que los guardias se adhieran y equilibren ambos comportamientos. Para el control basado en EMT, esto significaría enfrentar varios objetivos tácticos {qk}K k=1, y la pregunta sería cómo fusionar y equilibrarlos. Un mecanismo de equilibrio se puede aplicar para resolver este problema. Ten en cuenta que el control basado en EMT, al seleccionar una acción, crea un vector de preferencia sobre el conjunto de acciones basado en su rendimiento predicho con respecto a un objetivo dado. Si estos vectores de preferencia están normalizados, pueden combinarse en una única preferencia unificada. Esto requiere reemplazar la selección de acciones basada en EMT estándar por el algoritmo a continuación [15]: • Dado: - un conjunto de dinámicas objetivo {qk}K k=1, - vector de pesos w(k) • Seleccionar la acción de la siguiente manera - Para cada acción a ∈ A predecir la distribución de estado futuro ¯pa t+1 = Ta ∗ pt; - Para cada acción, calcular Da = H(¯pa t+1, pt, PDt) - Para cada a ∈ A y objetivo táctico qk, denotar V (a, k) = DKL (Da qk) pt. Sea Vk(a) = 1 Zk V (a, k), donde Zk = a∈A V (a, k) es un factor de normalización. - Selecciona a∗ = arg min a k k=1 w(k)Vk(a) El vector de pesos w = (w1, ..., wK ) permite ajustar la importancia entre las dinámicas objetivo sin necesidad de rediseñar los objetivos mismos. Este método de equilibrio también está integrado de forma transparente en el flujo de operación basado en EMT. Limitaciones del Control Basado en EMT El control basado en EMT es una representación subóptima (en el sentido de DBC) de la estructura DBC. Limita al Usuario al obligar a EMT a ser su algoritmo de seguimiento dinámico, y reemplaza la optimización del Agente por la selección de acciones codiciosa. Este tipo de combinación, sin embargo, es común en algoritmos en línea. Aunque es necesario un mayor desarrollo de los controladores basados en EMT, la evidencia hasta ahora sugiere que incluso la forma más simple del algoritmo posee una gran cantidad de potencia y muestra tendencias que son óptimas en el sentido de DBC. Hay dos limitaciones adicionales específicas de los técnicos en emergencias médicas (EMT, por sus siglas en inglés) al control basado en EMT que son evidentes en este momento. Ambos ya tienen soluciones parciales y son objeto de investigación continua. La primera limitación es el problema de la preferencia negativa. En el marco de POMDP, por ejemplo, esto se captura de manera sencilla, a través de The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 793 la aparición de valores con signos diferentes dentro de la estructura de recompensa. Para el control basado en EMT, sin embargo, la preferencia negativa significa que uno quisiera evitar cierta distribución sobre las secuencias de desarrollo del sistema; sin embargo, el control basado en EMT se concentra en acercarse lo más posible a una distribución. La evitación es, por lo tanto, antinatural en el control basado en EMT nativo. La segunda limitación proviene del hecho de que el modelado del entorno estándar puede crear acciones sensoriales puras, acciones que no cambian el estado del mundo y solo difieren en la forma en que se reciben las observaciones y la calidad de las observaciones recibidas. Dado que el estado del mundo no cambia, el control basado en EMT no sería capaz de diferenciar entre diferentes acciones sensoriales. Ten en cuenta que ambas limitaciones del control basado en EMT están ausentes en el marco general de DBC, ya que puede tener un algoritmo de seguimiento capaz de considerar acciones sensoriales puras y, a diferencia de la divergencia de Kullback-Leibler, una medida de desviación de distribución capaz de manejar preferencias negativas. 5. El juego de la \"carrera de relevos\" fue introducido por primera vez en [11]. Es un problema de agente único de capturar una presa, y pertenece a la clase de problemas de barrido de área. Un ejemplo de dominio se muestra en la Figura 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figura 2: Dominio de etiquetas; un agente (A) intenta buscar y capturar una presa (Q). El juego de etiquetas limita extremadamente la percepción de los agentes, de modo que el agente solo puede detectar la presa si están ubicados en la misma celda del mundo de cuadrícula. En la versión clásica del juego, la co-locación conduce a una observación especial, y la acción de Etiqueta se puede realizar. Modificamos ligeramente esta configuración: el momento en que ambos agentes ocupan la misma celda, el juego termina. Como resultado, tanto el agente como su presa tienen la misma capacidad de movimiento, lo que les permite moverse en cuatro direcciones, Norte, Sur, Este y Oeste. Estos forman un espacio formal de acciones dentro de un entorno markoviano. El espacio de estados del entorno formal markoviano está descrito por el producto cruz de las posiciones del agente y la presa. Para la Figura 2, sería S = {s0, ..., s23} × {s0, ..., s23}. Los efectos de una acción tomada por el agente son deterministas, pero el entorno en general tiene una respuesta estocástica debido al movimiento de la presa. Con probabilidad q0 1 se queda en su lugar, y con probabilidad 1 − q0 se mueve a una celda adyacente más lejos del agente 1. En nuestros experimentos, esto se tomó como q0 = 0.2. Por lo tanto, para la instancia mostrada en la Figura 2 y q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Aunque el comportamiento evasivo de la presa es conocido por el agente, la posición de la presa no lo es. La única información sensorial disponible para el agente es su propia ubicación. Utilizamos EMT y especificamos directamente la dinámica del objetivo. Para el juego de la mancha, podemos formular fácilmente tres tendencias principales: atrapar al perseguido, mantenerse en movimiento y acechar al perseguido. Esto resulta en las siguientes tres dinámicas de objetivo: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 en otro caso Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 en otro caso Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Nótese que ninguno de los objetivos anteriores es directamente alcanzable; por ejemplo, si Qt = s9 y At = s11, no hay ninguna acción que pueda llevar al agente a At+1 = s9 como lo requiere la dinámica de objetivo Tcatch. Realizamos varios experimentos para evaluar el rendimiento de EMT en el Juego de Etiquetas. Se utilizaron tres configuraciones del dominio mostrado en la Figura 3, cada una planteando un desafío diferente para el agente debido a la observabilidad parcial. En cada escenario, se realizó un conjunto de 1000 ejecuciones con un límite de tiempo de 100 pasos. En cada ejecución, la posición inicial tanto del agente como de su presa fue seleccionada al azar; esto significa que, en lo que respecta al agente, la posición inicial de la presa estaba distribuida de forma uniforme en todo el espacio de celdas del dominio. También utilizamos dos variaciones de la función de observabilidad del entorno. En la primera versión, la función de observabilidad mapeaba todas las posiciones articulares del cazador y la presa en la posición del cazador como observación. En el segundo, solo aquellas posiciones conjuntas en las que el cazador y la presa ocupaban ubicaciones diferentes fueron mapeadas en la ubicación del cazador. La segunda versión de hecho utilizó y expresó el hecho de que una vez que el cazador y la presa ocupan la misma celda, el juego termina. Los resultados de estos experimentos se muestran en la Tabla 2. Equilibrando la dinámica de captura, movimiento y seguimiento descrita en la sección anterior mediante el vector de peso [0.8, 0.1, 0.1], EMT logró un rendimiento estable en los tres dominios. Aunque las comparaciones directas son difíciles de hacer, el rendimiento de los EMT mostró una notable eficiencia en comparación con el enfoque POMDP. A pesar de una implementación simple e ineficiente del algoritmo EMT en Matlab, el tiempo de decisión para cualquier paso dado promedió significativamente por debajo de 1 segundo en todos los experimentos. Para el dominio de la arena abierta irregular, que resultó ser el más difícil, se completaron 1000 ejecuciones de experimentos limitadas a 100 pasos cada una, un total de 42411 pasos, en poco menos de 6 horas. Es decir, más de 4 × 104 pasos en línea tomaron una orden de magnitud menos tiempo que la computación sin conexión de la política POMDP en [11]. La importancia de esta diferencia se destaca aún más por el hecho de que, en caso de que cambien los parámetros del modelo del entorno, la naturaleza en línea de EMT le permitiría mantener su rendimiento, mientras que la política POMDP necesitaría ser recalculada, lo que requeriría una vez más un gran costo computacional adicional. También probamos la entropía de la frecuencia de las células de comportamiento, medidas empíricas de los datos de prueba. Como muestran la Figura 4 y la Figura 5, empir794 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 3: Se utilizaron estas configuraciones del espacio de juego de Tag: a) múltiples callejones sin salida, b) arena abierta irregular, c) pasillo circular. Tabla 2: Rendimiento de la solución basada en EMT en tres dominios de juego de Tag y dos modelos de observabilidad: I) cantera de omniposición, II) la cantera no está en la posición de los cazadores. Modelo Dominio Captura% E(Pasos) Tiempo/Paso I Callejones sin salida 100 14.8 72(mSeg) Arena 80.2 42.4 500(mSeg) Círculo 91.4 34.6 187(mSeg) II Callejones sin salida 100 13.2 91(mSeg) Arena 96.8 28.67 396(mSeg) Círculo 94.4 31.63 204(mSeg) La entropía lógica crece con la duración de la interacción. Para las carreras en las que la presa no fue capturada de inmediato, la entropía alcanza valores entre 0.85 y 0.952 para diferentes carreras y escenarios. A medida que el agente busca activamente la presa, la entropía nunca alcanza su máximo. Una característica del gráfico de entropía para el escenario de arena abierta llamó especialmente nuestra atención en el caso del modelo de observación de cantera omniposicional. Cerca del límite máximo de longitud de prueba (100 pasos), la entropía cayó repentinamente. Un análisis adicional de los datos mostró que bajo ciertas circunstancias, se produce un comportamiento fluctuante en el que el agente se enfrenta a versiones igualmente viables del comportamiento de seguir a la presa. Dado que el algoritmo EMT tiene una selección de acciones codiciosa, y el espacio de estados no codifica ninguna forma de compromiso (ni siquiera velocidad o aceleración), el agente queda atrapado dentro de una pequeña porción de celdas. Básicamente, se intenta seguir simultáneamente varios cursos de acción, todos los cuales son consistentes con la dinámica objetivo. Este comportamiento no ocurrió en nuestro segundo modelo de observación, ya que redujo significativamente el conjunto de cursos de acción elegibles, contribuyendo esencialmente a romper el empate entre ellos. 6. DISCUSIÓN El diseño de la solución EMT para el Juego de Etiquetas expone la diferencia fundamental en el enfoque de planificación y control entre EMT o DBC, por un lado, y el enfoque más familiar de POMDP, por otro lado. POMDP define una estructura de recompensa para optimizar e influencia la dinámica del sistema de forma indirecta a través de esa optimización. EMT descarta cualquier esquema de recompensa y, en cambio, mide e influye directamente en la dinámica del sistema. Se calculó la entropía 2 utilizando el logaritmo con base igual al número de ubicaciones posibles dentro del dominio; esto escala adecuadamente la expresión de entropía en el rango [0, 1] para todos los dominios. Por lo tanto, para el Juego de Etiquetas, no buscamos una función de recompensa que codificara y expresara nuestra preferencia sobre el comportamiento de los agentes, sino que establecimos directamente tres preferencias de comportamiento (heurísticas) como base para mantener la dinámica del objetivo. Los datos experimentales muestran que estos objetivos no necesitan ser directamente alcanzables a través de las acciones de los agentes. Sin embargo, aún queda por explorar la relación entre el rendimiento de la EMT y la alcanzabilidad de la dinámica objetivo. Los datos del experimento del juego de etiquetas también revelaron la diferente importancia que DBC y POMDPs otorgan a la formulación del espacio de estados del entorno. Los POMDPs dependen enteramente del mecanismo de maximización de la acumulación de recompensas, es decir, la formación del procedimiento de selección de acciones para lograr la secuenciación de estados necesaria. DBC, por otro lado, tiene dos fuentes de especificación de secuenciación: a través de las propiedades de un procedimiento de selección de acciones y a través de una especificación directa dentro de la dinámica objetivo. La importancia de la segunda fuente fue subrayada por los datos del experimento del Juego de Etiquetas, en el que el algoritmo EMT codicioso, aplicado a una especificación de espacio de estados tipo POMDP, falló, ya que la descripción del objetivo sobre dicho espacio de estados no era capaz de codificar las tendencias de comportamiento necesarias, por ejemplo, la resolución de empates y el compromiso con el movimiento dirigido. Las diferencias estructurales entre DBC (y EMT en particular) y POMDPs impiden la comparación directa de rendimiento y los sitúan en pistas complementarias, cada uno dentro de un nicho adecuado. Por ejemplo, los POMDP podrían ser vistos como una formulación mucho más natural de problemas económicos de toma de decisiones secuenciales, mientras que EMT se ajusta mejor a la demanda continua de cambios estocásticos, como ocurre en muchos problemas de robótica o de agentes incorporados. Las propiedades complementarias de POMDPs y EMT pueden ser aún más explotadas. Existe un interés reciente en utilizar POMDPs en soluciones híbridas [17], en las cuales los POMDPs pueden ser utilizados junto con otros enfoques de control para proporcionar resultados que no son fácilmente alcanzables con ninguno de los enfoques por sí solos. DBC puede ser un socio efectivo en una solución híbrida como esta. Por ejemplo, los POMDP tienen requisitos de tiempo fuera de línea prohibitivamente grandes para el cálculo de políticas, pero pueden ser fácilmente utilizados en entornos más simples para exponer tendencias de comportamiento beneficiosas; esto puede servir como una forma de dinámica objetivo que se proporciona a EMT en un dominio más grande para su operación en línea. 7. CONCLUSIONES Y TRABAJO FUTURO En este artículo, hemos presentado una nueva perspectiva sobre el proceso de planificación y control en entornos estocásticos, en forma del marco de Control Basado en Dinámicas (DBC). DBC formula la tarea de planificar como el apoyo a la dinámica de un sistema objetivo especificado, que describe las propiedades necesarias del cambio dentro del entorno. La optimalidad de los planes de acción de DBC se mide en el Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Calles sin salida 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Círculo Figura 4: Modelo de Observación I: Cantera omniposicional. Desarrollo de entropía con longitud de juego de etiqueta para los tres escenarios experimentales: a) múltiples callejones sin salida, b) arena abierta irregular, c) pasillo circular. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Callejones sin salida 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Círculo Figura 5: Modelo de observación II: cantera no observada en la posición de los cazadores. Desarrollo de la entropía con la longitud del juego de etiquetas para los tres escenarios experimentales: a) múltiples callejones sin salida, b) arena abierta irregular, c) pasillo circular, con respecto a la desviación de la dinámica del sistema real de la dinámica objetivo. Mostramos que una técnica recientemente desarrollada de Seguimiento Markov Extendido (EMT) [13] es una instancia de DBC. De hecho, EMT puede ser visto como un caso específico de la parametrización DBC, que emplea un procedimiento de selección de acciones codicioso. Dado que EMT exhibe las características clave del marco general DBC, así como una complejidad temporal polinómica, utilizamos la versión multitarget de EMT [15] para demostrar que la clase de problemas de barrido de área se presta naturalmente a descripciones basadas en dinámicas, tal como se ejemplifica en nuestros experimentos en el dominio del Juego de Etiquetas. Como se enumera en la Sección 4.2, EMT tiene varias limitaciones, como la dificultad para manejar la preferencia dinámica negativa. Esto evita la aplicación directa de la EMT a problemas como los Juegos de Rendezvous-Evasión (por ejemplo, [6]). Sin embargo, DBC en general no tiene tales limitaciones y permite fácilmente la formulación de juegos de evasión. En trabajos futuros, tenemos la intención de continuar con el desarrollo de controladores basados en dinámica para estos problemas. 8. AGRADECIMIENTO El trabajo de los dos primeros autores fue parcialmente apoyado por la subvención #898/05 de la Fundación para la Ciencia de Israel, y el tercer autor fue parcialmente apoyado por una subvención del Ministerio de Ciencia y Tecnología de Israel. REFERENCIAS [1] R. C. Arkin. Robótica basada en el comportamiento. MIT Press, 1998. [2] J. \n\nMIT Press, 1998. [2] J. A. Bilmes. Un tutorial detallado del algoritmo EM y su aplicación en la estimación de parámetros para mezclas gaussianas y modelos ocultos de Markov. Informe técnico TR-97-021, Departamento de Ingeniería Eléctrica e Informática, Universidad de California en Berkeley, 1998. [3] T. M. Cover y J. A. Thomas. Elementos de teoría de la información. Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz y M. J. Wolverton. Una encuesta de investigación en planificación distribuida y continua. Revista de Inteligencia Artificial, 4:13-22, 1999. [5] V. R. Konda y J. N. Tsitsiklis. Algoritmos Actor-Crítico. Revista SIAM de Control y Optimización, 42(4):1143-1166, 2003. [6] W. S. Lim. Un juego de encuentro-evasión en ubicaciones discretas con aleatorización conjunta. Avances en Probabilidad Aplicada, 29(4):1004-1017, diciembre de 1997. [7] M. L. Littman, T. L. Dean y L. P. Kaelbling. Sobre la complejidad de resolver problemas de decisión de Markov. En Actas de la 11ª Conferencia Anual sobre Incertidumbre en Inteligencia Artificial (UAI-95), páginas 394-402, 1995. [8] O. Madani, S. Hanks y A. Condon. Sobre la indecidibilidad de la planificación probabilística y problemas relacionados de optimización estocástica. Revista de Inteligencia Artificial, 147(1-2):5-34, julio de 2003. [9] R. M. Neal y G. E. Hinton. Una vista del algoritmo EM 796 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) que justifica enfoques incrementales, dispersos y otras variantes. En M. I. Jordan, editor, Aprendizaje en Modelos Gráficos, páginas 355-368. Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez y S. Kraus. Seguridad en sistemas multiagentes mediante la aleatorización de políticas. En Actas de AAMAS 2006, 2006. [11] J. Pineau, G. Gordon y S. Thrun. Iteración de valor basada en puntos: Un algoritmo en cualquier momento para POMDPs. En la Conferencia Conjunta Internacional de Inteligencia Artificial (IJCAI), páginas 1025-1032, agosto de 2003. [12] M. L. Puterman. Procesos de Decisión de Markov. Serie Wiley en Probabilidad y Estadística Matemática: Sección de Probabilidad y Estadística Aplicada. Publicación de Wiley-Interscience, Nueva York, 1994. [13] Z. Rabinovich y J. S. Rosenschein. Seguimiento Markov extendido con una aplicación al control. En el taller sobre seguimiento de agentes: modelando otros agentes a partir de observaciones, en la Tercera Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, páginas 95-100, Nueva York, julio de 2004. [14] Z. Rabinovich y J. S. Rosenschein. Coordinación multiagente mediante Seguimiento Markov Extendido. En la Cuarta Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, páginas 431-438, Utrecht, Países Bajos, julio de 2005. [15] Z. Rabinovich y J. S. Rosenschein. Sobre la respuesta del control basado en EMT a objetivos y modelos interactivos. En la Quinta Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, páginas 465-470, Hakodate, Japón, mayo de 2006. [16] R. F. Stengel. Control óptimo y estimación. Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, y P. Varakantham. Conflictos en el trabajo en equipo: Híbridos para el Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 797 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "game of tag": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Dynamics Based Control with an Application to Area-Sweeping Problems Zinovi Rabinovich Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel jeff@cs.huji.ac.il Gal A. Kaminka The MAVERICK Group Department of Computer Science Bar Ilan University, Israel galk@cs.biu.ac.il ABSTRACT In this paper we introduce Dynamics Based Control (DBC), an approach to planning and control of an agent in stochastic environments.",
                "Unlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics.",
                "We show that a recently developed planning and control approach, Extended Markov Tracking (EMT) is an instantiation of DBC.",
                "EMT employs greedy action selection to provide an efficient control algorithm in Markovian environments.",
                "We exploit this efficiency in a set of experiments that applied multitarget EMT to a class of area-sweeping problems (searching for moving targets).",
                "We show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation.",
                "Categories and Subject Descriptors I.2.8 [Problem Solving, Control Methods, and Search]: Control Theory; I.2.9 [Robotics]; I.2.11 [Distributed Artificial Intelligence]: Intelligent Agents General Terms Algorithms, Theory 1.",
                "INTRODUCTION Planning and control constitutes a central research area in multiagent systems and artificial intelligence.",
                "In recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in stochastic environments.",
                "In this framework, the planning and control problem is often addressed by imposing a reward function, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility.",
                "While theoretically attractive, the complexity of optimally solving a POMDP is prohibitive [8, 7].",
                "We take an alternative view of planning in stochastic environments.",
                "We do not use a (state-based) reward function, but instead optimize over a different criterion, a transition-based specification of the desired system dynamics.",
                "The idea here is to view planexecution as a process that compels a (stochastic) system to change, and a plan as a dynamic process that shapes that change according to desired criteria.",
                "We call this general planning framework Dynamics Based Control (DBC).",
                "In DBC, the goal of a planning (or control) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) target dynamics.",
                "As actual system behavior may deviate from that which is specified by target dynamics (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16].",
                "Here, optimality is measured in terms of probability of deviation magnitudes.",
                "In this paper, we present the structure of Dynamics Based Control.",
                "We show that the recently developed Extended Markov Tracking (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC.",
                "EMT is an efficient instantiation of DBC.",
                "To evaluate DBC, we carried out a set of experiments applying multi-target EMT to the Tag Game [11]; this is a variant on the area sweeping problem, where an agent is trying to tag a moving target (quarry) whose position is not known with certainty.",
                "Experimental data demonstrates that even with a simple model of the environment and a simple design of target dynamics, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agents position).",
                "The paper is organized as follows.",
                "In Section 2 we motivate DBC using area-sweeping problems, and discuss related work.",
                "Section 3 introduces the Dynamics Based Control (DBC) structure, and its specialization to Markovian environments.",
                "This is followed by a review of the Extended Markov Tracking (EMT) approach as a DBC-structured control regimen in Section 4.",
                "That section also discusses the limitations of EMT-based control relative to the general DBC framework.",
                "Experimental settings and results are then presented in Section 5.",
                "Section 6 provides a short discussion of the overall approach, and Section 7 gives some concluding remarks and directions for future work. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "MOTIVATION AND RELATED WORK Many real-life scenarios naturally have a stochastic target dynamics specification, especially those domains where there exists no ultimate goal, but rather system behavior (with specific properties) that has to be continually supported.",
                "For example, security guards perform persistent sweeps of an area to detect any sign of intrusion.",
                "Cunning thieves will attempt to track these sweeps, and time their operation to key points of the guards motion.",
                "It is thus advisable to make the guards motion dynamics appear irregular and random.",
                "Recent work by Paruchuri et al. [10] has addressed such randomization in the context of single-agent and distributed POMDPs.",
                "The goal in that work was to generate policies that provide a measure of action-selection randomization, while maintaining rewards within some acceptable levels.",
                "Our focus differs from this work in that DBC does not optimize expected rewards-indeed we do not consider rewards at all-but instead maintains desired dynamics (including, but not limited to, randomization).",
                "The <br>game of tag</br> is another example of the applicability of the approach.",
                "It was introduced in the work by Pineau et al. [11].",
                "There are two agents that can move about an area, which is divided into a grid.",
                "The grid may have blocked cells (holes) into which no agent can move.",
                "One agent (the hunter) seeks to move into a cell occupied by the other (the quarry), such that they are co-located (this is a successful tag).",
                "The quarry seeks to avoid the hunter agent, and is always aware of the hunters position, but does not know how the hunter will behave, which opens up the possibility for a hunter to surprise the prey.",
                "The hunter knows the quarrys probabilistic law of motion, but does not know its current location.",
                "Tag is an instance of a family of area-sweeping (pursuit-evasion) problems.",
                "In [11], the hunter modeled the problem using a POMDP.",
                "A reward function was defined, to reflect the desire to tag the quarry, and an action policy was computed to optimize the reward collected over time.",
                "Due to the intractable complexity of determining the optimal policy, the action policy computed in that paper was essentially an approximation.",
                "In this paper, instead of formulating a reward function, we use EMT to solve the problem, by directly specifying the target dynamics.",
                "In fact, any search problem with randomized motion, the socalled class of area sweeping problems, can be described through specification of such target system dynamics.",
                "Dynamics Based Control provides a natural approach to solving these problems. 3.",
                "DYNAMICS BASED CONTROL The specification of Dynamics Based Control (DBC) can be broken into three interacting levels: Environment Design Level, User Level, and Agent Level. • Environment Design Level is concerned with the formal specification and modeling of the environment.",
                "For example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation constant. • User Level in turn relies on the environment model produced by Environment Design to specify the target system dynamics it wishes to observe.",
                "The User Level also specifies the estimation or learning procedure for system dynamics, and the measure of deviation.",
                "In the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping. • Agent Level in turn combines the environment model from the Environment Design level, the dynamics estimation procedure, the deviation measure and the target dynamics specification from User Level, to produce a sequence of actions that create system dynamics as close as possible to the targeted specification.",
                "As we are interested in the continual development of a stochastic system, such as happens in classical control theory [16] and continual planning [4], as well as in our example of museum sweeps, the question becomes how the Agent Level is to treat the deviation measurements over time.",
                "To this end, we use a probability threshold-that is, we would like the Agent Level to maximize the probability that the deviation measure will remain below a certain threshold.",
                "Specific action selection then depends on system formalization.",
                "One possibility would be to create a mixture of available system trends, much like that which happens in Behavior-Based Robotic architectures [1].",
                "The other alternative would be to rely on the estimation procedure provided by the User Level-to utilize the Environment Design Level model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a certain dynamics has been achieved.",
                "Notice that this manipulation is not direct, but via the environment.",
                "Thus, for strong enough estimator algorithms, successful manipulation would mean a successful simulation of the specified target dynamics (i.e., beyond discerning via the available sensory input).",
                "DBC levels can also have a back-flow of information (see Figure 1).",
                "For instance, the Agent Level could provide data about target dynamics feasibility, allowing the User Level to modify the requirement, perhaps focusing on attainable features of system behavior.",
                "Data would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the User Level, this can provide an important tool for the environment model calibration at the Environment Design Level.",
                "UserEnv.",
                "Design Agent Model Ideal Dynamics Estimator Estimator Dynamics Feasibility System Response Data Figure 1: Data flow of the DBC framework Extending upon the idea of Actor-Critic algorithms [5], DBC data flow can provide a good basis for the design of a learning algorithm.",
                "For example, the User Level can operate as an exploratory device for a learning algorithm, inferring an ideal dynamics target from the environment model at hand that would expose and verify most critical features of system behavior.",
                "In this case, feasibility and system response data from the Agent Level would provide key information for an environment model update.",
                "In fact, the combination of feasibility and response data can provide a basis for the application of strong learning algorithms such as EM [2, 9]. 3.1 DBC for Markovian Environments For a Partially Observable Markovian Environment, DBC can be specified in a more rigorous manner.",
                "Notice how DBC discards rewards, and replaces it by another optimality criterion (structural differences are summarized in Table 1): • Environment Design level is to specify a tuple < S, A, T, O, Ω, s0 >, where: - S is the set of all possible environment states; - s0 is the initial state of the environment (which can also be viewed as a probability distribution over S); The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 791 - A is the set of all possible actions applicable in the environment; - T is the environments probabilistic transition function: T : S ×A → Π(S).",
                "That is, T(s |a, s) is the probability that the environment will move from state s to state s under action a; - O is the set of all possible observations.",
                "This is what the sensor input would look like for an outside observer; - Ω is the observation probability function: Ω : S × A × S → Π(O).",
                "That is, Ω(o|s , a, s) is the probability that one will observe o given that the environment has moved from state s to state s under action a. • User Level, in the case of Markovian environment, operates on the set of system dynamics described by a family of conditional probabilities F = {τ : S × A → Π(S)}.",
                "Thus specification of target dynamics can be expressed by q ∈ F, and the learning or tracking algorithm can be represented as a function L : O×(A×O)∗ → F; that is, it maps sequences of observations and actions performed so far into an estimate τ ∈ F of system dynamics.",
                "There are many possible variations available at the User Level to define divergence between system dynamics; several of them are: - Trace distance or L1 distance between two distributions p and q defined by D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Fidelity measure of distance F(p(·), q(·)) = x p(x)q(x) - Kullback-Leibler divergence DKL(p(·) q(·)) = x p(x) log p(x) q(x) Notice that the latter two are not actually metrics over the space of possible distributions, but nevertheless have meaningful and important interpretations.",
                "For instance, KullbackLeibler divergence is an important tool of information theory [3] that allows one to measure the price of encoding an information source governed by q, while assuming that it is governed by p. The User Level also defines the threshold of dynamics deviation probability θ. • Agent Level is then faced with a problem of selecting a control signal function a∗ to satisfy a minimization problem as follows: a∗ = arg min a Pr(d(τa, q) > θ) where d(τa, q) is a random variable describing deviation of the dynamics estimate τa, created by L under control signal a, from the ideal dynamics q.",
                "Implicit in this minimization problem is that L is manipulated via the environment, based on the environment model produced by the Environment Design Level. 3.2 DBC View of the State Space It is important to note the complementary view that DBC and POMDPs take on the state space of the environment.",
                "POMDPs regard state as a stationary snap-shot of the environment; whatever attributes of state sequencing one seeks are reached through properties of the control process, in this case reward accumulation.",
                "This can be viewed as if the sequencing of states and the attributes of that sequencing are only introduced by and for the controlling mechanism-the POMDP policy.",
                "DBC concentrates on the underlying principle of state sequencing, the system dynamics.",
                "DBCs target dynamics specification can use the environments state space as a means to describe, discern, and preserve changes that occur within the system.",
                "As a result, DBC has a greater ability to express state sequencing properties, which are grounded in the environment model and its state space definition.",
                "For example, consider the task of moving through rough terrain towards a goal and reaching it as fast as possible.",
                "POMDPs would encode terrain as state space points, while speed would be ensured by negative reward for every step taken without reaching the goalaccumulating higher reward can be reached only by faster motion.",
                "Alternatively, the state space could directly include the notion of speed.",
                "For POMDPs, this would mean that the same concept is encoded twice, in some sense: directly in the state space, and indirectly within reward accumulation.",
                "Now, even if the reward function would encode more, and finer, details of the properties of motion, the POMDP solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure.",
                "On the other hand, the tactical target expression of variations and correlations between position and speed of motion is now grounded in the state space representation.",
                "In this situation, any further constraints, e.g., smoothness of motion, speed limits in different locations, or speed reductions during sharp turns, are explicitly and uniformly expressed by the tactical target, and can result in faster and more effective action selection by a DBC algorithm. 4.",
                "EMT-BASED CONTROL AS A DBC Recently, a control algorithm was introduced called EMT-based Control [13], which instantiates the DBC framework.",
                "Although it provides an approximate greedy solution in the DBC sense, initial experiments using EMT-based control have been encouraging [14, 15].",
                "EMT-based control is based on the Markovian environment definition, as in the case with POMDPs, but its User and Agent Levels are of the Markovian DBC type of optimality. • User Level of EMT-based control defines a limited-case target system dynamics independent of action: qEMT : S → Π(S).",
                "It then utilizes the Kullback-Leibler divergence measure to compose a momentary system dynamics estimator-the Extended Markov Tracking (EMT) algorithm.",
                "The algorithm keeps a system dynamics estimate τt EMT that is capable of explaining recent change in an auxiliary Bayesian system state estimator from pt−1 to pt, and updates it conservatively using Kullback-Leibler divergence.",
                "Since τt EMT and pt−1,t are respectively the conditional and marginal probabilities over the systems state space, explanation simply means that pt(s ) = s τt EMT (s |s)pt−1(s), and the dynamics estimate update is performed by solving a 792 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: Structure of POMDP vs. Dynamics-Based Control in Markovian Environment Level Approach MDP Markovian DBC Environment < S, A, T, O, Ω >,where S - set of states A - set of actions Design T : S × A → Π(S) - transition O - observation set Ω : S × A × S → Π(O) User r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - reward function q - ideal dynamics F - reward remodeling L - dynamics estimator θ - threshold Agent π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) minimization problem: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • Agent Level in EMT-based control is suboptimal with respect to DBC (though it remains within the DBC framework), performing greedy action selection based on prediction of EMTs reaction.",
                "The prediction is based on the environment model provided by the Environment Design level, so that if we denote by Ta the environments transition function limited to action a, and pt−1 is the auxiliary Bayesian system state estimator, then the EMT-based control choice is described by a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1) Note that this follows the Markovian DBC framework precisely: the rewarding optimality of POMDPs is discarded, and in its place a dynamics estimator (EMT in this case) is manipulated via action effects on the environment to produce an estimate close to the specified target system dynamics.",
                "Yet as we mentioned, naive EMTbased control is suboptimal in the DBC sense, and has several additional limitations that do not exist in the general DBC framework (discussed in Section 4.2). 4.1 Multi-Target EMT At times, there may exist several behavioral preferences.",
                "For example, in the case of museum guards, some art items are more heavily guarded, requiring that the guards stay more often in their close vicinity.",
                "On the other hand, no corner of the museum is to be left unchecked, which demands constant motion.",
                "Successful museum security would demand that the guards adhere to, and balance, both of these behaviors.",
                "For EMT-based control, this would mean facing several tactical targets {qk}K k=1, and the question becomes how to merge and balance them.",
                "A balancing mechanism can be applied to resolve this issue.",
                "Note that EMT-based control, while selecting an action, creates a preference vector over the set of actions based on their predicted performance with respect to a given target.",
                "If these preference vectors are normalized, they can be combined into a single unified preference.",
                "This requires replacement of standard EMT-based action selection by the algorithm below [15]: • Given: - a set of target dynamics {qk}K k=1, - vector of weights w(k) • Select action as follows - For each action a ∈ A predict the future state distribution ¯pa t+1 = Ta ∗ pt; - For each action, compute Da = H(¯pa t+1, pt, PDt) - For each a ∈ A and qk tactical target, denote V (a, k) = DKL (Da qk) pt .",
                "Let Vk(a) = 1 Zk V (a, k), where Zk = a∈A V (a, k) is a normalization factor. - Select a∗ = arg min a k k=1 w(k)Vk(a) The weights vector w = (w1, ..., wK ) allows the additional tuning of importance among target dynamics without the need to redesign the targets themselves.",
                "This balancing method is also seamlessly integrated into the EMT-based control flow of operation. 4.2 EMT-based Control Limitations EMT-based control is a sub-optimal (in the DBC sense) representative of the DBC structure.",
                "It limits the User by forcing EMT to be its dynamic tracking algorithm, and replaces Agent optimization by greedy action selection.",
                "This kind of combination, however, is common for on-line algorithms.",
                "Although further development of EMT-based controllers is necessary, evidence so far suggests that even the simplest form of the algorithm possesses a great deal of power, and displays trends that are optimal in the DBC sense of the word.",
                "There are two further, EMT-specific, limitations to EMT-based control that are evident at this point.",
                "Both already have partial solutions and are subjects of ongoing research.",
                "The first limitation is the problem of negative preference.",
                "In the POMDP framework for example, this is captured simply, through The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 793 the appearance of values with different signs within the reward structure.",
                "For EMT-based control, however, negative preference means that one would like to avoid a certain distribution over system development sequences; EMT-based control, however, concentrates on getting as close as possible to a distribution.",
                "Avoidance is thus unnatural in native EMT-based control.",
                "The second limitation comes from the fact that standard environment modeling can create pure sensory actions-actions that do not change the state of the world, and differ only in the way observations are received and the quality of observations received.",
                "Since the world state does not change, EMT-based control would not be able to differentiate between different sensory actions.",
                "Notice that both of these limitations of EMT-based control are absent from the general DBC framework, since it may have a tracking algorithm capable of considering pure sensory actions and, unlike Kullback-Leibler divergence, a distribution deviation measure that is capable of dealing with negative preference. 5.",
                "EMT PLAYING TAG The <br>game of tag</br> was first introduced in [11].",
                "It is a single agent problem of capturing a quarry, and belongs to the class of area sweeping problems.",
                "An example domain is shown in Figure 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figure 2: Tag domain; an agent (A) attempts to seek and capture a quarry (Q) The <br>game of tag</br> extremely limits the agents perception, so that the agent is able to detect the quarry only if they are co-located in the same cell of the grid world.",
                "In the classical version of the game, co-location leads to a special observation, and the Tag action can be performed.",
                "We slightly modify this setting: the moment that both agents occupy the same cell, the game ends.",
                "As a result, both the agent and its quarry have the same motion capability, which allows them to move in four directions, North, South, East, and West.",
                "These form a formal space of actions within a Markovian environment.",
                "The state space of the formal Markovian environment is described by the cross-product of the agent and quarrys positions.",
                "For Figure 2, it would be S = {s0, ..., s23} × {s0, ..., s23}.",
                "The effects of an action taken by the agent are deterministic, but the environment in general has a stochastic response due to the motion of the quarry.",
                "With probability q0 1 it stays put, and with probability 1 − q0 it moves to an adjacent cell further away from the 1 In our experiments this was taken to be q0 = 0.2. agent.",
                "So for the instance shown in Figure 2 and q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Although the evasive behavior of the quarry is known to the agent, the quarrys position is not.",
                "The only sensory information available to the agent is its own location.",
                "We use EMT and directly specify the target dynamics.",
                "For the <br>game of tag</br>, we can easily formulate three major trends: catching the quarry, staying mobile, and stalking the quarry.",
                "This results in the following three target dynamics: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 otherwise Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 otherwise Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Note that none of the above targets are directly achievable; for instance, if Qt = s9 and At = s11, there is no action that can move the agent to At+1 = s9 as required by the Tcatch target dynamics.",
                "We ran several experiments to evaluate EMT performance in the Tag Game.",
                "Three configurations of the domain shown in Figure 3 were used, each posing a different challenge to the agent due to partial observability.",
                "In each setting, a set of 1000 runs was performed with a time limit of 100 steps.",
                "In every run, the initial position of both the agent and its quarry was selected at random; this means that as far as the agent was concerned, the quarrys initial position was uniformly distributed over the entire domain cell space.",
                "We also used two variations of the environment observability function.",
                "In the first version, observability function mapped all joint positions of hunter and quarry into the position of the hunter as an observation.",
                "In the second, only those joint positions in which hunter and quarry occupied different locations were mapped into the hunters location.",
                "The second version in fact utilized and expressed the fact that once hunter and quarry occupy the same cell the game ends.",
                "The results of these experiments are shown in Table 2.",
                "Balancing [15] the catch, move, and stalk target dynamics described in the previous section by the weight vector [0.8, 0.1, 0.1], EMT produced stable performance in all three domains.",
                "Although direct comparisons are difficult to make, the EMT performance displayed notable efficiency vis-`a-vis the POMDP approach.",
                "In spite of a simple and inefficient Matlab implementation of the EMT algorithm, the decision time for any given step averaged significantly below 1 second in all experiments.",
                "For the irregular open arena domain, which proved to be the most difficult, 1000 experiment runs bounded by 100 steps each, a total of 42411 steps, were completed in slightly under 6 hours.",
                "That is, over 4 × 104 online steps took an order of magnitude less time than the offline computation of POMDP policy in [11].",
                "The significance of this differential is made even more prominent by the fact that, should the environment model parameters change, the online nature of EMT would allow it to maintain its performance time, while the POMDP policy would need to be recomputed, requiring yet again a large overhead of computation.",
                "We also tested the behavior cell frequency entropy, empirical measures from trial data.",
                "As Figure 4 and Figure 5 show, empir794 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) A Q Q A 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 A Q Figure 3: These configurations of the Tag Game space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor Table 2: Performance of the EMT-based solution in three Tag Game domains and two observability models: I) omniposition quarry, II) quarry is not at hunters position Model Domain Capture% E(Steps) Time/Step I Dead-ends 100 14.8 72(mSec) Arena 80.2 42.4 500(mSec) Circle 91.4 34.6 187(mSec) II Dead-ends 100 13.2 91(mSec) Arena 96.8 28.67 396(mSec) Circle 94.4 31.63 204(mSec) ical entropy grows with the length of interaction.",
                "For runs where the quarry was not captured immediately, the entropy reaches between 0.85 and 0.952 for different runs and scenarios.",
                "As the agent actively seeks the quarry, the entropy never reaches its maximum.",
                "One characteristic of the entropy graph for the open arena scenario particularly caught our attention in the case of the omniposition quarry observation model.",
                "Near the maximum limit of trial length (100 steps), entropy suddenly dropped.",
                "Further analysis of the data showed that under certain circumstances, a fluctuating behavior occurs in which the agent faces equally viable versions of quarry-following behavior.",
                "Since the EMT algorithm has greedy action selection, and the state space does not encode any form of commitment (not even speed or acceleration), the agent is locked within a small portion of cells.",
                "It is essentially attempting to simultaneously follow several courses of action, all of which are consistent with the target dynamics.",
                "This behavior did not occur in our second observation model, since it significantly reduced the set of eligible courses of action-essentially contributing to tie-breaking among them. 6.",
                "DISCUSSION The design of the EMT solution for the Tag Game exposes the core difference in approach to planning and control between EMT or DBC, on the one hand, and the more familiar POMDP approach, on the other.",
                "POMDP defines a reward structure to optimize, and influences system dynamics indirectly through that optimization.",
                "EMT discards any reward scheme, and instead measures and influences system dynamics directly. 2 Entropy was calculated using log base equal to the number of possible locations within the domain; this properly scales entropy expression into the range [0, 1] for all domains.",
                "Thus for the Tag Game, we did not search for a reward function that would encode and express our preference over the agents behavior, but rather directly set three (heuristic) behavior preferences as the basis for target dynamics to be maintained.",
                "Experimental data shows that these targets need not be directly achievable via the agents actions.",
                "However, the ratio between EMT performance and achievability of target dynamics remains to be explored.",
                "The tag game experiment data also revealed the different emphasis DBC and POMDPs place on the formulation of an environment state space.",
                "POMDPs rely entirely on the mechanism of reward accumulation maximization, i.e., formation of the action selection procedure to achieve necessary state sequencing.",
                "DBC, on the other hand, has two sources of sequencing specification: through the properties of an action selection procedure, and through direct specification within the target dynamics.",
                "The importance of the second source was underlined by the Tag Game experiment data, in which the greedy EMT algorithm, applied to a POMDP-type state space specification, failed, since target description over such a state space was incapable of encoding the necessary behavior tendencies, e.g., tie-breaking and commitment to directed motion.",
                "The structural differences between DBC (and EMT in particular), and POMDPs, prohibits direct performance comparison, and places them on complementary tracks, each within a suitable niche.",
                "For instance, POMDPs could be seen as a much more natural formulation of economic sequential decision-making problems, while EMT is a better fit for continual demand for stochastic change, as happens in many robotic or embodied-agent problems.",
                "The complementary properties of POMDPs and EMT can be further exploited.",
                "There is recent interest in using POMDPs in hybrid solutions [17], in which the POMDPs can be used together with other control approaches to provide results not easily achievable with either approach by itself.",
                "DBC can be an effective partner in such a hybrid solution.",
                "For instance, POMDPs have prohibitively large off-line time requirements for policy computation, but can be readily used in simpler settings to expose beneficial behavioral trends; this can serve as a form of target dynamics that are provided to EMT in a larger domain for on-line operation. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have presented a novel perspective on the process of planning and control in stochastic environments, in the form of the Dynamics Based Control (DBC) framework.",
                "DBC formulates the task of planning as support of a specified target system dynamics, which describes the necessary properties of change within the environment.",
                "Optimality of DBC plans of action are measured The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 4: Observation Model I: Omniposition quarry.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 5: Observation Model II: quarry not observed at hunters position.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. with respect to the deviation of actual system dynamics from the target dynamics.",
                "We show that a recently developed technique of Extended Markov Tracking (EMT) [13] is an instantiation of DBC.",
                "In fact, EMT can be seen as a specific case of DBC parameterization, which employs a greedy action selection procedure.",
                "Since EMT exhibits the key features of the general DBC framework, as well as polynomial time complexity, we used the multitarget version of EMT [15] to demonstrate that the class of area sweeping problems naturally lends itself to dynamics-based descriptions, as instantiated by our experiments in the Tag Game domain.",
                "As enumerated in Section 4.2, EMT has a number of limitations, such as difficulty in dealing with negative dynamic preference.",
                "This prevents direct application of EMT to such problems as Rendezvous-Evasion Games (e.g., [6]).",
                "However, DBC in general has no such limitations, and readily enables the formulation of evasion games.",
                "In future work, we intend to proceed with the development of dynamics-based controllers for these problems. 8.",
                "ACKNOWLEDGMENT The work of the first two authors was partially supported by Israel Science Foundation grant #898/05, and the third author was partially supported by a grant from Israels Ministry of Science and Technology. 9.",
                "REFERENCES [1] R. C. Arkin.",
                "Behavior-Based Robotics.",
                "MIT Press, 1998. [2] J.",
                "A. Bilmes.",
                "A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and Hidden Markov Models.",
                "Technical Report TR-97-021, Department of Electrical Engeineering and Computer Science, University of California at Berkeley, 1998. [3] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of information theory.",
                "Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz, and M. J. Wolverton.",
                "A survey of research in distributed, continual planning.",
                "AI Magazine, 4:13-22, 1999. [5] V. R. Konda and J. N. Tsitsiklis.",
                "Actor-Critic algorithms.",
                "SIAM Journal on Control and Optimization, 42(4):1143-1166, 2003. [6] W. S. Lim.",
                "A rendezvous-evasion game on discrete locations with joint randomization.",
                "Advances in Applied Probability, 29(4):1004-1017, December 1997. [7] M. L. Littman, T. L. Dean, and L. P. Kaelbling.",
                "On the complexity of solving Markov decision problems.",
                "In Proceedings of the 11th Annual Conference on Uncertainty in Artificial Intelligence (UAI-95), pages 394-402, 1995. [8] O. Madani, S. Hanks, and A. Condon.",
                "On the undecidability of probabilistic planning and related stochastic optimization problems.",
                "Artificial Intelligence Journal, 147(1-2):5-34, July 2003. [9] R. M. Neal and G. E. Hinton.",
                "A view of the EM algorithm 796 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) that justifies incremental, sparse, and other variants.",
                "In M. I. Jordan, editor, Learning in Graphical Models, pages 355-368.",
                "Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In Proceeding of AAMAS 2006, 2006. [11] J. Pineau, G. Gordon, and S. Thrun.",
                "Point-based value iteration: An anytime algorithm for pomdps.",
                "In International Joint Conference on Artificial Intelligence (IJCAI), pages 1025-1032, August 2003. [12] M. L. Puterman.",
                "Markov Decision Processes.",
                "Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics Section.",
                "Wiley-Interscience Publication, New York, 1994. [13] Z. Rabinovich and J. S. Rosenschein.",
                "Extended Markov Tracking with an application to control.",
                "In The Workshop on Agent Tracking: Modeling Other Agents from Observations, at the Third International Joint Conference on Autonomous Agents and Multiagent Systems, pages 95-100, New-York, July 2004. [14] Z. Rabinovich and J. S. Rosenschein.",
                "Multiagent coordination by Extended Markov Tracking.",
                "In The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 431-438, Utrecht, The Netherlands, July 2005. [15] Z. Rabinovich and J. S. Rosenschein.",
                "On the response of EMT-based control to interacting targets and models.",
                "In The Fifth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 465-470, Hakodate, Japan, May 2006. [16] R. F. Stengel.",
                "Optimal Control and Estimation.",
                "Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, and P. Varakantham.",
                "Conflicts in teamwork: Hybrids to the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 797"
            ],
            "original_annotated_samples": [
                "The <br>game of tag</br> is another example of the applicability of the approach.",
                "EMT PLAYING TAG The <br>game of tag</br> was first introduced in [11].",
                "An example domain is shown in Figure 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figure 2: Tag domain; an agent (A) attempts to seek and capture a quarry (Q) The <br>game of tag</br> extremely limits the agents perception, so that the agent is able to detect the quarry only if they are co-located in the same cell of the grid world.",
                "For the <br>game of tag</br>, we can easily formulate three major trends: catching the quarry, staying mobile, and stalking the quarry."
            ],
            "translated_annotated_samples": [
                "El <br>juego de la mancha</br> es otro ejemplo de la aplicabilidad del enfoque.",
                "El <br>juego de la \"carrera de relevos\"</br> fue introducido por primera vez en [11].",
                "Un ejemplo de dominio se muestra en la Figura 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figura 2: Dominio de etiquetas; un agente (A) intenta buscar y capturar una presa (Q). El <br>juego de etiquetas</br> limita extremadamente la percepción de los agentes, de modo que el agente solo puede detectar la presa si están ubicados en la misma celda del mundo de cuadrícula.",
                "Para el <br>juego de la mancha</br>, podemos formular fácilmente tres tendencias principales: atrapar al perseguido, mantenerse en movimiento y acechar al perseguido."
            ],
            "translated_text": "En este artículo presentamos el Control Basado en Dinámicas (DBC), un enfoque para la planificación y control de un agente en entornos estocásticos. A diferencia de los enfoques existentes, que buscan optimizar las recompensas esperadas (por ejemplo, en Problemas de Decisión de Markov Parcialmente Observables (POMDP)), DBC optimiza el comportamiento del sistema hacia las dinámicas del sistema especificadas. Mostramos que un enfoque de planificación y control recientemente desarrollado, Seguimiento Extendido de Markov (EMT), es una instancia de DBC. EMT emplea la selección de acciones codiciosa para proporcionar un algoritmo de control eficiente en entornos markovianos. Explotamos esta eficiencia en un conjunto de experimentos que aplicaron EMT multitarget a una clase de problemas de barrido de área (búsqueda de objetivos en movimiento). Mostramos que tales problemas pueden ser definidos de forma natural y resueltos eficientemente utilizando el marco de DBC y su instanciación de EMT. Categorías y Descriptores de Asignaturas I.2.8 [Resolución de Problemas, Métodos de Control y Búsqueda]: Teoría de Control; I.2.9 [Robótica]; I.2.11 [Inteligencia Artificial Distribuida]: Agentes Inteligentes Términos Generales Algoritmos, Teoría 1. INTRODUCCIÓN La planificación y el control constituyen un área de investigación central en sistemas multiagentes e inteligencia artificial. En los últimos años, los Procesos de Decisión de Markov Parcialmente Observables (POMDPs) [12] se han convertido en una base formal popular para la planificación en entornos estocásticos. En este marco, el problema de planificación y control suele abordarse imponiendo una función de recompensa y calculando una política (de elección de acciones) que sea óptima, en el sentido de que resultará en la mayor utilidad esperada. Si bien teóricamente atractivo, la complejidad de resolver óptimamente un POMDP es prohibitiva [8, 7]. Tomamos una visión alternativa de la planificación en entornos estocásticos. No utilizamos una función de recompensa basada en el estado, sino que optimizamos según un criterio diferente, una especificación basada en transiciones de la dinámica del sistema deseado. La idea aquí es ver la ejecución del plan como un proceso que obliga a un sistema (estocástico) a cambiar, y un plan como un proceso dinámico que moldea ese cambio de acuerdo con criterios deseados. Llamamos a este marco de planificación general Control Basado en Dinámicas (DBC). En DBC, el objetivo de un proceso de planificación (o control) es garantizar que el sistema cambiará de acuerdo con dinámicas objetivo específicas (potencialmente estocásticas). Dado que el comportamiento real del sistema puede desviarse de lo especificado por la dinámica objetivo (debido a la naturaleza estocástica del sistema), la planificación en tales entornos debe ser continua, de manera similar a los controladores en lazo cerrado clásicos. Aquí, la optimalidad se mide en términos de la probabilidad de magnitudes de desviación. En este documento, presentamos la estructura del Control Basado en Dinámicas. Mostramos que el enfoque de Seguimiento Extendido de Markov (EMT) recientemente desarrollado [13, 14, 15] está subsumido por DBC, con EMT empleando selección de acciones codiciosa, que es una parametrización específica entre las opciones posibles dentro de DBC. EMT es una implementación eficiente de DBC. Para evaluar DBC, llevamos a cabo un conjunto de experimentos aplicando EMT de múltiples objetivos al Juego de Etiquetas [11]; esta es una variante del problema de barrido de área, donde un agente intenta etiquetar a un objetivo móvil (presa) cuya posición no se conoce con certeza. Los datos experimentales demuestran que incluso con un modelo simple del entorno y un diseño simple de la dinámica del objetivo, se pueden lograr altas tasas de éxito tanto en la captura de la presa como en sorprender a la presa (como se expresa por la entropía observada de la posición de los agentes controlados). El documento está organizado de la siguiente manera. En la Sección 2 motivamos DBC utilizando problemas de barrido de área, y discutimos trabajos relacionados. La Sección 3 introduce la estructura de Control Basado en Dinámicas (DBC) y su especialización en entornos Markovianos. Esto es seguido por una revisión del enfoque de Seguimiento Markov Extendido (EMT) como un régimen de control estructurado por DBC en la Sección 4. Esa sección también discute las limitaciones del control basado en EMT en relación con el marco general de DBC. Los ajustes experimentales y los resultados se presentan a continuación en la Sección 5. La Sección 6 proporciona una breve discusión del enfoque general, y la Sección 7 ofrece algunas observaciones finales y direcciones para trabajos futuros. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2. MOTIVACIÓN Y TRABAJO RELACIONADO Muchos escenarios de la vida real tienen naturalmente una especificación de dinámica de destino estocástica, especialmente en aquellos dominios donde no existe un objetivo final, sino más bien un comportamiento del sistema (con propiedades específicas) que debe ser continuamente respaldado. Por ejemplo, los guardias de seguridad realizan barridos persistentes de un área para detectar cualquier signo de intrusión. Los ladrones astutos intentarán rastrear estos barridos y sincronizar su operación con puntos clave del movimiento de los guardias. Por lo tanto, es recomendable que la dinámica de movimiento de los guardias parezca irregular y aleatoria. El trabajo reciente de Paruchuri et al. [10] ha abordado dicha aleatorización en el contexto de POMDPs de agente único y distribuidos. El objetivo de ese trabajo era generar políticas que proporcionen una medida de aleatorización en la selección de acciones, manteniendo al mismo tiempo las recompensas dentro de niveles aceptables. Nuestro enfoque difiere de este trabajo en que DBC no optimiza recompensas esperadas, de hecho, no consideramos recompensas en absoluto, sino que en su lugar mantenemos dinámicas deseadas (incluyendo, pero no limitado a, la aleatorización). El <br>juego de la mancha</br> es otro ejemplo de la aplicabilidad del enfoque. Fue introducido en el trabajo de Pineau et al. [11]. Hay dos agentes que pueden moverse por un área, la cual está dividida en una cuadrícula. La cuadrícula puede tener celdas bloqueadas (agujeros) en las cuales ningún agente puede moverse. Un agente (el cazador) intenta moverse a una celda ocupada por el otro (la presa), de modo que estén ubicados en el mismo lugar (esto es una etiqueta exitosa). La presa busca evitar al cazador, siempre está al tanto de la posición del cazador, pero no sabe cómo se comportará el cazador, lo que abre la posibilidad de que un cazador sorprenda a la presa. El cazador conoce la ley de movimiento probabilístico de la presa, pero no conoce su ubicación actual. Tag es una instancia de una familia de problemas de barrido de área (persecución-evasión). En [11], el cazador modeló el problema utilizando un POMDP. Se definió una función de recompensa para reflejar el deseo de marcar la presa, y se calculó una política de acción para optimizar la recompensa recolectada con el tiempo. Debido a la complejidad intratable de determinar la política óptima, la política de acción calculada en ese documento fue esencialmente una aproximación. En este artículo, en lugar de formular una función de recompensa, utilizamos EMT para resolver el problema, especificando directamente la dinámica objetivo. De hecho, cualquier problema de búsqueda con movimiento aleatorio, la llamada clase de problemas de barrido de área, puede ser descrito a través de la especificación de la dinámica de dicho sistema objetivo. El Control Basado en Dinámicas proporciona un enfoque natural para resolver estos problemas. CONTROL BASADO EN DINÁMICA La especificación del Control Basado en Dinámica (DBC) se puede dividir en tres niveles interactivos: Nivel de Diseño del Entorno, Nivel de Usuario y Nivel de Agente. • El Nivel de Diseño del Entorno se ocupa de la especificación formal y modelado del entorno. Por ejemplo, este nivel especificaría las leyes de la física dentro del sistema y establecería sus parámetros, como la constante de gravitación. El Nivel de Usuario, a su vez, se basa en el modelo de entorno producido por el Diseño de Entorno para especificar la dinámica del sistema objetivo que desea observar. El Nivel de Usuario también especifica el procedimiento de estimación o aprendizaje para la dinámica del sistema, y la medida de desviación. En el escenario del guardia de museo mencionado anteriormente, estos corresponderían a un horario de barrido estocástico y una medida de sorpresa relativa entre el barrido especificado y el real. • El Nivel de Agente, a su vez, combina el modelo del entorno del Nivel de Diseño de Entorno, el procedimiento de estimación de la dinámica, la medida de desviación y la especificación de la dinámica objetivo del Nivel de Usuario, para producir una secuencia de acciones que creen dinámicas del sistema lo más cercanas posible a la especificación deseada. Dado nuestro interés en el desarrollo continuo de un sistema estocástico, como ocurre en la teoría de control clásica [16] y la planificación continua [4], así como en nuestro ejemplo de recorridos por museos, la pregunta es cómo el Nivel de Agente debe tratar las mediciones de desviación a lo largo del tiempo. Con este fin, utilizamos un umbral de probabilidad, es decir, nos gustaría que el Nivel del Agente maximice la probabilidad de que la medida de desviación permanezca por debajo de cierto umbral. La selección de acciones específicas depende entonces de la formalización del sistema. Una posibilidad sería crear una mezcla de tendencias de sistemas disponibles, similar a lo que sucede en las arquitecturas robóticas basadas en el comportamiento [1]. La otra alternativa sería depender del procedimiento de estimación proporcionado por el Nivel de Usuario para utilizar el modelo del Nivel de Diseño del Entorno del entorno para elegir acciones, de manera que se manipule al estimador de dinámicas para que crea que se ha logrado cierta dinámica. Ten en cuenta que esta manipulación no es directa, sino a través del entorno. Por lo tanto, para algoritmos de estimación lo suficientemente fuertes, la manipulación exitosa significaría una simulación exitosa de la dinámica objetivo especificada (es decir, más allá de discernir a través de la entrada sensorial disponible). Los niveles de DBC también pueden tener un retroceso de información (ver Figura 1). Por ejemplo, el Nivel de Agente podría proporcionar datos sobre la viabilidad de la dinámica del objetivo, permitiendo al Nivel de Usuario modificar el requisito, quizás centrándose en las características alcanzables del comportamiento del sistema. Los datos también estarían disponibles sobre la respuesta del sistema a diferentes acciones realizadas; combinados con un estimador de dinámica definido por el Nivel de Usuario, esto puede proporcionar una herramienta importante para la calibración del modelo del entorno en el Nivel de Diseño del Entorno. UserEnv. El modelo de agente de diseño Estimador de Dinámicas Ideales Estimador de Dinámicas Factibilidad de Respuesta del Sistema de Datos Figura 1: Flujo de datos del marco de trabajo DBC Ampliando la idea de los algoritmos Actor-Crítico [5], el flujo de datos de DBC puede proporcionar una buena base para el diseño de un algoritmo de aprendizaje. Por ejemplo, el Nivel de Usuario puede funcionar como un dispositivo exploratorio para un algoritmo de aprendizaje, inferir un objetivo dinámico ideal a partir del modelo del entorno en cuestión que expondría y verificaría las características más críticas del comportamiento del sistema. En este caso, los datos de viabilidad y respuesta del sistema a nivel de agente proporcionarían información clave para una actualización del modelo del entorno. De hecho, la combinación de datos de viabilidad y respuesta puede proporcionar una base para la aplicación de algoritmos de aprendizaje sólidos como EM [2, 9]. 3.1 DBC para entornos markovianos Para un entorno markoviano parcialmente observable, DBC puede especificarse de una manera más rigurosa. Observa cómo DBC descarta las recompensas y las reemplaza por otro criterio de optimalidad (las diferencias estructurales se resumen en la Tabla 1): • El nivel de Diseño del Entorno es especificar una tupla < S, A, T, O, Ω, s0 >, donde: - S es el conjunto de todos los posibles estados del entorno; - s0 es el estado inicial del entorno (que también puede ser visto como una distribución de probabilidad sobre S); La Sexta Conferencia Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 791 - A es el conjunto de todas las acciones posibles aplicables en el entorno; - T es la función de transición probabilística del entorno: T: S × A → Π(S). Es decir, T(s | a, s) es la probabilidad de que el entorno se mueva del estado s al estado s bajo la acción a; - O es el conjunto de todas las observaciones posibles. Así es como se vería la entrada del sensor para un observador externo; - Ω es la función de probabilidad de observación: Ω: S × A × S → Π(O). Es decir, Ω(o|s, a, s) es la probabilidad de observar o dado que el entorno ha pasado del estado s al estado s bajo la acción a. • Nivel de Usuario, en el caso de un entorno markoviano, opera en el conjunto de dinámicas del sistema descritas por una familia de probabilidades condicionales F = {τ: S × A → Π(S)}. Por lo tanto, la especificación de la dinámica del objetivo puede expresarse como q ∈ F, y el algoritmo de aprendizaje o seguimiento puede representarse como una función L : O×(A×O)∗ → F; es decir, mapea secuencias de observaciones y acciones realizadas hasta el momento en una estimación τ ∈ F de la dinámica del sistema. Hay muchas variaciones posibles disponibles a nivel de usuario para definir la divergencia entre la dinámica del sistema; varias de ellas son: - Distancia de traza o distancia L1 entre dos distribuciones p y q definida por D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Medida de fidelidad de distancia F(p(·), q(·)) = x p(x)q(x) - Divergencia de Kullback-Leibler DKL(p(·) q(·)) = x p(x) log p(x) q(x) Nótese que las dos últimas no son en realidad métricas sobre el espacio de distribuciones posibles, pero tienen interpretaciones significativas e importantes. Por ejemplo, la divergencia de Kullback-Leibler es una herramienta importante de la teoría de la información [3] que permite medir el costo de codificar una fuente de información gobernada por q, asumiendo que está gobernada por p. El Nivel de Usuario también define el umbral de probabilidad de desviación de la dinámica θ. • El Nivel de Agente se enfrenta entonces a un problema de seleccionar una función de señal de control a∗ para satisfacer un problema de minimización de la siguiente manera: a∗ = arg min a Pr(d(τa, q) > θ) donde d(τa, q) es una variable aleatoria que describe la desviación de la estimación de la dinámica τa, creada por L bajo la señal de control a, de la dinámica ideal q. Implícito en este problema de minimización es que L es manipulado a través del entorno, basado en el modelo de entorno producido por el Nivel de Diseño del Entorno. 3.2 Vista DBC del Espacio de Estados Es importante tener en cuenta la visión complementaria que DBC y POMDPs tienen sobre el espacio de estados del entorno. Los POMDP consideran el estado como una instantánea estacionaria del entorno; cualquier atributo de la secuencia de estados que se busque se alcanza a través de las propiedades del proceso de control, en este caso la acumulación de recompensas. Esto se puede ver como si la secuencia de estados y los atributos de esa secuencia solo fueran introducidos por y para el mecanismo de control, la política POMDP. DBC se enfoca en el principio subyacente de la secuenciación de estados, la dinámica del sistema. La especificación de la dinámica de los DBCs puede utilizar el espacio de estado del entorno como un medio para describir, discernir y preservar los cambios que ocurren dentro del sistema. Como resultado, DBC tiene una mayor capacidad para expresar propiedades de secuenciación de estados, las cuales están fundamentadas en el modelo del entorno y su definición del espacio de estados. Por ejemplo, considera la tarea de moverse a través de terreno accidentado hacia un objetivo y alcanzarlo lo más rápido posible. Los POMDPs codificarían el terreno como puntos en el espacio de estados, mientras que la velocidad se garantizaría mediante una recompensa negativa por cada paso dado sin alcanzar la meta. Acumular una recompensa más alta solo se puede lograr mediante un movimiento más rápido. Alternativamente, el espacio de estados podría incluir directamente la noción de velocidad. Para los POMDPs, esto significaría que el mismo concepto está codificado dos veces, en cierto sentido: directamente en el espacio de estados e indirectamente en la acumulación de recompensas. Ahora, incluso si la función de recompensa codificara más y detalles más finos de las propiedades del movimiento, la solución POMDP tendría que buscar en un espacio mucho más grande de políticas, aunque aún esté guiada por el concepto implícito del procedimiento de acumulación de recompensas. Por otro lado, la expresión del objetivo táctico de variaciones y correlaciones entre la posición y la velocidad del movimiento ahora se basa en la representación del espacio de estados. En esta situación, cualquier restricción adicional, como la suavidad del movimiento, los límites de velocidad en diferentes ubicaciones o las reducciones de velocidad durante giros bruscos, son expresadas explícita y uniformemente por el objetivo táctico, y pueden resultar en una selección de acciones más rápida y efectiva por un algoritmo DBC. 4. El Control Basado en EMT como un DBC Recientemente, se introdujo un algoritmo de control llamado Control Basado en EMT [13], que instancia el marco de trabajo DBC. Aunque proporciona una solución ávida aproximada en el sentido de DBC, los experimentos iniciales utilizando el control basado en EMT han sido alentadores [14, 15]. El control basado en EMT se basa en la definición del entorno markoviano, como en el caso de los POMDP, pero sus Niveles de Usuario y Agente son del tipo de optimalidad DBC markoviana. • El Nivel de Usuario del control basado en EMT define una dinámica del sistema objetivo en un caso limitado independiente de la acción: qEMT: S → Π(S). Luego utiliza la medida de divergencia de Kullback-Leibler para componer un estimador de dinámica de sistemas momentáneos: el algoritmo de Seguimiento Extendido de Markov (EMT). El algoritmo mantiene una estimación de la dinámica del sistema τt EMT que es capaz de explicar el cambio reciente en un estimador de estado auxiliar bayesiano de pt−1 a pt, y lo actualiza de manera conservadora utilizando la divergencia de Kullback-Leibler. Dado que τt EMT y pt−1,t son respectivamente las probabilidades condicionales y marginales sobre el espacio de estados del sistema, la explicación simplemente significa que pt(s) = s τt EMT (s | s)pt−1(s), y la actualización de la estimación de la dinámica se realiza resolviendo un 792 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Tabla 1: Estructura de POMDP vs. Control basado en Dinámicas en Entorno Markoviano Nivel Enfoque MDP Markoviano DBC Entorno < S, A, T, O, Ω >, donde S - conjunto de estados A - conjunto de acciones Diseño T : S × A → Π(S) - transición O - conjunto de observaciones Ω : S × A × S → Π(O) Usuario r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - función de recompensa q - dinámicas ideales F - remodelación de recompensa L - estimador de dinámicas θ - umbral Agente π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) problema de minimización: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • El nivel de Agente en el control basado en EMT es subóptimo con respecto a DBC (aunque permanece dentro del marco de DBC), realizando selección de acciones codiciosas basadas en la predicción de la reacción de EMT. La predicción se basa en el modelo del entorno proporcionado por el nivel de Diseño del Entorno, de modo que si denotamos por Ta la función de transición de entornos limitada a la acción a, y pt−1 es el estimador de estado auxiliar del sistema bayesiano, entonces la elección de control basada en EMT se describe por a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1). Nótese que esto sigue precisamente el marco DBC Markoviano: la optimalidad recompensante de los POMDPs se descarta, y en su lugar se manipula un estimador de dinámicas (EMT en este caso) a través de los efectos de la acción en el entorno para producir una estimación cercana a la dinámica del sistema objetivo especificado. Sin embargo, como mencionamos, el control EMT ingenuo es subóptimo en el sentido de DBC y tiene varias limitaciones adicionales que no existen en el marco general de DBC (discutido en la Sección 4.2). 4.1 EMT de múltiples objetivos. En ocasiones, puede haber varias preferencias de comportamiento. Por ejemplo, en el caso de los guardias de museo, algunos objetos de arte están más fuertemente custodiados, lo que requiere que los guardias permanezcan más a menudo en su cercanía. Por otro lado, no se debe dejar ningún rincón del museo sin revisar, lo que requiere un movimiento constante. El éxito de la seguridad en un museo requeriría que los guardias se adhieran y equilibren ambos comportamientos. Para el control basado en EMT, esto significaría enfrentar varios objetivos tácticos {qk}K k=1, y la pregunta sería cómo fusionar y equilibrarlos. Un mecanismo de equilibrio se puede aplicar para resolver este problema. Ten en cuenta que el control basado en EMT, al seleccionar una acción, crea un vector de preferencia sobre el conjunto de acciones basado en su rendimiento predicho con respecto a un objetivo dado. Si estos vectores de preferencia están normalizados, pueden combinarse en una única preferencia unificada. Esto requiere reemplazar la selección de acciones basada en EMT estándar por el algoritmo a continuación [15]: • Dado: - un conjunto de dinámicas objetivo {qk}K k=1, - vector de pesos w(k) • Seleccionar la acción de la siguiente manera - Para cada acción a ∈ A predecir la distribución de estado futuro ¯pa t+1 = Ta ∗ pt; - Para cada acción, calcular Da = H(¯pa t+1, pt, PDt) - Para cada a ∈ A y objetivo táctico qk, denotar V (a, k) = DKL (Da qk) pt. Sea Vk(a) = 1 Zk V (a, k), donde Zk = a∈A V (a, k) es un factor de normalización. - Selecciona a∗ = arg min a k k=1 w(k)Vk(a) El vector de pesos w = (w1, ..., wK ) permite ajustar la importancia entre las dinámicas objetivo sin necesidad de rediseñar los objetivos mismos. Este método de equilibrio también está integrado de forma transparente en el flujo de operación basado en EMT. Limitaciones del Control Basado en EMT El control basado en EMT es una representación subóptima (en el sentido de DBC) de la estructura DBC. Limita al Usuario al obligar a EMT a ser su algoritmo de seguimiento dinámico, y reemplaza la optimización del Agente por la selección de acciones codiciosa. Este tipo de combinación, sin embargo, es común en algoritmos en línea. Aunque es necesario un mayor desarrollo de los controladores basados en EMT, la evidencia hasta ahora sugiere que incluso la forma más simple del algoritmo posee una gran cantidad de potencia y muestra tendencias que son óptimas en el sentido de DBC. Hay dos limitaciones adicionales específicas de los técnicos en emergencias médicas (EMT, por sus siglas en inglés) al control basado en EMT que son evidentes en este momento. Ambos ya tienen soluciones parciales y son objeto de investigación continua. La primera limitación es el problema de la preferencia negativa. En el marco de POMDP, por ejemplo, esto se captura de manera sencilla, a través de The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 793 la aparición de valores con signos diferentes dentro de la estructura de recompensa. Para el control basado en EMT, sin embargo, la preferencia negativa significa que uno quisiera evitar cierta distribución sobre las secuencias de desarrollo del sistema; sin embargo, el control basado en EMT se concentra en acercarse lo más posible a una distribución. La evitación es, por lo tanto, antinatural en el control basado en EMT nativo. La segunda limitación proviene del hecho de que el modelado del entorno estándar puede crear acciones sensoriales puras, acciones que no cambian el estado del mundo y solo difieren en la forma en que se reciben las observaciones y la calidad de las observaciones recibidas. Dado que el estado del mundo no cambia, el control basado en EMT no sería capaz de diferenciar entre diferentes acciones sensoriales. Ten en cuenta que ambas limitaciones del control basado en EMT están ausentes en el marco general de DBC, ya que puede tener un algoritmo de seguimiento capaz de considerar acciones sensoriales puras y, a diferencia de la divergencia de Kullback-Leibler, una medida de desviación de distribución capaz de manejar preferencias negativas. 5. El <br>juego de la \"carrera de relevos\"</br> fue introducido por primera vez en [11]. Es un problema de agente único de capturar una presa, y pertenece a la clase de problemas de barrido de área. Un ejemplo de dominio se muestra en la Figura 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figura 2: Dominio de etiquetas; un agente (A) intenta buscar y capturar una presa (Q). El <br>juego de etiquetas</br> limita extremadamente la percepción de los agentes, de modo que el agente solo puede detectar la presa si están ubicados en la misma celda del mundo de cuadrícula. En la versión clásica del juego, la co-locación conduce a una observación especial, y la acción de Etiqueta se puede realizar. Modificamos ligeramente esta configuración: el momento en que ambos agentes ocupan la misma celda, el juego termina. Como resultado, tanto el agente como su presa tienen la misma capacidad de movimiento, lo que les permite moverse en cuatro direcciones, Norte, Sur, Este y Oeste. Estos forman un espacio formal de acciones dentro de un entorno markoviano. El espacio de estados del entorno formal markoviano está descrito por el producto cruz de las posiciones del agente y la presa. Para la Figura 2, sería S = {s0, ..., s23} × {s0, ..., s23}. Los efectos de una acción tomada por el agente son deterministas, pero el entorno en general tiene una respuesta estocástica debido al movimiento de la presa. Con probabilidad q0 1 se queda en su lugar, y con probabilidad 1 − q0 se mueve a una celda adyacente más lejos del agente 1. En nuestros experimentos, esto se tomó como q0 = 0.2. Por lo tanto, para la instancia mostrada en la Figura 2 y q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Aunque el comportamiento evasivo de la presa es conocido por el agente, la posición de la presa no lo es. La única información sensorial disponible para el agente es su propia ubicación. Utilizamos EMT y especificamos directamente la dinámica del objetivo. Para el <br>juego de la mancha</br>, podemos formular fácilmente tres tendencias principales: atrapar al perseguido, mantenerse en movimiento y acechar al perseguido. Esto resulta en las siguientes tres dinámicas de objetivo: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 en otro caso Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 en otro caso Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Nótese que ninguno de los objetivos anteriores es directamente alcanzable; por ejemplo, si Qt = s9 y At = s11, no hay ninguna acción que pueda llevar al agente a At+1 = s9 como lo requiere la dinámica de objetivo Tcatch. Realizamos varios experimentos para evaluar el rendimiento de EMT en el Juego de Etiquetas. Se utilizaron tres configuraciones del dominio mostrado en la Figura 3, cada una planteando un desafío diferente para el agente debido a la observabilidad parcial. En cada escenario, se realizó un conjunto de 1000 ejecuciones con un límite de tiempo de 100 pasos. En cada ejecución, la posición inicial tanto del agente como de su presa fue seleccionada al azar; esto significa que, en lo que respecta al agente, la posición inicial de la presa estaba distribuida de forma uniforme en todo el espacio de celdas del dominio. También utilizamos dos variaciones de la función de observabilidad del entorno. En la primera versión, la función de observabilidad mapeaba todas las posiciones articulares del cazador y la presa en la posición del cazador como observación. En el segundo, solo aquellas posiciones conjuntas en las que el cazador y la presa ocupaban ubicaciones diferentes fueron mapeadas en la ubicación del cazador. La segunda versión de hecho utilizó y expresó el hecho de que una vez que el cazador y la presa ocupan la misma celda, el juego termina. Los resultados de estos experimentos se muestran en la Tabla 2. Equilibrando la dinámica de captura, movimiento y seguimiento descrita en la sección anterior mediante el vector de peso [0.8, 0.1, 0.1], EMT logró un rendimiento estable en los tres dominios. Aunque las comparaciones directas son difíciles de hacer, el rendimiento de los EMT mostró una notable eficiencia en comparación con el enfoque POMDP. A pesar de una implementación simple e ineficiente del algoritmo EMT en Matlab, el tiempo de decisión para cualquier paso dado promedió significativamente por debajo de 1 segundo en todos los experimentos. Para el dominio de la arena abierta irregular, que resultó ser el más difícil, se completaron 1000 ejecuciones de experimentos limitadas a 100 pasos cada una, un total de 42411 pasos, en poco menos de 6 horas. Es decir, más de 4 × 104 pasos en línea tomaron una orden de magnitud menos tiempo que la computación sin conexión de la política POMDP en [11]. La importancia de esta diferencia se destaca aún más por el hecho de que, en caso de que cambien los parámetros del modelo del entorno, la naturaleza en línea de EMT le permitiría mantener su rendimiento, mientras que la política POMDP necesitaría ser recalculada, lo que requeriría una vez más un gran costo computacional adicional. También probamos la entropía de la frecuencia de las células de comportamiento, medidas empíricas de los datos de prueba. Como muestran la Figura 4 y la Figura 5, empir794 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 3: Se utilizaron estas configuraciones del espacio de juego de Tag: a) múltiples callejones sin salida, b) arena abierta irregular, c) pasillo circular. Tabla 2: Rendimiento de la solución basada en EMT en tres dominios de juego de Tag y dos modelos de observabilidad: I) cantera de omniposición, II) la cantera no está en la posición de los cazadores. Modelo Dominio Captura% E(Pasos) Tiempo/Paso I Callejones sin salida 100 14.8 72(mSeg) Arena 80.2 42.4 500(mSeg) Círculo 91.4 34.6 187(mSeg) II Callejones sin salida 100 13.2 91(mSeg) Arena 96.8 28.67 396(mSeg) Círculo 94.4 31.63 204(mSeg) La entropía lógica crece con la duración de la interacción. Para las carreras en las que la presa no fue capturada de inmediato, la entropía alcanza valores entre 0.85 y 0.952 para diferentes carreras y escenarios. A medida que el agente busca activamente la presa, la entropía nunca alcanza su máximo. Una característica del gráfico de entropía para el escenario de arena abierta llamó especialmente nuestra atención en el caso del modelo de observación de cantera omniposicional. Cerca del límite máximo de longitud de prueba (100 pasos), la entropía cayó repentinamente. Un análisis adicional de los datos mostró que bajo ciertas circunstancias, se produce un comportamiento fluctuante en el que el agente se enfrenta a versiones igualmente viables del comportamiento de seguir a la presa. Dado que el algoritmo EMT tiene una selección de acciones codiciosa, y el espacio de estados no codifica ninguna forma de compromiso (ni siquiera velocidad o aceleración), el agente queda atrapado dentro de una pequeña porción de celdas. Básicamente, se intenta seguir simultáneamente varios cursos de acción, todos los cuales son consistentes con la dinámica objetivo. Este comportamiento no ocurrió en nuestro segundo modelo de observación, ya que redujo significativamente el conjunto de cursos de acción elegibles, contribuyendo esencialmente a romper el empate entre ellos. 6. DISCUSIÓN El diseño de la solución EMT para el Juego de Etiquetas expone la diferencia fundamental en el enfoque de planificación y control entre EMT o DBC, por un lado, y el enfoque más familiar de POMDP, por otro lado. POMDP define una estructura de recompensa para optimizar e influencia la dinámica del sistema de forma indirecta a través de esa optimización. EMT descarta cualquier esquema de recompensa y, en cambio, mide e influye directamente en la dinámica del sistema. Se calculó la entropía 2 utilizando el logaritmo con base igual al número de ubicaciones posibles dentro del dominio; esto escala adecuadamente la expresión de entropía en el rango [0, 1] para todos los dominios. Por lo tanto, para el Juego de Etiquetas, no buscamos una función de recompensa que codificara y expresara nuestra preferencia sobre el comportamiento de los agentes, sino que establecimos directamente tres preferencias de comportamiento (heurísticas) como base para mantener la dinámica del objetivo. Los datos experimentales muestran que estos objetivos no necesitan ser directamente alcanzables a través de las acciones de los agentes. Sin embargo, aún queda por explorar la relación entre el rendimiento de la EMT y la alcanzabilidad de la dinámica objetivo. Los datos del experimento del juego de etiquetas también revelaron la diferente importancia que DBC y POMDPs otorgan a la formulación del espacio de estados del entorno. Los POMDPs dependen enteramente del mecanismo de maximización de la acumulación de recompensas, es decir, la formación del procedimiento de selección de acciones para lograr la secuenciación de estados necesaria. DBC, por otro lado, tiene dos fuentes de especificación de secuenciación: a través de las propiedades de un procedimiento de selección de acciones y a través de una especificación directa dentro de la dinámica objetivo. La importancia de la segunda fuente fue subrayada por los datos del experimento del Juego de Etiquetas, en el que el algoritmo EMT codicioso, aplicado a una especificación de espacio de estados tipo POMDP, falló, ya que la descripción del objetivo sobre dicho espacio de estados no era capaz de codificar las tendencias de comportamiento necesarias, por ejemplo, la resolución de empates y el compromiso con el movimiento dirigido. Las diferencias estructurales entre DBC (y EMT en particular) y POMDPs impiden la comparación directa de rendimiento y los sitúan en pistas complementarias, cada uno dentro de un nicho adecuado. Por ejemplo, los POMDP podrían ser vistos como una formulación mucho más natural de problemas económicos de toma de decisiones secuenciales, mientras que EMT se ajusta mejor a la demanda continua de cambios estocásticos, como ocurre en muchos problemas de robótica o de agentes incorporados. Las propiedades complementarias de POMDPs y EMT pueden ser aún más explotadas. Existe un interés reciente en utilizar POMDPs en soluciones híbridas [17], en las cuales los POMDPs pueden ser utilizados junto con otros enfoques de control para proporcionar resultados que no son fácilmente alcanzables con ninguno de los enfoques por sí solos. DBC puede ser un socio efectivo en una solución híbrida como esta. Por ejemplo, los POMDP tienen requisitos de tiempo fuera de línea prohibitivamente grandes para el cálculo de políticas, pero pueden ser fácilmente utilizados en entornos más simples para exponer tendencias de comportamiento beneficiosas; esto puede servir como una forma de dinámica objetivo que se proporciona a EMT en un dominio más grande para su operación en línea. 7. CONCLUSIONES Y TRABAJO FUTURO En este artículo, hemos presentado una nueva perspectiva sobre el proceso de planificación y control en entornos estocásticos, en forma del marco de Control Basado en Dinámicas (DBC). DBC formula la tarea de planificar como el apoyo a la dinámica de un sistema objetivo especificado, que describe las propiedades necesarias del cambio dentro del entorno. La optimalidad de los planes de acción de DBC se mide en el Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Calles sin salida 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Círculo Figura 4: Modelo de Observación I: Cantera omniposicional. Desarrollo de entropía con longitud de juego de etiqueta para los tres escenarios experimentales: a) múltiples callejones sin salida, b) arena abierta irregular, c) pasillo circular. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Callejones sin salida 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Círculo Figura 5: Modelo de observación II: cantera no observada en la posición de los cazadores. Desarrollo de la entropía con la longitud del juego de etiquetas para los tres escenarios experimentales: a) múltiples callejones sin salida, b) arena abierta irregular, c) pasillo circular, con respecto a la desviación de la dinámica del sistema real de la dinámica objetivo. Mostramos que una técnica recientemente desarrollada de Seguimiento Markov Extendido (EMT) [13] es una instancia de DBC. De hecho, EMT puede ser visto como un caso específico de la parametrización DBC, que emplea un procedimiento de selección de acciones codicioso. Dado que EMT exhibe las características clave del marco general DBC, así como una complejidad temporal polinómica, utilizamos la versión multitarget de EMT [15] para demostrar que la clase de problemas de barrido de área se presta naturalmente a descripciones basadas en dinámicas, tal como se ejemplifica en nuestros experimentos en el dominio del Juego de Etiquetas. Como se enumera en la Sección 4.2, EMT tiene varias limitaciones, como la dificultad para manejar la preferencia dinámica negativa. Esto evita la aplicación directa de la EMT a problemas como los Juegos de Rendezvous-Evasión (por ejemplo, [6]). Sin embargo, DBC en general no tiene tales limitaciones y permite fácilmente la formulación de juegos de evasión. En trabajos futuros, tenemos la intención de continuar con el desarrollo de controladores basados en dinámica para estos problemas. 8. AGRADECIMIENTO El trabajo de los dos primeros autores fue parcialmente apoyado por la subvención #898/05 de la Fundación para la Ciencia de Israel, y el tercer autor fue parcialmente apoyado por una subvención del Ministerio de Ciencia y Tecnología de Israel. REFERENCIAS [1] R. C. Arkin. Robótica basada en el comportamiento. MIT Press, 1998. [2] J. \n\nMIT Press, 1998. [2] J. A. Bilmes. Un tutorial detallado del algoritmo EM y su aplicación en la estimación de parámetros para mezclas gaussianas y modelos ocultos de Markov. Informe técnico TR-97-021, Departamento de Ingeniería Eléctrica e Informática, Universidad de California en Berkeley, 1998. [3] T. M. Cover y J. A. Thomas. Elementos de teoría de la información. Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz y M. J. Wolverton. Una encuesta de investigación en planificación distribuida y continua. Revista de Inteligencia Artificial, 4:13-22, 1999. [5] V. R. Konda y J. N. Tsitsiklis. Algoritmos Actor-Crítico. Revista SIAM de Control y Optimización, 42(4):1143-1166, 2003. [6] W. S. Lim. Un juego de encuentro-evasión en ubicaciones discretas con aleatorización conjunta. Avances en Probabilidad Aplicada, 29(4):1004-1017, diciembre de 1997. [7] M. L. Littman, T. L. Dean y L. P. Kaelbling. Sobre la complejidad de resolver problemas de decisión de Markov. En Actas de la 11ª Conferencia Anual sobre Incertidumbre en Inteligencia Artificial (UAI-95), páginas 394-402, 1995. [8] O. Madani, S. Hanks y A. Condon. Sobre la indecidibilidad de la planificación probabilística y problemas relacionados de optimización estocástica. Revista de Inteligencia Artificial, 147(1-2):5-34, julio de 2003. [9] R. M. Neal y G. E. Hinton. Una vista del algoritmo EM 796 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) que justifica enfoques incrementales, dispersos y otras variantes. En M. I. Jordan, editor, Aprendizaje en Modelos Gráficos, páginas 355-368. Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez y S. Kraus. Seguridad en sistemas multiagentes mediante la aleatorización de políticas. En Actas de AAMAS 2006, 2006. [11] J. Pineau, G. Gordon y S. Thrun. Iteración de valor basada en puntos: Un algoritmo en cualquier momento para POMDPs. En la Conferencia Conjunta Internacional de Inteligencia Artificial (IJCAI), páginas 1025-1032, agosto de 2003. [12] M. L. Puterman. Procesos de Decisión de Markov. Serie Wiley en Probabilidad y Estadística Matemática: Sección de Probabilidad y Estadística Aplicada. Publicación de Wiley-Interscience, Nueva York, 1994. [13] Z. Rabinovich y J. S. Rosenschein. Seguimiento Markov extendido con una aplicación al control. En el taller sobre seguimiento de agentes: modelando otros agentes a partir de observaciones, en la Tercera Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, páginas 95-100, Nueva York, julio de 2004. [14] Z. Rabinovich y J. S. Rosenschein. Coordinación multiagente mediante Seguimiento Markov Extendido. En la Cuarta Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, páginas 431-438, Utrecht, Países Bajos, julio de 2005. [15] Z. Rabinovich y J. S. Rosenschein. Sobre la respuesta del control basado en EMT a objetivos y modelos interactivos. En la Quinta Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, páginas 465-470, Hakodate, Japón, mayo de 2006. [16] R. F. Stengel. Control óptimo y estimación. Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, y P. Varakantham. Conflictos en el trabajo en equipo: Híbridos para el Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 797 ",
            "candidates": [],
            "error": [
                [
                    "juego de la mancha",
                    "juego de la \"carrera de relevos\"",
                    "juego de etiquetas",
                    "juego de la mancha"
                ]
            ]
        },
        "tag game": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Dynamics Based Control with an Application to Area-Sweeping Problems Zinovi Rabinovich Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel jeff@cs.huji.ac.il Gal A. Kaminka The MAVERICK Group Department of Computer Science Bar Ilan University, Israel galk@cs.biu.ac.il ABSTRACT In this paper we introduce Dynamics Based Control (DBC), an approach to planning and control of an agent in stochastic environments.",
                "Unlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics.",
                "We show that a recently developed planning and control approach, Extended Markov Tracking (EMT) is an instantiation of DBC.",
                "EMT employs greedy action selection to provide an efficient control algorithm in Markovian environments.",
                "We exploit this efficiency in a set of experiments that applied multitarget EMT to a class of area-sweeping problems (searching for moving targets).",
                "We show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation.",
                "Categories and Subject Descriptors I.2.8 [Problem Solving, Control Methods, and Search]: Control Theory; I.2.9 [Robotics]; I.2.11 [Distributed Artificial Intelligence]: Intelligent Agents General Terms Algorithms, Theory 1.",
                "INTRODUCTION Planning and control constitutes a central research area in multiagent systems and artificial intelligence.",
                "In recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in stochastic environments.",
                "In this framework, the planning and control problem is often addressed by imposing a reward function, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility.",
                "While theoretically attractive, the complexity of optimally solving a POMDP is prohibitive [8, 7].",
                "We take an alternative view of planning in stochastic environments.",
                "We do not use a (state-based) reward function, but instead optimize over a different criterion, a transition-based specification of the desired system dynamics.",
                "The idea here is to view planexecution as a process that compels a (stochastic) system to change, and a plan as a dynamic process that shapes that change according to desired criteria.",
                "We call this general planning framework Dynamics Based Control (DBC).",
                "In DBC, the goal of a planning (or control) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) target dynamics.",
                "As actual system behavior may deviate from that which is specified by target dynamics (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16].",
                "Here, optimality is measured in terms of probability of deviation magnitudes.",
                "In this paper, we present the structure of Dynamics Based Control.",
                "We show that the recently developed Extended Markov Tracking (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC.",
                "EMT is an efficient instantiation of DBC.",
                "To evaluate DBC, we carried out a set of experiments applying multi-target EMT to the <br>tag game</br> [11]; this is a variant on the area sweeping problem, where an agent is trying to tag a moving target (quarry) whose position is not known with certainty.",
                "Experimental data demonstrates that even with a simple model of the environment and a simple design of target dynamics, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agents position).",
                "The paper is organized as follows.",
                "In Section 2 we motivate DBC using area-sweeping problems, and discuss related work.",
                "Section 3 introduces the Dynamics Based Control (DBC) structure, and its specialization to Markovian environments.",
                "This is followed by a review of the Extended Markov Tracking (EMT) approach as a DBC-structured control regimen in Section 4.",
                "That section also discusses the limitations of EMT-based control relative to the general DBC framework.",
                "Experimental settings and results are then presented in Section 5.",
                "Section 6 provides a short discussion of the overall approach, and Section 7 gives some concluding remarks and directions for future work. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "MOTIVATION AND RELATED WORK Many real-life scenarios naturally have a stochastic target dynamics specification, especially those domains where there exists no ultimate goal, but rather system behavior (with specific properties) that has to be continually supported.",
                "For example, security guards perform persistent sweeps of an area to detect any sign of intrusion.",
                "Cunning thieves will attempt to track these sweeps, and time their operation to key points of the guards motion.",
                "It is thus advisable to make the guards motion dynamics appear irregular and random.",
                "Recent work by Paruchuri et al. [10] has addressed such randomization in the context of single-agent and distributed POMDPs.",
                "The goal in that work was to generate policies that provide a measure of action-selection randomization, while maintaining rewards within some acceptable levels.",
                "Our focus differs from this work in that DBC does not optimize expected rewards-indeed we do not consider rewards at all-but instead maintains desired dynamics (including, but not limited to, randomization).",
                "The Game of Tag is another example of the applicability of the approach.",
                "It was introduced in the work by Pineau et al. [11].",
                "There are two agents that can move about an area, which is divided into a grid.",
                "The grid may have blocked cells (holes) into which no agent can move.",
                "One agent (the hunter) seeks to move into a cell occupied by the other (the quarry), such that they are co-located (this is a successful tag).",
                "The quarry seeks to avoid the hunter agent, and is always aware of the hunters position, but does not know how the hunter will behave, which opens up the possibility for a hunter to surprise the prey.",
                "The hunter knows the quarrys probabilistic law of motion, but does not know its current location.",
                "Tag is an instance of a family of area-sweeping (pursuit-evasion) problems.",
                "In [11], the hunter modeled the problem using a POMDP.",
                "A reward function was defined, to reflect the desire to tag the quarry, and an action policy was computed to optimize the reward collected over time.",
                "Due to the intractable complexity of determining the optimal policy, the action policy computed in that paper was essentially an approximation.",
                "In this paper, instead of formulating a reward function, we use EMT to solve the problem, by directly specifying the target dynamics.",
                "In fact, any search problem with randomized motion, the socalled class of area sweeping problems, can be described through specification of such target system dynamics.",
                "Dynamics Based Control provides a natural approach to solving these problems. 3.",
                "DYNAMICS BASED CONTROL The specification of Dynamics Based Control (DBC) can be broken into three interacting levels: Environment Design Level, User Level, and Agent Level. • Environment Design Level is concerned with the formal specification and modeling of the environment.",
                "For example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation constant. • User Level in turn relies on the environment model produced by Environment Design to specify the target system dynamics it wishes to observe.",
                "The User Level also specifies the estimation or learning procedure for system dynamics, and the measure of deviation.",
                "In the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping. • Agent Level in turn combines the environment model from the Environment Design level, the dynamics estimation procedure, the deviation measure and the target dynamics specification from User Level, to produce a sequence of actions that create system dynamics as close as possible to the targeted specification.",
                "As we are interested in the continual development of a stochastic system, such as happens in classical control theory [16] and continual planning [4], as well as in our example of museum sweeps, the question becomes how the Agent Level is to treat the deviation measurements over time.",
                "To this end, we use a probability threshold-that is, we would like the Agent Level to maximize the probability that the deviation measure will remain below a certain threshold.",
                "Specific action selection then depends on system formalization.",
                "One possibility would be to create a mixture of available system trends, much like that which happens in Behavior-Based Robotic architectures [1].",
                "The other alternative would be to rely on the estimation procedure provided by the User Level-to utilize the Environment Design Level model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a certain dynamics has been achieved.",
                "Notice that this manipulation is not direct, but via the environment.",
                "Thus, for strong enough estimator algorithms, successful manipulation would mean a successful simulation of the specified target dynamics (i.e., beyond discerning via the available sensory input).",
                "DBC levels can also have a back-flow of information (see Figure 1).",
                "For instance, the Agent Level could provide data about target dynamics feasibility, allowing the User Level to modify the requirement, perhaps focusing on attainable features of system behavior.",
                "Data would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the User Level, this can provide an important tool for the environment model calibration at the Environment Design Level.",
                "UserEnv.",
                "Design Agent Model Ideal Dynamics Estimator Estimator Dynamics Feasibility System Response Data Figure 1: Data flow of the DBC framework Extending upon the idea of Actor-Critic algorithms [5], DBC data flow can provide a good basis for the design of a learning algorithm.",
                "For example, the User Level can operate as an exploratory device for a learning algorithm, inferring an ideal dynamics target from the environment model at hand that would expose and verify most critical features of system behavior.",
                "In this case, feasibility and system response data from the Agent Level would provide key information for an environment model update.",
                "In fact, the combination of feasibility and response data can provide a basis for the application of strong learning algorithms such as EM [2, 9]. 3.1 DBC for Markovian Environments For a Partially Observable Markovian Environment, DBC can be specified in a more rigorous manner.",
                "Notice how DBC discards rewards, and replaces it by another optimality criterion (structural differences are summarized in Table 1): • Environment Design level is to specify a tuple < S, A, T, O, Ω, s0 >, where: - S is the set of all possible environment states; - s0 is the initial state of the environment (which can also be viewed as a probability distribution over S); The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 791 - A is the set of all possible actions applicable in the environment; - T is the environments probabilistic transition function: T : S ×A → Π(S).",
                "That is, T(s |a, s) is the probability that the environment will move from state s to state s under action a; - O is the set of all possible observations.",
                "This is what the sensor input would look like for an outside observer; - Ω is the observation probability function: Ω : S × A × S → Π(O).",
                "That is, Ω(o|s , a, s) is the probability that one will observe o given that the environment has moved from state s to state s under action a. • User Level, in the case of Markovian environment, operates on the set of system dynamics described by a family of conditional probabilities F = {τ : S × A → Π(S)}.",
                "Thus specification of target dynamics can be expressed by q ∈ F, and the learning or tracking algorithm can be represented as a function L : O×(A×O)∗ → F; that is, it maps sequences of observations and actions performed so far into an estimate τ ∈ F of system dynamics.",
                "There are many possible variations available at the User Level to define divergence between system dynamics; several of them are: - Trace distance or L1 distance between two distributions p and q defined by D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Fidelity measure of distance F(p(·), q(·)) = x p(x)q(x) - Kullback-Leibler divergence DKL(p(·) q(·)) = x p(x) log p(x) q(x) Notice that the latter two are not actually metrics over the space of possible distributions, but nevertheless have meaningful and important interpretations.",
                "For instance, KullbackLeibler divergence is an important tool of information theory [3] that allows one to measure the price of encoding an information source governed by q, while assuming that it is governed by p. The User Level also defines the threshold of dynamics deviation probability θ. • Agent Level is then faced with a problem of selecting a control signal function a∗ to satisfy a minimization problem as follows: a∗ = arg min a Pr(d(τa, q) > θ) where d(τa, q) is a random variable describing deviation of the dynamics estimate τa, created by L under control signal a, from the ideal dynamics q.",
                "Implicit in this minimization problem is that L is manipulated via the environment, based on the environment model produced by the Environment Design Level. 3.2 DBC View of the State Space It is important to note the complementary view that DBC and POMDPs take on the state space of the environment.",
                "POMDPs regard state as a stationary snap-shot of the environment; whatever attributes of state sequencing one seeks are reached through properties of the control process, in this case reward accumulation.",
                "This can be viewed as if the sequencing of states and the attributes of that sequencing are only introduced by and for the controlling mechanism-the POMDP policy.",
                "DBC concentrates on the underlying principle of state sequencing, the system dynamics.",
                "DBCs target dynamics specification can use the environments state space as a means to describe, discern, and preserve changes that occur within the system.",
                "As a result, DBC has a greater ability to express state sequencing properties, which are grounded in the environment model and its state space definition.",
                "For example, consider the task of moving through rough terrain towards a goal and reaching it as fast as possible.",
                "POMDPs would encode terrain as state space points, while speed would be ensured by negative reward for every step taken without reaching the goalaccumulating higher reward can be reached only by faster motion.",
                "Alternatively, the state space could directly include the notion of speed.",
                "For POMDPs, this would mean that the same concept is encoded twice, in some sense: directly in the state space, and indirectly within reward accumulation.",
                "Now, even if the reward function would encode more, and finer, details of the properties of motion, the POMDP solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure.",
                "On the other hand, the tactical target expression of variations and correlations between position and speed of motion is now grounded in the state space representation.",
                "In this situation, any further constraints, e.g., smoothness of motion, speed limits in different locations, or speed reductions during sharp turns, are explicitly and uniformly expressed by the tactical target, and can result in faster and more effective action selection by a DBC algorithm. 4.",
                "EMT-BASED CONTROL AS A DBC Recently, a control algorithm was introduced called EMT-based Control [13], which instantiates the DBC framework.",
                "Although it provides an approximate greedy solution in the DBC sense, initial experiments using EMT-based control have been encouraging [14, 15].",
                "EMT-based control is based on the Markovian environment definition, as in the case with POMDPs, but its User and Agent Levels are of the Markovian DBC type of optimality. • User Level of EMT-based control defines a limited-case target system dynamics independent of action: qEMT : S → Π(S).",
                "It then utilizes the Kullback-Leibler divergence measure to compose a momentary system dynamics estimator-the Extended Markov Tracking (EMT) algorithm.",
                "The algorithm keeps a system dynamics estimate τt EMT that is capable of explaining recent change in an auxiliary Bayesian system state estimator from pt−1 to pt, and updates it conservatively using Kullback-Leibler divergence.",
                "Since τt EMT and pt−1,t are respectively the conditional and marginal probabilities over the systems state space, explanation simply means that pt(s ) = s τt EMT (s |s)pt−1(s), and the dynamics estimate update is performed by solving a 792 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: Structure of POMDP vs. Dynamics-Based Control in Markovian Environment Level Approach MDP Markovian DBC Environment < S, A, T, O, Ω >,where S - set of states A - set of actions Design T : S × A → Π(S) - transition O - observation set Ω : S × A × S → Π(O) User r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - reward function q - ideal dynamics F - reward remodeling L - dynamics estimator θ - threshold Agent π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) minimization problem: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • Agent Level in EMT-based control is suboptimal with respect to DBC (though it remains within the DBC framework), performing greedy action selection based on prediction of EMTs reaction.",
                "The prediction is based on the environment model provided by the Environment Design level, so that if we denote by Ta the environments transition function limited to action a, and pt−1 is the auxiliary Bayesian system state estimator, then the EMT-based control choice is described by a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1) Note that this follows the Markovian DBC framework precisely: the rewarding optimality of POMDPs is discarded, and in its place a dynamics estimator (EMT in this case) is manipulated via action effects on the environment to produce an estimate close to the specified target system dynamics.",
                "Yet as we mentioned, naive EMTbased control is suboptimal in the DBC sense, and has several additional limitations that do not exist in the general DBC framework (discussed in Section 4.2). 4.1 Multi-Target EMT At times, there may exist several behavioral preferences.",
                "For example, in the case of museum guards, some art items are more heavily guarded, requiring that the guards stay more often in their close vicinity.",
                "On the other hand, no corner of the museum is to be left unchecked, which demands constant motion.",
                "Successful museum security would demand that the guards adhere to, and balance, both of these behaviors.",
                "For EMT-based control, this would mean facing several tactical targets {qk}K k=1, and the question becomes how to merge and balance them.",
                "A balancing mechanism can be applied to resolve this issue.",
                "Note that EMT-based control, while selecting an action, creates a preference vector over the set of actions based on their predicted performance with respect to a given target.",
                "If these preference vectors are normalized, they can be combined into a single unified preference.",
                "This requires replacement of standard EMT-based action selection by the algorithm below [15]: • Given: - a set of target dynamics {qk}K k=1, - vector of weights w(k) • Select action as follows - For each action a ∈ A predict the future state distribution ¯pa t+1 = Ta ∗ pt; - For each action, compute Da = H(¯pa t+1, pt, PDt) - For each a ∈ A and qk tactical target, denote V (a, k) = DKL (Da qk) pt .",
                "Let Vk(a) = 1 Zk V (a, k), where Zk = a∈A V (a, k) is a normalization factor. - Select a∗ = arg min a k k=1 w(k)Vk(a) The weights vector w = (w1, ..., wK ) allows the additional tuning of importance among target dynamics without the need to redesign the targets themselves.",
                "This balancing method is also seamlessly integrated into the EMT-based control flow of operation. 4.2 EMT-based Control Limitations EMT-based control is a sub-optimal (in the DBC sense) representative of the DBC structure.",
                "It limits the User by forcing EMT to be its dynamic tracking algorithm, and replaces Agent optimization by greedy action selection.",
                "This kind of combination, however, is common for on-line algorithms.",
                "Although further development of EMT-based controllers is necessary, evidence so far suggests that even the simplest form of the algorithm possesses a great deal of power, and displays trends that are optimal in the DBC sense of the word.",
                "There are two further, EMT-specific, limitations to EMT-based control that are evident at this point.",
                "Both already have partial solutions and are subjects of ongoing research.",
                "The first limitation is the problem of negative preference.",
                "In the POMDP framework for example, this is captured simply, through The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 793 the appearance of values with different signs within the reward structure.",
                "For EMT-based control, however, negative preference means that one would like to avoid a certain distribution over system development sequences; EMT-based control, however, concentrates on getting as close as possible to a distribution.",
                "Avoidance is thus unnatural in native EMT-based control.",
                "The second limitation comes from the fact that standard environment modeling can create pure sensory actions-actions that do not change the state of the world, and differ only in the way observations are received and the quality of observations received.",
                "Since the world state does not change, EMT-based control would not be able to differentiate between different sensory actions.",
                "Notice that both of these limitations of EMT-based control are absent from the general DBC framework, since it may have a tracking algorithm capable of considering pure sensory actions and, unlike Kullback-Leibler divergence, a distribution deviation measure that is capable of dealing with negative preference. 5.",
                "EMT PLAYING TAG The Game of Tag was first introduced in [11].",
                "It is a single agent problem of capturing a quarry, and belongs to the class of area sweeping problems.",
                "An example domain is shown in Figure 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figure 2: Tag domain; an agent (A) attempts to seek and capture a quarry (Q) The Game of Tag extremely limits the agents perception, so that the agent is able to detect the quarry only if they are co-located in the same cell of the grid world.",
                "In the classical version of the game, co-location leads to a special observation, and the Tag action can be performed.",
                "We slightly modify this setting: the moment that both agents occupy the same cell, the game ends.",
                "As a result, both the agent and its quarry have the same motion capability, which allows them to move in four directions, North, South, East, and West.",
                "These form a formal space of actions within a Markovian environment.",
                "The state space of the formal Markovian environment is described by the cross-product of the agent and quarrys positions.",
                "For Figure 2, it would be S = {s0, ..., s23} × {s0, ..., s23}.",
                "The effects of an action taken by the agent are deterministic, but the environment in general has a stochastic response due to the motion of the quarry.",
                "With probability q0 1 it stays put, and with probability 1 − q0 it moves to an adjacent cell further away from the 1 In our experiments this was taken to be q0 = 0.2. agent.",
                "So for the instance shown in Figure 2 and q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Although the evasive behavior of the quarry is known to the agent, the quarrys position is not.",
                "The only sensory information available to the agent is its own location.",
                "We use EMT and directly specify the target dynamics.",
                "For the Game of Tag, we can easily formulate three major trends: catching the quarry, staying mobile, and stalking the quarry.",
                "This results in the following three target dynamics: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 otherwise Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 otherwise Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Note that none of the above targets are directly achievable; for instance, if Qt = s9 and At = s11, there is no action that can move the agent to At+1 = s9 as required by the Tcatch target dynamics.",
                "We ran several experiments to evaluate EMT performance in the <br>tag game</br>.",
                "Three configurations of the domain shown in Figure 3 were used, each posing a different challenge to the agent due to partial observability.",
                "In each setting, a set of 1000 runs was performed with a time limit of 100 steps.",
                "In every run, the initial position of both the agent and its quarry was selected at random; this means that as far as the agent was concerned, the quarrys initial position was uniformly distributed over the entire domain cell space.",
                "We also used two variations of the environment observability function.",
                "In the first version, observability function mapped all joint positions of hunter and quarry into the position of the hunter as an observation.",
                "In the second, only those joint positions in which hunter and quarry occupied different locations were mapped into the hunters location.",
                "The second version in fact utilized and expressed the fact that once hunter and quarry occupy the same cell the game ends.",
                "The results of these experiments are shown in Table 2.",
                "Balancing [15] the catch, move, and stalk target dynamics described in the previous section by the weight vector [0.8, 0.1, 0.1], EMT produced stable performance in all three domains.",
                "Although direct comparisons are difficult to make, the EMT performance displayed notable efficiency vis-`a-vis the POMDP approach.",
                "In spite of a simple and inefficient Matlab implementation of the EMT algorithm, the decision time for any given step averaged significantly below 1 second in all experiments.",
                "For the irregular open arena domain, which proved to be the most difficult, 1000 experiment runs bounded by 100 steps each, a total of 42411 steps, were completed in slightly under 6 hours.",
                "That is, over 4 × 104 online steps took an order of magnitude less time than the offline computation of POMDP policy in [11].",
                "The significance of this differential is made even more prominent by the fact that, should the environment model parameters change, the online nature of EMT would allow it to maintain its performance time, while the POMDP policy would need to be recomputed, requiring yet again a large overhead of computation.",
                "We also tested the behavior cell frequency entropy, empirical measures from trial data.",
                "As Figure 4 and Figure 5 show, empir794 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) A Q Q A 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 A Q Figure 3: These configurations of the <br>tag game</br> space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor Table 2: Performance of the EMT-based solution in three <br>tag game</br> domains and two observability models: I) omniposition quarry, II) quarry is not at hunters position Model Domain Capture% E(Steps) Time/Step I Dead-ends 100 14.8 72(mSec) Arena 80.2 42.4 500(mSec) Circle 91.4 34.6 187(mSec) II Dead-ends 100 13.2 91(mSec) Arena 96.8 28.67 396(mSec) Circle 94.4 31.63 204(mSec) ical entropy grows with the length of interaction.",
                "For runs where the quarry was not captured immediately, the entropy reaches between 0.85 and 0.952 for different runs and scenarios.",
                "As the agent actively seeks the quarry, the entropy never reaches its maximum.",
                "One characteristic of the entropy graph for the open arena scenario particularly caught our attention in the case of the omniposition quarry observation model.",
                "Near the maximum limit of trial length (100 steps), entropy suddenly dropped.",
                "Further analysis of the data showed that under certain circumstances, a fluctuating behavior occurs in which the agent faces equally viable versions of quarry-following behavior.",
                "Since the EMT algorithm has greedy action selection, and the state space does not encode any form of commitment (not even speed or acceleration), the agent is locked within a small portion of cells.",
                "It is essentially attempting to simultaneously follow several courses of action, all of which are consistent with the target dynamics.",
                "This behavior did not occur in our second observation model, since it significantly reduced the set of eligible courses of action-essentially contributing to tie-breaking among them. 6.",
                "DISCUSSION The design of the EMT solution for the <br>tag game</br> exposes the core difference in approach to planning and control between EMT or DBC, on the one hand, and the more familiar POMDP approach, on the other.",
                "POMDP defines a reward structure to optimize, and influences system dynamics indirectly through that optimization.",
                "EMT discards any reward scheme, and instead measures and influences system dynamics directly. 2 Entropy was calculated using log base equal to the number of possible locations within the domain; this properly scales entropy expression into the range [0, 1] for all domains.",
                "Thus for the <br>tag game</br>, we did not search for a reward function that would encode and express our preference over the agents behavior, but rather directly set three (heuristic) behavior preferences as the basis for target dynamics to be maintained.",
                "Experimental data shows that these targets need not be directly achievable via the agents actions.",
                "However, the ratio between EMT performance and achievability of target dynamics remains to be explored.",
                "The <br>tag game</br> experiment data also revealed the different emphasis DBC and POMDPs place on the formulation of an environment state space.",
                "POMDPs rely entirely on the mechanism of reward accumulation maximization, i.e., formation of the action selection procedure to achieve necessary state sequencing.",
                "DBC, on the other hand, has two sources of sequencing specification: through the properties of an action selection procedure, and through direct specification within the target dynamics.",
                "The importance of the second source was underlined by the <br>tag game</br> experiment data, in which the greedy EMT algorithm, applied to a POMDP-type state space specification, failed, since target description over such a state space was incapable of encoding the necessary behavior tendencies, e.g., tie-breaking and commitment to directed motion.",
                "The structural differences between DBC (and EMT in particular), and POMDPs, prohibits direct performance comparison, and places them on complementary tracks, each within a suitable niche.",
                "For instance, POMDPs could be seen as a much more natural formulation of economic sequential decision-making problems, while EMT is a better fit for continual demand for stochastic change, as happens in many robotic or embodied-agent problems.",
                "The complementary properties of POMDPs and EMT can be further exploited.",
                "There is recent interest in using POMDPs in hybrid solutions [17], in which the POMDPs can be used together with other control approaches to provide results not easily achievable with either approach by itself.",
                "DBC can be an effective partner in such a hybrid solution.",
                "For instance, POMDPs have prohibitively large off-line time requirements for policy computation, but can be readily used in simpler settings to expose beneficial behavioral trends; this can serve as a form of target dynamics that are provided to EMT in a larger domain for on-line operation. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have presented a novel perspective on the process of planning and control in stochastic environments, in the form of the Dynamics Based Control (DBC) framework.",
                "DBC formulates the task of planning as support of a specified target system dynamics, which describes the necessary properties of change within the environment.",
                "Optimality of DBC plans of action are measured The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 4: Observation Model I: Omniposition quarry.",
                "Entropy development with length of <br>tag game</br> for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 5: Observation Model II: quarry not observed at hunters position.",
                "Entropy development with length of <br>tag game</br> for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. with respect to the deviation of actual system dynamics from the target dynamics.",
                "We show that a recently developed technique of Extended Markov Tracking (EMT) [13] is an instantiation of DBC.",
                "In fact, EMT can be seen as a specific case of DBC parameterization, which employs a greedy action selection procedure.",
                "Since EMT exhibits the key features of the general DBC framework, as well as polynomial time complexity, we used the multitarget version of EMT [15] to demonstrate that the class of area sweeping problems naturally lends itself to dynamics-based descriptions, as instantiated by our experiments in the <br>tag game</br> domain.",
                "As enumerated in Section 4.2, EMT has a number of limitations, such as difficulty in dealing with negative dynamic preference.",
                "This prevents direct application of EMT to such problems as Rendezvous-Evasion Games (e.g., [6]).",
                "However, DBC in general has no such limitations, and readily enables the formulation of evasion games.",
                "In future work, we intend to proceed with the development of dynamics-based controllers for these problems. 8.",
                "ACKNOWLEDGMENT The work of the first two authors was partially supported by Israel Science Foundation grant #898/05, and the third author was partially supported by a grant from Israels Ministry of Science and Technology. 9.",
                "REFERENCES [1] R. C. Arkin.",
                "Behavior-Based Robotics.",
                "MIT Press, 1998. [2] J.",
                "A. Bilmes.",
                "A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and Hidden Markov Models.",
                "Technical Report TR-97-021, Department of Electrical Engeineering and Computer Science, University of California at Berkeley, 1998. [3] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of information theory.",
                "Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz, and M. J. Wolverton.",
                "A survey of research in distributed, continual planning.",
                "AI Magazine, 4:13-22, 1999. [5] V. R. Konda and J. N. Tsitsiklis.",
                "Actor-Critic algorithms.",
                "SIAM Journal on Control and Optimization, 42(4):1143-1166, 2003. [6] W. S. Lim.",
                "A rendezvous-evasion game on discrete locations with joint randomization.",
                "Advances in Applied Probability, 29(4):1004-1017, December 1997. [7] M. L. Littman, T. L. Dean, and L. P. Kaelbling.",
                "On the complexity of solving Markov decision problems.",
                "In Proceedings of the 11th Annual Conference on Uncertainty in Artificial Intelligence (UAI-95), pages 394-402, 1995. [8] O. Madani, S. Hanks, and A. Condon.",
                "On the undecidability of probabilistic planning and related stochastic optimization problems.",
                "Artificial Intelligence Journal, 147(1-2):5-34, July 2003. [9] R. M. Neal and G. E. Hinton.",
                "A view of the EM algorithm 796 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) that justifies incremental, sparse, and other variants.",
                "In M. I. Jordan, editor, Learning in Graphical Models, pages 355-368.",
                "Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In Proceeding of AAMAS 2006, 2006. [11] J. Pineau, G. Gordon, and S. Thrun.",
                "Point-based value iteration: An anytime algorithm for pomdps.",
                "In International Joint Conference on Artificial Intelligence (IJCAI), pages 1025-1032, August 2003. [12] M. L. Puterman.",
                "Markov Decision Processes.",
                "Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics Section.",
                "Wiley-Interscience Publication, New York, 1994. [13] Z. Rabinovich and J. S. Rosenschein.",
                "Extended Markov Tracking with an application to control.",
                "In The Workshop on Agent Tracking: Modeling Other Agents from Observations, at the Third International Joint Conference on Autonomous Agents and Multiagent Systems, pages 95-100, New-York, July 2004. [14] Z. Rabinovich and J. S. Rosenschein.",
                "Multiagent coordination by Extended Markov Tracking.",
                "In The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 431-438, Utrecht, The Netherlands, July 2005. [15] Z. Rabinovich and J. S. Rosenschein.",
                "On the response of EMT-based control to interacting targets and models.",
                "In The Fifth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 465-470, Hakodate, Japan, May 2006. [16] R. F. Stengel.",
                "Optimal Control and Estimation.",
                "Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, and P. Varakantham.",
                "Conflicts in teamwork: Hybrids to the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 797"
            ],
            "original_annotated_samples": [
                "To evaluate DBC, we carried out a set of experiments applying multi-target EMT to the <br>tag game</br> [11]; this is a variant on the area sweeping problem, where an agent is trying to tag a moving target (quarry) whose position is not known with certainty.",
                "We ran several experiments to evaluate EMT performance in the <br>tag game</br>.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) A Q Q A 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 A Q Figure 3: These configurations of the <br>tag game</br> space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor Table 2: Performance of the EMT-based solution in three <br>tag game</br> domains and two observability models: I) omniposition quarry, II) quarry is not at hunters position Model Domain Capture% E(Steps) Time/Step I Dead-ends 100 14.8 72(mSec) Arena 80.2 42.4 500(mSec) Circle 91.4 34.6 187(mSec) II Dead-ends 100 13.2 91(mSec) Arena 96.8 28.67 396(mSec) Circle 94.4 31.63 204(mSec) ical entropy grows with the length of interaction.",
                "DISCUSSION The design of the EMT solution for the <br>tag game</br> exposes the core difference in approach to planning and control between EMT or DBC, on the one hand, and the more familiar POMDP approach, on the other.",
                "Thus for the <br>tag game</br>, we did not search for a reward function that would encode and express our preference over the agents behavior, but rather directly set three (heuristic) behavior preferences as the basis for target dynamics to be maintained."
            ],
            "translated_annotated_samples": [
                "Para evaluar DBC, llevamos a cabo un conjunto de experimentos aplicando EMT de múltiples objetivos al <br>Juego de Etiquetas</br> [11]; esta es una variante del problema de barrido de área, donde un agente intenta etiquetar a un objetivo móvil (presa) cuya posición no se conoce con certeza.",
                "Realizamos varios experimentos para evaluar el rendimiento de EMT en el <br>Juego de Etiquetas</br>.",
                "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 3: Se utilizaron estas configuraciones del espacio de <br>juego de Tag</br>: a) múltiples callejones sin salida, b) arena abierta irregular, c) pasillo circular. Tabla 2: Rendimiento de la solución basada en EMT en tres dominios de <br>juego de Tag</br> y dos modelos de observabilidad: I) cantera de omniposición, II) la cantera no está en la posición de los cazadores. Modelo Dominio Captura% E(Pasos) Tiempo/Paso I Callejones sin salida 100 14.8 72(mSeg) Arena 80.2 42.4 500(mSeg) Círculo 91.4 34.6 187(mSeg) II Callejones sin salida 100 13.2 91(mSeg) Arena 96.8 28.67 396(mSeg) Círculo 94.4 31.63 204(mSeg) La entropía lógica crece con la duración de la interacción.",
                "DISCUSIÓN El diseño de la solución EMT para el <br>Juego de Etiquetas</br> expone la diferencia fundamental en el enfoque de planificación y control entre EMT o DBC, por un lado, y el enfoque más familiar de POMDP, por otro lado.",
                "Por lo tanto, para el <br>Juego de Etiquetas</br>, no buscamos una función de recompensa que codificara y expresara nuestra preferencia sobre el comportamiento de los agentes, sino que establecimos directamente tres preferencias de comportamiento (heurísticas) como base para mantener la dinámica del objetivo."
            ],
            "translated_text": "En este artículo presentamos el Control Basado en Dinámicas (DBC), un enfoque para la planificación y control de un agente en entornos estocásticos. A diferencia de los enfoques existentes, que buscan optimizar las recompensas esperadas (por ejemplo, en Problemas de Decisión de Markov Parcialmente Observables (POMDP)), DBC optimiza el comportamiento del sistema hacia las dinámicas del sistema especificadas. Mostramos que un enfoque de planificación y control recientemente desarrollado, Seguimiento Extendido de Markov (EMT), es una instancia de DBC. EMT emplea la selección de acciones codiciosa para proporcionar un algoritmo de control eficiente en entornos markovianos. Explotamos esta eficiencia en un conjunto de experimentos que aplicaron EMT multitarget a una clase de problemas de barrido de área (búsqueda de objetivos en movimiento). Mostramos que tales problemas pueden ser definidos de forma natural y resueltos eficientemente utilizando el marco de DBC y su instanciación de EMT. Categorías y Descriptores de Asignaturas I.2.8 [Resolución de Problemas, Métodos de Control y Búsqueda]: Teoría de Control; I.2.9 [Robótica]; I.2.11 [Inteligencia Artificial Distribuida]: Agentes Inteligentes Términos Generales Algoritmos, Teoría 1. INTRODUCCIÓN La planificación y el control constituyen un área de investigación central en sistemas multiagentes e inteligencia artificial. En los últimos años, los Procesos de Decisión de Markov Parcialmente Observables (POMDPs) [12] se han convertido en una base formal popular para la planificación en entornos estocásticos. En este marco, el problema de planificación y control suele abordarse imponiendo una función de recompensa y calculando una política (de elección de acciones) que sea óptima, en el sentido de que resultará en la mayor utilidad esperada. Si bien teóricamente atractivo, la complejidad de resolver óptimamente un POMDP es prohibitiva [8, 7]. Tomamos una visión alternativa de la planificación en entornos estocásticos. No utilizamos una función de recompensa basada en el estado, sino que optimizamos según un criterio diferente, una especificación basada en transiciones de la dinámica del sistema deseado. La idea aquí es ver la ejecución del plan como un proceso que obliga a un sistema (estocástico) a cambiar, y un plan como un proceso dinámico que moldea ese cambio de acuerdo con criterios deseados. Llamamos a este marco de planificación general Control Basado en Dinámicas (DBC). En DBC, el objetivo de un proceso de planificación (o control) es garantizar que el sistema cambiará de acuerdo con dinámicas objetivo específicas (potencialmente estocásticas). Dado que el comportamiento real del sistema puede desviarse de lo especificado por la dinámica objetivo (debido a la naturaleza estocástica del sistema), la planificación en tales entornos debe ser continua, de manera similar a los controladores en lazo cerrado clásicos. Aquí, la optimalidad se mide en términos de la probabilidad de magnitudes de desviación. En este documento, presentamos la estructura del Control Basado en Dinámicas. Mostramos que el enfoque de Seguimiento Extendido de Markov (EMT) recientemente desarrollado [13, 14, 15] está subsumido por DBC, con EMT empleando selección de acciones codiciosa, que es una parametrización específica entre las opciones posibles dentro de DBC. EMT es una implementación eficiente de DBC. Para evaluar DBC, llevamos a cabo un conjunto de experimentos aplicando EMT de múltiples objetivos al <br>Juego de Etiquetas</br> [11]; esta es una variante del problema de barrido de área, donde un agente intenta etiquetar a un objetivo móvil (presa) cuya posición no se conoce con certeza. Los datos experimentales demuestran que incluso con un modelo simple del entorno y un diseño simple de la dinámica del objetivo, se pueden lograr altas tasas de éxito tanto en la captura de la presa como en sorprender a la presa (como se expresa por la entropía observada de la posición de los agentes controlados). El documento está organizado de la siguiente manera. En la Sección 2 motivamos DBC utilizando problemas de barrido de área, y discutimos trabajos relacionados. La Sección 3 introduce la estructura de Control Basado en Dinámicas (DBC) y su especialización en entornos Markovianos. Esto es seguido por una revisión del enfoque de Seguimiento Markov Extendido (EMT) como un régimen de control estructurado por DBC en la Sección 4. Esa sección también discute las limitaciones del control basado en EMT en relación con el marco general de DBC. Los ajustes experimentales y los resultados se presentan a continuación en la Sección 5. La Sección 6 proporciona una breve discusión del enfoque general, y la Sección 7 ofrece algunas observaciones finales y direcciones para trabajos futuros. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2. MOTIVACIÓN Y TRABAJO RELACIONADO Muchos escenarios de la vida real tienen naturalmente una especificación de dinámica de destino estocástica, especialmente en aquellos dominios donde no existe un objetivo final, sino más bien un comportamiento del sistema (con propiedades específicas) que debe ser continuamente respaldado. Por ejemplo, los guardias de seguridad realizan barridos persistentes de un área para detectar cualquier signo de intrusión. Los ladrones astutos intentarán rastrear estos barridos y sincronizar su operación con puntos clave del movimiento de los guardias. Por lo tanto, es recomendable que la dinámica de movimiento de los guardias parezca irregular y aleatoria. El trabajo reciente de Paruchuri et al. [10] ha abordado dicha aleatorización en el contexto de POMDPs de agente único y distribuidos. El objetivo de ese trabajo era generar políticas que proporcionen una medida de aleatorización en la selección de acciones, manteniendo al mismo tiempo las recompensas dentro de niveles aceptables. Nuestro enfoque difiere de este trabajo en que DBC no optimiza recompensas esperadas, de hecho, no consideramos recompensas en absoluto, sino que en su lugar mantenemos dinámicas deseadas (incluyendo, pero no limitado a, la aleatorización). El juego de la mancha es otro ejemplo de la aplicabilidad del enfoque. Fue introducido en el trabajo de Pineau et al. [11]. Hay dos agentes que pueden moverse por un área, la cual está dividida en una cuadrícula. La cuadrícula puede tener celdas bloqueadas (agujeros) en las cuales ningún agente puede moverse. Un agente (el cazador) intenta moverse a una celda ocupada por el otro (la presa), de modo que estén ubicados en el mismo lugar (esto es una etiqueta exitosa). La presa busca evitar al cazador, siempre está al tanto de la posición del cazador, pero no sabe cómo se comportará el cazador, lo que abre la posibilidad de que un cazador sorprenda a la presa. El cazador conoce la ley de movimiento probabilístico de la presa, pero no conoce su ubicación actual. Tag es una instancia de una familia de problemas de barrido de área (persecución-evasión). En [11], el cazador modeló el problema utilizando un POMDP. Se definió una función de recompensa para reflejar el deseo de marcar la presa, y se calculó una política de acción para optimizar la recompensa recolectada con el tiempo. Debido a la complejidad intratable de determinar la política óptima, la política de acción calculada en ese documento fue esencialmente una aproximación. En este artículo, en lugar de formular una función de recompensa, utilizamos EMT para resolver el problema, especificando directamente la dinámica objetivo. De hecho, cualquier problema de búsqueda con movimiento aleatorio, la llamada clase de problemas de barrido de área, puede ser descrito a través de la especificación de la dinámica de dicho sistema objetivo. El Control Basado en Dinámicas proporciona un enfoque natural para resolver estos problemas. CONTROL BASADO EN DINÁMICA La especificación del Control Basado en Dinámica (DBC) se puede dividir en tres niveles interactivos: Nivel de Diseño del Entorno, Nivel de Usuario y Nivel de Agente. • El Nivel de Diseño del Entorno se ocupa de la especificación formal y modelado del entorno. Por ejemplo, este nivel especificaría las leyes de la física dentro del sistema y establecería sus parámetros, como la constante de gravitación. El Nivel de Usuario, a su vez, se basa en el modelo de entorno producido por el Diseño de Entorno para especificar la dinámica del sistema objetivo que desea observar. El Nivel de Usuario también especifica el procedimiento de estimación o aprendizaje para la dinámica del sistema, y la medida de desviación. En el escenario del guardia de museo mencionado anteriormente, estos corresponderían a un horario de barrido estocástico y una medida de sorpresa relativa entre el barrido especificado y el real. • El Nivel de Agente, a su vez, combina el modelo del entorno del Nivel de Diseño de Entorno, el procedimiento de estimación de la dinámica, la medida de desviación y la especificación de la dinámica objetivo del Nivel de Usuario, para producir una secuencia de acciones que creen dinámicas del sistema lo más cercanas posible a la especificación deseada. Dado nuestro interés en el desarrollo continuo de un sistema estocástico, como ocurre en la teoría de control clásica [16] y la planificación continua [4], así como en nuestro ejemplo de recorridos por museos, la pregunta es cómo el Nivel de Agente debe tratar las mediciones de desviación a lo largo del tiempo. Con este fin, utilizamos un umbral de probabilidad, es decir, nos gustaría que el Nivel del Agente maximice la probabilidad de que la medida de desviación permanezca por debajo de cierto umbral. La selección de acciones específicas depende entonces de la formalización del sistema. Una posibilidad sería crear una mezcla de tendencias de sistemas disponibles, similar a lo que sucede en las arquitecturas robóticas basadas en el comportamiento [1]. La otra alternativa sería depender del procedimiento de estimación proporcionado por el Nivel de Usuario para utilizar el modelo del Nivel de Diseño del Entorno del entorno para elegir acciones, de manera que se manipule al estimador de dinámicas para que crea que se ha logrado cierta dinámica. Ten en cuenta que esta manipulación no es directa, sino a través del entorno. Por lo tanto, para algoritmos de estimación lo suficientemente fuertes, la manipulación exitosa significaría una simulación exitosa de la dinámica objetivo especificada (es decir, más allá de discernir a través de la entrada sensorial disponible). Los niveles de DBC también pueden tener un retroceso de información (ver Figura 1). Por ejemplo, el Nivel de Agente podría proporcionar datos sobre la viabilidad de la dinámica del objetivo, permitiendo al Nivel de Usuario modificar el requisito, quizás centrándose en las características alcanzables del comportamiento del sistema. Los datos también estarían disponibles sobre la respuesta del sistema a diferentes acciones realizadas; combinados con un estimador de dinámica definido por el Nivel de Usuario, esto puede proporcionar una herramienta importante para la calibración del modelo del entorno en el Nivel de Diseño del Entorno. UserEnv. El modelo de agente de diseño Estimador de Dinámicas Ideales Estimador de Dinámicas Factibilidad de Respuesta del Sistema de Datos Figura 1: Flujo de datos del marco de trabajo DBC Ampliando la idea de los algoritmos Actor-Crítico [5], el flujo de datos de DBC puede proporcionar una buena base para el diseño de un algoritmo de aprendizaje. Por ejemplo, el Nivel de Usuario puede funcionar como un dispositivo exploratorio para un algoritmo de aprendizaje, inferir un objetivo dinámico ideal a partir del modelo del entorno en cuestión que expondría y verificaría las características más críticas del comportamiento del sistema. En este caso, los datos de viabilidad y respuesta del sistema a nivel de agente proporcionarían información clave para una actualización del modelo del entorno. De hecho, la combinación de datos de viabilidad y respuesta puede proporcionar una base para la aplicación de algoritmos de aprendizaje sólidos como EM [2, 9]. 3.1 DBC para entornos markovianos Para un entorno markoviano parcialmente observable, DBC puede especificarse de una manera más rigurosa. Observa cómo DBC descarta las recompensas y las reemplaza por otro criterio de optimalidad (las diferencias estructurales se resumen en la Tabla 1): • El nivel de Diseño del Entorno es especificar una tupla < S, A, T, O, Ω, s0 >, donde: - S es el conjunto de todos los posibles estados del entorno; - s0 es el estado inicial del entorno (que también puede ser visto como una distribución de probabilidad sobre S); La Sexta Conferencia Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 791 - A es el conjunto de todas las acciones posibles aplicables en el entorno; - T es la función de transición probabilística del entorno: T: S × A → Π(S). Es decir, T(s | a, s) es la probabilidad de que el entorno se mueva del estado s al estado s bajo la acción a; - O es el conjunto de todas las observaciones posibles. Así es como se vería la entrada del sensor para un observador externo; - Ω es la función de probabilidad de observación: Ω: S × A × S → Π(O). Es decir, Ω(o|s, a, s) es la probabilidad de observar o dado que el entorno ha pasado del estado s al estado s bajo la acción a. • Nivel de Usuario, en el caso de un entorno markoviano, opera en el conjunto de dinámicas del sistema descritas por una familia de probabilidades condicionales F = {τ: S × A → Π(S)}. Por lo tanto, la especificación de la dinámica del objetivo puede expresarse como q ∈ F, y el algoritmo de aprendizaje o seguimiento puede representarse como una función L : O×(A×O)∗ → F; es decir, mapea secuencias de observaciones y acciones realizadas hasta el momento en una estimación τ ∈ F de la dinámica del sistema. Hay muchas variaciones posibles disponibles a nivel de usuario para definir la divergencia entre la dinámica del sistema; varias de ellas son: - Distancia de traza o distancia L1 entre dos distribuciones p y q definida por D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Medida de fidelidad de distancia F(p(·), q(·)) = x p(x)q(x) - Divergencia de Kullback-Leibler DKL(p(·) q(·)) = x p(x) log p(x) q(x) Nótese que las dos últimas no son en realidad métricas sobre el espacio de distribuciones posibles, pero tienen interpretaciones significativas e importantes. Por ejemplo, la divergencia de Kullback-Leibler es una herramienta importante de la teoría de la información [3] que permite medir el costo de codificar una fuente de información gobernada por q, asumiendo que está gobernada por p. El Nivel de Usuario también define el umbral de probabilidad de desviación de la dinámica θ. • El Nivel de Agente se enfrenta entonces a un problema de seleccionar una función de señal de control a∗ para satisfacer un problema de minimización de la siguiente manera: a∗ = arg min a Pr(d(τa, q) > θ) donde d(τa, q) es una variable aleatoria que describe la desviación de la estimación de la dinámica τa, creada por L bajo la señal de control a, de la dinámica ideal q. Implícito en este problema de minimización es que L es manipulado a través del entorno, basado en el modelo de entorno producido por el Nivel de Diseño del Entorno. 3.2 Vista DBC del Espacio de Estados Es importante tener en cuenta la visión complementaria que DBC y POMDPs tienen sobre el espacio de estados del entorno. Los POMDP consideran el estado como una instantánea estacionaria del entorno; cualquier atributo de la secuencia de estados que se busque se alcanza a través de las propiedades del proceso de control, en este caso la acumulación de recompensas. Esto se puede ver como si la secuencia de estados y los atributos de esa secuencia solo fueran introducidos por y para el mecanismo de control, la política POMDP. DBC se enfoca en el principio subyacente de la secuenciación de estados, la dinámica del sistema. La especificación de la dinámica de los DBCs puede utilizar el espacio de estado del entorno como un medio para describir, discernir y preservar los cambios que ocurren dentro del sistema. Como resultado, DBC tiene una mayor capacidad para expresar propiedades de secuenciación de estados, las cuales están fundamentadas en el modelo del entorno y su definición del espacio de estados. Por ejemplo, considera la tarea de moverse a través de terreno accidentado hacia un objetivo y alcanzarlo lo más rápido posible. Los POMDPs codificarían el terreno como puntos en el espacio de estados, mientras que la velocidad se garantizaría mediante una recompensa negativa por cada paso dado sin alcanzar la meta. Acumular una recompensa más alta solo se puede lograr mediante un movimiento más rápido. Alternativamente, el espacio de estados podría incluir directamente la noción de velocidad. Para los POMDPs, esto significaría que el mismo concepto está codificado dos veces, en cierto sentido: directamente en el espacio de estados e indirectamente en la acumulación de recompensas. Ahora, incluso si la función de recompensa codificara más y detalles más finos de las propiedades del movimiento, la solución POMDP tendría que buscar en un espacio mucho más grande de políticas, aunque aún esté guiada por el concepto implícito del procedimiento de acumulación de recompensas. Por otro lado, la expresión del objetivo táctico de variaciones y correlaciones entre la posición y la velocidad del movimiento ahora se basa en la representación del espacio de estados. En esta situación, cualquier restricción adicional, como la suavidad del movimiento, los límites de velocidad en diferentes ubicaciones o las reducciones de velocidad durante giros bruscos, son expresadas explícita y uniformemente por el objetivo táctico, y pueden resultar en una selección de acciones más rápida y efectiva por un algoritmo DBC. 4. El Control Basado en EMT como un DBC Recientemente, se introdujo un algoritmo de control llamado Control Basado en EMT [13], que instancia el marco de trabajo DBC. Aunque proporciona una solución ávida aproximada en el sentido de DBC, los experimentos iniciales utilizando el control basado en EMT han sido alentadores [14, 15]. El control basado en EMT se basa en la definición del entorno markoviano, como en el caso de los POMDP, pero sus Niveles de Usuario y Agente son del tipo de optimalidad DBC markoviana. • El Nivel de Usuario del control basado en EMT define una dinámica del sistema objetivo en un caso limitado independiente de la acción: qEMT: S → Π(S). Luego utiliza la medida de divergencia de Kullback-Leibler para componer un estimador de dinámica de sistemas momentáneos: el algoritmo de Seguimiento Extendido de Markov (EMT). El algoritmo mantiene una estimación de la dinámica del sistema τt EMT que es capaz de explicar el cambio reciente en un estimador de estado auxiliar bayesiano de pt−1 a pt, y lo actualiza de manera conservadora utilizando la divergencia de Kullback-Leibler. Dado que τt EMT y pt−1,t son respectivamente las probabilidades condicionales y marginales sobre el espacio de estados del sistema, la explicación simplemente significa que pt(s) = s τt EMT (s | s)pt−1(s), y la actualización de la estimación de la dinámica se realiza resolviendo un 792 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Tabla 1: Estructura de POMDP vs. Control basado en Dinámicas en Entorno Markoviano Nivel Enfoque MDP Markoviano DBC Entorno < S, A, T, O, Ω >, donde S - conjunto de estados A - conjunto de acciones Diseño T : S × A → Π(S) - transición O - conjunto de observaciones Ω : S × A × S → Π(O) Usuario r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - función de recompensa q - dinámicas ideales F - remodelación de recompensa L - estimador de dinámicas θ - umbral Agente π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) problema de minimización: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • El nivel de Agente en el control basado en EMT es subóptimo con respecto a DBC (aunque permanece dentro del marco de DBC), realizando selección de acciones codiciosas basadas en la predicción de la reacción de EMT. La predicción se basa en el modelo del entorno proporcionado por el nivel de Diseño del Entorno, de modo que si denotamos por Ta la función de transición de entornos limitada a la acción a, y pt−1 es el estimador de estado auxiliar del sistema bayesiano, entonces la elección de control basada en EMT se describe por a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1). Nótese que esto sigue precisamente el marco DBC Markoviano: la optimalidad recompensante de los POMDPs se descarta, y en su lugar se manipula un estimador de dinámicas (EMT en este caso) a través de los efectos de la acción en el entorno para producir una estimación cercana a la dinámica del sistema objetivo especificado. Sin embargo, como mencionamos, el control EMT ingenuo es subóptimo en el sentido de DBC y tiene varias limitaciones adicionales que no existen en el marco general de DBC (discutido en la Sección 4.2). 4.1 EMT de múltiples objetivos. En ocasiones, puede haber varias preferencias de comportamiento. Por ejemplo, en el caso de los guardias de museo, algunos objetos de arte están más fuertemente custodiados, lo que requiere que los guardias permanezcan más a menudo en su cercanía. Por otro lado, no se debe dejar ningún rincón del museo sin revisar, lo que requiere un movimiento constante. El éxito de la seguridad en un museo requeriría que los guardias se adhieran y equilibren ambos comportamientos. Para el control basado en EMT, esto significaría enfrentar varios objetivos tácticos {qk}K k=1, y la pregunta sería cómo fusionar y equilibrarlos. Un mecanismo de equilibrio se puede aplicar para resolver este problema. Ten en cuenta que el control basado en EMT, al seleccionar una acción, crea un vector de preferencia sobre el conjunto de acciones basado en su rendimiento predicho con respecto a un objetivo dado. Si estos vectores de preferencia están normalizados, pueden combinarse en una única preferencia unificada. Esto requiere reemplazar la selección de acciones basada en EMT estándar por el algoritmo a continuación [15]: • Dado: - un conjunto de dinámicas objetivo {qk}K k=1, - vector de pesos w(k) • Seleccionar la acción de la siguiente manera - Para cada acción a ∈ A predecir la distribución de estado futuro ¯pa t+1 = Ta ∗ pt; - Para cada acción, calcular Da = H(¯pa t+1, pt, PDt) - Para cada a ∈ A y objetivo táctico qk, denotar V (a, k) = DKL (Da qk) pt. Sea Vk(a) = 1 Zk V (a, k), donde Zk = a∈A V (a, k) es un factor de normalización. - Selecciona a∗ = arg min a k k=1 w(k)Vk(a) El vector de pesos w = (w1, ..., wK ) permite ajustar la importancia entre las dinámicas objetivo sin necesidad de rediseñar los objetivos mismos. Este método de equilibrio también está integrado de forma transparente en el flujo de operación basado en EMT. Limitaciones del Control Basado en EMT El control basado en EMT es una representación subóptima (en el sentido de DBC) de la estructura DBC. Limita al Usuario al obligar a EMT a ser su algoritmo de seguimiento dinámico, y reemplaza la optimización del Agente por la selección de acciones codiciosa. Este tipo de combinación, sin embargo, es común en algoritmos en línea. Aunque es necesario un mayor desarrollo de los controladores basados en EMT, la evidencia hasta ahora sugiere que incluso la forma más simple del algoritmo posee una gran cantidad de potencia y muestra tendencias que son óptimas en el sentido de DBC. Hay dos limitaciones adicionales específicas de los técnicos en emergencias médicas (EMT, por sus siglas en inglés) al control basado en EMT que son evidentes en este momento. Ambos ya tienen soluciones parciales y son objeto de investigación continua. La primera limitación es el problema de la preferencia negativa. En el marco de POMDP, por ejemplo, esto se captura de manera sencilla, a través de The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 793 la aparición de valores con signos diferentes dentro de la estructura de recompensa. Para el control basado en EMT, sin embargo, la preferencia negativa significa que uno quisiera evitar cierta distribución sobre las secuencias de desarrollo del sistema; sin embargo, el control basado en EMT se concentra en acercarse lo más posible a una distribución. La evitación es, por lo tanto, antinatural en el control basado en EMT nativo. La segunda limitación proviene del hecho de que el modelado del entorno estándar puede crear acciones sensoriales puras, acciones que no cambian el estado del mundo y solo difieren en la forma en que se reciben las observaciones y la calidad de las observaciones recibidas. Dado que el estado del mundo no cambia, el control basado en EMT no sería capaz de diferenciar entre diferentes acciones sensoriales. Ten en cuenta que ambas limitaciones del control basado en EMT están ausentes en el marco general de DBC, ya que puede tener un algoritmo de seguimiento capaz de considerar acciones sensoriales puras y, a diferencia de la divergencia de Kullback-Leibler, una medida de desviación de distribución capaz de manejar preferencias negativas. 5. El juego de la \"carrera de relevos\" fue introducido por primera vez en [11]. Es un problema de agente único de capturar una presa, y pertenece a la clase de problemas de barrido de área. Un ejemplo de dominio se muestra en la Figura 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figura 2: Dominio de etiquetas; un agente (A) intenta buscar y capturar una presa (Q). El juego de etiquetas limita extremadamente la percepción de los agentes, de modo que el agente solo puede detectar la presa si están ubicados en la misma celda del mundo de cuadrícula. En la versión clásica del juego, la co-locación conduce a una observación especial, y la acción de Etiqueta se puede realizar. Modificamos ligeramente esta configuración: el momento en que ambos agentes ocupan la misma celda, el juego termina. Como resultado, tanto el agente como su presa tienen la misma capacidad de movimiento, lo que les permite moverse en cuatro direcciones, Norte, Sur, Este y Oeste. Estos forman un espacio formal de acciones dentro de un entorno markoviano. El espacio de estados del entorno formal markoviano está descrito por el producto cruz de las posiciones del agente y la presa. Para la Figura 2, sería S = {s0, ..., s23} × {s0, ..., s23}. Los efectos de una acción tomada por el agente son deterministas, pero el entorno en general tiene una respuesta estocástica debido al movimiento de la presa. Con probabilidad q0 1 se queda en su lugar, y con probabilidad 1 − q0 se mueve a una celda adyacente más lejos del agente 1. En nuestros experimentos, esto se tomó como q0 = 0.2. Por lo tanto, para la instancia mostrada en la Figura 2 y q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Aunque el comportamiento evasivo de la presa es conocido por el agente, la posición de la presa no lo es. La única información sensorial disponible para el agente es su propia ubicación. Utilizamos EMT y especificamos directamente la dinámica del objetivo. Para el juego de la mancha, podemos formular fácilmente tres tendencias principales: atrapar al perseguido, mantenerse en movimiento y acechar al perseguido. Esto resulta en las siguientes tres dinámicas de objetivo: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 en otro caso Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 en otro caso Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Nótese que ninguno de los objetivos anteriores es directamente alcanzable; por ejemplo, si Qt = s9 y At = s11, no hay ninguna acción que pueda llevar al agente a At+1 = s9 como lo requiere la dinámica de objetivo Tcatch. Realizamos varios experimentos para evaluar el rendimiento de EMT en el <br>Juego de Etiquetas</br>. Se utilizaron tres configuraciones del dominio mostrado en la Figura 3, cada una planteando un desafío diferente para el agente debido a la observabilidad parcial. En cada escenario, se realizó un conjunto de 1000 ejecuciones con un límite de tiempo de 100 pasos. En cada ejecución, la posición inicial tanto del agente como de su presa fue seleccionada al azar; esto significa que, en lo que respecta al agente, la posición inicial de la presa estaba distribuida de forma uniforme en todo el espacio de celdas del dominio. También utilizamos dos variaciones de la función de observabilidad del entorno. En la primera versión, la función de observabilidad mapeaba todas las posiciones articulares del cazador y la presa en la posición del cazador como observación. En el segundo, solo aquellas posiciones conjuntas en las que el cazador y la presa ocupaban ubicaciones diferentes fueron mapeadas en la ubicación del cazador. La segunda versión de hecho utilizó y expresó el hecho de que una vez que el cazador y la presa ocupan la misma celda, el juego termina. Los resultados de estos experimentos se muestran en la Tabla 2. Equilibrando la dinámica de captura, movimiento y seguimiento descrita en la sección anterior mediante el vector de peso [0.8, 0.1, 0.1], EMT logró un rendimiento estable en los tres dominios. Aunque las comparaciones directas son difíciles de hacer, el rendimiento de los EMT mostró una notable eficiencia en comparación con el enfoque POMDP. A pesar de una implementación simple e ineficiente del algoritmo EMT en Matlab, el tiempo de decisión para cualquier paso dado promedió significativamente por debajo de 1 segundo en todos los experimentos. Para el dominio de la arena abierta irregular, que resultó ser el más difícil, se completaron 1000 ejecuciones de experimentos limitadas a 100 pasos cada una, un total de 42411 pasos, en poco menos de 6 horas. Es decir, más de 4 × 104 pasos en línea tomaron una orden de magnitud menos tiempo que la computación sin conexión de la política POMDP en [11]. La importancia de esta diferencia se destaca aún más por el hecho de que, en caso de que cambien los parámetros del modelo del entorno, la naturaleza en línea de EMT le permitiría mantener su rendimiento, mientras que la política POMDP necesitaría ser recalculada, lo que requeriría una vez más un gran costo computacional adicional. También probamos la entropía de la frecuencia de las células de comportamiento, medidas empíricas de los datos de prueba. Como muestran la Figura 4 y la Figura 5, empir794 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 3: Se utilizaron estas configuraciones del espacio de <br>juego de Tag</br>: a) múltiples callejones sin salida, b) arena abierta irregular, c) pasillo circular. Tabla 2: Rendimiento de la solución basada en EMT en tres dominios de <br>juego de Tag</br> y dos modelos de observabilidad: I) cantera de omniposición, II) la cantera no está en la posición de los cazadores. Modelo Dominio Captura% E(Pasos) Tiempo/Paso I Callejones sin salida 100 14.8 72(mSeg) Arena 80.2 42.4 500(mSeg) Círculo 91.4 34.6 187(mSeg) II Callejones sin salida 100 13.2 91(mSeg) Arena 96.8 28.67 396(mSeg) Círculo 94.4 31.63 204(mSeg) La entropía lógica crece con la duración de la interacción. Para las carreras en las que la presa no fue capturada de inmediato, la entropía alcanza valores entre 0.85 y 0.952 para diferentes carreras y escenarios. A medida que el agente busca activamente la presa, la entropía nunca alcanza su máximo. Una característica del gráfico de entropía para el escenario de arena abierta llamó especialmente nuestra atención en el caso del modelo de observación de cantera omniposicional. Cerca del límite máximo de longitud de prueba (100 pasos), la entropía cayó repentinamente. Un análisis adicional de los datos mostró que bajo ciertas circunstancias, se produce un comportamiento fluctuante en el que el agente se enfrenta a versiones igualmente viables del comportamiento de seguir a la presa. Dado que el algoritmo EMT tiene una selección de acciones codiciosa, y el espacio de estados no codifica ninguna forma de compromiso (ni siquiera velocidad o aceleración), el agente queda atrapado dentro de una pequeña porción de celdas. Básicamente, se intenta seguir simultáneamente varios cursos de acción, todos los cuales son consistentes con la dinámica objetivo. Este comportamiento no ocurrió en nuestro segundo modelo de observación, ya que redujo significativamente el conjunto de cursos de acción elegibles, contribuyendo esencialmente a romper el empate entre ellos. 6. DISCUSIÓN El diseño de la solución EMT para el <br>Juego de Etiquetas</br> expone la diferencia fundamental en el enfoque de planificación y control entre EMT o DBC, por un lado, y el enfoque más familiar de POMDP, por otro lado. POMDP define una estructura de recompensa para optimizar e influencia la dinámica del sistema de forma indirecta a través de esa optimización. EMT descarta cualquier esquema de recompensa y, en cambio, mide e influye directamente en la dinámica del sistema. Se calculó la entropía 2 utilizando el logaritmo con base igual al número de ubicaciones posibles dentro del dominio; esto escala adecuadamente la expresión de entropía en el rango [0, 1] para todos los dominios. Por lo tanto, para el <br>Juego de Etiquetas</br>, no buscamos una función de recompensa que codificara y expresara nuestra preferencia sobre el comportamiento de los agentes, sino que establecimos directamente tres preferencias de comportamiento (heurísticas) como base para mantener la dinámica del objetivo. ",
            "candidates": [],
            "error": [
                [
                    "Juego de Etiquetas",
                    "Juego de Etiquetas",
                    "juego de Tag",
                    "juego de Tag",
                    "Juego de Etiquetas",
                    "Juego de Etiquetas"
                ]
            ]
        },
        "environment design level": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Dynamics Based Control with an Application to Area-Sweeping Problems Zinovi Rabinovich Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel jeff@cs.huji.ac.il Gal A. Kaminka The MAVERICK Group Department of Computer Science Bar Ilan University, Israel galk@cs.biu.ac.il ABSTRACT In this paper we introduce Dynamics Based Control (DBC), an approach to planning and control of an agent in stochastic environments.",
                "Unlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics.",
                "We show that a recently developed planning and control approach, Extended Markov Tracking (EMT) is an instantiation of DBC.",
                "EMT employs greedy action selection to provide an efficient control algorithm in Markovian environments.",
                "We exploit this efficiency in a set of experiments that applied multitarget EMT to a class of area-sweeping problems (searching for moving targets).",
                "We show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation.",
                "Categories and Subject Descriptors I.2.8 [Problem Solving, Control Methods, and Search]: Control Theory; I.2.9 [Robotics]; I.2.11 [Distributed Artificial Intelligence]: Intelligent Agents General Terms Algorithms, Theory 1.",
                "INTRODUCTION Planning and control constitutes a central research area in multiagent systems and artificial intelligence.",
                "In recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in stochastic environments.",
                "In this framework, the planning and control problem is often addressed by imposing a reward function, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility.",
                "While theoretically attractive, the complexity of optimally solving a POMDP is prohibitive [8, 7].",
                "We take an alternative view of planning in stochastic environments.",
                "We do not use a (state-based) reward function, but instead optimize over a different criterion, a transition-based specification of the desired system dynamics.",
                "The idea here is to view planexecution as a process that compels a (stochastic) system to change, and a plan as a dynamic process that shapes that change according to desired criteria.",
                "We call this general planning framework Dynamics Based Control (DBC).",
                "In DBC, the goal of a planning (or control) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) target dynamics.",
                "As actual system behavior may deviate from that which is specified by target dynamics (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16].",
                "Here, optimality is measured in terms of probability of deviation magnitudes.",
                "In this paper, we present the structure of Dynamics Based Control.",
                "We show that the recently developed Extended Markov Tracking (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC.",
                "EMT is an efficient instantiation of DBC.",
                "To evaluate DBC, we carried out a set of experiments applying multi-target EMT to the Tag Game [11]; this is a variant on the area sweeping problem, where an agent is trying to tag a moving target (quarry) whose position is not known with certainty.",
                "Experimental data demonstrates that even with a simple model of the environment and a simple design of target dynamics, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agents position).",
                "The paper is organized as follows.",
                "In Section 2 we motivate DBC using area-sweeping problems, and discuss related work.",
                "Section 3 introduces the Dynamics Based Control (DBC) structure, and its specialization to Markovian environments.",
                "This is followed by a review of the Extended Markov Tracking (EMT) approach as a DBC-structured control regimen in Section 4.",
                "That section also discusses the limitations of EMT-based control relative to the general DBC framework.",
                "Experimental settings and results are then presented in Section 5.",
                "Section 6 provides a short discussion of the overall approach, and Section 7 gives some concluding remarks and directions for future work. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "MOTIVATION AND RELATED WORK Many real-life scenarios naturally have a stochastic target dynamics specification, especially those domains where there exists no ultimate goal, but rather system behavior (with specific properties) that has to be continually supported.",
                "For example, security guards perform persistent sweeps of an area to detect any sign of intrusion.",
                "Cunning thieves will attempt to track these sweeps, and time their operation to key points of the guards motion.",
                "It is thus advisable to make the guards motion dynamics appear irregular and random.",
                "Recent work by Paruchuri et al. [10] has addressed such randomization in the context of single-agent and distributed POMDPs.",
                "The goal in that work was to generate policies that provide a measure of action-selection randomization, while maintaining rewards within some acceptable levels.",
                "Our focus differs from this work in that DBC does not optimize expected rewards-indeed we do not consider rewards at all-but instead maintains desired dynamics (including, but not limited to, randomization).",
                "The Game of Tag is another example of the applicability of the approach.",
                "It was introduced in the work by Pineau et al. [11].",
                "There are two agents that can move about an area, which is divided into a grid.",
                "The grid may have blocked cells (holes) into which no agent can move.",
                "One agent (the hunter) seeks to move into a cell occupied by the other (the quarry), such that they are co-located (this is a successful tag).",
                "The quarry seeks to avoid the hunter agent, and is always aware of the hunters position, but does not know how the hunter will behave, which opens up the possibility for a hunter to surprise the prey.",
                "The hunter knows the quarrys probabilistic law of motion, but does not know its current location.",
                "Tag is an instance of a family of area-sweeping (pursuit-evasion) problems.",
                "In [11], the hunter modeled the problem using a POMDP.",
                "A reward function was defined, to reflect the desire to tag the quarry, and an action policy was computed to optimize the reward collected over time.",
                "Due to the intractable complexity of determining the optimal policy, the action policy computed in that paper was essentially an approximation.",
                "In this paper, instead of formulating a reward function, we use EMT to solve the problem, by directly specifying the target dynamics.",
                "In fact, any search problem with randomized motion, the socalled class of area sweeping problems, can be described through specification of such target system dynamics.",
                "Dynamics Based Control provides a natural approach to solving these problems. 3.",
                "DYNAMICS BASED CONTROL The specification of Dynamics Based Control (DBC) can be broken into three interacting levels: <br>environment design level</br>, User Level, and Agent Level. • <br>environment design level</br> is concerned with the formal specification and modeling of the environment.",
                "For example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation constant. • User Level in turn relies on the environment model produced by Environment Design to specify the target system dynamics it wishes to observe.",
                "The User Level also specifies the estimation or learning procedure for system dynamics, and the measure of deviation.",
                "In the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping. • Agent Level in turn combines the environment model from the <br>environment design level</br>, the dynamics estimation procedure, the deviation measure and the target dynamics specification from User Level, to produce a sequence of actions that create system dynamics as close as possible to the targeted specification.",
                "As we are interested in the continual development of a stochastic system, such as happens in classical control theory [16] and continual planning [4], as well as in our example of museum sweeps, the question becomes how the Agent Level is to treat the deviation measurements over time.",
                "To this end, we use a probability threshold-that is, we would like the Agent Level to maximize the probability that the deviation measure will remain below a certain threshold.",
                "Specific action selection then depends on system formalization.",
                "One possibility would be to create a mixture of available system trends, much like that which happens in Behavior-Based Robotic architectures [1].",
                "The other alternative would be to rely on the estimation procedure provided by the User Level-to utilize the <br>environment design level</br> model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a certain dynamics has been achieved.",
                "Notice that this manipulation is not direct, but via the environment.",
                "Thus, for strong enough estimator algorithms, successful manipulation would mean a successful simulation of the specified target dynamics (i.e., beyond discerning via the available sensory input).",
                "DBC levels can also have a back-flow of information (see Figure 1).",
                "For instance, the Agent Level could provide data about target dynamics feasibility, allowing the User Level to modify the requirement, perhaps focusing on attainable features of system behavior.",
                "Data would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the User Level, this can provide an important tool for the environment model calibration at the <br>environment design level</br>.",
                "UserEnv.",
                "Design Agent Model Ideal Dynamics Estimator Estimator Dynamics Feasibility System Response Data Figure 1: Data flow of the DBC framework Extending upon the idea of Actor-Critic algorithms [5], DBC data flow can provide a good basis for the design of a learning algorithm.",
                "For example, the User Level can operate as an exploratory device for a learning algorithm, inferring an ideal dynamics target from the environment model at hand that would expose and verify most critical features of system behavior.",
                "In this case, feasibility and system response data from the Agent Level would provide key information for an environment model update.",
                "In fact, the combination of feasibility and response data can provide a basis for the application of strong learning algorithms such as EM [2, 9]. 3.1 DBC for Markovian Environments For a Partially Observable Markovian Environment, DBC can be specified in a more rigorous manner.",
                "Notice how DBC discards rewards, and replaces it by another optimality criterion (structural differences are summarized in Table 1): • <br>environment design level</br> is to specify a tuple < S, A, T, O, Ω, s0 >, where: - S is the set of all possible environment states; - s0 is the initial state of the environment (which can also be viewed as a probability distribution over S); The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 791 - A is the set of all possible actions applicable in the environment; - T is the environments probabilistic transition function: T : S ×A → Π(S).",
                "That is, T(s |a, s) is the probability that the environment will move from state s to state s under action a; - O is the set of all possible observations.",
                "This is what the sensor input would look like for an outside observer; - Ω is the observation probability function: Ω : S × A × S → Π(O).",
                "That is, Ω(o|s , a, s) is the probability that one will observe o given that the environment has moved from state s to state s under action a. • User Level, in the case of Markovian environment, operates on the set of system dynamics described by a family of conditional probabilities F = {τ : S × A → Π(S)}.",
                "Thus specification of target dynamics can be expressed by q ∈ F, and the learning or tracking algorithm can be represented as a function L : O×(A×O)∗ → F; that is, it maps sequences of observations and actions performed so far into an estimate τ ∈ F of system dynamics.",
                "There are many possible variations available at the User Level to define divergence between system dynamics; several of them are: - Trace distance or L1 distance between two distributions p and q defined by D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Fidelity measure of distance F(p(·), q(·)) = x p(x)q(x) - Kullback-Leibler divergence DKL(p(·) q(·)) = x p(x) log p(x) q(x) Notice that the latter two are not actually metrics over the space of possible distributions, but nevertheless have meaningful and important interpretations.",
                "For instance, KullbackLeibler divergence is an important tool of information theory [3] that allows one to measure the price of encoding an information source governed by q, while assuming that it is governed by p. The User Level also defines the threshold of dynamics deviation probability θ. • Agent Level is then faced with a problem of selecting a control signal function a∗ to satisfy a minimization problem as follows: a∗ = arg min a Pr(d(τa, q) > θ) where d(τa, q) is a random variable describing deviation of the dynamics estimate τa, created by L under control signal a, from the ideal dynamics q.",
                "Implicit in this minimization problem is that L is manipulated via the environment, based on the environment model produced by the <br>environment design level</br>. 3.2 DBC View of the State Space It is important to note the complementary view that DBC and POMDPs take on the state space of the environment.",
                "POMDPs regard state as a stationary snap-shot of the environment; whatever attributes of state sequencing one seeks are reached through properties of the control process, in this case reward accumulation.",
                "This can be viewed as if the sequencing of states and the attributes of that sequencing are only introduced by and for the controlling mechanism-the POMDP policy.",
                "DBC concentrates on the underlying principle of state sequencing, the system dynamics.",
                "DBCs target dynamics specification can use the environments state space as a means to describe, discern, and preserve changes that occur within the system.",
                "As a result, DBC has a greater ability to express state sequencing properties, which are grounded in the environment model and its state space definition.",
                "For example, consider the task of moving through rough terrain towards a goal and reaching it as fast as possible.",
                "POMDPs would encode terrain as state space points, while speed would be ensured by negative reward for every step taken without reaching the goalaccumulating higher reward can be reached only by faster motion.",
                "Alternatively, the state space could directly include the notion of speed.",
                "For POMDPs, this would mean that the same concept is encoded twice, in some sense: directly in the state space, and indirectly within reward accumulation.",
                "Now, even if the reward function would encode more, and finer, details of the properties of motion, the POMDP solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure.",
                "On the other hand, the tactical target expression of variations and correlations between position and speed of motion is now grounded in the state space representation.",
                "In this situation, any further constraints, e.g., smoothness of motion, speed limits in different locations, or speed reductions during sharp turns, are explicitly and uniformly expressed by the tactical target, and can result in faster and more effective action selection by a DBC algorithm. 4.",
                "EMT-BASED CONTROL AS A DBC Recently, a control algorithm was introduced called EMT-based Control [13], which instantiates the DBC framework.",
                "Although it provides an approximate greedy solution in the DBC sense, initial experiments using EMT-based control have been encouraging [14, 15].",
                "EMT-based control is based on the Markovian environment definition, as in the case with POMDPs, but its User and Agent Levels are of the Markovian DBC type of optimality. • User Level of EMT-based control defines a limited-case target system dynamics independent of action: qEMT : S → Π(S).",
                "It then utilizes the Kullback-Leibler divergence measure to compose a momentary system dynamics estimator-the Extended Markov Tracking (EMT) algorithm.",
                "The algorithm keeps a system dynamics estimate τt EMT that is capable of explaining recent change in an auxiliary Bayesian system state estimator from pt−1 to pt, and updates it conservatively using Kullback-Leibler divergence.",
                "Since τt EMT and pt−1,t are respectively the conditional and marginal probabilities over the systems state space, explanation simply means that pt(s ) = s τt EMT (s |s)pt−1(s), and the dynamics estimate update is performed by solving a 792 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: Structure of POMDP vs. Dynamics-Based Control in Markovian Environment Level Approach MDP Markovian DBC Environment < S, A, T, O, Ω >,where S - set of states A - set of actions Design T : S × A → Π(S) - transition O - observation set Ω : S × A × S → Π(O) User r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - reward function q - ideal dynamics F - reward remodeling L - dynamics estimator θ - threshold Agent π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) minimization problem: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • Agent Level in EMT-based control is suboptimal with respect to DBC (though it remains within the DBC framework), performing greedy action selection based on prediction of EMTs reaction.",
                "The prediction is based on the environment model provided by the <br>environment design level</br>, so that if we denote by Ta the environments transition function limited to action a, and pt−1 is the auxiliary Bayesian system state estimator, then the EMT-based control choice is described by a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1) Note that this follows the Markovian DBC framework precisely: the rewarding optimality of POMDPs is discarded, and in its place a dynamics estimator (EMT in this case) is manipulated via action effects on the environment to produce an estimate close to the specified target system dynamics.",
                "Yet as we mentioned, naive EMTbased control is suboptimal in the DBC sense, and has several additional limitations that do not exist in the general DBC framework (discussed in Section 4.2). 4.1 Multi-Target EMT At times, there may exist several behavioral preferences.",
                "For example, in the case of museum guards, some art items are more heavily guarded, requiring that the guards stay more often in their close vicinity.",
                "On the other hand, no corner of the museum is to be left unchecked, which demands constant motion.",
                "Successful museum security would demand that the guards adhere to, and balance, both of these behaviors.",
                "For EMT-based control, this would mean facing several tactical targets {qk}K k=1, and the question becomes how to merge and balance them.",
                "A balancing mechanism can be applied to resolve this issue.",
                "Note that EMT-based control, while selecting an action, creates a preference vector over the set of actions based on their predicted performance with respect to a given target.",
                "If these preference vectors are normalized, they can be combined into a single unified preference.",
                "This requires replacement of standard EMT-based action selection by the algorithm below [15]: • Given: - a set of target dynamics {qk}K k=1, - vector of weights w(k) • Select action as follows - For each action a ∈ A predict the future state distribution ¯pa t+1 = Ta ∗ pt; - For each action, compute Da = H(¯pa t+1, pt, PDt) - For each a ∈ A and qk tactical target, denote V (a, k) = DKL (Da qk) pt .",
                "Let Vk(a) = 1 Zk V (a, k), where Zk = a∈A V (a, k) is a normalization factor. - Select a∗ = arg min a k k=1 w(k)Vk(a) The weights vector w = (w1, ..., wK ) allows the additional tuning of importance among target dynamics without the need to redesign the targets themselves.",
                "This balancing method is also seamlessly integrated into the EMT-based control flow of operation. 4.2 EMT-based Control Limitations EMT-based control is a sub-optimal (in the DBC sense) representative of the DBC structure.",
                "It limits the User by forcing EMT to be its dynamic tracking algorithm, and replaces Agent optimization by greedy action selection.",
                "This kind of combination, however, is common for on-line algorithms.",
                "Although further development of EMT-based controllers is necessary, evidence so far suggests that even the simplest form of the algorithm possesses a great deal of power, and displays trends that are optimal in the DBC sense of the word.",
                "There are two further, EMT-specific, limitations to EMT-based control that are evident at this point.",
                "Both already have partial solutions and are subjects of ongoing research.",
                "The first limitation is the problem of negative preference.",
                "In the POMDP framework for example, this is captured simply, through The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 793 the appearance of values with different signs within the reward structure.",
                "For EMT-based control, however, negative preference means that one would like to avoid a certain distribution over system development sequences; EMT-based control, however, concentrates on getting as close as possible to a distribution.",
                "Avoidance is thus unnatural in native EMT-based control.",
                "The second limitation comes from the fact that standard environment modeling can create pure sensory actions-actions that do not change the state of the world, and differ only in the way observations are received and the quality of observations received.",
                "Since the world state does not change, EMT-based control would not be able to differentiate between different sensory actions.",
                "Notice that both of these limitations of EMT-based control are absent from the general DBC framework, since it may have a tracking algorithm capable of considering pure sensory actions and, unlike Kullback-Leibler divergence, a distribution deviation measure that is capable of dealing with negative preference. 5.",
                "EMT PLAYING TAG The Game of Tag was first introduced in [11].",
                "It is a single agent problem of capturing a quarry, and belongs to the class of area sweeping problems.",
                "An example domain is shown in Figure 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figure 2: Tag domain; an agent (A) attempts to seek and capture a quarry (Q) The Game of Tag extremely limits the agents perception, so that the agent is able to detect the quarry only if they are co-located in the same cell of the grid world.",
                "In the classical version of the game, co-location leads to a special observation, and the Tag action can be performed.",
                "We slightly modify this setting: the moment that both agents occupy the same cell, the game ends.",
                "As a result, both the agent and its quarry have the same motion capability, which allows them to move in four directions, North, South, East, and West.",
                "These form a formal space of actions within a Markovian environment.",
                "The state space of the formal Markovian environment is described by the cross-product of the agent and quarrys positions.",
                "For Figure 2, it would be S = {s0, ..., s23} × {s0, ..., s23}.",
                "The effects of an action taken by the agent are deterministic, but the environment in general has a stochastic response due to the motion of the quarry.",
                "With probability q0 1 it stays put, and with probability 1 − q0 it moves to an adjacent cell further away from the 1 In our experiments this was taken to be q0 = 0.2. agent.",
                "So for the instance shown in Figure 2 and q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Although the evasive behavior of the quarry is known to the agent, the quarrys position is not.",
                "The only sensory information available to the agent is its own location.",
                "We use EMT and directly specify the target dynamics.",
                "For the Game of Tag, we can easily formulate three major trends: catching the quarry, staying mobile, and stalking the quarry.",
                "This results in the following three target dynamics: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 otherwise Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 otherwise Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Note that none of the above targets are directly achievable; for instance, if Qt = s9 and At = s11, there is no action that can move the agent to At+1 = s9 as required by the Tcatch target dynamics.",
                "We ran several experiments to evaluate EMT performance in the Tag Game.",
                "Three configurations of the domain shown in Figure 3 were used, each posing a different challenge to the agent due to partial observability.",
                "In each setting, a set of 1000 runs was performed with a time limit of 100 steps.",
                "In every run, the initial position of both the agent and its quarry was selected at random; this means that as far as the agent was concerned, the quarrys initial position was uniformly distributed over the entire domain cell space.",
                "We also used two variations of the environment observability function.",
                "In the first version, observability function mapped all joint positions of hunter and quarry into the position of the hunter as an observation.",
                "In the second, only those joint positions in which hunter and quarry occupied different locations were mapped into the hunters location.",
                "The second version in fact utilized and expressed the fact that once hunter and quarry occupy the same cell the game ends.",
                "The results of these experiments are shown in Table 2.",
                "Balancing [15] the catch, move, and stalk target dynamics described in the previous section by the weight vector [0.8, 0.1, 0.1], EMT produced stable performance in all three domains.",
                "Although direct comparisons are difficult to make, the EMT performance displayed notable efficiency vis-`a-vis the POMDP approach.",
                "In spite of a simple and inefficient Matlab implementation of the EMT algorithm, the decision time for any given step averaged significantly below 1 second in all experiments.",
                "For the irregular open arena domain, which proved to be the most difficult, 1000 experiment runs bounded by 100 steps each, a total of 42411 steps, were completed in slightly under 6 hours.",
                "That is, over 4 × 104 online steps took an order of magnitude less time than the offline computation of POMDP policy in [11].",
                "The significance of this differential is made even more prominent by the fact that, should the environment model parameters change, the online nature of EMT would allow it to maintain its performance time, while the POMDP policy would need to be recomputed, requiring yet again a large overhead of computation.",
                "We also tested the behavior cell frequency entropy, empirical measures from trial data.",
                "As Figure 4 and Figure 5 show, empir794 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) A Q Q A 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 A Q Figure 3: These configurations of the Tag Game space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor Table 2: Performance of the EMT-based solution in three Tag Game domains and two observability models: I) omniposition quarry, II) quarry is not at hunters position Model Domain Capture% E(Steps) Time/Step I Dead-ends 100 14.8 72(mSec) Arena 80.2 42.4 500(mSec) Circle 91.4 34.6 187(mSec) II Dead-ends 100 13.2 91(mSec) Arena 96.8 28.67 396(mSec) Circle 94.4 31.63 204(mSec) ical entropy grows with the length of interaction.",
                "For runs where the quarry was not captured immediately, the entropy reaches between 0.85 and 0.952 for different runs and scenarios.",
                "As the agent actively seeks the quarry, the entropy never reaches its maximum.",
                "One characteristic of the entropy graph for the open arena scenario particularly caught our attention in the case of the omniposition quarry observation model.",
                "Near the maximum limit of trial length (100 steps), entropy suddenly dropped.",
                "Further analysis of the data showed that under certain circumstances, a fluctuating behavior occurs in which the agent faces equally viable versions of quarry-following behavior.",
                "Since the EMT algorithm has greedy action selection, and the state space does not encode any form of commitment (not even speed or acceleration), the agent is locked within a small portion of cells.",
                "It is essentially attempting to simultaneously follow several courses of action, all of which are consistent with the target dynamics.",
                "This behavior did not occur in our second observation model, since it significantly reduced the set of eligible courses of action-essentially contributing to tie-breaking among them. 6.",
                "DISCUSSION The design of the EMT solution for the Tag Game exposes the core difference in approach to planning and control between EMT or DBC, on the one hand, and the more familiar POMDP approach, on the other.",
                "POMDP defines a reward structure to optimize, and influences system dynamics indirectly through that optimization.",
                "EMT discards any reward scheme, and instead measures and influences system dynamics directly. 2 Entropy was calculated using log base equal to the number of possible locations within the domain; this properly scales entropy expression into the range [0, 1] for all domains.",
                "Thus for the Tag Game, we did not search for a reward function that would encode and express our preference over the agents behavior, but rather directly set three (heuristic) behavior preferences as the basis for target dynamics to be maintained.",
                "Experimental data shows that these targets need not be directly achievable via the agents actions.",
                "However, the ratio between EMT performance and achievability of target dynamics remains to be explored.",
                "The tag game experiment data also revealed the different emphasis DBC and POMDPs place on the formulation of an environment state space.",
                "POMDPs rely entirely on the mechanism of reward accumulation maximization, i.e., formation of the action selection procedure to achieve necessary state sequencing.",
                "DBC, on the other hand, has two sources of sequencing specification: through the properties of an action selection procedure, and through direct specification within the target dynamics.",
                "The importance of the second source was underlined by the Tag Game experiment data, in which the greedy EMT algorithm, applied to a POMDP-type state space specification, failed, since target description over such a state space was incapable of encoding the necessary behavior tendencies, e.g., tie-breaking and commitment to directed motion.",
                "The structural differences between DBC (and EMT in particular), and POMDPs, prohibits direct performance comparison, and places them on complementary tracks, each within a suitable niche.",
                "For instance, POMDPs could be seen as a much more natural formulation of economic sequential decision-making problems, while EMT is a better fit for continual demand for stochastic change, as happens in many robotic or embodied-agent problems.",
                "The complementary properties of POMDPs and EMT can be further exploited.",
                "There is recent interest in using POMDPs in hybrid solutions [17], in which the POMDPs can be used together with other control approaches to provide results not easily achievable with either approach by itself.",
                "DBC can be an effective partner in such a hybrid solution.",
                "For instance, POMDPs have prohibitively large off-line time requirements for policy computation, but can be readily used in simpler settings to expose beneficial behavioral trends; this can serve as a form of target dynamics that are provided to EMT in a larger domain for on-line operation. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have presented a novel perspective on the process of planning and control in stochastic environments, in the form of the Dynamics Based Control (DBC) framework.",
                "DBC formulates the task of planning as support of a specified target system dynamics, which describes the necessary properties of change within the environment.",
                "Optimality of DBC plans of action are measured The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 4: Observation Model I: Omniposition quarry.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 5: Observation Model II: quarry not observed at hunters position.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. with respect to the deviation of actual system dynamics from the target dynamics.",
                "We show that a recently developed technique of Extended Markov Tracking (EMT) [13] is an instantiation of DBC.",
                "In fact, EMT can be seen as a specific case of DBC parameterization, which employs a greedy action selection procedure.",
                "Since EMT exhibits the key features of the general DBC framework, as well as polynomial time complexity, we used the multitarget version of EMT [15] to demonstrate that the class of area sweeping problems naturally lends itself to dynamics-based descriptions, as instantiated by our experiments in the Tag Game domain.",
                "As enumerated in Section 4.2, EMT has a number of limitations, such as difficulty in dealing with negative dynamic preference.",
                "This prevents direct application of EMT to such problems as Rendezvous-Evasion Games (e.g., [6]).",
                "However, DBC in general has no such limitations, and readily enables the formulation of evasion games.",
                "In future work, we intend to proceed with the development of dynamics-based controllers for these problems. 8.",
                "ACKNOWLEDGMENT The work of the first two authors was partially supported by Israel Science Foundation grant #898/05, and the third author was partially supported by a grant from Israels Ministry of Science and Technology. 9.",
                "REFERENCES [1] R. C. Arkin.",
                "Behavior-Based Robotics.",
                "MIT Press, 1998. [2] J.",
                "A. Bilmes.",
                "A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and Hidden Markov Models.",
                "Technical Report TR-97-021, Department of Electrical Engeineering and Computer Science, University of California at Berkeley, 1998. [3] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of information theory.",
                "Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz, and M. J. Wolverton.",
                "A survey of research in distributed, continual planning.",
                "AI Magazine, 4:13-22, 1999. [5] V. R. Konda and J. N. Tsitsiklis.",
                "Actor-Critic algorithms.",
                "SIAM Journal on Control and Optimization, 42(4):1143-1166, 2003. [6] W. S. Lim.",
                "A rendezvous-evasion game on discrete locations with joint randomization.",
                "Advances in Applied Probability, 29(4):1004-1017, December 1997. [7] M. L. Littman, T. L. Dean, and L. P. Kaelbling.",
                "On the complexity of solving Markov decision problems.",
                "In Proceedings of the 11th Annual Conference on Uncertainty in Artificial Intelligence (UAI-95), pages 394-402, 1995. [8] O. Madani, S. Hanks, and A. Condon.",
                "On the undecidability of probabilistic planning and related stochastic optimization problems.",
                "Artificial Intelligence Journal, 147(1-2):5-34, July 2003. [9] R. M. Neal and G. E. Hinton.",
                "A view of the EM algorithm 796 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) that justifies incremental, sparse, and other variants.",
                "In M. I. Jordan, editor, Learning in Graphical Models, pages 355-368.",
                "Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In Proceeding of AAMAS 2006, 2006. [11] J. Pineau, G. Gordon, and S. Thrun.",
                "Point-based value iteration: An anytime algorithm for pomdps.",
                "In International Joint Conference on Artificial Intelligence (IJCAI), pages 1025-1032, August 2003. [12] M. L. Puterman.",
                "Markov Decision Processes.",
                "Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics Section.",
                "Wiley-Interscience Publication, New York, 1994. [13] Z. Rabinovich and J. S. Rosenschein.",
                "Extended Markov Tracking with an application to control.",
                "In The Workshop on Agent Tracking: Modeling Other Agents from Observations, at the Third International Joint Conference on Autonomous Agents and Multiagent Systems, pages 95-100, New-York, July 2004. [14] Z. Rabinovich and J. S. Rosenschein.",
                "Multiagent coordination by Extended Markov Tracking.",
                "In The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 431-438, Utrecht, The Netherlands, July 2005. [15] Z. Rabinovich and J. S. Rosenschein.",
                "On the response of EMT-based control to interacting targets and models.",
                "In The Fifth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 465-470, Hakodate, Japan, May 2006. [16] R. F. Stengel.",
                "Optimal Control and Estimation.",
                "Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, and P. Varakantham.",
                "Conflicts in teamwork: Hybrids to the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 797"
            ],
            "original_annotated_samples": [
                "DYNAMICS BASED CONTROL The specification of Dynamics Based Control (DBC) can be broken into three interacting levels: <br>environment design level</br>, User Level, and Agent Level. • <br>environment design level</br> is concerned with the formal specification and modeling of the environment.",
                "In the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping. • Agent Level in turn combines the environment model from the <br>environment design level</br>, the dynamics estimation procedure, the deviation measure and the target dynamics specification from User Level, to produce a sequence of actions that create system dynamics as close as possible to the targeted specification.",
                "The other alternative would be to rely on the estimation procedure provided by the User Level-to utilize the <br>environment design level</br> model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a certain dynamics has been achieved.",
                "Data would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the User Level, this can provide an important tool for the environment model calibration at the <br>environment design level</br>.",
                "Notice how DBC discards rewards, and replaces it by another optimality criterion (structural differences are summarized in Table 1): • <br>environment design level</br> is to specify a tuple < S, A, T, O, Ω, s0 >, where: - S is the set of all possible environment states; - s0 is the initial state of the environment (which can also be viewed as a probability distribution over S); The Sixth Intl."
            ],
            "translated_annotated_samples": [
                "CONTROL BASADO EN DINÁMICA La especificación del Control Basado en Dinámica (DBC) se puede dividir en tres niveles interactivos: Nivel de Diseño del Entorno, Nivel de Usuario y Nivel de Agente. • El Nivel de Diseño del Entorno se ocupa de la especificación formal y modelado del entorno.",
                "En el escenario del guardia de museo mencionado anteriormente, estos corresponderían a un horario de barrido estocástico y una medida de sorpresa relativa entre el barrido especificado y el real. • El Nivel de Agente, a su vez, combina el modelo del entorno del <br>Nivel de Diseño de Entorno</br>, el procedimiento de estimación de la dinámica, la medida de desviación y la especificación de la dinámica objetivo del Nivel de Usuario, para producir una secuencia de acciones que creen dinámicas del sistema lo más cercanas posible a la especificación deseada.",
                "La otra alternativa sería depender del procedimiento de estimación proporcionado por el Nivel de Usuario para utilizar el modelo del Nivel de Diseño del Entorno del entorno para elegir acciones, de manera que se manipule al estimador de dinámicas para que crea que se ha logrado cierta dinámica.",
                "Los datos también estarían disponibles sobre la respuesta del sistema a diferentes acciones realizadas; combinados con un estimador de dinámica definido por el Nivel de Usuario, esto puede proporcionar una herramienta importante para la calibración del modelo del entorno en el Nivel de Diseño del Entorno.",
                "Observa cómo DBC descarta las recompensas y las reemplaza por otro criterio de optimalidad (las diferencias estructurales se resumen en la Tabla 1): • El <br>nivel de Diseño del Entorno</br> es especificar una tupla < S, A, T, O, Ω, s0 >, donde: - S es el conjunto de todos los posibles estados del entorno; - s0 es el estado inicial del entorno (que también puede ser visto como una distribución de probabilidad sobre S); La Sexta Conferencia Internacional."
            ],
            "translated_text": "En este artículo presentamos el Control Basado en Dinámicas (DBC), un enfoque para la planificación y control de un agente en entornos estocásticos. A diferencia de los enfoques existentes, que buscan optimizar las recompensas esperadas (por ejemplo, en Problemas de Decisión de Markov Parcialmente Observables (POMDP)), DBC optimiza el comportamiento del sistema hacia las dinámicas del sistema especificadas. Mostramos que un enfoque de planificación y control recientemente desarrollado, Seguimiento Extendido de Markov (EMT), es una instancia de DBC. EMT emplea la selección de acciones codiciosa para proporcionar un algoritmo de control eficiente en entornos markovianos. Explotamos esta eficiencia en un conjunto de experimentos que aplicaron EMT multitarget a una clase de problemas de barrido de área (búsqueda de objetivos en movimiento). Mostramos que tales problemas pueden ser definidos de forma natural y resueltos eficientemente utilizando el marco de DBC y su instanciación de EMT. Categorías y Descriptores de Asignaturas I.2.8 [Resolución de Problemas, Métodos de Control y Búsqueda]: Teoría de Control; I.2.9 [Robótica]; I.2.11 [Inteligencia Artificial Distribuida]: Agentes Inteligentes Términos Generales Algoritmos, Teoría 1. INTRODUCCIÓN La planificación y el control constituyen un área de investigación central en sistemas multiagentes e inteligencia artificial. En los últimos años, los Procesos de Decisión de Markov Parcialmente Observables (POMDPs) [12] se han convertido en una base formal popular para la planificación en entornos estocásticos. En este marco, el problema de planificación y control suele abordarse imponiendo una función de recompensa y calculando una política (de elección de acciones) que sea óptima, en el sentido de que resultará en la mayor utilidad esperada. Si bien teóricamente atractivo, la complejidad de resolver óptimamente un POMDP es prohibitiva [8, 7]. Tomamos una visión alternativa de la planificación en entornos estocásticos. No utilizamos una función de recompensa basada en el estado, sino que optimizamos según un criterio diferente, una especificación basada en transiciones de la dinámica del sistema deseado. La idea aquí es ver la ejecución del plan como un proceso que obliga a un sistema (estocástico) a cambiar, y un plan como un proceso dinámico que moldea ese cambio de acuerdo con criterios deseados. Llamamos a este marco de planificación general Control Basado en Dinámicas (DBC). En DBC, el objetivo de un proceso de planificación (o control) es garantizar que el sistema cambiará de acuerdo con dinámicas objetivo específicas (potencialmente estocásticas). Dado que el comportamiento real del sistema puede desviarse de lo especificado por la dinámica objetivo (debido a la naturaleza estocástica del sistema), la planificación en tales entornos debe ser continua, de manera similar a los controladores en lazo cerrado clásicos. Aquí, la optimalidad se mide en términos de la probabilidad de magnitudes de desviación. En este documento, presentamos la estructura del Control Basado en Dinámicas. Mostramos que el enfoque de Seguimiento Extendido de Markov (EMT) recientemente desarrollado [13, 14, 15] está subsumido por DBC, con EMT empleando selección de acciones codiciosa, que es una parametrización específica entre las opciones posibles dentro de DBC. EMT es una implementación eficiente de DBC. Para evaluar DBC, llevamos a cabo un conjunto de experimentos aplicando EMT de múltiples objetivos al Juego de Etiquetas [11]; esta es una variante del problema de barrido de área, donde un agente intenta etiquetar a un objetivo móvil (presa) cuya posición no se conoce con certeza. Los datos experimentales demuestran que incluso con un modelo simple del entorno y un diseño simple de la dinámica del objetivo, se pueden lograr altas tasas de éxito tanto en la captura de la presa como en sorprender a la presa (como se expresa por la entropía observada de la posición de los agentes controlados). El documento está organizado de la siguiente manera. En la Sección 2 motivamos DBC utilizando problemas de barrido de área, y discutimos trabajos relacionados. La Sección 3 introduce la estructura de Control Basado en Dinámicas (DBC) y su especialización en entornos Markovianos. Esto es seguido por una revisión del enfoque de Seguimiento Markov Extendido (EMT) como un régimen de control estructurado por DBC en la Sección 4. Esa sección también discute las limitaciones del control basado en EMT en relación con el marco general de DBC. Los ajustes experimentales y los resultados se presentan a continuación en la Sección 5. La Sección 6 proporciona una breve discusión del enfoque general, y la Sección 7 ofrece algunas observaciones finales y direcciones para trabajos futuros. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2. MOTIVACIÓN Y TRABAJO RELACIONADO Muchos escenarios de la vida real tienen naturalmente una especificación de dinámica de destino estocástica, especialmente en aquellos dominios donde no existe un objetivo final, sino más bien un comportamiento del sistema (con propiedades específicas) que debe ser continuamente respaldado. Por ejemplo, los guardias de seguridad realizan barridos persistentes de un área para detectar cualquier signo de intrusión. Los ladrones astutos intentarán rastrear estos barridos y sincronizar su operación con puntos clave del movimiento de los guardias. Por lo tanto, es recomendable que la dinámica de movimiento de los guardias parezca irregular y aleatoria. El trabajo reciente de Paruchuri et al. [10] ha abordado dicha aleatorización en el contexto de POMDPs de agente único y distribuidos. El objetivo de ese trabajo era generar políticas que proporcionen una medida de aleatorización en la selección de acciones, manteniendo al mismo tiempo las recompensas dentro de niveles aceptables. Nuestro enfoque difiere de este trabajo en que DBC no optimiza recompensas esperadas, de hecho, no consideramos recompensas en absoluto, sino que en su lugar mantenemos dinámicas deseadas (incluyendo, pero no limitado a, la aleatorización). El juego de la mancha es otro ejemplo de la aplicabilidad del enfoque. Fue introducido en el trabajo de Pineau et al. [11]. Hay dos agentes que pueden moverse por un área, la cual está dividida en una cuadrícula. La cuadrícula puede tener celdas bloqueadas (agujeros) en las cuales ningún agente puede moverse. Un agente (el cazador) intenta moverse a una celda ocupada por el otro (la presa), de modo que estén ubicados en el mismo lugar (esto es una etiqueta exitosa). La presa busca evitar al cazador, siempre está al tanto de la posición del cazador, pero no sabe cómo se comportará el cazador, lo que abre la posibilidad de que un cazador sorprenda a la presa. El cazador conoce la ley de movimiento probabilístico de la presa, pero no conoce su ubicación actual. Tag es una instancia de una familia de problemas de barrido de área (persecución-evasión). En [11], el cazador modeló el problema utilizando un POMDP. Se definió una función de recompensa para reflejar el deseo de marcar la presa, y se calculó una política de acción para optimizar la recompensa recolectada con el tiempo. Debido a la complejidad intratable de determinar la política óptima, la política de acción calculada en ese documento fue esencialmente una aproximación. En este artículo, en lugar de formular una función de recompensa, utilizamos EMT para resolver el problema, especificando directamente la dinámica objetivo. De hecho, cualquier problema de búsqueda con movimiento aleatorio, la llamada clase de problemas de barrido de área, puede ser descrito a través de la especificación de la dinámica de dicho sistema objetivo. El Control Basado en Dinámicas proporciona un enfoque natural para resolver estos problemas. CONTROL BASADO EN DINÁMICA La especificación del Control Basado en Dinámica (DBC) se puede dividir en tres niveles interactivos: Nivel de Diseño del Entorno, Nivel de Usuario y Nivel de Agente. • El Nivel de Diseño del Entorno se ocupa de la especificación formal y modelado del entorno. Por ejemplo, este nivel especificaría las leyes de la física dentro del sistema y establecería sus parámetros, como la constante de gravitación. El Nivel de Usuario, a su vez, se basa en el modelo de entorno producido por el Diseño de Entorno para especificar la dinámica del sistema objetivo que desea observar. El Nivel de Usuario también especifica el procedimiento de estimación o aprendizaje para la dinámica del sistema, y la medida de desviación. En el escenario del guardia de museo mencionado anteriormente, estos corresponderían a un horario de barrido estocástico y una medida de sorpresa relativa entre el barrido especificado y el real. • El Nivel de Agente, a su vez, combina el modelo del entorno del <br>Nivel de Diseño de Entorno</br>, el procedimiento de estimación de la dinámica, la medida de desviación y la especificación de la dinámica objetivo del Nivel de Usuario, para producir una secuencia de acciones que creen dinámicas del sistema lo más cercanas posible a la especificación deseada. Dado nuestro interés en el desarrollo continuo de un sistema estocástico, como ocurre en la teoría de control clásica [16] y la planificación continua [4], así como en nuestro ejemplo de recorridos por museos, la pregunta es cómo el Nivel de Agente debe tratar las mediciones de desviación a lo largo del tiempo. Con este fin, utilizamos un umbral de probabilidad, es decir, nos gustaría que el Nivel del Agente maximice la probabilidad de que la medida de desviación permanezca por debajo de cierto umbral. La selección de acciones específicas depende entonces de la formalización del sistema. Una posibilidad sería crear una mezcla de tendencias de sistemas disponibles, similar a lo que sucede en las arquitecturas robóticas basadas en el comportamiento [1]. La otra alternativa sería depender del procedimiento de estimación proporcionado por el Nivel de Usuario para utilizar el modelo del Nivel de Diseño del Entorno del entorno para elegir acciones, de manera que se manipule al estimador de dinámicas para que crea que se ha logrado cierta dinámica. Ten en cuenta que esta manipulación no es directa, sino a través del entorno. Por lo tanto, para algoritmos de estimación lo suficientemente fuertes, la manipulación exitosa significaría una simulación exitosa de la dinámica objetivo especificada (es decir, más allá de discernir a través de la entrada sensorial disponible). Los niveles de DBC también pueden tener un retroceso de información (ver Figura 1). Por ejemplo, el Nivel de Agente podría proporcionar datos sobre la viabilidad de la dinámica del objetivo, permitiendo al Nivel de Usuario modificar el requisito, quizás centrándose en las características alcanzables del comportamiento del sistema. Los datos también estarían disponibles sobre la respuesta del sistema a diferentes acciones realizadas; combinados con un estimador de dinámica definido por el Nivel de Usuario, esto puede proporcionar una herramienta importante para la calibración del modelo del entorno en el Nivel de Diseño del Entorno. UserEnv. El modelo de agente de diseño Estimador de Dinámicas Ideales Estimador de Dinámicas Factibilidad de Respuesta del Sistema de Datos Figura 1: Flujo de datos del marco de trabajo DBC Ampliando la idea de los algoritmos Actor-Crítico [5], el flujo de datos de DBC puede proporcionar una buena base para el diseño de un algoritmo de aprendizaje. Por ejemplo, el Nivel de Usuario puede funcionar como un dispositivo exploratorio para un algoritmo de aprendizaje, inferir un objetivo dinámico ideal a partir del modelo del entorno en cuestión que expondría y verificaría las características más críticas del comportamiento del sistema. En este caso, los datos de viabilidad y respuesta del sistema a nivel de agente proporcionarían información clave para una actualización del modelo del entorno. De hecho, la combinación de datos de viabilidad y respuesta puede proporcionar una base para la aplicación de algoritmos de aprendizaje sólidos como EM [2, 9]. 3.1 DBC para entornos markovianos Para un entorno markoviano parcialmente observable, DBC puede especificarse de una manera más rigurosa. Observa cómo DBC descarta las recompensas y las reemplaza por otro criterio de optimalidad (las diferencias estructurales se resumen en la Tabla 1): • El <br>nivel de Diseño del Entorno</br> es especificar una tupla < S, A, T, O, Ω, s0 >, donde: - S es el conjunto de todos los posibles estados del entorno; - s0 es el estado inicial del entorno (que también puede ser visto como una distribución de probabilidad sobre S); La Sexta Conferencia Internacional. ",
            "candidates": [],
            "error": [
                [
                    "Nivel de Diseño de Entorno",
                    "nivel de Diseño del Entorno"
                ]
            ]
        },
        "user level": {
            "translated_key": "Nivel de Usuario",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Dynamics Based Control with an Application to Area-Sweeping Problems Zinovi Rabinovich Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel jeff@cs.huji.ac.il Gal A. Kaminka The MAVERICK Group Department of Computer Science Bar Ilan University, Israel galk@cs.biu.ac.il ABSTRACT In this paper we introduce Dynamics Based Control (DBC), an approach to planning and control of an agent in stochastic environments.",
                "Unlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics.",
                "We show that a recently developed planning and control approach, Extended Markov Tracking (EMT) is an instantiation of DBC.",
                "EMT employs greedy action selection to provide an efficient control algorithm in Markovian environments.",
                "We exploit this efficiency in a set of experiments that applied multitarget EMT to a class of area-sweeping problems (searching for moving targets).",
                "We show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation.",
                "Categories and Subject Descriptors I.2.8 [Problem Solving, Control Methods, and Search]: Control Theory; I.2.9 [Robotics]; I.2.11 [Distributed Artificial Intelligence]: Intelligent Agents General Terms Algorithms, Theory 1.",
                "INTRODUCTION Planning and control constitutes a central research area in multiagent systems and artificial intelligence.",
                "In recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in stochastic environments.",
                "In this framework, the planning and control problem is often addressed by imposing a reward function, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility.",
                "While theoretically attractive, the complexity of optimally solving a POMDP is prohibitive [8, 7].",
                "We take an alternative view of planning in stochastic environments.",
                "We do not use a (state-based) reward function, but instead optimize over a different criterion, a transition-based specification of the desired system dynamics.",
                "The idea here is to view planexecution as a process that compels a (stochastic) system to change, and a plan as a dynamic process that shapes that change according to desired criteria.",
                "We call this general planning framework Dynamics Based Control (DBC).",
                "In DBC, the goal of a planning (or control) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) target dynamics.",
                "As actual system behavior may deviate from that which is specified by target dynamics (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16].",
                "Here, optimality is measured in terms of probability of deviation magnitudes.",
                "In this paper, we present the structure of Dynamics Based Control.",
                "We show that the recently developed Extended Markov Tracking (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC.",
                "EMT is an efficient instantiation of DBC.",
                "To evaluate DBC, we carried out a set of experiments applying multi-target EMT to the Tag Game [11]; this is a variant on the area sweeping problem, where an agent is trying to tag a moving target (quarry) whose position is not known with certainty.",
                "Experimental data demonstrates that even with a simple model of the environment and a simple design of target dynamics, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agents position).",
                "The paper is organized as follows.",
                "In Section 2 we motivate DBC using area-sweeping problems, and discuss related work.",
                "Section 3 introduces the Dynamics Based Control (DBC) structure, and its specialization to Markovian environments.",
                "This is followed by a review of the Extended Markov Tracking (EMT) approach as a DBC-structured control regimen in Section 4.",
                "That section also discusses the limitations of EMT-based control relative to the general DBC framework.",
                "Experimental settings and results are then presented in Section 5.",
                "Section 6 provides a short discussion of the overall approach, and Section 7 gives some concluding remarks and directions for future work. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "MOTIVATION AND RELATED WORK Many real-life scenarios naturally have a stochastic target dynamics specification, especially those domains where there exists no ultimate goal, but rather system behavior (with specific properties) that has to be continually supported.",
                "For example, security guards perform persistent sweeps of an area to detect any sign of intrusion.",
                "Cunning thieves will attempt to track these sweeps, and time their operation to key points of the guards motion.",
                "It is thus advisable to make the guards motion dynamics appear irregular and random.",
                "Recent work by Paruchuri et al. [10] has addressed such randomization in the context of single-agent and distributed POMDPs.",
                "The goal in that work was to generate policies that provide a measure of action-selection randomization, while maintaining rewards within some acceptable levels.",
                "Our focus differs from this work in that DBC does not optimize expected rewards-indeed we do not consider rewards at all-but instead maintains desired dynamics (including, but not limited to, randomization).",
                "The Game of Tag is another example of the applicability of the approach.",
                "It was introduced in the work by Pineau et al. [11].",
                "There are two agents that can move about an area, which is divided into a grid.",
                "The grid may have blocked cells (holes) into which no agent can move.",
                "One agent (the hunter) seeks to move into a cell occupied by the other (the quarry), such that they are co-located (this is a successful tag).",
                "The quarry seeks to avoid the hunter agent, and is always aware of the hunters position, but does not know how the hunter will behave, which opens up the possibility for a hunter to surprise the prey.",
                "The hunter knows the quarrys probabilistic law of motion, but does not know its current location.",
                "Tag is an instance of a family of area-sweeping (pursuit-evasion) problems.",
                "In [11], the hunter modeled the problem using a POMDP.",
                "A reward function was defined, to reflect the desire to tag the quarry, and an action policy was computed to optimize the reward collected over time.",
                "Due to the intractable complexity of determining the optimal policy, the action policy computed in that paper was essentially an approximation.",
                "In this paper, instead of formulating a reward function, we use EMT to solve the problem, by directly specifying the target dynamics.",
                "In fact, any search problem with randomized motion, the socalled class of area sweeping problems, can be described through specification of such target system dynamics.",
                "Dynamics Based Control provides a natural approach to solving these problems. 3.",
                "DYNAMICS BASED CONTROL The specification of Dynamics Based Control (DBC) can be broken into three interacting levels: Environment Design Level, <br>user level</br>, and Agent Level. • Environment Design Level is concerned with the formal specification and modeling of the environment.",
                "For example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation constant. • <br>user level</br> in turn relies on the environment model produced by Environment Design to specify the target system dynamics it wishes to observe.",
                "The <br>user level</br> also specifies the estimation or learning procedure for system dynamics, and the measure of deviation.",
                "In the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping. • Agent Level in turn combines the environment model from the Environment Design level, the dynamics estimation procedure, the deviation measure and the target dynamics specification from <br>user level</br>, to produce a sequence of actions that create system dynamics as close as possible to the targeted specification.",
                "As we are interested in the continual development of a stochastic system, such as happens in classical control theory [16] and continual planning [4], as well as in our example of museum sweeps, the question becomes how the Agent Level is to treat the deviation measurements over time.",
                "To this end, we use a probability threshold-that is, we would like the Agent Level to maximize the probability that the deviation measure will remain below a certain threshold.",
                "Specific action selection then depends on system formalization.",
                "One possibility would be to create a mixture of available system trends, much like that which happens in Behavior-Based Robotic architectures [1].",
                "The other alternative would be to rely on the estimation procedure provided by the <br>user level</br>-to utilize the Environment Design Level model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a certain dynamics has been achieved.",
                "Notice that this manipulation is not direct, but via the environment.",
                "Thus, for strong enough estimator algorithms, successful manipulation would mean a successful simulation of the specified target dynamics (i.e., beyond discerning via the available sensory input).",
                "DBC levels can also have a back-flow of information (see Figure 1).",
                "For instance, the Agent Level could provide data about target dynamics feasibility, allowing the <br>user level</br> to modify the requirement, perhaps focusing on attainable features of system behavior.",
                "Data would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the <br>user level</br>, this can provide an important tool for the environment model calibration at the Environment Design Level.",
                "UserEnv.",
                "Design Agent Model Ideal Dynamics Estimator Estimator Dynamics Feasibility System Response Data Figure 1: Data flow of the DBC framework Extending upon the idea of Actor-Critic algorithms [5], DBC data flow can provide a good basis for the design of a learning algorithm.",
                "For example, the <br>user level</br> can operate as an exploratory device for a learning algorithm, inferring an ideal dynamics target from the environment model at hand that would expose and verify most critical features of system behavior.",
                "In this case, feasibility and system response data from the Agent Level would provide key information for an environment model update.",
                "In fact, the combination of feasibility and response data can provide a basis for the application of strong learning algorithms such as EM [2, 9]. 3.1 DBC for Markovian Environments For a Partially Observable Markovian Environment, DBC can be specified in a more rigorous manner.",
                "Notice how DBC discards rewards, and replaces it by another optimality criterion (structural differences are summarized in Table 1): • Environment Design level is to specify a tuple < S, A, T, O, Ω, s0 >, where: - S is the set of all possible environment states; - s0 is the initial state of the environment (which can also be viewed as a probability distribution over S); The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 791 - A is the set of all possible actions applicable in the environment; - T is the environments probabilistic transition function: T : S ×A → Π(S).",
                "That is, T(s |a, s) is the probability that the environment will move from state s to state s under action a; - O is the set of all possible observations.",
                "This is what the sensor input would look like for an outside observer; - Ω is the observation probability function: Ω : S × A × S → Π(O).",
                "That is, Ω(o|s , a, s) is the probability that one will observe o given that the environment has moved from state s to state s under action a. • <br>user level</br>, in the case of Markovian environment, operates on the set of system dynamics described by a family of conditional probabilities F = {τ : S × A → Π(S)}.",
                "Thus specification of target dynamics can be expressed by q ∈ F, and the learning or tracking algorithm can be represented as a function L : O×(A×O)∗ → F; that is, it maps sequences of observations and actions performed so far into an estimate τ ∈ F of system dynamics.",
                "There are many possible variations available at the <br>user level</br> to define divergence between system dynamics; several of them are: - Trace distance or L1 distance between two distributions p and q defined by D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Fidelity measure of distance F(p(·), q(·)) = x p(x)q(x) - Kullback-Leibler divergence DKL(p(·) q(·)) = x p(x) log p(x) q(x) Notice that the latter two are not actually metrics over the space of possible distributions, but nevertheless have meaningful and important interpretations.",
                "For instance, KullbackLeibler divergence is an important tool of information theory [3] that allows one to measure the price of encoding an information source governed by q, while assuming that it is governed by p. The <br>user level</br> also defines the threshold of dynamics deviation probability θ. • Agent Level is then faced with a problem of selecting a control signal function a∗ to satisfy a minimization problem as follows: a∗ = arg min a Pr(d(τa, q) > θ) where d(τa, q) is a random variable describing deviation of the dynamics estimate τa, created by L under control signal a, from the ideal dynamics q.",
                "Implicit in this minimization problem is that L is manipulated via the environment, based on the environment model produced by the Environment Design Level. 3.2 DBC View of the State Space It is important to note the complementary view that DBC and POMDPs take on the state space of the environment.",
                "POMDPs regard state as a stationary snap-shot of the environment; whatever attributes of state sequencing one seeks are reached through properties of the control process, in this case reward accumulation.",
                "This can be viewed as if the sequencing of states and the attributes of that sequencing are only introduced by and for the controlling mechanism-the POMDP policy.",
                "DBC concentrates on the underlying principle of state sequencing, the system dynamics.",
                "DBCs target dynamics specification can use the environments state space as a means to describe, discern, and preserve changes that occur within the system.",
                "As a result, DBC has a greater ability to express state sequencing properties, which are grounded in the environment model and its state space definition.",
                "For example, consider the task of moving through rough terrain towards a goal and reaching it as fast as possible.",
                "POMDPs would encode terrain as state space points, while speed would be ensured by negative reward for every step taken without reaching the goalaccumulating higher reward can be reached only by faster motion.",
                "Alternatively, the state space could directly include the notion of speed.",
                "For POMDPs, this would mean that the same concept is encoded twice, in some sense: directly in the state space, and indirectly within reward accumulation.",
                "Now, even if the reward function would encode more, and finer, details of the properties of motion, the POMDP solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure.",
                "On the other hand, the tactical target expression of variations and correlations between position and speed of motion is now grounded in the state space representation.",
                "In this situation, any further constraints, e.g., smoothness of motion, speed limits in different locations, or speed reductions during sharp turns, are explicitly and uniformly expressed by the tactical target, and can result in faster and more effective action selection by a DBC algorithm. 4.",
                "EMT-BASED CONTROL AS A DBC Recently, a control algorithm was introduced called EMT-based Control [13], which instantiates the DBC framework.",
                "Although it provides an approximate greedy solution in the DBC sense, initial experiments using EMT-based control have been encouraging [14, 15].",
                "EMT-based control is based on the Markovian environment definition, as in the case with POMDPs, but its User and Agent Levels are of the Markovian DBC type of optimality. • <br>user level</br> of EMT-based control defines a limited-case target system dynamics independent of action: qEMT : S → Π(S).",
                "It then utilizes the Kullback-Leibler divergence measure to compose a momentary system dynamics estimator-the Extended Markov Tracking (EMT) algorithm.",
                "The algorithm keeps a system dynamics estimate τt EMT that is capable of explaining recent change in an auxiliary Bayesian system state estimator from pt−1 to pt, and updates it conservatively using Kullback-Leibler divergence.",
                "Since τt EMT and pt−1,t are respectively the conditional and marginal probabilities over the systems state space, explanation simply means that pt(s ) = s τt EMT (s |s)pt−1(s), and the dynamics estimate update is performed by solving a 792 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: Structure of POMDP vs. Dynamics-Based Control in Markovian Environment Level Approach MDP Markovian DBC Environment < S, A, T, O, Ω >,where S - set of states A - set of actions Design T : S × A → Π(S) - transition O - observation set Ω : S × A × S → Π(O) User r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - reward function q - ideal dynamics F - reward remodeling L - dynamics estimator θ - threshold Agent π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) minimization problem: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • Agent Level in EMT-based control is suboptimal with respect to DBC (though it remains within the DBC framework), performing greedy action selection based on prediction of EMTs reaction.",
                "The prediction is based on the environment model provided by the Environment Design level, so that if we denote by Ta the environments transition function limited to action a, and pt−1 is the auxiliary Bayesian system state estimator, then the EMT-based control choice is described by a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1) Note that this follows the Markovian DBC framework precisely: the rewarding optimality of POMDPs is discarded, and in its place a dynamics estimator (EMT in this case) is manipulated via action effects on the environment to produce an estimate close to the specified target system dynamics.",
                "Yet as we mentioned, naive EMTbased control is suboptimal in the DBC sense, and has several additional limitations that do not exist in the general DBC framework (discussed in Section 4.2). 4.1 Multi-Target EMT At times, there may exist several behavioral preferences.",
                "For example, in the case of museum guards, some art items are more heavily guarded, requiring that the guards stay more often in their close vicinity.",
                "On the other hand, no corner of the museum is to be left unchecked, which demands constant motion.",
                "Successful museum security would demand that the guards adhere to, and balance, both of these behaviors.",
                "For EMT-based control, this would mean facing several tactical targets {qk}K k=1, and the question becomes how to merge and balance them.",
                "A balancing mechanism can be applied to resolve this issue.",
                "Note that EMT-based control, while selecting an action, creates a preference vector over the set of actions based on their predicted performance with respect to a given target.",
                "If these preference vectors are normalized, they can be combined into a single unified preference.",
                "This requires replacement of standard EMT-based action selection by the algorithm below [15]: • Given: - a set of target dynamics {qk}K k=1, - vector of weights w(k) • Select action as follows - For each action a ∈ A predict the future state distribution ¯pa t+1 = Ta ∗ pt; - For each action, compute Da = H(¯pa t+1, pt, PDt) - For each a ∈ A and qk tactical target, denote V (a, k) = DKL (Da qk) pt .",
                "Let Vk(a) = 1 Zk V (a, k), where Zk = a∈A V (a, k) is a normalization factor. - Select a∗ = arg min a k k=1 w(k)Vk(a) The weights vector w = (w1, ..., wK ) allows the additional tuning of importance among target dynamics without the need to redesign the targets themselves.",
                "This balancing method is also seamlessly integrated into the EMT-based control flow of operation. 4.2 EMT-based Control Limitations EMT-based control is a sub-optimal (in the DBC sense) representative of the DBC structure.",
                "It limits the User by forcing EMT to be its dynamic tracking algorithm, and replaces Agent optimization by greedy action selection.",
                "This kind of combination, however, is common for on-line algorithms.",
                "Although further development of EMT-based controllers is necessary, evidence so far suggests that even the simplest form of the algorithm possesses a great deal of power, and displays trends that are optimal in the DBC sense of the word.",
                "There are two further, EMT-specific, limitations to EMT-based control that are evident at this point.",
                "Both already have partial solutions and are subjects of ongoing research.",
                "The first limitation is the problem of negative preference.",
                "In the POMDP framework for example, this is captured simply, through The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 793 the appearance of values with different signs within the reward structure.",
                "For EMT-based control, however, negative preference means that one would like to avoid a certain distribution over system development sequences; EMT-based control, however, concentrates on getting as close as possible to a distribution.",
                "Avoidance is thus unnatural in native EMT-based control.",
                "The second limitation comes from the fact that standard environment modeling can create pure sensory actions-actions that do not change the state of the world, and differ only in the way observations are received and the quality of observations received.",
                "Since the world state does not change, EMT-based control would not be able to differentiate between different sensory actions.",
                "Notice that both of these limitations of EMT-based control are absent from the general DBC framework, since it may have a tracking algorithm capable of considering pure sensory actions and, unlike Kullback-Leibler divergence, a distribution deviation measure that is capable of dealing with negative preference. 5.",
                "EMT PLAYING TAG The Game of Tag was first introduced in [11].",
                "It is a single agent problem of capturing a quarry, and belongs to the class of area sweeping problems.",
                "An example domain is shown in Figure 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figure 2: Tag domain; an agent (A) attempts to seek and capture a quarry (Q) The Game of Tag extremely limits the agents perception, so that the agent is able to detect the quarry only if they are co-located in the same cell of the grid world.",
                "In the classical version of the game, co-location leads to a special observation, and the Tag action can be performed.",
                "We slightly modify this setting: the moment that both agents occupy the same cell, the game ends.",
                "As a result, both the agent and its quarry have the same motion capability, which allows them to move in four directions, North, South, East, and West.",
                "These form a formal space of actions within a Markovian environment.",
                "The state space of the formal Markovian environment is described by the cross-product of the agent and quarrys positions.",
                "For Figure 2, it would be S = {s0, ..., s23} × {s0, ..., s23}.",
                "The effects of an action taken by the agent are deterministic, but the environment in general has a stochastic response due to the motion of the quarry.",
                "With probability q0 1 it stays put, and with probability 1 − q0 it moves to an adjacent cell further away from the 1 In our experiments this was taken to be q0 = 0.2. agent.",
                "So for the instance shown in Figure 2 and q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Although the evasive behavior of the quarry is known to the agent, the quarrys position is not.",
                "The only sensory information available to the agent is its own location.",
                "We use EMT and directly specify the target dynamics.",
                "For the Game of Tag, we can easily formulate three major trends: catching the quarry, staying mobile, and stalking the quarry.",
                "This results in the following three target dynamics: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 otherwise Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 otherwise Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Note that none of the above targets are directly achievable; for instance, if Qt = s9 and At = s11, there is no action that can move the agent to At+1 = s9 as required by the Tcatch target dynamics.",
                "We ran several experiments to evaluate EMT performance in the Tag Game.",
                "Three configurations of the domain shown in Figure 3 were used, each posing a different challenge to the agent due to partial observability.",
                "In each setting, a set of 1000 runs was performed with a time limit of 100 steps.",
                "In every run, the initial position of both the agent and its quarry was selected at random; this means that as far as the agent was concerned, the quarrys initial position was uniformly distributed over the entire domain cell space.",
                "We also used two variations of the environment observability function.",
                "In the first version, observability function mapped all joint positions of hunter and quarry into the position of the hunter as an observation.",
                "In the second, only those joint positions in which hunter and quarry occupied different locations were mapped into the hunters location.",
                "The second version in fact utilized and expressed the fact that once hunter and quarry occupy the same cell the game ends.",
                "The results of these experiments are shown in Table 2.",
                "Balancing [15] the catch, move, and stalk target dynamics described in the previous section by the weight vector [0.8, 0.1, 0.1], EMT produced stable performance in all three domains.",
                "Although direct comparisons are difficult to make, the EMT performance displayed notable efficiency vis-`a-vis the POMDP approach.",
                "In spite of a simple and inefficient Matlab implementation of the EMT algorithm, the decision time for any given step averaged significantly below 1 second in all experiments.",
                "For the irregular open arena domain, which proved to be the most difficult, 1000 experiment runs bounded by 100 steps each, a total of 42411 steps, were completed in slightly under 6 hours.",
                "That is, over 4 × 104 online steps took an order of magnitude less time than the offline computation of POMDP policy in [11].",
                "The significance of this differential is made even more prominent by the fact that, should the environment model parameters change, the online nature of EMT would allow it to maintain its performance time, while the POMDP policy would need to be recomputed, requiring yet again a large overhead of computation.",
                "We also tested the behavior cell frequency entropy, empirical measures from trial data.",
                "As Figure 4 and Figure 5 show, empir794 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) A Q Q A 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 A Q Figure 3: These configurations of the Tag Game space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor Table 2: Performance of the EMT-based solution in three Tag Game domains and two observability models: I) omniposition quarry, II) quarry is not at hunters position Model Domain Capture% E(Steps) Time/Step I Dead-ends 100 14.8 72(mSec) Arena 80.2 42.4 500(mSec) Circle 91.4 34.6 187(mSec) II Dead-ends 100 13.2 91(mSec) Arena 96.8 28.67 396(mSec) Circle 94.4 31.63 204(mSec) ical entropy grows with the length of interaction.",
                "For runs where the quarry was not captured immediately, the entropy reaches between 0.85 and 0.952 for different runs and scenarios.",
                "As the agent actively seeks the quarry, the entropy never reaches its maximum.",
                "One characteristic of the entropy graph for the open arena scenario particularly caught our attention in the case of the omniposition quarry observation model.",
                "Near the maximum limit of trial length (100 steps), entropy suddenly dropped.",
                "Further analysis of the data showed that under certain circumstances, a fluctuating behavior occurs in which the agent faces equally viable versions of quarry-following behavior.",
                "Since the EMT algorithm has greedy action selection, and the state space does not encode any form of commitment (not even speed or acceleration), the agent is locked within a small portion of cells.",
                "It is essentially attempting to simultaneously follow several courses of action, all of which are consistent with the target dynamics.",
                "This behavior did not occur in our second observation model, since it significantly reduced the set of eligible courses of action-essentially contributing to tie-breaking among them. 6.",
                "DISCUSSION The design of the EMT solution for the Tag Game exposes the core difference in approach to planning and control between EMT or DBC, on the one hand, and the more familiar POMDP approach, on the other.",
                "POMDP defines a reward structure to optimize, and influences system dynamics indirectly through that optimization.",
                "EMT discards any reward scheme, and instead measures and influences system dynamics directly. 2 Entropy was calculated using log base equal to the number of possible locations within the domain; this properly scales entropy expression into the range [0, 1] for all domains.",
                "Thus for the Tag Game, we did not search for a reward function that would encode and express our preference over the agents behavior, but rather directly set three (heuristic) behavior preferences as the basis for target dynamics to be maintained.",
                "Experimental data shows that these targets need not be directly achievable via the agents actions.",
                "However, the ratio between EMT performance and achievability of target dynamics remains to be explored.",
                "The tag game experiment data also revealed the different emphasis DBC and POMDPs place on the formulation of an environment state space.",
                "POMDPs rely entirely on the mechanism of reward accumulation maximization, i.e., formation of the action selection procedure to achieve necessary state sequencing.",
                "DBC, on the other hand, has two sources of sequencing specification: through the properties of an action selection procedure, and through direct specification within the target dynamics.",
                "The importance of the second source was underlined by the Tag Game experiment data, in which the greedy EMT algorithm, applied to a POMDP-type state space specification, failed, since target description over such a state space was incapable of encoding the necessary behavior tendencies, e.g., tie-breaking and commitment to directed motion.",
                "The structural differences between DBC (and EMT in particular), and POMDPs, prohibits direct performance comparison, and places them on complementary tracks, each within a suitable niche.",
                "For instance, POMDPs could be seen as a much more natural formulation of economic sequential decision-making problems, while EMT is a better fit for continual demand for stochastic change, as happens in many robotic or embodied-agent problems.",
                "The complementary properties of POMDPs and EMT can be further exploited.",
                "There is recent interest in using POMDPs in hybrid solutions [17], in which the POMDPs can be used together with other control approaches to provide results not easily achievable with either approach by itself.",
                "DBC can be an effective partner in such a hybrid solution.",
                "For instance, POMDPs have prohibitively large off-line time requirements for policy computation, but can be readily used in simpler settings to expose beneficial behavioral trends; this can serve as a form of target dynamics that are provided to EMT in a larger domain for on-line operation. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have presented a novel perspective on the process of planning and control in stochastic environments, in the form of the Dynamics Based Control (DBC) framework.",
                "DBC formulates the task of planning as support of a specified target system dynamics, which describes the necessary properties of change within the environment.",
                "Optimality of DBC plans of action are measured The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 4: Observation Model I: Omniposition quarry.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 5: Observation Model II: quarry not observed at hunters position.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. with respect to the deviation of actual system dynamics from the target dynamics.",
                "We show that a recently developed technique of Extended Markov Tracking (EMT) [13] is an instantiation of DBC.",
                "In fact, EMT can be seen as a specific case of DBC parameterization, which employs a greedy action selection procedure.",
                "Since EMT exhibits the key features of the general DBC framework, as well as polynomial time complexity, we used the multitarget version of EMT [15] to demonstrate that the class of area sweeping problems naturally lends itself to dynamics-based descriptions, as instantiated by our experiments in the Tag Game domain.",
                "As enumerated in Section 4.2, EMT has a number of limitations, such as difficulty in dealing with negative dynamic preference.",
                "This prevents direct application of EMT to such problems as Rendezvous-Evasion Games (e.g., [6]).",
                "However, DBC in general has no such limitations, and readily enables the formulation of evasion games.",
                "In future work, we intend to proceed with the development of dynamics-based controllers for these problems. 8.",
                "ACKNOWLEDGMENT The work of the first two authors was partially supported by Israel Science Foundation grant #898/05, and the third author was partially supported by a grant from Israels Ministry of Science and Technology. 9.",
                "REFERENCES [1] R. C. Arkin.",
                "Behavior-Based Robotics.",
                "MIT Press, 1998. [2] J.",
                "A. Bilmes.",
                "A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and Hidden Markov Models.",
                "Technical Report TR-97-021, Department of Electrical Engeineering and Computer Science, University of California at Berkeley, 1998. [3] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of information theory.",
                "Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz, and M. J. Wolverton.",
                "A survey of research in distributed, continual planning.",
                "AI Magazine, 4:13-22, 1999. [5] V. R. Konda and J. N. Tsitsiklis.",
                "Actor-Critic algorithms.",
                "SIAM Journal on Control and Optimization, 42(4):1143-1166, 2003. [6] W. S. Lim.",
                "A rendezvous-evasion game on discrete locations with joint randomization.",
                "Advances in Applied Probability, 29(4):1004-1017, December 1997. [7] M. L. Littman, T. L. Dean, and L. P. Kaelbling.",
                "On the complexity of solving Markov decision problems.",
                "In Proceedings of the 11th Annual Conference on Uncertainty in Artificial Intelligence (UAI-95), pages 394-402, 1995. [8] O. Madani, S. Hanks, and A. Condon.",
                "On the undecidability of probabilistic planning and related stochastic optimization problems.",
                "Artificial Intelligence Journal, 147(1-2):5-34, July 2003. [9] R. M. Neal and G. E. Hinton.",
                "A view of the EM algorithm 796 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) that justifies incremental, sparse, and other variants.",
                "In M. I. Jordan, editor, Learning in Graphical Models, pages 355-368.",
                "Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In Proceeding of AAMAS 2006, 2006. [11] J. Pineau, G. Gordon, and S. Thrun.",
                "Point-based value iteration: An anytime algorithm for pomdps.",
                "In International Joint Conference on Artificial Intelligence (IJCAI), pages 1025-1032, August 2003. [12] M. L. Puterman.",
                "Markov Decision Processes.",
                "Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics Section.",
                "Wiley-Interscience Publication, New York, 1994. [13] Z. Rabinovich and J. S. Rosenschein.",
                "Extended Markov Tracking with an application to control.",
                "In The Workshop on Agent Tracking: Modeling Other Agents from Observations, at the Third International Joint Conference on Autonomous Agents and Multiagent Systems, pages 95-100, New-York, July 2004. [14] Z. Rabinovich and J. S. Rosenschein.",
                "Multiagent coordination by Extended Markov Tracking.",
                "In The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 431-438, Utrecht, The Netherlands, July 2005. [15] Z. Rabinovich and J. S. Rosenschein.",
                "On the response of EMT-based control to interacting targets and models.",
                "In The Fifth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 465-470, Hakodate, Japan, May 2006. [16] R. F. Stengel.",
                "Optimal Control and Estimation.",
                "Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, and P. Varakantham.",
                "Conflicts in teamwork: Hybrids to the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 797"
            ],
            "original_annotated_samples": [
                "DYNAMICS BASED CONTROL The specification of Dynamics Based Control (DBC) can be broken into three interacting levels: Environment Design Level, <br>user level</br>, and Agent Level. • Environment Design Level is concerned with the formal specification and modeling of the environment.",
                "For example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation constant. • <br>user level</br> in turn relies on the environment model produced by Environment Design to specify the target system dynamics it wishes to observe.",
                "The <br>user level</br> also specifies the estimation or learning procedure for system dynamics, and the measure of deviation.",
                "In the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping. • Agent Level in turn combines the environment model from the Environment Design level, the dynamics estimation procedure, the deviation measure and the target dynamics specification from <br>user level</br>, to produce a sequence of actions that create system dynamics as close as possible to the targeted specification.",
                "The other alternative would be to rely on the estimation procedure provided by the <br>user level</br>-to utilize the Environment Design Level model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a certain dynamics has been achieved."
            ],
            "translated_annotated_samples": [
                "CONTROL BASADO EN DINÁMICA La especificación del Control Basado en Dinámica (DBC) se puede dividir en tres niveles interactivos: Nivel de Diseño del Entorno, Nivel de Usuario y Nivel de Agente. • El Nivel de Diseño del Entorno se ocupa de la especificación formal y modelado del entorno.",
                "Por ejemplo, este nivel especificaría las leyes de la física dentro del sistema y establecería sus parámetros, como la constante de gravitación. El Nivel de Usuario, a su vez, se basa en el modelo de entorno producido por el Diseño de Entorno para especificar la dinámica del sistema objetivo que desea observar.",
                "El Nivel de Usuario también especifica el procedimiento de estimación o aprendizaje para la dinámica del sistema, y la medida de desviación.",
                "En el escenario del guardia de museo mencionado anteriormente, estos corresponderían a un horario de barrido estocástico y una medida de sorpresa relativa entre el barrido especificado y el real. • El Nivel de Agente, a su vez, combina el modelo del entorno del Nivel de Diseño de Entorno, el procedimiento de estimación de la dinámica, la medida de desviación y la especificación de la dinámica objetivo del Nivel de Usuario, para producir una secuencia de acciones que creen dinámicas del sistema lo más cercanas posible a la especificación deseada.",
                "La otra alternativa sería depender del procedimiento de estimación proporcionado por el <br>Nivel de Usuario</br> para utilizar el modelo del Nivel de Diseño del Entorno del entorno para elegir acciones, de manera que se manipule al estimador de dinámicas para que crea que se ha logrado cierta dinámica."
            ],
            "translated_text": "En este artículo presentamos el Control Basado en Dinámicas (DBC), un enfoque para la planificación y control de un agente en entornos estocásticos. A diferencia de los enfoques existentes, que buscan optimizar las recompensas esperadas (por ejemplo, en Problemas de Decisión de Markov Parcialmente Observables (POMDP)), DBC optimiza el comportamiento del sistema hacia las dinámicas del sistema especificadas. Mostramos que un enfoque de planificación y control recientemente desarrollado, Seguimiento Extendido de Markov (EMT), es una instancia de DBC. EMT emplea la selección de acciones codiciosa para proporcionar un algoritmo de control eficiente en entornos markovianos. Explotamos esta eficiencia en un conjunto de experimentos que aplicaron EMT multitarget a una clase de problemas de barrido de área (búsqueda de objetivos en movimiento). Mostramos que tales problemas pueden ser definidos de forma natural y resueltos eficientemente utilizando el marco de DBC y su instanciación de EMT. Categorías y Descriptores de Asignaturas I.2.8 [Resolución de Problemas, Métodos de Control y Búsqueda]: Teoría de Control; I.2.9 [Robótica]; I.2.11 [Inteligencia Artificial Distribuida]: Agentes Inteligentes Términos Generales Algoritmos, Teoría 1. INTRODUCCIÓN La planificación y el control constituyen un área de investigación central en sistemas multiagentes e inteligencia artificial. En los últimos años, los Procesos de Decisión de Markov Parcialmente Observables (POMDPs) [12] se han convertido en una base formal popular para la planificación en entornos estocásticos. En este marco, el problema de planificación y control suele abordarse imponiendo una función de recompensa y calculando una política (de elección de acciones) que sea óptima, en el sentido de que resultará en la mayor utilidad esperada. Si bien teóricamente atractivo, la complejidad de resolver óptimamente un POMDP es prohibitiva [8, 7]. Tomamos una visión alternativa de la planificación en entornos estocásticos. No utilizamos una función de recompensa basada en el estado, sino que optimizamos según un criterio diferente, una especificación basada en transiciones de la dinámica del sistema deseado. La idea aquí es ver la ejecución del plan como un proceso que obliga a un sistema (estocástico) a cambiar, y un plan como un proceso dinámico que moldea ese cambio de acuerdo con criterios deseados. Llamamos a este marco de planificación general Control Basado en Dinámicas (DBC). En DBC, el objetivo de un proceso de planificación (o control) es garantizar que el sistema cambiará de acuerdo con dinámicas objetivo específicas (potencialmente estocásticas). Dado que el comportamiento real del sistema puede desviarse de lo especificado por la dinámica objetivo (debido a la naturaleza estocástica del sistema), la planificación en tales entornos debe ser continua, de manera similar a los controladores en lazo cerrado clásicos. Aquí, la optimalidad se mide en términos de la probabilidad de magnitudes de desviación. En este documento, presentamos la estructura del Control Basado en Dinámicas. Mostramos que el enfoque de Seguimiento Extendido de Markov (EMT) recientemente desarrollado [13, 14, 15] está subsumido por DBC, con EMT empleando selección de acciones codiciosa, que es una parametrización específica entre las opciones posibles dentro de DBC. EMT es una implementación eficiente de DBC. Para evaluar DBC, llevamos a cabo un conjunto de experimentos aplicando EMT de múltiples objetivos al Juego de Etiquetas [11]; esta es una variante del problema de barrido de área, donde un agente intenta etiquetar a un objetivo móvil (presa) cuya posición no se conoce con certeza. Los datos experimentales demuestran que incluso con un modelo simple del entorno y un diseño simple de la dinámica del objetivo, se pueden lograr altas tasas de éxito tanto en la captura de la presa como en sorprender a la presa (como se expresa por la entropía observada de la posición de los agentes controlados). El documento está organizado de la siguiente manera. En la Sección 2 motivamos DBC utilizando problemas de barrido de área, y discutimos trabajos relacionados. La Sección 3 introduce la estructura de Control Basado en Dinámicas (DBC) y su especialización en entornos Markovianos. Esto es seguido por una revisión del enfoque de Seguimiento Markov Extendido (EMT) como un régimen de control estructurado por DBC en la Sección 4. Esa sección también discute las limitaciones del control basado en EMT en relación con el marco general de DBC. Los ajustes experimentales y los resultados se presentan a continuación en la Sección 5. La Sección 6 proporciona una breve discusión del enfoque general, y la Sección 7 ofrece algunas observaciones finales y direcciones para trabajos futuros. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2. MOTIVACIÓN Y TRABAJO RELACIONADO Muchos escenarios de la vida real tienen naturalmente una especificación de dinámica de destino estocástica, especialmente en aquellos dominios donde no existe un objetivo final, sino más bien un comportamiento del sistema (con propiedades específicas) que debe ser continuamente respaldado. Por ejemplo, los guardias de seguridad realizan barridos persistentes de un área para detectar cualquier signo de intrusión. Los ladrones astutos intentarán rastrear estos barridos y sincronizar su operación con puntos clave del movimiento de los guardias. Por lo tanto, es recomendable que la dinámica de movimiento de los guardias parezca irregular y aleatoria. El trabajo reciente de Paruchuri et al. [10] ha abordado dicha aleatorización en el contexto de POMDPs de agente único y distribuidos. El objetivo de ese trabajo era generar políticas que proporcionen una medida de aleatorización en la selección de acciones, manteniendo al mismo tiempo las recompensas dentro de niveles aceptables. Nuestro enfoque difiere de este trabajo en que DBC no optimiza recompensas esperadas, de hecho, no consideramos recompensas en absoluto, sino que en su lugar mantenemos dinámicas deseadas (incluyendo, pero no limitado a, la aleatorización). El juego de la mancha es otro ejemplo de la aplicabilidad del enfoque. Fue introducido en el trabajo de Pineau et al. [11]. Hay dos agentes que pueden moverse por un área, la cual está dividida en una cuadrícula. La cuadrícula puede tener celdas bloqueadas (agujeros) en las cuales ningún agente puede moverse. Un agente (el cazador) intenta moverse a una celda ocupada por el otro (la presa), de modo que estén ubicados en el mismo lugar (esto es una etiqueta exitosa). La presa busca evitar al cazador, siempre está al tanto de la posición del cazador, pero no sabe cómo se comportará el cazador, lo que abre la posibilidad de que un cazador sorprenda a la presa. El cazador conoce la ley de movimiento probabilístico de la presa, pero no conoce su ubicación actual. Tag es una instancia de una familia de problemas de barrido de área (persecución-evasión). En [11], el cazador modeló el problema utilizando un POMDP. Se definió una función de recompensa para reflejar el deseo de marcar la presa, y se calculó una política de acción para optimizar la recompensa recolectada con el tiempo. Debido a la complejidad intratable de determinar la política óptima, la política de acción calculada en ese documento fue esencialmente una aproximación. En este artículo, en lugar de formular una función de recompensa, utilizamos EMT para resolver el problema, especificando directamente la dinámica objetivo. De hecho, cualquier problema de búsqueda con movimiento aleatorio, la llamada clase de problemas de barrido de área, puede ser descrito a través de la especificación de la dinámica de dicho sistema objetivo. El Control Basado en Dinámicas proporciona un enfoque natural para resolver estos problemas. CONTROL BASADO EN DINÁMICA La especificación del Control Basado en Dinámica (DBC) se puede dividir en tres niveles interactivos: Nivel de Diseño del Entorno, Nivel de Usuario y Nivel de Agente. • El Nivel de Diseño del Entorno se ocupa de la especificación formal y modelado del entorno. Por ejemplo, este nivel especificaría las leyes de la física dentro del sistema y establecería sus parámetros, como la constante de gravitación. El Nivel de Usuario, a su vez, se basa en el modelo de entorno producido por el Diseño de Entorno para especificar la dinámica del sistema objetivo que desea observar. El Nivel de Usuario también especifica el procedimiento de estimación o aprendizaje para la dinámica del sistema, y la medida de desviación. En el escenario del guardia de museo mencionado anteriormente, estos corresponderían a un horario de barrido estocástico y una medida de sorpresa relativa entre el barrido especificado y el real. • El Nivel de Agente, a su vez, combina el modelo del entorno del Nivel de Diseño de Entorno, el procedimiento de estimación de la dinámica, la medida de desviación y la especificación de la dinámica objetivo del Nivel de Usuario, para producir una secuencia de acciones que creen dinámicas del sistema lo más cercanas posible a la especificación deseada. Dado nuestro interés en el desarrollo continuo de un sistema estocástico, como ocurre en la teoría de control clásica [16] y la planificación continua [4], así como en nuestro ejemplo de recorridos por museos, la pregunta es cómo el Nivel de Agente debe tratar las mediciones de desviación a lo largo del tiempo. Con este fin, utilizamos un umbral de probabilidad, es decir, nos gustaría que el Nivel del Agente maximice la probabilidad de que la medida de desviación permanezca por debajo de cierto umbral. La selección de acciones específicas depende entonces de la formalización del sistema. Una posibilidad sería crear una mezcla de tendencias de sistemas disponibles, similar a lo que sucede en las arquitecturas robóticas basadas en el comportamiento [1]. La otra alternativa sería depender del procedimiento de estimación proporcionado por el <br>Nivel de Usuario</br> para utilizar el modelo del Nivel de Diseño del Entorno del entorno para elegir acciones, de manera que se manipule al estimador de dinámicas para que crea que se ha logrado cierta dinámica. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "agent level": {
            "translated_key": "Nivel de Agente",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Dynamics Based Control with an Application to Area-Sweeping Problems Zinovi Rabinovich Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel jeff@cs.huji.ac.il Gal A. Kaminka The MAVERICK Group Department of Computer Science Bar Ilan University, Israel galk@cs.biu.ac.il ABSTRACT In this paper we introduce Dynamics Based Control (DBC), an approach to planning and control of an agent in stochastic environments.",
                "Unlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics.",
                "We show that a recently developed planning and control approach, Extended Markov Tracking (EMT) is an instantiation of DBC.",
                "EMT employs greedy action selection to provide an efficient control algorithm in Markovian environments.",
                "We exploit this efficiency in a set of experiments that applied multitarget EMT to a class of area-sweeping problems (searching for moving targets).",
                "We show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation.",
                "Categories and Subject Descriptors I.2.8 [Problem Solving, Control Methods, and Search]: Control Theory; I.2.9 [Robotics]; I.2.11 [Distributed Artificial Intelligence]: Intelligent Agents General Terms Algorithms, Theory 1.",
                "INTRODUCTION Planning and control constitutes a central research area in multiagent systems and artificial intelligence.",
                "In recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in stochastic environments.",
                "In this framework, the planning and control problem is often addressed by imposing a reward function, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility.",
                "While theoretically attractive, the complexity of optimally solving a POMDP is prohibitive [8, 7].",
                "We take an alternative view of planning in stochastic environments.",
                "We do not use a (state-based) reward function, but instead optimize over a different criterion, a transition-based specification of the desired system dynamics.",
                "The idea here is to view planexecution as a process that compels a (stochastic) system to change, and a plan as a dynamic process that shapes that change according to desired criteria.",
                "We call this general planning framework Dynamics Based Control (DBC).",
                "In DBC, the goal of a planning (or control) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) target dynamics.",
                "As actual system behavior may deviate from that which is specified by target dynamics (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16].",
                "Here, optimality is measured in terms of probability of deviation magnitudes.",
                "In this paper, we present the structure of Dynamics Based Control.",
                "We show that the recently developed Extended Markov Tracking (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC.",
                "EMT is an efficient instantiation of DBC.",
                "To evaluate DBC, we carried out a set of experiments applying multi-target EMT to the Tag Game [11]; this is a variant on the area sweeping problem, where an agent is trying to tag a moving target (quarry) whose position is not known with certainty.",
                "Experimental data demonstrates that even with a simple model of the environment and a simple design of target dynamics, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agents position).",
                "The paper is organized as follows.",
                "In Section 2 we motivate DBC using area-sweeping problems, and discuss related work.",
                "Section 3 introduces the Dynamics Based Control (DBC) structure, and its specialization to Markovian environments.",
                "This is followed by a review of the Extended Markov Tracking (EMT) approach as a DBC-structured control regimen in Section 4.",
                "That section also discusses the limitations of EMT-based control relative to the general DBC framework.",
                "Experimental settings and results are then presented in Section 5.",
                "Section 6 provides a short discussion of the overall approach, and Section 7 gives some concluding remarks and directions for future work. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "MOTIVATION AND RELATED WORK Many real-life scenarios naturally have a stochastic target dynamics specification, especially those domains where there exists no ultimate goal, but rather system behavior (with specific properties) that has to be continually supported.",
                "For example, security guards perform persistent sweeps of an area to detect any sign of intrusion.",
                "Cunning thieves will attempt to track these sweeps, and time their operation to key points of the guards motion.",
                "It is thus advisable to make the guards motion dynamics appear irregular and random.",
                "Recent work by Paruchuri et al. [10] has addressed such randomization in the context of single-agent and distributed POMDPs.",
                "The goal in that work was to generate policies that provide a measure of action-selection randomization, while maintaining rewards within some acceptable levels.",
                "Our focus differs from this work in that DBC does not optimize expected rewards-indeed we do not consider rewards at all-but instead maintains desired dynamics (including, but not limited to, randomization).",
                "The Game of Tag is another example of the applicability of the approach.",
                "It was introduced in the work by Pineau et al. [11].",
                "There are two agents that can move about an area, which is divided into a grid.",
                "The grid may have blocked cells (holes) into which no agent can move.",
                "One agent (the hunter) seeks to move into a cell occupied by the other (the quarry), such that they are co-located (this is a successful tag).",
                "The quarry seeks to avoid the hunter agent, and is always aware of the hunters position, but does not know how the hunter will behave, which opens up the possibility for a hunter to surprise the prey.",
                "The hunter knows the quarrys probabilistic law of motion, but does not know its current location.",
                "Tag is an instance of a family of area-sweeping (pursuit-evasion) problems.",
                "In [11], the hunter modeled the problem using a POMDP.",
                "A reward function was defined, to reflect the desire to tag the quarry, and an action policy was computed to optimize the reward collected over time.",
                "Due to the intractable complexity of determining the optimal policy, the action policy computed in that paper was essentially an approximation.",
                "In this paper, instead of formulating a reward function, we use EMT to solve the problem, by directly specifying the target dynamics.",
                "In fact, any search problem with randomized motion, the socalled class of area sweeping problems, can be described through specification of such target system dynamics.",
                "Dynamics Based Control provides a natural approach to solving these problems. 3.",
                "DYNAMICS BASED CONTROL The specification of Dynamics Based Control (DBC) can be broken into three interacting levels: Environment Design Level, User Level, and <br>agent level</br>. • Environment Design Level is concerned with the formal specification and modeling of the environment.",
                "For example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation constant. • User Level in turn relies on the environment model produced by Environment Design to specify the target system dynamics it wishes to observe.",
                "The User Level also specifies the estimation or learning procedure for system dynamics, and the measure of deviation.",
                "In the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping. • <br>agent level</br> in turn combines the environment model from the Environment Design level, the dynamics estimation procedure, the deviation measure and the target dynamics specification from User Level, to produce a sequence of actions that create system dynamics as close as possible to the targeted specification.",
                "As we are interested in the continual development of a stochastic system, such as happens in classical control theory [16] and continual planning [4], as well as in our example of museum sweeps, the question becomes how the <br>agent level</br> is to treat the deviation measurements over time.",
                "To this end, we use a probability threshold-that is, we would like the <br>agent level</br> to maximize the probability that the deviation measure will remain below a certain threshold.",
                "Specific action selection then depends on system formalization.",
                "One possibility would be to create a mixture of available system trends, much like that which happens in Behavior-Based Robotic architectures [1].",
                "The other alternative would be to rely on the estimation procedure provided by the User Level-to utilize the Environment Design Level model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a certain dynamics has been achieved.",
                "Notice that this manipulation is not direct, but via the environment.",
                "Thus, for strong enough estimator algorithms, successful manipulation would mean a successful simulation of the specified target dynamics (i.e., beyond discerning via the available sensory input).",
                "DBC levels can also have a back-flow of information (see Figure 1).",
                "For instance, the <br>agent level</br> could provide data about target dynamics feasibility, allowing the User Level to modify the requirement, perhaps focusing on attainable features of system behavior.",
                "Data would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the User Level, this can provide an important tool for the environment model calibration at the Environment Design Level.",
                "UserEnv.",
                "Design Agent Model Ideal Dynamics Estimator Estimator Dynamics Feasibility System Response Data Figure 1: Data flow of the DBC framework Extending upon the idea of Actor-Critic algorithms [5], DBC data flow can provide a good basis for the design of a learning algorithm.",
                "For example, the User Level can operate as an exploratory device for a learning algorithm, inferring an ideal dynamics target from the environment model at hand that would expose and verify most critical features of system behavior.",
                "In this case, feasibility and system response data from the <br>agent level</br> would provide key information for an environment model update.",
                "In fact, the combination of feasibility and response data can provide a basis for the application of strong learning algorithms such as EM [2, 9]. 3.1 DBC for Markovian Environments For a Partially Observable Markovian Environment, DBC can be specified in a more rigorous manner.",
                "Notice how DBC discards rewards, and replaces it by another optimality criterion (structural differences are summarized in Table 1): • Environment Design level is to specify a tuple < S, A, T, O, Ω, s0 >, where: - S is the set of all possible environment states; - s0 is the initial state of the environment (which can also be viewed as a probability distribution over S); The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 791 - A is the set of all possible actions applicable in the environment; - T is the environments probabilistic transition function: T : S ×A → Π(S).",
                "That is, T(s |a, s) is the probability that the environment will move from state s to state s under action a; - O is the set of all possible observations.",
                "This is what the sensor input would look like for an outside observer; - Ω is the observation probability function: Ω : S × A × S → Π(O).",
                "That is, Ω(o|s , a, s) is the probability that one will observe o given that the environment has moved from state s to state s under action a. • User Level, in the case of Markovian environment, operates on the set of system dynamics described by a family of conditional probabilities F = {τ : S × A → Π(S)}.",
                "Thus specification of target dynamics can be expressed by q ∈ F, and the learning or tracking algorithm can be represented as a function L : O×(A×O)∗ → F; that is, it maps sequences of observations and actions performed so far into an estimate τ ∈ F of system dynamics.",
                "There are many possible variations available at the User Level to define divergence between system dynamics; several of them are: - Trace distance or L1 distance between two distributions p and q defined by D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Fidelity measure of distance F(p(·), q(·)) = x p(x)q(x) - Kullback-Leibler divergence DKL(p(·) q(·)) = x p(x) log p(x) q(x) Notice that the latter two are not actually metrics over the space of possible distributions, but nevertheless have meaningful and important interpretations.",
                "For instance, KullbackLeibler divergence is an important tool of information theory [3] that allows one to measure the price of encoding an information source governed by q, while assuming that it is governed by p. The User Level also defines the threshold of dynamics deviation probability θ. • <br>agent level</br> is then faced with a problem of selecting a control signal function a∗ to satisfy a minimization problem as follows: a∗ = arg min a Pr(d(τa, q) > θ) where d(τa, q) is a random variable describing deviation of the dynamics estimate τa, created by L under control signal a, from the ideal dynamics q.",
                "Implicit in this minimization problem is that L is manipulated via the environment, based on the environment model produced by the Environment Design Level. 3.2 DBC View of the State Space It is important to note the complementary view that DBC and POMDPs take on the state space of the environment.",
                "POMDPs regard state as a stationary snap-shot of the environment; whatever attributes of state sequencing one seeks are reached through properties of the control process, in this case reward accumulation.",
                "This can be viewed as if the sequencing of states and the attributes of that sequencing are only introduced by and for the controlling mechanism-the POMDP policy.",
                "DBC concentrates on the underlying principle of state sequencing, the system dynamics.",
                "DBCs target dynamics specification can use the environments state space as a means to describe, discern, and preserve changes that occur within the system.",
                "As a result, DBC has a greater ability to express state sequencing properties, which are grounded in the environment model and its state space definition.",
                "For example, consider the task of moving through rough terrain towards a goal and reaching it as fast as possible.",
                "POMDPs would encode terrain as state space points, while speed would be ensured by negative reward for every step taken without reaching the goalaccumulating higher reward can be reached only by faster motion.",
                "Alternatively, the state space could directly include the notion of speed.",
                "For POMDPs, this would mean that the same concept is encoded twice, in some sense: directly in the state space, and indirectly within reward accumulation.",
                "Now, even if the reward function would encode more, and finer, details of the properties of motion, the POMDP solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure.",
                "On the other hand, the tactical target expression of variations and correlations between position and speed of motion is now grounded in the state space representation.",
                "In this situation, any further constraints, e.g., smoothness of motion, speed limits in different locations, or speed reductions during sharp turns, are explicitly and uniformly expressed by the tactical target, and can result in faster and more effective action selection by a DBC algorithm. 4.",
                "EMT-BASED CONTROL AS A DBC Recently, a control algorithm was introduced called EMT-based Control [13], which instantiates the DBC framework.",
                "Although it provides an approximate greedy solution in the DBC sense, initial experiments using EMT-based control have been encouraging [14, 15].",
                "EMT-based control is based on the Markovian environment definition, as in the case with POMDPs, but its User and Agent Levels are of the Markovian DBC type of optimality. • User Level of EMT-based control defines a limited-case target system dynamics independent of action: qEMT : S → Π(S).",
                "It then utilizes the Kullback-Leibler divergence measure to compose a momentary system dynamics estimator-the Extended Markov Tracking (EMT) algorithm.",
                "The algorithm keeps a system dynamics estimate τt EMT that is capable of explaining recent change in an auxiliary Bayesian system state estimator from pt−1 to pt, and updates it conservatively using Kullback-Leibler divergence.",
                "Since τt EMT and pt−1,t are respectively the conditional and marginal probabilities over the systems state space, explanation simply means that pt(s ) = s τt EMT (s |s)pt−1(s), and the dynamics estimate update is performed by solving a 792 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: Structure of POMDP vs. Dynamics-Based Control in Markovian Environment Level Approach MDP Markovian DBC Environment < S, A, T, O, Ω >,where S - set of states A - set of actions Design T : S × A → Π(S) - transition O - observation set Ω : S × A × S → Π(O) User r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - reward function q - ideal dynamics F - reward remodeling L - dynamics estimator θ - threshold Agent π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) minimization problem: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • <br>agent level</br> in EMT-based control is suboptimal with respect to DBC (though it remains within the DBC framework), performing greedy action selection based on prediction of EMTs reaction.",
                "The prediction is based on the environment model provided by the Environment Design level, so that if we denote by Ta the environments transition function limited to action a, and pt−1 is the auxiliary Bayesian system state estimator, then the EMT-based control choice is described by a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1) Note that this follows the Markovian DBC framework precisely: the rewarding optimality of POMDPs is discarded, and in its place a dynamics estimator (EMT in this case) is manipulated via action effects on the environment to produce an estimate close to the specified target system dynamics.",
                "Yet as we mentioned, naive EMTbased control is suboptimal in the DBC sense, and has several additional limitations that do not exist in the general DBC framework (discussed in Section 4.2). 4.1 Multi-Target EMT At times, there may exist several behavioral preferences.",
                "For example, in the case of museum guards, some art items are more heavily guarded, requiring that the guards stay more often in their close vicinity.",
                "On the other hand, no corner of the museum is to be left unchecked, which demands constant motion.",
                "Successful museum security would demand that the guards adhere to, and balance, both of these behaviors.",
                "For EMT-based control, this would mean facing several tactical targets {qk}K k=1, and the question becomes how to merge and balance them.",
                "A balancing mechanism can be applied to resolve this issue.",
                "Note that EMT-based control, while selecting an action, creates a preference vector over the set of actions based on their predicted performance with respect to a given target.",
                "If these preference vectors are normalized, they can be combined into a single unified preference.",
                "This requires replacement of standard EMT-based action selection by the algorithm below [15]: • Given: - a set of target dynamics {qk}K k=1, - vector of weights w(k) • Select action as follows - For each action a ∈ A predict the future state distribution ¯pa t+1 = Ta ∗ pt; - For each action, compute Da = H(¯pa t+1, pt, PDt) - For each a ∈ A and qk tactical target, denote V (a, k) = DKL (Da qk) pt .",
                "Let Vk(a) = 1 Zk V (a, k), where Zk = a∈A V (a, k) is a normalization factor. - Select a∗ = arg min a k k=1 w(k)Vk(a) The weights vector w = (w1, ..., wK ) allows the additional tuning of importance among target dynamics without the need to redesign the targets themselves.",
                "This balancing method is also seamlessly integrated into the EMT-based control flow of operation. 4.2 EMT-based Control Limitations EMT-based control is a sub-optimal (in the DBC sense) representative of the DBC structure.",
                "It limits the User by forcing EMT to be its dynamic tracking algorithm, and replaces Agent optimization by greedy action selection.",
                "This kind of combination, however, is common for on-line algorithms.",
                "Although further development of EMT-based controllers is necessary, evidence so far suggests that even the simplest form of the algorithm possesses a great deal of power, and displays trends that are optimal in the DBC sense of the word.",
                "There are two further, EMT-specific, limitations to EMT-based control that are evident at this point.",
                "Both already have partial solutions and are subjects of ongoing research.",
                "The first limitation is the problem of negative preference.",
                "In the POMDP framework for example, this is captured simply, through The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 793 the appearance of values with different signs within the reward structure.",
                "For EMT-based control, however, negative preference means that one would like to avoid a certain distribution over system development sequences; EMT-based control, however, concentrates on getting as close as possible to a distribution.",
                "Avoidance is thus unnatural in native EMT-based control.",
                "The second limitation comes from the fact that standard environment modeling can create pure sensory actions-actions that do not change the state of the world, and differ only in the way observations are received and the quality of observations received.",
                "Since the world state does not change, EMT-based control would not be able to differentiate between different sensory actions.",
                "Notice that both of these limitations of EMT-based control are absent from the general DBC framework, since it may have a tracking algorithm capable of considering pure sensory actions and, unlike Kullback-Leibler divergence, a distribution deviation measure that is capable of dealing with negative preference. 5.",
                "EMT PLAYING TAG The Game of Tag was first introduced in [11].",
                "It is a single agent problem of capturing a quarry, and belongs to the class of area sweeping problems.",
                "An example domain is shown in Figure 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figure 2: Tag domain; an agent (A) attempts to seek and capture a quarry (Q) The Game of Tag extremely limits the agents perception, so that the agent is able to detect the quarry only if they are co-located in the same cell of the grid world.",
                "In the classical version of the game, co-location leads to a special observation, and the Tag action can be performed.",
                "We slightly modify this setting: the moment that both agents occupy the same cell, the game ends.",
                "As a result, both the agent and its quarry have the same motion capability, which allows them to move in four directions, North, South, East, and West.",
                "These form a formal space of actions within a Markovian environment.",
                "The state space of the formal Markovian environment is described by the cross-product of the agent and quarrys positions.",
                "For Figure 2, it would be S = {s0, ..., s23} × {s0, ..., s23}.",
                "The effects of an action taken by the agent are deterministic, but the environment in general has a stochastic response due to the motion of the quarry.",
                "With probability q0 1 it stays put, and with probability 1 − q0 it moves to an adjacent cell further away from the 1 In our experiments this was taken to be q0 = 0.2. agent.",
                "So for the instance shown in Figure 2 and q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Although the evasive behavior of the quarry is known to the agent, the quarrys position is not.",
                "The only sensory information available to the agent is its own location.",
                "We use EMT and directly specify the target dynamics.",
                "For the Game of Tag, we can easily formulate three major trends: catching the quarry, staying mobile, and stalking the quarry.",
                "This results in the following three target dynamics: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 otherwise Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 otherwise Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Note that none of the above targets are directly achievable; for instance, if Qt = s9 and At = s11, there is no action that can move the agent to At+1 = s9 as required by the Tcatch target dynamics.",
                "We ran several experiments to evaluate EMT performance in the Tag Game.",
                "Three configurations of the domain shown in Figure 3 were used, each posing a different challenge to the agent due to partial observability.",
                "In each setting, a set of 1000 runs was performed with a time limit of 100 steps.",
                "In every run, the initial position of both the agent and its quarry was selected at random; this means that as far as the agent was concerned, the quarrys initial position was uniformly distributed over the entire domain cell space.",
                "We also used two variations of the environment observability function.",
                "In the first version, observability function mapped all joint positions of hunter and quarry into the position of the hunter as an observation.",
                "In the second, only those joint positions in which hunter and quarry occupied different locations were mapped into the hunters location.",
                "The second version in fact utilized and expressed the fact that once hunter and quarry occupy the same cell the game ends.",
                "The results of these experiments are shown in Table 2.",
                "Balancing [15] the catch, move, and stalk target dynamics described in the previous section by the weight vector [0.8, 0.1, 0.1], EMT produced stable performance in all three domains.",
                "Although direct comparisons are difficult to make, the EMT performance displayed notable efficiency vis-`a-vis the POMDP approach.",
                "In spite of a simple and inefficient Matlab implementation of the EMT algorithm, the decision time for any given step averaged significantly below 1 second in all experiments.",
                "For the irregular open arena domain, which proved to be the most difficult, 1000 experiment runs bounded by 100 steps each, a total of 42411 steps, were completed in slightly under 6 hours.",
                "That is, over 4 × 104 online steps took an order of magnitude less time than the offline computation of POMDP policy in [11].",
                "The significance of this differential is made even more prominent by the fact that, should the environment model parameters change, the online nature of EMT would allow it to maintain its performance time, while the POMDP policy would need to be recomputed, requiring yet again a large overhead of computation.",
                "We also tested the behavior cell frequency entropy, empirical measures from trial data.",
                "As Figure 4 and Figure 5 show, empir794 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) A Q Q A 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 A Q Figure 3: These configurations of the Tag Game space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor Table 2: Performance of the EMT-based solution in three Tag Game domains and two observability models: I) omniposition quarry, II) quarry is not at hunters position Model Domain Capture% E(Steps) Time/Step I Dead-ends 100 14.8 72(mSec) Arena 80.2 42.4 500(mSec) Circle 91.4 34.6 187(mSec) II Dead-ends 100 13.2 91(mSec) Arena 96.8 28.67 396(mSec) Circle 94.4 31.63 204(mSec) ical entropy grows with the length of interaction.",
                "For runs where the quarry was not captured immediately, the entropy reaches between 0.85 and 0.952 for different runs and scenarios.",
                "As the agent actively seeks the quarry, the entropy never reaches its maximum.",
                "One characteristic of the entropy graph for the open arena scenario particularly caught our attention in the case of the omniposition quarry observation model.",
                "Near the maximum limit of trial length (100 steps), entropy suddenly dropped.",
                "Further analysis of the data showed that under certain circumstances, a fluctuating behavior occurs in which the agent faces equally viable versions of quarry-following behavior.",
                "Since the EMT algorithm has greedy action selection, and the state space does not encode any form of commitment (not even speed or acceleration), the agent is locked within a small portion of cells.",
                "It is essentially attempting to simultaneously follow several courses of action, all of which are consistent with the target dynamics.",
                "This behavior did not occur in our second observation model, since it significantly reduced the set of eligible courses of action-essentially contributing to tie-breaking among them. 6.",
                "DISCUSSION The design of the EMT solution for the Tag Game exposes the core difference in approach to planning and control between EMT or DBC, on the one hand, and the more familiar POMDP approach, on the other.",
                "POMDP defines a reward structure to optimize, and influences system dynamics indirectly through that optimization.",
                "EMT discards any reward scheme, and instead measures and influences system dynamics directly. 2 Entropy was calculated using log base equal to the number of possible locations within the domain; this properly scales entropy expression into the range [0, 1] for all domains.",
                "Thus for the Tag Game, we did not search for a reward function that would encode and express our preference over the agents behavior, but rather directly set three (heuristic) behavior preferences as the basis for target dynamics to be maintained.",
                "Experimental data shows that these targets need not be directly achievable via the agents actions.",
                "However, the ratio between EMT performance and achievability of target dynamics remains to be explored.",
                "The tag game experiment data also revealed the different emphasis DBC and POMDPs place on the formulation of an environment state space.",
                "POMDPs rely entirely on the mechanism of reward accumulation maximization, i.e., formation of the action selection procedure to achieve necessary state sequencing.",
                "DBC, on the other hand, has two sources of sequencing specification: through the properties of an action selection procedure, and through direct specification within the target dynamics.",
                "The importance of the second source was underlined by the Tag Game experiment data, in which the greedy EMT algorithm, applied to a POMDP-type state space specification, failed, since target description over such a state space was incapable of encoding the necessary behavior tendencies, e.g., tie-breaking and commitment to directed motion.",
                "The structural differences between DBC (and EMT in particular), and POMDPs, prohibits direct performance comparison, and places them on complementary tracks, each within a suitable niche.",
                "For instance, POMDPs could be seen as a much more natural formulation of economic sequential decision-making problems, while EMT is a better fit for continual demand for stochastic change, as happens in many robotic or embodied-agent problems.",
                "The complementary properties of POMDPs and EMT can be further exploited.",
                "There is recent interest in using POMDPs in hybrid solutions [17], in which the POMDPs can be used together with other control approaches to provide results not easily achievable with either approach by itself.",
                "DBC can be an effective partner in such a hybrid solution.",
                "For instance, POMDPs have prohibitively large off-line time requirements for policy computation, but can be readily used in simpler settings to expose beneficial behavioral trends; this can serve as a form of target dynamics that are provided to EMT in a larger domain for on-line operation. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have presented a novel perspective on the process of planning and control in stochastic environments, in the form of the Dynamics Based Control (DBC) framework.",
                "DBC formulates the task of planning as support of a specified target system dynamics, which describes the necessary properties of change within the environment.",
                "Optimality of DBC plans of action are measured The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 4: Observation Model I: Omniposition quarry.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 5: Observation Model II: quarry not observed at hunters position.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. with respect to the deviation of actual system dynamics from the target dynamics.",
                "We show that a recently developed technique of Extended Markov Tracking (EMT) [13] is an instantiation of DBC.",
                "In fact, EMT can be seen as a specific case of DBC parameterization, which employs a greedy action selection procedure.",
                "Since EMT exhibits the key features of the general DBC framework, as well as polynomial time complexity, we used the multitarget version of EMT [15] to demonstrate that the class of area sweeping problems naturally lends itself to dynamics-based descriptions, as instantiated by our experiments in the Tag Game domain.",
                "As enumerated in Section 4.2, EMT has a number of limitations, such as difficulty in dealing with negative dynamic preference.",
                "This prevents direct application of EMT to such problems as Rendezvous-Evasion Games (e.g., [6]).",
                "However, DBC in general has no such limitations, and readily enables the formulation of evasion games.",
                "In future work, we intend to proceed with the development of dynamics-based controllers for these problems. 8.",
                "ACKNOWLEDGMENT The work of the first two authors was partially supported by Israel Science Foundation grant #898/05, and the third author was partially supported by a grant from Israels Ministry of Science and Technology. 9.",
                "REFERENCES [1] R. C. Arkin.",
                "Behavior-Based Robotics.",
                "MIT Press, 1998. [2] J.",
                "A. Bilmes.",
                "A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and Hidden Markov Models.",
                "Technical Report TR-97-021, Department of Electrical Engeineering and Computer Science, University of California at Berkeley, 1998. [3] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of information theory.",
                "Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz, and M. J. Wolverton.",
                "A survey of research in distributed, continual planning.",
                "AI Magazine, 4:13-22, 1999. [5] V. R. Konda and J. N. Tsitsiklis.",
                "Actor-Critic algorithms.",
                "SIAM Journal on Control and Optimization, 42(4):1143-1166, 2003. [6] W. S. Lim.",
                "A rendezvous-evasion game on discrete locations with joint randomization.",
                "Advances in Applied Probability, 29(4):1004-1017, December 1997. [7] M. L. Littman, T. L. Dean, and L. P. Kaelbling.",
                "On the complexity of solving Markov decision problems.",
                "In Proceedings of the 11th Annual Conference on Uncertainty in Artificial Intelligence (UAI-95), pages 394-402, 1995. [8] O. Madani, S. Hanks, and A. Condon.",
                "On the undecidability of probabilistic planning and related stochastic optimization problems.",
                "Artificial Intelligence Journal, 147(1-2):5-34, July 2003. [9] R. M. Neal and G. E. Hinton.",
                "A view of the EM algorithm 796 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) that justifies incremental, sparse, and other variants.",
                "In M. I. Jordan, editor, Learning in Graphical Models, pages 355-368.",
                "Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In Proceeding of AAMAS 2006, 2006. [11] J. Pineau, G. Gordon, and S. Thrun.",
                "Point-based value iteration: An anytime algorithm for pomdps.",
                "In International Joint Conference on Artificial Intelligence (IJCAI), pages 1025-1032, August 2003. [12] M. L. Puterman.",
                "Markov Decision Processes.",
                "Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics Section.",
                "Wiley-Interscience Publication, New York, 1994. [13] Z. Rabinovich and J. S. Rosenschein.",
                "Extended Markov Tracking with an application to control.",
                "In The Workshop on Agent Tracking: Modeling Other Agents from Observations, at the Third International Joint Conference on Autonomous Agents and Multiagent Systems, pages 95-100, New-York, July 2004. [14] Z. Rabinovich and J. S. Rosenschein.",
                "Multiagent coordination by Extended Markov Tracking.",
                "In The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 431-438, Utrecht, The Netherlands, July 2005. [15] Z. Rabinovich and J. S. Rosenschein.",
                "On the response of EMT-based control to interacting targets and models.",
                "In The Fifth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 465-470, Hakodate, Japan, May 2006. [16] R. F. Stengel.",
                "Optimal Control and Estimation.",
                "Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, and P. Varakantham.",
                "Conflicts in teamwork: Hybrids to the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 797"
            ],
            "original_annotated_samples": [
                "DYNAMICS BASED CONTROL The specification of Dynamics Based Control (DBC) can be broken into three interacting levels: Environment Design Level, User Level, and <br>agent level</br>. • Environment Design Level is concerned with the formal specification and modeling of the environment.",
                "In the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping. • <br>agent level</br> in turn combines the environment model from the Environment Design level, the dynamics estimation procedure, the deviation measure and the target dynamics specification from User Level, to produce a sequence of actions that create system dynamics as close as possible to the targeted specification.",
                "As we are interested in the continual development of a stochastic system, such as happens in classical control theory [16] and continual planning [4], as well as in our example of museum sweeps, the question becomes how the <br>agent level</br> is to treat the deviation measurements over time.",
                "To this end, we use a probability threshold-that is, we would like the <br>agent level</br> to maximize the probability that the deviation measure will remain below a certain threshold.",
                "For instance, the <br>agent level</br> could provide data about target dynamics feasibility, allowing the User Level to modify the requirement, perhaps focusing on attainable features of system behavior."
            ],
            "translated_annotated_samples": [
                "CONTROL BASADO EN DINÁMICA La especificación del Control Basado en Dinámica (DBC) se puede dividir en tres niveles interactivos: Nivel de Diseño del Entorno, Nivel de Usuario y Nivel de Agente. • El Nivel de Diseño del Entorno se ocupa de la especificación formal y modelado del entorno.",
                "En el escenario del guardia de museo mencionado anteriormente, estos corresponderían a un horario de barrido estocástico y una medida de sorpresa relativa entre el barrido especificado y el real. • El Nivel de Agente, a su vez, combina el modelo del entorno del Nivel de Diseño de Entorno, el procedimiento de estimación de la dinámica, la medida de desviación y la especificación de la dinámica objetivo del Nivel de Usuario, para producir una secuencia de acciones que creen dinámicas del sistema lo más cercanas posible a la especificación deseada.",
                "Dado nuestro interés en el desarrollo continuo de un sistema estocástico, como ocurre en la teoría de control clásica [16] y la planificación continua [4], así como en nuestro ejemplo de recorridos por museos, la pregunta es cómo el <br>Nivel de Agente</br> debe tratar las mediciones de desviación a lo largo del tiempo.",
                "Con este fin, utilizamos un umbral de probabilidad, es decir, nos gustaría que el Nivel del Agente maximice la probabilidad de que la medida de desviación permanezca por debajo de cierto umbral.",
                "Por ejemplo, el Nivel de Agente podría proporcionar datos sobre la viabilidad de la dinámica del objetivo, permitiendo al Nivel de Usuario modificar el requisito, quizás centrándose en las características alcanzables del comportamiento del sistema."
            ],
            "translated_text": "En este artículo presentamos el Control Basado en Dinámicas (DBC), un enfoque para la planificación y control de un agente en entornos estocásticos. A diferencia de los enfoques existentes, que buscan optimizar las recompensas esperadas (por ejemplo, en Problemas de Decisión de Markov Parcialmente Observables (POMDP)), DBC optimiza el comportamiento del sistema hacia las dinámicas del sistema especificadas. Mostramos que un enfoque de planificación y control recientemente desarrollado, Seguimiento Extendido de Markov (EMT), es una instancia de DBC. EMT emplea la selección de acciones codiciosa para proporcionar un algoritmo de control eficiente en entornos markovianos. Explotamos esta eficiencia en un conjunto de experimentos que aplicaron EMT multitarget a una clase de problemas de barrido de área (búsqueda de objetivos en movimiento). Mostramos que tales problemas pueden ser definidos de forma natural y resueltos eficientemente utilizando el marco de DBC y su instanciación de EMT. Categorías y Descriptores de Asignaturas I.2.8 [Resolución de Problemas, Métodos de Control y Búsqueda]: Teoría de Control; I.2.9 [Robótica]; I.2.11 [Inteligencia Artificial Distribuida]: Agentes Inteligentes Términos Generales Algoritmos, Teoría 1. INTRODUCCIÓN La planificación y el control constituyen un área de investigación central en sistemas multiagentes e inteligencia artificial. En los últimos años, los Procesos de Decisión de Markov Parcialmente Observables (POMDPs) [12] se han convertido en una base formal popular para la planificación en entornos estocásticos. En este marco, el problema de planificación y control suele abordarse imponiendo una función de recompensa y calculando una política (de elección de acciones) que sea óptima, en el sentido de que resultará en la mayor utilidad esperada. Si bien teóricamente atractivo, la complejidad de resolver óptimamente un POMDP es prohibitiva [8, 7]. Tomamos una visión alternativa de la planificación en entornos estocásticos. No utilizamos una función de recompensa basada en el estado, sino que optimizamos según un criterio diferente, una especificación basada en transiciones de la dinámica del sistema deseado. La idea aquí es ver la ejecución del plan como un proceso que obliga a un sistema (estocástico) a cambiar, y un plan como un proceso dinámico que moldea ese cambio de acuerdo con criterios deseados. Llamamos a este marco de planificación general Control Basado en Dinámicas (DBC). En DBC, el objetivo de un proceso de planificación (o control) es garantizar que el sistema cambiará de acuerdo con dinámicas objetivo específicas (potencialmente estocásticas). Dado que el comportamiento real del sistema puede desviarse de lo especificado por la dinámica objetivo (debido a la naturaleza estocástica del sistema), la planificación en tales entornos debe ser continua, de manera similar a los controladores en lazo cerrado clásicos. Aquí, la optimalidad se mide en términos de la probabilidad de magnitudes de desviación. En este documento, presentamos la estructura del Control Basado en Dinámicas. Mostramos que el enfoque de Seguimiento Extendido de Markov (EMT) recientemente desarrollado [13, 14, 15] está subsumido por DBC, con EMT empleando selección de acciones codiciosa, que es una parametrización específica entre las opciones posibles dentro de DBC. EMT es una implementación eficiente de DBC. Para evaluar DBC, llevamos a cabo un conjunto de experimentos aplicando EMT de múltiples objetivos al Juego de Etiquetas [11]; esta es una variante del problema de barrido de área, donde un agente intenta etiquetar a un objetivo móvil (presa) cuya posición no se conoce con certeza. Los datos experimentales demuestran que incluso con un modelo simple del entorno y un diseño simple de la dinámica del objetivo, se pueden lograr altas tasas de éxito tanto en la captura de la presa como en sorprender a la presa (como se expresa por la entropía observada de la posición de los agentes controlados). El documento está organizado de la siguiente manera. En la Sección 2 motivamos DBC utilizando problemas de barrido de área, y discutimos trabajos relacionados. La Sección 3 introduce la estructura de Control Basado en Dinámicas (DBC) y su especialización en entornos Markovianos. Esto es seguido por una revisión del enfoque de Seguimiento Markov Extendido (EMT) como un régimen de control estructurado por DBC en la Sección 4. Esa sección también discute las limitaciones del control basado en EMT en relación con el marco general de DBC. Los ajustes experimentales y los resultados se presentan a continuación en la Sección 5. La Sección 6 proporciona una breve discusión del enfoque general, y la Sección 7 ofrece algunas observaciones finales y direcciones para trabajos futuros. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2. MOTIVACIÓN Y TRABAJO RELACIONADO Muchos escenarios de la vida real tienen naturalmente una especificación de dinámica de destino estocástica, especialmente en aquellos dominios donde no existe un objetivo final, sino más bien un comportamiento del sistema (con propiedades específicas) que debe ser continuamente respaldado. Por ejemplo, los guardias de seguridad realizan barridos persistentes de un área para detectar cualquier signo de intrusión. Los ladrones astutos intentarán rastrear estos barridos y sincronizar su operación con puntos clave del movimiento de los guardias. Por lo tanto, es recomendable que la dinámica de movimiento de los guardias parezca irregular y aleatoria. El trabajo reciente de Paruchuri et al. [10] ha abordado dicha aleatorización en el contexto de POMDPs de agente único y distribuidos. El objetivo de ese trabajo era generar políticas que proporcionen una medida de aleatorización en la selección de acciones, manteniendo al mismo tiempo las recompensas dentro de niveles aceptables. Nuestro enfoque difiere de este trabajo en que DBC no optimiza recompensas esperadas, de hecho, no consideramos recompensas en absoluto, sino que en su lugar mantenemos dinámicas deseadas (incluyendo, pero no limitado a, la aleatorización). El juego de la mancha es otro ejemplo de la aplicabilidad del enfoque. Fue introducido en el trabajo de Pineau et al. [11]. Hay dos agentes que pueden moverse por un área, la cual está dividida en una cuadrícula. La cuadrícula puede tener celdas bloqueadas (agujeros) en las cuales ningún agente puede moverse. Un agente (el cazador) intenta moverse a una celda ocupada por el otro (la presa), de modo que estén ubicados en el mismo lugar (esto es una etiqueta exitosa). La presa busca evitar al cazador, siempre está al tanto de la posición del cazador, pero no sabe cómo se comportará el cazador, lo que abre la posibilidad de que un cazador sorprenda a la presa. El cazador conoce la ley de movimiento probabilístico de la presa, pero no conoce su ubicación actual. Tag es una instancia de una familia de problemas de barrido de área (persecución-evasión). En [11], el cazador modeló el problema utilizando un POMDP. Se definió una función de recompensa para reflejar el deseo de marcar la presa, y se calculó una política de acción para optimizar la recompensa recolectada con el tiempo. Debido a la complejidad intratable de determinar la política óptima, la política de acción calculada en ese documento fue esencialmente una aproximación. En este artículo, en lugar de formular una función de recompensa, utilizamos EMT para resolver el problema, especificando directamente la dinámica objetivo. De hecho, cualquier problema de búsqueda con movimiento aleatorio, la llamada clase de problemas de barrido de área, puede ser descrito a través de la especificación de la dinámica de dicho sistema objetivo. El Control Basado en Dinámicas proporciona un enfoque natural para resolver estos problemas. CONTROL BASADO EN DINÁMICA La especificación del Control Basado en Dinámica (DBC) se puede dividir en tres niveles interactivos: Nivel de Diseño del Entorno, Nivel de Usuario y Nivel de Agente. • El Nivel de Diseño del Entorno se ocupa de la especificación formal y modelado del entorno. Por ejemplo, este nivel especificaría las leyes de la física dentro del sistema y establecería sus parámetros, como la constante de gravitación. El Nivel de Usuario, a su vez, se basa en el modelo de entorno producido por el Diseño de Entorno para especificar la dinámica del sistema objetivo que desea observar. El Nivel de Usuario también especifica el procedimiento de estimación o aprendizaje para la dinámica del sistema, y la medida de desviación. En el escenario del guardia de museo mencionado anteriormente, estos corresponderían a un horario de barrido estocástico y una medida de sorpresa relativa entre el barrido especificado y el real. • El Nivel de Agente, a su vez, combina el modelo del entorno del Nivel de Diseño de Entorno, el procedimiento de estimación de la dinámica, la medida de desviación y la especificación de la dinámica objetivo del Nivel de Usuario, para producir una secuencia de acciones que creen dinámicas del sistema lo más cercanas posible a la especificación deseada. Dado nuestro interés en el desarrollo continuo de un sistema estocástico, como ocurre en la teoría de control clásica [16] y la planificación continua [4], así como en nuestro ejemplo de recorridos por museos, la pregunta es cómo el <br>Nivel de Agente</br> debe tratar las mediciones de desviación a lo largo del tiempo. Con este fin, utilizamos un umbral de probabilidad, es decir, nos gustaría que el Nivel del Agente maximice la probabilidad de que la medida de desviación permanezca por debajo de cierto umbral. La selección de acciones específicas depende entonces de la formalización del sistema. Una posibilidad sería crear una mezcla de tendencias de sistemas disponibles, similar a lo que sucede en las arquitecturas robóticas basadas en el comportamiento [1]. La otra alternativa sería depender del procedimiento de estimación proporcionado por el Nivel de Usuario para utilizar el modelo del Nivel de Diseño del Entorno del entorno para elegir acciones, de manera que se manipule al estimador de dinámicas para que crea que se ha logrado cierta dinámica. Ten en cuenta que esta manipulación no es directa, sino a través del entorno. Por lo tanto, para algoritmos de estimación lo suficientemente fuertes, la manipulación exitosa significaría una simulación exitosa de la dinámica objetivo especificada (es decir, más allá de discernir a través de la entrada sensorial disponible). Los niveles de DBC también pueden tener un retroceso de información (ver Figura 1). Por ejemplo, el Nivel de Agente podría proporcionar datos sobre la viabilidad de la dinámica del objetivo, permitiendo al Nivel de Usuario modificar el requisito, quizás centrándose en las características alcanzables del comportamiento del sistema. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "system dynamics": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Dynamics Based Control with an Application to Area-Sweeping Problems Zinovi Rabinovich Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel jeff@cs.huji.ac.il Gal A. Kaminka The MAVERICK Group Department of Computer Science Bar Ilan University, Israel galk@cs.biu.ac.il ABSTRACT In this paper we introduce Dynamics Based Control (DBC), an approach to planning and control of an agent in stochastic environments.",
                "Unlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified <br>system dynamics</br>.",
                "We show that a recently developed planning and control approach, Extended Markov Tracking (EMT) is an instantiation of DBC.",
                "EMT employs greedy action selection to provide an efficient control algorithm in Markovian environments.",
                "We exploit this efficiency in a set of experiments that applied multitarget EMT to a class of area-sweeping problems (searching for moving targets).",
                "We show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation.",
                "Categories and Subject Descriptors I.2.8 [Problem Solving, Control Methods, and Search]: Control Theory; I.2.9 [Robotics]; I.2.11 [Distributed Artificial Intelligence]: Intelligent Agents General Terms Algorithms, Theory 1.",
                "INTRODUCTION Planning and control constitutes a central research area in multiagent systems and artificial intelligence.",
                "In recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in stochastic environments.",
                "In this framework, the planning and control problem is often addressed by imposing a reward function, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility.",
                "While theoretically attractive, the complexity of optimally solving a POMDP is prohibitive [8, 7].",
                "We take an alternative view of planning in stochastic environments.",
                "We do not use a (state-based) reward function, but instead optimize over a different criterion, a transition-based specification of the desired <br>system dynamics</br>.",
                "The idea here is to view planexecution as a process that compels a (stochastic) system to change, and a plan as a dynamic process that shapes that change according to desired criteria.",
                "We call this general planning framework Dynamics Based Control (DBC).",
                "In DBC, the goal of a planning (or control) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) target dynamics.",
                "As actual system behavior may deviate from that which is specified by target dynamics (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16].",
                "Here, optimality is measured in terms of probability of deviation magnitudes.",
                "In this paper, we present the structure of Dynamics Based Control.",
                "We show that the recently developed Extended Markov Tracking (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC.",
                "EMT is an efficient instantiation of DBC.",
                "To evaluate DBC, we carried out a set of experiments applying multi-target EMT to the Tag Game [11]; this is a variant on the area sweeping problem, where an agent is trying to tag a moving target (quarry) whose position is not known with certainty.",
                "Experimental data demonstrates that even with a simple model of the environment and a simple design of target dynamics, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agents position).",
                "The paper is organized as follows.",
                "In Section 2 we motivate DBC using area-sweeping problems, and discuss related work.",
                "Section 3 introduces the Dynamics Based Control (DBC) structure, and its specialization to Markovian environments.",
                "This is followed by a review of the Extended Markov Tracking (EMT) approach as a DBC-structured control regimen in Section 4.",
                "That section also discusses the limitations of EMT-based control relative to the general DBC framework.",
                "Experimental settings and results are then presented in Section 5.",
                "Section 6 provides a short discussion of the overall approach, and Section 7 gives some concluding remarks and directions for future work. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "MOTIVATION AND RELATED WORK Many real-life scenarios naturally have a stochastic target dynamics specification, especially those domains where there exists no ultimate goal, but rather system behavior (with specific properties) that has to be continually supported.",
                "For example, security guards perform persistent sweeps of an area to detect any sign of intrusion.",
                "Cunning thieves will attempt to track these sweeps, and time their operation to key points of the guards motion.",
                "It is thus advisable to make the guards motion dynamics appear irregular and random.",
                "Recent work by Paruchuri et al. [10] has addressed such randomization in the context of single-agent and distributed POMDPs.",
                "The goal in that work was to generate policies that provide a measure of action-selection randomization, while maintaining rewards within some acceptable levels.",
                "Our focus differs from this work in that DBC does not optimize expected rewards-indeed we do not consider rewards at all-but instead maintains desired dynamics (including, but not limited to, randomization).",
                "The Game of Tag is another example of the applicability of the approach.",
                "It was introduced in the work by Pineau et al. [11].",
                "There are two agents that can move about an area, which is divided into a grid.",
                "The grid may have blocked cells (holes) into which no agent can move.",
                "One agent (the hunter) seeks to move into a cell occupied by the other (the quarry), such that they are co-located (this is a successful tag).",
                "The quarry seeks to avoid the hunter agent, and is always aware of the hunters position, but does not know how the hunter will behave, which opens up the possibility for a hunter to surprise the prey.",
                "The hunter knows the quarrys probabilistic law of motion, but does not know its current location.",
                "Tag is an instance of a family of area-sweeping (pursuit-evasion) problems.",
                "In [11], the hunter modeled the problem using a POMDP.",
                "A reward function was defined, to reflect the desire to tag the quarry, and an action policy was computed to optimize the reward collected over time.",
                "Due to the intractable complexity of determining the optimal policy, the action policy computed in that paper was essentially an approximation.",
                "In this paper, instead of formulating a reward function, we use EMT to solve the problem, by directly specifying the target dynamics.",
                "In fact, any search problem with randomized motion, the socalled class of area sweeping problems, can be described through specification of such target <br>system dynamics</br>.",
                "Dynamics Based Control provides a natural approach to solving these problems. 3.",
                "DYNAMICS BASED CONTROL The specification of Dynamics Based Control (DBC) can be broken into three interacting levels: Environment Design Level, User Level, and Agent Level. • Environment Design Level is concerned with the formal specification and modeling of the environment.",
                "For example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation constant. • User Level in turn relies on the environment model produced by Environment Design to specify the target <br>system dynamics</br> it wishes to observe.",
                "The User Level also specifies the estimation or learning procedure for <br>system dynamics</br>, and the measure of deviation.",
                "In the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping. • Agent Level in turn combines the environment model from the Environment Design level, the dynamics estimation procedure, the deviation measure and the target dynamics specification from User Level, to produce a sequence of actions that create <br>system dynamics</br> as close as possible to the targeted specification.",
                "As we are interested in the continual development of a stochastic system, such as happens in classical control theory [16] and continual planning [4], as well as in our example of museum sweeps, the question becomes how the Agent Level is to treat the deviation measurements over time.",
                "To this end, we use a probability threshold-that is, we would like the Agent Level to maximize the probability that the deviation measure will remain below a certain threshold.",
                "Specific action selection then depends on system formalization.",
                "One possibility would be to create a mixture of available system trends, much like that which happens in Behavior-Based Robotic architectures [1].",
                "The other alternative would be to rely on the estimation procedure provided by the User Level-to utilize the Environment Design Level model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a certain dynamics has been achieved.",
                "Notice that this manipulation is not direct, but via the environment.",
                "Thus, for strong enough estimator algorithms, successful manipulation would mean a successful simulation of the specified target dynamics (i.e., beyond discerning via the available sensory input).",
                "DBC levels can also have a back-flow of information (see Figure 1).",
                "For instance, the Agent Level could provide data about target dynamics feasibility, allowing the User Level to modify the requirement, perhaps focusing on attainable features of system behavior.",
                "Data would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the User Level, this can provide an important tool for the environment model calibration at the Environment Design Level.",
                "UserEnv.",
                "Design Agent Model Ideal Dynamics Estimator Estimator Dynamics Feasibility System Response Data Figure 1: Data flow of the DBC framework Extending upon the idea of Actor-Critic algorithms [5], DBC data flow can provide a good basis for the design of a learning algorithm.",
                "For example, the User Level can operate as an exploratory device for a learning algorithm, inferring an ideal dynamics target from the environment model at hand that would expose and verify most critical features of system behavior.",
                "In this case, feasibility and system response data from the Agent Level would provide key information for an environment model update.",
                "In fact, the combination of feasibility and response data can provide a basis for the application of strong learning algorithms such as EM [2, 9]. 3.1 DBC for Markovian Environments For a Partially Observable Markovian Environment, DBC can be specified in a more rigorous manner.",
                "Notice how DBC discards rewards, and replaces it by another optimality criterion (structural differences are summarized in Table 1): • Environment Design level is to specify a tuple < S, A, T, O, Ω, s0 >, where: - S is the set of all possible environment states; - s0 is the initial state of the environment (which can also be viewed as a probability distribution over S); The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 791 - A is the set of all possible actions applicable in the environment; - T is the environments probabilistic transition function: T : S ×A → Π(S).",
                "That is, T(s |a, s) is the probability that the environment will move from state s to state s under action a; - O is the set of all possible observations.",
                "This is what the sensor input would look like for an outside observer; - Ω is the observation probability function: Ω : S × A × S → Π(O).",
                "That is, Ω(o|s , a, s) is the probability that one will observe o given that the environment has moved from state s to state s under action a. • User Level, in the case of Markovian environment, operates on the set of <br>system dynamics</br> described by a family of conditional probabilities F = {τ : S × A → Π(S)}.",
                "Thus specification of target dynamics can be expressed by q ∈ F, and the learning or tracking algorithm can be represented as a function L : O×(A×O)∗ → F; that is, it maps sequences of observations and actions performed so far into an estimate τ ∈ F of <br>system dynamics</br>.",
                "There are many possible variations available at the User Level to define divergence between <br>system dynamics</br>; several of them are: - Trace distance or L1 distance between two distributions p and q defined by D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Fidelity measure of distance F(p(·), q(·)) = x p(x)q(x) - Kullback-Leibler divergence DKL(p(·) q(·)) = x p(x) log p(x) q(x) Notice that the latter two are not actually metrics over the space of possible distributions, but nevertheless have meaningful and important interpretations.",
                "For instance, KullbackLeibler divergence is an important tool of information theory [3] that allows one to measure the price of encoding an information source governed by q, while assuming that it is governed by p. The User Level also defines the threshold of dynamics deviation probability θ. • Agent Level is then faced with a problem of selecting a control signal function a∗ to satisfy a minimization problem as follows: a∗ = arg min a Pr(d(τa, q) > θ) where d(τa, q) is a random variable describing deviation of the dynamics estimate τa, created by L under control signal a, from the ideal dynamics q.",
                "Implicit in this minimization problem is that L is manipulated via the environment, based on the environment model produced by the Environment Design Level. 3.2 DBC View of the State Space It is important to note the complementary view that DBC and POMDPs take on the state space of the environment.",
                "POMDPs regard state as a stationary snap-shot of the environment; whatever attributes of state sequencing one seeks are reached through properties of the control process, in this case reward accumulation.",
                "This can be viewed as if the sequencing of states and the attributes of that sequencing are only introduced by and for the controlling mechanism-the POMDP policy.",
                "DBC concentrates on the underlying principle of state sequencing, the <br>system dynamics</br>.",
                "DBCs target dynamics specification can use the environments state space as a means to describe, discern, and preserve changes that occur within the system.",
                "As a result, DBC has a greater ability to express state sequencing properties, which are grounded in the environment model and its state space definition.",
                "For example, consider the task of moving through rough terrain towards a goal and reaching it as fast as possible.",
                "POMDPs would encode terrain as state space points, while speed would be ensured by negative reward for every step taken without reaching the goalaccumulating higher reward can be reached only by faster motion.",
                "Alternatively, the state space could directly include the notion of speed.",
                "For POMDPs, this would mean that the same concept is encoded twice, in some sense: directly in the state space, and indirectly within reward accumulation.",
                "Now, even if the reward function would encode more, and finer, details of the properties of motion, the POMDP solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure.",
                "On the other hand, the tactical target expression of variations and correlations between position and speed of motion is now grounded in the state space representation.",
                "In this situation, any further constraints, e.g., smoothness of motion, speed limits in different locations, or speed reductions during sharp turns, are explicitly and uniformly expressed by the tactical target, and can result in faster and more effective action selection by a DBC algorithm. 4.",
                "EMT-BASED CONTROL AS A DBC Recently, a control algorithm was introduced called EMT-based Control [13], which instantiates the DBC framework.",
                "Although it provides an approximate greedy solution in the DBC sense, initial experiments using EMT-based control have been encouraging [14, 15].",
                "EMT-based control is based on the Markovian environment definition, as in the case with POMDPs, but its User and Agent Levels are of the Markovian DBC type of optimality. • User Level of EMT-based control defines a limited-case target <br>system dynamics</br> independent of action: qEMT : S → Π(S).",
                "It then utilizes the Kullback-Leibler divergence measure to compose a momentary <br>system dynamics</br> estimator-the Extended Markov Tracking (EMT) algorithm.",
                "The algorithm keeps a <br>system dynamics</br> estimate τt EMT that is capable of explaining recent change in an auxiliary Bayesian system state estimator from pt−1 to pt, and updates it conservatively using Kullback-Leibler divergence.",
                "Since τt EMT and pt−1,t are respectively the conditional and marginal probabilities over the systems state space, explanation simply means that pt(s ) = s τt EMT (s |s)pt−1(s), and the dynamics estimate update is performed by solving a 792 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: Structure of POMDP vs. Dynamics-Based Control in Markovian Environment Level Approach MDP Markovian DBC Environment < S, A, T, O, Ω >,where S - set of states A - set of actions Design T : S × A → Π(S) - transition O - observation set Ω : S × A × S → Π(O) User r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - reward function q - ideal dynamics F - reward remodeling L - dynamics estimator θ - threshold Agent π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) minimization problem: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • Agent Level in EMT-based control is suboptimal with respect to DBC (though it remains within the DBC framework), performing greedy action selection based on prediction of EMTs reaction.",
                "The prediction is based on the environment model provided by the Environment Design level, so that if we denote by Ta the environments transition function limited to action a, and pt−1 is the auxiliary Bayesian system state estimator, then the EMT-based control choice is described by a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1) Note that this follows the Markovian DBC framework precisely: the rewarding optimality of POMDPs is discarded, and in its place a dynamics estimator (EMT in this case) is manipulated via action effects on the environment to produce an estimate close to the specified target <br>system dynamics</br>.",
                "Yet as we mentioned, naive EMTbased control is suboptimal in the DBC sense, and has several additional limitations that do not exist in the general DBC framework (discussed in Section 4.2). 4.1 Multi-Target EMT At times, there may exist several behavioral preferences.",
                "For example, in the case of museum guards, some art items are more heavily guarded, requiring that the guards stay more often in their close vicinity.",
                "On the other hand, no corner of the museum is to be left unchecked, which demands constant motion.",
                "Successful museum security would demand that the guards adhere to, and balance, both of these behaviors.",
                "For EMT-based control, this would mean facing several tactical targets {qk}K k=1, and the question becomes how to merge and balance them.",
                "A balancing mechanism can be applied to resolve this issue.",
                "Note that EMT-based control, while selecting an action, creates a preference vector over the set of actions based on their predicted performance with respect to a given target.",
                "If these preference vectors are normalized, they can be combined into a single unified preference.",
                "This requires replacement of standard EMT-based action selection by the algorithm below [15]: • Given: - a set of target dynamics {qk}K k=1, - vector of weights w(k) • Select action as follows - For each action a ∈ A predict the future state distribution ¯pa t+1 = Ta ∗ pt; - For each action, compute Da = H(¯pa t+1, pt, PDt) - For each a ∈ A and qk tactical target, denote V (a, k) = DKL (Da qk) pt .",
                "Let Vk(a) = 1 Zk V (a, k), where Zk = a∈A V (a, k) is a normalization factor. - Select a∗ = arg min a k k=1 w(k)Vk(a) The weights vector w = (w1, ..., wK ) allows the additional tuning of importance among target dynamics without the need to redesign the targets themselves.",
                "This balancing method is also seamlessly integrated into the EMT-based control flow of operation. 4.2 EMT-based Control Limitations EMT-based control is a sub-optimal (in the DBC sense) representative of the DBC structure.",
                "It limits the User by forcing EMT to be its dynamic tracking algorithm, and replaces Agent optimization by greedy action selection.",
                "This kind of combination, however, is common for on-line algorithms.",
                "Although further development of EMT-based controllers is necessary, evidence so far suggests that even the simplest form of the algorithm possesses a great deal of power, and displays trends that are optimal in the DBC sense of the word.",
                "There are two further, EMT-specific, limitations to EMT-based control that are evident at this point.",
                "Both already have partial solutions and are subjects of ongoing research.",
                "The first limitation is the problem of negative preference.",
                "In the POMDP framework for example, this is captured simply, through The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 793 the appearance of values with different signs within the reward structure.",
                "For EMT-based control, however, negative preference means that one would like to avoid a certain distribution over system development sequences; EMT-based control, however, concentrates on getting as close as possible to a distribution.",
                "Avoidance is thus unnatural in native EMT-based control.",
                "The second limitation comes from the fact that standard environment modeling can create pure sensory actions-actions that do not change the state of the world, and differ only in the way observations are received and the quality of observations received.",
                "Since the world state does not change, EMT-based control would not be able to differentiate between different sensory actions.",
                "Notice that both of these limitations of EMT-based control are absent from the general DBC framework, since it may have a tracking algorithm capable of considering pure sensory actions and, unlike Kullback-Leibler divergence, a distribution deviation measure that is capable of dealing with negative preference. 5.",
                "EMT PLAYING TAG The Game of Tag was first introduced in [11].",
                "It is a single agent problem of capturing a quarry, and belongs to the class of area sweeping problems.",
                "An example domain is shown in Figure 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figure 2: Tag domain; an agent (A) attempts to seek and capture a quarry (Q) The Game of Tag extremely limits the agents perception, so that the agent is able to detect the quarry only if they are co-located in the same cell of the grid world.",
                "In the classical version of the game, co-location leads to a special observation, and the Tag action can be performed.",
                "We slightly modify this setting: the moment that both agents occupy the same cell, the game ends.",
                "As a result, both the agent and its quarry have the same motion capability, which allows them to move in four directions, North, South, East, and West.",
                "These form a formal space of actions within a Markovian environment.",
                "The state space of the formal Markovian environment is described by the cross-product of the agent and quarrys positions.",
                "For Figure 2, it would be S = {s0, ..., s23} × {s0, ..., s23}.",
                "The effects of an action taken by the agent are deterministic, but the environment in general has a stochastic response due to the motion of the quarry.",
                "With probability q0 1 it stays put, and with probability 1 − q0 it moves to an adjacent cell further away from the 1 In our experiments this was taken to be q0 = 0.2. agent.",
                "So for the instance shown in Figure 2 and q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Although the evasive behavior of the quarry is known to the agent, the quarrys position is not.",
                "The only sensory information available to the agent is its own location.",
                "We use EMT and directly specify the target dynamics.",
                "For the Game of Tag, we can easily formulate three major trends: catching the quarry, staying mobile, and stalking the quarry.",
                "This results in the following three target dynamics: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 otherwise Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 otherwise Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Note that none of the above targets are directly achievable; for instance, if Qt = s9 and At = s11, there is no action that can move the agent to At+1 = s9 as required by the Tcatch target dynamics.",
                "We ran several experiments to evaluate EMT performance in the Tag Game.",
                "Three configurations of the domain shown in Figure 3 were used, each posing a different challenge to the agent due to partial observability.",
                "In each setting, a set of 1000 runs was performed with a time limit of 100 steps.",
                "In every run, the initial position of both the agent and its quarry was selected at random; this means that as far as the agent was concerned, the quarrys initial position was uniformly distributed over the entire domain cell space.",
                "We also used two variations of the environment observability function.",
                "In the first version, observability function mapped all joint positions of hunter and quarry into the position of the hunter as an observation.",
                "In the second, only those joint positions in which hunter and quarry occupied different locations were mapped into the hunters location.",
                "The second version in fact utilized and expressed the fact that once hunter and quarry occupy the same cell the game ends.",
                "The results of these experiments are shown in Table 2.",
                "Balancing [15] the catch, move, and stalk target dynamics described in the previous section by the weight vector [0.8, 0.1, 0.1], EMT produced stable performance in all three domains.",
                "Although direct comparisons are difficult to make, the EMT performance displayed notable efficiency vis-`a-vis the POMDP approach.",
                "In spite of a simple and inefficient Matlab implementation of the EMT algorithm, the decision time for any given step averaged significantly below 1 second in all experiments.",
                "For the irregular open arena domain, which proved to be the most difficult, 1000 experiment runs bounded by 100 steps each, a total of 42411 steps, were completed in slightly under 6 hours.",
                "That is, over 4 × 104 online steps took an order of magnitude less time than the offline computation of POMDP policy in [11].",
                "The significance of this differential is made even more prominent by the fact that, should the environment model parameters change, the online nature of EMT would allow it to maintain its performance time, while the POMDP policy would need to be recomputed, requiring yet again a large overhead of computation.",
                "We also tested the behavior cell frequency entropy, empirical measures from trial data.",
                "As Figure 4 and Figure 5 show, empir794 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) A Q Q A 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 A Q Figure 3: These configurations of the Tag Game space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor Table 2: Performance of the EMT-based solution in three Tag Game domains and two observability models: I) omniposition quarry, II) quarry is not at hunters position Model Domain Capture% E(Steps) Time/Step I Dead-ends 100 14.8 72(mSec) Arena 80.2 42.4 500(mSec) Circle 91.4 34.6 187(mSec) II Dead-ends 100 13.2 91(mSec) Arena 96.8 28.67 396(mSec) Circle 94.4 31.63 204(mSec) ical entropy grows with the length of interaction.",
                "For runs where the quarry was not captured immediately, the entropy reaches between 0.85 and 0.952 for different runs and scenarios.",
                "As the agent actively seeks the quarry, the entropy never reaches its maximum.",
                "One characteristic of the entropy graph for the open arena scenario particularly caught our attention in the case of the omniposition quarry observation model.",
                "Near the maximum limit of trial length (100 steps), entropy suddenly dropped.",
                "Further analysis of the data showed that under certain circumstances, a fluctuating behavior occurs in which the agent faces equally viable versions of quarry-following behavior.",
                "Since the EMT algorithm has greedy action selection, and the state space does not encode any form of commitment (not even speed or acceleration), the agent is locked within a small portion of cells.",
                "It is essentially attempting to simultaneously follow several courses of action, all of which are consistent with the target dynamics.",
                "This behavior did not occur in our second observation model, since it significantly reduced the set of eligible courses of action-essentially contributing to tie-breaking among them. 6.",
                "DISCUSSION The design of the EMT solution for the Tag Game exposes the core difference in approach to planning and control between EMT or DBC, on the one hand, and the more familiar POMDP approach, on the other.",
                "POMDP defines a reward structure to optimize, and influences <br>system dynamics</br> indirectly through that optimization.",
                "EMT discards any reward scheme, and instead measures and influences <br>system dynamics</br> directly. 2 Entropy was calculated using log base equal to the number of possible locations within the domain; this properly scales entropy expression into the range [0, 1] for all domains.",
                "Thus for the Tag Game, we did not search for a reward function that would encode and express our preference over the agents behavior, but rather directly set three (heuristic) behavior preferences as the basis for target dynamics to be maintained.",
                "Experimental data shows that these targets need not be directly achievable via the agents actions.",
                "However, the ratio between EMT performance and achievability of target dynamics remains to be explored.",
                "The tag game experiment data also revealed the different emphasis DBC and POMDPs place on the formulation of an environment state space.",
                "POMDPs rely entirely on the mechanism of reward accumulation maximization, i.e., formation of the action selection procedure to achieve necessary state sequencing.",
                "DBC, on the other hand, has two sources of sequencing specification: through the properties of an action selection procedure, and through direct specification within the target dynamics.",
                "The importance of the second source was underlined by the Tag Game experiment data, in which the greedy EMT algorithm, applied to a POMDP-type state space specification, failed, since target description over such a state space was incapable of encoding the necessary behavior tendencies, e.g., tie-breaking and commitment to directed motion.",
                "The structural differences between DBC (and EMT in particular), and POMDPs, prohibits direct performance comparison, and places them on complementary tracks, each within a suitable niche.",
                "For instance, POMDPs could be seen as a much more natural formulation of economic sequential decision-making problems, while EMT is a better fit for continual demand for stochastic change, as happens in many robotic or embodied-agent problems.",
                "The complementary properties of POMDPs and EMT can be further exploited.",
                "There is recent interest in using POMDPs in hybrid solutions [17], in which the POMDPs can be used together with other control approaches to provide results not easily achievable with either approach by itself.",
                "DBC can be an effective partner in such a hybrid solution.",
                "For instance, POMDPs have prohibitively large off-line time requirements for policy computation, but can be readily used in simpler settings to expose beneficial behavioral trends; this can serve as a form of target dynamics that are provided to EMT in a larger domain for on-line operation. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have presented a novel perspective on the process of planning and control in stochastic environments, in the form of the Dynamics Based Control (DBC) framework.",
                "DBC formulates the task of planning as support of a specified target <br>system dynamics</br>, which describes the necessary properties of change within the environment.",
                "Optimality of DBC plans of action are measured The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 4: Observation Model I: Omniposition quarry.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 5: Observation Model II: quarry not observed at hunters position.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. with respect to the deviation of actual <br>system dynamics</br> from the target dynamics.",
                "We show that a recently developed technique of Extended Markov Tracking (EMT) [13] is an instantiation of DBC.",
                "In fact, EMT can be seen as a specific case of DBC parameterization, which employs a greedy action selection procedure.",
                "Since EMT exhibits the key features of the general DBC framework, as well as polynomial time complexity, we used the multitarget version of EMT [15] to demonstrate that the class of area sweeping problems naturally lends itself to dynamics-based descriptions, as instantiated by our experiments in the Tag Game domain.",
                "As enumerated in Section 4.2, EMT has a number of limitations, such as difficulty in dealing with negative dynamic preference.",
                "This prevents direct application of EMT to such problems as Rendezvous-Evasion Games (e.g., [6]).",
                "However, DBC in general has no such limitations, and readily enables the formulation of evasion games.",
                "In future work, we intend to proceed with the development of dynamics-based controllers for these problems. 8.",
                "ACKNOWLEDGMENT The work of the first two authors was partially supported by Israel Science Foundation grant #898/05, and the third author was partially supported by a grant from Israels Ministry of Science and Technology. 9.",
                "REFERENCES [1] R. C. Arkin.",
                "Behavior-Based Robotics.",
                "MIT Press, 1998. [2] J.",
                "A. Bilmes.",
                "A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and Hidden Markov Models.",
                "Technical Report TR-97-021, Department of Electrical Engeineering and Computer Science, University of California at Berkeley, 1998. [3] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of information theory.",
                "Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz, and M. J. Wolverton.",
                "A survey of research in distributed, continual planning.",
                "AI Magazine, 4:13-22, 1999. [5] V. R. Konda and J. N. Tsitsiklis.",
                "Actor-Critic algorithms.",
                "SIAM Journal on Control and Optimization, 42(4):1143-1166, 2003. [6] W. S. Lim.",
                "A rendezvous-evasion game on discrete locations with joint randomization.",
                "Advances in Applied Probability, 29(4):1004-1017, December 1997. [7] M. L. Littman, T. L. Dean, and L. P. Kaelbling.",
                "On the complexity of solving Markov decision problems.",
                "In Proceedings of the 11th Annual Conference on Uncertainty in Artificial Intelligence (UAI-95), pages 394-402, 1995. [8] O. Madani, S. Hanks, and A. Condon.",
                "On the undecidability of probabilistic planning and related stochastic optimization problems.",
                "Artificial Intelligence Journal, 147(1-2):5-34, July 2003. [9] R. M. Neal and G. E. Hinton.",
                "A view of the EM algorithm 796 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) that justifies incremental, sparse, and other variants.",
                "In M. I. Jordan, editor, Learning in Graphical Models, pages 355-368.",
                "Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In Proceeding of AAMAS 2006, 2006. [11] J. Pineau, G. Gordon, and S. Thrun.",
                "Point-based value iteration: An anytime algorithm for pomdps.",
                "In International Joint Conference on Artificial Intelligence (IJCAI), pages 1025-1032, August 2003. [12] M. L. Puterman.",
                "Markov Decision Processes.",
                "Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics Section.",
                "Wiley-Interscience Publication, New York, 1994. [13] Z. Rabinovich and J. S. Rosenschein.",
                "Extended Markov Tracking with an application to control.",
                "In The Workshop on Agent Tracking: Modeling Other Agents from Observations, at the Third International Joint Conference on Autonomous Agents and Multiagent Systems, pages 95-100, New-York, July 2004. [14] Z. Rabinovich and J. S. Rosenschein.",
                "Multiagent coordination by Extended Markov Tracking.",
                "In The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 431-438, Utrecht, The Netherlands, July 2005. [15] Z. Rabinovich and J. S. Rosenschein.",
                "On the response of EMT-based control to interacting targets and models.",
                "In The Fifth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 465-470, Hakodate, Japan, May 2006. [16] R. F. Stengel.",
                "Optimal Control and Estimation.",
                "Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, and P. Varakantham.",
                "Conflicts in teamwork: Hybrids to the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 797"
            ],
            "original_annotated_samples": [
                "Unlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified <br>system dynamics</br>.",
                "We do not use a (state-based) reward function, but instead optimize over a different criterion, a transition-based specification of the desired <br>system dynamics</br>.",
                "In fact, any search problem with randomized motion, the socalled class of area sweeping problems, can be described through specification of such target <br>system dynamics</br>.",
                "For example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation constant. • User Level in turn relies on the environment model produced by Environment Design to specify the target <br>system dynamics</br> it wishes to observe.",
                "The User Level also specifies the estimation or learning procedure for <br>system dynamics</br>, and the measure of deviation."
            ],
            "translated_annotated_samples": [
                "A diferencia de los enfoques existentes, que buscan optimizar las recompensas esperadas (por ejemplo, en Problemas de Decisión de Markov Parcialmente Observables (POMDP)), DBC optimiza el comportamiento del sistema hacia las <br>dinámicas del sistema</br> especificadas.",
                "No utilizamos una función de recompensa basada en el estado, sino que optimizamos según un criterio diferente, una especificación basada en transiciones de la <br>dinámica del sistema</br> deseado.",
                "De hecho, cualquier problema de búsqueda con movimiento aleatorio, la llamada clase de problemas de barrido de área, puede ser descrito a través de la especificación de la <br>dinámica de dicho sistema</br> objetivo.",
                "Por ejemplo, este nivel especificaría las leyes de la física dentro del sistema y establecería sus parámetros, como la constante de gravitación. El Nivel de Usuario, a su vez, se basa en el modelo de entorno producido por el Diseño de Entorno para especificar la <br>dinámica del sistema</br> objetivo que desea observar.",
                "El Nivel de Usuario también especifica el procedimiento de estimación o aprendizaje para la <br>dinámica del sistema</br>, y la medida de desviación."
            ],
            "translated_text": "En este artículo presentamos el Control Basado en Dinámicas (DBC), un enfoque para la planificación y control de un agente en entornos estocásticos. A diferencia de los enfoques existentes, que buscan optimizar las recompensas esperadas (por ejemplo, en Problemas de Decisión de Markov Parcialmente Observables (POMDP)), DBC optimiza el comportamiento del sistema hacia las <br>dinámicas del sistema</br> especificadas. Mostramos que un enfoque de planificación y control recientemente desarrollado, Seguimiento Extendido de Markov (EMT), es una instancia de DBC. EMT emplea la selección de acciones codiciosa para proporcionar un algoritmo de control eficiente en entornos markovianos. Explotamos esta eficiencia en un conjunto de experimentos que aplicaron EMT multitarget a una clase de problemas de barrido de área (búsqueda de objetivos en movimiento). Mostramos que tales problemas pueden ser definidos de forma natural y resueltos eficientemente utilizando el marco de DBC y su instanciación de EMT. Categorías y Descriptores de Asignaturas I.2.8 [Resolución de Problemas, Métodos de Control y Búsqueda]: Teoría de Control; I.2.9 [Robótica]; I.2.11 [Inteligencia Artificial Distribuida]: Agentes Inteligentes Términos Generales Algoritmos, Teoría 1. INTRODUCCIÓN La planificación y el control constituyen un área de investigación central en sistemas multiagentes e inteligencia artificial. En los últimos años, los Procesos de Decisión de Markov Parcialmente Observables (POMDPs) [12] se han convertido en una base formal popular para la planificación en entornos estocásticos. En este marco, el problema de planificación y control suele abordarse imponiendo una función de recompensa y calculando una política (de elección de acciones) que sea óptima, en el sentido de que resultará en la mayor utilidad esperada. Si bien teóricamente atractivo, la complejidad de resolver óptimamente un POMDP es prohibitiva [8, 7]. Tomamos una visión alternativa de la planificación en entornos estocásticos. No utilizamos una función de recompensa basada en el estado, sino que optimizamos según un criterio diferente, una especificación basada en transiciones de la <br>dinámica del sistema</br> deseado. La idea aquí es ver la ejecución del plan como un proceso que obliga a un sistema (estocástico) a cambiar, y un plan como un proceso dinámico que moldea ese cambio de acuerdo con criterios deseados. Llamamos a este marco de planificación general Control Basado en Dinámicas (DBC). En DBC, el objetivo de un proceso de planificación (o control) es garantizar que el sistema cambiará de acuerdo con dinámicas objetivo específicas (potencialmente estocásticas). Dado que el comportamiento real del sistema puede desviarse de lo especificado por la dinámica objetivo (debido a la naturaleza estocástica del sistema), la planificación en tales entornos debe ser continua, de manera similar a los controladores en lazo cerrado clásicos. Aquí, la optimalidad se mide en términos de la probabilidad de magnitudes de desviación. En este documento, presentamos la estructura del Control Basado en Dinámicas. Mostramos que el enfoque de Seguimiento Extendido de Markov (EMT) recientemente desarrollado [13, 14, 15] está subsumido por DBC, con EMT empleando selección de acciones codiciosa, que es una parametrización específica entre las opciones posibles dentro de DBC. EMT es una implementación eficiente de DBC. Para evaluar DBC, llevamos a cabo un conjunto de experimentos aplicando EMT de múltiples objetivos al Juego de Etiquetas [11]; esta es una variante del problema de barrido de área, donde un agente intenta etiquetar a un objetivo móvil (presa) cuya posición no se conoce con certeza. Los datos experimentales demuestran que incluso con un modelo simple del entorno y un diseño simple de la dinámica del objetivo, se pueden lograr altas tasas de éxito tanto en la captura de la presa como en sorprender a la presa (como se expresa por la entropía observada de la posición de los agentes controlados). El documento está organizado de la siguiente manera. En la Sección 2 motivamos DBC utilizando problemas de barrido de área, y discutimos trabajos relacionados. La Sección 3 introduce la estructura de Control Basado en Dinámicas (DBC) y su especialización en entornos Markovianos. Esto es seguido por una revisión del enfoque de Seguimiento Markov Extendido (EMT) como un régimen de control estructurado por DBC en la Sección 4. Esa sección también discute las limitaciones del control basado en EMT en relación con el marco general de DBC. Los ajustes experimentales y los resultados se presentan a continuación en la Sección 5. La Sección 6 proporciona una breve discusión del enfoque general, y la Sección 7 ofrece algunas observaciones finales y direcciones para trabajos futuros. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2. MOTIVACIÓN Y TRABAJO RELACIONADO Muchos escenarios de la vida real tienen naturalmente una especificación de dinámica de destino estocástica, especialmente en aquellos dominios donde no existe un objetivo final, sino más bien un comportamiento del sistema (con propiedades específicas) que debe ser continuamente respaldado. Por ejemplo, los guardias de seguridad realizan barridos persistentes de un área para detectar cualquier signo de intrusión. Los ladrones astutos intentarán rastrear estos barridos y sincronizar su operación con puntos clave del movimiento de los guardias. Por lo tanto, es recomendable que la dinámica de movimiento de los guardias parezca irregular y aleatoria. El trabajo reciente de Paruchuri et al. [10] ha abordado dicha aleatorización en el contexto de POMDPs de agente único y distribuidos. El objetivo de ese trabajo era generar políticas que proporcionen una medida de aleatorización en la selección de acciones, manteniendo al mismo tiempo las recompensas dentro de niveles aceptables. Nuestro enfoque difiere de este trabajo en que DBC no optimiza recompensas esperadas, de hecho, no consideramos recompensas en absoluto, sino que en su lugar mantenemos dinámicas deseadas (incluyendo, pero no limitado a, la aleatorización). El juego de la mancha es otro ejemplo de la aplicabilidad del enfoque. Fue introducido en el trabajo de Pineau et al. [11]. Hay dos agentes que pueden moverse por un área, la cual está dividida en una cuadrícula. La cuadrícula puede tener celdas bloqueadas (agujeros) en las cuales ningún agente puede moverse. Un agente (el cazador) intenta moverse a una celda ocupada por el otro (la presa), de modo que estén ubicados en el mismo lugar (esto es una etiqueta exitosa). La presa busca evitar al cazador, siempre está al tanto de la posición del cazador, pero no sabe cómo se comportará el cazador, lo que abre la posibilidad de que un cazador sorprenda a la presa. El cazador conoce la ley de movimiento probabilístico de la presa, pero no conoce su ubicación actual. Tag es una instancia de una familia de problemas de barrido de área (persecución-evasión). En [11], el cazador modeló el problema utilizando un POMDP. Se definió una función de recompensa para reflejar el deseo de marcar la presa, y se calculó una política de acción para optimizar la recompensa recolectada con el tiempo. Debido a la complejidad intratable de determinar la política óptima, la política de acción calculada en ese documento fue esencialmente una aproximación. En este artículo, en lugar de formular una función de recompensa, utilizamos EMT para resolver el problema, especificando directamente la dinámica objetivo. De hecho, cualquier problema de búsqueda con movimiento aleatorio, la llamada clase de problemas de barrido de área, puede ser descrito a través de la especificación de la <br>dinámica de dicho sistema</br> objetivo. El Control Basado en Dinámicas proporciona un enfoque natural para resolver estos problemas. CONTROL BASADO EN DINÁMICA La especificación del Control Basado en Dinámica (DBC) se puede dividir en tres niveles interactivos: Nivel de Diseño del Entorno, Nivel de Usuario y Nivel de Agente. • El Nivel de Diseño del Entorno se ocupa de la especificación formal y modelado del entorno. Por ejemplo, este nivel especificaría las leyes de la física dentro del sistema y establecería sus parámetros, como la constante de gravitación. El Nivel de Usuario, a su vez, se basa en el modelo de entorno producido por el Diseño de Entorno para especificar la <br>dinámica del sistema</br> objetivo que desea observar. El Nivel de Usuario también especifica el procedimiento de estimación o aprendizaje para la <br>dinámica del sistema</br>, y la medida de desviación. ",
            "candidates": [],
            "error": [
                [
                    "dinámicas del sistema",
                    "dinámica del sistema",
                    "dinámica de dicho sistema",
                    "dinámica del sistema",
                    "dinámica del sistema"
                ]
            ]
        },
        "control": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Dynamics Based <br>control</br> with an Application to Area-Sweeping Problems Zinovi Rabinovich Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel jeff@cs.huji.ac.il Gal A. Kaminka The MAVERICK Group Department of Computer Science Bar Ilan University, Israel galk@cs.biu.ac.il ABSTRACT In this paper we introduce Dynamics Based <br>control</br> (DBC), an approach to planning and control of an agent in stochastic environments.",
                "Unlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics.",
                "We show that a recently developed planning and <br>control</br> approach, Extended Markov Tracking (EMT) is an instantiation of DBC.",
                "EMT employs greedy action selection to provide an efficient <br>control</br> algorithm in Markovian environments.",
                "We exploit this efficiency in a set of experiments that applied multitarget EMT to a class of area-sweeping problems (searching for moving targets).",
                "We show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation.",
                "Categories and Subject Descriptors I.2.8 [Problem Solving, <br>control</br> Methods, and Search]: <br>control</br> Theory; I.2.9 [Robotics]; I.2.11 [Distributed Artificial Intelligence]: Intelligent Agents General Terms Algorithms, Theory 1.",
                "INTRODUCTION Planning and <br>control</br> constitutes a central research area in multiagent systems and artificial intelligence.",
                "In recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in stochastic environments.",
                "In this framework, the planning and <br>control</br> problem is often addressed by imposing a reward function, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility.",
                "While theoretically attractive, the complexity of optimally solving a POMDP is prohibitive [8, 7].",
                "We take an alternative view of planning in stochastic environments.",
                "We do not use a (state-based) reward function, but instead optimize over a different criterion, a transition-based specification of the desired system dynamics.",
                "The idea here is to view planexecution as a process that compels a (stochastic) system to change, and a plan as a dynamic process that shapes that change according to desired criteria.",
                "We call this general planning framework Dynamics Based <br>control</br> (DBC).",
                "In DBC, the goal of a planning (or <br>control</br>) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) target dynamics.",
                "As actual system behavior may deviate from that which is specified by target dynamics (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16].",
                "Here, optimality is measured in terms of probability of deviation magnitudes.",
                "In this paper, we present the structure of Dynamics Based <br>control</br>.",
                "We show that the recently developed Extended Markov Tracking (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC.",
                "EMT is an efficient instantiation of DBC.",
                "To evaluate DBC, we carried out a set of experiments applying multi-target EMT to the Tag Game [11]; this is a variant on the area sweeping problem, where an agent is trying to tag a moving target (quarry) whose position is not known with certainty.",
                "Experimental data demonstrates that even with a simple model of the environment and a simple design of target dynamics, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agents position).",
                "The paper is organized as follows.",
                "In Section 2 we motivate DBC using area-sweeping problems, and discuss related work.",
                "Section 3 introduces the Dynamics Based <br>control</br> (DBC) structure, and its specialization to Markovian environments.",
                "This is followed by a review of the Extended Markov Tracking (EMT) approach as a DBC-structured <br>control</br> regimen in Section 4.",
                "That section also discusses the limitations of EMT-based <br>control</br> relative to the general DBC framework.",
                "Experimental settings and results are then presented in Section 5.",
                "Section 6 provides a short discussion of the overall approach, and Section 7 gives some concluding remarks and directions for future work. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "MOTIVATION AND RELATED WORK Many real-life scenarios naturally have a stochastic target dynamics specification, especially those domains where there exists no ultimate goal, but rather system behavior (with specific properties) that has to be continually supported.",
                "For example, security guards perform persistent sweeps of an area to detect any sign of intrusion.",
                "Cunning thieves will attempt to track these sweeps, and time their operation to key points of the guards motion.",
                "It is thus advisable to make the guards motion dynamics appear irregular and random.",
                "Recent work by Paruchuri et al. [10] has addressed such randomization in the context of single-agent and distributed POMDPs.",
                "The goal in that work was to generate policies that provide a measure of action-selection randomization, while maintaining rewards within some acceptable levels.",
                "Our focus differs from this work in that DBC does not optimize expected rewards-indeed we do not consider rewards at all-but instead maintains desired dynamics (including, but not limited to, randomization).",
                "The Game of Tag is another example of the applicability of the approach.",
                "It was introduced in the work by Pineau et al. [11].",
                "There are two agents that can move about an area, which is divided into a grid.",
                "The grid may have blocked cells (holes) into which no agent can move.",
                "One agent (the hunter) seeks to move into a cell occupied by the other (the quarry), such that they are co-located (this is a successful tag).",
                "The quarry seeks to avoid the hunter agent, and is always aware of the hunters position, but does not know how the hunter will behave, which opens up the possibility for a hunter to surprise the prey.",
                "The hunter knows the quarrys probabilistic law of motion, but does not know its current location.",
                "Tag is an instance of a family of area-sweeping (pursuit-evasion) problems.",
                "In [11], the hunter modeled the problem using a POMDP.",
                "A reward function was defined, to reflect the desire to tag the quarry, and an action policy was computed to optimize the reward collected over time.",
                "Due to the intractable complexity of determining the optimal policy, the action policy computed in that paper was essentially an approximation.",
                "In this paper, instead of formulating a reward function, we use EMT to solve the problem, by directly specifying the target dynamics.",
                "In fact, any search problem with randomized motion, the socalled class of area sweeping problems, can be described through specification of such target system dynamics.",
                "Dynamics Based <br>control</br> provides a natural approach to solving these problems. 3.",
                "DYNAMICS BASED <br>control</br> The specification of Dynamics Based <br>control</br> (DBC) can be broken into three interacting levels: Environment Design Level, User Level, and Agent Level. • Environment Design Level is concerned with the formal specification and modeling of the environment.",
                "For example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation constant. • User Level in turn relies on the environment model produced by Environment Design to specify the target system dynamics it wishes to observe.",
                "The User Level also specifies the estimation or learning procedure for system dynamics, and the measure of deviation.",
                "In the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping. • Agent Level in turn combines the environment model from the Environment Design level, the dynamics estimation procedure, the deviation measure and the target dynamics specification from User Level, to produce a sequence of actions that create system dynamics as close as possible to the targeted specification.",
                "As we are interested in the continual development of a stochastic system, such as happens in classical <br>control</br> theory [16] and continual planning [4], as well as in our example of museum sweeps, the question becomes how the Agent Level is to treat the deviation measurements over time.",
                "To this end, we use a probability threshold-that is, we would like the Agent Level to maximize the probability that the deviation measure will remain below a certain threshold.",
                "Specific action selection then depends on system formalization.",
                "One possibility would be to create a mixture of available system trends, much like that which happens in Behavior-Based Robotic architectures [1].",
                "The other alternative would be to rely on the estimation procedure provided by the User Level-to utilize the Environment Design Level model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a certain dynamics has been achieved.",
                "Notice that this manipulation is not direct, but via the environment.",
                "Thus, for strong enough estimator algorithms, successful manipulation would mean a successful simulation of the specified target dynamics (i.e., beyond discerning via the available sensory input).",
                "DBC levels can also have a back-flow of information (see Figure 1).",
                "For instance, the Agent Level could provide data about target dynamics feasibility, allowing the User Level to modify the requirement, perhaps focusing on attainable features of system behavior.",
                "Data would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the User Level, this can provide an important tool for the environment model calibration at the Environment Design Level.",
                "UserEnv.",
                "Design Agent Model Ideal Dynamics Estimator Estimator Dynamics Feasibility System Response Data Figure 1: Data flow of the DBC framework Extending upon the idea of Actor-Critic algorithms [5], DBC data flow can provide a good basis for the design of a learning algorithm.",
                "For example, the User Level can operate as an exploratory device for a learning algorithm, inferring an ideal dynamics target from the environment model at hand that would expose and verify most critical features of system behavior.",
                "In this case, feasibility and system response data from the Agent Level would provide key information for an environment model update.",
                "In fact, the combination of feasibility and response data can provide a basis for the application of strong learning algorithms such as EM [2, 9]. 3.1 DBC for Markovian Environments For a Partially Observable Markovian Environment, DBC can be specified in a more rigorous manner.",
                "Notice how DBC discards rewards, and replaces it by another optimality criterion (structural differences are summarized in Table 1): • Environment Design level is to specify a tuple < S, A, T, O, Ω, s0 >, where: - S is the set of all possible environment states; - s0 is the initial state of the environment (which can also be viewed as a probability distribution over S); The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 791 - A is the set of all possible actions applicable in the environment; - T is the environments probabilistic transition function: T : S ×A → Π(S).",
                "That is, T(s |a, s) is the probability that the environment will move from state s to state s under action a; - O is the set of all possible observations.",
                "This is what the sensor input would look like for an outside observer; - Ω is the observation probability function: Ω : S × A × S → Π(O).",
                "That is, Ω(o|s , a, s) is the probability that one will observe o given that the environment has moved from state s to state s under action a. • User Level, in the case of Markovian environment, operates on the set of system dynamics described by a family of conditional probabilities F = {τ : S × A → Π(S)}.",
                "Thus specification of target dynamics can be expressed by q ∈ F, and the learning or tracking algorithm can be represented as a function L : O×(A×O)∗ → F; that is, it maps sequences of observations and actions performed so far into an estimate τ ∈ F of system dynamics.",
                "There are many possible variations available at the User Level to define divergence between system dynamics; several of them are: - Trace distance or L1 distance between two distributions p and q defined by D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Fidelity measure of distance F(p(·), q(·)) = x p(x)q(x) - Kullback-Leibler divergence DKL(p(·) q(·)) = x p(x) log p(x) q(x) Notice that the latter two are not actually metrics over the space of possible distributions, but nevertheless have meaningful and important interpretations.",
                "For instance, KullbackLeibler divergence is an important tool of information theory [3] that allows one to measure the price of encoding an information source governed by q, while assuming that it is governed by p. The User Level also defines the threshold of dynamics deviation probability θ. • Agent Level is then faced with a problem of selecting a <br>control</br> signal function a∗ to satisfy a minimization problem as follows: a∗ = arg min a Pr(d(τa, q) > θ) where d(τa, q) is a random variable describing deviation of the dynamics estimate τa, created by L under <br>control</br> signal a, from the ideal dynamics q.",
                "Implicit in this minimization problem is that L is manipulated via the environment, based on the environment model produced by the Environment Design Level. 3.2 DBC View of the State Space It is important to note the complementary view that DBC and POMDPs take on the state space of the environment.",
                "POMDPs regard state as a stationary snap-shot of the environment; whatever attributes of state sequencing one seeks are reached through properties of the <br>control</br> process, in this case reward accumulation.",
                "This can be viewed as if the sequencing of states and the attributes of that sequencing are only introduced by and for the controlling mechanism-the POMDP policy.",
                "DBC concentrates on the underlying principle of state sequencing, the system dynamics.",
                "DBCs target dynamics specification can use the environments state space as a means to describe, discern, and preserve changes that occur within the system.",
                "As a result, DBC has a greater ability to express state sequencing properties, which are grounded in the environment model and its state space definition.",
                "For example, consider the task of moving through rough terrain towards a goal and reaching it as fast as possible.",
                "POMDPs would encode terrain as state space points, while speed would be ensured by negative reward for every step taken without reaching the goalaccumulating higher reward can be reached only by faster motion.",
                "Alternatively, the state space could directly include the notion of speed.",
                "For POMDPs, this would mean that the same concept is encoded twice, in some sense: directly in the state space, and indirectly within reward accumulation.",
                "Now, even if the reward function would encode more, and finer, details of the properties of motion, the POMDP solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure.",
                "On the other hand, the tactical target expression of variations and correlations between position and speed of motion is now grounded in the state space representation.",
                "In this situation, any further constraints, e.g., smoothness of motion, speed limits in different locations, or speed reductions during sharp turns, are explicitly and uniformly expressed by the tactical target, and can result in faster and more effective action selection by a DBC algorithm. 4.",
                "EMT-BASED <br>control</br> AS A DBC Recently, a <br>control</br> algorithm was introduced called EMT-based Control [13], which instantiates the DBC framework.",
                "Although it provides an approximate greedy solution in the DBC sense, initial experiments using EMT-based <br>control</br> have been encouraging [14, 15].",
                "EMT-based <br>control</br> is based on the Markovian environment definition, as in the case with POMDPs, but its User and Agent Levels are of the Markovian DBC type of optimality. • User Level of EMT-based <br>control</br> defines a limited-case target system dynamics independent of action: qEMT : S → Π(S).",
                "It then utilizes the Kullback-Leibler divergence measure to compose a momentary system dynamics estimator-the Extended Markov Tracking (EMT) algorithm.",
                "The algorithm keeps a system dynamics estimate τt EMT that is capable of explaining recent change in an auxiliary Bayesian system state estimator from pt−1 to pt, and updates it conservatively using Kullback-Leibler divergence.",
                "Since τt EMT and pt−1,t are respectively the conditional and marginal probabilities over the systems state space, explanation simply means that pt(s ) = s τt EMT (s |s)pt−1(s), and the dynamics estimate update is performed by solving a 792 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: Structure of POMDP vs. Dynamics-Based <br>control</br> in Markovian Environment Level Approach MDP Markovian DBC Environment < S, A, T, O, Ω >,where S - set of states A - set of actions Design T : S × A → Π(S) - transition O - observation set Ω : S × A × S → Π(O) User r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - reward function q - ideal dynamics F - reward remodeling L - dynamics estimator θ - threshold Agent π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) minimization problem: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • Agent Level in EMT-based <br>control</br> is suboptimal with respect to DBC (though it remains within the DBC framework), performing greedy action selection based on prediction of EMTs reaction.",
                "The prediction is based on the environment model provided by the Environment Design level, so that if we denote by Ta the environments transition function limited to action a, and pt−1 is the auxiliary Bayesian system state estimator, then the EMT-based <br>control</br> choice is described by a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1) Note that this follows the Markovian DBC framework precisely: the rewarding optimality of POMDPs is discarded, and in its place a dynamics estimator (EMT in this case) is manipulated via action effects on the environment to produce an estimate close to the specified target system dynamics.",
                "Yet as we mentioned, naive EMTbased <br>control</br> is suboptimal in the DBC sense, and has several additional limitations that do not exist in the general DBC framework (discussed in Section 4.2). 4.1 Multi-Target EMT At times, there may exist several behavioral preferences.",
                "For example, in the case of museum guards, some art items are more heavily guarded, requiring that the guards stay more often in their close vicinity.",
                "On the other hand, no corner of the museum is to be left unchecked, which demands constant motion.",
                "Successful museum security would demand that the guards adhere to, and balance, both of these behaviors.",
                "For EMT-based <br>control</br>, this would mean facing several tactical targets {qk}K k=1, and the question becomes how to merge and balance them.",
                "A balancing mechanism can be applied to resolve this issue.",
                "Note that EMT-based <br>control</br>, while selecting an action, creates a preference vector over the set of actions based on their predicted performance with respect to a given target.",
                "If these preference vectors are normalized, they can be combined into a single unified preference.",
                "This requires replacement of standard EMT-based action selection by the algorithm below [15]: • Given: - a set of target dynamics {qk}K k=1, - vector of weights w(k) • Select action as follows - For each action a ∈ A predict the future state distribution ¯pa t+1 = Ta ∗ pt; - For each action, compute Da = H(¯pa t+1, pt, PDt) - For each a ∈ A and qk tactical target, denote V (a, k) = DKL (Da qk) pt .",
                "Let Vk(a) = 1 Zk V (a, k), where Zk = a∈A V (a, k) is a normalization factor. - Select a∗ = arg min a k k=1 w(k)Vk(a) The weights vector w = (w1, ..., wK ) allows the additional tuning of importance among target dynamics without the need to redesign the targets themselves.",
                "This balancing method is also seamlessly integrated into the EMT-based <br>control</br> flow of operation. 4.2 EMT-based <br>control</br> Limitations EMT-based control is a sub-optimal (in the DBC sense) representative of the DBC structure.",
                "It limits the User by forcing EMT to be its dynamic tracking algorithm, and replaces Agent optimization by greedy action selection.",
                "This kind of combination, however, is common for on-line algorithms.",
                "Although further development of EMT-based controllers is necessary, evidence so far suggests that even the simplest form of the algorithm possesses a great deal of power, and displays trends that are optimal in the DBC sense of the word.",
                "There are two further, EMT-specific, limitations to EMT-based <br>control</br> that are evident at this point.",
                "Both already have partial solutions and are subjects of ongoing research.",
                "The first limitation is the problem of negative preference.",
                "In the POMDP framework for example, this is captured simply, through The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 793 the appearance of values with different signs within the reward structure.",
                "For EMT-based <br>control</br>, however, negative preference means that one would like to avoid a certain distribution over system development sequences; EMT-based <br>control</br>, however, concentrates on getting as close as possible to a distribution.",
                "Avoidance is thus unnatural in native EMT-based <br>control</br>.",
                "The second limitation comes from the fact that standard environment modeling can create pure sensory actions-actions that do not change the state of the world, and differ only in the way observations are received and the quality of observations received.",
                "Since the world state does not change, EMT-based <br>control</br> would not be able to differentiate between different sensory actions.",
                "Notice that both of these limitations of EMT-based <br>control</br> are absent from the general DBC framework, since it may have a tracking algorithm capable of considering pure sensory actions and, unlike Kullback-Leibler divergence, a distribution deviation measure that is capable of dealing with negative preference. 5.",
                "EMT PLAYING TAG The Game of Tag was first introduced in [11].",
                "It is a single agent problem of capturing a quarry, and belongs to the class of area sweeping problems.",
                "An example domain is shown in Figure 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figure 2: Tag domain; an agent (A) attempts to seek and capture a quarry (Q) The Game of Tag extremely limits the agents perception, so that the agent is able to detect the quarry only if they are co-located in the same cell of the grid world.",
                "In the classical version of the game, co-location leads to a special observation, and the Tag action can be performed.",
                "We slightly modify this setting: the moment that both agents occupy the same cell, the game ends.",
                "As a result, both the agent and its quarry have the same motion capability, which allows them to move in four directions, North, South, East, and West.",
                "These form a formal space of actions within a Markovian environment.",
                "The state space of the formal Markovian environment is described by the cross-product of the agent and quarrys positions.",
                "For Figure 2, it would be S = {s0, ..., s23} × {s0, ..., s23}.",
                "The effects of an action taken by the agent are deterministic, but the environment in general has a stochastic response due to the motion of the quarry.",
                "With probability q0 1 it stays put, and with probability 1 − q0 it moves to an adjacent cell further away from the 1 In our experiments this was taken to be q0 = 0.2. agent.",
                "So for the instance shown in Figure 2 and q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Although the evasive behavior of the quarry is known to the agent, the quarrys position is not.",
                "The only sensory information available to the agent is its own location.",
                "We use EMT and directly specify the target dynamics.",
                "For the Game of Tag, we can easily formulate three major trends: catching the quarry, staying mobile, and stalking the quarry.",
                "This results in the following three target dynamics: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 otherwise Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 otherwise Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Note that none of the above targets are directly achievable; for instance, if Qt = s9 and At = s11, there is no action that can move the agent to At+1 = s9 as required by the Tcatch target dynamics.",
                "We ran several experiments to evaluate EMT performance in the Tag Game.",
                "Three configurations of the domain shown in Figure 3 were used, each posing a different challenge to the agent due to partial observability.",
                "In each setting, a set of 1000 runs was performed with a time limit of 100 steps.",
                "In every run, the initial position of both the agent and its quarry was selected at random; this means that as far as the agent was concerned, the quarrys initial position was uniformly distributed over the entire domain cell space.",
                "We also used two variations of the environment observability function.",
                "In the first version, observability function mapped all joint positions of hunter and quarry into the position of the hunter as an observation.",
                "In the second, only those joint positions in which hunter and quarry occupied different locations were mapped into the hunters location.",
                "The second version in fact utilized and expressed the fact that once hunter and quarry occupy the same cell the game ends.",
                "The results of these experiments are shown in Table 2.",
                "Balancing [15] the catch, move, and stalk target dynamics described in the previous section by the weight vector [0.8, 0.1, 0.1], EMT produced stable performance in all three domains.",
                "Although direct comparisons are difficult to make, the EMT performance displayed notable efficiency vis-`a-vis the POMDP approach.",
                "In spite of a simple and inefficient Matlab implementation of the EMT algorithm, the decision time for any given step averaged significantly below 1 second in all experiments.",
                "For the irregular open arena domain, which proved to be the most difficult, 1000 experiment runs bounded by 100 steps each, a total of 42411 steps, were completed in slightly under 6 hours.",
                "That is, over 4 × 104 online steps took an order of magnitude less time than the offline computation of POMDP policy in [11].",
                "The significance of this differential is made even more prominent by the fact that, should the environment model parameters change, the online nature of EMT would allow it to maintain its performance time, while the POMDP policy would need to be recomputed, requiring yet again a large overhead of computation.",
                "We also tested the behavior cell frequency entropy, empirical measures from trial data.",
                "As Figure 4 and Figure 5 show, empir794 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) A Q Q A 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 A Q Figure 3: These configurations of the Tag Game space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor Table 2: Performance of the EMT-based solution in three Tag Game domains and two observability models: I) omniposition quarry, II) quarry is not at hunters position Model Domain Capture% E(Steps) Time/Step I Dead-ends 100 14.8 72(mSec) Arena 80.2 42.4 500(mSec) Circle 91.4 34.6 187(mSec) II Dead-ends 100 13.2 91(mSec) Arena 96.8 28.67 396(mSec) Circle 94.4 31.63 204(mSec) ical entropy grows with the length of interaction.",
                "For runs where the quarry was not captured immediately, the entropy reaches between 0.85 and 0.952 for different runs and scenarios.",
                "As the agent actively seeks the quarry, the entropy never reaches its maximum.",
                "One characteristic of the entropy graph for the open arena scenario particularly caught our attention in the case of the omniposition quarry observation model.",
                "Near the maximum limit of trial length (100 steps), entropy suddenly dropped.",
                "Further analysis of the data showed that under certain circumstances, a fluctuating behavior occurs in which the agent faces equally viable versions of quarry-following behavior.",
                "Since the EMT algorithm has greedy action selection, and the state space does not encode any form of commitment (not even speed or acceleration), the agent is locked within a small portion of cells.",
                "It is essentially attempting to simultaneously follow several courses of action, all of which are consistent with the target dynamics.",
                "This behavior did not occur in our second observation model, since it significantly reduced the set of eligible courses of action-essentially contributing to tie-breaking among them. 6.",
                "DISCUSSION The design of the EMT solution for the Tag Game exposes the core difference in approach to planning and <br>control</br> between EMT or DBC, on the one hand, and the more familiar POMDP approach, on the other.",
                "POMDP defines a reward structure to optimize, and influences system dynamics indirectly through that optimization.",
                "EMT discards any reward scheme, and instead measures and influences system dynamics directly. 2 Entropy was calculated using log base equal to the number of possible locations within the domain; this properly scales entropy expression into the range [0, 1] for all domains.",
                "Thus for the Tag Game, we did not search for a reward function that would encode and express our preference over the agents behavior, but rather directly set three (heuristic) behavior preferences as the basis for target dynamics to be maintained.",
                "Experimental data shows that these targets need not be directly achievable via the agents actions.",
                "However, the ratio between EMT performance and achievability of target dynamics remains to be explored.",
                "The tag game experiment data also revealed the different emphasis DBC and POMDPs place on the formulation of an environment state space.",
                "POMDPs rely entirely on the mechanism of reward accumulation maximization, i.e., formation of the action selection procedure to achieve necessary state sequencing.",
                "DBC, on the other hand, has two sources of sequencing specification: through the properties of an action selection procedure, and through direct specification within the target dynamics.",
                "The importance of the second source was underlined by the Tag Game experiment data, in which the greedy EMT algorithm, applied to a POMDP-type state space specification, failed, since target description over such a state space was incapable of encoding the necessary behavior tendencies, e.g., tie-breaking and commitment to directed motion.",
                "The structural differences between DBC (and EMT in particular), and POMDPs, prohibits direct performance comparison, and places them on complementary tracks, each within a suitable niche.",
                "For instance, POMDPs could be seen as a much more natural formulation of economic sequential decision-making problems, while EMT is a better fit for continual demand for stochastic change, as happens in many robotic or embodied-agent problems.",
                "The complementary properties of POMDPs and EMT can be further exploited.",
                "There is recent interest in using POMDPs in hybrid solutions [17], in which the POMDPs can be used together with other <br>control</br> approaches to provide results not easily achievable with either approach by itself.",
                "DBC can be an effective partner in such a hybrid solution.",
                "For instance, POMDPs have prohibitively large off-line time requirements for policy computation, but can be readily used in simpler settings to expose beneficial behavioral trends; this can serve as a form of target dynamics that are provided to EMT in a larger domain for on-line operation. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have presented a novel perspective on the process of planning and <br>control</br> in stochastic environments, in the form of the Dynamics Based <br>control</br> (DBC) framework.",
                "DBC formulates the task of planning as support of a specified target system dynamics, which describes the necessary properties of change within the environment.",
                "Optimality of DBC plans of action are measured The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 4: Observation Model I: Omniposition quarry.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 5: Observation Model II: quarry not observed at hunters position.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. with respect to the deviation of actual system dynamics from the target dynamics.",
                "We show that a recently developed technique of Extended Markov Tracking (EMT) [13] is an instantiation of DBC.",
                "In fact, EMT can be seen as a specific case of DBC parameterization, which employs a greedy action selection procedure.",
                "Since EMT exhibits the key features of the general DBC framework, as well as polynomial time complexity, we used the multitarget version of EMT [15] to demonstrate that the class of area sweeping problems naturally lends itself to dynamics-based descriptions, as instantiated by our experiments in the Tag Game domain.",
                "As enumerated in Section 4.2, EMT has a number of limitations, such as difficulty in dealing with negative dynamic preference.",
                "This prevents direct application of EMT to such problems as Rendezvous-Evasion Games (e.g., [6]).",
                "However, DBC in general has no such limitations, and readily enables the formulation of evasion games.",
                "In future work, we intend to proceed with the development of dynamics-based controllers for these problems. 8.",
                "ACKNOWLEDGMENT The work of the first two authors was partially supported by Israel Science Foundation grant #898/05, and the third author was partially supported by a grant from Israels Ministry of Science and Technology. 9.",
                "REFERENCES [1] R. C. Arkin.",
                "Behavior-Based Robotics.",
                "MIT Press, 1998. [2] J.",
                "A. Bilmes.",
                "A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and Hidden Markov Models.",
                "Technical Report TR-97-021, Department of Electrical Engeineering and Computer Science, University of California at Berkeley, 1998. [3] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of information theory.",
                "Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz, and M. J. Wolverton.",
                "A survey of research in distributed, continual planning.",
                "AI Magazine, 4:13-22, 1999. [5] V. R. Konda and J. N. Tsitsiklis.",
                "Actor-Critic algorithms.",
                "SIAM Journal on <br>control</br> and Optimization, 42(4):1143-1166, 2003. [6] W. S. Lim.",
                "A rendezvous-evasion game on discrete locations with joint randomization.",
                "Advances in Applied Probability, 29(4):1004-1017, December 1997. [7] M. L. Littman, T. L. Dean, and L. P. Kaelbling.",
                "On the complexity of solving Markov decision problems.",
                "In Proceedings of the 11th Annual Conference on Uncertainty in Artificial Intelligence (UAI-95), pages 394-402, 1995. [8] O. Madani, S. Hanks, and A. Condon.",
                "On the undecidability of probabilistic planning and related stochastic optimization problems.",
                "Artificial Intelligence Journal, 147(1-2):5-34, July 2003. [9] R. M. Neal and G. E. Hinton.",
                "A view of the EM algorithm 796 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) that justifies incremental, sparse, and other variants.",
                "In M. I. Jordan, editor, Learning in Graphical Models, pages 355-368.",
                "Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In Proceeding of AAMAS 2006, 2006. [11] J. Pineau, G. Gordon, and S. Thrun.",
                "Point-based value iteration: An anytime algorithm for pomdps.",
                "In International Joint Conference on Artificial Intelligence (IJCAI), pages 1025-1032, August 2003. [12] M. L. Puterman.",
                "Markov Decision Processes.",
                "Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics Section.",
                "Wiley-Interscience Publication, New York, 1994. [13] Z. Rabinovich and J. S. Rosenschein.",
                "Extended Markov Tracking with an application to <br>control</br>.",
                "In The Workshop on Agent Tracking: Modeling Other Agents from Observations, at the Third International Joint Conference on Autonomous Agents and Multiagent Systems, pages 95-100, New-York, July 2004. [14] Z. Rabinovich and J. S. Rosenschein.",
                "Multiagent coordination by Extended Markov Tracking.",
                "In The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 431-438, Utrecht, The Netherlands, July 2005. [15] Z. Rabinovich and J. S. Rosenschein.",
                "On the response of EMT-based <br>control</br> to interacting targets and models.",
                "In The Fifth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 465-470, Hakodate, Japan, May 2006. [16] R. F. Stengel.",
                "Optimal <br>control</br> and Estimation.",
                "Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, and P. Varakantham.",
                "Conflicts in teamwork: Hybrids to the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 797"
            ],
            "original_annotated_samples": [
                "Dynamics Based <br>control</br> with an Application to Area-Sweeping Problems Zinovi Rabinovich Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel jeff@cs.huji.ac.il Gal A. Kaminka The MAVERICK Group Department of Computer Science Bar Ilan University, Israel galk@cs.biu.ac.il ABSTRACT In this paper we introduce Dynamics Based <br>control</br> (DBC), an approach to planning and control of an agent in stochastic environments.",
                "We show that a recently developed planning and <br>control</br> approach, Extended Markov Tracking (EMT) is an instantiation of DBC.",
                "EMT employs greedy action selection to provide an efficient <br>control</br> algorithm in Markovian environments.",
                "Categories and Subject Descriptors I.2.8 [Problem Solving, <br>control</br> Methods, and Search]: <br>control</br> Theory; I.2.9 [Robotics]; I.2.11 [Distributed Artificial Intelligence]: Intelligent Agents General Terms Algorithms, Theory 1.",
                "INTRODUCTION Planning and <br>control</br> constitutes a central research area in multiagent systems and artificial intelligence."
            ],
            "translated_annotated_samples": [
                "En este artículo presentamos el <br>Control Basado en Dinámicas</br> (DBC), un enfoque para la planificación y control de un agente en entornos estocásticos.",
                "Mostramos que un enfoque de planificación y <br>control</br> recientemente desarrollado, Seguimiento Extendido de Markov (EMT), es una instancia de DBC.",
                "EMT emplea la selección de acciones codiciosa para proporcionar un algoritmo de <br>control</br> eficiente en entornos markovianos.",
                "Categorías y Descriptores de Asignaturas I.2.8 [Resolución de Problemas, Métodos de Control y Búsqueda]: Teoría de Control; I.2.9 [Robótica]; I.2.11 [Inteligencia Artificial Distribuida]: Agentes Inteligentes Términos Generales Algoritmos, Teoría 1.",
                "INTRODUCCIÓN La planificación y el <br>control</br> constituyen un área de investigación central en sistemas multiagentes e inteligencia artificial."
            ],
            "translated_text": "En este artículo presentamos el <br>Control Basado en Dinámicas</br> (DBC), un enfoque para la planificación y control de un agente en entornos estocásticos. A diferencia de los enfoques existentes, que buscan optimizar las recompensas esperadas (por ejemplo, en Problemas de Decisión de Markov Parcialmente Observables (POMDP)), DBC optimiza el comportamiento del sistema hacia las dinámicas del sistema especificadas. Mostramos que un enfoque de planificación y <br>control</br> recientemente desarrollado, Seguimiento Extendido de Markov (EMT), es una instancia de DBC. EMT emplea la selección de acciones codiciosa para proporcionar un algoritmo de <br>control</br> eficiente en entornos markovianos. Explotamos esta eficiencia en un conjunto de experimentos que aplicaron EMT multitarget a una clase de problemas de barrido de área (búsqueda de objetivos en movimiento). Mostramos que tales problemas pueden ser definidos de forma natural y resueltos eficientemente utilizando el marco de DBC y su instanciación de EMT. Categorías y Descriptores de Asignaturas I.2.8 [Resolución de Problemas, Métodos de Control y Búsqueda]: Teoría de Control; I.2.9 [Robótica]; I.2.11 [Inteligencia Artificial Distribuida]: Agentes Inteligentes Términos Generales Algoritmos, Teoría 1. INTRODUCCIÓN La planificación y el <br>control</br> constituyen un área de investigación central en sistemas multiagentes e inteligencia artificial. ",
            "candidates": [],
            "error": [
                [
                    "Control Basado en Dinámicas",
                    "control",
                    "control",
                    "control"
                ]
            ]
        },
        "robotic": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Dynamics Based Control with an Application to Area-Sweeping Problems Zinovi Rabinovich Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel jeff@cs.huji.ac.il Gal A. Kaminka The MAVERICK Group Department of Computer Science Bar Ilan University, Israel galk@cs.biu.ac.il ABSTRACT In this paper we introduce Dynamics Based Control (DBC), an approach to planning and control of an agent in stochastic environments.",
                "Unlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics.",
                "We show that a recently developed planning and control approach, Extended Markov Tracking (EMT) is an instantiation of DBC.",
                "EMT employs greedy action selection to provide an efficient control algorithm in Markovian environments.",
                "We exploit this efficiency in a set of experiments that applied multitarget EMT to a class of area-sweeping problems (searching for moving targets).",
                "We show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation.",
                "Categories and Subject Descriptors I.2.8 [Problem Solving, Control Methods, and Search]: Control Theory; I.2.9 [Robotics]; I.2.11 [Distributed Artificial Intelligence]: Intelligent Agents General Terms Algorithms, Theory 1.",
                "INTRODUCTION Planning and control constitutes a central research area in multiagent systems and artificial intelligence.",
                "In recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in stochastic environments.",
                "In this framework, the planning and control problem is often addressed by imposing a reward function, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility.",
                "While theoretically attractive, the complexity of optimally solving a POMDP is prohibitive [8, 7].",
                "We take an alternative view of planning in stochastic environments.",
                "We do not use a (state-based) reward function, but instead optimize over a different criterion, a transition-based specification of the desired system dynamics.",
                "The idea here is to view planexecution as a process that compels a (stochastic) system to change, and a plan as a dynamic process that shapes that change according to desired criteria.",
                "We call this general planning framework Dynamics Based Control (DBC).",
                "In DBC, the goal of a planning (or control) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) target dynamics.",
                "As actual system behavior may deviate from that which is specified by target dynamics (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16].",
                "Here, optimality is measured in terms of probability of deviation magnitudes.",
                "In this paper, we present the structure of Dynamics Based Control.",
                "We show that the recently developed Extended Markov Tracking (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC.",
                "EMT is an efficient instantiation of DBC.",
                "To evaluate DBC, we carried out a set of experiments applying multi-target EMT to the Tag Game [11]; this is a variant on the area sweeping problem, where an agent is trying to tag a moving target (quarry) whose position is not known with certainty.",
                "Experimental data demonstrates that even with a simple model of the environment and a simple design of target dynamics, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agents position).",
                "The paper is organized as follows.",
                "In Section 2 we motivate DBC using area-sweeping problems, and discuss related work.",
                "Section 3 introduces the Dynamics Based Control (DBC) structure, and its specialization to Markovian environments.",
                "This is followed by a review of the Extended Markov Tracking (EMT) approach as a DBC-structured control regimen in Section 4.",
                "That section also discusses the limitations of EMT-based control relative to the general DBC framework.",
                "Experimental settings and results are then presented in Section 5.",
                "Section 6 provides a short discussion of the overall approach, and Section 7 gives some concluding remarks and directions for future work. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "MOTIVATION AND RELATED WORK Many real-life scenarios naturally have a stochastic target dynamics specification, especially those domains where there exists no ultimate goal, but rather system behavior (with specific properties) that has to be continually supported.",
                "For example, security guards perform persistent sweeps of an area to detect any sign of intrusion.",
                "Cunning thieves will attempt to track these sweeps, and time their operation to key points of the guards motion.",
                "It is thus advisable to make the guards motion dynamics appear irregular and random.",
                "Recent work by Paruchuri et al. [10] has addressed such randomization in the context of single-agent and distributed POMDPs.",
                "The goal in that work was to generate policies that provide a measure of action-selection randomization, while maintaining rewards within some acceptable levels.",
                "Our focus differs from this work in that DBC does not optimize expected rewards-indeed we do not consider rewards at all-but instead maintains desired dynamics (including, but not limited to, randomization).",
                "The Game of Tag is another example of the applicability of the approach.",
                "It was introduced in the work by Pineau et al. [11].",
                "There are two agents that can move about an area, which is divided into a grid.",
                "The grid may have blocked cells (holes) into which no agent can move.",
                "One agent (the hunter) seeks to move into a cell occupied by the other (the quarry), such that they are co-located (this is a successful tag).",
                "The quarry seeks to avoid the hunter agent, and is always aware of the hunters position, but does not know how the hunter will behave, which opens up the possibility for a hunter to surprise the prey.",
                "The hunter knows the quarrys probabilistic law of motion, but does not know its current location.",
                "Tag is an instance of a family of area-sweeping (pursuit-evasion) problems.",
                "In [11], the hunter modeled the problem using a POMDP.",
                "A reward function was defined, to reflect the desire to tag the quarry, and an action policy was computed to optimize the reward collected over time.",
                "Due to the intractable complexity of determining the optimal policy, the action policy computed in that paper was essentially an approximation.",
                "In this paper, instead of formulating a reward function, we use EMT to solve the problem, by directly specifying the target dynamics.",
                "In fact, any search problem with randomized motion, the socalled class of area sweeping problems, can be described through specification of such target system dynamics.",
                "Dynamics Based Control provides a natural approach to solving these problems. 3.",
                "DYNAMICS BASED CONTROL The specification of Dynamics Based Control (DBC) can be broken into three interacting levels: Environment Design Level, User Level, and Agent Level. • Environment Design Level is concerned with the formal specification and modeling of the environment.",
                "For example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation constant. • User Level in turn relies on the environment model produced by Environment Design to specify the target system dynamics it wishes to observe.",
                "The User Level also specifies the estimation or learning procedure for system dynamics, and the measure of deviation.",
                "In the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping. • Agent Level in turn combines the environment model from the Environment Design level, the dynamics estimation procedure, the deviation measure and the target dynamics specification from User Level, to produce a sequence of actions that create system dynamics as close as possible to the targeted specification.",
                "As we are interested in the continual development of a stochastic system, such as happens in classical control theory [16] and continual planning [4], as well as in our example of museum sweeps, the question becomes how the Agent Level is to treat the deviation measurements over time.",
                "To this end, we use a probability threshold-that is, we would like the Agent Level to maximize the probability that the deviation measure will remain below a certain threshold.",
                "Specific action selection then depends on system formalization.",
                "One possibility would be to create a mixture of available system trends, much like that which happens in Behavior-Based <br>robotic</br> architectures [1].",
                "The other alternative would be to rely on the estimation procedure provided by the User Level-to utilize the Environment Design Level model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a certain dynamics has been achieved.",
                "Notice that this manipulation is not direct, but via the environment.",
                "Thus, for strong enough estimator algorithms, successful manipulation would mean a successful simulation of the specified target dynamics (i.e., beyond discerning via the available sensory input).",
                "DBC levels can also have a back-flow of information (see Figure 1).",
                "For instance, the Agent Level could provide data about target dynamics feasibility, allowing the User Level to modify the requirement, perhaps focusing on attainable features of system behavior.",
                "Data would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the User Level, this can provide an important tool for the environment model calibration at the Environment Design Level.",
                "UserEnv.",
                "Design Agent Model Ideal Dynamics Estimator Estimator Dynamics Feasibility System Response Data Figure 1: Data flow of the DBC framework Extending upon the idea of Actor-Critic algorithms [5], DBC data flow can provide a good basis for the design of a learning algorithm.",
                "For example, the User Level can operate as an exploratory device for a learning algorithm, inferring an ideal dynamics target from the environment model at hand that would expose and verify most critical features of system behavior.",
                "In this case, feasibility and system response data from the Agent Level would provide key information for an environment model update.",
                "In fact, the combination of feasibility and response data can provide a basis for the application of strong learning algorithms such as EM [2, 9]. 3.1 DBC for Markovian Environments For a Partially Observable Markovian Environment, DBC can be specified in a more rigorous manner.",
                "Notice how DBC discards rewards, and replaces it by another optimality criterion (structural differences are summarized in Table 1): • Environment Design level is to specify a tuple < S, A, T, O, Ω, s0 >, where: - S is the set of all possible environment states; - s0 is the initial state of the environment (which can also be viewed as a probability distribution over S); The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 791 - A is the set of all possible actions applicable in the environment; - T is the environments probabilistic transition function: T : S ×A → Π(S).",
                "That is, T(s |a, s) is the probability that the environment will move from state s to state s under action a; - O is the set of all possible observations.",
                "This is what the sensor input would look like for an outside observer; - Ω is the observation probability function: Ω : S × A × S → Π(O).",
                "That is, Ω(o|s , a, s) is the probability that one will observe o given that the environment has moved from state s to state s under action a. • User Level, in the case of Markovian environment, operates on the set of system dynamics described by a family of conditional probabilities F = {τ : S × A → Π(S)}.",
                "Thus specification of target dynamics can be expressed by q ∈ F, and the learning or tracking algorithm can be represented as a function L : O×(A×O)∗ → F; that is, it maps sequences of observations and actions performed so far into an estimate τ ∈ F of system dynamics.",
                "There are many possible variations available at the User Level to define divergence between system dynamics; several of them are: - Trace distance or L1 distance between two distributions p and q defined by D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Fidelity measure of distance F(p(·), q(·)) = x p(x)q(x) - Kullback-Leibler divergence DKL(p(·) q(·)) = x p(x) log p(x) q(x) Notice that the latter two are not actually metrics over the space of possible distributions, but nevertheless have meaningful and important interpretations.",
                "For instance, KullbackLeibler divergence is an important tool of information theory [3] that allows one to measure the price of encoding an information source governed by q, while assuming that it is governed by p. The User Level also defines the threshold of dynamics deviation probability θ. • Agent Level is then faced with a problem of selecting a control signal function a∗ to satisfy a minimization problem as follows: a∗ = arg min a Pr(d(τa, q) > θ) where d(τa, q) is a random variable describing deviation of the dynamics estimate τa, created by L under control signal a, from the ideal dynamics q.",
                "Implicit in this minimization problem is that L is manipulated via the environment, based on the environment model produced by the Environment Design Level. 3.2 DBC View of the State Space It is important to note the complementary view that DBC and POMDPs take on the state space of the environment.",
                "POMDPs regard state as a stationary snap-shot of the environment; whatever attributes of state sequencing one seeks are reached through properties of the control process, in this case reward accumulation.",
                "This can be viewed as if the sequencing of states and the attributes of that sequencing are only introduced by and for the controlling mechanism-the POMDP policy.",
                "DBC concentrates on the underlying principle of state sequencing, the system dynamics.",
                "DBCs target dynamics specification can use the environments state space as a means to describe, discern, and preserve changes that occur within the system.",
                "As a result, DBC has a greater ability to express state sequencing properties, which are grounded in the environment model and its state space definition.",
                "For example, consider the task of moving through rough terrain towards a goal and reaching it as fast as possible.",
                "POMDPs would encode terrain as state space points, while speed would be ensured by negative reward for every step taken without reaching the goalaccumulating higher reward can be reached only by faster motion.",
                "Alternatively, the state space could directly include the notion of speed.",
                "For POMDPs, this would mean that the same concept is encoded twice, in some sense: directly in the state space, and indirectly within reward accumulation.",
                "Now, even if the reward function would encode more, and finer, details of the properties of motion, the POMDP solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure.",
                "On the other hand, the tactical target expression of variations and correlations between position and speed of motion is now grounded in the state space representation.",
                "In this situation, any further constraints, e.g., smoothness of motion, speed limits in different locations, or speed reductions during sharp turns, are explicitly and uniformly expressed by the tactical target, and can result in faster and more effective action selection by a DBC algorithm. 4.",
                "EMT-BASED CONTROL AS A DBC Recently, a control algorithm was introduced called EMT-based Control [13], which instantiates the DBC framework.",
                "Although it provides an approximate greedy solution in the DBC sense, initial experiments using EMT-based control have been encouraging [14, 15].",
                "EMT-based control is based on the Markovian environment definition, as in the case with POMDPs, but its User and Agent Levels are of the Markovian DBC type of optimality. • User Level of EMT-based control defines a limited-case target system dynamics independent of action: qEMT : S → Π(S).",
                "It then utilizes the Kullback-Leibler divergence measure to compose a momentary system dynamics estimator-the Extended Markov Tracking (EMT) algorithm.",
                "The algorithm keeps a system dynamics estimate τt EMT that is capable of explaining recent change in an auxiliary Bayesian system state estimator from pt−1 to pt, and updates it conservatively using Kullback-Leibler divergence.",
                "Since τt EMT and pt−1,t are respectively the conditional and marginal probabilities over the systems state space, explanation simply means that pt(s ) = s τt EMT (s |s)pt−1(s), and the dynamics estimate update is performed by solving a 792 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: Structure of POMDP vs. Dynamics-Based Control in Markovian Environment Level Approach MDP Markovian DBC Environment < S, A, T, O, Ω >,where S - set of states A - set of actions Design T : S × A → Π(S) - transition O - observation set Ω : S × A × S → Π(O) User r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - reward function q - ideal dynamics F - reward remodeling L - dynamics estimator θ - threshold Agent π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) minimization problem: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • Agent Level in EMT-based control is suboptimal with respect to DBC (though it remains within the DBC framework), performing greedy action selection based on prediction of EMTs reaction.",
                "The prediction is based on the environment model provided by the Environment Design level, so that if we denote by Ta the environments transition function limited to action a, and pt−1 is the auxiliary Bayesian system state estimator, then the EMT-based control choice is described by a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1) Note that this follows the Markovian DBC framework precisely: the rewarding optimality of POMDPs is discarded, and in its place a dynamics estimator (EMT in this case) is manipulated via action effects on the environment to produce an estimate close to the specified target system dynamics.",
                "Yet as we mentioned, naive EMTbased control is suboptimal in the DBC sense, and has several additional limitations that do not exist in the general DBC framework (discussed in Section 4.2). 4.1 Multi-Target EMT At times, there may exist several behavioral preferences.",
                "For example, in the case of museum guards, some art items are more heavily guarded, requiring that the guards stay more often in their close vicinity.",
                "On the other hand, no corner of the museum is to be left unchecked, which demands constant motion.",
                "Successful museum security would demand that the guards adhere to, and balance, both of these behaviors.",
                "For EMT-based control, this would mean facing several tactical targets {qk}K k=1, and the question becomes how to merge and balance them.",
                "A balancing mechanism can be applied to resolve this issue.",
                "Note that EMT-based control, while selecting an action, creates a preference vector over the set of actions based on their predicted performance with respect to a given target.",
                "If these preference vectors are normalized, they can be combined into a single unified preference.",
                "This requires replacement of standard EMT-based action selection by the algorithm below [15]: • Given: - a set of target dynamics {qk}K k=1, - vector of weights w(k) • Select action as follows - For each action a ∈ A predict the future state distribution ¯pa t+1 = Ta ∗ pt; - For each action, compute Da = H(¯pa t+1, pt, PDt) - For each a ∈ A and qk tactical target, denote V (a, k) = DKL (Da qk) pt .",
                "Let Vk(a) = 1 Zk V (a, k), where Zk = a∈A V (a, k) is a normalization factor. - Select a∗ = arg min a k k=1 w(k)Vk(a) The weights vector w = (w1, ..., wK ) allows the additional tuning of importance among target dynamics without the need to redesign the targets themselves.",
                "This balancing method is also seamlessly integrated into the EMT-based control flow of operation. 4.2 EMT-based Control Limitations EMT-based control is a sub-optimal (in the DBC sense) representative of the DBC structure.",
                "It limits the User by forcing EMT to be its dynamic tracking algorithm, and replaces Agent optimization by greedy action selection.",
                "This kind of combination, however, is common for on-line algorithms.",
                "Although further development of EMT-based controllers is necessary, evidence so far suggests that even the simplest form of the algorithm possesses a great deal of power, and displays trends that are optimal in the DBC sense of the word.",
                "There are two further, EMT-specific, limitations to EMT-based control that are evident at this point.",
                "Both already have partial solutions and are subjects of ongoing research.",
                "The first limitation is the problem of negative preference.",
                "In the POMDP framework for example, this is captured simply, through The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 793 the appearance of values with different signs within the reward structure.",
                "For EMT-based control, however, negative preference means that one would like to avoid a certain distribution over system development sequences; EMT-based control, however, concentrates on getting as close as possible to a distribution.",
                "Avoidance is thus unnatural in native EMT-based control.",
                "The second limitation comes from the fact that standard environment modeling can create pure sensory actions-actions that do not change the state of the world, and differ only in the way observations are received and the quality of observations received.",
                "Since the world state does not change, EMT-based control would not be able to differentiate between different sensory actions.",
                "Notice that both of these limitations of EMT-based control are absent from the general DBC framework, since it may have a tracking algorithm capable of considering pure sensory actions and, unlike Kullback-Leibler divergence, a distribution deviation measure that is capable of dealing with negative preference. 5.",
                "EMT PLAYING TAG The Game of Tag was first introduced in [11].",
                "It is a single agent problem of capturing a quarry, and belongs to the class of area sweeping problems.",
                "An example domain is shown in Figure 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figure 2: Tag domain; an agent (A) attempts to seek and capture a quarry (Q) The Game of Tag extremely limits the agents perception, so that the agent is able to detect the quarry only if they are co-located in the same cell of the grid world.",
                "In the classical version of the game, co-location leads to a special observation, and the Tag action can be performed.",
                "We slightly modify this setting: the moment that both agents occupy the same cell, the game ends.",
                "As a result, both the agent and its quarry have the same motion capability, which allows them to move in four directions, North, South, East, and West.",
                "These form a formal space of actions within a Markovian environment.",
                "The state space of the formal Markovian environment is described by the cross-product of the agent and quarrys positions.",
                "For Figure 2, it would be S = {s0, ..., s23} × {s0, ..., s23}.",
                "The effects of an action taken by the agent are deterministic, but the environment in general has a stochastic response due to the motion of the quarry.",
                "With probability q0 1 it stays put, and with probability 1 − q0 it moves to an adjacent cell further away from the 1 In our experiments this was taken to be q0 = 0.2. agent.",
                "So for the instance shown in Figure 2 and q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Although the evasive behavior of the quarry is known to the agent, the quarrys position is not.",
                "The only sensory information available to the agent is its own location.",
                "We use EMT and directly specify the target dynamics.",
                "For the Game of Tag, we can easily formulate three major trends: catching the quarry, staying mobile, and stalking the quarry.",
                "This results in the following three target dynamics: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 otherwise Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 otherwise Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Note that none of the above targets are directly achievable; for instance, if Qt = s9 and At = s11, there is no action that can move the agent to At+1 = s9 as required by the Tcatch target dynamics.",
                "We ran several experiments to evaluate EMT performance in the Tag Game.",
                "Three configurations of the domain shown in Figure 3 were used, each posing a different challenge to the agent due to partial observability.",
                "In each setting, a set of 1000 runs was performed with a time limit of 100 steps.",
                "In every run, the initial position of both the agent and its quarry was selected at random; this means that as far as the agent was concerned, the quarrys initial position was uniformly distributed over the entire domain cell space.",
                "We also used two variations of the environment observability function.",
                "In the first version, observability function mapped all joint positions of hunter and quarry into the position of the hunter as an observation.",
                "In the second, only those joint positions in which hunter and quarry occupied different locations were mapped into the hunters location.",
                "The second version in fact utilized and expressed the fact that once hunter and quarry occupy the same cell the game ends.",
                "The results of these experiments are shown in Table 2.",
                "Balancing [15] the catch, move, and stalk target dynamics described in the previous section by the weight vector [0.8, 0.1, 0.1], EMT produced stable performance in all three domains.",
                "Although direct comparisons are difficult to make, the EMT performance displayed notable efficiency vis-`a-vis the POMDP approach.",
                "In spite of a simple and inefficient Matlab implementation of the EMT algorithm, the decision time for any given step averaged significantly below 1 second in all experiments.",
                "For the irregular open arena domain, which proved to be the most difficult, 1000 experiment runs bounded by 100 steps each, a total of 42411 steps, were completed in slightly under 6 hours.",
                "That is, over 4 × 104 online steps took an order of magnitude less time than the offline computation of POMDP policy in [11].",
                "The significance of this differential is made even more prominent by the fact that, should the environment model parameters change, the online nature of EMT would allow it to maintain its performance time, while the POMDP policy would need to be recomputed, requiring yet again a large overhead of computation.",
                "We also tested the behavior cell frequency entropy, empirical measures from trial data.",
                "As Figure 4 and Figure 5 show, empir794 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) A Q Q A 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 A Q Figure 3: These configurations of the Tag Game space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor Table 2: Performance of the EMT-based solution in three Tag Game domains and two observability models: I) omniposition quarry, II) quarry is not at hunters position Model Domain Capture% E(Steps) Time/Step I Dead-ends 100 14.8 72(mSec) Arena 80.2 42.4 500(mSec) Circle 91.4 34.6 187(mSec) II Dead-ends 100 13.2 91(mSec) Arena 96.8 28.67 396(mSec) Circle 94.4 31.63 204(mSec) ical entropy grows with the length of interaction.",
                "For runs where the quarry was not captured immediately, the entropy reaches between 0.85 and 0.952 for different runs and scenarios.",
                "As the agent actively seeks the quarry, the entropy never reaches its maximum.",
                "One characteristic of the entropy graph for the open arena scenario particularly caught our attention in the case of the omniposition quarry observation model.",
                "Near the maximum limit of trial length (100 steps), entropy suddenly dropped.",
                "Further analysis of the data showed that under certain circumstances, a fluctuating behavior occurs in which the agent faces equally viable versions of quarry-following behavior.",
                "Since the EMT algorithm has greedy action selection, and the state space does not encode any form of commitment (not even speed or acceleration), the agent is locked within a small portion of cells.",
                "It is essentially attempting to simultaneously follow several courses of action, all of which are consistent with the target dynamics.",
                "This behavior did not occur in our second observation model, since it significantly reduced the set of eligible courses of action-essentially contributing to tie-breaking among them. 6.",
                "DISCUSSION The design of the EMT solution for the Tag Game exposes the core difference in approach to planning and control between EMT or DBC, on the one hand, and the more familiar POMDP approach, on the other.",
                "POMDP defines a reward structure to optimize, and influences system dynamics indirectly through that optimization.",
                "EMT discards any reward scheme, and instead measures and influences system dynamics directly. 2 Entropy was calculated using log base equal to the number of possible locations within the domain; this properly scales entropy expression into the range [0, 1] for all domains.",
                "Thus for the Tag Game, we did not search for a reward function that would encode and express our preference over the agents behavior, but rather directly set three (heuristic) behavior preferences as the basis for target dynamics to be maintained.",
                "Experimental data shows that these targets need not be directly achievable via the agents actions.",
                "However, the ratio between EMT performance and achievability of target dynamics remains to be explored.",
                "The tag game experiment data also revealed the different emphasis DBC and POMDPs place on the formulation of an environment state space.",
                "POMDPs rely entirely on the mechanism of reward accumulation maximization, i.e., formation of the action selection procedure to achieve necessary state sequencing.",
                "DBC, on the other hand, has two sources of sequencing specification: through the properties of an action selection procedure, and through direct specification within the target dynamics.",
                "The importance of the second source was underlined by the Tag Game experiment data, in which the greedy EMT algorithm, applied to a POMDP-type state space specification, failed, since target description over such a state space was incapable of encoding the necessary behavior tendencies, e.g., tie-breaking and commitment to directed motion.",
                "The structural differences between DBC (and EMT in particular), and POMDPs, prohibits direct performance comparison, and places them on complementary tracks, each within a suitable niche.",
                "For instance, POMDPs could be seen as a much more natural formulation of economic sequential decision-making problems, while EMT is a better fit for continual demand for stochastic change, as happens in many <br>robotic</br> or embodied-agent problems.",
                "The complementary properties of POMDPs and EMT can be further exploited.",
                "There is recent interest in using POMDPs in hybrid solutions [17], in which the POMDPs can be used together with other control approaches to provide results not easily achievable with either approach by itself.",
                "DBC can be an effective partner in such a hybrid solution.",
                "For instance, POMDPs have prohibitively large off-line time requirements for policy computation, but can be readily used in simpler settings to expose beneficial behavioral trends; this can serve as a form of target dynamics that are provided to EMT in a larger domain for on-line operation. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have presented a novel perspective on the process of planning and control in stochastic environments, in the form of the Dynamics Based Control (DBC) framework.",
                "DBC formulates the task of planning as support of a specified target system dynamics, which describes the necessary properties of change within the environment.",
                "Optimality of DBC plans of action are measured The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 4: Observation Model I: Omniposition quarry.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 5: Observation Model II: quarry not observed at hunters position.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. with respect to the deviation of actual system dynamics from the target dynamics.",
                "We show that a recently developed technique of Extended Markov Tracking (EMT) [13] is an instantiation of DBC.",
                "In fact, EMT can be seen as a specific case of DBC parameterization, which employs a greedy action selection procedure.",
                "Since EMT exhibits the key features of the general DBC framework, as well as polynomial time complexity, we used the multitarget version of EMT [15] to demonstrate that the class of area sweeping problems naturally lends itself to dynamics-based descriptions, as instantiated by our experiments in the Tag Game domain.",
                "As enumerated in Section 4.2, EMT has a number of limitations, such as difficulty in dealing with negative dynamic preference.",
                "This prevents direct application of EMT to such problems as Rendezvous-Evasion Games (e.g., [6]).",
                "However, DBC in general has no such limitations, and readily enables the formulation of evasion games.",
                "In future work, we intend to proceed with the development of dynamics-based controllers for these problems. 8.",
                "ACKNOWLEDGMENT The work of the first two authors was partially supported by Israel Science Foundation grant #898/05, and the third author was partially supported by a grant from Israels Ministry of Science and Technology. 9.",
                "REFERENCES [1] R. C. Arkin.",
                "Behavior-Based Robotics.",
                "MIT Press, 1998. [2] J.",
                "A. Bilmes.",
                "A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and Hidden Markov Models.",
                "Technical Report TR-97-021, Department of Electrical Engeineering and Computer Science, University of California at Berkeley, 1998. [3] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of information theory.",
                "Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz, and M. J. Wolverton.",
                "A survey of research in distributed, continual planning.",
                "AI Magazine, 4:13-22, 1999. [5] V. R. Konda and J. N. Tsitsiklis.",
                "Actor-Critic algorithms.",
                "SIAM Journal on Control and Optimization, 42(4):1143-1166, 2003. [6] W. S. Lim.",
                "A rendezvous-evasion game on discrete locations with joint randomization.",
                "Advances in Applied Probability, 29(4):1004-1017, December 1997. [7] M. L. Littman, T. L. Dean, and L. P. Kaelbling.",
                "On the complexity of solving Markov decision problems.",
                "In Proceedings of the 11th Annual Conference on Uncertainty in Artificial Intelligence (UAI-95), pages 394-402, 1995. [8] O. Madani, S. Hanks, and A. Condon.",
                "On the undecidability of probabilistic planning and related stochastic optimization problems.",
                "Artificial Intelligence Journal, 147(1-2):5-34, July 2003. [9] R. M. Neal and G. E. Hinton.",
                "A view of the EM algorithm 796 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) that justifies incremental, sparse, and other variants.",
                "In M. I. Jordan, editor, Learning in Graphical Models, pages 355-368.",
                "Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In Proceeding of AAMAS 2006, 2006. [11] J. Pineau, G. Gordon, and S. Thrun.",
                "Point-based value iteration: An anytime algorithm for pomdps.",
                "In International Joint Conference on Artificial Intelligence (IJCAI), pages 1025-1032, August 2003. [12] M. L. Puterman.",
                "Markov Decision Processes.",
                "Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics Section.",
                "Wiley-Interscience Publication, New York, 1994. [13] Z. Rabinovich and J. S. Rosenschein.",
                "Extended Markov Tracking with an application to control.",
                "In The Workshop on Agent Tracking: Modeling Other Agents from Observations, at the Third International Joint Conference on Autonomous Agents and Multiagent Systems, pages 95-100, New-York, July 2004. [14] Z. Rabinovich and J. S. Rosenschein.",
                "Multiagent coordination by Extended Markov Tracking.",
                "In The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 431-438, Utrecht, The Netherlands, July 2005. [15] Z. Rabinovich and J. S. Rosenschein.",
                "On the response of EMT-based control to interacting targets and models.",
                "In The Fifth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 465-470, Hakodate, Japan, May 2006. [16] R. F. Stengel.",
                "Optimal Control and Estimation.",
                "Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, and P. Varakantham.",
                "Conflicts in teamwork: Hybrids to the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 797"
            ],
            "original_annotated_samples": [
                "One possibility would be to create a mixture of available system trends, much like that which happens in Behavior-Based <br>robotic</br> architectures [1].",
                "For instance, POMDPs could be seen as a much more natural formulation of economic sequential decision-making problems, while EMT is a better fit for continual demand for stochastic change, as happens in many <br>robotic</br> or embodied-agent problems."
            ],
            "translated_annotated_samples": [
                "Una posibilidad sería crear una mezcla de tendencias de sistemas disponibles, similar a lo que sucede en las arquitecturas <br>robóticas</br> basadas en el comportamiento [1].",
                "Por ejemplo, los POMDP podrían ser vistos como una formulación mucho más natural de problemas económicos de toma de decisiones secuenciales, mientras que EMT se ajusta mejor a la demanda continua de cambios estocásticos, como ocurre en muchos problemas de <br>robótica</br> o de agentes incorporados."
            ],
            "translated_text": "En este artículo presentamos el Control Basado en Dinámicas (DBC), un enfoque para la planificación y control de un agente en entornos estocásticos. A diferencia de los enfoques existentes, que buscan optimizar las recompensas esperadas (por ejemplo, en Problemas de Decisión de Markov Parcialmente Observables (POMDP)), DBC optimiza el comportamiento del sistema hacia las dinámicas del sistema especificadas. Mostramos que un enfoque de planificación y control recientemente desarrollado, Seguimiento Extendido de Markov (EMT), es una instancia de DBC. EMT emplea la selección de acciones codiciosa para proporcionar un algoritmo de control eficiente en entornos markovianos. Explotamos esta eficiencia en un conjunto de experimentos que aplicaron EMT multitarget a una clase de problemas de barrido de área (búsqueda de objetivos en movimiento). Mostramos que tales problemas pueden ser definidos de forma natural y resueltos eficientemente utilizando el marco de DBC y su instanciación de EMT. Categorías y Descriptores de Asignaturas I.2.8 [Resolución de Problemas, Métodos de Control y Búsqueda]: Teoría de Control; I.2.9 [Robótica]; I.2.11 [Inteligencia Artificial Distribuida]: Agentes Inteligentes Términos Generales Algoritmos, Teoría 1. INTRODUCCIÓN La planificación y el control constituyen un área de investigación central en sistemas multiagentes e inteligencia artificial. En los últimos años, los Procesos de Decisión de Markov Parcialmente Observables (POMDPs) [12] se han convertido en una base formal popular para la planificación en entornos estocásticos. En este marco, el problema de planificación y control suele abordarse imponiendo una función de recompensa y calculando una política (de elección de acciones) que sea óptima, en el sentido de que resultará en la mayor utilidad esperada. Si bien teóricamente atractivo, la complejidad de resolver óptimamente un POMDP es prohibitiva [8, 7]. Tomamos una visión alternativa de la planificación en entornos estocásticos. No utilizamos una función de recompensa basada en el estado, sino que optimizamos según un criterio diferente, una especificación basada en transiciones de la dinámica del sistema deseado. La idea aquí es ver la ejecución del plan como un proceso que obliga a un sistema (estocástico) a cambiar, y un plan como un proceso dinámico que moldea ese cambio de acuerdo con criterios deseados. Llamamos a este marco de planificación general Control Basado en Dinámicas (DBC). En DBC, el objetivo de un proceso de planificación (o control) es garantizar que el sistema cambiará de acuerdo con dinámicas objetivo específicas (potencialmente estocásticas). Dado que el comportamiento real del sistema puede desviarse de lo especificado por la dinámica objetivo (debido a la naturaleza estocástica del sistema), la planificación en tales entornos debe ser continua, de manera similar a los controladores en lazo cerrado clásicos. Aquí, la optimalidad se mide en términos de la probabilidad de magnitudes de desviación. En este documento, presentamos la estructura del Control Basado en Dinámicas. Mostramos que el enfoque de Seguimiento Extendido de Markov (EMT) recientemente desarrollado [13, 14, 15] está subsumido por DBC, con EMT empleando selección de acciones codiciosa, que es una parametrización específica entre las opciones posibles dentro de DBC. EMT es una implementación eficiente de DBC. Para evaluar DBC, llevamos a cabo un conjunto de experimentos aplicando EMT de múltiples objetivos al Juego de Etiquetas [11]; esta es una variante del problema de barrido de área, donde un agente intenta etiquetar a un objetivo móvil (presa) cuya posición no se conoce con certeza. Los datos experimentales demuestran que incluso con un modelo simple del entorno y un diseño simple de la dinámica del objetivo, se pueden lograr altas tasas de éxito tanto en la captura de la presa como en sorprender a la presa (como se expresa por la entropía observada de la posición de los agentes controlados). El documento está organizado de la siguiente manera. En la Sección 2 motivamos DBC utilizando problemas de barrido de área, y discutimos trabajos relacionados. La Sección 3 introduce la estructura de Control Basado en Dinámicas (DBC) y su especialización en entornos Markovianos. Esto es seguido por una revisión del enfoque de Seguimiento Markov Extendido (EMT) como un régimen de control estructurado por DBC en la Sección 4. Esa sección también discute las limitaciones del control basado en EMT en relación con el marco general de DBC. Los ajustes experimentales y los resultados se presentan a continuación en la Sección 5. La Sección 6 proporciona una breve discusión del enfoque general, y la Sección 7 ofrece algunas observaciones finales y direcciones para trabajos futuros. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2. MOTIVACIÓN Y TRABAJO RELACIONADO Muchos escenarios de la vida real tienen naturalmente una especificación de dinámica de destino estocástica, especialmente en aquellos dominios donde no existe un objetivo final, sino más bien un comportamiento del sistema (con propiedades específicas) que debe ser continuamente respaldado. Por ejemplo, los guardias de seguridad realizan barridos persistentes de un área para detectar cualquier signo de intrusión. Los ladrones astutos intentarán rastrear estos barridos y sincronizar su operación con puntos clave del movimiento de los guardias. Por lo tanto, es recomendable que la dinámica de movimiento de los guardias parezca irregular y aleatoria. El trabajo reciente de Paruchuri et al. [10] ha abordado dicha aleatorización en el contexto de POMDPs de agente único y distribuidos. El objetivo de ese trabajo era generar políticas que proporcionen una medida de aleatorización en la selección de acciones, manteniendo al mismo tiempo las recompensas dentro de niveles aceptables. Nuestro enfoque difiere de este trabajo en que DBC no optimiza recompensas esperadas, de hecho, no consideramos recompensas en absoluto, sino que en su lugar mantenemos dinámicas deseadas (incluyendo, pero no limitado a, la aleatorización). El juego de la mancha es otro ejemplo de la aplicabilidad del enfoque. Fue introducido en el trabajo de Pineau et al. [11]. Hay dos agentes que pueden moverse por un área, la cual está dividida en una cuadrícula. La cuadrícula puede tener celdas bloqueadas (agujeros) en las cuales ningún agente puede moverse. Un agente (el cazador) intenta moverse a una celda ocupada por el otro (la presa), de modo que estén ubicados en el mismo lugar (esto es una etiqueta exitosa). La presa busca evitar al cazador, siempre está al tanto de la posición del cazador, pero no sabe cómo se comportará el cazador, lo que abre la posibilidad de que un cazador sorprenda a la presa. El cazador conoce la ley de movimiento probabilístico de la presa, pero no conoce su ubicación actual. Tag es una instancia de una familia de problemas de barrido de área (persecución-evasión). En [11], el cazador modeló el problema utilizando un POMDP. Se definió una función de recompensa para reflejar el deseo de marcar la presa, y se calculó una política de acción para optimizar la recompensa recolectada con el tiempo. Debido a la complejidad intratable de determinar la política óptima, la política de acción calculada en ese documento fue esencialmente una aproximación. En este artículo, en lugar de formular una función de recompensa, utilizamos EMT para resolver el problema, especificando directamente la dinámica objetivo. De hecho, cualquier problema de búsqueda con movimiento aleatorio, la llamada clase de problemas de barrido de área, puede ser descrito a través de la especificación de la dinámica de dicho sistema objetivo. El Control Basado en Dinámicas proporciona un enfoque natural para resolver estos problemas. CONTROL BASADO EN DINÁMICA La especificación del Control Basado en Dinámica (DBC) se puede dividir en tres niveles interactivos: Nivel de Diseño del Entorno, Nivel de Usuario y Nivel de Agente. • El Nivel de Diseño del Entorno se ocupa de la especificación formal y modelado del entorno. Por ejemplo, este nivel especificaría las leyes de la física dentro del sistema y establecería sus parámetros, como la constante de gravitación. El Nivel de Usuario, a su vez, se basa en el modelo de entorno producido por el Diseño de Entorno para especificar la dinámica del sistema objetivo que desea observar. El Nivel de Usuario también especifica el procedimiento de estimación o aprendizaje para la dinámica del sistema, y la medida de desviación. En el escenario del guardia de museo mencionado anteriormente, estos corresponderían a un horario de barrido estocástico y una medida de sorpresa relativa entre el barrido especificado y el real. • El Nivel de Agente, a su vez, combina el modelo del entorno del Nivel de Diseño de Entorno, el procedimiento de estimación de la dinámica, la medida de desviación y la especificación de la dinámica objetivo del Nivel de Usuario, para producir una secuencia de acciones que creen dinámicas del sistema lo más cercanas posible a la especificación deseada. Dado nuestro interés en el desarrollo continuo de un sistema estocástico, como ocurre en la teoría de control clásica [16] y la planificación continua [4], así como en nuestro ejemplo de recorridos por museos, la pregunta es cómo el Nivel de Agente debe tratar las mediciones de desviación a lo largo del tiempo. Con este fin, utilizamos un umbral de probabilidad, es decir, nos gustaría que el Nivel del Agente maximice la probabilidad de que la medida de desviación permanezca por debajo de cierto umbral. La selección de acciones específicas depende entonces de la formalización del sistema. Una posibilidad sería crear una mezcla de tendencias de sistemas disponibles, similar a lo que sucede en las arquitecturas <br>robóticas</br> basadas en el comportamiento [1]. La otra alternativa sería depender del procedimiento de estimación proporcionado por el Nivel de Usuario para utilizar el modelo del Nivel de Diseño del Entorno del entorno para elegir acciones, de manera que se manipule al estimador de dinámicas para que crea que se ha logrado cierta dinámica. Ten en cuenta que esta manipulación no es directa, sino a través del entorno. Por lo tanto, para algoritmos de estimación lo suficientemente fuertes, la manipulación exitosa significaría una simulación exitosa de la dinámica objetivo especificada (es decir, más allá de discernir a través de la entrada sensorial disponible). Los niveles de DBC también pueden tener un retroceso de información (ver Figura 1). Por ejemplo, el Nivel de Agente podría proporcionar datos sobre la viabilidad de la dinámica del objetivo, permitiendo al Nivel de Usuario modificar el requisito, quizás centrándose en las características alcanzables del comportamiento del sistema. Los datos también estarían disponibles sobre la respuesta del sistema a diferentes acciones realizadas; combinados con un estimador de dinámica definido por el Nivel de Usuario, esto puede proporcionar una herramienta importante para la calibración del modelo del entorno en el Nivel de Diseño del Entorno. UserEnv. El modelo de agente de diseño Estimador de Dinámicas Ideales Estimador de Dinámicas Factibilidad de Respuesta del Sistema de Datos Figura 1: Flujo de datos del marco de trabajo DBC Ampliando la idea de los algoritmos Actor-Crítico [5], el flujo de datos de DBC puede proporcionar una buena base para el diseño de un algoritmo de aprendizaje. Por ejemplo, el Nivel de Usuario puede funcionar como un dispositivo exploratorio para un algoritmo de aprendizaje, inferir un objetivo dinámico ideal a partir del modelo del entorno en cuestión que expondría y verificaría las características más críticas del comportamiento del sistema. En este caso, los datos de viabilidad y respuesta del sistema a nivel de agente proporcionarían información clave para una actualización del modelo del entorno. De hecho, la combinación de datos de viabilidad y respuesta puede proporcionar una base para la aplicación de algoritmos de aprendizaje sólidos como EM [2, 9]. 3.1 DBC para entornos markovianos Para un entorno markoviano parcialmente observable, DBC puede especificarse de una manera más rigurosa. Observa cómo DBC descarta las recompensas y las reemplaza por otro criterio de optimalidad (las diferencias estructurales se resumen en la Tabla 1): • El nivel de Diseño del Entorno es especificar una tupla < S, A, T, O, Ω, s0 >, donde: - S es el conjunto de todos los posibles estados del entorno; - s0 es el estado inicial del entorno (que también puede ser visto como una distribución de probabilidad sobre S); La Sexta Conferencia Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 791 - A es el conjunto de todas las acciones posibles aplicables en el entorno; - T es la función de transición probabilística del entorno: T: S × A → Π(S). Es decir, T(s | a, s) es la probabilidad de que el entorno se mueva del estado s al estado s bajo la acción a; - O es el conjunto de todas las observaciones posibles. Así es como se vería la entrada del sensor para un observador externo; - Ω es la función de probabilidad de observación: Ω: S × A × S → Π(O). Es decir, Ω(o|s, a, s) es la probabilidad de observar o dado que el entorno ha pasado del estado s al estado s bajo la acción a. • Nivel de Usuario, en el caso de un entorno markoviano, opera en el conjunto de dinámicas del sistema descritas por una familia de probabilidades condicionales F = {τ: S × A → Π(S)}. Por lo tanto, la especificación de la dinámica del objetivo puede expresarse como q ∈ F, y el algoritmo de aprendizaje o seguimiento puede representarse como una función L : O×(A×O)∗ → F; es decir, mapea secuencias de observaciones y acciones realizadas hasta el momento en una estimación τ ∈ F de la dinámica del sistema. Hay muchas variaciones posibles disponibles a nivel de usuario para definir la divergencia entre la dinámica del sistema; varias de ellas son: - Distancia de traza o distancia L1 entre dos distribuciones p y q definida por D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Medida de fidelidad de distancia F(p(·), q(·)) = x p(x)q(x) - Divergencia de Kullback-Leibler DKL(p(·) q(·)) = x p(x) log p(x) q(x) Nótese que las dos últimas no son en realidad métricas sobre el espacio de distribuciones posibles, pero tienen interpretaciones significativas e importantes. Por ejemplo, la divergencia de Kullback-Leibler es una herramienta importante de la teoría de la información [3] que permite medir el costo de codificar una fuente de información gobernada por q, asumiendo que está gobernada por p. El Nivel de Usuario también define el umbral de probabilidad de desviación de la dinámica θ. • El Nivel de Agente se enfrenta entonces a un problema de seleccionar una función de señal de control a∗ para satisfacer un problema de minimización de la siguiente manera: a∗ = arg min a Pr(d(τa, q) > θ) donde d(τa, q) es una variable aleatoria que describe la desviación de la estimación de la dinámica τa, creada por L bajo la señal de control a, de la dinámica ideal q. Implícito en este problema de minimización es que L es manipulado a través del entorno, basado en el modelo de entorno producido por el Nivel de Diseño del Entorno. 3.2 Vista DBC del Espacio de Estados Es importante tener en cuenta la visión complementaria que DBC y POMDPs tienen sobre el espacio de estados del entorno. Los POMDP consideran el estado como una instantánea estacionaria del entorno; cualquier atributo de la secuencia de estados que se busque se alcanza a través de las propiedades del proceso de control, en este caso la acumulación de recompensas. Esto se puede ver como si la secuencia de estados y los atributos de esa secuencia solo fueran introducidos por y para el mecanismo de control, la política POMDP. DBC se enfoca en el principio subyacente de la secuenciación de estados, la dinámica del sistema. La especificación de la dinámica de los DBCs puede utilizar el espacio de estado del entorno como un medio para describir, discernir y preservar los cambios que ocurren dentro del sistema. Como resultado, DBC tiene una mayor capacidad para expresar propiedades de secuenciación de estados, las cuales están fundamentadas en el modelo del entorno y su definición del espacio de estados. Por ejemplo, considera la tarea de moverse a través de terreno accidentado hacia un objetivo y alcanzarlo lo más rápido posible. Los POMDPs codificarían el terreno como puntos en el espacio de estados, mientras que la velocidad se garantizaría mediante una recompensa negativa por cada paso dado sin alcanzar la meta. Acumular una recompensa más alta solo se puede lograr mediante un movimiento más rápido. Alternativamente, el espacio de estados podría incluir directamente la noción de velocidad. Para los POMDPs, esto significaría que el mismo concepto está codificado dos veces, en cierto sentido: directamente en el espacio de estados e indirectamente en la acumulación de recompensas. Ahora, incluso si la función de recompensa codificara más y detalles más finos de las propiedades del movimiento, la solución POMDP tendría que buscar en un espacio mucho más grande de políticas, aunque aún esté guiada por el concepto implícito del procedimiento de acumulación de recompensas. Por otro lado, la expresión del objetivo táctico de variaciones y correlaciones entre la posición y la velocidad del movimiento ahora se basa en la representación del espacio de estados. En esta situación, cualquier restricción adicional, como la suavidad del movimiento, los límites de velocidad en diferentes ubicaciones o las reducciones de velocidad durante giros bruscos, son expresadas explícita y uniformemente por el objetivo táctico, y pueden resultar en una selección de acciones más rápida y efectiva por un algoritmo DBC. 4. El Control Basado en EMT como un DBC Recientemente, se introdujo un algoritmo de control llamado Control Basado en EMT [13], que instancia el marco de trabajo DBC. Aunque proporciona una solución ávida aproximada en el sentido de DBC, los experimentos iniciales utilizando el control basado en EMT han sido alentadores [14, 15]. El control basado en EMT se basa en la definición del entorno markoviano, como en el caso de los POMDP, pero sus Niveles de Usuario y Agente son del tipo de optimalidad DBC markoviana. • El Nivel de Usuario del control basado en EMT define una dinámica del sistema objetivo en un caso limitado independiente de la acción: qEMT: S → Π(S). Luego utiliza la medida de divergencia de Kullback-Leibler para componer un estimador de dinámica de sistemas momentáneos: el algoritmo de Seguimiento Extendido de Markov (EMT). El algoritmo mantiene una estimación de la dinámica del sistema τt EMT que es capaz de explicar el cambio reciente en un estimador de estado auxiliar bayesiano de pt−1 a pt, y lo actualiza de manera conservadora utilizando la divergencia de Kullback-Leibler. Dado que τt EMT y pt−1,t son respectivamente las probabilidades condicionales y marginales sobre el espacio de estados del sistema, la explicación simplemente significa que pt(s) = s τt EMT (s | s)pt−1(s), y la actualización de la estimación de la dinámica se realiza resolviendo un 792 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Tabla 1: Estructura de POMDP vs. Control basado en Dinámicas en Entorno Markoviano Nivel Enfoque MDP Markoviano DBC Entorno < S, A, T, O, Ω >, donde S - conjunto de estados A - conjunto de acciones Diseño T : S × A → Π(S) - transición O - conjunto de observaciones Ω : S × A × S → Π(O) Usuario r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - función de recompensa q - dinámicas ideales F - remodelación de recompensa L - estimador de dinámicas θ - umbral Agente π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) problema de minimización: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • El nivel de Agente en el control basado en EMT es subóptimo con respecto a DBC (aunque permanece dentro del marco de DBC), realizando selección de acciones codiciosas basadas en la predicción de la reacción de EMT. La predicción se basa en el modelo del entorno proporcionado por el nivel de Diseño del Entorno, de modo que si denotamos por Ta la función de transición de entornos limitada a la acción a, y pt−1 es el estimador de estado auxiliar del sistema bayesiano, entonces la elección de control basada en EMT se describe por a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1). Nótese que esto sigue precisamente el marco DBC Markoviano: la optimalidad recompensante de los POMDPs se descarta, y en su lugar se manipula un estimador de dinámicas (EMT en este caso) a través de los efectos de la acción en el entorno para producir una estimación cercana a la dinámica del sistema objetivo especificado. Sin embargo, como mencionamos, el control EMT ingenuo es subóptimo en el sentido de DBC y tiene varias limitaciones adicionales que no existen en el marco general de DBC (discutido en la Sección 4.2). 4.1 EMT de múltiples objetivos. En ocasiones, puede haber varias preferencias de comportamiento. Por ejemplo, en el caso de los guardias de museo, algunos objetos de arte están más fuertemente custodiados, lo que requiere que los guardias permanezcan más a menudo en su cercanía. Por otro lado, no se debe dejar ningún rincón del museo sin revisar, lo que requiere un movimiento constante. El éxito de la seguridad en un museo requeriría que los guardias se adhieran y equilibren ambos comportamientos. Para el control basado en EMT, esto significaría enfrentar varios objetivos tácticos {qk}K k=1, y la pregunta sería cómo fusionar y equilibrarlos. Un mecanismo de equilibrio se puede aplicar para resolver este problema. Ten en cuenta que el control basado en EMT, al seleccionar una acción, crea un vector de preferencia sobre el conjunto de acciones basado en su rendimiento predicho con respecto a un objetivo dado. Si estos vectores de preferencia están normalizados, pueden combinarse en una única preferencia unificada. Esto requiere reemplazar la selección de acciones basada en EMT estándar por el algoritmo a continuación [15]: • Dado: - un conjunto de dinámicas objetivo {qk}K k=1, - vector de pesos w(k) • Seleccionar la acción de la siguiente manera - Para cada acción a ∈ A predecir la distribución de estado futuro ¯pa t+1 = Ta ∗ pt; - Para cada acción, calcular Da = H(¯pa t+1, pt, PDt) - Para cada a ∈ A y objetivo táctico qk, denotar V (a, k) = DKL (Da qk) pt. Sea Vk(a) = 1 Zk V (a, k), donde Zk = a∈A V (a, k) es un factor de normalización. - Selecciona a∗ = arg min a k k=1 w(k)Vk(a) El vector de pesos w = (w1, ..., wK ) permite ajustar la importancia entre las dinámicas objetivo sin necesidad de rediseñar los objetivos mismos. Este método de equilibrio también está integrado de forma transparente en el flujo de operación basado en EMT. Limitaciones del Control Basado en EMT El control basado en EMT es una representación subóptima (en el sentido de DBC) de la estructura DBC. Limita al Usuario al obligar a EMT a ser su algoritmo de seguimiento dinámico, y reemplaza la optimización del Agente por la selección de acciones codiciosa. Este tipo de combinación, sin embargo, es común en algoritmos en línea. Aunque es necesario un mayor desarrollo de los controladores basados en EMT, la evidencia hasta ahora sugiere que incluso la forma más simple del algoritmo posee una gran cantidad de potencia y muestra tendencias que son óptimas en el sentido de DBC. Hay dos limitaciones adicionales específicas de los técnicos en emergencias médicas (EMT, por sus siglas en inglés) al control basado en EMT que son evidentes en este momento. Ambos ya tienen soluciones parciales y son objeto de investigación continua. La primera limitación es el problema de la preferencia negativa. En el marco de POMDP, por ejemplo, esto se captura de manera sencilla, a través de The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 793 la aparición de valores con signos diferentes dentro de la estructura de recompensa. Para el control basado en EMT, sin embargo, la preferencia negativa significa que uno quisiera evitar cierta distribución sobre las secuencias de desarrollo del sistema; sin embargo, el control basado en EMT se concentra en acercarse lo más posible a una distribución. La evitación es, por lo tanto, antinatural en el control basado en EMT nativo. La segunda limitación proviene del hecho de que el modelado del entorno estándar puede crear acciones sensoriales puras, acciones que no cambian el estado del mundo y solo difieren en la forma en que se reciben las observaciones y la calidad de las observaciones recibidas. Dado que el estado del mundo no cambia, el control basado en EMT no sería capaz de diferenciar entre diferentes acciones sensoriales. Ten en cuenta que ambas limitaciones del control basado en EMT están ausentes en el marco general de DBC, ya que puede tener un algoritmo de seguimiento capaz de considerar acciones sensoriales puras y, a diferencia de la divergencia de Kullback-Leibler, una medida de desviación de distribución capaz de manejar preferencias negativas. 5. El juego de la \"carrera de relevos\" fue introducido por primera vez en [11]. Es un problema de agente único de capturar una presa, y pertenece a la clase de problemas de barrido de área. Un ejemplo de dominio se muestra en la Figura 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figura 2: Dominio de etiquetas; un agente (A) intenta buscar y capturar una presa (Q). El juego de etiquetas limita extremadamente la percepción de los agentes, de modo que el agente solo puede detectar la presa si están ubicados en la misma celda del mundo de cuadrícula. En la versión clásica del juego, la co-locación conduce a una observación especial, y la acción de Etiqueta se puede realizar. Modificamos ligeramente esta configuración: el momento en que ambos agentes ocupan la misma celda, el juego termina. Como resultado, tanto el agente como su presa tienen la misma capacidad de movimiento, lo que les permite moverse en cuatro direcciones, Norte, Sur, Este y Oeste. Estos forman un espacio formal de acciones dentro de un entorno markoviano. El espacio de estados del entorno formal markoviano está descrito por el producto cruz de las posiciones del agente y la presa. Para la Figura 2, sería S = {s0, ..., s23} × {s0, ..., s23}. Los efectos de una acción tomada por el agente son deterministas, pero el entorno en general tiene una respuesta estocástica debido al movimiento de la presa. Con probabilidad q0 1 se queda en su lugar, y con probabilidad 1 − q0 se mueve a una celda adyacente más lejos del agente 1. En nuestros experimentos, esto se tomó como q0 = 0.2. Por lo tanto, para la instancia mostrada en la Figura 2 y q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Aunque el comportamiento evasivo de la presa es conocido por el agente, la posición de la presa no lo es. La única información sensorial disponible para el agente es su propia ubicación. Utilizamos EMT y especificamos directamente la dinámica del objetivo. Para el juego de la mancha, podemos formular fácilmente tres tendencias principales: atrapar al perseguido, mantenerse en movimiento y acechar al perseguido. Esto resulta en las siguientes tres dinámicas de objetivo: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 en otro caso Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 en otro caso Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Nótese que ninguno de los objetivos anteriores es directamente alcanzable; por ejemplo, si Qt = s9 y At = s11, no hay ninguna acción que pueda llevar al agente a At+1 = s9 como lo requiere la dinámica de objetivo Tcatch. Realizamos varios experimentos para evaluar el rendimiento de EMT en el Juego de Etiquetas. Se utilizaron tres configuraciones del dominio mostrado en la Figura 3, cada una planteando un desafío diferente para el agente debido a la observabilidad parcial. En cada escenario, se realizó un conjunto de 1000 ejecuciones con un límite de tiempo de 100 pasos. En cada ejecución, la posición inicial tanto del agente como de su presa fue seleccionada al azar; esto significa que, en lo que respecta al agente, la posición inicial de la presa estaba distribuida de forma uniforme en todo el espacio de celdas del dominio. También utilizamos dos variaciones de la función de observabilidad del entorno. En la primera versión, la función de observabilidad mapeaba todas las posiciones articulares del cazador y la presa en la posición del cazador como observación. En el segundo, solo aquellas posiciones conjuntas en las que el cazador y la presa ocupaban ubicaciones diferentes fueron mapeadas en la ubicación del cazador. La segunda versión de hecho utilizó y expresó el hecho de que una vez que el cazador y la presa ocupan la misma celda, el juego termina. Los resultados de estos experimentos se muestran en la Tabla 2. Equilibrando la dinámica de captura, movimiento y seguimiento descrita en la sección anterior mediante el vector de peso [0.8, 0.1, 0.1], EMT logró un rendimiento estable en los tres dominios. Aunque las comparaciones directas son difíciles de hacer, el rendimiento de los EMT mostró una notable eficiencia en comparación con el enfoque POMDP. A pesar de una implementación simple e ineficiente del algoritmo EMT en Matlab, el tiempo de decisión para cualquier paso dado promedió significativamente por debajo de 1 segundo en todos los experimentos. Para el dominio de la arena abierta irregular, que resultó ser el más difícil, se completaron 1000 ejecuciones de experimentos limitadas a 100 pasos cada una, un total de 42411 pasos, en poco menos de 6 horas. Es decir, más de 4 × 104 pasos en línea tomaron una orden de magnitud menos tiempo que la computación sin conexión de la política POMDP en [11]. La importancia de esta diferencia se destaca aún más por el hecho de que, en caso de que cambien los parámetros del modelo del entorno, la naturaleza en línea de EMT le permitiría mantener su rendimiento, mientras que la política POMDP necesitaría ser recalculada, lo que requeriría una vez más un gran costo computacional adicional. También probamos la entropía de la frecuencia de las células de comportamiento, medidas empíricas de los datos de prueba. Como muestran la Figura 4 y la Figura 5, empir794 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 3: Se utilizaron estas configuraciones del espacio de juego de Tag: a) múltiples callejones sin salida, b) arena abierta irregular, c) pasillo circular. Tabla 2: Rendimiento de la solución basada en EMT en tres dominios de juego de Tag y dos modelos de observabilidad: I) cantera de omniposición, II) la cantera no está en la posición de los cazadores. Modelo Dominio Captura% E(Pasos) Tiempo/Paso I Callejones sin salida 100 14.8 72(mSeg) Arena 80.2 42.4 500(mSeg) Círculo 91.4 34.6 187(mSeg) II Callejones sin salida 100 13.2 91(mSeg) Arena 96.8 28.67 396(mSeg) Círculo 94.4 31.63 204(mSeg) La entropía lógica crece con la duración de la interacción. Para las carreras en las que la presa no fue capturada de inmediato, la entropía alcanza valores entre 0.85 y 0.952 para diferentes carreras y escenarios. A medida que el agente busca activamente la presa, la entropía nunca alcanza su máximo. Una característica del gráfico de entropía para el escenario de arena abierta llamó especialmente nuestra atención en el caso del modelo de observación de cantera omniposicional. Cerca del límite máximo de longitud de prueba (100 pasos), la entropía cayó repentinamente. Un análisis adicional de los datos mostró que bajo ciertas circunstancias, se produce un comportamiento fluctuante en el que el agente se enfrenta a versiones igualmente viables del comportamiento de seguir a la presa. Dado que el algoritmo EMT tiene una selección de acciones codiciosa, y el espacio de estados no codifica ninguna forma de compromiso (ni siquiera velocidad o aceleración), el agente queda atrapado dentro de una pequeña porción de celdas. Básicamente, se intenta seguir simultáneamente varios cursos de acción, todos los cuales son consistentes con la dinámica objetivo. Este comportamiento no ocurrió en nuestro segundo modelo de observación, ya que redujo significativamente el conjunto de cursos de acción elegibles, contribuyendo esencialmente a romper el empate entre ellos. 6. DISCUSIÓN El diseño de la solución EMT para el Juego de Etiquetas expone la diferencia fundamental en el enfoque de planificación y control entre EMT o DBC, por un lado, y el enfoque más familiar de POMDP, por otro lado. POMDP define una estructura de recompensa para optimizar e influencia la dinámica del sistema de forma indirecta a través de esa optimización. EMT descarta cualquier esquema de recompensa y, en cambio, mide e influye directamente en la dinámica del sistema. Se calculó la entropía 2 utilizando el logaritmo con base igual al número de ubicaciones posibles dentro del dominio; esto escala adecuadamente la expresión de entropía en el rango [0, 1] para todos los dominios. Por lo tanto, para el Juego de Etiquetas, no buscamos una función de recompensa que codificara y expresara nuestra preferencia sobre el comportamiento de los agentes, sino que establecimos directamente tres preferencias de comportamiento (heurísticas) como base para mantener la dinámica del objetivo. Los datos experimentales muestran que estos objetivos no necesitan ser directamente alcanzables a través de las acciones de los agentes. Sin embargo, aún queda por explorar la relación entre el rendimiento de la EMT y la alcanzabilidad de la dinámica objetivo. Los datos del experimento del juego de etiquetas también revelaron la diferente importancia que DBC y POMDPs otorgan a la formulación del espacio de estados del entorno. Los POMDPs dependen enteramente del mecanismo de maximización de la acumulación de recompensas, es decir, la formación del procedimiento de selección de acciones para lograr la secuenciación de estados necesaria. DBC, por otro lado, tiene dos fuentes de especificación de secuenciación: a través de las propiedades de un procedimiento de selección de acciones y a través de una especificación directa dentro de la dinámica objetivo. La importancia de la segunda fuente fue subrayada por los datos del experimento del Juego de Etiquetas, en el que el algoritmo EMT codicioso, aplicado a una especificación de espacio de estados tipo POMDP, falló, ya que la descripción del objetivo sobre dicho espacio de estados no era capaz de codificar las tendencias de comportamiento necesarias, por ejemplo, la resolución de empates y el compromiso con el movimiento dirigido. Las diferencias estructurales entre DBC (y EMT en particular) y POMDPs impiden la comparación directa de rendimiento y los sitúan en pistas complementarias, cada uno dentro de un nicho adecuado. Por ejemplo, los POMDP podrían ser vistos como una formulación mucho más natural de problemas económicos de toma de decisiones secuenciales, mientras que EMT se ajusta mejor a la demanda continua de cambios estocásticos, como ocurre en muchos problemas de <br>robótica</br> o de agentes incorporados. Las propiedades complementarias de POMDPs y EMT pueden ser aún más explotadas. Existe un interés reciente en utilizar POMDPs en soluciones híbridas [17], en las cuales los POMDPs pueden ser utilizados junto con otros enfoques de control para proporcionar resultados que no son fácilmente alcanzables con ninguno de los enfoques por sí solos. DBC puede ser un socio efectivo en una solución híbrida como esta. Por ejemplo, los POMDP tienen requisitos de tiempo fuera de línea prohibitivamente grandes para el cálculo de políticas, pero pueden ser fácilmente utilizados en entornos más simples para exponer tendencias de comportamiento beneficiosas; esto puede servir como una forma de dinámica objetivo que se proporciona a EMT en un dominio más grande para su operación en línea. 7. CONCLUSIONES Y TRABAJO FUTURO En este artículo, hemos presentado una nueva perspectiva sobre el proceso de planificación y control en entornos estocásticos, en forma del marco de Control Basado en Dinámicas (DBC). DBC formula la tarea de planificar como el apoyo a la dinámica de un sistema objetivo especificado, que describe las propiedades necesarias del cambio dentro del entorno. La optimalidad de los planes de acción de DBC se mide en el Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Calles sin salida 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Círculo Figura 4: Modelo de Observación I: Cantera omniposicional. Desarrollo de entropía con longitud de juego de etiqueta para los tres escenarios experimentales: a) múltiples callejones sin salida, b) arena abierta irregular, c) pasillo circular. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Callejones sin salida 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Círculo Figura 5: Modelo de observación II: cantera no observada en la posición de los cazadores. Desarrollo de la entropía con la longitud del juego de etiquetas para los tres escenarios experimentales: a) múltiples callejones sin salida, b) arena abierta irregular, c) pasillo circular, con respecto a la desviación de la dinámica del sistema real de la dinámica objetivo. Mostramos que una técnica recientemente desarrollada de Seguimiento Markov Extendido (EMT) [13] es una instancia de DBC. De hecho, EMT puede ser visto como un caso específico de la parametrización DBC, que emplea un procedimiento de selección de acciones codicioso. Dado que EMT exhibe las características clave del marco general DBC, así como una complejidad temporal polinómica, utilizamos la versión multitarget de EMT [15] para demostrar que la clase de problemas de barrido de área se presta naturalmente a descripciones basadas en dinámicas, tal como se ejemplifica en nuestros experimentos en el dominio del Juego de Etiquetas. Como se enumera en la Sección 4.2, EMT tiene varias limitaciones, como la dificultad para manejar la preferencia dinámica negativa. Esto evita la aplicación directa de la EMT a problemas como los Juegos de Rendezvous-Evasión (por ejemplo, [6]). Sin embargo, DBC en general no tiene tales limitaciones y permite fácilmente la formulación de juegos de evasión. En trabajos futuros, tenemos la intención de continuar con el desarrollo de controladores basados en dinámica para estos problemas. 8. AGRADECIMIENTO El trabajo de los dos primeros autores fue parcialmente apoyado por la subvención #898/05 de la Fundación para la Ciencia de Israel, y el tercer autor fue parcialmente apoyado por una subvención del Ministerio de Ciencia y Tecnología de Israel. REFERENCIAS [1] R. C. Arkin. Robótica basada en el comportamiento. MIT Press, 1998. [2] J. \n\nMIT Press, 1998. [2] J. A. Bilmes. Un tutorial detallado del algoritmo EM y su aplicación en la estimación de parámetros para mezclas gaussianas y modelos ocultos de Markov. Informe técnico TR-97-021, Departamento de Ingeniería Eléctrica e Informática, Universidad de California en Berkeley, 1998. [3] T. M. Cover y J. A. Thomas. Elementos de teoría de la información. Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz y M. J. Wolverton. Una encuesta de investigación en planificación distribuida y continua. Revista de Inteligencia Artificial, 4:13-22, 1999. [5] V. R. Konda y J. N. Tsitsiklis. Algoritmos Actor-Crítico. Revista SIAM de Control y Optimización, 42(4):1143-1166, 2003. [6] W. S. Lim. Un juego de encuentro-evasión en ubicaciones discretas con aleatorización conjunta. Avances en Probabilidad Aplicada, 29(4):1004-1017, diciembre de 1997. [7] M. L. Littman, T. L. Dean y L. P. Kaelbling. Sobre la complejidad de resolver problemas de decisión de Markov. En Actas de la 11ª Conferencia Anual sobre Incertidumbre en Inteligencia Artificial (UAI-95), páginas 394-402, 1995. [8] O. Madani, S. Hanks y A. Condon. Sobre la indecidibilidad de la planificación probabilística y problemas relacionados de optimización estocástica. Revista de Inteligencia Artificial, 147(1-2):5-34, julio de 2003. [9] R. M. Neal y G. E. Hinton. Una vista del algoritmo EM 796 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) que justifica enfoques incrementales, dispersos y otras variantes. En M. I. Jordan, editor, Aprendizaje en Modelos Gráficos, páginas 355-368. Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez y S. Kraus. Seguridad en sistemas multiagentes mediante la aleatorización de políticas. En Actas de AAMAS 2006, 2006. [11] J. Pineau, G. Gordon y S. Thrun. Iteración de valor basada en puntos: Un algoritmo en cualquier momento para POMDPs. En la Conferencia Conjunta Internacional de Inteligencia Artificial (IJCAI), páginas 1025-1032, agosto de 2003. [12] M. L. Puterman. Procesos de Decisión de Markov. Serie Wiley en Probabilidad y Estadística Matemática: Sección de Probabilidad y Estadística Aplicada. Publicación de Wiley-Interscience, Nueva York, 1994. [13] Z. Rabinovich y J. S. Rosenschein. Seguimiento Markov extendido con una aplicación al control. En el taller sobre seguimiento de agentes: modelando otros agentes a partir de observaciones, en la Tercera Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, páginas 95-100, Nueva York, julio de 2004. [14] Z. Rabinovich y J. S. Rosenschein. Coordinación multiagente mediante Seguimiento Markov Extendido. En la Cuarta Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, páginas 431-438, Utrecht, Países Bajos, julio de 2005. [15] Z. Rabinovich y J. S. Rosenschein. Sobre la respuesta del control basado en EMT a objetivos y modelos interactivos. En la Quinta Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, páginas 465-470, Hakodate, Japón, mayo de 2006. [16] R. F. Stengel. Control óptimo y estimación. Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, y P. Varakantham. Conflictos en el trabajo en equipo: Híbridos para el Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 797 ",
            "candidates": [],
            "error": [
                [
                    "robóticas",
                    "robótica"
                ]
            ]
        },
        "dynamics base control": {
            "translated_key": "control de base de dinámicas",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Dynamics Based Control with an Application to Area-Sweeping Problems Zinovi Rabinovich Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel jeff@cs.huji.ac.il Gal A. Kaminka The MAVERICK Group Department of Computer Science Bar Ilan University, Israel galk@cs.biu.ac.il ABSTRACT In this paper we introduce Dynamics Based Control (DBC), an approach to planning and control of an agent in stochastic environments.",
                "Unlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics.",
                "We show that a recently developed planning and control approach, Extended Markov Tracking (EMT) is an instantiation of DBC.",
                "EMT employs greedy action selection to provide an efficient control algorithm in Markovian environments.",
                "We exploit this efficiency in a set of experiments that applied multitarget EMT to a class of area-sweeping problems (searching for moving targets).",
                "We show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation.",
                "Categories and Subject Descriptors I.2.8 [Problem Solving, Control Methods, and Search]: Control Theory; I.2.9 [Robotics]; I.2.11 [Distributed Artificial Intelligence]: Intelligent Agents General Terms Algorithms, Theory 1.",
                "INTRODUCTION Planning and control constitutes a central research area in multiagent systems and artificial intelligence.",
                "In recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in stochastic environments.",
                "In this framework, the planning and control problem is often addressed by imposing a reward function, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility.",
                "While theoretically attractive, the complexity of optimally solving a POMDP is prohibitive [8, 7].",
                "We take an alternative view of planning in stochastic environments.",
                "We do not use a (state-based) reward function, but instead optimize over a different criterion, a transition-based specification of the desired system dynamics.",
                "The idea here is to view planexecution as a process that compels a (stochastic) system to change, and a plan as a dynamic process that shapes that change according to desired criteria.",
                "We call this general planning framework Dynamics Based Control (DBC).",
                "In DBC, the goal of a planning (or control) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) target dynamics.",
                "As actual system behavior may deviate from that which is specified by target dynamics (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16].",
                "Here, optimality is measured in terms of probability of deviation magnitudes.",
                "In this paper, we present the structure of Dynamics Based Control.",
                "We show that the recently developed Extended Markov Tracking (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC.",
                "EMT is an efficient instantiation of DBC.",
                "To evaluate DBC, we carried out a set of experiments applying multi-target EMT to the Tag Game [11]; this is a variant on the area sweeping problem, where an agent is trying to tag a moving target (quarry) whose position is not known with certainty.",
                "Experimental data demonstrates that even with a simple model of the environment and a simple design of target dynamics, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agents position).",
                "The paper is organized as follows.",
                "In Section 2 we motivate DBC using area-sweeping problems, and discuss related work.",
                "Section 3 introduces the Dynamics Based Control (DBC) structure, and its specialization to Markovian environments.",
                "This is followed by a review of the Extended Markov Tracking (EMT) approach as a DBC-structured control regimen in Section 4.",
                "That section also discusses the limitations of EMT-based control relative to the general DBC framework.",
                "Experimental settings and results are then presented in Section 5.",
                "Section 6 provides a short discussion of the overall approach, and Section 7 gives some concluding remarks and directions for future work. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "MOTIVATION AND RELATED WORK Many real-life scenarios naturally have a stochastic target dynamics specification, especially those domains where there exists no ultimate goal, but rather system behavior (with specific properties) that has to be continually supported.",
                "For example, security guards perform persistent sweeps of an area to detect any sign of intrusion.",
                "Cunning thieves will attempt to track these sweeps, and time their operation to key points of the guards motion.",
                "It is thus advisable to make the guards motion dynamics appear irregular and random.",
                "Recent work by Paruchuri et al. [10] has addressed such randomization in the context of single-agent and distributed POMDPs.",
                "The goal in that work was to generate policies that provide a measure of action-selection randomization, while maintaining rewards within some acceptable levels.",
                "Our focus differs from this work in that DBC does not optimize expected rewards-indeed we do not consider rewards at all-but instead maintains desired dynamics (including, but not limited to, randomization).",
                "The Game of Tag is another example of the applicability of the approach.",
                "It was introduced in the work by Pineau et al. [11].",
                "There are two agents that can move about an area, which is divided into a grid.",
                "The grid may have blocked cells (holes) into which no agent can move.",
                "One agent (the hunter) seeks to move into a cell occupied by the other (the quarry), such that they are co-located (this is a successful tag).",
                "The quarry seeks to avoid the hunter agent, and is always aware of the hunters position, but does not know how the hunter will behave, which opens up the possibility for a hunter to surprise the prey.",
                "The hunter knows the quarrys probabilistic law of motion, but does not know its current location.",
                "Tag is an instance of a family of area-sweeping (pursuit-evasion) problems.",
                "In [11], the hunter modeled the problem using a POMDP.",
                "A reward function was defined, to reflect the desire to tag the quarry, and an action policy was computed to optimize the reward collected over time.",
                "Due to the intractable complexity of determining the optimal policy, the action policy computed in that paper was essentially an approximation.",
                "In this paper, instead of formulating a reward function, we use EMT to solve the problem, by directly specifying the target dynamics.",
                "In fact, any search problem with randomized motion, the socalled class of area sweeping problems, can be described through specification of such target system dynamics.",
                "Dynamics Based Control provides a natural approach to solving these problems. 3.",
                "DYNAMICS BASED CONTROL The specification of Dynamics Based Control (DBC) can be broken into three interacting levels: Environment Design Level, User Level, and Agent Level. • Environment Design Level is concerned with the formal specification and modeling of the environment.",
                "For example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation constant. • User Level in turn relies on the environment model produced by Environment Design to specify the target system dynamics it wishes to observe.",
                "The User Level also specifies the estimation or learning procedure for system dynamics, and the measure of deviation.",
                "In the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping. • Agent Level in turn combines the environment model from the Environment Design level, the dynamics estimation procedure, the deviation measure and the target dynamics specification from User Level, to produce a sequence of actions that create system dynamics as close as possible to the targeted specification.",
                "As we are interested in the continual development of a stochastic system, such as happens in classical control theory [16] and continual planning [4], as well as in our example of museum sweeps, the question becomes how the Agent Level is to treat the deviation measurements over time.",
                "To this end, we use a probability threshold-that is, we would like the Agent Level to maximize the probability that the deviation measure will remain below a certain threshold.",
                "Specific action selection then depends on system formalization.",
                "One possibility would be to create a mixture of available system trends, much like that which happens in Behavior-Based Robotic architectures [1].",
                "The other alternative would be to rely on the estimation procedure provided by the User Level-to utilize the Environment Design Level model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a certain dynamics has been achieved.",
                "Notice that this manipulation is not direct, but via the environment.",
                "Thus, for strong enough estimator algorithms, successful manipulation would mean a successful simulation of the specified target dynamics (i.e., beyond discerning via the available sensory input).",
                "DBC levels can also have a back-flow of information (see Figure 1).",
                "For instance, the Agent Level could provide data about target dynamics feasibility, allowing the User Level to modify the requirement, perhaps focusing on attainable features of system behavior.",
                "Data would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the User Level, this can provide an important tool for the environment model calibration at the Environment Design Level.",
                "UserEnv.",
                "Design Agent Model Ideal Dynamics Estimator Estimator Dynamics Feasibility System Response Data Figure 1: Data flow of the DBC framework Extending upon the idea of Actor-Critic algorithms [5], DBC data flow can provide a good basis for the design of a learning algorithm.",
                "For example, the User Level can operate as an exploratory device for a learning algorithm, inferring an ideal dynamics target from the environment model at hand that would expose and verify most critical features of system behavior.",
                "In this case, feasibility and system response data from the Agent Level would provide key information for an environment model update.",
                "In fact, the combination of feasibility and response data can provide a basis for the application of strong learning algorithms such as EM [2, 9]. 3.1 DBC for Markovian Environments For a Partially Observable Markovian Environment, DBC can be specified in a more rigorous manner.",
                "Notice how DBC discards rewards, and replaces it by another optimality criterion (structural differences are summarized in Table 1): • Environment Design level is to specify a tuple < S, A, T, O, Ω, s0 >, where: - S is the set of all possible environment states; - s0 is the initial state of the environment (which can also be viewed as a probability distribution over S); The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 791 - A is the set of all possible actions applicable in the environment; - T is the environments probabilistic transition function: T : S ×A → Π(S).",
                "That is, T(s |a, s) is the probability that the environment will move from state s to state s under action a; - O is the set of all possible observations.",
                "This is what the sensor input would look like for an outside observer; - Ω is the observation probability function: Ω : S × A × S → Π(O).",
                "That is, Ω(o|s , a, s) is the probability that one will observe o given that the environment has moved from state s to state s under action a. • User Level, in the case of Markovian environment, operates on the set of system dynamics described by a family of conditional probabilities F = {τ : S × A → Π(S)}.",
                "Thus specification of target dynamics can be expressed by q ∈ F, and the learning or tracking algorithm can be represented as a function L : O×(A×O)∗ → F; that is, it maps sequences of observations and actions performed so far into an estimate τ ∈ F of system dynamics.",
                "There are many possible variations available at the User Level to define divergence between system dynamics; several of them are: - Trace distance or L1 distance between two distributions p and q defined by D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Fidelity measure of distance F(p(·), q(·)) = x p(x)q(x) - Kullback-Leibler divergence DKL(p(·) q(·)) = x p(x) log p(x) q(x) Notice that the latter two are not actually metrics over the space of possible distributions, but nevertheless have meaningful and important interpretations.",
                "For instance, KullbackLeibler divergence is an important tool of information theory [3] that allows one to measure the price of encoding an information source governed by q, while assuming that it is governed by p. The User Level also defines the threshold of dynamics deviation probability θ. • Agent Level is then faced with a problem of selecting a control signal function a∗ to satisfy a minimization problem as follows: a∗ = arg min a Pr(d(τa, q) > θ) where d(τa, q) is a random variable describing deviation of the dynamics estimate τa, created by L under control signal a, from the ideal dynamics q.",
                "Implicit in this minimization problem is that L is manipulated via the environment, based on the environment model produced by the Environment Design Level. 3.2 DBC View of the State Space It is important to note the complementary view that DBC and POMDPs take on the state space of the environment.",
                "POMDPs regard state as a stationary snap-shot of the environment; whatever attributes of state sequencing one seeks are reached through properties of the control process, in this case reward accumulation.",
                "This can be viewed as if the sequencing of states and the attributes of that sequencing are only introduced by and for the controlling mechanism-the POMDP policy.",
                "DBC concentrates on the underlying principle of state sequencing, the system dynamics.",
                "DBCs target dynamics specification can use the environments state space as a means to describe, discern, and preserve changes that occur within the system.",
                "As a result, DBC has a greater ability to express state sequencing properties, which are grounded in the environment model and its state space definition.",
                "For example, consider the task of moving through rough terrain towards a goal and reaching it as fast as possible.",
                "POMDPs would encode terrain as state space points, while speed would be ensured by negative reward for every step taken without reaching the goalaccumulating higher reward can be reached only by faster motion.",
                "Alternatively, the state space could directly include the notion of speed.",
                "For POMDPs, this would mean that the same concept is encoded twice, in some sense: directly in the state space, and indirectly within reward accumulation.",
                "Now, even if the reward function would encode more, and finer, details of the properties of motion, the POMDP solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure.",
                "On the other hand, the tactical target expression of variations and correlations between position and speed of motion is now grounded in the state space representation.",
                "In this situation, any further constraints, e.g., smoothness of motion, speed limits in different locations, or speed reductions during sharp turns, are explicitly and uniformly expressed by the tactical target, and can result in faster and more effective action selection by a DBC algorithm. 4.",
                "EMT-BASED CONTROL AS A DBC Recently, a control algorithm was introduced called EMT-based Control [13], which instantiates the DBC framework.",
                "Although it provides an approximate greedy solution in the DBC sense, initial experiments using EMT-based control have been encouraging [14, 15].",
                "EMT-based control is based on the Markovian environment definition, as in the case with POMDPs, but its User and Agent Levels are of the Markovian DBC type of optimality. • User Level of EMT-based control defines a limited-case target system dynamics independent of action: qEMT : S → Π(S).",
                "It then utilizes the Kullback-Leibler divergence measure to compose a momentary system dynamics estimator-the Extended Markov Tracking (EMT) algorithm.",
                "The algorithm keeps a system dynamics estimate τt EMT that is capable of explaining recent change in an auxiliary Bayesian system state estimator from pt−1 to pt, and updates it conservatively using Kullback-Leibler divergence.",
                "Since τt EMT and pt−1,t are respectively the conditional and marginal probabilities over the systems state space, explanation simply means that pt(s ) = s τt EMT (s |s)pt−1(s), and the dynamics estimate update is performed by solving a 792 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: Structure of POMDP vs. Dynamics-Based Control in Markovian Environment Level Approach MDP Markovian DBC Environment < S, A, T, O, Ω >,where S - set of states A - set of actions Design T : S × A → Π(S) - transition O - observation set Ω : S × A × S → Π(O) User r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - reward function q - ideal dynamics F - reward remodeling L - dynamics estimator θ - threshold Agent π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) minimization problem: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • Agent Level in EMT-based control is suboptimal with respect to DBC (though it remains within the DBC framework), performing greedy action selection based on prediction of EMTs reaction.",
                "The prediction is based on the environment model provided by the Environment Design level, so that if we denote by Ta the environments transition function limited to action a, and pt−1 is the auxiliary Bayesian system state estimator, then the EMT-based control choice is described by a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1) Note that this follows the Markovian DBC framework precisely: the rewarding optimality of POMDPs is discarded, and in its place a dynamics estimator (EMT in this case) is manipulated via action effects on the environment to produce an estimate close to the specified target system dynamics.",
                "Yet as we mentioned, naive EMTbased control is suboptimal in the DBC sense, and has several additional limitations that do not exist in the general DBC framework (discussed in Section 4.2). 4.1 Multi-Target EMT At times, there may exist several behavioral preferences.",
                "For example, in the case of museum guards, some art items are more heavily guarded, requiring that the guards stay more often in their close vicinity.",
                "On the other hand, no corner of the museum is to be left unchecked, which demands constant motion.",
                "Successful museum security would demand that the guards adhere to, and balance, both of these behaviors.",
                "For EMT-based control, this would mean facing several tactical targets {qk}K k=1, and the question becomes how to merge and balance them.",
                "A balancing mechanism can be applied to resolve this issue.",
                "Note that EMT-based control, while selecting an action, creates a preference vector over the set of actions based on their predicted performance with respect to a given target.",
                "If these preference vectors are normalized, they can be combined into a single unified preference.",
                "This requires replacement of standard EMT-based action selection by the algorithm below [15]: • Given: - a set of target dynamics {qk}K k=1, - vector of weights w(k) • Select action as follows - For each action a ∈ A predict the future state distribution ¯pa t+1 = Ta ∗ pt; - For each action, compute Da = H(¯pa t+1, pt, PDt) - For each a ∈ A and qk tactical target, denote V (a, k) = DKL (Da qk) pt .",
                "Let Vk(a) = 1 Zk V (a, k), where Zk = a∈A V (a, k) is a normalization factor. - Select a∗ = arg min a k k=1 w(k)Vk(a) The weights vector w = (w1, ..., wK ) allows the additional tuning of importance among target dynamics without the need to redesign the targets themselves.",
                "This balancing method is also seamlessly integrated into the EMT-based control flow of operation. 4.2 EMT-based Control Limitations EMT-based control is a sub-optimal (in the DBC sense) representative of the DBC structure.",
                "It limits the User by forcing EMT to be its dynamic tracking algorithm, and replaces Agent optimization by greedy action selection.",
                "This kind of combination, however, is common for on-line algorithms.",
                "Although further development of EMT-based controllers is necessary, evidence so far suggests that even the simplest form of the algorithm possesses a great deal of power, and displays trends that are optimal in the DBC sense of the word.",
                "There are two further, EMT-specific, limitations to EMT-based control that are evident at this point.",
                "Both already have partial solutions and are subjects of ongoing research.",
                "The first limitation is the problem of negative preference.",
                "In the POMDP framework for example, this is captured simply, through The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 793 the appearance of values with different signs within the reward structure.",
                "For EMT-based control, however, negative preference means that one would like to avoid a certain distribution over system development sequences; EMT-based control, however, concentrates on getting as close as possible to a distribution.",
                "Avoidance is thus unnatural in native EMT-based control.",
                "The second limitation comes from the fact that standard environment modeling can create pure sensory actions-actions that do not change the state of the world, and differ only in the way observations are received and the quality of observations received.",
                "Since the world state does not change, EMT-based control would not be able to differentiate between different sensory actions.",
                "Notice that both of these limitations of EMT-based control are absent from the general DBC framework, since it may have a tracking algorithm capable of considering pure sensory actions and, unlike Kullback-Leibler divergence, a distribution deviation measure that is capable of dealing with negative preference. 5.",
                "EMT PLAYING TAG The Game of Tag was first introduced in [11].",
                "It is a single agent problem of capturing a quarry, and belongs to the class of area sweeping problems.",
                "An example domain is shown in Figure 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figure 2: Tag domain; an agent (A) attempts to seek and capture a quarry (Q) The Game of Tag extremely limits the agents perception, so that the agent is able to detect the quarry only if they are co-located in the same cell of the grid world.",
                "In the classical version of the game, co-location leads to a special observation, and the Tag action can be performed.",
                "We slightly modify this setting: the moment that both agents occupy the same cell, the game ends.",
                "As a result, both the agent and its quarry have the same motion capability, which allows them to move in four directions, North, South, East, and West.",
                "These form a formal space of actions within a Markovian environment.",
                "The state space of the formal Markovian environment is described by the cross-product of the agent and quarrys positions.",
                "For Figure 2, it would be S = {s0, ..., s23} × {s0, ..., s23}.",
                "The effects of an action taken by the agent are deterministic, but the environment in general has a stochastic response due to the motion of the quarry.",
                "With probability q0 1 it stays put, and with probability 1 − q0 it moves to an adjacent cell further away from the 1 In our experiments this was taken to be q0 = 0.2. agent.",
                "So for the instance shown in Figure 2 and q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Although the evasive behavior of the quarry is known to the agent, the quarrys position is not.",
                "The only sensory information available to the agent is its own location.",
                "We use EMT and directly specify the target dynamics.",
                "For the Game of Tag, we can easily formulate three major trends: catching the quarry, staying mobile, and stalking the quarry.",
                "This results in the following three target dynamics: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 otherwise Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 otherwise Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Note that none of the above targets are directly achievable; for instance, if Qt = s9 and At = s11, there is no action that can move the agent to At+1 = s9 as required by the Tcatch target dynamics.",
                "We ran several experiments to evaluate EMT performance in the Tag Game.",
                "Three configurations of the domain shown in Figure 3 were used, each posing a different challenge to the agent due to partial observability.",
                "In each setting, a set of 1000 runs was performed with a time limit of 100 steps.",
                "In every run, the initial position of both the agent and its quarry was selected at random; this means that as far as the agent was concerned, the quarrys initial position was uniformly distributed over the entire domain cell space.",
                "We also used two variations of the environment observability function.",
                "In the first version, observability function mapped all joint positions of hunter and quarry into the position of the hunter as an observation.",
                "In the second, only those joint positions in which hunter and quarry occupied different locations were mapped into the hunters location.",
                "The second version in fact utilized and expressed the fact that once hunter and quarry occupy the same cell the game ends.",
                "The results of these experiments are shown in Table 2.",
                "Balancing [15] the catch, move, and stalk target dynamics described in the previous section by the weight vector [0.8, 0.1, 0.1], EMT produced stable performance in all three domains.",
                "Although direct comparisons are difficult to make, the EMT performance displayed notable efficiency vis-`a-vis the POMDP approach.",
                "In spite of a simple and inefficient Matlab implementation of the EMT algorithm, the decision time for any given step averaged significantly below 1 second in all experiments.",
                "For the irregular open arena domain, which proved to be the most difficult, 1000 experiment runs bounded by 100 steps each, a total of 42411 steps, were completed in slightly under 6 hours.",
                "That is, over 4 × 104 online steps took an order of magnitude less time than the offline computation of POMDP policy in [11].",
                "The significance of this differential is made even more prominent by the fact that, should the environment model parameters change, the online nature of EMT would allow it to maintain its performance time, while the POMDP policy would need to be recomputed, requiring yet again a large overhead of computation.",
                "We also tested the behavior cell frequency entropy, empirical measures from trial data.",
                "As Figure 4 and Figure 5 show, empir794 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) A Q Q A 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 A Q Figure 3: These configurations of the Tag Game space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor Table 2: Performance of the EMT-based solution in three Tag Game domains and two observability models: I) omniposition quarry, II) quarry is not at hunters position Model Domain Capture% E(Steps) Time/Step I Dead-ends 100 14.8 72(mSec) Arena 80.2 42.4 500(mSec) Circle 91.4 34.6 187(mSec) II Dead-ends 100 13.2 91(mSec) Arena 96.8 28.67 396(mSec) Circle 94.4 31.63 204(mSec) ical entropy grows with the length of interaction.",
                "For runs where the quarry was not captured immediately, the entropy reaches between 0.85 and 0.952 for different runs and scenarios.",
                "As the agent actively seeks the quarry, the entropy never reaches its maximum.",
                "One characteristic of the entropy graph for the open arena scenario particularly caught our attention in the case of the omniposition quarry observation model.",
                "Near the maximum limit of trial length (100 steps), entropy suddenly dropped.",
                "Further analysis of the data showed that under certain circumstances, a fluctuating behavior occurs in which the agent faces equally viable versions of quarry-following behavior.",
                "Since the EMT algorithm has greedy action selection, and the state space does not encode any form of commitment (not even speed or acceleration), the agent is locked within a small portion of cells.",
                "It is essentially attempting to simultaneously follow several courses of action, all of which are consistent with the target dynamics.",
                "This behavior did not occur in our second observation model, since it significantly reduced the set of eligible courses of action-essentially contributing to tie-breaking among them. 6.",
                "DISCUSSION The design of the EMT solution for the Tag Game exposes the core difference in approach to planning and control between EMT or DBC, on the one hand, and the more familiar POMDP approach, on the other.",
                "POMDP defines a reward structure to optimize, and influences system dynamics indirectly through that optimization.",
                "EMT discards any reward scheme, and instead measures and influences system dynamics directly. 2 Entropy was calculated using log base equal to the number of possible locations within the domain; this properly scales entropy expression into the range [0, 1] for all domains.",
                "Thus for the Tag Game, we did not search for a reward function that would encode and express our preference over the agents behavior, but rather directly set three (heuristic) behavior preferences as the basis for target dynamics to be maintained.",
                "Experimental data shows that these targets need not be directly achievable via the agents actions.",
                "However, the ratio between EMT performance and achievability of target dynamics remains to be explored.",
                "The tag game experiment data also revealed the different emphasis DBC and POMDPs place on the formulation of an environment state space.",
                "POMDPs rely entirely on the mechanism of reward accumulation maximization, i.e., formation of the action selection procedure to achieve necessary state sequencing.",
                "DBC, on the other hand, has two sources of sequencing specification: through the properties of an action selection procedure, and through direct specification within the target dynamics.",
                "The importance of the second source was underlined by the Tag Game experiment data, in which the greedy EMT algorithm, applied to a POMDP-type state space specification, failed, since target description over such a state space was incapable of encoding the necessary behavior tendencies, e.g., tie-breaking and commitment to directed motion.",
                "The structural differences between DBC (and EMT in particular), and POMDPs, prohibits direct performance comparison, and places them on complementary tracks, each within a suitable niche.",
                "For instance, POMDPs could be seen as a much more natural formulation of economic sequential decision-making problems, while EMT is a better fit for continual demand for stochastic change, as happens in many robotic or embodied-agent problems.",
                "The complementary properties of POMDPs and EMT can be further exploited.",
                "There is recent interest in using POMDPs in hybrid solutions [17], in which the POMDPs can be used together with other control approaches to provide results not easily achievable with either approach by itself.",
                "DBC can be an effective partner in such a hybrid solution.",
                "For instance, POMDPs have prohibitively large off-line time requirements for policy computation, but can be readily used in simpler settings to expose beneficial behavioral trends; this can serve as a form of target dynamics that are provided to EMT in a larger domain for on-line operation. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have presented a novel perspective on the process of planning and control in stochastic environments, in the form of the Dynamics Based Control (DBC) framework.",
                "DBC formulates the task of planning as support of a specified target system dynamics, which describes the necessary properties of change within the environment.",
                "Optimality of DBC plans of action are measured The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 4: Observation Model I: Omniposition quarry.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 5: Observation Model II: quarry not observed at hunters position.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. with respect to the deviation of actual system dynamics from the target dynamics.",
                "We show that a recently developed technique of Extended Markov Tracking (EMT) [13] is an instantiation of DBC.",
                "In fact, EMT can be seen as a specific case of DBC parameterization, which employs a greedy action selection procedure.",
                "Since EMT exhibits the key features of the general DBC framework, as well as polynomial time complexity, we used the multitarget version of EMT [15] to demonstrate that the class of area sweeping problems naturally lends itself to dynamics-based descriptions, as instantiated by our experiments in the Tag Game domain.",
                "As enumerated in Section 4.2, EMT has a number of limitations, such as difficulty in dealing with negative dynamic preference.",
                "This prevents direct application of EMT to such problems as Rendezvous-Evasion Games (e.g., [6]).",
                "However, DBC in general has no such limitations, and readily enables the formulation of evasion games.",
                "In future work, we intend to proceed with the development of dynamics-based controllers for these problems. 8.",
                "ACKNOWLEDGMENT The work of the first two authors was partially supported by Israel Science Foundation grant #898/05, and the third author was partially supported by a grant from Israels Ministry of Science and Technology. 9.",
                "REFERENCES [1] R. C. Arkin.",
                "Behavior-Based Robotics.",
                "MIT Press, 1998. [2] J.",
                "A. Bilmes.",
                "A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and Hidden Markov Models.",
                "Technical Report TR-97-021, Department of Electrical Engeineering and Computer Science, University of California at Berkeley, 1998. [3] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of information theory.",
                "Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz, and M. J. Wolverton.",
                "A survey of research in distributed, continual planning.",
                "AI Magazine, 4:13-22, 1999. [5] V. R. Konda and J. N. Tsitsiklis.",
                "Actor-Critic algorithms.",
                "SIAM Journal on Control and Optimization, 42(4):1143-1166, 2003. [6] W. S. Lim.",
                "A rendezvous-evasion game on discrete locations with joint randomization.",
                "Advances in Applied Probability, 29(4):1004-1017, December 1997. [7] M. L. Littman, T. L. Dean, and L. P. Kaelbling.",
                "On the complexity of solving Markov decision problems.",
                "In Proceedings of the 11th Annual Conference on Uncertainty in Artificial Intelligence (UAI-95), pages 394-402, 1995. [8] O. Madani, S. Hanks, and A. Condon.",
                "On the undecidability of probabilistic planning and related stochastic optimization problems.",
                "Artificial Intelligence Journal, 147(1-2):5-34, July 2003. [9] R. M. Neal and G. E. Hinton.",
                "A view of the EM algorithm 796 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) that justifies incremental, sparse, and other variants.",
                "In M. I. Jordan, editor, Learning in Graphical Models, pages 355-368.",
                "Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In Proceeding of AAMAS 2006, 2006. [11] J. Pineau, G. Gordon, and S. Thrun.",
                "Point-based value iteration: An anytime algorithm for pomdps.",
                "In International Joint Conference on Artificial Intelligence (IJCAI), pages 1025-1032, August 2003. [12] M. L. Puterman.",
                "Markov Decision Processes.",
                "Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics Section.",
                "Wiley-Interscience Publication, New York, 1994. [13] Z. Rabinovich and J. S. Rosenschein.",
                "Extended Markov Tracking with an application to control.",
                "In The Workshop on Agent Tracking: Modeling Other Agents from Observations, at the Third International Joint Conference on Autonomous Agents and Multiagent Systems, pages 95-100, New-York, July 2004. [14] Z. Rabinovich and J. S. Rosenschein.",
                "Multiagent coordination by Extended Markov Tracking.",
                "In The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 431-438, Utrecht, The Netherlands, July 2005. [15] Z. Rabinovich and J. S. Rosenschein.",
                "On the response of EMT-based control to interacting targets and models.",
                "In The Fifth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 465-470, Hakodate, Japan, May 2006. [16] R. F. Stengel.",
                "Optimal Control and Estimation.",
                "Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, and P. Varakantham.",
                "Conflicts in teamwork: Hybrids to the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 797"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        }
    }
}